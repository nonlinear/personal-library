#!/usr/bin/env python3
"""
Index books using LlamaIndex + local sentence-transformers embeddings
Auto-partitions by topic for MCP lazy-loading optimization
"""

import os
import sys
import json
import subprocess
from pathlib import Path

from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, StorageContext, Settings, Document
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.readers.file import EpubReader

# Setup local embeddings (384-dim, no API key needed)
MODELS_DIR = Path(__file__).parent.parent / "models"
embed_model = HuggingFaceEmbedding(
    model_name="sentence-transformers/all-MiniLM-L6-v2",
    cache_folder=str(MODELS_DIR)
)
Settings.embed_model = embed_model
print("‚úì Using local embedding model: all-MiniLM-L6-v2 (384-dim)")

# Paths
BOOKS_DIR = Path(__file__).parent.parent / "books"
METADATA_FILE = BOOKS_DIR / "metadata.json"

print("üìö Indexing books with local embeddings")
print("=" * 50)

# Load metadata
print("\n1. Loading metadata...")
with open(METADATA_FILE, 'r') as f:
    metadata_json = json.load(f)

print(f"   Found {len(metadata_json['topics'])} topics")

# Load all documents
print("\n2. Loading books...")
all_documents = []

for topic in metadata_json['topics']:
    topic_id = topic['id']
    topic_label = topic['label']
    # Use label (original folder name) not id (slugified)
    topic_dir = BOOKS_DIR / topic_label

    print(f"\n   Topic: {topic_label}")

    for book in topic['books']:
        book_path = topic_dir / book['filename']

        if not book_path.exists():
            print(f"      ‚ö†Ô∏è  Not found: {book['filename']}")
            continue

        print(f"      Loading: {book['title']}")

        # Load EPUB
        reader = EpubReader()
        try:
            docs = reader.load_data(str(book_path))

            # Add metadata to each document
            for doc in docs:
                doc.metadata = {
                    'book_id': book['id'],
                    'book_title': book['title'],
                    'book_author': book['author'],
                    'topic_id': topic_id,
                    'topic_label': topic_label,
                    'tags': ','.join(book['tags'])
                }

            all_documents.extend(docs)
            print(f"         ‚úì Loaded {len(docs)} chunks")

        except Exception as e:
            print(f"         ‚ùå Error: {e}")

print(f"\n   Total: {len(all_documents)} document chunks")

# Create index
print("\n3. Building vector index...")
print("   This will take ~1-2 minutes with local embeddings...")

index = VectorStoreIndex.from_documents(all_documents, show_progress=True)

print("   ‚úì Index built")

# Save index temporarily (needed for partitioning)
print("\n4. Saving to temporary storage...")
import tempfile
temp_dir = tempfile.mkdtemp()
index.storage_context.persist(persist_dir=temp_dir)

# Partition by topic for lazy loading
print("\n5. Partitioning by topic for MCP optimization...")
import subprocess
partition_script = Path(__file__).parent / "partition_storage.py"
result = subprocess.run([sys.executable, str(partition_script), temp_dir], capture_output=True, text=True)
if result.returncode == 0:
    print("   ‚úì Storage partitioned by topic")

    # Clean up temporary directory
    import shutil
    shutil.rmtree(temp_dir)
    print("   ‚úì Cleaned up temporary files")
else:
    print(f"   ‚ö†Ô∏è  Partitioning failed: {result.stderr}")

print("\nüéâ Indexing complete!")
print(f"   Documents: {len(all_documents):,}")
print(f"   Storage: books/<topic>/")
print(f"   Structure: topic-partitioned (lazy-loading ready)")
