[
  {
    "chunk_full": "![image](../images/9780262542043_FC.jpg)\n\n\nCode as Creative Medium\n\n\nCode as Creative Medium\n\nA Handbook for Computational Art and Design\n\nBy Golan Levin and Tega Brain\n\nThe MIT Press\n\nCambridge, Massachusetts\n\nLondon, England\n\n\n© 2021 Massachusetts Institute of Technology\n\nAll rights reserved. No part of this book may be reproduced in any form by any\nelectronic or mechanical means (including photocopying, recording, or\ninformation storage and retrieval) without permission in writing from the\npublisher.\n\nLibrary of Congress Cataloging-in-Publication Data\n\nNames: Levin, Golan, author. | Brain, Tega, author.\n\nTitle: Code as creative medium : a handbook for computational art and design /\nby Golan Levin and Tega Brain.\n\nDescription: Cambridge : The MIT Press, [2021] | Includes bibliographical references and index.\n\nIdentifiers: LCCN 2020022997 | ISBN 9780262542043 (paperback)\n\nSubjects: LCSH: Computer art—Study and teaching. | Computer art—Problems, exercises, etc. | Computer programming—Problems, exercises, etc.\n\nClassification: LCC N7433.83 .L48 2021 | DDC 776.07—dc23\n\nLC record available at <https://lccn.loc.gov/2020022997>\n\n10 9 8 7 6 5 4 3 2 1\n\nThis project was supported in part by the National Endowment for the Arts,\nthrough Art Works grant #1855045-34-19 in the Media Arts discipline. To find\nout more about how National Endowment for the Arts grants impact individuals\nand communities, visit [www.arts.gov](http://www.arts.gov).\n\nThis project was also made possible through support from the Frank-Ratchye\nFund for Art @ the Frontier (FRFAF), and the generosity of Edward H. Frank and\nSarah G. Ratchye, administered by the Frank-Ratchye STUDIO for Creative\nInquiry at Carnegie Mellon University; and from the Integrated Digital Media\nProgram at NYU Tandon School of Engineering.\n\n![c0-fig-0001.jpg](../images/c0-fig-0001.jpg)\n![c0-fig-0002.jpg](../images/c0-fig-0002.jpg)\n![c0-fig-0003.jpg](../images/c0-fig-0003.jpg)\n\nd_r0\n\n\n# Contents\n\n  1. [Foreword](c000a.xhtml#f2-title-0001)\n  2. [Introduction](c000z.xhtml#f99-title-0001)\n  3. [Part One: Assignments](p01.xhtml#p1-title-0001)\n     1. [Iterative Pattern](c001.xhtml#c1-title-0002)\n     2. [Face Generator](c002.xhtml#c2-title-0001)\n     3. [Clock](c003.xhtml#c3-title-0001)\n     4. [Generative Landscape](c004.xhtml#c4-title-0001)\n     5. [Virtual Creature](c005.xhtml#c5-title-0001)\n     6. [Custom Pixel](c006.xhtml#c6-title-0001)\n     7. [Drawing Machine](c007.xhtml#c7-title-0001)\n     8. [Modular Alphabet](c008.xhtml#c8-title-0001)\n     9. [Data Self-Portrait](c009.xhtml#c9-title-0001)\n     10. [Augmented Projection](c010.xhtml#c10-title-0001)\n     11. [One-Button Game](c011.xhtml#c11-title-0001)\n     12. [Bot](c012.xhtml#c12-title-0001)\n     13. [Collective Memory](c013.xhtml#c13-title-0001)\n     14. [Experimental Chat](c014.xhtml#c14-title-0001)\n     15. [Browser Extension](c015.xhtml#c15-title-0001)\n     16. [Creative Cryptography](c016.xhtml#c16-title-0001)\n     17. [Voice Machine](c017.xhtml#c17-title-0001)\n     18. [Measuring Device](c018.xhtml#c18-title-0001)\n     19. [Personal Prosthetic](c019.xhtml#c19-title-0001)\n     20. [Parametric Object](c020.xhtml#c20-title-0001)\n     21. [Virtual Public Sculpture](c021.xhtml#c21-title-0001)\n     22. [Extrapolated Body](c022.xhtml#c22-title-0001)\n     23. [Synesthetic Instrument](c023.xhtml#c23-title-0001)\n  4. [Part Two: Exercises](p02.xhtml#p2-title-0001)\n     1. [Computing without a Computer](c024.xhtml#c24-title-0002)\n     2. [Graphic Elements](c025.xhtml#c25-title-0001)\n     3. [Iteration](c026.xhtml#c26-title-0001)\n     4. [Color](c027.xhtml#c27-title-0001)\n     5. [Conditional Testing](c028.xhtml#c28-title-0001)\n     6. [Unpredictability](c029.xhtml#c29-title-0001)\n     7. [Arrays](c030.xhtml#c30-title-0001)\n     8. [Time and Interactivity](c031.xhtml#c31-title-0001)\n     9. [Typography](c032.xhtml#c32-title-0001)\n     10. [Curves](c033.xhtml#c33-title-0001)\n     11. [Shapes](c034.xhtml#c34-title-0001)\n     12. [Geometry](c035.xhtml#c35-title-0001)\n     13. [Image](c036.xhtml#c36-title-0001)\n     14. [Visualization](c037.xhtml#c37-title-0001)\n     15. [Text and Language](c038.xhtml#c38-title-0001)\n     16. [Simulation](c039.xhtml#c39-title-0001)\n     17. [Machine Learning](c040.xhtml#c40-title-0001)\n     18. [Sound](c041.xhtml#c41-title-0001)\n     19. [Games](c042.xhtml#c42-title-0001)\n  5. [Part Three: Interviews](p03.xhtml#p3-title-0001)\n     1. [Teaching Programming to Artists and Designers](c043.xhtml#c43-title-0002)\n     2. [The Bimodal Classroom](c044.xhtml#c44-title-0001)\n     3. [Encouraging a Point of View](c045.xhtml#c45-title-0001)\n     4. [The First Day](c046.xhtml#c46-title-0001)\n     5. [Favorite Assignment](c047.xhtml#c47-title-0001)\n     6. [When Things Go Wrong](c048.xhtml#c48-title-0001)\n     7. [Most Memorable Response](c049.xhtml#c49-title-0001)\n     8. [Advice for New Educators](c050.xhtml#c50-title-0001)\n  6. [Classroom Techniques](bapp01.xhtml#b1-title-0001)\n  7. [Provenance](bapp02.xhtml#b2-title-0001)\n  8. [Appendices](p04.xhtml#p4-title-0001)\n     1. [Authors and Contributors](bapp03.xhtml#b3-title-0002)\n     2. [Notes on Computational Book Design](bapp04.xhtml#b4-title-0001)\n     3. [Acknowledgments](bapp05.xhtml#b5-title-0001)\n  9. [Bibliographies](p05.xhtml#p5-title-0001)\n     1. [Related Resources](c444.xhtml#b6-title-0001)\n     2. [Illustration Credits](c444a.xhtml#b7-title-0001)\n  10. [Indexes](p06.xhtml#p6-title-0001)\n     1. [Name Index](c555.xhtml#b8-title-0001)\n     2. [Subject Index](c555a.xhtml#b9-title-0001)\n\n## Guide\n\n  1. [Cover](cover.xhtml)\n  2. [Table of Contents](contents.xhtml)\n  3. [Begin Reading](c000z.xhtml)\n\n\n# Foreword\n\nCasey Reas\n\nJune 2020\n\nWhen I was first learning to code, this book would have made a world of\ndifference. I had been trying to learn for three years on and off—on my own\nand then through an evening extension class in 1997—and there were no\nresources I knew of at that time that were devoted to coding through the\nvisual arts. My class only provided examples and exercises in the domains of\nmath and text. (My final assignment was to make an accounting system for a\nfictional bank.) It was a challenge and a chore, but I knew I needed\nprogramming skills to make what I wanted to make.\n\nOutside of class, when I finally knew enough C to start making visual things,\neverything changed. My motivation kicked in and I learned more in a few weeks\nthan I had learned in months prior. The resulting collection of cryptic\nexperiments called _Reactive 006_ opened the door for me to join John Maeda's\nAesthetics + Computation Group (ACG) at the MIT Media Lab in 1999. The ACG was\nthe place I had been looking for. As a small research group, it brought\ntogether a wide range of artists, designers, and coders to explore a new kind\nof synthesis between those domains. Some of those early experiments evolved\ninto the initial set of examples for Processing 1.0, which Ben Fry and I\nreleased in 2001. My two years in ACG clarified my future and led to my first\nexperiences teaching code.\n\nAs I discovered firsthand, the teaching style for learning to code within\ncomputer science rarely worked for visual arts students, so those of us who\ntaught visual artists and designers invented new approaches to immerse\nstudents in this way of thinking and making. This meant breaking the existing\nteaching methods apart and building them again in new ways. Starting with the\nexercises in John Maeda's _Design By Numbers_ book, I made guesses about what\nmight work in my classroom and then slowly improved the curriculum year by\nyear by adding and removing assignments while figuring out the balance between\ncode and ideas. I compared notes with others and, over time, something very\ndifferent from the original computer science curriculum emerged.\n\nThe first Eyeo Festival in 2011 was a pivotal moment in this story, when a\nloose online network of artists, designers, educators, and technologists\nconverged in Minneapolis to meet in person for the first time. The 2013\nfestival supported the first “Code+Ed” summit, which brought together a large\ngroup of committed educators. Tega and Golan both participated in this full\nday of sharing and recording new ideas for a creative coding curriculum, and\nit was where they began to research and collect the teaching techniques and\nstrategies of hundreds of educators. This book is the first to collect this\ncommunity's wisdom in one place, to better share it with new generations of\ninstructors.\n\nGreat artworks get remembered, but not the humble methods of artist training.\nOur collective online courses are fragile, with more and more of that material\nlost month by month as URLs and servers change. This book is important in the\nsame way that Johannes Itten's _Design and Form_ opened a window into Bauhaus\npedagogy. It preserves assignments and exercises at this moment of transition\nin arts education when we're all collectively trying to figure it out.\n\nLike many others then and now, I first learned to code through reading books;\nyet all books about coding grapple with which language to use. Python, Java,\nC++, Javascript? Any choice excludes groups of educators and learners and\nnarrows the audience. Tega and Golan have addressed this dilemma by making\n_Code as Creative Medium_ language-agnostic—it smartly doesn't include code\nwithin the book. This decision has allowed them to focus on higher-level\nconcepts related to code and the arts, without the requirements of explaining\ncoding fundamentals. Subjects like color, drawing, landscapes, and self-\nportraits become the primary axes and technical topics like variables,\nfunctions, and arrays are secondary. This is an important and exciting\nreversal. _How refreshing to have a creative coding book that won't quickly\nbecome obsolete!_\n\nHow can we engage “creative people” with the strange way of writing that is\ncode? How can we engage “code people” with a sophisticated visual arts\ncurriculum? _Code as Creative Medium_ tackles these difficult questions by\ncurating over 30 years of exploration in visual arts education. It not only\noffers guidance on new ways to involve students; instructors will find\nthemselves challenged and inspired as well. I've taught visual arts students\nfor two decades and I learned something new on every page of this book.\nThere's enough material here to build curricula for multiple, diverse courses.\nIn addition, it can be used for teaching a weekend workshop, to build a\ncreative coding module for high school students, or to seed a new certificate\nprogram. It is an _essential_ resource for a rapidly evolving field.\n\nThank you, Tega and Golan, for such a thoughtful and generous gift to our\nexpanding community. I'm amazed at how far we've come in the last twenty\nyears. With this book as our guide, we can travel so much further. Onward!\n\n\n# Introduction\n\nLate one summer, we found ourselves discussing the highs and lows of teaching\ncomputational art and design as we prepared our syllabi for the fast-\napproaching term. As we swapped ideas for exercises, we discovered that we had\neach chosen to assign “the clock,” a project that asks students to develop a\ndynamic representation of time. We had been educated in different\ncountries—Golan in the USA and Tega in Australia—and yet it became clear that\nwe shared an extensive tradition of such assignments: traded, like folk tales,\nby those teaching code in art and design schools around the world. These\npedagogic traditions, however, were mostly absent from the programming primers\non our bookshelves, which dealt above all with _how_ to write code, rather\nthan _what_ to make when learning to code and _why_. This book is our attempt\nto address this gap and capture an evolving vernacular. It is a book for\nartists, designers, and poets who are already working with code as a creative\nmedium, or who are curious about doing so. It is for computer scientists,\nsoftware developers, and engineers who are looking for more open-ended,\nexpressive, or poignant ways to apply their skills. And it is a playbook for\neducators in all of these fields—a companion to help plan a semester's\njourney, and a guide to bring students into a creative, thoughtful, and\nultimately transformative engagement with computation. Our project draws on\nresearch, educational materials, and candid firsthand accounts from a vibrant\ncommunity of practitioners and educators, all of whom are navigating the\nchallenges of blending engineering and poetry in their courses and creative\nwork.\n\nIt's difficult to overstate the importance of computational literacy in 21st-\ncentury life. Computer programming, once an esoteric skill in engineering and\nbusiness, has now acquired broad applicability in fine arts, design,\narchitecture, music, humanities, journalism, activism, poetry, and many other\ncreative fields. But in the classroom, the adaptation of programming education\nto students with different objectives and learning styles has not kept apace.\nInstead, for more than half a century, computer science departments have\ndefined the design, delivery, and cultural norms of programming curricula—and\nthe resulting mismatch between traditional computer science education and art\nand design students has become a persistent challenge. As designer-engineer\nLeah Buechley has observed, traditional computer science courses often fail to\nconnect with students who learn best from concrete experiences, not abstract\nprinciples; who prefer to work improvisationally, instead of following\nformulas; and who aim to create things that are expressive, rather than\nutilitarian.1 We are witnessing the shear between the history of _computer\nscience as a discipline_ , concerned with the nature and optimality of\nalgorithms, and the new reality of _computer programming as a skill_ , a form\nof basic literacy with practical utility (and idioms) in every field. There is\nalso a growing understanding that programming education needs to reach more\ndiverse and previously underserved populations, and an appreciation that new\nand more culturally potent teaching methods are required to achieve this.2\nFortunately, such alternative pedagogies—in the form of arts-oriented\nprogramming toolkits, classroom approaches, assignments, and community support\nstructures—have arisen within the growing set of cultural practices known as\n“creative coding.”\n\n“Creative coders” are artists, designers, architects, musicians, and poets who\nuse computer programming and custom software as their chosen media. These\npractitioners blur the distinction between art and design and science and\nengineering, and in their slippery interdisciplinarity, may best be described\nwith the German word _Gestaltern_ , or “creators of form.” Their tools of\nchoice are programming environments like Processing, p5.js, Tracery,\nMax/MSP/Jitter, Arduino, Cinder, openFrameworks, and Unity, all of which have\nbeen developed to honor the particular needs and working styles of both\nprofessionals and students with visual-spatial, musical-rhythmic, verbal-\nlinguistic, and bodily-kinesthetic intelligences.4 Because many of these\ncreative coding toolkits are free and open-source, they have radically\ndemocratized software development, positioning programming as a potent mode of\ncultural inquiry. The exercises and assignments in this book assume that the\nlearner is using one or another of these creative coding tools, or something\nlike them.\n\n* * *\n\n_Processing seeks to ruin the careers of talented designers by tempting them\naway from their usual tools and into the world of programming and computation.\nSimilarly, the project is designed to turn engineers and computer scientists\nto less gainful employment as artists and designers._\n\n—Ben Fry and Casey Reas3\n\n* * *\n\nWho Is This Book For?\n\nThis book is a manual and sourcebook for teaching and learning the use of code\nas a creative medium, and for exploring programming in a more vibrant cultural\ncontext. It contends with software as an artistic material, offering pathways\nto learn its grain and texture, strengths and limitations. We hope this book\nwill find use among university educators (in fields like media arts, design,\ninformatics, media studies, and human-computer interaction); those who teach\ncomputer programming in high schools, or as a general undergraduate education\nrequirement, such as “CS for non-majors” courses; and workshop leaders (at\nmakerspaces, hackerspaces, code schools, educational retreats, and adult\neducation programs). We also believe this book may be directly useful to\nartists, designers, and creative autodidacts who seek prompts for self-\ndirected software projects.\n\n* * *\n\n_To me, the title_ artist _said nothing about one's medium, message or format.\nIt spoke instead to a certain type of wideness, a flexible approach to\nconsidering the world._\n\n—Mimi Onuoha5\n\n* * *\n\nWhy make a book for _teachers_? There are many books that serve to introduce\nstudents to the basics of computational art and design, but few have been\nwritten with the educator in mind. This is surprising given that so many of us\nteach. We teach to nurture and mentor the next generation, we teach to support\nour art and design practices, and we teach for the love of being in dialogue\nwith those who are different than ourselves. We often do this in an\nincreasingly corporatized university system that relies heavily on income from\nteaching, but provides almost no training to faculty on how to do it well.\nWith little else to go on, approaching the lectern after the completion of\ngraduate studies often involves reverse engineering the pedagogy of our own\nfavorite teachers: how did they manage class time? How did they structure\ntheir assignments? To such a reader, this book offers a collection of helpful\npatterns. They are taken from our own experiences as educators, collected\ndirectly from more than a dozen of our colleagues, and distilled from the\nonline syllabi and other resources posted by hundreds of our peers.\n\nWhat's Worth Making, and Why?\n\nCreative coding courses are now a standard offering in many art and design\nprograms, reflecting not only the incursion of computing into everyday life,\nbut also the ongoing work of software arts tooling, teaching practices, and\ncommunity-building. We are indebted to decades of work on toolkits that\naccommodate diverse learning styles, have clear syntax, and are supported by\ngenerous and inclusive documentation. The creative coding community's focus on\n_how_ to code has dramatically expanded to _whom_ it has become accessible.\nBut it has also led us to equally important questions about _what_ to make and\n_why_ to make it.\n\nIn computational art and design, many responses to the questions of _what_ and\n_why_ continue historic lines of creative inquiry centered on procedure,\nconnection, abstraction, authorship, the nature of time, and the role of\nchance. The pursuit of these formal and conceptual concerns in the medium of\ncomputation has created new practices and aesthetics, and has also heightened\na sensibility to the forces and flows of computation itself. Creators have\nbecome attuned to the ways in which working computationally encourages certain\nperspectives and occludes others. Media artists, computational designers, and\nother creative coders are the bards of a generative and interactive poetics,\nand, in the words of Julie Perini, “can provide dis-alienating experiences for\na society desperately in need of healing.” 7\n\n* * *\n\n_The main challenge is trying to create work that touches people at an\nemotional level, as opposed to them thinking about the technology or wondering\nabout how it was made. Making poems, not demos, is how we refer to it, i.e.\nmaking work that is like a poem, short yet dense, re-tellable, rhythmic,\nmeaningful as opposed to a demo that feels like technology for technology's\nsake._\n\n—Zach Lieberman6\n\n* * *\n\nIncreasingly, computational artists and designers have also developed more\novertly political practices that both anticipate and respond to technological\ntransformations in society—allowing for “experimentation with new ways of\nseeing, being, and relating, as well as opportunities to develop innovative\nstrategies and tools for resistance movements.” 10 Such practices adopt and\nrecombine methods from speculative and critical design, relational aesthetics,\nand critical engineering to provoke reconsideration of the origins and\nconsequences of new technologies and their uneven impact on different\nindividuals and communities. The assignments and exercises in this book are\ndesigned to provide entry points into the technical skills, aesthetic issues,\nand social considerations involved in these and many other modes of practice.\n\n* * *\n\n_Art is the only ethical use of AI._\n\n—Allison Parrish8\n\n* * *\n\n* * *\n\n_My apps are as useful as a song._\n\n—Scott Snibbe9\n\n* * *\n\n* * *\n\n_We need a multifaceted and transdisciplinary approach blending art, science,\ntheory, and hands-on experimentation. The media will talk about how it all\nworks, but to fully understand, to appropriately educate others, to devise\nsuitable policies, and to form strategies of resistance, we need to know how\nit breaks._\n\n—Heather Dewey-Hagborg11\n\n* * *\n\nMarshall McLuhan observed that art is a “Distant Early Warning system that can\nalways be relied on to tell the old culture what is beginning to happen to\nit.” 12 Artistic engagements with computation and emerging technologies,\nperhaps even more so than science fiction, offer a tangible glimpse of what\nmay eventually become the everyday.13 Whom does this clairvoyance serve? For\nfive decades, the most prominent computer science research centers (including\nBell Labs, Xerox Parc, MIT Media Lab, and Google ATAP) have included\ntechnologically literate artists in the project of “inventing the future,”\ninstrumentalizing artistic inquiry in the service of capital. As Stewart Brand\nexplained, following his residency at MIT: “The deal was very clear. The Lab\nwas not there for the artists. The artists were there for the Lab. Their job\nwas to supplement the scientists and engineers in three important ways: they\nwere to be cognitive pioneers; they were to ensure that all demos were done\nwith art—that is, presentational craft; and they were to keep things\nculturally innovative.” 14\n\n* * *\n\n_Digital art and design rework technology into culture, and reread technology\nas culture. What's more, they do so in a concrete, applied way, manipulating\nthe technology itself, with a latitude that admits misapplication and\nadaptation, rewiring and hacking, pseudofunctionality and accident. Creative\npractice also fractures that technocultural material into millions of\nheterogeneous interests and agendas, specific investigations, aesthetics,\napproaches, and projects._\n\n—Mitchell Whitelaw15\n\n* * *\n\nWith the fracturing of civic life after social media, the malignant growth of\ndigital authoritarianism, and the looming threat of environmental catastrophe,\nthe sheen has come off Silicon Valley and the folly of technological\nsolutionism has become clear. To the extent that we continue to prototype new\nfutures within the framework of late capitalism, and echoing McLuhan's notion\nof the arts as a “warning system,” there is a new urgency for artists and\ndesigners to have a seat at the tables where technological agendas are set. As\nMichael Naimark has argued, “Artists bring criticality to environments that\nare otherwise vulnerable to the Achilles’ heel of nerd culture: techno-\noptimism and technophilic enthusiasm.” 16 The technologically literate artist\nor designer has an essential role to play in checking society's worst\nimpulses. Not only can they ring the alarms when freedom and imagination are\nthreatened; they can also use their privilege to create systems that, as Elvia\nWilk writes, “champion subjective experience,” “expand the range of human\nexpression,” and make space in our conversations and institutions for as many\nperspectives as possible.17\n\n* * *\n\n_Amidst all the attention given to the sciences as to how they can lead to the\ncure of all diseases and daily problems of mankind, I believe that the biggest\nbreakthrough will be the realization that the arts, which are conventionally\nconsidered “useless,” will be recognized as the whole reason why we ever try\nto live longer or live more prosperously. The arts are the science of enjoying\nlife._\n\n—John Maeda18\n\n* * *\n\n* * *\n\n_We can participate in fantasies that see technology bringing the world into\npredictable control, but I prefer to work through an alternative vision that\nsees technology embracing the messiness and uncertainty of the world to\ncultivate experiences of wonder, curiosity, enchantment, and surprise that\ncome from seeing oneself as small part of a great number of wonders that\nsurround us in everyday life._\n\n—Laura Devendorf19\n\n* * *\n\nThis book is an argument for creators with hybrid skills and open hearts. We\ninsist upon the value of arts literacies within engineering spaces, and\nengineering literacies in the arts—and we believe that doing so is critically\nimportant at a time when education systems increasingly prioritize corporate\nagendas over the cultivation of capacities like critique, imagination,\nempathy, and justice.20 An arts literacy provides a vocabulary for recognizing\nthe politics of technologies, and for negotiating and renegotiating the values\nand priorities they reinforce.21 The world is not a computer, it is not a\nsystem to be optimized, and it will always refuse to be neatly resolved into\nstable categories or fixed value systems.22 Education in the arts cultivates\ncapacities for navigating this messiness, as well as celebrating qualitative\nways of knowing. As software continues to permeate our lives, we need to\nfoster culturally enmeshed ways to contextualize it, question it, modify it,\nand develop shared understandings for working with it. Just as “everyone\nshould learn to code,” everyone should also be equipped with the intellectual\ntools of the arts. Education in culturally oriented computational practices,\nsuch as those gathered together in this book, offers a rich way to do both.\n\n* * *\n\n_Humanists must be educated with a deep appreciation of modern science.\nScientists and engineers must be steeped in humanistic learning. And all\nlearning must be linked with a broad concern for the complex effects of\ntechnology on our evolving culture._\n\n—Jerome B. Wiesner23\n\n* * *\n\n* * *\n\n_Why focus on projects? We take seriously the analogy between coding and\nwriting. When you learn to write, it's not enough to learn spelling, grammar,\nand punctuation. It's important to learn to tell stories and communicate your\nideas. The same is true for coding._\n\n—Mitchel Resnick24\n\n* * *\n\nA Field Guide for Teaching\n\nThis playbook presents resources and practices for building what we believe\nare good assignments, good lectures, and good classrooms in our field. As a\ndistillation of our shared values, it lays out teaching tools for project-\nbased learning, materials for composing lectures that highlight a range of\napproaches from a diverse group of makers, and strategies for the creation of\nexciting classrooms.\n\n_Project-based learning_ is a key pedagogical approach in the arts. Unlike\nmost programming textbooks, the assignments shared here are open-ended\nprompts, encouraging curiosity-driven and improvisational approaches to the\nuse of code. We believe a good assignment should present a set of constraints\nthat are tight enough to learn necessary skills, but at the same time, inspire\na wide range of possible responses, allowing for productive comparisons in\ncritique. A good assignment should also offer a way to examine a timely social\nquestion—as Paolo Pedercini writes, it should constitute “an entry point to a\ncritical issue.” 25 And for the computational artist or designer in\nparticular, it should preserve room for the critical, imaginative, or\nexpressive reconsideration of technology and its possibilities. We believe a\ngood creative coding assignment should be an invitation to look at technology\nwith fresh eyes: defamiliarized, recontextualized, and reinterpreted. We are\nhardly the first educators to advocate for this sort of “socially situated\nlearning-by-making”; rather, we continue the work of pioneers like Idit Harel\nand Seymour Papert, and others like Sherry Turkle and Mitchel Resnick, whose\nvisions for “situated constructionism” in technology education, though decades\nold, have yet to be fully realized.26\n\nThe assignments in this book have been chosen with these criteria in mind,\noffering opportunities for personalizing, weirding, debating, and queering\ncomputation. Each is elaborated in a syllabus module that includes variations\nfor differing skill levels or learning objectives. Each places equal\nimportance on the cultural valence of computer technologies, codecraft, the\nsubjectivity of the practitioner, and awareness of historic lines of inquiry.\nThese prompts make space for diverse and personalized work, and aim to\npreserve the student's dignity, curiosity, whimsy, and criticality in an\neducational landscape where students are all too often asked to create (or\nworse, re-create) bank software. Importantly, the assignments in this book are\nalso language-agnostic, allowing students to develop responses using whichever\nprogramming toolkit is preferred.\n\n* * *\n\n_To me an assignment is useful if it does any of the following: (1)\nconstitutes an entry point to a critical issue; (2) builds transferable skills\n(blinking an LED on an Arduino is dull but introduces some important\nconcepts); (3) allows students to make something personally relevant: a chance\nto develop their own creative practice, or make a portfolio piece, regardless\nof their elected sub-discipline._\n\n—Paolo Pedercini27\n\n* * *\n\n* * *\n\n_The task, then, is to challenge not only forms of discriminatory design in\nour inner and outer lives, but to work with others to imagine and create\nalternatives to the_ techno quo _—business as usual when it comes to\ntechnoscience—as part of a larger struggle to materialize collective freedoms\nand flourishing._\n\n—Ruha Benjamin28\n\n* * *\n\nTo demonstrate some of the ways in which these assignments can be approached,\nwe have illustrated these modules with art and design projects selected on the\nbasis of their _pedagogic value_ : that is, not only for their excellence as\nprojects, but also for how easily they can be explained in a classroom\nsetting. The exemplars in our illustrations are thus not intended to\narticulate a canon of computational artworks and design projects, but rather\nto support a canon of assignments. Where we have omitted classic and\ninfluential works of media art or design—and there are so many—it is for this\nreason. Likewise, the high value we place on explicability also underpins our\noccasional inclusion of little-known projects by students.\n\nWe acknowledge that the landscape of software-based creative practices is\nvastly greater than the scope of this book. So too are the ways in which these\nthemes can be approached. In selecting examples to illustrate each assignment,\nwe have attempted to represent a diverse range of practitioners. But there is\nmuch more work to be done. As Heather Dewey-Hagborg has noted, “The\nintersection of arts and technology brings the worst of both worlds together.\nThe tech industry is so white male, and the art world also prioritizes white\nmen. But then, when you put those two together, it's like it just explodes.”\n29 This has significant implications for who identifies as being able to work\ncreatively with software, and we emphatically affirm the ongoing need to\ndiversify who is represented in lectures and syllabi as an essential part of\ndismantling the systemic racism and sexism in our institutions and cultures.30\n\nGiving space to a multitude of voices and perspectives is not only critical\nfor counteracting the obliviousness of those in positions of power to\nunderstanding the potential risks or harms of what they design,31 but also for\nwhat Kamal Sinclair calls “democratizing the imagination for the future.” 32\nBoth are urgent challenges in hybrid fields where computation and the\nimagination come together. However, attempts to address these biases in\nisolation still leave other shortcomings in place. We acknowledge, for\nexample—as we write within elite universities on the East Coast of North\nAmerica—that this book emerges from our particular context, and that it\nreifies the myopia of the communities in which we participate. We also\nrecognize, with humility, that vocabularies and values inherited from the arts\ncan guide the way: art gives us tools to reveal partial perspectives,\nacknowledge subjectivity, transform how we see, and address unjust power\nrelations. Taking inclusion as a starting point, creative coding pedagogies\ncan offer a powerful way to address these histories and futures, broadening\nhow programming skills are taught, why they matter, who is positioned to use\nthem, and how.\n\n* * *\n\n_Every single student comes with something. [People] are not empty vessels.\nThey come to the table with something valuable from a community and a culture.\nThe more you can help them make that bridge between what they have and what\nthey want to do, the more engaged they will be, and the longer they can stick\nwith it. We have to make that connection._\n\n—Nettrice Gaskins33\n\n* * *\n\nIn addition to providing resources for assignments and lectures, this book\nalso provides strategies for sustaining classrooms that minimize frustration\nand maximize the freedom to make mistakes. We recognize that developing\nfluency in a new medium can demand silencing one's inner critic, at least for\na while. Computing technologies, in particular, are often discouragingly\nbrittle and impersonal, making the hurdles of creative work even steeper. In\nan educational environment, methodologies from traditional computer science\nclassrooms (such as emphasizing solitary work policed by plagiarism detectors)\ncan exacerbate individual frustration and erode group morale.34 The creative\ncoding educator, in addition to explaining interdisciplinary techniques and\nshowing inspiring projects, must also be sensitive to the mood of their\nclassroom and students. As author and educator bell hooks observes, the\nnecessity of actively orchestrating the emotional dimensions of a classroom is\noften overlooked. For hooks, _excitement_ , “generated by collective effort,”\nreinforces a group's mutual investment in each other's work and is a key\ningredient in establishing an environment that is conducive to learning,\nempowerment, and transformation.35 The Classroom Techniques and the Interviews\nsections of this book compile the wisdom of numerous educators regarding how\nthey build classroom communities and help individuals cope with frustration.\nThese sections share techniques for guiding arts practitioners who do not\nnecessarily identify as programmers through their initial fear and inevitable\nfrustrations.\n\nHow This Book Is Organized\n\nThis book contains three primary sections: Assignments, Exercises, and\nInterviews.\n\nPart One, Assignments, is a collection of syllabus modules. Each module is\nbuilt around an open-ended assignment or project prompt. These are recurring\nand even “classic” project briefs, collected from dozens of our peers and\nmentors, that we have received, tested, adapted, or observed over the course\nof more than two decades in higher education. Although loosely ordered by\ndifficulty, the modules may also be grouped by their emphasis on key concerns\nin computational arts and design, including:\n\n  * **generativity**  \nIterative Pattern, Face Generator, Generative Landscape, Parametric Alphabet,\nParametric Object\n\n  * **interactivity**  \nVirtual Creature, Drawing Machine, One-Button Game, Conversation Machine\n\n  * **transcoding and transmediality**  \nClock, Custom Pixel, Data Self-Portrait, Measuring Device, Synesthetic\nInstrument\n\n  * **connectivity**  \nBot, Collective Memory, Experimental Chat, Browser Extension, Creative\nCryptography\n\n  * **corporeality and virtuality**  \nAugmented Projection, Personal Prosthetic, Virtual Public Sculpture,\nExtrapolated Body\n\nEach module includes a _Brief_ that summarizes the artistic prompt; _Learning\nObjectives_ , which define the goals of the module in terms of demonstrable\nskills or knowledge acquisition; _Variations_ , a set of easements and further\nopportunities for experimentation; and a short essay, _Making it Meaningful_ ,\nthat serves as a guide to understanding the assignment's significance,\nchallenges, and potential avenues of approach. An illustrated collection of\nannotated examples of historic and contemporary projects—our\ntouchstones—demonstrate how different artists and students have previously\napproached the assignment's premise. An appendix on the provenance of these\nassignments provides more information about their history and (where possible)\ntheir original authors.\n\nPart Two, Exercises, consists of short programming prompts that hone the\nmastery of specific technical skills, while remaining idiomatically relevant\nto artists and designers. These exercises develop the student's skills in the\nuse of computational techniques to control elementary visual (or in some\ncases, auditory or textual) patterns and forms, and may be assigned as\nhomework or as in-class activities. Although the exercises grant some latitude\nas to the specific details of their implementation—they have no single correct\nsolution—they are nevertheless written to make both perceptual and technical\nevaluations possible. One exercise on the theme of iteration, for example,\nrequires the student to write code that generates the alternating squares of a\ncheckerboard. The exercises are organized into topical sections, including\nconditional testing, iteration, typography, and visualization, and they range\nin level of difficulty.\n\nIn creating a compendium of assignments and exercises, we have taken\ninspiration in and comfort from several key precursors: _Draw It with Your\nEyes Closed: The Art of the Art Assignment_ , by Dushko Petrovich and Roger\nWhite (2012); _Taking a Line for a Walk: Assignments in Design Education_ , by\nNina Paim, Emilia Bergmark, and Corinne Gisel (2016); _The Photographer's\nPlaybook: 307 Assignments and Ideas_ , by Jason Fulford and Gregory Halpern\n(2014), and _Wicked Arts Assignments: Practising Creativity in Contemporary\nArts Education_ , by Emiel Heijnen and Melissa Bremmer (2020). Each is a kind\nof pedagogic potluck for their respective creative spheres and showed us how a\nbook could bring together projects, exercises, and approaches from a field of\npractice. These books also demonstrate that there is no one-size-fits-all way\nof designing a curriculum or running a classroom—as confirmed by the educators\nwhose varied and even contradictory advice is shared in our next section.\n\nPart Three, Interviews, presents thematically organized conversations with\nthirteen diverse and renowned educators. In this section, instructors like Dan\nShiffman, Lauren McCarthy, and Taeyoon Choi speak to the practical,\nphilosophical, and spiritual challenges of teaching expressive and critical\nstudio arts through the often unwelcoming toolsets of software development.\nUnderpinning many of our questions is a concern for how education might repair\nthe persistent split between the sciences and humanities:36 How can we\nencourage “heart” in the midst of technical education? What pedagogic\nstrategies are necessary to teach technical material to artists? How do you\ncorral students with diverse skillsets? These questions are unique to the\nhybrid nature of computational art and design, and they reveal themselves in a\nparticularly stark way when the educator must rapidly oscillate between\nteaching art and math. In their responses, our interview subjects offer nuts-\nand-bolts advice for software arts instruction, as well as broader strategies\nfor interdisciplinary education.\n\nTake This Book and Run with It\n\nAt first glance, it may seem curious that in a book about coding pedagogy,\nthere is no code to be found. For those interested, we have provided an open\ncode repository online that contains solutions to our exercises in several\npopular programming languages, as well as helpful “starter” code, where\npossible, for the larger, open-ended assignments. There are two reasons why we\nhave not provided code in the pages of this book itself. The first speaks to\nthe trade-off that occurs when students are provided with starter code: on one\nhand, it gives them a head start, but in our experience, it also dramatically\nnarrows the students’ imaginations and the scope of the resulting projects.\nThe second reason, more importantly to us, concerns the longevity of this\nbook. In not tying the assignments to specific programming languages and\ndevelopment environments, we hope to preserve their relevance as generations\nof platforms inevitably come and go. We will be updating our online code\nrepository as technologies change, and we warmly invite contributions from\nthose who wish to port or extend our examples to topics and toolkits outside\nour own expertise.\n\nWhether you are teaching or learning creative coding in your kitchen, in a\nschool or at a university, in an arts program or in a STEM field, with others\nor by yourself—we believe there will be something here to enrich your\nexperience. We can't wait to see what you or your students make.\n\n## Notes\n\n1 Leah Buechley, “Expressive Electronics: Sketching, Sewing, and Sharing”\n(lecture, wats:ON? Festival, Carnegie Mellon University, Pittsburgh, PA, April\n2012), <https://vimeo.com/62890915>. 2 See, for example, Mark Guzdial,\n“Computing Education Lessons Learned from the 2010s: What I Got Wrong,”\n_Computing Education Research Blog_ , January 13, 2020,\n<https://computinged.wordpress.com/2020/01/13/computing-education-lessons-\nlearned-from-the-2010s-what-i-got-wrong/>. 3 Ben Fry and Casey Reas,\n“Processing 2.0 (or: The Modern Prometheus)” (lecture, Eyeo Festival,\nMinneapolis, MN, June 2011), 0:45, <https://vimeo.com/28117873>. 4 Howard\nGardner, _Frames of Mind: The Theory of Multiple Intelligences_ (New York:\nBasic Books, 2011). 5 Mimi Onuoha, “On Art and Technology: The Power of\nCreating Our Own Worlds,” Knight Foundation, last modified March 2, 2018,\n<https://knightfoundation.org/articles/authors/mimi-onuoha/>. 6 “YesYesNo,”\n_IdN Magazine_ 19, no. 5 October 2012): 30–31. 7 Julie Perini, “Art as\nIntervention: A Guide to Today's Radical Art Practices,” in _Uses of a\nWhirlwind: Movement, Movements, and Contemporary Radical Currents in the\nUnited States_ , ed. Team Colors Collective (Chico, CA: AK Press, 2010), 183,\n<http://sites.psu.edu/comm292/wp-content/uploads/sites/5180/2014/10/Perini-\nArt_as_Intervention.pdf>. 8 Claire Evans (@YACHT), “Thanks so much for having\nus! Full credit due to the brilliant @aparrish for saying ‘art is the only\nethical use of AI’ during a panel we hosted in NYC a few months back. It's\nbecome our mantra <3,” Twitter, December 2, 2019, 5:01 PM. 9 Scott Snibbe,\npersonal communication to Golan Levin. See Golan Levin (@golan), “@snibbe used\nto say, it's “as useful as a song”. Case closed.” Twitter, September 4, 2018,\n9:11 PM. 10 Perini, 183. 11 Heather Dewey-Hagborg, “Sci-Fi Crime Drama with a\nStrong Black Lead,” _The New Inquiry_ , July 6, 2015,\n<https://thenewinquiry.com/sci-fi-crime-drama-with-a-strong-black-lead/>. 12\nMarshall McLuhan, _Understanding Media: The Extensions of Man_ (New York:\nMcGraw Hill, 1964), 22. 13 Golan Levin, “New Media Artworks: Prequels to\nEveryday Life,” _Flong_ (blog), July 19, 2009,\n<http://www.flong.com/blog/2009/new-media-artworks-prequels-to-everyday-\nlife/>. 14 Stewart Brand, “Creating Creating,” _WIRED_ , January 1, 1993,\n<https://www.wired.com/1993/01/creating/>. 15 Adapted (with permission) from\nMitchell Whitelaw, _Metacreation: Art and Artificial Life_ (Cambridge, MA: The\nMIT Press, 2004), 5. See Mitchell Whitelaw (@mtchl), “Written by my younger\nand more idealistic self - if I could revise it now I'd expand / disperse\n‘art’ to encompass a wider range of practices (including design). But thanks\nSara [Hendren]!,” Twitter, April 29, 2020, 11:48 PM. 16 Michael Naimark,\npersonal communication to Golan Levin, 2014. 17 Elvia Wilk, “What Can WE Do?\nThe International Artist in the Age of Resurgent Nationalism,” _The Towner_ ,\nSeptember 11, 2016, <http://www.thetowner.com/international-artists-\nnationalism/>. 18 John Maeda, in “VOICES; John Maeda,” _The New York Times_ ,\nNovember 11, 2003, <https://www.nytimes.com/2003/11/11/science/voices-john-\nmaeda.html>. 19 Laura Kay Devendorf, “Strange and Unstable Fabrication” (PhD\ndiss., University of California, Berkeley, 2016), 81,\n<https://digitalassets.lib.berkeley.edu/etd/ucb/text/Devendorf_berkeley_0028E_16717.pdf>.\n20 Orit Halpern, “A History of the MIT Media Lab Shows Why the Recent Epstein\nScandal Is No Surprise,” _Art and America_ , November 21, 2019,\n<https://www.artnews.com/art-in-america/features/mit-media-lab-jeffrey-\nepstein-joi-ito-nicholas-negroponte-1202668520/>. 21 Danielle Allen, “The\nFuture of Democracy,” _HUMANITIES_ 37, no. 2 (Spring 2016),\n<https://www.neh.gov/humanities/2016/spring/feature/the-future-the-humanities-\ndemocracy>. 22 Tega Brain, “The Environment Is Not a System,” _A Peer-Reviewed\nJournal About_ 7, no. 1 (2018): 152–165. 23 In _Momentum_ , MIT Media\nLaboratory, 2003, <http://momentum.media.mit.edu/dedication.html>. 24 From\nMitchel Resnick, “Computational Fluency,” Medium, September 16, 2018,\n<https://medium.com/@mres/computational-fluency-776143c8d725>. (Adapted from\nhis book _Lifelong Kindergarten_.) See also Yasmin Kafai and Mitchel Resnick,\n_Constructionism in Practice: Designing, Thinking, and Learning in A Digital\nWorld_ (New York: Routledge, 1996). 25 Paolo Pedercini, personal communication\nto Golan Levin, June 16, 2020. 26 Idit Harel and Seymour Papert, “Situating\nConstructionism,” in _Constructionism_ (Norwood, NJ: Ablex Publishing, 1991),\n<http://web.media.mit.edu/~calla/web_comunidad/Reading-\nEn/situating_constructionism.pdf>. 27 Pedercini, June 16, 2020. 28 Ruha\nBenjamin, ed., _Captivating Technology: Race, Carceral Technoscience, and\nLiberatory Imagination in Everyday Life_ (Durham, NC: Duke University Press,\n2019), 12. 29 Heather Dewey-Hagborg, “Hacking Biopolitics” (lecture, The\nInfluencers 2016: Unconventional Art, Guerrilla Communication, Radical\nEntertainment, CCCB, Barcelona, October 2016), 48:20,\n<https://vimeo.com/192627655>. 30 See the Refresh campaign and project that\nhas addressed gender bias in the Prix Ars Electronica awards:\n<https://refreshart.tech/>. 31 Catherine D’Ignazio and Lauren F. Klein call\nthis the “privilege hazard” in their book _Data Feminism_ (Cambridge, MA: MIT\nPress, 2020), 57. 32 Kamal Sinclair, “Democratize Design,” Making a New\nReality, May 18, 2018, <https://makinganewreality.org/democratize-\ndesign-86d2385865bd>. 33 “The Technologists in the Studio: Nettrice Gaskins\nHighlights the Connections between Communities, Cultures, Arts, and STEM,”\nWogrammer, April 24, 2019, <https://wogrammer.org/stories/nettrice>. 34 James\nW. Malazita and Korryn Resetar, “Infrastructures of Abstraction: How Computer\nScience Education Produces Anti-Political Subjects,” _Digital Creativity_ 30,\nno. 4 (December 2019): 300–312,\n<https://doi.org/10.1080/14626268.2019.1682616>. Malazita and Resetar discuss\nhow the ubiquitous use of plagiarism tools in computer science\ninfrastructurally discourages collaboration, discussion, and knowledge\nsharing. This can be observed firsthand in David Kosbie's Automated Plagiarism\nDetection Tool tutorial, developed for CMU introductory programming course\n15-110, <https://www.youtube.com/watch?v=LdU0dTPaueU>. 35 bell hooks,\n_Teaching to Transgress_ (New York: Routledge, 2014), 7–8. 36 As identified by\nC. P. Snow in _The Two Cultures_ (Rede Lecture, University of Cambridge,\n1959).\n\n\n# Iterative Pattern\n\nGenerating a texture or textile design\n\n![c1-fig-0001.jpg](../images/c1-fig-0001.jpg)\n\n1\\. In _Spamgetto_ (2009), the Italian design agency Todo presents\ncomputationally generated wallpaper whose elements include text from thousands\nof spam emails.\n\n## Brief\n\nWrite code to generate a tiling pattern or textural composition, as for\nwallpaper or fabric. Give consideration to aesthetic issues like symmetry,\nrhythm, color; detail at multiple scales; precise control of shape; and the\nbalance between organic and geometric forms.\n\nYour pattern should be designed so that it could be infinitely tiled or\nextended. Design something you would like to put on the walls or floor of your\nhome, or that you could imagine yourself wearing. Export your pattern in a\nhigh-resolution format, and print it as large as possible for your peers’\nreview. Remember to sketch first.\n\n## Learning Objectives\n\n  * Create visual designs using the Cartesian coordinate system and combining drawing functions\n  * Use functional abstraction to encapsulate the code for modular design elements\n  * Generate and critique designs with symmetries and/or seamless repetition\n\n## Variations\n\n  * Experiment with 2D graphics transformations, such as rotation, scaling, and mirror reflections.\n  * Use nested iteration to develop 2D rhythms or other gridlike visual structures.\n  * Create a helper function to abstract the way in which a complex visual element (such as a flower, animal, fruit, or fleur-de-lis) is rendered throughout your design.\n  * Reproduce a preexisting textile or wallpaper design using code only.\n  * Make a kaleidoscope by incorporating a photographic image or video feed into a pattern with symmetric reflections.\n  * Have your pattern printed on real fabric or wrapping paper. Consider other output devices or on-demand services for realizing your pattern, such as computer-controlled laser cutters, knitting machines, or lace-making machines.\n  * Make an animated “dynamic wallpaper” loop to use in the background of your videoconferences. Your design should scale gracefully by rendering correctly at different canvas resolutions.i\n\n## Making It Meaningful\n\nPattern is the starting point from which we perceive and impose order in the\nworld. Examples of functional, decorative, and expressive pattern-making date\nfrom ancient times and take the form of mosaics, calendars, tapestry,\nquilting, jewelry, calligraphy, furniture, and architecture. There is an\nintimate connection between pattern design, visual rhythm, geometry,\nmathematics, and iterative algorithms. This prompt invites the creator to hone\ntheir understanding of these relationships in formal terms. An important\nvariation of this prompt is to realize designs physically, through either\ndigital printing, fabrication in an unusual material, or at an unexpected\nscale. This can be a watershed moment of synthesis for software artists who\ncrave making something physical.\n\n![c1-fig-0002.jpg](../images/c1-fig-0002.jpg)\n\n2\\. Georg Pólya's illustrations (1924) of the seventeen periodic plane\nsymmetry groups had a profound influence on the algorithmic patternmaking of\nM. C. Escher.\n\n![c1-fig-0003.jpg](../images/c1-fig-0003.jpg)\n\n3\\. Zellige terracotta tiles in Marrakech (17th century) form edge-to-edge,\nregular, and other tessellations.\n\n![c1-fig-0004.jpg](../images/c1-fig-0004.jpg)\n\n4\\. Casey Reas's _One Non-Narcotic Pill A Day_ (2013) presents a dynamic\ncollage pattern generated from a video recording.\n\n![c1-fig-0005.jpg](../images/c1-fig-0005.jpg)\n\n5\\. Alison Gondek, a scenic design student at Carnegie Mellon studying\nintroductory programming, used p5.js to create this pattern inspired by the\n“Circular Gallifreyan” language from _Doctor Who_.\n\n![c1-fig-0006.jpg](../images/c1-fig-0006.jpg)\n\n6\\. Vera Molnár was among the first artists to use a computer. Her 1974\nuntitled plotter drawing demonstrates patterns arising from the interaction\nbetween procedural iteration and randomized omission.\n\n![c1-fig-0007.jpg](../images/c1-fig-0007.jpg)\n\n7\\. Leah Buechley explores the intersection of computation and craft. The\ndesign of her lasercut curtain (2017), generated in Processing, features\nmultiple forms of iteration and controlled randomness.\n\n## Additional Projects\n\n  * Dave Bollinger, _Density Series_ , 2007, generative image series.\n  * Liu Chang, _Nature and Algorithm_ , 2016, algorithmic images, satellite imagery, ink on paper.\n  * Joshua Davis, _Chocolate, Honey and Mint_ , 2013, generative image series.\n  * Saskia Freeke, _Daily Art_ , 2010–2020, generative image series.\n  * Manolo Gamboa Naon, _Mantel Blue_ , 2018, ink on paper.\n  * Tyler Hobbs, _Isohedral III_ , 2017, inkjet print on paper, 19 x 31”.\n  * Lia, _4jonathan_ , 2001, generative image series.\n  * Holger Lippmann, _The Abracadabra Series_ , 2018, generative image series.\n  * Jonathan McCabe, _Multi-Scale Belousov-Zhabotinsky Reaction Number Seven_ , 2018, generative image series.\n  * Vera Molnár, _Structure de Quadrilateres (Square Structures)_ , 1987, ink on paper.\n  * Nontsikelelo Mutiti, _Thread_ , 2012–2014, screen print on linoleum tiles.\n  * Nervous System, _Patchwork Amoeba Puzzle_ , 2012, lasercut plywood.\n  * Helena Sarin, _GANcommedia Erudita_ , 2020, inkjet printed book.\n  * Mary Ellen Solt, _Lilac_ , 1963, concrete poetry.\n  * Jennifer Steinkamp, _Daisy Bell_ , 2008, video projection.\n  * Victor Vasarely, _Alom (Rêve)_ , 1966, collage on plywood, 99 1/5 x 99 1/5”.\n  * Marius Watz, _Wall Exploder B_ , 2011, wall drawing, 9 x 3.6 m.\n\n## Readings\n\n  1. David Bailey, _David Bailey's World of Escher-Like Tessellations_ , 2009, tess-elation.co.uk.\n  2. P. R. Cromwell, “The Search for Quasi-Periodicity in Islamic 5-fold Ornament,” _The Mathematical Intelligencer_ 31 (2009): 36–56.\n  3. Anne Dixon, _The Handweaver's Pattern Directory: Over 600 Weaves for 4-shaft Looms_ (Loveland, CO: Interweave Press, 2007).\n  4. Ron Eglash, _African Fractals: Modern Computing and Indigenous Design_ (New Brunswick, NJ: Rutgers University Press, 1999).\n  5. Samuel Goff, “Fabric Cybernetics,” _Tribune_ (blog), August 23, 2020.\n  6. Branko Grünbaum and G. C. Shephard, _Tilings and Patterns_ (New York: W. H. Freeman & Company, 1987).\n  7. “Wallpaper Collection,” Collections, Historic New England, [historicnewengland.org](http://historicnewengland.org).\n  8. Owen Jones, _The Grammar of Ornament_ (London: Bernard Quaritch Ltd., 1868).\n  9. Albert-Charles-Auguste Racinet, _L’Ornement Polychrome_ (Paris: Firmin Didot et Cie, 1873).\n  10. Casey Reas et al., _{Software} Structures_ , 2004–2016, [artport.whitney.org](http://artport.whitney.org).\n  11. Petra Schmidt, _Patterns in Design, Art and Architecture_ (Vienna: Birkhäuser, 2006).\n\n## Notes\n\ni This variation was contributed by Tom White (@dribnet).\n\n\n# Face Generator\n\nDrawing parametric faces\n\n![c2-fig-0008.jpg](../images/c2-fig-0008.jpg)\n\n8\\. Although Matthias Dörfelt's _Weird Faces_ (2012) look hand-drawn, they are\nentirely generated by custom software.\n\n## Brief\n\nWrite code to design an image of a face that is parameterized by at least\nthree dimensions of variability, but preferably more. For example, you might\nhave variables that specify the size, position, color, or other visual\ncharacteristics of the eyes, nose, and mouth. The variations in these features\nmay be used to alter the face's expression (happy, sad, angry); the face's\nidentity (John, Maria); and/or the face's species (cat, monkey, zombie,\nalien). Give special consideration to controlling the precise shape of face\nparts, such as the curves of the nose, chin, ears, and jowls, as well as\ncharacteristics like skin color, stubble, hairstyle, blemishes, interpupillary\ndistance, facial asymmetry, cephalic index, and prognathism. Differentiate\ncontinuous parameters (such as size and position of features) and discrete\nparameters (such as the presence of piercings, or the number of eyeballs).\nWill your faces be 2D or 3D? Will they be shown in a frontal, profile, or\nthree-quarters view? Your system should generate a new face whenever the user\npresses a button.\n\n## Learning Objectives\n\n  * Design parametric forms using drawing functions and the Cartesian coordinate system\n  * Apply generative design principles to expressive character design\n  * Conduct meta-design (design a system to design things)\n\n## Variations\n\n  * Use your software to generate a deck of collectible trading cards (like Pokémon or baseball cards) featuring a group of imaginary heroes or monsters. Print out the cards.\n  * Try using real-world multivariate data as the basis for generating new faces, instead of randomness.\n  * Create an interactive tool that allows people to make self-portraits in a cartoon style. Document your software with an example.\n  * Consider the comparative merits of a design in which faces are assembled from a diverse collection of ready-made assets (mustaches, noses, etc.), versus a design in which faces are generated from continuously variable curves and shapes.\n  * Add functionality to your face so that it responds to audio, microphone, or speech input.\n\n## Making It Meaningful\n\nHumans are equipped with an exquisite sensitivity to faces. From infancy, we\neasily recognize faces and can detect very subtle shifts in expressions, often\nbeing able to discern the slightest change in mood and sincerity in ways that\nremain impossible for computers. Faces also allow us to readily identify\nfamily resemblances or recognize friends in crowds. Faces are so central to\nvisual perception that “the impairment of our face-processing ability is seen\nas a disorder, called _prosopagnosia_ , while unconsciously seeing faces where\nthere are none is an almost universal kind of _pareidolia_.” i\n\nThis assignment draws inspiration from the “Chernoff face” data visualization\ntechnique, which leverages this sensitivity by using facial features to\nrepresent multivariate data. In Chernoff faces, features such as the eyes,\nears, mouth, and nose represent data according to their shape, size,\nplacement, and orientation. Whereas Herman Chernoff used 18 variables to\nsynthesize a face, Paul Ekman and Wallace Friesen's _Facial Action Coding\nSystem_ analyzes faces with 46, each variable corresponding to the action of a\ndifferent facial muscle.\n\nWorks that generate faces present the conceptual opportunity to devise a\npossibility space or an imaginative context for portraits—like a family album,\nhigh school yearbook, or tradeable card deck.\n\n![c2-fig-0009.jpg](../images/c2-fig-0009.jpg)\n\n9\\. In Heather Dewey-Hagborg's astounding _Stranger Visions_ (2012), forensic\n3D portraits are computed from found DNA fragments.\n\n![c2-fig-0010.jpg](../images/c2-fig-0010.jpg)\n\n10\\. “Chernoff faces” (1973) represent multivariate data by parameterizing the\nshape, size, position, and orientation of the parts of the face.\n\n![c2-fig-0011.jpg](../images/c2-fig-0011.jpg)\n\n11\\. In Kate Compton's software _Evolving Faces with User Input_ (2009), a\ngenetic algorithm governs a population of faces, each described with an array\nof floating point numbers. By selecting a favorite face, the user can\ninteractively guide the evolution of the population.\n\n![c2-fig-0012.jpg](../images/c2-fig-0012.jpg)\n\n12\\. In Mike Pelletier's _Parametric Expression_ (2013), values that govern\nthe articulation of facial models are pushed beyond their normal limits.\n\n![c2-fig-0013.jpg](../images/c2-fig-0013.jpg)\n\n13\\. In Karolina Sobecka's _All the Universe Is Full of the Lives of Perfect\nCreatures_ (2012), the visitor puppeteers an avatar according to the movements\nof their own face.\n\n![c2-fig-0014.jpg](../images/c2-fig-0014.jpg)\n\n14\\. _Prescribed to Death_ is a wall memorial comprised of 22,000 pills carved\nwith human faces, representing Americans who died from opioid addiction in\n2017. Every 24 minutes—the frequency of U.S. opioid deaths—a Rhino Grasshopper\nscript directs an onsite CNC machine to carve a new face into an additional\npill. The project was developed for the National Safety Council's “Stop\nEveryday Killers” campaign by artist collective Hyphen-Labs, Energy BBDO,\nMssngPeces, and Tucker Walsh.\n\n## Additional Projects\n\n  * Zach Blas, _Facial Weaponization Suite_ , 2011–2014, masks computationally modeled from aggregate face data.\n  * Lorenzo Bravi, _Bla Bla Bla_ , 2010, sound- reactive phone application.\n  * Joy Buolamwini, _Aspire Mirror_ , 2015, mirror and generative face system.\n  * Heather Dewey-Hagborg, _How Do You See Me_ , 2019, self portraits generated via adversarial processes.\n  * Adam Harvey and Jules LaPlace, _Megapixels_ , 2017, art and research project.\n  * Hyphen-Labs and Adam Harvey, _HyperFace_ , 2017, computer vision camouflage textile.\n  * Mario Klingemann, _Memories of Passersby I_ , 2018, system for synthesizing portraits using neural nets.\n  * Golan Levin and Zachary Lieberman, _Reface [Portrait Sequencer]_ , 2007, system for generating face composites.\n  * Jillian Mayer, _IMPRESSIONS_ , 2017, facial analysis and billboard campaign.\n  * Macawnivore, _Nose Chart_ , 2014, digital drawing.\n  * Kyle McDonald and Arturo Castro, _Face Substitution_ , 2012, face-swapping application and installation.\n  * Orlan, _The Reincarnation of Saint ORLAN_ , 1990–1993, facial surgery as performance.\n  * Ken Perlin, _FaceDemo_ , 1997, interactive face simulation.\n\n## Readings\n\n  1. Greg Borenstein, “Machine Pareidolia: Hello Little Fella Meets Facetracker, _Idea for Dozens_ (blog), [UrbanHonking.com](http://UrbanHonking.com), January 14, 2012.\n  2. Charles Darwin, _The Expression of Emotions in Man and Animals_ (London: John Murray, 1872).\n  3. Heather Dewey-Hagborg, “Sci-Fi Crime Drama with a Strong Black Lead,” _The New Inquiry_ , July 16, 2015.\n  4. Paul Ekman and Wallace Friesen, _Facial Action Coding System (FACS)_ , 1976.\n  5. Zachary Lieberman, “Más Que la Cara Overview,” [Medium.com](http://Medium.com), April 3, 2017.\n  6. Bruno Munari, _Design as Art_ (London: Penguin Books Ltd., 1971).\n  7. Jean Robert and Francois Robert, _Face to Face_ (Zurich: Lars Muller Publishers, 1996).\n  8. George Tscherny, _Changing Faces_ (New York: Princeton Architectural Press, 2004).\n\n## Notes\n\ni Kyle McDonald, “Face as Interface,” GitHub repository for Appropriating New\nTechnologies (NYU ITP), last modified May 11, 2017.\n\n\n# Clock\n\nRepresenting time\n\n![c3-fig-0015.jpg](../images/c3-fig-0015.jpg)\n\n15\\. Lee Byron's _Center Clock_ (2007) displays the time as countable, bouncy\ncircles. After each minute passes, 60 white “second” circles coalesce to form\na new violet “minute” circle, and so on.\n\n## Brief\n\nDesign a “visual clock” that displays a novel or unconventional representation\nof the time. Your clock should appear different at all times of the day, and\nit should repeat its appearance every 24 hours (or other relevant cycle, if\ndesired). Challenge yourself to convey the time without numerals.\n\nYou are encouraged to question basic assumptions about how time is mediated\nand represented. Ponder concepts like biological time (chronobiology),\nultradian and infradian rhythms, solar and lunar cycles, celestial time and\nsidereal time, decimal time, metric time, geological time, historical time,\npsychological time, and subjective time. Inform your design by reading about\nthe history of timekeeping systems and devices and their transformative\neffects on society.\n\n## Learning Objectives\n\n  * Review and research historical methods, devices, and systems for timekeeping\n  * Devise graphic concepts and technologies for representing time that go beyond conventional methods of visualization and mediation\n  * Use programming to design, through the control of shape, color, form, and motion\n  * Apply motion graphics techniques to the representation of temporal information\n\n## Variations\n\n  * Feel free to experiment with any of the tools at your disposal, including transparency, color, sound, dynamism, and physical actuation. Reactivity to the cursor is optional.\n  * Avoid using Roman, Arabic, or Chinese numerals, but make the time readable through other means, such as by visualizing numeric bit patterns or using iteration to present countable graphic elements.\n  * Make a clock that operates at a much slower time scale, changing over months, seasons, or human lifespans.\n  * Develop your clock for a portable or wearable device, such as a mobile phone, smart watch, fitness tracker, or other standalone computer with a miniature display. Consider incorporating data from your device's other sensors into your design, such as the user's image, movements, body temperature, or heartbeat.\n  * Free yourself from the desktop or laptop screen, and design your clock for a context of your own choosing. If you could place your clock anywhere, where would it be? On the side of a building? In a piece of furniture? In a pocket? On someone's skin, as a digital tattoo? Include a drawing, rendering, or other mockup showing your clock as you imagine it in situ.\n\n## Making It Meaningful\n\nAttempts to mark time stretch back many thousands of years, with some of the\nearliest timekeeping technologies being gnomons, sundials, water clocks, and\nlunar calendars. Even today's standard representation of time, with hours and\nminutes divided into 60 parts, is a legacy inherited from the ancient\nSumerians, who used a sexagesimal counting system.\n\nThe history of timekeeping is a history driven by economic and militaristic\ndesires for greater precision, accuracy, and synchronization. Every increase\nin our ability to precisely measure time has had a profound impact on science,\nagriculture, navigation, communications, and, as always, warcraft.\n\nDespite the widespread adoption of machinic standards, there are many other\nways to understand time. Psychological time contracts and expands with\nattention; biological cycles affect our moods and behavior; ecological time is\nobserved in species and resource dynamics; geological or planetary rhythms can\nspan millennia. In the twentieth century, Einstein's theory of relativity\nfurther upended our understanding of time, showing that it does not flow in a\nconstant way, but rather in relation to the position from which it is\nmeasured—a possibly surprising return to the significance of the observer.\n\n![c3-fig-0016.jpg](../images/c3-fig-0016.jpg)\n\n16\\. Using a slit-scan technique, Jussi Ängeslevä and Ross Cooper's _Last\nClock_ (2002) presents activity traces from a live video feed at three\ndifferent time scales: one minute, one hour, and one day.\n\n![c3-fig-0017.jpg](../images/c3-fig-0017.jpg)\n\n17\\. In Golan Levin's _Banded Clock_ (1999), the seconds, minutes, and hours\nof the current time are represented as a series of countable stripes.\n\n![c3-fig-0018.jpg](../images/c3-fig-0018.jpg)\n\n18\\. Drawing from a gargantuan database of tweets, _All the Minutes_ by\nJonathan Puckey and Studio Moniker (2014) is a Twitter bot that reposts\nmentions of the current time.\n\n![c3-fig-0019.jpg](../images/c3-fig-0019.jpg)\n\n19\\. Mark Formanek's _Standard Time_ (2003) is a 24-hour performance in which\n70 workers constantly construct and deconstruct a large wooden “digital”\ndisplay of the current time.\n\n![c3-fig-0020.jpg](../images/c3-fig-0020.jpg)\n\n20\\. _Ink Calendar_ by Oscar Diaz (2009) uses the capillary action of ink\nspreading across paper to display the date.\n\n## Additional Projects\n\n  * Maarten Baas, _Real Time: Schiphol Clock_ , 2016, performance and video, Amsterdam Airport Schiphol.\n  * Maarten Baas, _Sweeper's Clock_ , 2009, performance and video, Museum of Modern Art, New York.\n  * Marco Biegert and Andreas Funk, _Qlocktwo Matrix Clock_ , US Patent D744,862 S, filed May 8, 2009, and issued December 8, 2015.\n  * Jim Campbell, _Untitled (For The Sun)_ , 1999, light sensor, software and LED number display, White Light Inc., San Francisco.\n  * Bruce Cannon, _Ten Things I Can Count On_ , 1997–1999, counting machines with digital displays.\n  * Mitchell N. Charity, _Dot Clock_ , 2001, online application.\n  * Taeyoon Choi and E Roon Kang, _Personal Timekeeper_ , 2015, interactive hardware and software system, Los Angeles Museum of Art, Los Angeles.\n  * Revital Cohen and Tuur Van Balen, _Artificial Biological Clock_ , 2008, data-driven mechanical sculpture.\n  * Skot Croshere, _Four Letter Clock_ , 2011, modified electronic alarm clock.\n  * Daniel Duarte, _Time Machine_ , 2013, custom analog electronics.\n  * Ruth Ewan, _Back to the Fields_ , 2016, botanic installation.\n  * Daniel Craig Giffen, _Human Clock_ , 2001–2014, website.\n  * Danny Hillis et al., _The Clock of the Long Now_ , 1986, mechanical system, Texas.\n  * Masaaki Hiromura, _Book Clock_ , 2013, video, MUJI SHIBUYA, Tokyo.\n  * Tehching Hsieh, _One Year Performance (Time Clock Piece)_ , 1980–1981, performance.\n  * Humans since 1982, _The Clock Clock_ , 2010, aluminum and analog electronics.\n  * Humans since 1982, _A Million Times_ , 2013, aluminum and analog electronics.\n  * Natalie Jeremijenko, Tega Brain, Jake Richardson, and Blacki Migliozzi, _Phenology Clock,_ 2014, phenology data, software, and hardware system.\n  * Zelf Koelman, _Ferrolic_ , 2015, software, hardware, and ferrolic fluid.\n  * Rafael Lozano-Hemmer, _Zero Noon_ , 2013, software system with digital display.\n  * George Maciunas, _10-Hour Flux Clock_ , 1969, plastic clock with inserted offset face, Museum of Modern Art, New York.\n  * John Maeda, _12 O’Clocks_ , 1996, software.\n  * Christian Marclay, _The Clock_ , 2010, film, 24:00, White Cube, London.\n  * Ali Miharbi, _Last Time_ , 2009, analog wall clock and interactive hardware.\n  * Mojoptix, _Digital Sundial_ , 2015, 3D-printed form.\n  * Eric Morzier, _Horloge Tactile_ , 2005, interactive software and screen.\n  * Sander Mulder, _Pong Clock_ , 2005, inverted LCD screen and software.\n  * Sander Mulder, _Continue Time Clock_ , 2007, mechanical system.\n  * Bruno Munari, _L’Ora X Clock_ , 1945, plastic, aluminum, and spring mechanism, Museum of Modern Art, New York.\n  * Yugo Nakamura, _Industrious Clock_ , 2001, Flash program and installation.\n  * Katie Paterson, _Time Pieces_ , 2014, modified analog clocks, Ingleby Gallery, Edinburgh.\n  * Random International, _A Study Of Time_ , 2011, aluminum, copper, LEDs, and software, Carpenters Workshop Gallery, London.\n  * Saqoosha, _Sonicode Clock_ , 2008, 2008, audio waveform generator.\n  * Yen-Wen Tseng, _Hand in Hand_ , 2010, modified analog electronic clock.\n  * Laurence Willmott, _It's About Time_ , 2007, language data, software, and hardware.\n  * Agustina Woodgate, _National Times_ , 2016, modified electric clock system.\n\n## Readings\n\n  1. Donna Carroll, “It's About Time: A Brief History of the Calendar and Time Keeping” (lecture, University Maastricht University, Maastricht, Netherlands, February 23, 2016).\n  2. Johanna Drucker, “Timekeeping,” in _Graphesis: Visual Forms of Knowledge Production_ (Cambridge, MA: Harvard University Press, 2014).\n  3. John Durham Peters, “The Times and the Seasons: Sky Media II (Kairos),” in _The Marvelous Clouds: Toward a Philosophy of Elemental Media_ (Chicago: University of Chicago Press, 2015).\n  4. Joshua Foer, “A Minor History of Time without Clocks,” _Cabinet Magazine_ , Spring 2008.\n  5. Amelia Groom, _Time (Documents of Contemporary Art)_ (Cambridge, MA: MIT Press, 2013).\n  6. Golan Levin, “Clocks in New Media,” GitHub, 2016.\n  7. Richard Lewis, “How Different Cultures Understand Time,” _Business Insider_ , June 1, 2014.\n  8. Leo Padron, “A History of Timekeeping in Six Minutes,” August 29, 2011, video, 6:37.\n\n\n# Generative Landscape\n\nWorld-making and terraforming\n\n![c4-fig-0021.jpg](../images/c4-fig-0021.jpg)\n\n21\\. Daniel Brown generates dystopian housing projects in his beautifully lit\nfractal series, _Travelling by Numbers_ (2016).\n\n## Brief\n\nWrite a program that presents an ever-changing, imaginative “landscape.”\nPopulate your landscape with features that are suitable for your concept:\ntrees, buildings, vehicles, animals, people, food items, body parts, hairs,\nseaweed, space junk, zombies, etc.\n\nGive consideration to the depth of variation in your landscape: after how much\ntime does your landscape become predictable? How might you forestall this as\nlong as possible? How can you generate a landscape that is both coherent and\nengaging?\n\nConsider: foreground, middle-ground, and background “layers”; variation at the\nmacro-scale, meso-scale, and micro-scale; natural and human-made features;\nutopia, dystopia, and heterotopia; the immersive use of motion parallax; and\nthe potential for surprise through the placement of infrequent features.\n\n## Learning Objectives\n\n  * Apply principles of generative design to terrain, scenery, and worlds of the imagination\n  * Bias randomness to carefully regulate probabilities\n  * Carry out a metadesign process\n\n## Variations\n\n  * Populate your landscape with one or more of the “three verticals” (people, trees, and buildings): according to Jungian psychology, these are the defining psychological features of landscapes.\n  * Pay attention to the manner in which the landscape moves past the “camera.” For example, it might appear to scroll by (as if you were looking out the window of a train); or approach from a first-person point of view (as if you were driving, or riding a roller coaster), or slide underneath (as if you were looking out of a glass-bottomed airplane). Consider a moving or even roving camera, capable of rotation as well as translation.\n  * Depict an outside scene, an interior one (such as objects on a conveyor belt), or an altogether dreamlike one.\n  * Experiment with 3D (as in noise terrains); 2D (as in side-scrolling video games); “2.5D” layered spaces; orthographic views; or even nonlinear, non-Cartesian geometries.\n  * Give consideration to sound and the possibility for audiovisual synchronicities (as in _Guitar Hero_).\n  * Make an autonomous creature, vehicle, or other character traverse your landscape.\n  * Implement features in your landscape that grow, evolve, or erode over time.\n\n## Making It Meaningful\n\nWe are a migrant species, instilled with a wanderlust that continually clamors\nfor new horizons. Before the modern era of mobility, landscape paintings were\noften the primary means by which people could visualize faraway lands and\nmentally escape to them.\n\nToday, eight-year-olds trade “seeds” for favored Minecraft worlds, and\nprocedurally generated environments have become commonplace in video games,\nwhere the algorithmic production of novel landscapes is an economic necessity\nfor inexhaustible play. For the meta-designer and artist-programmer, there is\nassuredly something godlike about calling forth world upon world. It is\nprobably not a coincidence that the first all-CGI sequence in a feature film\ndepicted the synthesis of an entire planet, in the triumphant “Genesis\nSequence” of _Star Trek II_ (1982).\n\nGenerative design systems, whether used to create faces, landscapes,\ncreatures, or chairs, define seemingly infinite possibility spaces. Pay heed,\nhowever, to what Kate Compton calls the “10,000 Bowls of Oatmeal Problem”: “I\ncan easily generate 10,000 bowls of plain oatmeal, with each oat being in a\ndifferent position and different orientation, and mathematically speaking they\nwill all be completely unique. But the user will likely just see a lot of\noatmeal.” i As Compton indicates, the challenge and opportunity of meta-design\nis in architecting systems whose results offer _perceptual uniqueness_ , and\nare thus meaningfully distinct.\n\nThis assignment asks you to bring forth a world from your imagination.\nAlternatively, you may create an accurate computational representation of a\nvery real place—and generate “more” of it.\n\n![c4-fig-0022.jpg](../images/c4-fig-0022.jpg)\n\n22\\. Kristyn Janae Solie's _Lonely Planets_ (2013) is a stylized 3D terrain\nthat shifts between minimalism and psychedelia. The work was created for Casey\nReas's undergraduate course, Live Cinema through Creative Coding.\n\n![c4-fig-0023.jpg](../images/c4-fig-0023.jpg)\n\n23\\. “Fractional noise” mountains (c. 1982), developed by Benoît Mandelbrot\nand Richard F. Voss at IBM, were a landmark in mathematical terrain synthesis.\n\n![c4-fig-0024.jpg](../images/c4-fig-0024.jpg)\n\n24\\. Everest Pipkin generates barren flowerpot landscapes in _Mirror Lake_\n(2015), a poetic and mysterious browser experience.\n\n![c4-fig-0025.jpg](../images/c4-fig-0025.jpg)\n\n25\\. In Jared Tarbell's classic _Substrate_ (2003), simulated urban tectonics\narise from elementary principles of accretion, branching, and feedback.\n\n## Additional Projects\n\n  * Memo Akten and Daniel Berio, _Bozork Quest_ , 2013, scene generated with a fragment shader.\n  * Tom Beddard, _Surface Detail_ , 2011, evolving fractal landscape.\n  * Tom Betts, _British Countryside Generator_ , 2014, procedural world engine.\n  * Ian Cheng, _Emissaries_ , 2015–2017, trilogy of evolving animated worlds.\n  * Char Davies, _Osmose_ , 1995, interactive VR.\n  * Field.io, _Interim Camp_ , 2009, generative software and film.\n  * Simon Geilfus, _Muon glNext_ , 2014, landscape generation software.\n  * Chaim Gingold, _Earth: A Primer_ , 2015, interactive book app.\n  * Beatrice Glow, _Mannahatta VR: Envisioning Lenapeway_ , 2016, immersive visualization.\n  * Michel Gondry, _Chemical Brothers “Star Guitar,”_ 2003, video clip.\n  * Vi Hart et al., _Float_ , 2015, virtual reality game.\n  * Hello Games, _No Man's Sky_ , 2016, multiplayer video game.\n  * Robert Hodgin, _Audio-Generated Landscape_ , 2008, audio-generated landscape system.\n  * Robert Hodgin, _Meander_ , 2020, procedural map generator.\n  * Anders Hoff, _Isopleth_ , 2015, virtual landscape generator.\n  * Joanie Lemercier, _La Montagne_ , 2016–2018, digital print on paper and projection.\n  * Jon McCormack, _Morphogenesis Series_ , 2001–2004, computer model and prints on photo media.\n  * Joe McKay, _Sunset Solitaire_ , 2007, software projection and performance.\n  * Vera Molnár, _Variations St. Victoire_ , 1989–1996, silkscreen prints on canvas.\n  * Anastasia Opara, _Procedural Lake Village_ , 2017, generative 3D landscape.\n  * Paolo Pedercini and Everest Pipkin, _Lichenia_ , 2019, city building game.\n  * Planetside Software, _Terragen_ , 2008, scenery generator software.\n  * Davide Quayola, _Pleasant Places_ , 2015, digital paintings.\n  * Jonathan Zawada, _Over Time_ , 2011, 3D models and oil on canvas.\n\n## Readings\n\n  1. Kate Compton, Joseph C. Osborn, and Michael Mateas, “Generative Methods” (paper presented at 4th Workshop on Procedural Content Generation in Games, Chania, Greece, May 2013).\n  2. Ian Cheng, “Worlding Raga: 2—What Is a World?” _Ribbonfarm, Constructions in Magical Thinking_ (blog), March 5, 2019.\n  3. Philip Galanter, “Generative Art Theory,” in _A Companion to Digital Art_ , ed. Christiane Paul (Hoboken, NJ: John Wiley & Sons, Inc., 2016), 146–175.\n  4. Robert Hodgin, “Default Title, Double Click to Edit” (lecture, Eyeo Festival, Minneapolis, MN, June 2014).\n  5. Jon McCormack et al., “Ten Questions Concerning Generative Computer Art,” _Leonardo_ 47, no. 2 (April 2014): 135–141.\n  6. Paolo Pedercini, “SimCities and SimCrises” (lecture, 1st International City Gaming Conference, Rotterdam, Netherlands, 2017).\n\n\n# Virtual Creature\n\nCreating artificial life\n\n![c5-fig-0026.jpg](../images/c5-fig-0026.jpg)\n\n26\\. Brent Watanabe's _San Andreas Streaming Deer Cam_ (2015–2016) is a live\nvideo stream from a computer running a modified version of _Grand Theft Auto\nV_. The artist's mod creates an autonomous deer and follows it as it wanders\nthrough a fictional city, interacting with its surroundings and the game's AI\ncharacters. During one of its streaming episodes, the deer wandered along a\nmoonlit beach, caused a traffic jam on a major freeway, got caught in a\ngangland gun battle, and was chased by the police.\n\n## Brief\n\nYour job, Dr. Frankenstein, is to create new life. Program a species of\nvirtual organism: it could be a sensate creature, a dynamic flock or swarm, an\nartificial cell-culture, a novel plant, or an ecosystem. Your software should\nalgorithmically generate the form and behavior of your new lifeform(s). Will\nthey be able to sleep, reproduce, die, or eat one another? Consider the\nrelationships between the individuals in your species and develop a\ncorresponding interplay of simulated forces such as attraction or avoidance.\nYour creature may benefit from inhabiting an ecosystem or environment with\nabiotic elements that present additional constraints or opportunities.\n\nGive consideration to the potential for your creature to operate as a cultural\nartifact. Can it attain special relevance through metaphor or commentary or by\naddressing a real human need or interest?\n\n## Learning Objectives\n\n  * Review, discuss, and write functions to animate different types of organic motion\n  * Design and implement programs using an object-oriented programming approach\n  * Program an interaction between objects\n\n## Variations\n\n  * Create an ecosystem containing a pair or “dyad” of creatures that respond to each other in some way: predator/prey, symbionts, etc.\n  * Program your creature so that its appearance arises from its behaviors, or vice versa. For example, consider how an amoeba's pseudopod is both the visual boundary of its body and also the expression of a tropism. Perhaps the form of your creature's body emerges from an underlying particle simulation, ragdoll physics, or reinforcement learning system.\n  * Write object-oriented code to encapsulate your species of creature. Exchange your code with other students whose creatures implement the same protocols (eat, sleep, forage, etc.). Collect at least two other species and combine them in an ecosystem. _This variation can be executed using a versioning tool such as GitHub, spurring insights into collaborative software development and the importance of code comments._\n  * Present your digital ecosystem as an augmented projection, siting it on a specific surface. Can your (virtual) lifeforms respond to the physical characteristics of your (real) chosen location?\n\n## Making It Meaningful\n\nAs the myths of Pygmalion, Golem, and Frankenstein show, the god-like desire\nto create artificial life (AL) persists throughout our folklore. This impulse\nalso underlies the history of robotics, where gestures are mechanically\nautomated in the _Karikuri_ of Japan and in the early automata of Europe, like\nTurriano's ”Praying Monk” (c. 1560) or de Vaucanson's “Defecating Duck”\n(1738). With computers, software simulations of life systems allow behaviors\nand interactions to be programmed and scaled across massive multiagent\nsystems. Many of these systems exhibit _emergence,_ where self-regulation,\napparent intelligence, and coordinated behaviors arise from simple rules\nfollowed by many actors.\n\nWhether the medium is hardware or software, the goal of AL is to create the\nimpression that an engineered system is _alive_. Unlike the creative work of\n”character design,” where the focus is on visual appearance, this assignment\nis concerned with the construction of a creature with responsive, dynamic\nbehaviors that are contingent on environmental interactions. To emphasize\nthis, an instructor may challenge students to instill lifelike behavior in\ncreatures whose bodies are restricted to ultra-minimal forms, such as a pair\nof rectangles.\n\nA creature without a context is boring—with nothing to do, and no one to do it\nto. Stories happen, character is perceived, meanings are made when an agent\noperates on or within an environment that likewise acts on it. By placing a\ncreature into feedback with external forces or subjects, and especially with\nthe actions of an interacting user, we can create companions that stave off\nloneliness or appear to have feelings, virtual pets like the Tamagotchi that\nevoke empathy through their fragility, or sublime simulated ecosystems that\nevolve in surprising ways.\n\n![c5-fig-0027.jpg](../images/c5-fig-0027.jpg)\n\n27\\. _Connected Worlds_ (2015) by Design IO (Theo Watson and Emily Gobeille)\nis a large-scale interactive installation developed for New York's Great Hall\nof Science. The work presents six immersive habitats, projected across both\nwalls and floors, that allow visitors to interact with simulated water flows\nand learn about the role of the water cycle in different ecosystems. Watson\nand Gobeille devised dozens of responsive creature species to populate the\nspace.\n\n![c5-fig-0028.jpg](../images/c5-fig-0028.jpg)\n\n28\\. Neurophysiologist William Grey Walter's “tortoises” of 1948–1949 were\nearly electronic robots capable of phototaxis and object avoidance.\n\n![c5-fig-0029.jpg](../images/c5-fig-0029.jpg)\n\n29\\. Karl Sims's _Evolved Virtual Creatures_ (1994) presents simulated\ncreatures that evolve charming and idiosyncratic methods of locomotion using\ngenetic algorithms.\n\n## Additional Projects\n\n  * Ian Cheng, _Bob (Bag of Beliefs)_ , 2018–2019, animations of evolving artificial lifeforms.\n  * James Conway, _Game of Life,_ 1970, cellular automaton.\n  * Sofia Crespo, _Neural Zoo_ , 2018, creatures generated with neural nets.\n  * Wim Delvoye, _Cloaca_ , 2000–2007, large-scale digestion machine.\n  * Ulrike Gabriel, _Terrain 01_ , 1993, photoresponsive robotic installation.\n  * Alexandra Daisy Ginsberg. _The Substitute_ , 2019, video installation and animation.\n  * Edward Ihnatowicz, _Senster_ , 1970, interactive robotic sculpture.\n  * William Latham, _Mutator C_ , 1993, generated 3D renderings.\n  * Golan Levin et al., _Single Cell_ and _Double Cell_ , 2001–2002, online bestiary.\n  * Jon McCormack, _Morphogenesis Series_ , 2002, computer model and prints on photo media.\n  * Brandon Morse, _A Confidence of Vertices_ , 2008, generated animation.\n  * Adrià Navarro, _Generative Play_ , 2013, generated characters and card game.\n  * Jane Prophet and Gordon Selley, _TechnoSphere_ , 1995–2002, online environment and generative design tool.\n  * Matt Pyke (Universal Everything), _Nokia Friends_ , 2008, generative squishy characters.\n  * Susana Soares, _Upflanze_ , 2014, hypothetical plant archetypes.\n  * Christa Sommerer and Laurent Mignonneau, _A-Volve_ , 1994–1995, interactive installation.\n  * Christa Sommerer and Laurent Mignonneau, _Lifewriter_ , 2006, interactive installation.\n  * Francis Tseng and Fei Liu, _Humans of Simulated New York_ , 2016, participatory economic simulation.\n  * Juanelo Turriano, _Automaton of a Friar_ , c. 1560, Smithsonian Institution, National Museum of American History.\n  * Jacques de Vaucanson, _Canard Digérateur_ , 1739, automaton in the form of a duck.\n  * Lukas Vojir, _Processing Monsters_ , 2008–2010, online bestiary.\n  * Will Wright and Chaim Gingold et al., _Spore Creature Creator,_ 2002–2008, creature construction software.\n\n## Readings\n\n  1. Jean Baudrillard, _Simulacra and Simulation_ (Ann Arbor: University of Michigan Press, 1994).\n  2. Valentino Braitenberg, _Vehicles: Experiments in Synthetic Psychology_ (Cambridge, MA: MIT Press, 1984).\n  3. Bert Wang-Chak Chan, “Lenia: Biology of Artificial Life,” _Complex Systems_ 28, no. 3 (2019), 251–286.\n  4. Ian Cheng et al., _Emissaries Guide to Worlding_ (London: Koenig Books, 2018).\n  5. Craig W. Reynolds, “Steering Behaviors For Autonomous Characters,” _Proceedings of the Game Developers Conference_ (1999), 763–782.\n  6. Daniel Shiffman, _The Nature of Code: Simulating Natural Systems with Processing_ (self-pub., 2012).\n  7. Mitchell Whitelaw, _Metacreation: Art and Artificial Life_ (Cambridge, MA: MIT Press, 2006).\n\n\n# Custom Pixel\n\nReimagining the display\n\n![c6-fig-0030.jpg](../images/c6-fig-0030.jpg)\n\n30\\. Aram Bartholl's _0,16_ (2009) is a wholly analog light installation that\nuses translucent paper to transform the visitors’ shadows into large-scale\npixels.\n\n## Brief\n\nPixels are the fundamental building blocks of bitmap images and digital\ndisplays. Conventionally, pixels are square, uniform, and parked in a static\nCartesian grid. Challenge these assumptions by writing code to reconstruct a\nspecific photographic image using “custom picture elements” of your own\ndevising. What if pixels were hexagonal? Could they be arranged on an\nirregular lattice? What if they overlapped, moved, or had a variety of sizes?\nWhat if an image was itself constructed from fragments of other images, or\nfrom a database of tiny icons, symbols, flags, or emojis? Think carefully\nabout the relationships between your “pixel concept” and the image you choose\nto transform. At no time should your original image be directly visible.\n\n## Learning Objectives\n\n  * Reflect on the constitution and perception of images in art, design, and digital imaging\n  * Explore the relationship between an image and its component parts\n  * Appropriately access low-level pixel data\n\n## Variations\n\n  * Implement your own style of “Divisionism” by constructing picture elements from pairs or triplets of contrasting patches.\n  * Instead of square pixels, use polar coordinates to construct an image from annular sectors.\n  * Design picture elements that resemble brushstrokes, with control parameters like “thickness” and “irregularity.” How can these elements reproduce higher-level visual features in your original image, such as its edges, gradients, and spots? Can your brushstrokes capture the orientations of these features?\n  * Modify your code so that it interprets a video or live webcam stream, giving consideration to how you can reduce frame-to-frame differences and flickering. What sort of subject matter is best suited to your algorithm?\n  * Invent a display that produces images by moving or rearranging a collection of real, physical objects (such as pebbles or candy).\n  * Some image compression algorithms operate by means of specially designed picture elements. Read about quadtrees, run-length encoding, dictionaries of 8x8-pixel JPEG blocks, and Gabor wavelets. Devise a compression algorithm inspired by one of these techniques. What are the aesthetics and artifacts of your algorithm?\n\n## Making It Meaningful\n\nArtworks comprised of novel pixels are literally “new media”; they illustrate\nMcLuhan's well-worn adage that the medium (itself) is the message. The impact\nof this message can be weakened, however, when a novel imaging technique is\napplied without consideration to its subject. Is the project “just a display”?\nA key path to the production of meaning in this genre of work, therefore, is\nthe purposeful creation of a relationship between form and content: between\nthe nature of a display's constituent picture elements and the subjects they\nportray. Projects by Chris Jordan, El Anatsui, and Jenny Odell, for example,\npresent the conceptual contradiction that arises when something valuable\nappears to be wholly constructed out of trash or detritus.\n\nCustom picture elements can create intrigue at multiple scales, allowing for\nnew ways of seeing that urge the viewer to reconsider the experience of image\nconsumption. What was formerly an instantaneous phenomenon (“I saw it”)\nbecomes an interactive process of close observation (“I examined it”). A\ncommon strategy is to alternately create and resolve visual ambiguity. Images\nconstructed using a photomosaic technique, for example, provide an engrossing\nmeans by which fans can “zoom in” to reminisce about a favorite subject. The\nPointillist and Divisionist painters of the late 1800s, by contrast, created\npaintings that required the viewer to “zoom out,” actively fusing spots of\ncolors in their mind's eye in order to recognize the painting's subject.\nMulti-scale works also defy easy reproduction, increasing their aura.\n\nThe use of custom picture elements can prompt reflection about the conditions\nof image production. When we pore over a Byzantine micromosaic, we marvel at\nthe evident labor that went into creating and placing each individual tile.\nContemporary artworks like _10,000 Cents_ by Koblin and Kawashima address\nthese conditions directly, making use of networked labor markets like\nMechanical Turk to question the nature of digital economies and authorship.\n\n![c6-fig-0031.jpg](../images/c6-fig-0031.jpg)\n\n31\\. Textile arts such as weaving, knitting, and needlepoint are ancient\ncultural practices of pixel logic. In works like _South of The Border_ (1958,\nshown here in detail), textile designer and Bauhaus educator Anni Albers used\ningenious combinations of threads to produce custom pixels.\n\n![c6-fig-0032.jpg](../images/c6-fig-0032.jpg)\n\n32\\. Leon Harmon and Ken Knowlton's _Studies in Perception #1_ (1966) is a\nmosaic of small scientific symbols, arranged into the form of a nude using\nKnowlton's BEFLIX programming language. It appeared on the front page of the\n_New York Times_ on October 11, 1967, and was shown in the Museum of Modern\nArt's 1968 exhibition _The Machine as Seen at the End of the Mechanical Age_ ,\nintroducing computer art to a broad American public for the first time.\n\n![c6-fig-0033.jpg](../images/c6-fig-0033.jpg)\n\n33\\. Jenny Odell describes her _Garbage Selfie_ as “a self portrait composed\nof everything I threw away, recycled or composted between February 10 and\nMarch 1, 2014.”\n\n![c6-fig-0034.jpg](../images/c6-fig-0034.jpg)\n\n34\\. Charles Gaines dissects photographs into grids of hand-painted elements,\nusing indeterminacy and other mathematical principles, in order to explore the\nconstructed nature of representation. Shown here and in detail is his _Numbers\nand Trees: Central Park Series II: Tree #8, Amelia_ (2016), a photograph with\nacrylic on plexiglas.\n\n![c6-fig-0035.jpg](../images/c6-fig-0035.jpg)\n\n35\\. Aaron Koblin and Takashi Kawashima's _10,000 Cents_ (2008, shown here in\ndetail) is a crowdsourcing system that produced a representation of a US $100\nbill. The artists write: “Using a custom drawing tool, thousands of\nindividuals working in isolation from one another painted a tiny part of the\nbill without knowledge of the overall task. Workers were paid one cent each\nvia Amazon's Mechanical Turk.” The project's conditions of production and\ndistribution perhaps inadvertently highlight the inequity of distributed labor\nmarkets.\n\n![c6-fig-0036.jpg](../images/c6-fig-0036.jpg)\n\n36\\. Daniel Rozin has spent decades producing hand-crafted pixels from every\nmaterial imaginable including wood, fur, and even toy penguins. His _Peg\nMirror_ (2007) consists of 650 wooden dowels that are cut on an angle and\nindividually motorized. As they rotate, the pegs catch light or cast shadows,\nrecreating the scene captured by a central camera.\n\n![c6-fig-0037.jpg](../images/c6-fig-0037.jpg)\n\n37\\. Scott Blake's _Chuck Close Filter_ software (2001–2012) transforms any\nimage into the pixelated style of a Chuck Close painting. Following its\nrelease, Blake was threatened with legal action by Close, who asserted that\nthe software trivialized his art and threatened his livelihood. Blake's _Self\nPortrait Made with Lucas Tiles_ (2012) is wholly constructed of squares taken\nfrom Close's painting _Lucas_ (1991).\n\n## Additional Projects\n\n  * El Anatsui, _Earth Shedding Its Skin_ , 2019, aluminum and copper wire.\n  * Angela Bulloch, _Horizontal Technicolor_ , 2002, modular light sculpture.\n  * Jim Campbell, _Reconstruction Series_ , 2002–2009, custom electronics, LEDs, and cast resin screens.\n  * Evil Mad Scientist Laboratories, _StippleGen,_ 2012, image processing software.\n  * Frédéric Eyl and Gunnar Green, _Aperture_ , 2004–05, interactive facade.\n  * Kelly Heaton, _Reflection Loop_ , 2001, kinetic sculpture with Furby pixels.\n  * Chris Jordan, _Running the Numbers II: Portraits of Global Mass Culture_ , 2009–, variable material.\n  * Rafael Lozano-Hemmer, _Pareidolium_ , 2018, software, camera, and ultrasonic atomizers.\n  * Vik Muniz, _Pictures of Garbage_ , 2008, photographed arrangements of detritus.\n  * Everest Pipkin, _Unicode Birds_ , 2013–2019, unicode and twitter bot.\n  * Gonzalo Reyes-Araos, _RGB Paintings_ , 2018, oil on paper, mounted on aluminum, KINDL Berlin.\n  * Elliat Rich, _What colour is the sky_ , 2011, printed aluminum swatches and steel frame.\n  * Peiqi Su, _The Penis Wall_ , 2014, kinetic sculpture.\n  * Tali Weinberg, _0.01% of vacant potential homes,_ 2012, archival pigment print on paper.\n\n## Readings\n\n  1. Scott Blake, “My Chuck Close Problem,” _Hyperallergic_ , July 9, 2012.\n  2. Meredith Hoy, _From Point to Pixel: A Genealogy of Digital Aesthetics_ (Hanover, NH: Dartmouth College Press, 2017).\n  3. Christopher Jobson, “People as Pixels,” _This Is Colossal_ , February 24, 2012.\n  4. Julius Nelson, _Artyping_ (Johnstown, PA: Artyping Bureau, 1939).\n  5. Omar Shehata, “Unraveling the JPEG,” _Parametric Press_ 1 (Spring 2019): n.p.\n  6. Rob Silvers, “Photomosaics: Putting Pictures in Their Place” (master's thesis, MIT, 1996).\n  7. Barrie Tullett, _Typewriter Art: A Modern Anthology_ (London: Laurence King Publishing, 2014).\n\n\n# Drawing Machine\n\nTools for doodling and depicting\n\n![c7-fig-0038.jpg](../images/c7-fig-0038.jpg)\n\n38\\. The paintings in Addie Wagenknecht's _Alone Together_ series (2017) are\nproduced by a Roomba domestic robot as it traces around the artist's reclining\nbody, smearing International Klein Blue paint as it goes. The work responds to\nboth the technologized automation of labor and the history of action painting,\nwhere women's bodies were sometimes used as paint brushes.\n\n## Brief\n\nCreate a program that expands, augments, muddles, complicates, questions,\nanalyzes, spoils, undermines, improves, accelerates, or otherwise alters the\nconcept or act of drawing. Clarify the intent of your system, such as whether\nyour project is a tool, toy, game, or performance instrument. Demonstrate its\nunique properties by using it to produce a series of at least three drawings.\n\n## Learning Objectives\n\n  * Deeply examine the process of drawing\n  * Evaluate the affordances and artifacts of creative tools\n  * Use appropriate data structures for recording, storing, and manipulating gesture data\n\n## Variations\n\n  * Create a system that brings drawings to life.\n  * Question intent. Derive marks from the actions of an entity that is (probably) unaware that it is drawing: a pedestrian, a turtle, a kite.\n  * Deprivilege the dominant hand. Make a drawing tool that is operated by the user's face, voice, or some other part of their body.\n  * Drawing is usually conceived as a solo activity. Create a drawing machine that requires two people to operate it.\n  * Make a “single purpose” drawing machine, such as a tool that can only draw ducks.\n  * Identify and challenge some other basic assumption about drawing, such as the notion that drawings are made on a flat surface; that drawings are recordings that are meant to endure; that drawings can be “finished”; or that drawings are distinct from text.\n  * Make a tool that functions critically—for instance, that rejects the technological imperatives of accuracy, realism, and utility and instead prioritizes expressivity, irreproducibility, and whimsy.\n  * Develop a system that analyzes and derives insights from a database of drawings.\n\n## Making It Meaningful\n\n“Make your own paintbrush” is a classic art school prompt, encouraging\nstudents to create tools from parts of the body or found materials. The\nexercise personalizes and defamiliarizes the act of mark-making, and invites a\ndeeper consideration of how tools and technologies shape artistic expression.\nThe Drawing Machine assignment captures this spirit in the domain of software,\nopening up questions of constraints, autonomy, and augmentation in human-\nmachine collaboration.\n\nThe act of drawing translates gesture from the body to the page through an\napparatus. The primogenitor of today's computational drawing tools was\nSketchpad, developed by Ivan Sutherland as part of his MIT PhD thesis in 1963,\nwhich made it possible for a person and a computer “to converse rapidly\nthrough the medium of line drawings.” i Widely credited as the first graphical\nuser interface, Sketchpad substituted page for screen and leveraged the\ninfinite malleability of virtual form for the first time. Its descendants,\nlike AutoCAD, Photoshop, and Illustrator, are now mature products, and their\ninteraction vocabularies have become standardized and ubiquitous and thus\ntaken for granted. Innovating in this space now requires either breaking a\ncore assumption about drawing, or experimenting with mark-making in unfamiliar\ncontexts.\n\n![c7-fig-0039.jpg](../images/c7-fig-0039.jpg)\n\n39\\. Sougwen Chung collaborates with semi-autonomous robot arms to produce\ndrawings like those in her _Drawing Operations_ series (2015–2018).\n\n![c7-fig-0040.jpg](../images/c7-fig-0040.jpg)\n\n40\\. To create _Tree Drawings_ (2007), Tim Knowles attaches pencils to the\nlimbs of trees and captures the resulting marks on paper.\n\n![c7-fig-0041.jpg](../images/c7-fig-0041.jpg)\n\n41\\. The _Sketch Furniture_ (2007) system by Swedish agency Front Design\ncombines motion capture and digital fabrication. Using this system, a designer\ncan record three-dimensional, freehand drawings of virtual furniture at a 1:1\nscale, and then fabricate these forms using a large-scale 3D printer.\n\n![c7-fig-0042.jpg](../images/c7-fig-0042.jpg)\n\n42\\. Graffiti Research Lab was a collective dedicated to outfitting artists\nand protesters with open-source technologies for urban communication. In\n_L.A.S.E.R. Tag_ (2007), a computer vision system tracks the spot of a user's\nhandheld laser pointer. A high-power video projection, calibrated to the\ncamera system, enables the user to display their tags at an architectural\nscale.\n\n![c7-fig-0043.jpg](../images/c7-fig-0043.jpg)\n\n43\\. _Sloppy Forgeries_ (2018) by Jonah Warren is a game in which players,\nusing computer mice and simplified color palettes, race to draw the most\naccurate replicas of famous paintings.\n\n![c7-fig-0044.jpg](../images/c7-fig-0044.jpg)\n\n44\\. In Julien Maire's _Digit_ performance (2006), poetic texts magically\nappear beneath the artist's fingertip as he runs it across a sheet of white\npaper. Mechanisms from a thermal printer have been discreetly concealed under\nhis hand.\n\n![c7-fig-0045.jpg](../images/c7-fig-0045.jpg)\n\n45\\. Paul Haeberli's _DynaDraw_ (1989) is an early computational drawing\nenvironment in which the brush is modeled as a springy physical object with\nsimulated mass, velocity, and friction. The augmentation of the drawing\nprocess with exaggerated virtual physics, and a range of adjustable\nparameters, leads to new forms of gestural and calligraphic play.\n\n## Additional Projects\n\n  * Akay, _Tool No. 10: Robo-Rainbow_ , 2010, device for spray-painting rainbows.\n  * Peter Edmunds, _SwarmSketch_ , 2005, collaborative digital canvas.\n  * Free Art and Technology (F.A.T.) Lab, _Eyewriter_ , 2009, eye-tracking drawing tool.\n  * William Forsythe, _Lectures from Improvisation Technologies_ , 1994, video series.\n  * Ben Fry, _FugPaint_ , 1998, antagonistic paint program.\n  * Johannes Gees, _Communimage_ , 1999, digital collaborative collage.\n  * David Ha, Jonas Jongejan, and Ian Johnson, _Sketch-RNN Demos_ , 2017, neural network drawing experiment.\n  * Desmond Paul Henry, _Serpent_ , 1962, pen and ink mechanical drawing, Victoria and Albert Museum, London.\n  * Jarryd Huntley, _Art Club Challenge_ , 2018, drawing game iOS app.\n  * Jonas Jongejan et al., _Quick, Draw!_ , 2017, drawing game with neural net.\n  * So Kanno and Takahiro Yamaguchi, _Senseless Drawing Bot_ , 2011, chaotic drawing robot.\n  * Soonho Kwon, Harsh Kedia, and Akshat Prakash, _Anti-Drawing Machine_ , 2019, antagonistic drawing machine.\n  * Louise Latter and Holly Gramazio, _Doodle_ , 2020, exhibition of software art at Birmingham Open Media (BOM).\n  * Jürg Lehni, _Viktor_ , 2011, scalable robotic drawing machine.\n  * Golan Levin, _Yellowtail_ , 1998, audiovisual animation software.\n  * Zachary Lieberman, _Drawn_ , 2005, interactive installation.\n  * Zachary Lieberman, _Inkspace_ , 2015, accelerometer-dependent drawing app.\n  * John Maeda, _Timepaint_ , 1993, drawing software demonstration.\n  * Kyle McDonald and Matt Mets, _Blind Self-Portrait_ , 2011, machine-aided drawing system.\n  * JT Nimoy, _Scribble Variations_ , 2001, drawing software.\n  * Daphne Oram, _Oramics Machine_ , 1962, photo-input synthesizer for “drawing sound.”\n  * James Paterson and Amit Pitaru, _Rhonda Forever_ , 2003–2010, 3D drawing tool.\n  * Pablo Picasso, _Light Paintings,_ 1950, long-exposure photographs by Gjon Mili, Museum of Modern Art, New York.\n  * Amit Pitaru, _Sonic Wire Sculptor_ , 2003, app for 3D drawing and composition.\n  * Eric Rosenbaum and Jay Silver, _SingingFingers (Finger Paint with Your Voice)_ , 2010, software for fingerpainting with sound.\n  * Toby Schachman, _Recursive Drawing_ , 2012, drawing software.\n  * Karina Smigla-Bobinski, _ADA_ , 2011, analog interactive installation.\n  * Alvy Ray Smith and Dick Shoup, _SuperPaint_ , 1973–75, 8-bit paint system.\n  * Scott Snibbe, _Bubble Harp_ , 1997, drawing software implementing Voronoi diagrams.\n  * Scott Snibbe, _Motion Phone_ , 1995–2012, networked animation system.\n  * Scott Snibbe et al., _Haptic Sculpting_ , 1998, prototype software for physically mediated haptic sculpting.\n  * Laurie Spiegel, _VAMPIRE_ , 1974–1979, color-music mixing software.\n  * Christine Sugrue and Damian Stewart, _A Cable Plays_ , 2008, audiovisual performance.\n  * Ivan E. Sutherland, _Sketchpad: A Man-Machine Graphical Communication System_ , 1964, drawing program.\n  * Clement Valla, _A Sequence of Lines Consecutively Traced by Five Hundred Individuals_ , 2011, video.\n  * Jeremy Wood, _GPS Drawings_ , 2014, drawings from GPS data.\n  * Iannis Xenakis, _UPIC_ , 1977, graphical music scoring system.\n\n## Readings\n\n  1. Pablo Garcia, “Drawing Machines,” DrawingMachines.org, accessed April 14, 2020.\n  2. Jennifer Jacobs, Joel Brandt, Radomír Mech, and Mitchel Resnick, “Extending Manual Drawing Practices with Artist-Centric Programming Tools,” in _Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems_ (New York: Association for Computing Machinery, 2018), 1–13.\n  3. Golan Levin, “2-02 (Drawing),” Interactive Art and Computational Design, Carnegie Mellon University, Spring 2016, accessed April 14, 2020.\n  4. Zach Lieberman, “From Point A to Point B” (lecture, Eyeo Festival, Minneapolis, MN, June 2015), video, 7:20–19:00.\n  5. Scott Snibbe and Golan Levin, “Instruments for Dynamic Abstraction,” in _Proceedings of the Symposium on Nonphotorealistic Animation and Rendering_ (New York: Association for Computing Machinery, 2000).\n\n## Notes\n\ni Ivan E. Sutherland, “Sketchpad: A Man-Machine Graphical Communication\nSystem,” _Simulation_ 2, no. 5 (1964): R-3.\n\n\n# Modular Alphabet\n\nStructuring letterforms with a common model\n\n![c8-fig-0046.jpg](../images/c8-fig-0046.jpg)\n\n46\\. Nikita Pashenkov's _Alphabot_ (2000) is a Transformer-like virtual robot\n“that communicates with humans by changing its shape to form characters of the\nEnglish alphabet.” The 3D robot is constructed from eight hinged segments, and\ncan smoothly re-fold its shape into any letterform.\n\n## Brief\n\nDesign a typeface (using any graphic primitives you prefer) so that all of the\nletters of the alphabet are structured by the same software parameters and\ngraphic logic. For example, you might design an alphabet in which every letter\nis exclusively constructed from three arcs, or from four rectangles, or from a\nsmall grid of squares. After you have designed all of your letters, typeset\nthe entire alphabet in a single image so that it can be seen at a glance.\n\nAn essential technical goal is for you to store descriptive parameters for\nyour letters in some kind of array or object-oriented data structure, and then\ncreate a single function that renders any requested letter from this data. If\nyou're writing individual functions to draw each letter, you're doing\nsomething wrong.\n\n## Learning Objectives\n\n  * Conceive and appraise graphical concepts for dynamic typography\n  * Use parameters to manipulate and/or animate letterforms\n  * Apply principles of metadesign to font design\n  * Use arrays to store geometric data\n\n## Variations\n\n  * Typeset a carefully chosen word that has a special relationship to your letterforms’ design.\n  * Give your letters inherently unstable properties. Animate them by deflecting their control points with a sinusoidal wiggle, Perlin noise, or real-time interactivity.\n  * Create a system to animate the transitions between letterforms, allowing any letter to smoothly morph into any other. Pressing a key should initiate an animated transition (of approximately a second's duration) from the previous letter to the next desired letter. _Instructors: for introductory students, it may be helpful to provide a template for transitioning between letterforms._\n  * Consider a design in which letters are traces deposited by moving particles—whose paths are affected by forces from different spatial configurations of attractors and repulsors.\n  * A “forced aligner” is a computer program that takes audio files and their transcripts and returns extremely precise timing information. Using your typeface and a “forced aligner” (such as _Gentle_ by Ochshorn & Hawkins), create time-synced dynamic typography that not only synchronizes perfectly with a speech file, but also responds parametrically to the sound of the speaker's voice.\n\n## Making It Meaningful\n\nExtending from Adrian Frutiger's Univers (1954), Donald Knuth's computational\nMETAFONT (1977), and Adobe's “Multiple Master” fonts (1994), it has become\nincreasingly common practice to design highly adaptable type _systems_ that go\nfar beyond the rigid limits of static type _faces_. Peter Biľak writes: “Prior\nto Univers, type designers concerned themselves with the relationships between\nletters of the same set, how an ’A’ is different from a ’B’. Univers goes\nbeyond the quest to design individual letters, attempting instead to create a\nsystem of relationships between different sets of shapes which share\ndistinctive parameters.” i\n\nThis prompt prioritizes the creative value of constraints. Restricted to\ndesigning letterforms with shared parameters, it requires modularity, economy,\nand an ingenuity about shapes whose variety and complexity students often take\nfor granted. The expressive potential for contingent, interactive, and subtly\ntime-varying form systems should not be overlooked. Take a moment to reflect\non your resultant type system. For which letters does the structuring pattern\nsucceed best or fail hardest?\n\n![c8-fig-0047.jpg](../images/c8-fig-0047.jpg)\n\n47\\. Mary Huang's _Typeface: A Typographic Photobooth_ (2010) is a font whose\nparameters (such as slant, x-height, etc.) are governed by signals from a\nreal-time face tracker.\n\n![c8-fig-0048.jpg](../images/c8-fig-0048.jpg)\n\n48\\. David Lu's _Letter 3_ (2002) presents an interactive alphabet whose\nletters are formed by manipulating the control points of a single, compound\nBézier curve. Each letter can fluidly transform into any other.\n\n![c8-fig-0049.jpg](../images/c8-fig-0049.jpg)\n\n49\\. In Peter Cho's classic _Type Me, Type Me Not_ (1997), each letter is\nconstructed from two “Pac-Man” filled arcs and is represented by just ten\nnumbers.\n\n![c8-fig-0050.jpg](../images/c8-fig-0050.jpg)\n\n50\\. In Yuichiro Katsumoto's _Mojigen & Sujigen_ (2016), six interconnected\nelectromechanical elements form the letters of the alphabet by moving into\ndifferent positions.\n\n![c8-fig-0051.jpg](../images/c8-fig-0051.jpg)\n\n51\\. Bruno Munari's _ABC with Imagination_ (1960), a children's alphabet book,\nincluded a set of modular pieces for typographic play.\n\n![c8-fig-0052.jpg](../images/c8-fig-0052.jpg)\n\n52\\. In 1878, amidst widespread use of the calligraphic Fraktur blackletter,\ninventor Friedrich Soennecken sought to modernize and rationalize German\ntypography and penmanship. Soennecken developed Schriftsystem, a method for\nconstructing glyphs exclusively from arcs and straight lines. Decades later,\nhis system influenced the design of the German Institute for Standardization's\nDIN 1451 Engschrift typeface, now used throughout Germany for many purposes.\n\n![c8-fig-0053.jpg](../images/c8-fig-0053.jpg)\n\n53\\. _Fregio Mecano_ (“mechanic ornament”) is a set of 20 modular, geometric\nshapes for constructing letters and images in highly flexible ways. Devised in\nItaly by Giulio da Milano in the early 1930s, it was released by the Nebiolo\ntypefoundry.\n\n![c8-fig-0054.jpg](../images/c8-fig-0054.jpg)\n\n54\\. Julius Popp's _bit.fall_ (2001–2016) produces ephemeral texts by\nreleasing droplets of water through a series of precisely timed, computer-\ncontrolled solenoid valves.\n\n## Additional Projects\n\n  * Agyei Archer, _Crispy_ , 2017, variable font.\n  * Erik Bernhardsson, _Analyzing 50K Fonts Using Deep Neural Networks_ , 2016, machine-learning font.\n  * Michael Flückiger and Nicolas Kunz, _LAIKA: A Dynamic Typeface_ , 2009, interactive font.\n  * Christoph Knoth, _Computed Type_ , 2011, interactive typographic system.\n  * Donald Knuth, _METAFONT_ , 1977–1984, font design system.\n  * Urs Lehni, Jürg Lehni, and Rafael Koch, _Lego Font Creator_ , 2001, font.\n  * John Maeda, _Tangram Font_ , 1993, font.\n  * JT Nimoy, _Robotic Type_ , 2004, robotic typographic system.\n  * Michael Schmitz, _genoTyp_ , 2004, genetic typography tool.\n  * Kyuha (Q) Shim, _Code & Type_, 2013, book and website.\n  * Joan Trochut, _Supertipo Veloz_ , 1942, modular type system digitized by Andreu Balius and Àlex Trochut in 2004.\n  * Julieta Ulanovsky, _Montserrat_ , 2019, variable font.\n  * Flavia Zimbardi, _Lygia_ , 2020, variable font.\n\n## Readings\n\n  1. Johanna Drucker, _The Alphabetic Labyrinth: The Letters in History and Imagination_ (London: Thames and Hudson, 1995).\n  2. C. S. Jones, “What Is Algorithmic Typography?,” _Creative Market_ (blog), May 2, 2016.\n  3. Christoph Knoth, “Computed Type Design” (master's thesis, École Cantonal d’Art de Lausanne, 2011).\n  4. Donald Knuth, “The Concept of a Meta-Font,” _Visible Language_ 16, no. 1 (1982): 3–27.\n  5. Jürg Lehni, “Typeface as Programme,” Typotheque.\n  6. Ellen Lupton, _Thinking with Type: A Critical Guide for Designers, Writers, Editors, and Students_ (New York: Princeton Architectural Press, 2010).\n  7. Rune Madsen, “Typography,” lecture for Programming Design Systems (NYU).\n  8. “Modular Typography,” Tumblr.\n  9. François Rappo and Jürg Lehni, _Typeface as Programme_ (Lausanne, France: École Cantonal d’Art de Lausanne, 2009).\n  10. Dexter Sinister, “Letter & Spirit,” _Bulletins of the Serving Library_ 3 (2012).\n  11. Alexander Tochilovsky, “Super-Veloz: Modular Modern” (lecture, San Francisco Public Library, March 13, 2018).\n\n## Notes\n\ni Peter Biľak, “Designing Type Systems,” 2012,\n[ilovetypography.com](http://ilovetypography.com).\n\n\n# Data Self-Portrait\n\nQuantified selfie\n\n![c9-fig-0055.jpg](../images/c9-fig-0055.jpg)\n\n55\\. Shan Huang's _Favicon Diary_ (2014), developed while she was a student at\nCarnegie Mellon, is a chronologically ordered compilation of the favicons from\nevery website that she visited over the course of several months. In addition\nto producing her own data self-portrait, Huang also released a Google Chrome\nbrowser extension so that others could do the same.\n\n## Brief\n\nCreate a visualization that presents insights from a dataset about yourself.\nYou may use pre-existing data (such as your email archive, fitness tracker\ndata, etc.) or create a new system specifically for collecting data about an\naspect of your life. The data you collect need not be temporal; for example,\nyou might reimagine your wardrobe as a database of interrelated items. Can you\ncollect data about phenomena that nobody has seen or thought of before?\n\n_Your visualization is a tool you're building to help you answer a question\nabout yourself._ You can use existing measurement technologies, or devise new\nmanual or automatic data collection techniques. You're encouraged (but not\nrequired) to combine multiple sources of data, to make interesting\ncomparisons.\n\n## Learning Objectives\n\n  * Implement the complete pipeline of information visualization: from data acquisition, parsing, and cleaning to representation, distillation, and interaction\n  * Identify and use fundamental data structures\n  * Carry out an in-depth consideration of information aesthetics\n\n## Variations\n\n  * Give yourself permission to be specific. Sometimes focusing your investigation on a subset of your data is not only simpler, but can be much more interesting. For example, instead of visualizing all of your text messages (a study of your texting behavior), what if you only examine the ones you exchanged with a certain person (a study of your relationship)?\n  * Take time-stamped data and present it in a way that does not use a timeline. Consider your email: While you can certainly visualize it as a timeline (e.g., the number of emails you process per day), you could also view it as a graph (e.g., the social network of your contacts), or as a histogram (e.g., the words you use most often).\n  * Build a custom device to capture data about yourself, using a novel sensor, or by mounting a sensor in an unexpected place.\n  * Give your data-capture system to your friends. Ask them to collect data for two weeks. Produce a set of what Edward Tufte calls “small multiples”: uniform visualizations that allow your friends’ data to be easily compared.\n  * Create an interactive visualization. Consider how your project can allow operations like zooming, sorting, filtering, and/or querying.\n\n## Making It Meaningful\n\nDatabases are amassed from our digital communications, search histories,\ntransactions, step counts, sleep patterns, and journeys. How do we make sense\nof this “data exhaust” and how does this change our understanding of\nourselves? What data is collected and what is not? What sorts of activities\nresist quantification and measurement and why? This task invites a deep\nexploration of portraiture and self-representation in the age of\nquantification.\n\nCorporate and governmental surveillance is changing our lives on both personal\nand societal scales. How does the knowledge that our lives are being recorded\nchange them? Consider how fitness tracking, initially celebrated by the\nquantified self community for its promise of new insight, was later\naggressively promoted by the insurance industry and, in some cases, became\nrequired by employers. Likewise, consider the data collected by major social\nmedia platforms, and how this data feeds targeted advertising, structures the\nalgorithmic presentation of online content, and produces contemporary\nphenomena like filter bubbles.\n\n![c9-fig-0056.jpg](../images/c9-fig-0056.jpg)\n\n56\\. In _Dear Data_ (2016), a year-long project, Giorgia Lupi and Stephanie\nPosavec mailed weekly hand-drawn postcards to each other that visualized some\n(quantified) aspect of their lives, such as the number of doors they entered,\nor how many times they laughed.\n\n![c9-fig-0057.jpg](../images/c9-fig-0057.jpg)\n\n57\\. Tracey Emin's _Everyone I Have Ever Slept With 1963–1995_ (1995) is a\ntent with the names of Emin's lovers embroidered on the inside.\n\n![c9-fig-0058.jpg](../images/c9-fig-0058.jpg)\n\n58\\. Fernanda Viégas's _Themail_ (2006) analyzes the contents of email\ncorrespondence, showing the significant words that characterize each\nrelationship.\n\n![c9-fig-0059.jpg](../images/c9-fig-0059.jpg)\n\n59\\. Responding to a late-1970s surge in interest in the body's natural\ncircadian rhythms, Sonya Rapoport's _Biorhythm Audience Participation\nPerformance_ (1981) used a commercial kit to predict her daily biorhythms,\nthen compared her own experience with the computerized predictions.\n\n![c9-fig-0060.jpg](../images/c9-fig-0060.jpg)\n\n60\\. _Stay_ (2011) is an example of Hasan Elahi's ongoing “self-surveillance”\nwork, in which he collects photographs of his daily life and preemptively\nsends them to the FBI.\n\n## Additional Projects\n\n  * Rachel Binx, _Wi-Fi Diary_ , 2014, image capture software.\n  * Beatriz da Costa, Jamie Schulte, and Brooke Singer, _Swipe_ , 2004, performance and installation.\n  * Hang Do Thi Duc and Regina Flores Mir, _Data Selfie_ , 2017, browser extension.\n  * W. E. B. Du Bois, charts prepared for the Negro Exhibit of the American Section at the Paris Exposition Universelle, 1900, ink on posterboard.\n  * Luke DuBois, _Hindsight Is Always 20/20_ , 2008, light boxes, software, and presidential speech transcripts.\n  * Takehito Etani, _Masticator_ , 2005, electronic wearable with audiovisual feedback.\n  * Nick Felton, _Annual Reports_ , 2005–2014, letterpress and lithograph.\n  * Laurie Frick, _Time Blocks Series_ , 2014–2015, wood-based data visualization.\n  * Brian House, _Quotidian Record_ , 2012, vinyl record.\n  * Jen Lowe, _One Human HeartBeat_ , 2014, online biometrics visualization.\n  * Katie McCurdy, _Pictal Health: Health History Visualization_ , 2014, health data software.\n  * Lam Thuy Vo, _Quantified Breakup_ (blog), Tumblr, October 23, 2013–September 25, 2015.\n  * Stephen Wolfram, “The Personal Analytics of My Life,” _Stephen Wolfram: Writings_ (blog), March 8, 2012.\n\n## Readings\n\n  1. Witney Battle-Baptiste and Britt Rusert, eds., _W. E. B. Du Bois's Data Portraits: Visualizing Black America_ (San Francisco: Chronicle Books, 2018).\n  2. Robert Crease, “Measurement and Its Discontents,” _New York Times_ , October 22, 2011.\n  3. Judith Donath et al., “Data Portraits,” in _Proceedings of SIGGRAPH 2010_ (New York: Association for Computing Machinery, 2010), 375–83.\n  4. Ben Fry, _Visualizing Data: Exploring and Explaining Data with the Processing Environment_ (Sebastopol, CA: O’Reilly Media, Inc., 2008).\n  5. Giorgia Lupi, “Data Humanism: The Revolutionary Future of Data Visualization,” _Printmag_ , January 30, 2017.\n  6. Chris McDowall and Tim Denee, _We Are Here: An Atlas of Aotearoa_ (Auckland, NZ: Massey University Press, 2019).\n  7. Scott Murray, _Creative Coding and Data Visualization with p5.js: Drawing on the Web with JavaScript_ (Sebastopol, CA: O’Reilly Media, Inc., 2017).\n  8. Gina Neff and Dawn Nafus, _Self-Tracking_ (Cambridge, MA: MIT Press, 2016).\n  9. Maureen O’Connor, “Heartbreak and the Quantified Selfie,” _NY Magazine_ , December 2, 2013.\n  10. Brooke Singer, “A Chronology of Tactics: Art Tackles Big Data and the Environment,” _Big Data and Society_ 3, no. 2 (December 2016).\n  11. Edward R. Tufte, _The Visual Display of Quantitative Information_ (Cheshire, CT: Graphics Press, 2001).\n  12. Jacoba Urist, “From Paint to Pixels,” _The Atlantic_ , May 14, 2015.\n\n\n# Augmented Projection\n\nIlluminated interventions\n\n![c10-fig-0061.jpg](../images/c10-fig-0061.jpg)\n\n61\\. Krzysztof Wodiczko's _Warsaw Projection_ (2005) augmented the façade of\nthe Zachęta National Gallery to focus on the unequal social status of women.\nVideos of women supporting an entablature inscribed _Artibus_ (“for the arts”)\nwere presented as architectural caryatids. (Image © Krzysztof Wodiczko,\ncourtesy Galerie Lelong & Co., New York.)\n\n## Brief\n\nDesign a projected decal for a physical place or object. Your response could\nbe a time-based display of graphics or video specifically composed to\nilluminate something _other_ than a blank projection screen. Your imagery\nmight relate to (and elicit new meanings from) otherwise banal architectural\nfeatures in a wall, such as a power outlet, doorknob, water spigot, elevator\nbuttons, or window frame. You might design a projection for a specific object,\nor, if you are able, for a dynamic site such as a vehicle or a performer's\nbody.\n\nSketch some poetic or playful concepts for imagery that relates to the\ngeometric, structural, historical, or political features of your site. Create\nyour concept in code, and project your imagery onto your site or object,\ntaking special care to document it. If your design requires precise alignment\nbetween virtual and physical spaces, you'll need a tool for projector\ncalibration and keystone correction. Consider libraries like Keystone (for\nProcessing), ofxWarp (for openFrameworks), Cinder-Warping (for Cinder), or\nsoftware like Millumin or TouchDesigner.\n\n## Learning Objectives\n\n  * Review techniques for working with projectors\n  * Employ code and light to create relationships between real and virtual, physical and digital\n  * Design and realize a site-specific artistic concept or experiment\n\n## Variations\n\n  * Reinterpret a classic arcade game such as Pong, Snake, Qix, or Pac-Man so that it uses a specific wall as a playing field. Incorporate the wall's windows and other features into the game as obstacles.\n  * Create an audiovisual presentation in which features of a real environment are activated by colored projections, synchronized to music.\n  * Create an “in-situ data visualization” that projects information about a site onto that site. This could take the form of a superimposed “x-ray,” datagraph, political commentary, etc.\n  * Use a physics library, such as Box2D, to orchestrate realistic-looking “collisions” between your projected graphics and the actual physical features of your site.\n  * Give consideration to scale. Small is OK.\n  * Use computer vision to assist with feature detection, projector calibration, and alignment.\n  * Create an ecosystem of virtual creatures using simulation principles such as flocking. Have the creatures respond to the features of your chosen wall.\n\n## Making It Meaningful\n\nA wide range of new meanings awaits when an illuminated virtual layer is\nsuperimposed onto the physical world. Projection can operate as a commentary\nor a critique, an intervention to probe the historical or political dimensions\nof a building or monument. In other uses, the projection is a site-specific\ninformation visualization, revealing the internal structures of places or\nobjects, the activities that occur in relation to them, or the resources\nrequired in their operation. In performance contexts, projections have created\nnew opportunities for responsive set designs, shadow-play, and choreographies\nof “digital costumes”: responsive, projected displays that are tightly coupled\nto a performer's body and movements. Augmented projections range from\nspectacular public illusions that make buildings appear to shimmer, to\nintimate poetic gestures at the scale of the human face.\n\nThe key challenge is to create a strongly motivated relationship between real\nand virtual: between the projected light and the specific person, place, or\nthing onto which it is projected. Exemplary works accomplish this through a\ncombination of both formal and conceptual engagement with the site. Choices\nabout what to project on, and what to project on it, are made simultaneously.\n\n![c10-fig-0062.jpg](../images/c10-fig-0062.jpg)\n\n62\\. In Michael Naimark's _Displacements_ (1980), a slowly rotating motion\npicture of a fabricated (and inhabited) living room is projected back onto the\nselfsame objects, which have been painted white.\n\n![c10-fig-0063.jpg](../images/c10-fig-0063.jpg)\n\n63\\. In _Sunset Solitaire_ (2007), Joe McKay performs on custom software to\ncreate a projection that continually “matches” the sunset.\n\n![c10-fig-0064.jpg](../images/c10-fig-0064.jpg)\n\n64\\. _Nuage Vert_ (2008) by HeHe (Helen Evans and Heiko Hansen) visualizes a\npower station's energy consumption with a projected laser outline.\n\n![c10-fig-0065.jpg](../images/c10-fig-0065.jpg)\n\n65\\. In _Scenic Jogging_ (2010), Jillian Mayer enters a landscape projected\nfrom a moving vehicle.\n\n![c10-fig-0066.jpg](../images/c10-fig-0066.jpg)\n\n66\\. _Apparition_ (2004), a dance performance by Klaus Obermaier and the Ars\nElectronica Futurelab, uses computer vision to project imagery onto Desiree\nKongerod and Robert Tannion.\n\n![c10-fig-0067.jpg](../images/c10-fig-0067.jpg)\n\n67\\. _Keyfleas_ (2013) was an interactive projection developed by a first-year\nundergraduate, Miles Hiroo Peyton. A flock of small creatures appear to\ninhabit the surface of a computer keyboard, swarming to investigate keys as\nthey are pressed.\n\n![c10-fig-0068.jpg](../images/c10-fig-0068.jpg)\n\n68\\. In Pablo Valbuena's influential _Augmented Sculpture_ (2006–2007), a\ncollection of simple rectangular volumes form the physical base for a virtual\nprojection, which appears to transform its underlying geometry.\n\n![c10-fig-0069.jpg](../images/c10-fig-0069.jpg)\n\n69\\. _Delicate Boundaries_ (2006) by Christine Sugrue presents an interaction\nin which projected organisms “leave the screen” and climb onto a viewer's\noutstretched arm, detected by an overhead camera.\n\n![c10-fig-0070.jpg](../images/c10-fig-0070.jpg)\n\n70\\. In Karolina Sobecka's _Wildlife_ (2006), a running tiger is projected\nfrom a moving car onto urban surfaces. The tiger's speed is proportional to\nthe speed of the car's wheels, as determined by a sensor; when the car stops,\nthe tiger stops also.\n\n## Additional Projects\n\n  * Emily Andersen et al., _The Illuminator_ , 2012–2020, site-specific projection.\n  * AntiVJ (Romain Tardy and Thomas Vaquié), _O (Omicron)_ , 2012, sound and site-specific projection, Hala Stulecia, Wrocław, Poland.\n  * Chris Baker, _Architectural Integration Tests_ , 2009, online video.\n  * Toni Dove, _Mesmer: Secrets of the Human Frame_ , 1990, computationally driven slide installation.\n  * Eric Forman, _Perceptio Lucis_ , 2009, video projector, software, and painted wood form, New York.\n  * Benjamin Gaulon and Arjan Westerdiep, _DePong_ , 2003, projected game, Groningen, Netherlands.\n  * GMUNK (Bradley Munkowitz), _BOX_ , September 2013, video, 5:15.\n  * Michael Guidetti, _Bounce Room_ , 2009, watercolor on canvas with animated digital projection, Jancar Jones Gallery, Los Angeles.\n  * Andreas Gysin and Sidi Vanetti, _Casse_ , 2006, projection, sound, and speakers.\n  * YesYesNo, _Night Lights_ , 2011, interactive light projection, Auckland Ferry Terminal, Auckland.\n\n## Readings\n\n  1. Gerardus Blokdyk, _Projection Mapping: A Complete Guide_ (n.p.: 5STARCooks, 2018).\n  2. Justin Cone, “Building Projection Roundup,” [Motionographer.com](http://Motionographer.com), July 24, 2009.\n  3. Donato Maniello, _Augmented Reality in Public Spaces: Basic Techniques for Video Mapping_ (n.p.: Le Penseur, 2015).\n  4. Ali Momeni and Stephanie Sherman, _Manual for Urban Projection_ (self-pub., Center for Urban Intervention Research, 2014).\n  5. Francesco Murano, _Light Works: Experimental Projection Mapping_ (Rome: Aracne Publishing, 2014).\n  6. Studio Moniker, _The Designer's Guide to Overprojection_ (projection on posters, presented at Typojanchi: 3rd International Typography Bienniale, Seoul, South Korea, August 30–October 11, 2013).\n\n\n# One-Button Game\n\nDesigning within tight constraints\n\n![c11-fig-0071.jpg](../images/c11-fig-0071.jpg)\n\n71\\. The popular mobile game _Flappy Bird_ was released by Dong Nguyen in\n2013. Tapping the screen boosts the flight of a small bird, keeping it aloft\nand helping it avoid obstacles as they scroll by.\n\n## Brief\n\nYour challenge is to make a game whose interface is limited to a single\nbutton.\n\nA button has two states: pressed and released. How can you design a core game\nmechanic solely using the changes between these states? These actions could\ncontrol movement (running, jumping, flying), an action (attacking,\ntransforming), or a change in the environment (gravity, weather, friction).\nFor multiplayer games, your system may use one button per player. You are\nencouraged to think beyond making a “runner” game—the most common one-button\ngame mechanic, in which the player's avatar jumps over pits and obstacles.\n\n## Learning Objectives\n\n  * Design and realize a game within tight design constraints\n  * Discuss, differentiate, and appraise the range of game mechanics possible with a single binary input\n  * Develop an aesthetic or narrative treatment that supports the play experience\n\n## Variations\n\n  * Keyboard emulators like the Makey-Makey allow the rapid construction of whimsical button controllers from household materials like bananas and Play-Doh. Using a keyboard emulator, construct a custom game controller that is part of a piece of clothing, furniture, or architecture.\n  * Convert a classic arcade game into a one-button game by “automating” (eliminating) interactions that are customarily under control of the player. For example, _Space Invaders_ uses two sets of controls: one to move the player's laser cannon back and forth, and another to fire at descending aliens. It can be changed into a one-button game by making the cannon move back and forth on an automatic schedule.\n  * Modify the problem scope so that your game uses continuously valued data from a single sensor, such as a slider, knob, photoresistor, or microphone.\n\n## Making It Meaningful\n\nThe one-button game belongs to a classic category of games wherein user\ninteraction is limited to a single binary input. As the popularity of _Tiny\nWings_ and _Flappy Bird_ make evident, this category remains more relevant\nthan ever, particularly in the context of small mobile devices. Yet today's\nbountiful computational resources tend to encourage a focus on sumptuous\naudiovisuals, making it easy to overlook how strict design constraints can\ncounterintuitively produce engrossing experiences. As Andy Nealen, Adam\nSaltsman, and Eddy Boxerman have observed, much can be done with simple\ninputs, narrow decision spaces, and minimal graphics, as they help focus a\ndesigner on “the most relevant rules, mechanics and representations of a\nsystem, while still providing for an intractably large possibility space.” i\n\nDespite its minimal affordances, a single button can allow a surprisingly wide\nrange of expressive interactions—and therefore, game design strategies—through\nthe manipulation of timing. The duration of a button press, for example, can\nbe used to regulate the amount of “energy” applied to a virtual game object\n(such as charging a battery or stretching a slingshot). A game mechanic may\noperate by counting how often a player presses the button within a unit of\ntime (i.e., taps per second); measuring the precision of the player's rhythmic\nsensibility (i.e., how accurately they can achieve a pulse of periodic taps);\nor quantifying the player's feel for timing (whether their button taps are\nearly or late, relative to another game event). Sequences of long and short\nbutton presses can even be used to communicate text through Morse code.\n\nThe provocative potential of the one-button game is unleashed when the\ncontroller is placed deliberately in the world and interpreted in new physical\nforms. As designers like Kaho Abe, Kurt Bieg, and Ramsey Nasser show, when\nattached to different parts of the body, novel multiplayer interactions can be\nchoreographed and agilities tested. Taken together, these strategies outline\nways in which a designer can savor tight constraints to make compelling and\nnovel game play.\n\n![c11-fig-0072.jpg](../images/c11-fig-0072.jpg)\n\n72\\. Artist Rafaël Rozendaal, co-creator of _Finger Battle_ (2011), writes:\n“The game is very simple: two players, tap as fast as you can, the fastest\ntapper wins.” Each player is restricted to tapping in their own zone (blue or\nred). If a player taps faster than their opponent, their zone grows in\nsize—making it easier to tap the screen, and accelerating the game to its\nconclusion.\n\n![c11-fig-0073.jpg](../images/c11-fig-0073.jpg)\n\n73\\. In Major Bueno's _Moon Waltz_ (2016), a single-button side-scroller, the\nplayer does not directly control the main character. Instead, the game's\nbutton performs a narrative function in a chain of cause and effect: pressing\nthe button parts the clouds, which reveals the moon, which causes the main\ncharacter to transform into a werewolf, enabling new modes of attack.\n\n![c11-fig-0074.jpg](../images/c11-fig-0074.jpg)\n\n74\\. Jonathan Rubock's _One Button Nipple Golf_ (2016) employs a tap-and-hold\ninteraction and partial automation to control both the orientation and\nstrength of the player's putt. Before the putt, a rotating indicator\ncontinually orbits around the tee; the player determines the orientation of\ntheir putt from this indicator by deciding when to press the button. The\nstrength of the player's putt is then regulated by how long they hold the\nbutton down. The golf course is a landscape of human torsos, in which nipples\nare the golf holes.\n\n![c11-fig-0075.jpg](../images/c11-fig-0075.jpg)\n\n75\\. Kaho Abe's _Hit Me!_ (2011) is a physical, screenless game in which each\nplayer wears a button on their head. The objective is to tap your opponent's\nbutton before they tap yours.\n\n![c11-fig-0076.jpg](../images/c11-fig-0076.jpg)\n\n76\\. Moving the button to a different location on the body, Kurt Bieg and\nRamsey Nasser extend Abe's core game mechanic with richly suggestive play. In\n_Sword Fight_ (2012), each player sports an Atari-style joystick strapped to\ntheir groin, with which they attempt to strike their opponent's action button.\nAwkward hilarity ensues.\n\n## Additional Projects\n\n  * Kaho Abe and Ramsey Nasser, _Shake it Up!_ , 2013, two-player physical game.\n  * Atari, _Steeplechase_ , 1975, arcade game.\n  * Stéphane Bura, _War and Peace,_ 2010, online game.\n  * Peter Calver (Supersoft), _Blitz,_ 1979, video game for Commodore.\n  * Bill Gates and Neil Konzen, _DONKEY.BAS_ , 1981, video game distributed with the original IBM PC.\n  * Andreas Illiger, _Tiny Wings_ , 2011, mobile game.\n  * Kokoromi Collective, _Gamma IV Showcase: One Button Games_ , 2009, one-button game competition website.\n  * Konami, _Badlands,_ 1984, laserdisc cowboy-themed shooter game.\n  * Paolo Pedercini (Molleindustria), _Flabby Physics,_ 2010, online game.\n  * Adam Saltsman (Atomic), _Canabalt,_ 2009, video game.\n  * SMG Studio, _One More Line,_ 2015, online and mobile game.\n  * Phillipp Stollenmayer, _Zip-Zap_ , 2016, mobile game.\n\n## Readings\n\n  1. Barrie Ellis, “Physical Barriers in Video Games,” [OneSwitch.org.uk](http://OneSwitch.org.uk)., accessed April 14, 2020.\n  2. Berbank Green, “One Button Games,” [Gamasutra.com](http://Gamasutra.com), June 2, 2005.\n  3. Paolo Pedercini, syllabi for Experimental Game Design (CMU School of Art, Fall 2010–2020).\n  4. Paolo Pedercini, “Two Hundred Fifty Things a Game Designer Should Know,” [Molleindustria.org](http://Molleindustria.org), accessed July 20, 2020.\n  5. George S. Greene, “Boys Can Have a Carnival of Fun with This Simply Built High Striker,” _Popular Science_ (September 1933): 59–60.\n  6. Katie Salen and Eric Zimmerman, _Rules of Play: Game Design Fundamentals_ (Cambridge, MA: MIT Press, 2005).\n\n## Notes\n\ni Andy Nealen, Adam Saltsman, and Eddy Boxerman, “Towards Minimalist Game\nDesign,” in _Proceedings of the 6th International Conference on Foundations of\nDigital Games_ (ACM Digital Library, 2011), 38–45.\n\n\n# Bot\n\nAutonomous artistic agent\n\n![c12-fig-0077.jpg](../images/c12-fig-0077.jpg)\n\n77\\. Jeff Thompson's _Art Assignment Bot_ (2013) generates creative prompts\nfor thousands of followers.\n\n## Brief\n\nCreate an autonomous software agent, or “bot,” that generates posts to an\nonline social media platform at regular intervals. Your bot might generate\ndialogue, stories, recipes, lies, witty quips, or poems, or it might publish\nwholly non-textual media such as images, sounds, melodies, comic strips, or\nanimated GIFs. Your project may be a publishing platform, intermittently\nsharing content to an audience of subscribers, or it may operate as a social\nactor, interacting directly with other users. It might explore a particular\ntheme, perform a role, communicate an emotional disposition, or promote a\nspecific agenda. Your bot could also work as a filter, by aggregating,\nrepublishing, or reinterpreting content from other sources. Explore the\ndifferent interactions afforded by the API of your chosen platform. The key is\nthat your machine must publicly share its media online, and it must be\norchestrated through code.\n\nSome important ground rules apply. Your program may not perform illegal\nactions. Your program should not spread hate speech (whether purposefully or\naccidentally), harass people, or make threats. It is wise for your bot to\nidentify itself as an automated process. If you wish your program to interact\nwith real individuals, those persons must first “opt- in” by following your\nbot. If you disobey this restriction, your project is likely to be short-\nlived; you risk having your account blocked or terminated _very_ quickly.\n\n## Learning Objectives\n\n  * Design a creative work that uses an API\n  * Integrate computational techniques for content generation (or filtering) in a social media context\n  * Differentiate and appraise the aesthetics of systems for generating public performances\n\n## Variations\n\n  * _For introductory students:_ Start by using an online notification service such as If This Then That (IFTTT) to automate online interactions with minimal coding.\n  * Have your bot reveal something interesting about the API of its online platform.\n  * Create a bot that responds to an online data stream (such as a newspaper) or that presents items pulled from a cultural archive (such as a museum collection). This will likely require you to negotiate another API in addition to that of your bot's social media platform.\n\n## Making It Meaningful\n\nOnline bots interact with humans, serve political agendas, broadcast real-time\ndata, and share artistic, literary, and scientific content. Poetic bot-maker\nAllison Parrish describes her agents as “cute robot explorers” sent to probe\n(and transmit reports about) unknown territories of “semantic space.” i For\ncomputational artists, social media platforms offer the key mechanism by which\na bot's procedurally generated content can be published to willing\nsubscribers. No matter how obscure the bot, there is probably an audience for\nit.\n\nThe use of non-human software agents, once primarily the domain of computer\nscience researchers and artists, is now so prevalent that bots generate a\nmajority of Internet traffic. Search engines use “spider” bots to index the\nweb, while malicious “botnets” conduct coordinated attacks, spread\nmisinformation, sow distrust, and amplify extreme points of view on digital\nplatforms. The ability of chatbots to convincingly simulate conversation, or\npass the “Turing test,” remains fraught, as demonstrated by the great “bot\npurges” of 2014–2018. These purges saw social media platforms suspend\nautomated accounts and increase regulation in response to public concern about\nautomated media manipulation impacting real electoral processes and political\ndiscourse.\n\nFrom ELIZA to Alexa, chatbots continue to learn to mimic human communication\nthrough interactions with their users, reviving questions like: What areas of\nour lives should be automated by software? What does it mean to be human? Bots\nwith AI inspire hope and anxiety, dual sentiments that are palpable in\nfictional depictions like _2001_ 's HAL 9000 and Samantha in the movie _Her._\n\n![c12-fig-0078.jpg](../images/c12-fig-0078.jpg)\n\n78\\. The _Ephemerides_ (2015) Twitter bot by Allison Parrish publishes\npairings of generated poetry and randomly selected NASA images.\n\n![c12-fig-0079.jpg](../images/c12-fig-0079.jpg)\n\n79\\. Everest Pipkin and Loren Schmidt's _Moth Generator_ (2015) is a Twitter\nbot that synthesizes images of imaginary moths, accompanied by generated\nnomenclatures for each specimen. More than 11,000 people have signed up to\nreceive these “lepidoptera automata.”\n\n![c12-fig-0080.jpg](../images/c12-fig-0080.jpg)\n\n80\\. _Reverse OCR_ (2014) by Darius Kazemi is a Tumblr bot that selects a\nrandom word and draws semi-random lines until an OCR library recognizes the\nresulting image as that word.\n\n![c12-fig-0081.jpg](../images/c12-fig-0081.jpg)\n\n81\\. The _Random Darknet Shopper_ (2014) by !Mediengruppe Bitnik is an\nautomated online shopping bot that spends an allowance of $100 in bitcoins per\nweek on purchases from the Darknet.\n\n![c12-fig-0082.jpg](../images/c12-fig-0082.jpg)\n\n82\\. _CSPAN 5_ (2015) by Sam Lavigne is a YouTube bot that automatically edits\nvideos from the C-SPAN channel, reducing them to their most frequent words and\nphrases.\n\n## Additional Projects\n\n  * Anonymous, _Congress Edits_ , 2014, Twitter bot.\n  * American Artist, _Sandy Speaks_ , 2017, AI chat platform.\n  * Ranjit Bhatnagar, _Pentametron_ , 2012, Twitter bot.\n  * Tega Brain, _Post the Met_ , 2014, Craigslist bot.\n  * James Bridle, _Dronestagram_ , 2012, social media bots.\n  * George Buckenham, _Cheap Bots, Done Quick!_ , 2016, bot-making tool.\n  * Kate Compton, _Tracery_ , 2015, generative text-authoring tool.\n  * Voldemars Dudums, _Hungry Birds_ , 2011, Twitter bot.\n  * Constant Dullaart, _attention.rip_ , 2017, Instagram bot.\n  * shawné michaelain holloway, _*~ FAUNE ~* : EDIT FLESH.PNG_ , 2013, Tumblr bot.\n  * Surya Mattu, _NY Post Poetics_ , 2015, Twitter bot.\n  * Kyle McDonald, _KeyTweeter_ , 2010, Twitter bot.\n  * Ramsey Nasser, _Top Gun Bot_ , 2014, Twitter bot.\n  * Pall Thayer, _I Am Still Alive_ , 2009, Twitter bot.\n  * Thricedotted, _How 2 Sext_ , 2014, Twitter bot.\n  * Jia Zhang, _CensusAmericans_ , 2015, Twitter bot.\n\n## Readings\n\n  1. danah boyd, “What Is the Value of a Bot?,” _Points_ (blog), [datasociety.net](http://datasociety.net), February 25, 2016.\n  2. Michael Cook, “A Brief History of the Future of Twitter Bots,” [GamesbyAngelina.org](http://GamesbyAngelina.org), updated January 13, 2015.\n  3. Madeleine Elish, “On Paying Attention: How to Think about Bots as Social Actors,” _Points_ (blog), [datasociety.net](http://datasociety.net), February 25, 2016.\n  4. Lainna Fader, “A Brief Survey of Journalistic Twitter Bot Projects,” _Points_ (blog), [datasociety.net](http://datasociety.net), February 26, 2016.\n  5. Jad Krulwich and Robert Krulwich, “Talking to Machines,” _Radiolab_ (podcast), May 30, 2011.\n  6. Rhett Jones, “The 10 Best Twitter Bots That Are Also Net Art,” _Animal_ , January 9, 2015.\n  7. Darius Kazemi, “The Bot Scare,” _Notes_ (blog), [tinysubversions.com](http://tinysubversions.com), December 31, 2019.\n  8. Rachael Graham Lussos, “Twitter Bots as Digital Writing Assignments,” _Kairos: A Journal of Rhetoric, Technology, and Pedagogy_ 22, no. 2 (Spring 2018), n.p.\n  9. Allison Parrish, “Bots: A Definition and Some Historical Threads,” _Points_ (blog), [datasociety.net](http://datasociety.net), February 24, 2016.\n  10. James Pennebaker, “The Secret Life of Pronouns,” filmed February 2013 in Austin, TX, TED video, 17:58.\n  11. Elizaveta Pritychenko, _Twitter Bot Encyclopedia_ (self-pub., Post-Digital Publishing Archive, 2014).\n  12. Mark Sample, “A Protest Bot Is a Bot So Specific You Can't Mistake It for Bullshit: A Call for Bots of Conviction,” [Medium.com](http://Medium.com), updated May 30, 2014.\n  13. Saiph Savage, “Activist Bots: Helpful but Missing Human Love?,” _Points_ (blog), [datasociety.net](http://datasociety.net), November 29, 2015.\n  14. Jer Thorp, “Art and the API,” _blprnt.blg_ (blog), [blrpnt.com](http://blrpnt.com), August 6, 2013.\n  15. Samuel Woolley et al., “How to Think about Bots,” _Points_ (blog), [datasociety.net](http://datasociety.net), February 24, 2016.\n\n## Notes\n\ni Allison Parrish, “Exploring (Semantic) Space with (Literal) Robots”\n(lecture, Eyeo Festival, Minneapolis, MN, June 2015).\n\n\n# Collective Memory\n\nCreative crowdsourcing\n\n![c13-fig-0083.jpg](../images/c13-fig-0083.jpg)\n\n83\\. In _Exhausting a Crowd_ (2015), Kyle McDonald invites online audiences to\nannotate a 12-hour video recording of a public place with their own\nobservations of the scene. Inspired by Georges Perec's 1974 experimental\nliterary work _An Attempt at Exhausting a Place in Paris_ , McDonald opens up\na process of close observation to a broad public, crowdsourcing the discovery\nand preservation of moments that might otherwise go unnoticed.\n\n## Brief\n\nCreate an online, open system that invites visitors to collaborate on or\ncontribute to a collectively produced media object. Your project should enable\nits participants to make changes that persist over time for others to\nexperience (and potentially, modify). The result should be a dynamically\nevolving visual, textual, sonic, or physical artifact that develops from a\nnovel interaction between friends, siblings, collaborators, neighbors, or\nstrangers. Carefully consider the kinds of actions or authorship you hope to\nelicit, and how the interaction design of your system influences individual\n(and hence collective) behavior. Paradoxically, the tightest constraints often\nproduce the most interesting results. Can you create the conditions for\nunexpected emergent behaviors to arise?\n\nThe problem of “bootstrapping” sometimes arises in crowdsourced endeavors\nwhere it can be challenging to attract the first wave of participants,\nparticularly if the system does not become interesting until there is\nsignificant participation. Does your project need an enlistment strategy?\n\n## Learning Objectives\n\n  * Differentiate and appraise the aesthetics, design, and concept of systems that support collaboration and emergent creative behavior\n  * Design an interface, balancing constraints and incentives for participation\n  * Implement fundamental data structures\n\n## Variations\n\n  * Consider how (or whether) your system scales. Would your project survive “going viral”?\n  * Implement a form of automated “weathering,” in which your system gradually organizes, alters, or prunes its participants’ contributions over time.\n  * Your project may attract trolls, bigots, spammers, bots, vandals, and other visitors acting in bad faith. Be prepared to consider the problem of content moderation (automated or otherwise). Are your users anonymous, or are they somehow accountable?\n\n## Making It Meaningful\n\nFrom wasp nests to Wikipedia, principles of emergence explain how the\ncumulative contributions of thousands of independent agents can produce\nsophisticated forms. On the Internet, the art of orchestrating large groups of\npeople is called “crowdsourcing,” and the technologies for such information\nhusbandry offer rich opportunities to explore cybernetic concepts like\nfeedback, autopoiesis, and the nuances of collective behavior. The simplest\ncrowdsourcing rulesets can yield a startling glimpse of the consciousness of\nthe global hive-mind, as in Kevan Davis's _Typophile: The Smaller Picture_\n(2002), or the epic Place experiment on Reddit (2017). It should surprise no\none that crowdsourcing may also channel our darkest impulses, as happened with\nMicrosoft's Tay, an AI chatbot that developed offensive speech patterns\nthrough conversations with the crowd.\n\nCollectively created artifacts vary considerably. At one extreme are\nphenomena, like desire paths or the Great Pacific Garbage Patch, that arise as\nthe inadvertent by-products of myriad human actions. “Memory quilts” and\ntraditional folk songs thread together deliberate contributions from many\nindividuals, but are typically structured according to fixed, widely\nunderstood cultural idioms (grids, rhyme schemes, etc.). Other collectively\nauthored artifacts aggregate independent contributions through an undirected\nprocess of accumulation; examples of such works include cairns, gum walls,\n“love padlock” bridges, Yayoi Kusama's _Obliteration Room_ installation, and,\non the Internet, graffiti walls like Drawball and SwarmSketch.\n\nInternet artists have enlisted online participants to collaboratively tend\ngardens, write poetry, produce drawings, interpret imagery, annotate video,\nand contribute to historical archives. In the genre of the crowdsourced meta-\nartwork, the creator defines browser-based interactions that scope a user's\ncreative influence—and then distributes artistic agency to a large, open, and\noften rapidly evolving group. Success depends on carefully balancing\nconstraints (such as limiting available “ink,” or whether or not users are\npermitted to alter another's contribution) and incentives (such as the\nopportunity to engage creatively with a favorite song, or participate in a\nnovel creative activity or political action).\n\n![c13-fig-0084.jpg](../images/c13-fig-0084.jpg)\n\n84\\. A _cairn_ is a human-made pile of stones, often accreted over the course\nof centuries. Since prehistoric times, these collaboratively produced\nstructures have served as landmarks, trail markers, and memorials.\n\n![c13-fig-0085.jpg](../images/c13-fig-0085.jpg)\n\n85\\. _Telegarden_ (1995) by Ken Goldberg and Joseph Santarromana is an\ninteractive networked installation consisting of a garden and a robotic arm.\nOnline visitors view the garden via a webcam, and can plant, water, and tend\nseedlings by controlling the arm.\n\n![c13-fig-0086.jpg](../images/c13-fig-0086.jpg)\n\n86\\. Roopa Vasudevan's _Sluts across America_ (2012) is a compilation of user-\nsubmitted messages advocating for reproductive freedoms, geolocated and\ndisplayed on a U.S. map.\n\n![c13-fig-0087.jpg](../images/c13-fig-0087.jpg)\n\n87\\. In _Dead Drops_ (2010), Aram Bartholl creates an offline file-sharing\nservice by covertly mounting USB storage devices throughout a city. If a\npasserby connects their computer to one of the USB drives, they can download\nwhatever information prior visitors have contributed, or upload new files of\ntheir own.\n\n![c13-fig-0088.jpg](../images/c13-fig-0088.jpg)\n\n88\\. Studio Moniker's _Do Not Touch_ (2013) is a crowdsourced music video.\nVisitors to an interactive web application listen to a song, during which they\nreceive simple instructions on how to move their mouse cursor (“Catch the\ngreen dot”). Thus orchestrated, these cursor movements are recorded and then\nplayed back in sync with the song.\n\n![c13-fig-0089.jpg](../images/c13-fig-0089.jpg)\n\n89\\. _Typophile: The Smaller Picture_ (2002) is a website by Kevan Davis\nwherein visitors are prompted to contribute to the design of a letterform,\nsuch as the letter ’A’. Each visitor's creative options are tightly\nrestricted: their only choice is to decide whether a given pixel, in a 20x20\ngrid, should be black or white. The letterform's grid is initialized with\nrandom noise. Gradually, as members of the crowd contribute decisions about\nindividual pixels, a collectively designed typeface is produced.\n\n![c13-fig-0090.jpg](../images/c13-fig-0090.jpg)\n\n90\\. _Place_ (shown here in detail) was a 1000x1000 pixel collaborative canvas\nhosted on Reddit. During a 72-hour period in April 2017, registered users\ncould edit the canvas by selecting the color of a single pixel from a 16-color\npalette. Because each user could only alter one pixel every five minutes,\ncrowds developed elaborate methods to collectively draw images, write\nmessages, create flags, and overwrite the contributions of others.\n\n![c13-fig-0091.jpg](../images/c13-fig-0091.jpg)\n\n91\\. For the _Iyapo Repository_ (2015), Salome Asega and Ayodamola Okunseinde\ncrowdsource speculations on the future from people of African descent. The\nartists take the resultant blueprints for cultural technological artifacts,\nrealize them as objects, and acquisition them into the collection.\n\n## Additional Projects\n\n  * Olivier Auber, _Poietic Generator_ , 1986, contemplative social network game.\n  * Andrew Badr, _Your World of Text_ , 2009, collaborative online text space.\n  * Douglas Davis, _The World's First Collaborative Sentence_ , 1994, collaborative online text.\n  * Peter Edmunds, _SwarmSketch_ , 2005, collaborative online digital canvas.\n  * Lee Felsenstein, Mark Szpakowski, and Efrem Lipkin, _Community Memory_ , 1973, public computerized bulletin board system.\n  * Miranda July and Harrell Fletcher, _Learning to Love You More_ , 2002–2009, assignments and crowdsourced responses.\n  * Agnieszka Kurant, _Post-Fordite 2_ , 2019, fossilized enamel paint sourced from shuttered car manufacturing plants.\n  * Yayoi Kusama, _The Obliteration Room_ , 2002, interactive installation.\n  * Mark Napier, _net.flag_ , 2002, online app for flag design.\n  * Yoko Ono, _Wish Tree_ , 1996, interactive installation.\n  * Evan Roth, _White Glove Tracking_ , 2007, crowdsourced data collection.\n  * Jirō Yoshihara, _Please Draw Freely_ , 1956, paint and marker on wood, outdoor Gutai Art Exhibition, Ashiya, Japan.\n\n## Readings\n\n  1. Paul Ryan Hiebert, “Crowdsourced Art: When the Masses Play Nice,” _Flavorwire_ , April 23, 2010.\n  2. Kevin Kelly, “Hive Mind,” in _Out of Control: The New Biology of Machines, Social Systems, & the Economic World_ (New York: Basic Books, 1995).\n  3. Ioana Literat, “The Work of Art in the Age of Mediated Participation: Crowdsourced Art and Collective Creativity,” _International Journal of Communication_ 6 (2012): 2962–2984.\n  4. Dan Lockton, Delanie Ricketts, Shruti Chowdhury, and Chang Hee Lee, “Exploring Qualitative Displays and Interfaces” (paper presented at CHI ’17: CHI Conference on Human Factors in Computing Systems, Denver, CO, May 2017).\n  5. Trent Morse, “All Together Now: Artists and Crowdsourcing,” _ARTnews_ , September 2, 2014.\n  6. Manuela Naveau, _Crowd and Art: Kunst und Partizipation im Internet,_ Image 107 (Bielefeld, Germany: Transcript Verlag, 2017).\n  7. Howard Rheingold, _Smart Mobs: The Next Social Revolution_ (New York: Basic Books, 2002).\n  8. Clay Shirky, “Wikipedia – An Unplanned Miracle,” _The Guardian_ , January 14, 2011.\n  9. Carol Strickland, “Crowdsourcing: The Art of a Crowd,” _Christian Science Monitor_ , January 14, 2011.\n  10. “When Pixels Collide,” [sudoscript.com](http://sudoscript.com), 4 April 2017.\n\n\n# Experimental Chat\n\nInterrogating “togetherness”\n\n![c14-fig-0092.jpg](../images/c14-fig-0092.jpg)\n\n92\\. _Hole in Space_ (1980), a “communication sculpture” by Kit Galloway and\nSherrie Rabinowitz, used live two-way video to connect street-level\npedestrians in Los Angeles and New York City.\n\n## Brief\n\nDesign a multi-user environment that allows people in different locations to\ncommunicate with each other in a new way. Your system could facilitate\nlanguage-based interactions like typing, speaking, or reading. Alternatively,\nit could convey non-verbal aspects of presence, such as gestures or breathing,\nto explore what Heidegger calls _Dasein_ , or “being there together.”\nCarefully consider the agency of participants in your system, and the timing\nand directionality of their messages. Are the users passive observers,\nlistening to the murmurings of a crowd, or are they contributors to a grand\nconversation? Is communication asynchronous, wherein a user's traces are\nencountered by others later? Or is it synchronous, allowing for simultaneous\nparticipation in a live event? Is your system intended for one-to-one, one-to-\nmany, many-to-one, or many-to-many?\n\n## Learning Objectives\n\n  * Differentiate, discuss, and appraise modalities of verbal and non-verbal communication\n  * Use a server-client model and a library for real-time networked communication\n  * Develop, design, and realize a concept for a social space\n\n## Variations\n\n  * Think beyond your laptop: consider the location(s) where your chat system can be situated, the typical activities people do there, and the relationships they may have. Is your system in a bus station? An orchestra pit? A bedroom? A helmet? Are your participants mutual strangers? Collaborators? Lovers? Parents and their infants?\n  * Create an environment in which networked users “feel” each other's presence in a novel way.i\n  * Explore the senses. How might your users communicate with colors? Vibrations? Odors?\n  * Consider asymmetries of time, scale, agency, or ability. Perhaps one user is limited to communicating with a single fingertip, while the other must use their entire body.\n  * Create a multiuser environment in which one of the “users” is a crowd (such as a Mechanical Turk workgroup), or an artificial intelligence, such as an ELIZA chatbot, a translation service, Apple Siri, or an automated online assistant.\n  * Develop a MUD (Multi-User Dungeon), a text-based, multiplayer virtual world. Design a fixed lexicon and syntax of commands for your players, and a task or problem that your players must solve using this vocabulary.\n  * Use your chat system in a live public performance that incorporates scripted or improvised contributions from both local and remote participants.\n\n## Making It Meaningful\n\nElectronic networks provide powerful ways of connecting people who are\nphysically separate. The creation of communication tools (and virtual social\nspaces) has characterized networked computation since its inception, beginning\nwith the CTSS instant messaging feature (1961), AUTODIN email service (1962),\nDouglas Engelbart's collaborative real-time text editor (1968), the Talkomatic\nconferencing system (1973), and multiplayer games like _Colossal Cave\nAdventure_ (1975).\n\nComputational artists and designers have aimed to expand telecommunication\nbeyond the exchange of character symbols into an experience that employs the\nfull sensory and expressive capabilities of our bodies. Myron Krueger's\n_Videoplace_ (1974–1990) and Kit Galloway and Sherrie Rabinowitz's _Hole in\nSpace_ (1980) were early systems that took different approaches to full-body\ntelepresence. Today's VR and teledildonics are obvious (and increasingly\ncommercial) extensions of this.\n\nCounterintuitively, some of the most compelling communication systems use\nstrictly limited symbol-sets or rigid constraints on bandwidth. (Consider the\ntelephone, or micro-blogging tools like Twitter.) In the absence of high-\nresolution sensory information, our imaginations fill in the rest.\n\n![c14-fig-0093.jpg](../images/c14-fig-0093.jpg)\n\n93\\. In _The Trace_ (1995) by Rafael Lozano-Hemmer, a participant encounters\nthe moving, ghostlike, real-time “presence” of another person, represented by\nthe glowing intersection of a pair of light-beams. The other person is located\nin an identical but separate room—and perceives the first participant in the\nsame way.\n\n![c14-fig-0094.jpg](../images/c14-fig-0094.jpg)\n\n94\\. _the space between us_ (2015) by David Horvitz is a mobile app that\nconnects two people's phones. Once connected, the app displays the distance to\nand direction of the other person.\n\n![c14-fig-0095.jpg](../images/c14-fig-0095.jpg)\n\n95\\. Scott Snibbe's _Motion Phone_ (1995) is a collaborative, multiplayer\ndrawing environment in which participants draw animated forms onto a shared\ncanvas.\n\n![c14-fig-0096.jpg](../images/c14-fig-0096.jpg)\n\n96\\. _Poop Chat Pro_ (2016) by Maddy Varner is both a real-time chat space and\nan asynchronous graffiti wall, in which messages are deposited as quivering\nanimations for others to find later.\n\n![c14-fig-0097.jpg](../images/c14-fig-0097.jpg)\n\n97\\. _Meatspace_ (2013) by Jen Fong-Adwent and Soledad Penadés is an ephemeral\nconversation space in which participants’ written messages are accompanied by\nanimated GIF loops, instantaneously captured from their webcams.\n\n![c14-fig-0098.jpg](../images/c14-fig-0098.jpg)\n\n98\\. The _Online Museum of Multiplayer Art_ (2020), a virtual wing of Paolo\nPedercini's LIKELIKE neo-arcade, features “playful environments that\ninterrogate our notions of mediated sociality and digital embodiment.” Among\nthe museum's many conceptually oriented chat installations is the “Censorship\nRoom” in which “each word can only be uttered once and never again.”\n\n![c14-fig-0099.jpg](../images/c14-fig-0099.jpg)\n\n99\\. American Artist's installation _Sandy Speaks_ (2017) explores the\npossibilities of chat from beyond the grave. Using a custom-trained AI\nchatbot, this project places a visitor in conversation with the real words of\nSandra Bland, who discussed racism and police brutality in an extensive series\nof YouTube videos just weeks before her own death in police custody.\n\n## Additional Projects\n\n  * Olivier Auber, _Poietic Generator_ , 1986, contemplative social network game.\n  * Wafaa Bilal, _Domestic Tension_ , 2007, interactive video installation.\n  * Black Socialists of America, _BSA's Clapback Chest_ , 2019, custom search engine.\n  * Tega Brain and Sam Lavigne, _Smell Dating_ , 2016, smell-based dating service.\n  * Jonah Brucker-Cohen, _BumpList_ , 2003, email list.\n  * Dries Depoorter and David Surprenant, _Die With Me_ , 2018, mobile chat app.\n  * Dinahmoe, __plink_ , 2011, multiplayer online instrument.\n  * Exonemo, _The Internet Bedroom_ , 2015, online event.\n  * Zach Gage, _Can We Talk?_ , 2011, chat program.\n  * Ken Goldberg and Joseph Santarromana, _Telegarden_ , 1995, collaborative garden with industrial robot arm, Ars Electronica Museum, Linz, Austria.\n  * Max Hawkins, _Call in the Night_ , 2013, phone call service.\n  * Miranda July, _Somebody_ , 2014, messaging service.\n  * Darius Kazemi, _Dolphin Town_ , 2017, dolphin- inspired social network.\n  * Myron Krueger, _Videoplace_ , 1974–1990, multiuser interactive installation.\n  * Sam Lavigne, Joshua Cohen, Adrian Chen, and Alix Rule, _PCKWCK_ , 2015, digital novel written in real time.\n  * John Lewis, _Intralocutor_ , 2006, voice-activated installation.\n  * Jillian Mayer, _The Sleep Site, A Place for Online Dreaming_ , 2013, website.\n  * Lauren McCarthy, _Social Turkers_ , 2013, system for crowdsourced relationship feedback.\n  * Kyle McDonald, _Exhausting a Crowd_ , 2015, video with crowdsourced annotations.\n  * László Moholy-Nagy, _Telephone Picture_ , 1923, porcelain enamel on steel.\n  * Nontsikelelo Mutiti and Julia Novitch, _Braiding Braiding,_ 2015, experimental publishing project.\n  * Jason Rohrer, _Sleep Is Death (Geisterfahrer)_ , 2010, two-player storytelling game.\n  * Paul Sermon, _Telematic Dreaming_ , 1992, live telematic video installation.\n  * Sokpop Collective, _sok-worlds_ , 2020, multiplayer collage.\n  * Tale of Tales, _The Endless Forest_ , 2005, multiplayer online role-playing game.\n  * TenthBit Inc., _ThumbKiss_ (renamed as the _Couple_ app), 2013, mobile messaging app.\n  * Jingwen Zhu, _Real Me_ , 2015, chat app with biometric sensors.\n\n## Readings\n\n  1. Roy Ascott, _Telematic Embrace: Visionary Theories of Art, Technology, and Consciousness_ , ed. Edward A. Shanken (Berkeley: University of California Press, 2007).\n  2. Lauren McCarthy, syllabus for Conversation and Computation (NYU, Spring 2015).\n  3. Joanne McNeil, “Anonymity,” in _Lurking: How a Person Became a User_ (New York: Macmillan, 2020).\n  4. Joana Moll and Andrea Noni, _Critical Interfaces Toolbox_ (2016), [crit.hangar.org](http://crit.hangar.org), accessed July 20, 2020.\n  5. Kris Paulsen, _Here/There: Telepresence, Touch, and Art at the Interface_. (Cambridge, MA: MIT Press, 2017).\n  6. Casey Reas, “Exercises,” syllabus for Interactive Environments (UCLA D|MA, Winter 2004).\n\n## Notes\n\ni This variation is adapted from Paolo Pedercini, “Remote Play / Remote Work,”\nsyllabus for Experimental Game Design (CMU School of Art, Fall 2020).\n\n\n# Browser Extension\n\nLens for the Internet\n\n![c15-fig-0100.jpg](../images/c15-fig-0100.jpg)\n\n100\\. Melanie Hoff's _Decodelia_ (2016) uses principles of color theory to\ntransform the way a web browser renders pages, making their content legible\nonly to those wearing red-tinted glasses.\n\n## Brief\n\nA _browser extension_ is a software add-on that alters the behavior of a web\nbrowser application. It can serve as a jumping-off point for creative\nintervention in the online realm. Extensions can change the appearance of\nspecific online content, add additional information layers, redirect a viewer\nto different URLs, or change browser behaviors. Popular extensions serve to\nblock ads, obscure the user's identity, circumvent censorship, fact-check\npoliticians, and provide dictionary definitions.\n\nIn this assignment, you are asked to design and build a browser extension that\nalters the appearance of (a part of) the Internet, or that augments,\ndefamiliarizes, or estranges a viewer's browsing experience in a poetic or\ncritical way. Publish your extension to a public platform, such as the Chrome\nWeb Store or the Firefox Add-ons site.\n\n## Learning Objectives\n\n  * Review protocols for website structure and display\n  * Identify and creatively experiment with Internet browser functionality and APIs\n  * Develop a creative browser-based intervention\n\n## Variations\n\n  * _For introductory students:_ Begin by modifying website code in the console of the browser. The styling and content of a page can be temporarily edited by changing the CSS or HTML of a page directly or algorithmically using JavaScript. This exercise introduces students to the console, and to the underlying structure of a website; it can later scaffold the creation of their extension.\n  * Focus on one specific intervention, such as a text modification, a word or image find-and-replace, a style-based CSS change, or a content augmentation in which an extra layer of information is added to websites.\n  * Incorporate a regular expression.\n  * Make an extension that augments the Web through the use of data from an external online API or database.\n  * Build a plugin that incorporates multiuser functionality, allowing its users to become aware of (or interact with) others using the same extension. This may require the use of WebSockets.\n\n## Making It Meaningful\n\nAs a window mediates a view of the physical world, the browser mediates the\nonline world of the Internet. Altering this quotidian equipment with a custom\nextension is a key opportunity for creative play and disruption at both the\nsystem level and the content level. Doing so requires direct engagement with\nthe infrastructure and protocols of the browser, a favorite topic of “software\nstudies” scholars like Alexander Galloway, Matthew Fuller, and Wendy Hui Kyong\nChun, who aim to lay bare the means by which the Web is constructed and how it\nshapes our experience of the world.\n\nSome experimental extensions that manipulate the display of everyday content\nbuild on ideas from the history of conceptual art, such as the Situationist\nstrategies of defamiliarization and _détournement,_ which estrange the mundane\nand enhance perception of the familiar. Browser extensions can also be\nvehicles for culture jamming, activism, critical design, and parody, by\nexplicating and revealing otherwise obscure relationships, or by calling\nattention to or strategically editing political doubletalk, newspeak, dog-\nwhistles, and spin.\n\nReleasing tools can be a form of critical and contextual creative practice.\nOutlets for the publication of browser extensions, like the Chrome Web Store\nor the Firefox Add-ons site, powerfully amplify the author's ability to enlist\nthe participation of the public in shaping new power dynamics. That said,\nthese spaces are tightly controlled sandboxes whose terms of service are\nultimately aligned with their owners’ business model. Projects that challenge\nthe terms of their gatekeepers, such as _AdNauseam_ (which falsifies\nadvertising engagement), may be removed or disabled.\n\n![c15-fig-0101.jpg](../images/c15-fig-0101.jpg)\n\n101\\. Jonas Lund's _We See in Every Direction_ (2013) connects all of its\nconcurrent users in a collaborative browsing experience.\n\n![c15-fig-0102.jpg](../images/c15-fig-0102.jpg)\n\n102\\. Steve Lambert's _Add Art_ (2008) plugin for the Firefox browser\nautomatically replaces online advertisements with art.\n\n![c15-fig-0103.jpg](../images/c15-fig-0103.jpg)\n\n103\\. _Newstweek_ (2011) by Julian Oliver and Danja Vasiliev is a custom\nInternet router that enables the artists to alter how news websites appear to\nother people on their WiFi network.\n\n![c15-fig-0104.jpg](../images/c15-fig-0104.jpg)\n\n104\\. _Us+_ (2013) by Lauren McCarthy and Kyle Mcdonald is a dystopian add-on\nfor Google's video chat software. Using facial analysis, speech-to-text, and\nnatural language processing, the _Us+_ software analyzes the users’\nconversation and attempts to offer suggestions for improving their\ninteraction.\n\n## Additional Projects\n\n  * Todd Anderson, _Hitchhiker_ , 2020, browser extension for live performance.\n  * American Artist, _Looted,_ 2020, website intervention.\n  * BookIndy, _BookIndy: Browse Amazon, Buy Local_ , 2015, browser extension.\n  * Allison Burtch, _Internet Illuminator_ , 2014, browser extension.\n  * Brian House, _Tanglr_ , 2013, browser extension.\n  * Daniel C. Howe, Helen Nissenbaum, and Vincent Toubiana, _TrackMeNot_ , 2006, browser extension.\n  * Daniel C. Howe, Helen Nissenbaum, and Mushon Zer-Aviv, _AdNauseam_ , 2014, browser extension.\n  * Darius Kazemi, _Ethical Ad Blocker_ , 2015, browser extension.\n  * Surya Mattu and Kashmir Hill, _People You May Know Inspector_ , 2018, app.\n  * Joanne McNeil, _Emotional Labor_ , 2015, browser extension.\n  * Dan Phiffer and Mushon Zer-Aviv, _ShiftSpace_ , 2007, browser extension.\n  * Radical Software Group, _Carnivore_ , 2001, Processing library.\n  * Sara Rothberg, _Scroll-o-meter_ , 2015, browser extension.\n  * Rafaël Rozendaal, _Abstract Browsing_ , 2014, browser extension.\n  * Joel Simon, _FB Graffiti_ , 2014, browser extension.\n  * Sunlight Foundation, _Influence Explorer_ , 2013, browser extension.\n  * The Yes Men et al., _The New York Times Special Edition_ , November 2008, website.\n\n## Readings\n\n  1. Guy Debord and Gil J. Wolman, “A User's Guide to Détournement,” trans. Ken Knabb, _Les Lèvres Nues_ no. 8 (May 1956).\n  2. Alexander Galloway, “Protocol: How Control Exists after Decentralization,” _Rethinking Marxism_ 13, nos. 3–4 (Fall–Winter 2001): 81–88.\n  3. Joana Moll and Andrea Noni, _Critical Interfaces Toolbox_ (2016), [crit.hangar.org](http://crit.hangar.org), accessed July 20, 2020.\n  4. Rhizome.org, “Net Art Anthology,” accessed April 11, 2019.\n  5. Aja Romano, “How Your Web Browser Affects Your Online Reality, Explained in One Image,” _Vox_ , May 3, 2018.\n\n\n# Creative Cryptography\n\nPoetic encodings\n\n![c16-fig-0105.jpg](../images/c16-fig-0105.jpg)\n\n105\\. Amy Suo Wu's _Thunderclap_ (2017) employs steganography to distribute\nthe suppressed work of Chinese anarcho-feminist He-Yin Zhen (1886–1920)\nthrough the medium of clothing accessories. The work co-opts shanzhai fashion\n(a style featuring nonsense English), together with QR codes, as a covert\nsystem to publish subversive writings for a Chinese context.\n\n## Brief\n\nCreate a digital system that encodes a message into a media object, and also\nprovides a means of decoding it. Your system should be designed to hide\ninformation in plain sight—within images, text, video, sound, or physical\nforms. Think carefully about the specific information your system is designed\nto encode, and the significance of the relationship between this information\nand the medium that contains it. You may choose to encode text, data, code,\nimages, music, or other information of your choosing. Present one or more\nexamples of your system in use. This assignment can be tailored to focus on\n_steganography_ (the act of hiding a message in another medium) or\n_cryptography_ (the act of encoding a message).\n\n## Learning Objectives\n\n  * Review common steganographic and cryptographic techniques\n  * Apply methods for working with data like compression, encoding, and decoding\n  * Explore conceptual and poetic possibilities of encoding and translation\n\n## Variations\n\n  * Add layers of encryption into your system for added security or poetry.\n  * Consider different constraints that might determine the encoding technique you develop. For example, if your means of transmission is extremely low bandwidth (as was the case when Morse code was developed), your system will be shaped by this parameter. The encoding method could also constrain your message to be perceived in one sensory domain (e.g., hearing, sight, or touch) as is the case with Braille or sign languages.\n\n## Making It Meaningful\n\nThe advent of the written word created a need for secure communications, both\nfor those in power and those who would oppose them. This need for secure\nmessaging has long inspired techniques of cryptography, defined as the\nencoding of messages. Some historic cryptographic approaches rely on the use\nof pre-computational devices like the decoding wheel, the Polybius square, and\nthe Enigma machine, while others use simple ciphers like the pigpen cipher,\nMorse code, Braille, or ROT13. For children, there can be a wholesome joy in\nexchanging secret messages with one's friends, and a thrill in the effort of\nunlocking and revealing them.\n\nSteganography is the special cryptographic technique of concealing information\nwithin another media format. Early cryptographers like Sir Francis Bacon and\nWilliam Friedman were fond of steganography, building systems to hide messages\nin music scores, photographs, and even flower arrangements. Digital\ntechnologies offer many possibilities for creative steganography. Unused\npixels or bytes in files can be filled with invisible information, data can be\ntranscoded into different media, and (as with acrostics) messages can be\ndistributed across huge volumes of online communications.\n\nThe 2013 Snowden revelations confirmed that the US government was recording\nthe online communications of the American public, and produced a collective\nrealization that unencrypted communications across digital networks are\ninherently insecure. On the Internet, it is impossible to know if\ncommunications are intercepted, making the development of practical and\nreliable cryptographic technologies an ongoing urgent challenge.\n\n![c16-fig-0106.jpg](../images/c16-fig-0106.jpg)\n\n106\\. A botanical drawing of a flower by William and Elizebeth Friedman (1916)\nuses a bilateral cipher invented by Francis Bacon to encode “BACON” in the\nroots and the names of Elizabethan authors and books in the leaves.\n\n![c16-fig-0107.jpg](../images/c16-fig-0107.jpg)\n\n107\\. Maddy Varner's _KARDASHIAN KRYPT_ (2014) is a steganographic Chrome\nextension that covertly encodes messages into (and decodes messages from)\nphotographs of Kim Kardashian. The low-order bits of the image shown here\ncontain text from “A Room of One's Own” by Virginia Woolf.\n\n![c16-fig-0108.jpg](../images/c16-fig-0108.jpg)\n\n108\\. _Block Bills_ (2017) by Matthias Dörfelt (Moka) is a collection of 64\nbanknote designs generated from the Bitcoin blockchain.\n\n![c16-fig-0109.jpg](../images/c16-fig-0109.jpg)\n\n109\\. In _Disarming Corruptor_ , Matthew Plummer-Fernández presents a\nreversible algorithm for corrupting 3D CAD files. The artist's software twists\n3D meshes into illegible configurations, helping users circumvent surveillance\nand other limitations on file-sharing.\n\n![c16-fig-0110.jpg](../images/c16-fig-0110.jpg)\n\n110\\. _Biopresence_ (2005) by Shiho Fukuhara and Georg Tremmel, a\ncollaboration with scientist Joe Davis, uses Davis's DNA Manifold algorithm to\ntranscode human DNA into the DNA of a tree.\n\n![c16-fig-0111.jpg](../images/c16-fig-0111.jpg)\n\n111\\. The _Talking Popcorn_ installation by Nina Katchadourian (2001) uses a\nMorse code decoder to interpret messages “spoken” by a popcorn machine.\n\n![c16-fig-0112.jpg](../images/c16-fig-0112.jpg)\n\n112\\. Each copy of _Notepad_ (2007) by Matt Kenyon and Douglas Easterly\nresembles a standard yellow legal pad. The ruled lines on its pages, however,\nactually consist of microprinted text with the full names, dates, and\nlocations of each Iraqi civilian death on record from the first three years of\nthe Iraq War. One hundred copies of _Notepad_ were covertly distributed to US\nsenators and representatives, to help ensure that the names of Iraqi civilians\nwould be memorialized in official US archives.\n\n## Additional Projects\n\n  * Anonymous, _Genecoin_ , 2014, system for encoding human DNA in Bitcoin.\n  * Aram Bartholl, _Keepalive,_ 2015, fire-powered network.\n  * Liat Berdugo, Sam Kronick, Ben Lotan, and Tara Shi, _Encoded Forest_ , 2016, passwords stored in tree-planting patterns.\n  * Ingrid Burrington, _Secret Device for Remote Locations,_ 2011, solar-powered Morse code message.\n  * “Ciphers, Codes, & Steganography,” 2014, Folger Shakespeare Library exhibition.\n  * Cryptoart Publishers, _Cryptoart_ , 2020, system for storing Bitcoin in physical art.\n  * Heather Dewey-Hagborg, _How Do You See Me?_ , 2019, self-portraits generated via adversarial processes.\n  * Henry Fountain, “Hiding Secret Messages Within Human Code,” _The New York Times_ , June 22, 1999.\n  * Eduardo Kac, _Genesis_ , 1999, biblical passage encoded in DNA.\n  * Sam Lavigne, Aaron Cantu, and Brian Clifton, _You Can Encrypt Your Face,_ 2017, face masks.\n  * Lindsay Maizland, “Britney Spears's Instagram Is Secretly Being Used by Russian Hackers,” _Vox_ , June 8, 2017.\n  * Julian Oliver, _The Orchid Project_ , 2015, photographs with encoded firmware.\n  * Everest Pipkin, _Ladder_ , 2019, encoded poem.\n\n## Readings\n\n  1. Florian Cramer, _Hiding in Plain Sight: Amy Suo Wu's_ The Kandinsky Collective (Ljubljana, Slovenia: Aksioma Institute for Contemporary Art, 2017), e-brochure.\n  2. Manmohan Kaur, “Cryptography as a Pedagogical Tool,” _Primus_ 18, no. 2 (2008): 198–206.\n  3. Neal Koblitz, “Cryptography as a Teaching Tool,” _Cryptologia_ 21, no. 4 (1997): 317–326.\n  4. Susan Kuchera, “The Weavers and Their Information Webs: Steganography in the Textile Arts,” _Ada: A Journal of Gender, New Media, and Technology_ 13 (May 2018).\n  5. Shannon Mattern, “Encrypted Repositories: Techniques of Secret Storage, From Desks to Databases,” _Amodern_ 9 (April 2020).\n  6. Jussi Parikka, “Hidden in Plain Sight: The Steganographic Image,” unthinking.photography, February 2017.\n  7. Charles Petzold, _Code: The Hidden Language of Computer Hardware and Software_ (Hoboken, NJ: Microsoft Press, 2000).\n  8. Phillip Rogaway, “The Moral Character of Cryptographic Work” (paper presented at Asiacrypt 2015, Auckland, NZ, December 2015).\n  9. Simon Singh, “Chamber Guide,” The Black Chamber (website), accessed April 12, 2020.\n  10. Hito Steyerl, “A Sea of Data: Apophenia and Pattern (Mis-) Recognition,” _e-flux Journal_ 72 (2016).\n\n\n# Voice Machine\n\nSpoken word and voice play\n\n![c17-fig-0113.jpg](../images/c17-fig-0113.jpg)\n\n113\\. Lynn Hershman Leeson's _DiNA, Artificial Intelligent Agent Installation_\n(2002–2004) is an animated, artificially intelligent female character with\nspeech recognition and expressive facial gestures. DiNA converses with gallery\nguests, generating answers to questions and becoming “increasingly intelligent\nthrough interaction.”\n\n## Brief\n\nCreate an interactive chatbot, eccentric virtual character, or spoken word\ngame centered around computing with speech input and/or speech output. For\nexample, you might make a rhyming game or memory challenge; a voice-controlled\nbook; a voice-controlled painting tool; a text adventure; or an oracular\ninterlocutor (think: Monty Python's Keeper of the Bridge of Death). Consider\nthe creative affordances of using a tightly restricted vocabulary as well as\nthe dramatic potential of rhythm, intonation, and volume of speech. Keep in\nmind that speech recognition is error-prone, so find ways to embrace the lag\nand the glitches—at least, for another couple of years. Graphics are optional.\n\n## Learning Objectives\n\n  * Discuss and explore the expressive possibilities of working with voice and language as a creative medium\n  * Apply a toolkit for speech recognition and/or speech synthesis\n  * Design, advance, and execute a concept for a creative work with a voice interface\n\n## Variations\n\n  * Add speech interactions to an appliance or everyday object. What if your toaster could talk?\n  * Recall your favorite road-trip word games. Create a competitive, multiplayer speech game in which the computer is the referee.\n  * Appropriate or subvert a commercial voice assistant as a readymade for a performance.\n  * Take inspiration from the ways in which pets and babies use speech—often inferring meaning from the tone or prosody of a voice, rather than the words that are spoken. For example, you might make an interactive babbling machine, or a virtual pet that responds to your intonation.\n\n## Making It Meaningful\n\nThe capacity to speak has long been perceived as a sign of intelligence. For\nthis reason, machines that speak can seem uncanny or even supernatural, as\nthey decouple ancient bindings between voice, living matter, and intelligence.\nIn the field of interaction design, voice interfaces are thought to make\ntechnologies more intuitive and accessible than their visual or typographic\ncounterparts—but such anthropomorphized machines do this at the expense of our\nability to accurately estimate how much they actually “understand.”\n\nIn a conversation, information is transmitted and received on multiple\nregisters—in not only what is said, but also how it is delivered, and by whom.\nWe have exquisitely tuned capacities for inferring contextual information like\nemotion, gender, age, health, and socioeconomic status from a speaking voice.\nIntonation, rhythm, pace, and rhyme are also used to create drama, suspense,\nsarcasm, and humor. When creating new experiences through speech, simple\noperations may be the most generative. For example, a vast range of meanings\ncan arise just from altering the emphasis of words in a sentence.\n\nSpeech has a key paralinguistic social role. Through chit-chat and banter we\nestablish trust, build relations, and create intimacy. Wordplay, punnery, and\nother playful verbal exchanges create a protected space for this social\nactivity by exploiting ambiguities in the rules of language itself. Culture is\nembedded and propagated in the protocols of knock-knock jokes, call-and-\nresponse songs, and once-upon-a-time fairy tales. These rule-based media lend\nthemselves well to creative manipulation with code. Some potentially helpful\ntools to algorithmically generate speech include context-free grammars, Markov\nchains, recurrent neural nets (RNN), and long short-term memory (LSTM)\nsystems. _Note that some commercial speech analysis tools transmit the users’\nvoice data to the cloud, raising issues of data ownership and privacy._\n\n![c17-fig-0114.jpg](../images/c17-fig-0114.jpg)\n\n114\\. In _Conversations with Bina48_ (2014), Stephanie Dinkins performs\nimprovised conversations about algorithmic bias with BINA48, a chatbot-enabled\nface robot. BINA48 was commissioned by entrepreneur Martine Rothblatt, and\nconstructed by roboticist David Hanson, to resemble Rothblatt's wife Bina.\n\n![c17-fig-0115.jpg](../images/c17-fig-0115.jpg)\n\n115\\. _Hey Robot_ (2019) by Everybody House Games is a game in which teams\ncompete to make a smart home assistant (such as Amazon Alexa or Google Home)\nsay specific words.\n\n![c17-fig-0116.jpg](../images/c17-fig-0116.jpg)\n\n116\\. In David Rokeby's _The Giver of Names_ (1991–1997), a camera detects\nobjects placed on a pedestal by members of the audience. A computerized voice\nthen describes what it sees—with strange, uncanny, and often poetic results.\n\n![c17-fig-0117.jpg](../images/c17-fig-0117.jpg)\n\n117\\. _When Things Talk Back_ (2018), by Roi Lev and Anastasis Germanidis, is\na mobile AR app that gives voice to everyday objects. The software\nautomatically identifies objects observed by the system's camera, and\nanthropomorphizes them with AR overlays of simple faces. It then uses\nConceptNet, a freely available semantic network, to retrieve information about\nthe objects and their possible interrelationships. The app uses this\ninformation to generate humorous and sometimes poignant conversations between\nthe objects in the scene.\n\n![c17-fig-0118.jpg](../images/c17-fig-0118.jpg)\n\n118\\. David Lublin's _Game of Phones_ (2012) is “the children's game of\ntelephone, played by telephone.” Players receive a phone call and hear a\nprerecorded message left by the previous player; they are then prompted to re-\nrecord what they recall hearing for the next person in the queue. After a\nweek, the entire chain of messages is published online.\n\n![c17-fig-0119.jpg](../images/c17-fig-0119.jpg)\n\n119\\. Neil Thapen's playful _Pink Trombone_ (2017) is an interactive\narticulatory speech synthesizer for “bare-handed speech synthesis.” Using a\nrichly instrumented simulation of the human vocal tract, the project enables a\nwide range of vocal noisemaking in the browser.\n\n![c17-fig-0120.jpg](../images/c17-fig-0120.jpg)\n\n120\\. Kelly Dobson's _Blendie_ (2003–2004) is a 1950s Osterizer blender,\nadapted to respond empathetically to a user's voice. A person induces the\nblender to spin by vocalizing. _Blendie_ then mechanically mimics the person's\npitch and power level, from a low growl to a screaming howl.\n\n![c17-fig-0121.jpg](../images/c17-fig-0121.jpg)\n\n121\\. In Nicole He's speech-driven forensics game, _ENHANCE.COMPUTER_ (2018),\nplayers yell out commands like “Enhance!”—living out a science-fiction fantasy\nof infinitely zoomable images.\n\n## Additional Projects\n\n  * Tim Anderson, Marc Blank, Bruce Daniels, and Dave Lebling, _Zork_ , 1977–1979, interactive text-based computer game.\n  * Isaac Blankensmith with Smooth Technology, _Paper Signals_ , 2017, system for making voice-controlled paper objects.\n  * Mike Bodge, _Meme Buddy_ , 2017, voice-driven app for generating memes.\n  * Stephanie Dinkins, _Not The Only One_ , 2017–2019, voice-driven sculpture trained on oral histories.\n  * Homer Dudley, _The Voder_ , 1939, device to electronically synthesize human speech.\n  * Ken Feingold, _If/Then_ , 2001, sculptural installation with generated dialogue.\n  * Sidney Fels and Geoff Hinton, _Glove Talk II_ , 1998, neural network-driven interface translating gesture to speech.\n  * Wesley Goatly, _Chthonic Rites_ , 2020, narrative installation with Alexa and Siri.\n  * Suzanne Kite, _Íŋyaŋ Iyé (Telling Rock)_ , 2019, voice-activated installation.\n  * Jürg Lehni, _Apple Talk_ , 2002–2007, computer interaction via text to speech and voice recognition software.\n  * Golan Levin and Zach Lieberman, _Hidden Worlds of Noise and Voice_ , 2002, sound-activated augmented reality installation, Ars Electronica Futurelab, Linz.\n  * Golan Levin and Zach Lieberman with Jaap Blonk and Joan La Barbara, _Messa di Voce_ , 2003, voice-driven performance with projection.\n  * Rafael Lozano-Hemmer, _Voice Tunnel_ , 2013, large-scale interactive installation in the Park Avenue Tunnel, New York City.\n  * Lauren McCarthy, _Conversacube_ , 2010, interactive conversation-steering devices.\n  * Lauren McCarthy, _LAUREN_ , 2017, smart home performance.\n  * Ben Rubin and Mark Hansen, _Listening Post_ , 2002, installation displaying and voicing real-time chatroom utterances.\n  * Harpreet Sareen, _Project Oasis_ , 2018, interactive weather visualization and self-sustaining plant ecosystem.\n  * Superflux, _Our Friends Electric_ , 2017, film.\n  * Joseph Weizenbaum, _ELIZA_ , 1964, natural language processing program.\n\n## Readings\n\n  1. Zed Adams and Shannon Mattern, “April 2: Contemporary Vocal Interfaces,” readings from Thinking through Interfaces (The New School, Spring 2019).\n  2. Takayuki Arai et al., “Hands-On Speech Science Exhibition for Children at a Science Museum” (paper presented at WOCCI 2012, Portland, OR, September 2012).\n  3. Melissa Brinks, “The Weird and Wonderful World of Nicole He's Technological Art,” _Forbes_ , October 29, 2018.\n  4. Geoff Cox and Christopher Alex McLean, “Vocable Code,” in _Speaking Code: Coding as Aesthetic and Political Expression_ (Cambridge, MA: MIT Press, 2013), 18–38.\n  5. Stephanie Dinkins, “Five Artificial Intelligence Insiders in Their Own Words,” _New York Times_ , October 19, 2018.\n  6. Andrea L. Guzman, “Voices in and of the Machine: Source Orientation toward Mobile Virtual Assistants,” _Computers in Human Behavior_ 90 (2019): 343–350.\n  7. Nicole He, “Fifteen Unconventional Uses of Voice Technology,” Medium.com, November 26, 2018.\n  8. Nicole He, “Talking to Computers” (lecture, awwwards conference, New York, NY, November 28, 2018).\n  9. Halcyon M. Lawrence, “Inauthentically Speaking: Speech Technology, Accent Bias and Digital Imperialism” (lecture, SIGCIS Command Lines: Software, Power & Performance, Mountain View, CA, March 2017), video, 1:26–17:16.\n  10. Halcyon M. Lawrence and Lauren Neefe, “When I Talk to Siri,” _TechStyle: Flash Readings_ 4 (podcast), September 6, 2017, 10:14.\n  11. Shannon Mattern, “Urban Auscultation; or, Perceiving the Action of the Heart,” _Places Journal_ , April 2020.\n  12. Mara Mills, “Media and Prosthesis: The Vocoder, the Artificial Larynx, and the History of Signal Processing,” _Qui Parle: Critical Humanities and Social Sciences_ 21, no. 1 (2012): 107–149.\n  13. Danielle Van Jaarsveld and Winifred Poster, “Call Centers: Emotional Labor over the Phone,” in _Emotional Labor in the 21st Century: Diverse Perspectives on Emotion Regulation at Work,_ ed. Alicia A. Grandey, James M. Diefendorff, and Deborah E. Rupp (New York: Routledge, 2012): 153–73.\n  14. “Vocal Vowels,” Exploratorium Online Exhibits, accessed April 14, 2020.\n  15. Adelheid Voshkul, “Humans, Machines, and Conversations: An Ethnographic Study of the Making of Automatic Speech Recognition Technologies,” _Social Studies of Science_ 34, no. 3 (2004).\n\n\n# Measuring Device\n\nSensing as a critical and creative act\n\n![c18-fig-0122.jpg](../images/c18-fig-0122.jpg)\n\n122\\. Presented as a device for measuring the hypothetical “Despondency Index”\nof a given locale, Natalie Jeremijenko and Kate Rich's _Suicide Box_ (1996)\nnevertheless records very real data regarding suicide jumpers from the Golden\nGate Bridge.\n\n## Brief\n\nCreate a machine that asks a question of the world. Your machine should either\nmeasure something interesting, measure something in an interesting way, or\ncreate an interesting provocation by bringing an uncommon measurement to our\nattention. The focus here is on the selection and collection of intriguing\ndata (using a microcontroller and a sensor), rather than on the production of\nan attractive interpretation or visualization. What overlooked dynamics or\ninvisible rhythms can you discover?\n\nYour project's location is critically important: the situation of your device\nwill affect who encounters it, how it is perceived, and the meanings it\nevokes. It's up to you whether your device measures human activity or the\nactivity of something else in the environment (cars, animals, lights, doors,\netc.) Consider if you are measuring ambient, incidental, or deliberate\nactivity, and whether or not your device is passive or actively used. Be sure\nto make a video documenting your measurement device at the data collection\npoint. Although you may use any sensor you like, remember that even a humble\nswitch is a sensor—and that some switches, like tilt-switches, can measure\ninadvertent movements in the world. Likewise, having a proximity sensor\ndoesn't mean you have to measure proximity. Instead, you might measure the\namount of time that something is proximal to the sensor (recording seconds,\nnot centimeters). Or perhaps you might count the number of times that\nsomething has come close to the sensor.\n\nSometimes, student electronics projects can look suspicious. If you install\nyour device in a public place, be sure to secure necessary permissions (such\nas from your campus safety officer), and attach a small sign to your device\nwith appropriate information.\n\n## Learning Objectives\n\n  * Review and critique methods for collecting data\n  * Experiment with social, performative, and sculptural modes of data presentation\n  * Assemble and install sensor hardware\n\n## Variations\n\n  * Restrict data collection to a specific site, such as the classroom or a nearby park.\n  * Provide students with a screen or other display, such as a multi-digit 7-segment LED, so they can represent their sensor readings at the site of data collection—creating the potential for public interaction and additional poignance.\n\n## Making It Meaningful\n\nCensus historian James C. Scott points out that measurement is a political\nact. Artists like Natalie Jeremijenko collect measurements in order to prompt\nevidence-driven discussion; others, like Mimi Onuoha, point out that what is\n_not_ measured is equally revealing of a culture's biases and indifferences\n(the study of which is called _agnotology_). In the weird world of quantum\nphysics, the term “observer effect” refers to the idea that the very act of\nmeasurement changes the subject being measured. Measurement, or the collection\nof data, alters the world and the way we see it.\n\nData collection has become a key practice across many fields. “Citizen\nscience” is an educational and political movement that enlists everyday people\nin scientific activities and often focuses on monitoring local environmental\nconditions through distributed DIY sensing. For example, in the aftermath of\nthe Fukushima disaster, radiation sensors were distributed to a concerned\npublic, who transmitted readings to a central server.\n\nScholars Catherine D’Ignazio and Lauren Klein outline ways to responsibly work\nwith data, taking philosophical ideas from feminist thought and applying them\nto data collection and visualization practices. The principles of feminist\ndata visualization include acknowledging that data represents an incomplete\nperspective; emphasizing the context and the situation in which data was\ncollected; and providing a way for those represented in the data to respond to\nit.\n\nThere is often something absurd, poignant, or whimsically futile about the act\nof measurement—an attempt to reduce an infinitely complex experience to a\nhandful of numbers. In the arts, measurement can explicitly remind us that our\nunderstanding of reality is only ever an approximation.\n\n![c18-fig-0123.jpg](../images/c18-fig-0123.jpg)\n\n123\\. _The Deep Sweep_ (2015) by the Critical Engineering Working Group is an\naerospace probe that scans the otherwise out-of-reach signal space between\nland and stratosphere.\n\n![c18-fig-0124.jpg](../images/c18-fig-0124.jpg)\n\n124\\. Maddy Varner's _This or That_ (2013), a “DIY voting poster,” is a\nstudent project made from paper-mounted electronics. A passerby taps sticky\nnotes to vote between two options (e.g., “cats” versus “dogs”) proposed by\nother strangers.\n\n![c18-fig-0125.jpg](../images/c18-fig-0125.jpg)\n\n125\\. Michelle Ma's _Revolving Games_ (2013), another student project,\nmeasures the speed of a revolving door with an accelerometer, then displays\nthe high score on an LED. Ma's game encourages risk-taking in an otherwise\nquotidian setting.\n\n![c18-fig-0126.jpg](../images/c18-fig-0126.jpg)\n\n126\\. Catherine D’Ignazio's _Babbling Brook_ (2014) is a red networked flower\nsculpture containing water quality sensors. The flower is installed outdoors\nin a creek or stream and audibly reports its data in the form of bad jokes to\nanyone listening.\n\n![c18-fig-0127.jpg](../images/c18-fig-0127.jpg)\n\n127\\. In _Picture Sky_ (2015), Karolina Sobecka and Christopher Baker\ncoordinate groups of people to take photographs of the sky at the same moment\nthat a satellite captures an image from above.\n\n![c18-fig-0128.jpg](../images/c18-fig-0128.jpg)\n\n128\\. _Library of Missing Datasets_ (2016) by Mimi Onuoha is a systematized\narchive of hypothetical datasets. These data voids stand as potent reminders\nof what a society chooses to ignore or overlook.\n\n## Additional Projects\n\n  * Timo Arnall, _Immaterials: Ghost in the Field_ , 2009, RFID probe, long-exposure photography, and animation.\n  * Timo Arnall, Jørn Knutsen, and Einar Sneve Martinussen, _Immaterials: Light Painting WiFi_ , 2011, WiFi network sensor, LED lights, and long-exposure photography.\n  * Tega Brain, _What the Frog's Nose Tells the Frog's Brain_ , 2012, custom fragrance, electronics, and home energy monitor.\n  * Centre for Genomic Gastronomy, _Smog Tasting_ , 2015, air samples, experimental food cart, and smog recipe.\n  * Hans Haacke, _Condensation Cube_ , 1963–1965, kinetic sculpture.\n  * Terike Hapooje, _Dialogue_ , 2008, real-time video of thermal camera imagery.\n  * Usman Haque, _Natural Fuse,_ 2009, network of electronically assisted plants.\n  * Joyce Hinterding, _Simple Forces_ , 2012, conductive graphite drawing and analog electronics.\n  * Osman Khan, _Net Worth_ , 2006, magnetic card reader, custom software, and interactive installation.\n  * Stacey Kuznetsov, Jian Cheung, George Davis, and Eric Paulos, _Air Quality Balloons_ , 2011, air quality sensors, microelectronics, and weather balloons.\n  * Rafael Lozano-Hemmer, _Pulse Room_ , 2006, interactive installation with heart-rate sensors activating an array of incandescent light bulbs.\n  * Rafael Lozano-Hemmer, _Tape Recorders_ , 2011, interactive installation with sensors and robotically activated measuring tapes.\n  * Agnes Meyer-Brandis, _Teacup Tools_ , 2014, tea cups and atmospheric sensors.\n  * Joana Moll, _CO2GLE_ , 2015, online carbon calculator.\n  * Joana Moll, _DEFOOOOOOOOOOOOOOO- OOOOOOREST_ , 2016, online visualization.\n  * Moon Ribas, _Seismic Sensor_ , 2007–2019, seismic-sensing body implants.\n  * Anri Sala, _Why the Lion Roars_ , 2020, temperature-based editor of feature films.\n  * Julijonas Urbonas, _Counting Door_ , 2009, modified video camera and door that tracks its visitor count.\n\n## Readings\n\n  1. Benjamin H. Bratton and Natalie Jeremijenko, _Suspicious Images, Latent Interfaces_ (New York: Architectural League of New York, 2008).\n  2. Catherine D’Ignazio and Lauren F. Klein, “On Rational, Scientific, Objective Viewpoints from Mythical, Imaginary, Impossible Standpoints,” in _Data Feminism_ (Cambridge, MA: MIT Press, 2020).\n  3. Jennifer Gabrys, “How to Connect Sensors” and “How to Devise Instruments,” in _How to Do Things with Sensors_ (Minneapolis: University of Minnesota Press, 2019), 29–71.\n  4. Natalie Jeremijenko, “A Futureproofed Power Meter,” _Whole Earth_ , Summer 2001.\n  5. Mimi Onuoha, “When Proof Is Not Enough,” _FiveThirtyEight_ (blog), ABC News Internet Ventures, July 1, 2020.\n  6. James C. Scott, _Seeing like a State: How Certain Schemes to Improve the Human Condition Have Failed_ (New Haven: Yale University Press, 1998).\n\n\n# Personal Prosthetic\n\nA new verb for the body\n\n![c19-fig-0129.jpg](../images/c19-fig-0129.jpg)\n\n129\\. Brazilian artist Lygia Clark pioneered the exploration of prosthetics\nwithin a conceptual art context. Her _Dialogue Goggles_ device (1964),\ndesigned to be worn by two simultaneous participants, restricts their field of\nvision exclusively to mutual eye contact.\n\n## Brief\n\nDesign a prosthetic device that responds to its wearer's behavior or\nenvironment, enabling “a new verb for the body.” Your device should sense\nsomething (movement, sound, temperature, online data), possibly with the aid\nof machine learning, and produce an electromechanical action or result in\nresponse. Make a video of your prosthetic in use. Your video could be candid,\ndocumenting your work in a public location, or it could be staged to tell the\nnarrative of your work.\n\nMarshall McLuhan considered all technologies to be an extension of the human\nbody, arguing that in one way or another they serve to amplify or accelerate\nexisting physical faculties or cognitive functions. Use McLuhan's terminology\nto discuss your project: is your wearable a physical, cognitive, or\ncommunicative extension of the body? What does your prosthetic augment,\nreplace, constrain, assist, reinforce, signal, reveal, communicate,\nstrengthen, amplify, or diminish?\n\n## Learning Objectives\n\n  * Review, discuss, and appraise the design of wearable technologies\n  * Generate and critique design concepts spanning interaction design, performance art, fashion, and biomimetics\n  * Design a physical computing system that combines sensors, actuators, and microcontrollers (such as an Arduino) into interactive electronic circuits\n\n## Variations\n\n  * Constrain your response to a single type of action, such as a physical gesture.\n  * Design a biomimetic prosthetic, inspired by an animal. What new power does it lend to the wearer?\n  * Design for one has often resulted in design for many. Pick a specific person. Interview them about their habits, and observe their daily routine. Create a prosthetic device just for them.\n  * Subvert individualism: design a prosthetic device that two or more people wear together.\n  * Suppose, through the wearable, that your body is connected to the Internet. What signal should your body communicate? With whom?\n  * Omit the sensor. Instead, make a preexisting dataset experienceable (or performable).\n  * _Instructors: Ask your students to present their prosthetics in a fashion show._\n\n## Making It Meaningful\n\nProsthetics are any artificial additions to the body. They are used in a wide\nrange of contexts, including medicine, combat, fitness, theater, and fashion.\nMany people's daily attire includes prosthetics that enhance bodily functions,\nincluding perception (such as eyeglasses and hearing aids) and mobility (such\nas walking canes and inline skates), or that protect the body from the\nenvironment (e.g., shoes, helmets, respirators, knee pads). Jewelry can be\nunderstood as “social prostheses,” operating as markers of social status,\nsignifiers of gender or community affiliations, tools of beautification,\ncarriers of personal meaning, or (as with amulets and phylactery) talismanic\nprotection.\n\nIncreasingly, prosthetics are conduits for digital data. Body-worn computers\nlike smartphones allow the wearer to exchange signals and media from any\nlocation, and across heretofore impossible distances. More specialized\ntelemetry appendages like fitness trackers, personal locator beacons, and\nparolee ankle monitors collect and broadcast the wearer's position, activity,\nand even metabolic data—sometimes to unintended recipients. Digital data now\npenetrates the body itself, in a burgeoning market of “intimate hardware” with\nBluetooth capabilities. In a quest to prototype a “better human,” proponents\nof the transhumanist “body hacking” movement invade the body's boundaries even\nfurther, implanting RFID chips and other circuits under their own skin. The\npropinquity of these devices brings new urgency to familiar problems in\nprivacy and security.\n\nProsthetics as a genre raises important questions about typical and atypical\nbodies, abilities, and identities. Rather than approaching these technologies\nas a means to restore the body to some sort of assumed norm, this prompt\ninvites a reframing of the discourse of disability—as scholar Sara Hendren\nurges, “rethinking the default bodily experience.”\n\nWhat could it mean for a person to have a prehensile tail, or chemosensitive\nantennae? What if one's appearance could disrupt the normal operation of\ncamera systems? The design of a novel prosthesis opens the door to imaginative\nplay with one's identity and abilities. Explored through the lens of costumes\nand performance, prosthetics also become a probe for remapping social rituals\nor, through fantasy and biomimicry, defamiliarizing biological norms. The new\nprosthetist can invent anatomies, performative appendages, and novel organs\nfor extrasensory perception. She might help a client assume a new social\nidentity, or forge a new relationship to infrastructures of surveillance, in\nways that offer a fresh perspective on technocultural systems we often take\nfor granted.\n\n![c19-fig-0130.jpg](../images/c19-fig-0130.jpg)\n\n130\\. Sputniko!'s _Menstruation Machine_ (2010), fitted with a blood-\ndispensing mechanism and electrodes that shock the abdomen, is a device that\nsimulates the pain and bleeding of a five-day menstruation process.\n\n![c19-fig-0131.jpg](../images/c19-fig-0131.jpg)\n\n131\\. _ScreamBody_ (1997–1998), a “wearable body organ” by Kelly Dobson, is a\nportable space for screaming. The scream is recorded and stored in the device,\nand can later be released in a place where, when, and how the wearer chooses.\n\n![c19-fig-0132.jpg](../images/c19-fig-0132.jpg)\n\n132\\. Sarah Ross's _Archisuits_ (2005–2006) are an edition of leisure suits\nthat enable the wearer to recline comfortably on public furniture otherwise\ndesigned to deter sleeping in public. The clothes contain large foam pads that\nfit into, onto, or around specific structures in the built environment of Los\nAngeles.\n\n![c19-fig-0133.jpg](../images/c19-fig-0133.jpg)\n\n133\\. Inspired by the perceptual powers of other species, Chris Woebken and\nKenichi Okada's _Animal Superpowers_ (2008–2015) is a series that augments or\namplifies the wearer's sensing abilities. Their _Ant Apparatus_ , for example,\nallows you to “see” with microscopes on your hands, while _Bat Vision Goggles_\nmakes ultrasonic sound audible to humans.\n\n![c19-fig-0134.jpg](../images/c19-fig-0134.jpg)\n\n134\\. Caitrin Lynch and Sara Hendren's _Engineering at Home_ (2016) presents\nan online archive of vernacular prosthetics and daily living tools devised by\nCindy, a quadruple amputee.\n\n![c19-fig-0135.jpg](../images/c19-fig-0135.jpg)\n\n135\\. _Entry Holes and Exit Wounds_ (2019), a performance by CMU sophomore\nSteven Montinar, uses a kinesthetically enhanced garment to make data\npalpable. In this project, cellphone vibrators, embedded in clothing and\nsequenced by an Arduino, index the gunshots that killed 12 black victims of\npolice brutality.\n\n![c19-fig-0136.jpg](../images/c19-fig-0136.jpg)\n\n136\\. _The Social Escape Dress_ (2016) is a part of the _Urban Armor_ project\nby Kathleen McDermott, a series of electronic wearables that investigate\npersonal and public space.\n\n![c19-fig-0137.jpg](../images/c19-fig-0137.jpg)\n\n137\\. _The Rift: An Afronaut's Journey_ (2015), a performance by Ayodamola\nOkunseinde, presents Afrofuturist wearable technologies for a time-traveling\nprotagonist from the future, Dr. Tanimowo, who seeks to understand the reasons\nfor the collapse of his culture. The performer's “Afronaut suit” includes a\ncommunication device, a feeding system, and a breathing apparatus.\n\n## Additional Projects\n\n  * Lea Albaugh, _Clothing for Moderns_ , 2014, electromechanical garments, Carnegie Mellon University, Pittsburgh.\n  * Siew Ming Cheng, _Spike Away – How to Protect Your Personal Space on the Subway_ , 2013, plastic vest with spikes, Singapore.\n  * Jennifer Crupi, _Unguarded Gestures 1–3_ , 2019, aluminum and silver implements.\n  * Amisha Gadani, _Animal Inspired Defensive Dresses_ , 2008–2011, interactive garments.\n  * Mattias Gommel, _Delayed_ , 2003, interactive sound installation, «Son Image», Laboratorio Arte Alameda, Mexico City.\n  * Neil Harbisson and Moon Ribas, _Cyborg Arts_ , 2010–2020, cyborg art organization.\n  * Kate Hartman, _Porcupine Experiments_ , 2016, lasercut cardboard with straps and fittings.\n  * Rebecca Horn, _Finger Gloves_ , 1972, finger extension sculptures.\n  * Di Mainstone et al., _Human Harp_ , 2012–2015, suspension bridge interactive sound art.\n  * Daito Manabe, _electric stimulus to face – test_ , 2009, bio-responsive wearable device.\n  * Lauren McCarthy, _Tools for Improving Social Interactivity_ , 2010, bio-responsive garments.\n  * MIT AgeLab, _AGNES (Age Gain Now Empathy System)_ , 2005, age-simulating garments and prostheses, MIT, Cambridge.\n  * Alexander Müller, Jochen Fuchs, and Konrad Röpke, _Skintimacy_ , 2011, haptic device and sound-processing software.\n  * Sascha Nordmeyer, _Communication Prosthesis (HyperLip)_ , 2009, facial prosthesis.\n  * Stelarc, _Third Hand_ , 1980, robotic prosthetic arm, Yokohama.\n  * Jesse Wolpert, _True Emotion Indicator_ , 2014, electronic headware.\n\n## Readings\n\n  1. Philip A. E. Brey, “Theories of Technology as Extension of Human Faculties,” _Metaphysics, Epistemology, and Technology_ , Research in Philosophy and Technology 19, ed. C. Mitcham (London: Elsevier/JAI Press, 2000), 59–78.\n  2. Erving Goffman, _The Presentation of Self in Everyday Life_ (Garden City, NY: Doubleday, 1959).\n  3. Donna J. Haraway, “A Cyborg Manifesto: Science, Technology, and Socialist-Feminism in the Late Twentieth Century,” in _Simians, Cyborgs, and Women: The Reinvention of Nature_ (New York: Routledge, 1991), 149–181.\n  4. N. Katherine Hayles, _How We Became Posthuman: Virtual Bodies in Cybernetics, Literature, and Informatics_ (Chicago: University of Chicago Press, 1999).\n  5. Sara Hendren, _What Can a Body Do?_ (New York: Riverhead Books, 2020).\n  6. Madeline Schwartzman, _See Yourself Sensing: Redefining Human Perception_ (London: Black Dog Press, 2011).\n\n\n# Parametric Object\n\nPost-industrial design\n\n![c20-fig-0138.jpg](../images/c20-fig-0138.jpg)\n\n138\\. _Fahz_ (2015), by Brooklyn design studio Des Des Res, is a fabrication\nsystem for producing unique custom vases. The negative space around each vase\nis generated from silhouettes of the customer's face.\n\n## Brief\n\nCreate a program that generates 3D objects. More specifically, your program\nshould generate a family of 3D objects that are all parameterized in the same\nway, but that differ when their parameters are set to different values. The\noutput from your program should be suitable, at least in principle, for\nprototyping in the real world.\n\nGive consideration to the way in which your parametric object operates as a\ncultural artifact. How might your software attain special relevance by\ngenerating things that address a real human need or interest? Can it make a\ndataset tangible or wearable? Is it possible for a generated object to be\ncritical or tactical? Is it a tool? Garment? Decoration? Can it be funny,\nsurprising, or unexpected? If you are hunting for a concept, it may be helpful\nto think of things in the world around us that are mass-produced but that\n_could_ be (or ought to be) personalized. Document your object so that it can\nbe shared with relevant audiences.\n\n## Learning Objectives\n\n  * Apply generative design principles to the design of 3D form\n  * Control solid geometry operations with algorithmic techniques\n  * Discuss form in relation to the contexts of the body, society, or the environment\n\n## Variations\n\n  * _For introductory students:_ Write software to define a surface of revolution that produces a series of vases, cups, candlesticks, spinning tops, etc.\n  * Working “unplugged” (without a computer), devise a set of rules for creating a class of objects from everyday materials. Your rules should incorporate an element of chance (such as a coin flip) or a dependency on real-world data (by collecting a measurement). Make at least two objects according to your rules.\n  * Write software that generates a tactile, 3D-printable map from cartographic data.\n\n## Making It Meaningful\n\nGenerative design is the activity of authoring a system of rules for\nautomating design decisions. In the case of the parametric object, a form is\nproduced with properties articulated by certain variables. Changing the values\nof these variables changes the form in response, and incorporating elements of\ncontingency or randomness can produce unique objects in every iteration. Among\nother approaches, parametric forms can be assembled from different\narrangements of modular components, lofted from mathematical curves and\nsurfaces, or produced through the actions of simulated physics.\n\n3D parametric objects can be rendered in physical materials with the help of a\nwide range of fabrication technologies. These include additive technologies\nlike 3D printers, which accrete or accumulate material, and subtractive\ntechnologies such as mills, which remove material from a piece of stock. The\nearliest of these, the CNC (computer-numeric controlled) milling machine, was\ndeveloped in the 1950s through a military-funded, Cold War initiative to\ncreate mathematically precise propellers for aircraft. The early 1960s saw the\nparallel development of CAD (computer-aided design) tools to simplify the\nprocess of specifying geometries for CNC manufacture, and it wasn't long\nbefore artists finagled access to these technologies. In 1965, around the same\ntime as the first exhibition of computer-generated plotter art, pioneer\nCharles “Chuck” Csuri became the first artist to employ CNC tooling for\nexpressive ends, in his abstract _Numeric Milling_ sculptures.\n\nThe creation of generative forms does not necessarily require computer\nprogramming. Traditions of process-based, open-ended and rule-based conceptual\nart create meaning in the tension between algorithmic logics and the material\nor social contingencies of the real world. Highly systematic thinking\nunderpins works like _SCUMAK_ by Roxy Paine (a metasculptural machine that\nextrudes randomly-shaped plastic blobs); Nathalie Miebach's handmade\nsculptures, structured according to weather data; and Laura Devendorf's _Being\nthe Machine_ project, an “alternative 3D printer” in which instructions\ntypically provided to fabrication machines are instead given to human makers.\n\nManufactured objects gain a role in our lives through their utility as\nfurniture, housewares, tools, toys, and ornaments. At times, however, the “one\nsize fits all” approach in industrial design means “no size fits any.”\nParametric design counters this by enabling “mass customization”—offering\npersonalization akin to handcrafted and homemade forms, but at a heretofore\nimpossible scale. Eyeglasses, prosthetics, and other wearables, for example,\ncan be customized using measurements or scans taken from an individual's body.\nMaps, histograms, and time series can be generated from data and rendered into\ntactile media; this “physical visualization” approach can be useful in\nwidening the accessibility of data for the visually impaired, or in producing\nmementos or souvenirs that encode information from highly personal\nexperiences. In situations where uniqueness is prized, algorithmic design\ntechniques can guarantee that no two items are alike. Parametric design also\nholds the alluring promise of creating “optimal” forms for a given situation\n(e.g., solving for the greatest strength per unit of material). While this is\nimportant for reducing the material impact of design processes, we must also\nrecognize that over-optimization risks producing unduly specific outcomes in a\ndynamic world.\n\n![c20-fig-0139.jpg](../images/c20-fig-0139.jpg)\n\n139\\. Jessica Rosenkrantz and Jesse Louis-Rosenberg, also known as the design\nstudio Nervous System, are pioneers of simulating natural phenomena to produce\n3D forms. Their _Kinematic Dress_ (2014) is made up of over 2000 unique,\ninterlocking triangular facets that are capable of accommodating the curves of\nthe body. The entire dress is 3D printed as a single folded piece of material.\n\n![c20-fig-0140.jpg](../images/c20-fig-0140.jpg)\n\n140\\. Jonathan Eisenman's _Vase Parametric Model_ (2014) illustrates how a\nsimple vase form may be parameterized.\n\n![c20-fig-0141.jpg](../images/c20-fig-0141.jpg)\n\n141\\. _Grand Old Party_ (2013) by Matthew Epler is a set of computationally\ngenerated butt plugs. Each form is a “physical visualization” whose diameter\nrepresents the voter approval rating, over time, of select presidential\ncandidates.\n\n![c20-fig-0142.jpg](../images/c20-fig-0142.jpg)\n\n142\\. _Filament Sculptures_ (2014), by the mononymous Austrian artist Lia, are\ncomputationally generated forms in which a 3D printer's hot filament has been\nallowed to droop in unpredictable ways. Each sculpture arises through a\nnegotiation between virtual and physical parameters, articulated by the CAD\nmodel, 3D printer settings, and natural forces in the physical world.\n\n![c20-fig-0143.jpg](../images/c20-fig-0143.jpg)\n\n143\\. Charles Csuri's _Numeric Milling_ (1968) sculpture is one of the\nearliest artworks produced with a computer-controlled milling machine. Csuri\nwrote: “While the device was capable of making a smooth surface, I decided it\nwas best to leave the tool's marks for the paths.”\n\n![c20-fig-0144.jpg](../images/c20-fig-0144.jpg)\n\n144\\. For his interactive installation _Wage Islands_ (2015), Ekene Ijeoma\ngenerated a new topography for New York City, in which elevation represents\nthe inverse of median wages. As the lasercut sculpture is submerged into ink-\ndyed water, it visualizes the “wage islands” where low-wage workers can afford\nto rent.\n\n![c20-fig-0145.jpg](../images/c20-fig-0145.jpg)\n\n145\\. Adrien Segal's _Cedar Fire Progression_ (2017) is a data-driven\nsculpture that depicts (from bottom to top) the evolving geographic contour of\na wildfire front over time.\n\n![c20-fig-0146.jpg](../images/c20-fig-0146.jpg)\n\n146\\. Amanda Ghassaei's _3D Printed Record_ (2012) is a playable, 12-inch,\n33rpm phonograph disc whose spiral groove geometry is computationally derived\nfrom audio data. The fidelity of its sound reveals the limits and artifacts of\ndigital fabrication.\n\n![c20-fig-0147.jpg](../images/c20-fig-0147.jpg)\n\n147\\. _Meshu_ (2012) by Rachel Binx and Sha Huang exemplifies the use of 3D\nfabrication pipelines to produce personalized jewelry pieces, on demand, that\nare derived from a customer's geospatial information.\n\n![c20-fig-0148.jpg](../images/c20-fig-0148.jpg)\n\n148\\. _Open Fit Lab_ (2013) by Lisa Kori Chung and Kyle McDonald is a\nperformance in which 3D scans of the audience's bodies are used to generate\ngarment patterns for on-the-spot assembly of custom-tailored pants.\n\n![c20-fig-0149.jpg](../images/c20-fig-0149.jpg)\n\n149\\. The objects in Morehshin Allahyari's _Material Speculation: ISIS_ series\n(2015–2016) are procedural reconstructions of ancient artifacts destroyed by\nISIS militants—systematically recreated from dozens of publicly available\nphotographs. Shown here is Allahyari's reconstruction of a Roman-period figure\nof King Uthal of Hatra, smashed at the Mosul Museum in 2015.\n\n## Additional Projects\n\n  * Alisa Andrasek et al., _Li-Quid_ , 2016, generated chair design.\n  * Ingrid Burrington, _Alchemy Studies_ , 2018, iPhone 5 cast in resin sphere.\n  * Mat Collishaw, _The Centrifugal Soul_ , 2017, mixed media three-dimensional zoetrope.\n  * David Dameron, in Paul Freiberger, “Sculptor Waxes Creative with Computer,” _InfoWorld_ , July 6, 1981.\n  * Laura Devendorf, _Being the Machine_ , 2014–2017, videos, instructable, and app for generating human-executable g-code fabrication instructions.\n  * Erwin Driessens & Maria Verstappen, _Tuboid_ , 2000, computationally generated wooden sculptures and virtual environments.\n  * John Edmark, _Blooms_ , 2015, 3D-printed stroboscopic sculptures.\n  * Madeline Gannon, _Reverberating across the Divide_ , 2014, context-aware modeling tool.\n  * Gelitin, _Tantamounter 24/7_ , 2005, copy machine performance.\n  * Nadeem Haidary, _In-Formed_ , 2009, fork as data visualization.\n  * Mike Kneupfel, _Keyboard Frequency Sculpture_ , 2011, 3D-printed information visualization.\n  * Golan Levin and Shawn Sims, _Free Universal Construction Kit_ , 2012, software and SLS nylon 3D prints.\n  * Nathalie Miebach, _To Hear an Ocean in a Whisper_ , 2013, data sculpture.\n  * Neri Oxman et al., _Silk Pavilion_ , 2013, structure made by silkworms.\n  * Roxy Paine, _SCUMAK (Auto Sculpture Maker)_ , 1998–2001, sculpture-making machine.\n  * Matthew Plummer-Fernández and Julien Deswaef, _Shiv Integer_ , 2016, Thingiverse mashup bot and SLS nylon 3D prints.\n  * Stephanie Rothenberg, _Reversal of Fortune: The Garden of Virtual Kinship_ , 2013, telematic garden visualizing philanthropic data.\n  * Jenny Sabin, _PolyMorph_ , 2014, modular ceramic sculpture.\n  * Jason Salavon, _Form Study #1_ , 2004, video of generated objects.\n  * Keith Tyson, _Geno Pheno Sculpture “Fractal Dice No. 1”_ , 2005, algorithmically generated sculpture.\n  * Wen Wang and Lining Yao et al., _Transformative Appetite_ , 2017, computationally shaped pasta.\n  * Mitchell Whitelaw, _Measuring Cup_ , 2010, generated cup visualizing 150 years of Sydney temperature data.\n  * Maria Yablonina, _Mobile Robotic Fabrication System for Filament Structures_ , 2015, fabrication system.\n\n## Readings\n\n  1. Christopher Alexander, “Introduction” and “Goodness of Fit,” _Notes on the Synthesis of Form_ (Cambridge, MA: Harvard University Press, 1964), 1–27.\n  2. Morehshin Allahyari and Daniel Rourke, _The 3D Additivist Cookbook_ (Amsterdam: The Institute of Network Cultures, 2017).\n  3. Nathalie Bredella and Carolin Höfler, eds., “Computational Tools in Architecture, Cybernetic Theory, Rationalization, and Objectivity,” special issue, _arq: Architectural Research Quarterly_ 21, no. 1 (March 2017).\n  4. Joseph Choma, _Morphing: A Guide to Mathematical Transformations for Architects and Designers_ (London: Laurence King Publishing, 2013).\n  5. Pierre Dragicevic and Yvonne Jansen, _List of Physical Visualizations and Related Artifacts,_ dataphys.org, accessed April 13, 2020.\n  6. Marc Fornes, _Scripted by Purpose: Explicit and Encoded Processes within Design_ , 2007, exhibition.\n  7. Wassim Jabi, _Parametric Design for Architecture_ (London: Laurence King Publishing, 2013).\n  8. Vassilis Kourkoutas, _Parametric Form Finding in Contemporary Architecture: The Simplicity within the Complexity of Modern Architectural Form_ (Riga, Latvia: Lambert Academic Publishing, 2012).\n  9. Golan Levin, “Parametric 3D Form” (lecture, Interactive Art & Computational Design, Carnegie Mellon University, Pittsburgh, Spring 2015).\n  10. D’Arcy Wentworth Thompson, _On Growth and Form_ , 2nd ed. (Cambridge, UK: Cambridge University Press, 1942).\n  11. Claire Warnier et al., eds., _Printing Things: Visions and Essentials for 3D Printing_ (New York: Gestalten, 2014).\n  12. Liss Werner, ed., _(En)Coding Architecture: The Book_ (Pittsburgh: Carnegie Mellon University School of Architecture, 2013).\n\n\n# Virtual Public Sculpture\n\nAugmenting a site\n\n![c21-fig-0150.jpg](../images/c21-fig-0150.jpg)\n\n150\\. Keiichi Matsuda's _HYPER-REALITY_ (2016) is a richly detailed short film\nthat anticipates a dystopic near-future of ubiquitous AR advertising, gamified\nconsumerism, and corporate surveillance.\n\n## Brief\n\nRobert Smithson has remarked that “the site is a place where a piece should be\nbut isn't.” In this assignment, you are asked to create the missing piece for\na site using augmented reality (AR). More specifically: place and view a\nvirtual object of your choice, at a scale of your choice, in a physical\nlocation of your choice, with a programmatic behavior of your choice.\n\nWhen choosing your site, consider the conceptual and aesthetic opportunities\noffered by its location and history, as well as the ways in which it is\noccupied. Write down some observations about who uses the site, and how. Your\nlocation may be public, generic, or private. For example, it could be a\nprominent landmark, an unspecified supermarket aisle, your bedroom, or even\nthe palm of your hand.\n\nYour virtual object may be appropriated, downloaded, recycled, modeled, or\nscanned. You might conceive of your object as a “sculpture,” “monument,”\n“installation,” or “decoration,” or as something else entirely (“anomaly,”\n“natural formation”). Write some code that makes it behave in a certain way.\nFor example, it could rotate slowly in place, emit a shower of particles, or\nchange size whenever the viewer gets close.\n\nAssume that your intervention will be viewed on a mobile device or tablet.\nDocument your project in video, capturing both “over-the-shoulder” and\n“through-the-device” perspectives. Your documentation should convey how an\naudience would experience your artwork. How does your project change and\nreflect relationships between physical and virtual, public and private, screen\nand site? Publish your intervention, and its documentation, so that it can be\nshared with others.\n\n## Learning Objectives\n\n  * Develop, design, and execute a creative intervention at a specific site\n  * Review the technical requirements and workflow of augmented reality\n  * Plan and create documentation of augmented reality projects\n\n## Variations\n\n  * Develop and enact a performance that makes use of your virtual object.\n  * Create an intervention that appears to remove an object, rather than adding one.\n  * Design an experience that connects both a virtual intervention (in AR) and a real, physical intervention (such as a prop) of your own design.\n  * “Gamify” your site with augmented reality objects that turn it into an obstacle course, treasure hunt, escape room, playing field, or game board.\n  * Research the history or current use of the site, and develop an augmentation that uses data derived from this place. Examples of data might include statistics about power consumption or pollution, architectural or geologic features, historic photographs, audio clips of interviews, or real-time weather information.\n\n## Making It Meaningful\n\nAugmented reality (AR) adds a layer of virtual information to the world. Media\nlike 3D animations, text, or images can be anchored to locations, landscape\nfeatures subtracted, and the world distorted. Whereas public artworks are\nsometimes criticized for their lack of engagement with the concerns of local\ncommunities (and scathingly derided as “plop art”), public artworks in AR\noffer opportunities to counter this criticism with their potential to be\ndynamic, interactive, and mutable by audiences. Situated in public space, AR\nartworks bring together the expressive languages and critical traditions of\npublic art, street performance, graffiti, and video games.\n\nAdvertising, branding, and other corporate media are ready targets for\nunsanctioned activist interventions using AR. As Banksy urges, “Any advert in\na public space that gives you no choice whether you see it or not is\nyours….You can do whatever you like with it. Asking for permission is like\nasking to keep a rock someone just threw at your head.” i AR in public space\noffers the possibility of articulating or even prototyping new power\nrelationships in ways that would be impossible within the controlled channels\nof institutions and mass media. As Mark Skwarek and Joseph Hocking demonstrate\nin _The Leak in Your Hometown_ (2010), one strategy to give interventions wide\ntraction is to use well-known logos or other ubiquitous symbols as visual\n“anchors” (targets) for critical augmentation, enabling a project to become\n“site-specific” in any place featuring that sign. For the purposes of culture\njamming and design activism, which aim to reframe issues and shift opinion\nthrough the viral manipulation of media, this combination of Internet-\ndistributed apps with a savvy selection of AR anchors can be particularly\neffective.\n\nThis prompt is similar to the Augmented Projection assignment, in that both\ninvite the artist to create a dynamic virtual addition to a specific site. In\nboth, the challenge is to design interventions that are tightly coupled to\ntheir contexts. There are, however, some key differences. Where projections\noperate like painting, obeying logics of representation, abstraction, and\nillusion, augmented reality is more akin to sculpture, deploying objects that\ndo not _represent_ , but simply _exist_ in space. Formally speaking, augmented\nreality does not require a projection surface, making it possible to suspend\nAR objects in mid-air or have them move around the viewer. Likewise, 3D forms\nin AR can exist at any apparent scale, from palm-sized to sky-filling, and can\neven appear behind or inside real-world objects (as with an X-ray). Taken\ntogether, these properties of AR allow for new ways of choreographing the\nactions and movements of audiences. Finally, the distributed nature of AR\ndisplays also means that people in the same place can see different things, or\n(as Skwarek and Hocking show) people in different places can see the same\nthing.\n\nFor better or worse, augmented worlds observed through phones, tablets, and\ngoggles are not universally viewable, and must be actively experienced by\nparticipants who are in on the joke—in what are ultimately private views in a\npublic space. This hyper-individual nature of AR may eventually have\nunforeseen political consequences, such as digital redlining, the creation of\nfilter bubbles, and the amplification of radical views.\n\n![c21-fig-0151.jpg](../images/c21-fig-0151.jpg)\n\n151\\. Jeffrey Shaw's _Golden Calf_ (1994) was a pioneering work of augmented\nreality. Using a handheld LCD screen fitted with a Polhemus position tracking\nsystem, viewers could observe a virtual 3D sculpture of a golden calf hovering\nabove an otherwise empty pedestal.\n\n![c21-fig-0152.jpg](../images/c21-fig-0152.jpg)\n\n152\\. _Nail Art Museum_ (2014) by Jeremy Bailey is a tiny virtual museum in\nwhich canonical works of art appear positioned on finger-mounted plinths.\n\n![c21-fig-0153.jpg](../images/c21-fig-0153.jpg)\n\n153\\. _The Whole Story_ (2017) by Y&R New York aims to address gender parity\nin public monuments by allowing users to view, share, and add virtual statues\nof notable women alongside existing public statuary.\n\n![c21-fig-0154.jpg](../images/c21-fig-0154.jpg)\n\n154\\. Mark Skwarek and Joseph Hocking created _The Leak in Your Hometown_\n(2010) in response to the BP Deepwater Horizon oil spill. The augmented\nreality smartphone app anchors an animation of a leaking oil pipe to any BP\nlogo.\n\n![c21-fig-0155.jpg](../images/c21-fig-0155.jpg)\n\n155\\. Nathan Shafer's _Exit Glacier_ project (2012) is a site-specific\nsmartphone app that depicts the historic extent of the Exit Glacier in the\nKenai Mountains of Alaska, visualizing glacial recession due to climate\nchange.\n\n![c21-fig-0156.jpg](../images/c21-fig-0156.jpg)\n\n156\\. _Augmented Nature_ (2019) by Anna Madeleine Raupach uses natural objects\nlike trees, stumps, and lichen-covered rocks as anchors for poetic site-\nspecific animations.\n\n## Additional Projects\n\n  * Awkward Silence Ltd, _Pigeon Panic AR_ , 2018, augmented reality pigeon game app.\n  * Aram Bartholl, _Keep Alive_ , 2015, outdoor boulder installation with fire-powered WiFi and digital survival guide repository.\n  * Janet Cardiff and George Bures Miller, _Alter Bahnhof Video Walk_ , 2012, augmented reality walking tour of the Alter Bahnhof, Kassel, Germany.\n  * Carla Gannis, _Selfie Drawings_ , 2016, artist book with AR experiences.\n  * Sara Hendren and Brian Glenney, _Accessible Icon Project_ , 2016, icon and participatory public intervention.\n  * Jeff Koons, _Snapchat: Augmented Reality World Lenses_ , 2017, augmented reality public sculpture installations.\n  * Zach Lieberman and Molmol Kuo, _Weird Type_ , 2018, augmented reality text app.\n  * Lily & Honglei (Lily Xiying Yang and Honglei Li), _Crystal Coffin_ , 2011, AR app, virtual China Pavilion at the 54th Venice Biennale.\n  * Jenny Odell, _The Bureau of Suspended Objects_ , 2015, archive of discarded belongings.\n  * Julian Oliver, _The Artvertiser_ , 2008, AR app for advertisement replacement.\n  * Damjan Pita and David Lobser, _MoMAR_ , 2018, AR art exhibition.\n  * Mark Skwarek, _US Iraq War Memorial_ , 2012, participatory AR memorial.\n\n## Readings\n\n  1. AtlasObscura.com, “Unusual Monuments,” accessed April 14, 2020.\n  2. Henry Chalfant and Martha Cooper, _Subway Art_ , 2nd ed. (New York: Thames and Hudson, 2016).\n  3. Vladimir Geroimenko, ed., _Augmented Reality Art: From an Emerging Technology to a Novel Creative Medium_ , 2nd ed. (New York: Springer, 2018).\n  4. Sara Hendren, “Notes on Design Activism,” accessibleicon.org, last modified 2015, accessed April 14, 2020.\n  5. Josh MacPhee, “Street Art and Social Movements,” Justseeds.org, last modified February 17, 2019, accessed April 14, 2020.\n  6. Ivan Sutherland, “The Ultimate Display,” _Proceedings of the IFIP Congress_ 65, vol. 1 (London: Macmillan and Co., 1965): 506–508.\n\n## Notes\n\ni Banksy, _Cut It Out_ (United Kingdom: Weapons of Mass Disruption, 2004).\n\n\n# Extrapolated Body\n\nInterpreting the dynamic human form\n\n![c22-fig-0157.jpg](../images/c22-fig-0157.jpg)\n\n157\\. Using pose estimation and speech recognition technologies, choreographer\nBill T. Jones collaborated with Google Creative Lab to produce _Body,\nMovement, Language_ (2019), a series of interaction studies in which\nparticipants can use their bodies to position their own spoken words in the\nspace around them.\n\n## Brief\n\nCreate a virtual mask or costume, and use it in a performance.\n\nIn this assignment, you are asked to write software that creatively interprets\nor responds to the movements of your face or body, as observed by a motion\ncapture or computer vision system. More precisely: develop a computational\ntreatment of spatiotemporal data captured from a person, such as the\ncoordinates of features on their face, the 3D locations of their joints, or\npoints along the 2D contour of their silhouette.\n\nConsider whether your treatment serves a ritual purpose, a practical purpose,\nor works to some other end. It may visualize or obfuscate your personal\ninformation. It may allow you to assume a new identity, including something\nnonhuman or even inanimate. It may have articulated parts and dynamic\nbehaviors. It may be part of a game. It may blur the line between self and\nothers, or between self and not-self.\n\nDepending on the materials at hand, your system may use a standard webcam or a\nspecialized peripheral (like a Kinect depth sensor). Furthermore, your\nsolution may require you to learn how to use APIs for real-time face tracking\nor pose estimation, receive and interpret data transmitted by precompiled\nbody-tracking apps (e.g., via OSC), or record and interpret data from a\nprofessional mocap system.\n\nDesign your software for a specific performance, and plan your performance\nwith your software in mind; be prepared to explain your creative decisions.\nRehearse and record your performance.\n\n## Learning Objectives\n\n  * Survey the tools and workflows for motion capture\n  * Use algorithmic techniques to develop a visual interpretation of motion data\n  * Explore aesthetic and conceptual possibilities in animating human form and movement\n\n## Variations\n\n  * _Instructors: It is helpful to provide students with a code template for a tracking library, such as PoseNet (via ml5.js) or FaceOSC. In so doing, this assignment may be restricted to just the face or body._\n  * You may perform your project yourself, or you may collaborate with any performer available to you. Is your software intended for a person with highly specialized movement skills (dancer, musician, athlete, actor), or can anyone operate it?\n  * The assignment brief poses the problem of creating interactive software that responds to real-time data. Instead, generate an animation using pre-recorded (offline) motion capture data. Create or select this data carefully. You might record data yourself, if you have access to a motion capture studio; use mocap data from an online source (such as a research archive or commercial vendor); or creatively augment a favorite YouTube video with the help of a pose estimation library.\n  * Remember that you may position your virtual “camera” anywhere; your motion capture data need not be displayed from the same point of view as your sensor. Consider rendering your performer's body from above, from a moving location, or even from their own point of view.\n  * In contrast to an _expressive_ concept (such as a character animation or playful interactive mirror), develop an _analytic_ treatment, such as an information visualization, that diagrams and compares the movements of body joints or facial landmarks over time. For example, your software could present comparisons between different people making similar expressions, or it could provide insight into the articulations of movements by a violinist.\n  * Consider using sound synchronized to your motion capture data. This sound might be the performer's speech, music to which they are dancing, or sounds synthetically generated by their movements.\n  * Rather than depicting the body itself, write software to visualize how an environment is disturbed by a body, as with footprints in sand.\n  * Use your face or body to puppeteer something nonhuman: a computer-generated animal, monster, plant, or customarily inanimate object.\n  * Visualize a relationship between two or more performers’ bodies.\n  * Focus on the actions of a single part of the body or face.\n\n## Making It Meaningful\n\nCostumes, masks, cosmetics, and digital face filters allow the wearer to fit\nin or act out. We dress up, hide or alter our identity, play with social\nsignifiers, or express our inner fursona. We use them to ritualistically mark\nlife events or spiritual occasions, or simply to obtain “temporary respite\nfrom more explicit and determinate forms of sociality, freeing us to interact\nmore imaginatively and playfully with others and ourselves.” i\n\nMany innovations in understanding, visualizing, and augmenting the dynamic\nhuman body originate as analytic tools (most often, for military purposes)\nthat are then creatively repurposed as expressive ones (for the arts and\nentertainment). The effect of this evolution is that scientific techniques for\ncapturing body movement, such as Étienne-Jules Marey's chronophotography,\ncontribute to the development of new artistic languages, such as Marcel\nDuchamp's Cubist abstraction. Additional examples include the motion capture\nsuit, first developed by Marey in 1883 in physiological research on soldier\nmovement, and the technique of what is now called “light painting” (long-\nexposure photographs of lights attached to moving bodies), explored in depth\nin 1914 by Frank and Lillian Gilbreth to analyze and optimize the activities\nof soldiers and workers.\n\nBody movement is an important dimension of storytelling, central to the\nvocabularies of performance and dance, and also to those of animation and\npuppeteering, where it is essential to creating the illusion of life. As Alan\nWarburton explains, animators develop character by “creating an equivalence\nbetween who someone is, and how they move…. Any audience should know instantly\nwho a character is just from their motion.” ii This conflation of _how we look\nand move_ and _who we are_ is also a foundational premise of video\nsurveillance technologies—which, extending from problematic disciplines like\nphysiognomy, phrenology, and somatotypology, aim to deduce a subject's moral\ncharacter from their outward appearance.\n\nThe face, with its significant role in identity and communication,iii is of\nparticular focus in carceral technologies. Facial recognition systems are\nperilous for many reasons: they enable automated nonconsensual identification\nand are therefore ripe for misuse in the context of policing and authoritarian\nregimes; they operate with the allure of objectivity despite being prone to\ncatastrophic inaccuracies; they are difficult to audit and contest by those\nthey impact most; and their use is often invisible. On the flip side, face\ntrackers have also been used for expressive, entertaining, and educational\npurposes. They serve as digital masks and face filters; as controllers for\ngames (as in Elliott Spelman's “eyebrow pinball” game, _Face Ball_); as\nmusical interfaces (Jack Kalish's _Sound Affects_ performance); as controllers\nfor richly parameterized graphic designs (Mary Huang's typographic\n_TypeFace_); as a means for furthering public understanding of surveillance\ntechnologies (Adam Harvey's _CV Dazzle_); and as tools for interrogating\ncontemporary culture (Christian Moeller's _Cheese_ or Hayden Anyasi's\n_StandardEyes_). In working with computational face and body tracking\nlibraries, media artists and interface designers are encouraged to reflect on\nthe origins of their tools, and the extent to which their use in a creative\nwork reinforces the teleology of carceral surveillance systems. How might\ncreative engagements activate what Ruha Benjamin calls “a liberatory\nimagination,” where the goal is to illuminate or circumvent these mechanisms\nand envisage a more just, egalitarian, and vibrant world?iv\n\n![c22-fig-0158.jpg](../images/c22-fig-0158.jpg)\n\n158\\. In partnership with the London 2012 Olympic and Paralympic Games, Memo\nAkten and Davide Quayola developed _Forms_ , a series of abstract 3D\nvisualizations of athletes’ trajectories and mechanics.\n\n![c22-fig-0159.jpg](../images/c22-fig-0159.jpg)\n\n159\\. Referencing the utopian formalism of 1960s architecture, _Walking City_\n(2014) by Universal Everything is a “slowly evolving video sculpture”\nstructured around a cycle of human locomotion.\n\n![c22-fig-0160.jpg](../images/c22-fig-0160.jpg)\n\n160\\. While students at the University of Paris 8, Sophie Daste, Karleen\nGroupierre, and Adrien Mazaud developed _Miroir_ (2011), an interactive\ninstallation in which a spectator sees a ghostly animal head superimposed onto\nthe reflection of their own face. The anthropomorphized double closely follows\ntheir movements and expressions.\n\n![c22-fig-0161.jpg](../images/c22-fig-0161.jpg)\n\n161\\. _Más Que la Cara_ (2016), a street-level interactive installation by\nYesYesNo, presents imaginative, poster-like interpretations of spectators’\nfaces.\n\n## Additional Projects\n\n  * Jack Adam, _Tiny Face_ , 2011, face measurement app.\n  * Rebecca Allen, _Catherine Wheel_ , 1982, computer-generated character.\n  * Hayden Anyasi, _StandardEyes_ , 2016, interactive art installation.\n  * Nobumichi Asai, Hiroto Kuwahara, and Paul Lacroix, _OMOTE_ , 2014, real-time tracking and facial projection mapping.\n  * Jeremy Bailey, _The Future of Marriage_ , 2013, software.\n  * Jeremy Bailey, _The Future of Television_ , 2012, software demo.\n  * Jeremy Bailey, _Suck & Blow Facial Gesture Interface Test #1_, 2014, software test.\n  * Jeremy Bailey and Kristen D. Schaffer, _Preterna_ , 2016, virtual reality experience.\n  * Zach Blas, _Facial Weaponization Suite_ , 2011–2014, masks digitally modeled from aggregate data.\n  * Nick Cave, _Sound Suits_ , 1992–, wearable sculptures.\n  * A. M. Darke, _Open Source Afro Hair Library_ , 2020, 3D model database.\n  * Marnix de Nijs, _Physiognomic Scrutinizer_ , 2008, interactive installation.\n  * Arthur Elsenaar, _Face Shift_ , 2005, live performance and video.\n  * William Fetter, _Boeing Man_ , 1960, 3D computer graphic.\n  * William Forsythe, _Improvisation Technologies_ , 1999, rotoscoped video series.\n  * Daniel Franke and Cedric Kiefer, _unnamed soundsculpture_ , 2012, virtual sculpture using volumetric video data.\n  * Tobias Gremmler, _Kung Fu Motion Visualization_ , 2016, motion data visualization.\n  * Paddy Hartley, _Face Corset_ , 2002–2013, speculative fashion design.\n  * Adam Harvey, _CV Dazzle_ , 2010–, counter-surveillance fashion design.\n  * Max Hawkins, _FaceFlip_ , 2011, video chat add-on.\n  * Lingdong Huang, _Face-Powered Shooter_ , 2017, facially controlled game.\n  * Jack Kalish, _Sound Affects_ , 2012, face-controlled instruments and performance.\n  * Keith Lafuente, _Mark and Emily_ , 2011, video.\n  * Béatrice Lartigue and Cyril Diagne, _Les Métamorphoses de Mr. Kalia_ , 2014, interactive installation.\n  * David Lewandowski, _Going to the Store_ , 2011, digital animation and video footage.\n  * Zach Lieberman, _Walk Cycle / Circle Study_ , 2016, computer graphic animation.\n  * Rafael Lozano-Hemmer, _The Year's Midnight_ , 2010, interactive installation.\n  * Lauren McCarthy and Kyle McDonald, _How We Act Together_ , 2016, participatory online performance.\n  * Kyle McDonald, _Sharing Faces_ , 2013, interactive installation.\n  * Christian Moeller, _Cheese_ , 2003, smile analysis software and video installation.\n  * Nexus Studio, _Face Pinball_ , 2018, game with facial input.\n  * Klaus Obermaier with Stefano D’Alessio and Martina Menegon, _EGO_ , 2015, interactive installation.\n  * Orlan, _Surgery-Performances_ , 1990–1993, surgical operations as performance.\n  * Joachim Sauter and Dirk Lüsebrink, _Iconoclast / Zerseher_ , 1992, eye-responsive installation.\n  * Oskar Schlemmer, _Slat Dance_ , 1928, ballet.\n  * Karolina Sobecka, _All the Universe Is Full of the Lives of Perfect Creatures_ , 2012, interactive mirror.\n  * Elliott Spelman, _Expressions_ , 2018, face-controlled computer input system.\n  * Keijiro Takahashi, _GVoxelizer_ , 2017, animation software tool.\n  * Universal Everything, _Furry's Posse_ , 2009, digital animation.\n  * Camille Utterback, _Entangled_ , 2015, interactive generative projection.\n  * Theo Watson, _Autosmiley_ , 2010, whimsical vision-based keyboard automator.\n  * Ari Weinkle, _Moodles_ , 2017, animation.\n\n## Readings\n\n  1. Greg Borenstein, “Machine Pareidolia: Hello Little Fella Meets Facetracker,” _Ideas for Dozens_ (blog), [UrbanHonking.com](http://UrbanHonking.com), January 14, 2012.\n  2. Joy Buolamwini and Timnit Gebru, _Gender Shades_ , 2018, research project, dataset, and thesis.\n  3. Kate Crawford and Trevor Paglen, “Excavating AI: The Politics of Images in Machine Learning Training Sets, excavating.ai, September 19, 2019.\n  4. Regine Debatty, “The Chronocyclegraph,” _We Make Money Not Art_ (blog), May 6, 2012.\n  5. Söke Dinkla, “The History of the Interface in Interactive Art,” [kenfeingold.com](http://kenfeingold.com), accessed April 17, 2020.\n  6. Paul Gallagher, “It's Murder on the Dancefloor: Incredible Expressionist Dance Costumes from the 1920s,” [DangerousMinds.net](http://DangerousMinds.net), May 30, 2019.\n  7. Ali Gray, “A Brief History of Motion-Capture in the Movies,” _IGN_ , July 11, 2014.\n  8. Katja Kwastek, _Aesthetics of Interaction in Digital Art_ (Cambridge, MA: MIT Press, 2013).\n  9. Daito Manabe, “Human Form and Motion,” GitHub, updated July 5, 2018.\n  10. Kyle McDonald, “Faces in Media Art,” GitHub repository for Appropriating New Technologies (NYU ITP), updated July 12, 2015.\n  11. Kyle McDonald, “Face as Interface,” 2017, workshop.\n  12. Jason D. Page, “History,” [LightPaintingPhotography.com](http://LightPaintingPhotography.com), accessed April 17, 2020.\n  13. Shreeya Sinha, Zach Lieberman, and Leslye Davis, “A Visual Journey Through Addiction,” _New York Times_ , December 18, 2018.\n  14. Scott Snibbe and Hayes Raffle, “Social Immersive Media: Pursuing Best Practices for Multi-User Interactive Camera/Projector Exhibits,” in _Proceedings of the SIGCHI Conference on Human Factors in Computing Systems_ (New York: Association for Computing Machinery, 2009).\n  15. Nathaniel Stern, _Interactive Art and Embodiment: The Implicit Body as Performance_ (Canterbury, UK: Gylphi Limited, 2013).\n  16. Alexandria Symonds, “How We Created a New Way to Depict Addiction Visually,” _New York Times_ , December 20, 2018.\n  17. Alan Warburton, _Goodbye Uncanny Valley_ , 2017, animation.\n\n## Notes\n\ni Nathan Ferguson, “2019: A Face Odyssey,” _Cyborgology_ , July 17, 2019,\naccessed July 27, 2019. ii Alan Warburton, “Fairytales of Motion,” Tate\nExchange video essay, April 24, 2019, accessed July 27, 2019. iii See the Face\nGenerator assignment. iv Ruha Benjamin, ed., _Captivating Technology: Race,\nCarceral Technoscience, and Liberatory Imagination in Everyday Life_ (Durham,\nNC: Duke University Press, 2019), 12.\n\n\n# Synesthetic Instrument\n\nA machine for performing sound and image\n\n![c23-fig-0162.jpg](../images/c23-fig-0162.jpg)\n\n162\\. In the Web-based performance _In C_ (2015) by Luisa Pereira, Yotam Mann,\nand Kevin Siwoff, a group of performers use mice and keyboards to control the\npace at which their individual instruments advance through a graphical score,\nproducing highly variable musical results.\n\n## Brief\n\nCreate an “audiovisual instrument” that allows a performer to produce tightly\ncoupled sound and visuals. Your software should make possible the creation of\nboth dynamic imagery and noise/sound/music, simultaneously, in real time.\n\nYou are challenged to create an open-ended system in which sonic and visual\nmodalities are equally expressive. Its results should be inexhaustible, deeply\nvariable, and contingent on the performer's choices, and the basic principles\nof its operation should be easy to deduce, yet also allow for sophisticated\nexpression. Interactions with your instrument should generate predictable\nresults.\n\nAssume your instrument receives input from the actions and gestures of a\nperformer. Will they use a keyboard, mouse, multi-touch trackpad, pose\ntracker, or a less common sensor? Select (or construct) your instrument's\nphysical interface with care, giving consideration to its expressive\naffordances. Categorize the data streams it provides: are these continuous\nvalues, or changes in logical states? Do they have a perceptible duration, and\nare they persistent or instantaneous? Are they one-, two-, three-, or four-\ndimensional?\n\nAssume your instrument generates output for a graphic display and audio\nsystem. Think through the possibilities offered by different visual variables\nlike hue, saturation, texture, shape, and motion, and different auditory\nelements including pitch, dynamics, timbre, scales, and rhythm. Link these\nsonic and visual elements together by establishing _mappings_ between your\nsystem's input and output. For example: the faster the performer moves her\ncursor, the brighter her cursor appears, and the higher the pitch of a\nsynthesized tone. Such mappings help set up expectations that can be\nmanipulated by a performer to create contrast, tension, surprise, and even\nhumor.\n\nAttune us to your instrument's unique expressive qualities by using it in a\nbrief performance. One suggestion for structuring this performance is to begin\nwith an expository demonstration of your instrument's interface, guiding the\naudience into understanding how it operates before presenting more complex\nmaterial. The repetition and subsequent elaboration of themes is also a\nhelpful compositional strategy.\n\n## Learning Objectives\n\n  * Review historical precedents in audiovisual instrument design\n  * Explore ways to control and connect sound and visuals\n  * Review and implement event-driven programming\n  * Apply interaction design principles to the development of performance instruments, with special attention to a system's responsiveness, predictability, ease of use, and interface\n\n## Variations\n\n  * _Instructors: require all students to use the same type of physical interface (such as a game controller, pressure-sensitive stylus, or barcode reader). This will allow them to better contrast their solutions._\n  * Develop a “QWERTY instrument” that is wholly played through typed actions on a standard computer keyboard. This simplified format can helpfully limit your design to the use of discrete inputs (button presses, rather than continuous gestures), and discrete outputs (triggering pre-recorded and pre-rendered media, rather than synthesizing sounds and graphics through the real-time modulation of continuous parameters). Embed your project in a web page and publish it online. Consider how the provenance of your sounds and images can enrich your concept.\n  * _For advanced students:_ Develop your project with a pair of programming environments that handle sound and image separately. Some arts-engineering toolkits (like Max/MSP/Jitter, Pure Data, SuperCollider, and ChucK) excel at sound synthesis, while other development environments (like Processing, openFrameworks, Cinder, and Unity) have richer feature sets for graphics. Signals can be shared between the applications by means of a communications protocol like OSC or Syphon.\n  * Mapping performance gestures to your instrument's control parameters is perhaps the foremost design challenge of this assignment. Body movements that feel logically simple may produce hard-to-interpret sensor data; likewise, small changes to your instrument's control parameters may have nonlinear perceptual effects. Improve the intuitiveness of your instrument's mappings by incorporating a tool for real-time regression, such as Rebecca Fiebrink's Wekinator or Nick Gillian's Gesture Recognition Toolkit.\n  * Create an instrument that will only be used by one person. (This is at odds with commercial agendas and corporatized HCI education, in which the expectation is that design is something done for the widest range of users.)\n\n## Making It Meaningful\n\nGood instruments offer inexhaustible possibilities for expression,\ncomposition, and collaboration. To a performer, the value of an instrument\nhinges on how well it supports the creative feedback loop known as “flow.” i\nAn instrument that is responsive but crunchy can be more gratifying than a\ndevice that easily begets dazzling results. Hence, in this assignment, the\nemphasis is on the suppleness of an instrument's interaction design and the\nrange of expression it makes possible (the performer's experience), rather\nthan on the aesthetics of the audiovisuals it produces.\n\nA “North Star” for instrument design is to create something “instantly\nknowable, yet infinitely masterable.” ii Consider the pencil, or the piano:\nits basic principles of operation are simple enough for a child to deduce, yet\none can spend a lifetime using it and still find more to say; sophisticated\nexpressions are possible, and mastery is elusive. From the standpoint of\nsystems design, our challenge is that ease of learning and expressive range\nare antithetical design requirements: optimize for one, and the other suffers.\nIn making an instrument for simultaneous sound and image, this challenge is\ncompounded by another: expressive malleability in one modality often comes at\nthe expense of rigidity in the other.\n\nIt is helpful to consider the typology of audiovisual systems and the design\nstrategies they use. Sound visualization, for example, is a common feature in\ndesktop music players, VJ software, and phonology tools. Principles of image\nsonification underpin film scores, game music, and some tools for the visually\nimpaired. The term “visual music,” used by creators of some color organs and\nabstract films, can even refer to a strictly silent medium, one comprised\nsolely of animated imagery with temporal structures that are analogous to\nmusical ones. In the realm of computer-based performance systems, designers\nhave used a variety of visual interface metaphors to control and represent\nsound. “Control panel” interfaces, for example, use knobs, sliders, buttons,\nand dials to govern synthesis parameters, evoking the look of vintage\nsynthesizers. “Diagrammatic” interfaces, such as scores and timelines, use the\ngraphical conventions of information visualization to organize representations\nof sound along axes like time and frequency. Others use “audiovisual objects,”\nwherein a performer stretches, manipulates, or knocks virtual objects together\nin order to trigger or modulate corresponding sounds. In “painterly\ninterfaces,” gestural marks performed on a 2D surface conjure and influence a\nfine-grained aural material.\n\n![c23-fig-0163.jpg](../images/c23-fig-0163.jpg)\n\n163\\. Amit Pitaru's _Sonic Wire Sculptor_ (2003) is a tablet-based system for\nauthoring looping scores. Melodies are represented by drawings that curl\naround a cylindrical 3D space.\n\n![c23-fig-0164.jpg](../images/c23-fig-0164.jpg)\n\n164\\. _Psychic Synth_ (2014) by Pia Van Gelder is a responsive audiovisual\nenvironment in which a participant's brainwaves are captured by an EEG\nheadset, in order to establish an immersive biofeedback loop that governs\nvideo projection, colored light, and immersive sound.\n\n![c23-fig-0165.jpg](../images/c23-fig-0165.jpg)\n\n165\\. _Patatap_ (2012) by Jono Brandel and Lullatone is a browser-based app\nfor the simultaneous performance of sound and animated imagery. Each key on a\nstandard computer keyboard triggers a unique noise and snappy animation.\n\n![c23-fig-0166.jpg](../images/c23-fig-0166.jpg)\n\n166\\. Jace Clayton's _Sufi Plug Ins_ (2012) are a suite of seven free tools\nthat extend the functionality of Ableton Live, a commercial music software\nsequencer. These software-as-art visual interfaces are designed to support\nnon-Western conceptions of sound, such as North African maqam scales and\nquartertone tuning. The interface is written in the Berber language of\nTamazight, using its neo-Tifinagh script.\n\n![c23-fig-0167.jpg](../images/c23-fig-0167.jpg)\n\n167\\. _Orca_ (2018) by Hundred Rabbits is an esoteric programming language and\nlive-coding interface for creating and performing procedural sound sequencers.\n\n## Additional Projects\n\n  * Louis-Bertrand Castel, _Clavecin Oculaire (Ocular Harpsichord)_ , 1725–1740, proposed mechanical audiovisual instrument.\n  * Alex Chen and Yotam Mann, _Dot Piano_ , 2017, online musical instrument.\n  * Rebecca Fiebrink, _Wekinator_ , 2009, machine learning software for building interactive systems.\n  * Google Creative Lab, _Semi-Conductor_ , 2018, gesture-driven online virtual orchestra.\n  * Mary Elizabeth Hallock-Greenewalt, _Sarabet_ , 1919–1926, mechanical audiovisual synthesizer.\n  * Imogen Heap et al., _Mi.Mu Gloves_ , 2013–2014, gloves as gestural control interface.\n  * Toshio Iwai, _Piano – As Image Media_ , 1995, interactive installation with grand piano and projection.\n  * Toshio Iwai and Maxis Software Inc., _SimTunes_ , 1996, interactive audiovisual performance and composition game.\n  * Sergi Jordà et al., _ReacTable_ , 2003–2009, software-based audiovisual instrument.\n  * Frederic Kastner, _Pyrophone_ , 1873, flame-driven pipe organ.\n  * Erkki Kurenniemi, _DIMI-O_ , 1971, electronic audio-visual synthesizer.\n  * Golan Levin and Zach Lieberman, _The Manual Input Workstation_ , 2004, audiovisual performance with interactive software.\n  * Yotam Mann, _Echo_ , 2014, musical puzzle game, website, and app.\n  * JT Nimoy, _BallDroppings_ , 2003–2009, animated musical game for Chrome.\n  * Daphne Oram, _Oramics Machine_ , 1962, photo-input synthesizer for “drawing sound.”\n  * Allison Parrish, _New Interfaces for Textual Expression_ , 2008, series of textual interfaces.\n  * Gordon Pask and McKinnon Wood, _Musicolour Machine_ , 1953–1957, performance system connecting audio input with colored lighting output.\n  * James Patten, _Audiopad_ , 2002, software instrument for electronic music composition and performance.\n  * David Rokeby, _Very Nervous System_ , 1982–1991, gesturally controlled software instrument using computer vision.\n  * Laurie Spiegel, _VAMPIRE (Video and Music Program for Interactive Realtime Exploration/Experimentation)_ , 1974–1979, software instrument for audiovisual composition.\n  * Iannis Xenakis, _UPIC_ , 1977, graphics tablet input device for controlling sound.\n\n## Readings\n\n  1. Adriano Abbado, “Perceptual Correspondences of Abstract Animation and Synthetic Sound,” _Leonardo_ 21, no. 5 (1988): 3–5.\n  2. Dieter Daniels et al., eds., _See This Sound: Audiovisuology: A Reader_ (Cologne, Germany: Walther Koenig Books, 2015).\n  3. Sylvie Duplaix et al., _Sons et Lumieres: Une Histoire du Son dans L’art du XXe Siecle_ (Paris: Editions du Centre Pompidou, Catalogues Du M.N.A.M., 2004).\n  4. Michael Faulkner (D-FUSE), _vj audio-visual art + vj culture_ (London: Laurence King Publishing Ltd., 2006).\n  5. Mick Grierson, “Audiovisual Composition” (DPhil thesis, University of Kent, 2005).\n  6. Thomas L. Hankins and Robert J. Silverman, _Instruments and the Imagination_ (Princeton, NJ: Princeton University Press, 1995).\n  7. Roger Johnson, _Scores: An Anthology of New Music_ (New York: Schirmer/Macmillan, 1981).\n  8. Golan Levin, “Audiovisual Software Art: A Partial History,” in _See This Sound: Audiovisuology: A Reader_ , ed. Dieter Daniels et al. (Cologne, Germany: Walther Koenig Books, 2015).\n  9. Luisa Pereira, “Making Your Own Musical Instruments with P5.js, Arduino, and WebMIDI,” [Medium.com](http://Medium.com), October 23, 2018.\n  10. Martin Pichlmair and Fares Kayali, “Levels of Sound: On the Principles of Interactivity in Music Video Games,” in _Proceedings of the 2007 DiGRA International Conference: Situated Play_ , vol. 4 (2007): 424–430.\n  11. Don Ritter, “Interactive Video as a Way of Life,” _Musicworks_ 56 (Fall 1993): 48–54.\n  12. Maurice Tuchman and Judi Freeman, _The Spiritual in Art: Abstract Painting, 1890–1985_ (Los Angeles: Los Angeles County Museum of Art, 1986).\n  13. John Whitney, _Digital Harmony: On the Complementarity of Music and Visual Art_ (New York: Byte Books/McGraw-Hill, 1980).\n  14. John Whitney, “Fifty Years of Composing Computer Music and Graphics: How Time's New Solid-State Tactability Has Challenged Audio Visual Perspectives,” _Leonardo_ 24, no. 5 (1991): 597–599.\n\n## Notes\n\ni Mihaly Czikszentmihalyi, _Flow: The Psychology of Optimal Experience_ (New\nYork: Harper Perennial Modern Classics, 2008). ii Golan Levin, “Painterly\nInterfaces for Audiovisual Performance” (master's thesis, MIT, 2000).\n\n\n# Computing without a Computer\n\n![c24-fig-5001.jpg](../images/c24-fig-5001.jpg)\n\n**Ono's _Grapefruit_**\n\nBrowse the prompts in Yoko Ono's _Grapefruit_ series, and, if possible,\nexecute one.1 Contemplate giving instructions as a mode of creative practice.\nDevise an instructional painting in Ono's style.\n\n![c24-fig-5002.jpg](../images/c24-fig-5002.jpg)\n\n**_Wall Drawing_ _#__118_**\n\nExecute Sol LeWitt's _Wall Drawing #118_ (1971): “On a wall surface, any\ncontinuous stretch of wall, using a hard pencil, place fifty points at random.\nThe points should be evenly distributed over the area of the wall. All of the\npoints should be connected by straight lines.” 2\n\n![c24-fig-5003.jpg](../images/c24-fig-5003.jpg)\n\n**Conditional Design: The Beach**\n\nOrganize into groups of four, and give each person their own color marker.\nExecute “The Beach” from the “Conditional Design Manifesto” by Luna Maurer et\nal.: “Each turn, find the most empty space on the paper and place a dot in the\nmiddle of it.” 3\n\n![c24-fig-5004.jpg](../images/c24-fig-5004.jpg)\n\n**Procedural Drawing**\n\nDevelop your own procedural drawing rule set in the spirit of Sol LeWitt or\nthe prompts from [conditionaldesign.org](http://conditionaldesign.org). Have\none or more of your peers produce a drawing with your system. Consider\ndevising systems using other materials such as tape or string.\n\n![c24-fig-5005.jpg](../images/c24-fig-5005.jpg)\n\n**Drawing Games**\n\nIn pairs, play a game of _Dots and Boxes_ (by Édouard Lucas) and _Sprouts_ (by\nJohn H. Conway and Michael S. Paterson) to deepen your understanding of rule-\nbased drawing games.4\n\n![c24-fig-5006.jpg](../images/c24-fig-5006.jpg)\n\n**Zoom Schwartz Profigliano**\n\nIn groups of five, play the rule-based conversation game “Zoom Schwartz\nProfigliano,” in which an ever-expanding vocabulary of whimsical nonsense\nwords establishes precise rules for what can be spoken, to whom, when, and\nhow.5 (Photo: College of DuPage.)\n\n![c24-fig-5007.jpg](../images/c24-fig-5007.jpg)\n\n**Be the Computer I**\n\nArrange the class in a grid configuration and give each person a sheet of\npaper so that each student is responsible for a pixel. One person takes charge\nand programs the pixels by showing the group a script or by giving them direct\ncommands.6\n\n![c24-fig-5008.jpg](../images/c24-fig-5008.jpg)\n\n**Be the Computer II**\n\nWrite a simple program to create a static drawing. Give your code to a peer,\nand (without showing them your screen) ask them to predict and hand-draw the\nresult. When they are finished, compare the computer's drawing with their\nhand-drawn work.\n\n![c24-fig-5009.jpg](../images/c24-fig-5009.jpg)\n\n**Human Fax Machine**\n\nIn groups of two or four, devise a sound language for describing how marks are\nmade. You may use a sound-making device (two spoons, a set of keys) or your\nown mouth noises, so long as you use no spoken words. Write down the code,\nthen split your group into “transmitters” and “receivers,” with a visual\nbarrier in between. Test the system. A “transmitter” should take a simple\nhand-drawn image and transmit it across the barrier to the “receiver.” When\nfinished, compare the original with the transmitted image and fix any problems\nin your system. Do this with several images, and discuss.7\n\n## Additional References\n\n  1. Casey Reas, _{Software} Structures_ , 2004, <http://artport.whitney.org/commissions/softwarestructures/text.html>.\n  2. Basil Safwat, _Processing.A4_ , 2013, <http://www.basilsafwat.com/projects/processing.a4/>.\n  3. FoAM, notes for Mathematickal Arts workshop, 2011, <https://libarynth.org/mathematickal_arts_2011>.\n  4. J. Meejin Yoon, “Serial Notations / Drift Drawings,” 2003, <https://ocw.mit.edu/courses/architecture/4-123-architectural-design-level-i-perceptions-and-processes-fall-2003/assignments/problem1.pdf>.\n\n## Notes\n\n1 Yoko Ono, _Grapefruit_ (London: Simon & Schuster, 2000). 2 Andrew Russeth,\n“Here Are the Instructions for Sol LeWitt's 1971 Wall Drawing for the School\nof the MFA Boston,” Observer, October 1, 2012,\n<https://observer.com/2012/10/here-are-the-instructions-for-sol-\nlewitts-1971-wall-drawing-for-the-school-of-the-mfa-boston/>. 3 Luna Maurer,\nEdo Paulus, Jonathan Puckey, and Roel Wouters, “Conditional Design: A\nManifesto for Artists and Designers,” accessed April 14, 2020,\n<https://conditionaldesign.org/workshops/the-beach/>. 4 Wikipedia, “Dots-and-\nBoxes,” <http://en.wikipedia.org/wiki/Dots_and_Boxes>; Wikipedia, “Sprouts,”\n<http://en.wikipedia.org/wiki/Sprouts_(game)>. 5 David King, “Zoom Schwartz\nProfigliano,” 1998, <https://www.scottpages.net/ZSP-Rules-2012.pdf>. 6 John\nMaeda, _Creative Code_ (New York: Thames and Hudson, 2004), 216. 7 Brogan Bunt\nand Lucas Ihlein, “The Human Fax Machine Experiment,” _Scan (Sydney): Journal\nof Media Arts Culture_ 10, no. 2 (2013): 1–26.\n\n\n# Graphic Elements\n\n![c25-fig-5001.jpg](../images/c25-fig-5001.jpg)\n\n**One with Everything**\n\nExplore your graphics toolset by drawing one of each type of primitive it\nprovides. For example, you might draw a rectangle, ellipse, arc, line segment,\nBézier curve, polyline, and polygon. Experiment with their options and\nparameters, such as fill color, stroke weight, etc.\n\n![c25-fig-5002.jpg](../images/c25-fig-5002.jpg)\n\n**Quadrilateral Zoo**\n\nWrite commands to plot the vertices of a family of quadrilaterals: square,\nrectangle, parallelogram, rhombus, trapezoid, dart, and kite.\n\n![c25-fig-5003.jpg](../images/c25-fig-5003.jpg)\n\n**Draw Your Initials**\n\nDraw your initials with primitive shapes and lines.\n\n![c25-fig-5004.jpg](../images/c25-fig-5004.jpg)\n\n**Braille Tool**\n\nReproduce the Braille alphabet using filled and unfilled circles. If you can,\nstore a representation of these patterns in an array, and create a tool that\nallows a user to compose, print, and (with a stylus) emboss Braille messages.\n\n![c25-fig-5005.jpg](../images/c25-fig-5005.jpg)\n\n**Coding Mondrian**\n\nUsing code, reproduce a Mondrian painting, such as Composition No. III, with\nRed, Blue, Yellow and Black (1929). Pay attention to detail.\n\n![c25-fig-5006.jpg](../images/c25-fig-5006.jpg)\n\n**Coding _Stadia II_**\n\nSelect and crop a small rectangular region from Julie Mehretu's painting\n_Stadia II_. Using a program such as Photoshop, read out the colors and\ncoordinate data from this fragment. Use this data to faithfully recreate the\nfragment with shapes, lines, curves, and custom shape functions.\n\n![c25-fig-5007.jpg](../images/c25-fig-5007.jpg)\n\n**Draw, Then Code**\n\nSpend 20 minutes drawing on paper: a self-portrait, landscape, still life, or\ngeometric design. Create your drawing with adequate care and detail. Now\nrecreate your drawing using code. (Image: p5.js Self-Portrait, a student\nproject by Zainab Aliyu, 2015.)\n\n![c25-fig-5008.jpg](../images/c25-fig-5008.jpg)\n\n**Kaleidoscope**\n\nDevise a small graphical motif. Write a program that uses your toolset's\ngraphics transforms to translate, reflect, and rotate copies of this motif, in\nthe manner of a kaleidoscope.\n\n\n# Iteration\n\n![c26-fig-5001.jpg](../images/c26-fig-5001.jpg)\n\n**Simple Iteration: Seven Circles**\n\nUse iteration to copy the figure on the left, in which seven circles are\npositioned across the canvas. The position of each circle should be computed\nusing your loop's counting variable. Make sure the first circle is inset by a\nmargin; it should not lie on the edge of the canvas.\n\n![c26-fig-5002.jpg](../images/c26-fig-5002.jpg)\n\n**Transitioning Rectangles**\n\nRecall that _any_ visual property can be linked to a loop variable, not just\nposition. Use iteration to generate a series of rectangles. Your code should\nsimultaneously control several of the rectangles’ visual properties, including\ntheir position, height, and fill color.\n\n![c26-fig-5003.jpg](../images/c26-fig-5003.jpg)\n\n**String Art Challenge**\n\nUse iteration to recreate the figure on the left. Your code should draw\nexactly eight lines.\n\n![c26-fig-5004.jpg](../images/c26-fig-5004.jpg)\n\n**Mini-Calendar**\n\nUse iteration to render a row of visual elements: one element for each of the\ndays of the current month. All of the elements should be drawn identically,\n_except_ for the one whose index corresponds to the current day. Differentiate\nthis element in some way.\n\n![c26-fig-5005.jpg](../images/c26-fig-5005.jpg)\n\n**Receding Landscape**\n\nUse iteration to create a series of vertical lines across the screen. Create\nthe illusion of a receding landscape by placing the lines’ endpoints more\nclosely together at the top of the canvas.\n\n![c26-fig-5006.jpg](../images/c26-fig-5006.jpg)\n\n**Lines to the Cursor**\n\nUse iteration to create an interactive display featuring a series of ten\nlines. Each line should connect the cursor to one of a series of points\ndistributed evenly across the canvas.\n\n![c26-fig-5007.jpg](../images/c26-fig-5007.jpg)\n\n**Color-Bar Gradient**\n\nUsing iteration, generate a gradient that “lerps” (linearly interpolates) from\none fill color to another across exactly 17 rectangles. Implement some code\nthat randomizes the two terminal colors whenever the user clicks a button.\n\n![c26-fig-5008.jpg](../images/c26-fig-5008.jpg)\n\n**Dashed Line**\n\nWrite a program that generates a dashed line between two points. (You may not\nuse a “ready-made” dashed line.) Your dashes should always have a fixed\nlength, such as 10 pixels, so that longer lines require more dashes. Connect\none of your line's endpoints to the cursor.\n\n![c26-fig-5009.jpg](../images/c26-fig-5009.jpg)\n\n**Nested Iteration: Checkers**\n\nCreate a checkerboard using a nested loop. Recall that a checkerboard is an\n8x8 grid of alternating black and white squares, starting with white in the\ntop left corner.\n\n![c26-fig-5010.jpg](../images/c26-fig-5010.jpg)\n\n**Iteration with Functions**\n\nWrite a function that encapsulates the code to render a simple visual element\n(a leaf, face, etc.). Give your function arguments that determine where the\nelement will be positioned. Using iteration, call your function to display a\ngrid of these elements.\n\n![c26-fig-5011.jpg](../images/c26-fig-5011.jpg)\n\n**Stochastic Elements**\n\nCreate an evocative composition in which an iterative loop deposits small\nelements in random locations around the canvas. These elements might look like\ncraters, potholes, pimples, ants, chocolate chips, holes in Swiss cheese…\n\n![c26-fig-5012.jpg](../images/c26-fig-5012.jpg)\n\n**Interrupted Grid**\n\nUse nested iteration to generate a grid of visual elements. Write code such\nthat on each iteration, with a small random probability, an alternative\nelement is occasionally drawn instead.\n\n![c26-fig-5013.jpg](../images/c26-fig-5013.jpg)\n\n**Geometric Progression**\n\nGenerate a series of visual elements whose dimensions exhibit a geometric\nprogression. The size of each element should be computed by multiplying the\nsize of the one before it by a constant ratio. In the design at left, each\ncircle is 1.3 times larger than the previous one inside it.\n\n![c26-fig-5014.jpg](../images/c26-fig-5014.jpg)\n\n**Moiré Patterns**\n\nGenerate a set of parallel lines or curves, spaced at narrow intervals.\nOverlap two copies of your line set, differing by a small rotation, to create\na moiré pattern. Place some dimension of variability, such as the line\nseparation or rotation angle, under interactive control.\n\n![c26-fig-5015.jpg](../images/c26-fig-5015.jpg)\n\n**Recoding _Schotter_**\n\n_Schotter_ by Georg Nees (1968) is a classic algorithmic artwork that depicts\na gradient from order to chaos across a 12x22 grid of squares. In it, the\nsquares’ orientations become increasingly randomized towards the bottom of the\npage. Re-code this work, paying attention to detail.\n\n![c26-fig-5016.jpg](../images/c26-fig-5016.jpg)\n\n**Hexagonal Grid**\n\nWrite a custom function that draws a hexagon. Using iteration, call this\nfunction to fill your sketch with a grid of hexagons. For regular hexagons,\nyou may need to consult some trigonometry.\n\n\n# Color\n\n![c27-fig-5001.jpg](../images/c27-fig-5001.jpg)\n\n**Color Observation**\n\nCarefully observe the color of your shirt, your table, the wall of your room,\nand the palm of your hand. Proceeding exclusively by modifying numbers in\ncode, and without using a camera or scanning device, reproduce these colors to\nthe best of your ability.\n\n![c27-fig-5002.jpg](../images/c27-fig-5002.jpg)\n\n**Overlapping Color**\n\nOverlap three semi-transparent circles, each with a different color, to create\nregions of overlapping colors. Explore the effects of different colors and\ntransparency values. Experiment with different pixel transfer modes (also\ncalled “blend modes”). Draw the circles without outlines.\n\n![c27-fig-5003.jpg](../images/c27-fig-5003.jpg)\n\n**Constructing a Gradient**\n\nCreate a smooth gradient between two colors. _Hint_ : use iteration to render\nmany thin, adjacent parallel lines, each with subtly different colors. How\ndoes your choice of color model (RGB or HSB) affect the result?\n\n![c27-fig-5004.jpg](../images/c27-fig-5004.jpg)\n\n**Color Wheel**\n\nUsing the HSB color model, create a program that displays every hue on the\nscreen in the form of a color wheel.\n\n![c27-fig-5005.jpg](../images/c27-fig-5005.jpg)\n\n**Threshold of Perception**\n\nCreate a composition showing the smallest interval between colors that you can\ndistinguish. One possible composition is a filled circle in front of a flat\nbackground, such that the colors of the background and circle are almost\nimperceptibly different.\n\n![c27-fig-5006.jpg](../images/c27-fig-5006.jpg)\n\n**Interactive Complement**\n\nComplementary colors are 180° degrees apart on the color wheel. Split the\ncanvas into two equal-sized rectangles, side by side. Using the HSB color\nmodel, create an interaction in which one rectangle's color is controlled by\nthe cursor position and the other rectangle contains the complement of this\ncolor.\n\n![c27-fig-5007.jpg](../images/c27-fig-5007.jpg)\n\n**Accented Palette**\n\nCreate a color scheme or palette with a dominant “base color” and an “accent\ncolor” formed from its complement. Devise a method of randomly sampling this\npalette such that the base color is selected roughly 75% of the time. Fill a\ngrid of squares with colors selected using this method.\n\n![c27-fig-5008.jpg](../images/c27-fig-5008.jpg)\n\n**Split Complements**\n\nA color's “split complements” are a pair of colors just adjacent (±15–30°) on\nthe color wheel. Create an interactive sketch that displays swatches of the\nsplit complements for a color selected by the user.\n\n![c27-fig-5009.jpg](../images/c27-fig-5009.jpg)\n\n**Josef Albers Color Relativity I**\n\nStudy Josef Albers's color relativity exercises and write a program, like the\none shown at left, that makes three colors look like four.1 Control the\ncomponents of the third color (in the spots) using your cursor. Under what\nconditions do the spots look the same or different?\n\n![c27-fig-5010.jpg](../images/c27-fig-5010.jpg)\n\n**Josef Albers Color Relativity II**\n\nUsing similar techniques as above, create a sketch in which four colors look\nlike three. Demonstrate the relativity of color by duplicating the spots’\ncolors in a location where they can be more easily compared. Can you write a\nprogram to generate novel color sets that always fulfill the four-look-like-\nthree condition?\n\n![c27-fig-5011.jpg](../images/c27-fig-5011.jpg)\n\n**Color Inspector**\n\nLoad and display a colorful image. Draw three ellipses to the screen and fill\nthem with the red value, the green value, and the blue value, respectively, of\nan image pixel under the cursor.\n\n![c27-fig-5012.jpg](../images/c27-fig-5012.jpg)\n\n**Color Survey**\n\nCreate an interface for specifying a color, using sliders to control its red,\ngreen, and blue components. Ask some friends to use your tool to create the\ncolors mauve, teal, and plum. Save the data they create. In another sketch,\nload and display this data to compare their opinions.\n\n![c27-fig-5013.jpg](../images/c27-fig-5013.jpg)\n\n**Palette from Photo**\n\nSelect a photo. Write a program that derives an optimal five-color palette\nfrom your chosen image. (There are many ways to do this, but one of the most\neffective is k-means clustering.) If you can, use your palette to generate\n“separations” of your original image, like for silkscreening or risograph\nprinting.\n\n## Additional References\n\n  1. Tauba Auerbach, _RGB Colorspace Atlas_ (2011), <http://taubaauerbach.com/view.php?id=286>.\n  2. Carolyn L. Kane, _Chromatic Algorithms: Synthetic Color, Computer Art, and Aesthetics after Code_ (Chicago: University of Chicago Press, 2014).\n  3. Rune Madsen, <http://printingcode.runemadsen.com/lecture-color/>.\n  4. Rune Madsen, <https://programmingdesignsystems.com/color/color-models-and-color-spaces/index.html>.\n  5. Rune Madsen, <https://programmingdesignsystems.com/color/perceptually-uniform-color-spaces/index.html>.\n  6. Robert Simmon, “Subtleties of Color,” NASA Earth Observatory: Elegant Figures, last modified August 5, 2013, <https://earthobservatory.nasa.gov/blogs/elegantfigures/2013/08/05/subtleties-of-color-part-1-of-6/>.\n\n## Note\n\n1 See Ticha Sethapakdi, _The Colorist Cookbook_ (self-pub., 2015),\n<https://strangerbedfellows.files.wordpress.com/2015/12/the-colorist-\ncookbook.pdf>.\n\n\n# Conditional Testing\n\n![c28-fig-5001.jpg](../images/c28-fig-5001.jpg)\n\n**Left or Right**\n\nCreate a sketch in which a text display indicates whether the cursor is on the\nleft side or the right side of the canvas.\n\n![c28-fig-5002.jpg](../images/c28-fig-5002.jpg)\n\n**Billiard Ball**\n\nCreate a sketch with a moving ball that bounces off the edges of the canvas.\nTake care that the ball never appears to overlap the edge of the canvas.\n\n![c28-fig-5003.jpg](../images/c28-fig-5003.jpg)\n\n**One-Person Pong**\n\nCreate a sketch that recreates the game of Pong for a single user. Can you add\na scoring system?\n\n![c28-fig-5004.jpg](../images/c28-fig-5004.jpg)\n\n**Choose Your Own Adventure**\n\nCreate a branching narrative experience in which clicking in different regions\nof the screen leads to different rooms, situations, and outcomes. Enhance the\nexperience with sound effects (e.g., door latches) and ambient audio\nrecordings.\n\n![c28-fig-5005.jpg](../images/c28-fig-5005.jpg)\n\n**State Machine I**\n\nPlace a white square on a gray background. Create an interaction wherein a\nclick inside the square turns it black, after which it stays that way. (In\nthis exercise and those that follow, make sure that clicks outside the square\nhave no effect.)\n\n![c28-fig-5006.jpg](../images/c28-fig-5006.jpg)\n\n**State Machine II**\n\nPlace a white square on a gray background. Create an interaction wherein each\nclick in the square flips its color. It should flip from white to black (if it\nis white) or from black to white (if it is black).\n\n![c28-fig-5007.jpg](../images/c28-fig-5007.jpg)\n\n**State Machine III**\n\nPlace a white square on a gray background. Create an interaction wherein two\nclicks in the square are required to turn it from black to white, but three\nclicks are required to turn it from white to black.\n\n![c28-fig-5008.jpg](../images/c28-fig-5008.jpg)\n\n**State Machine IV**\n\nPlace a white square on a gray background. Turn the square into a button with\na “hover” state: make it white when inactive, yellow when the user is hovering\nover it (without having clicked), and black when the user is actively holding\nthe mouse button down inside of it.\n\n\n# Unpredictability\n\n![c29-fig-5001.jpg](../images/c29-fig-5001.jpg)\n\n**Coin Toss**\n\nCreate a sketch for a coin that is “tossed” every time the mouse button is\nclicked. Your coin should be evenly weighted so that heads and tails have the\nsame likelihood of appearing. Test your program over ten tosses. What were the\nobserved frequencies of heads and tails?\n\n![c29-fig-5002.jpg](../images/c29-fig-5002.jpg)\n\n**Roll the Dice**\n\nCreate an app in which a virtual die is “rolled” every time the mouse is\nclicked. Your die should be evenly weighted, and each result should have an\neven 1-in-6 chance of appearing. Test your program over 18 clicks. How well\ndoes it perform? Modify your code so that it rolls six dice.\n\n![c29-fig-5003.jpg](../images/c29-fig-5003.jpg)\n\n**Exquisite Corpse Machine**\n\nAsk some friends to prepare drawings of heads, torsos, and legs. Make sure\nthese parts are able to line up interchangeably at the neck and waist. Create\nan “Exquisite Corpse” generator in which these parts are randomly recombined\nwhenever a button is clicked. (Image: Tatyana Mustakos)\n\n![c29-fig-5004.jpg](../images/c29-fig-5004.jpg)\n\n**Intermittent Events**\n\nCreate a time-based sketch in which a brief event (such as a sound or\nanimation) is triggered sporadically and unpredictably, with a very low\nprobability. You may also display a scrolling timeline showing a history of\nwhen the event occurred.\n\n![c29-fig-5005.jpg](../images/c29-fig-5005.jpg)\n\n**Order to Chaos**\n\nMake an interactive composition that depicts “order” when the cursor is on the\nleft side of the canvas, and “chaos” when it is on the right. The amount of\norder or chaos (entropy) should vary smoothly according to the cursor\nposition. The map() and constrain() functions may be helpful.\n\n![c29-fig-5006.jpg](../images/c29-fig-5006.jpg)\n\n**Drunk Walk I: Brownian Motion**\n\nCreate a sketch in which a small 2D element travels erratically from one\nmoment to the next, leaving a trail across the canvas as it moves. At every\ntimestep, it should update its current position with a small random\ndisplacement in both X and Y.\n\n![c29-fig-5007.jpg](../images/c29-fig-5007.jpg)\n\n**Drunk Walk II: Random Lattice Walk**\n\nCreate a sketch in which a small element travels erratically from one moment\nto the next, leaving a trail as it moves. At every timestep, it should have a\n1-in-4 chance of moving up, down, left, or right from its previous position.\n\n![c29-fig-5008.jpg](../images/c29-fig-5008.jpg)\n\n**Drunk Walk III: Smoothed Noise**\n\nThe smoothed quality of Perlin noise allows you to make an animation that is\nunpredictable, but also has temporal coherence. Use Perlin noise to\ndynamically regulate the size and position of a circle. Observe the effects of\nchanging the parameters to the noise function.\n\n![c29-fig-5009.jpg](../images/c29-fig-5009.jpg)\n\n**10 PRINT**\n\nIn each cell of a square grid, choose with random probability to place a\ndiagonal line pointing up or down. The fascinating maze-like patterns that\narise are discussed at length in the book 10 PRINT CHR$(205.5+RND(1)); : GOTO\n10 by Nick Montfort et al.1\n\n![c29-fig-5010.jpg](../images/c29-fig-5010.jpg)\n\n**Duotone Truchet Tiling**\n\nCreate a set of four duotone “Truchet tiles,” each with two quarter-circles\nconnecting the midpoints of adjacent sides. Be sure to create all possible\norientations and all possible colorings (corners light or corners dark).\nRandomly place these tiles in a square grid, enforcing continuity of color.\n\n![c29-fig-5011.jpg](../images/c29-fig-5011.jpg)\n\n**Hitomezashi Sashiko Stitching**\n\nRandomly label the columns and rows of a grid with 0s and 1s. For each column,\ndraw a vertical sequence of alternating dashes and gaps, beginning with a dash\n(for columns labeled 1) or a gap (for columns labeled 0). Similarly, draw\nhorizontal sequences of dashes and gaps for the rows. Observe the resulting\npatterns.2\n\n![c29-fig-5012.jpg](../images/c29-fig-5012.jpg)\n\n**Noise Mountains**\n\nGenerate some mountains by creating a series of parallel lines with no spacing\nbetween them, spanning from the bottom of the screen to an unpredicable\nheight. Use a Perlin noise function to determine the height of each line.\n\n![c29-fig-5013.jpg](../images/c29-fig-5013.jpg)\n\n**Imaginary Islands**\n\nUse a two-dimensional Perlin noise function to generate a map of imaginary\nislands. For every pixel: if the value of the noise function is below some\nthreshold value, color the pixel blue (for water); if it is higher than the\nthreshold, color it brown (for land).\n\n![c29-fig-5014.jpg](../images/c29-fig-5014.jpg)\n\n**Recoding Molnár's _Interruptions_**\n\nCarefully study some reproductions of the computational plotter work\n_Interruptions_ (1968–1969) by Vera Molnár.3 Note how it consists of a grid of\nlines whose orientations have been randomized. Some lines are also missing, in\npatches. Write down ten additional observations about the work. To the best of\nyour abilities, write a program to “re-code” _Interruptions_. Write a few\nsentences describing how your version succeeds or falls short.\n\n## Notes\n\n1  _10 PRINT CHR$(205.5+RND(1)); : GOTO 10_ , ed. Nick Montfort et al.\n(Cambridge, MA: The MIT Press, 2012). 2 See Annie Perkins, tweet of March 29,\n2020, <https://twitter.com/anniek_p/status/1244220881347502080>. 3 Vera\nMolnár, _Interruptions_ , plotter artwork, 1968–1969,\n<http://dam.org/artists/phase-one/vera-molnar/artworks-bodies-of-work/works-\nfrom-the-1960s-70s>.\n\n\n# Arrays\n\n![c30-fig-5001.jpg](../images/c30-fig-5001.jpg)\n\n**Living Line I**\n\nCreate an interaction that stores the past 100 mouse positions and displays\nthem as a polyline. Store the mouse data in three different ways: in two 1D\narrays (one for X, one for Y); in one 2D array; and in an array of Point2D\nobjects.\n\n![c30-fig-5002.jpg](../images/c30-fig-5002.jpg)\n\n**Living Line II**\n\nCreate an interaction that stores the past 100 mouse positions and displays\nthis path as a polyline. Move an animated ellipse along this path. When it\nreaches the end, return it to the start and repeat, or have it travel in the\nother direction.\n\n![c30-fig-5003.jpg](../images/c30-fig-5003.jpg)\n\n**Living Line III**\n\nCreate a line using the past 100 mouse positions, as above. Bring your line to\nlife by progressively adding some randomness to each point.\n\n![c30-fig-5004.jpg](../images/c30-fig-5004.jpg)\n\n**Calligraphic Polyline**\n\nCreate a sketch that stores the past 100 mouse positions. Draw a line between\neach point and the one previous to it. Make the thickness of this line\ninversely proportional to the distance between each pair of points, so that\nfaster marks make thinner lines.\n\n![c30-fig-5005.jpg](../images/c30-fig-5005.jpg)\n\n**Animated Walk Cycle**\n\nLoad each frame of an animated walk cycle into an array of images, and loop\nthrough these to display the character in motion. When a button is pressed,\nreverse the playback of the animation. (Image from Eadweard Muybridge, Animals\nin Motion, 1902.)\n\n![c30-fig-5006.jpg](../images/c30-fig-5006.jpg)\n\n**Plant the Flag**\n\nA bumpy “landscape” or terrain, consisting of an array of height values, has\nbeen provided for you. Write code that searches through this array and draws\n“flags” on the peaks (i.e., local maxima).\n\n![c30-fig-5007.jpg](../images/c30-fig-5007.jpg)\n\n**Longest Line Search**\n\nWrite a program that draws straight lines when the user clicks and drags.\nColor the longest line red.\n\n![c30-fig-5008.jpg](../images/c30-fig-5008.jpg)\n\n**Reordering Rectangles**\n\nCreate a program that stores the coordinates for some rectangles in an array.\nReading from this array, your program should paint the rectangles over each\nother as they are rendered. Write code to reverse the array. Sort the\nrectangles from right to left. Sort the rectangles by their area.\n\n\n# Time and Interactivity\n\n![c31-fig-5001.jpg](../images/c31-fig-5001.jpg)\n\n**Eyes Following Cursor**\n\nDraw one or more eyes with pupils that follow the cursor. If you can,\nconstrain each pupil to stay within its eyeball.\n\n![c31-fig-5002.jpg](../images/c31-fig-5002.jpg)\n\n**Fuse (or Progress Bar)**\n\nProgram a virtual fuse that takes precisely five seconds to burn. Trigger an\ninteresting event when it finishes, such as fireworks or a volcanic eruption.\nConsider alternate “skins” to tell a different story with the code for your\nfuse: a progress bar, a balloon inflating (and bursting).\n\n![c31-fig-5003.jpg](../images/c31-fig-5003.jpg)\n\n**Ripples in a Pond**\n\nCreate a program in which animated circular “ripples” emanate outward from the\ncursor each time the mouse button is clicked. Consider the speed at which your\nripples expand. Implement your ripples in an object-oriented coding style.\n\n![c31-fig-5004.jpg](../images/c31-fig-5004.jpg)\n\n**Rain Catcher**\n\nUsing code, animate a rainy day. Raindrops should appear from beyond the top\nof your screen and fall at random intervals. (It may be helpful to create a\nraindrop class.) Create a simple game in which the user can “catch” raindrops\nthat fall close to the cursor.\n\n![c31-fig-5005.jpg](../images/c31-fig-5005.jpg)\n\n**Abstract Typewriter**\n\nCreate an expressive keyboard-based performance instrument. Each key should\ntrigger a different animation, image, or sound. Carefully consider the\naesthetics of the experience of playing your system. For inspiration, try\nPatatap, an audiovisual keyboard app by Jono Brandel.\n\n![c31-fig-5006.jpg](../images/c31-fig-5006.jpg)\n\n**Easing: Filtering a Variable**\n\nObjects rarely move at a constant speed in the real world. Create a sketch in\nwhich an ellipse follows the position of the cursor but decelerates on\napproach. This requires an easing function or a filter for the mouse position\nvalues.\n\n![c31-fig-5007.jpg](../images/c31-fig-5007.jpg)\n\n**Smoothing Data**\n\nCreate a polyline by storing and displaying a sequence of cursor positions.\nNow smooth it by progressively averaging each point along the line with its\nneighbors. Give consideration to how you handle the line's terminal points.\n\n![c31-fig-5008.jpg](../images/c31-fig-5008.jpg)\n\n**Audio-Sensitive Animation**\n\nCreate graphics that respond to the amplitude of microphone sounds. For\nexample, you might illustrate the sound with a sleeping animal that is\nawakened by loud noises, or a face that appears to be voicing the microphone\nsound. Audio waveforms should be a starting point, not an end.\n\n\n# Typography\n\n![c32-fig-5001.jpg](../images/c32-fig-5001.jpg)\n\n**Ransom Letter**\n\nCreate a sketch to display a brief text in which each letter is individually\nset with a random typeface and font size.\n\n![c32-fig-5002.jpg](../images/c32-fig-5002.jpg)\n\n**One-Line Typewriter**\n\nCreate a sketch that accumulates and displays a string of letters as they are\ntyped. For an additional challenge, delete characters when the backspace key\nis pressed.\n\n![c32-fig-5003.jpg](../images/c32-fig-5003.jpg)\n\n**Dynamic Text**\n\nChoose a word describing a bodily action, such as “grow,” “shiver,” or “jump.”\nAnimate this word on your screen in a way that represents its meaning. For\nexample, the word “grow” could become larger over time.\n\n![c32-fig-5004.jpg](../images/c32-fig-5004.jpg)\n\n**Responsive Text**\n\nAssign an interactive behavior to a word, such as “avoid” or “tickle,” that\nexpressively defines its personality relative to the cursor.1\n\n![c32-fig-5005.jpg](../images/c32-fig-5005.jpg)\n\n**Scrolling Headlines (Chyron)**\n\nChoose a series of recent news headlines and write a program that displays\nthem scrolling along the bottom of the screen. For an additional challenge,\nfetch the latest headlines automatically using a news API.\n\n![c32-fig-5006.jpg](../images/c32-fig-5006.jpg)\n\n**Split-Flap Display (Word Ladder)**\n\nCurate a list of words that all have the same number of letters. Create a\ndisplay that interpolates from one word to the next, in the manner of a split-\nflap airport sign, by iterating through the intermediate letters in\nalphabetical order.\n\n![c32-fig-5007.jpg](../images/c32-fig-5007.jpg)\n\n**Word Finder**\n\nTypeset a brief text, such as a poem or email. Create a program that locates\ninstances of a “word of interest” in that text. Your program should perform a\nvisual treatment of that word, such as highlighting, underlining, or\nredaction.2\n\n![c32-fig-5008.jpg](../images/c32-fig-5008.jpg)\n\n**Letterform Collage Tool**\n\nCreate a tool that allows a user to make abstract collages or concrete poetry\nwith letterforms. The user should be able to select a letter with the keyboard\nand then place it using the mouse. Save three compositions made with your\ntool.\n\n![c32-fig-5009.jpg](../images/c32-fig-5009.jpg)\n\n**Procrustean Typography**\n\nIn Greek mythology, Procrustes stretched or cut different people to make them\nall fit in a certain iron bed. You will do the same with type. Create a\nprogram that captures words typed by the user and adjusts their font size so\nthat they always fit the precise width of the canvas.\n\n![c32-fig-5010.jpg](../images/c32-fig-5010.jpg)\n\n**Text along a Curve**\n\nSet text along a curve, writing code to position each letter. If you can,\norient each letter so that it is locally perpendicular to the curve.3\n\n![c32-fig-5011.jpg](../images/c32-fig-5011.jpg)\n\n**Glyph Hacking: Playing with Outlines**\n\nIn this exercise you will load a typeface, modify its characters, and save out\na new font of your own. Identify a method for loading font outlines into your\npreferred programming environment, such as by converting a TTF typeface to SVG\nusing FontForge, or by accessing glyph contours directly with textToPoints()\n(in p5.js), opentype.js, the Rune.Font plugin4 (in Rune.js), or the\nGeomerative library5 (in Java Processing). Using code, create a treatment for\nall of the letters in the alphabet, such as making them puffy or spiky. Name\nyour new font, and use your font to typeset its name. If you can, export your\nnew glyphs to .ttf or another font format when you have finished.\n\n![c32-fig-5012.jpg](../images/c32-fig-5012.jpg)\n\n**Tiny Word Processor**\n\nUsing a fixed-width font, create a word processor that allows a user to type\ncharacters into a field of cells. Provide a text cursor whose position can be\nmoved with the arrow keys and mouse. Implement word-wrapping, deleting (with\nthe backspace key), and text file exporting.\n\n![c32-fig-5013.jpg](../images/c32-fig-5013.jpg)\n\n**ASCII Vision**\n\nWrite a program to generate an ASCII art representation of an image. To do\nthis, you'll need a sequence of characters that represents different\nbrightness values in an image. A common character ramp for representing 10\nlevels of grey is “@%#*+=-:. “6\n\n## Notes\n\n1 Adapted from Casey Reas and Ben Fry, “Typography,”\n<https://processing.org/tutorials/typography/>. 2 The text in the poem is from\nSonia Sanchez, “This Is Not a Small Voice” (1995),\n<https://poets.org/poem/not-small-voice>. 3 Daniel Shiffman, “Strings and\nDrawing Text,” <https://processing.org/tutorials/text/>. 4 Rune Madsen,\n“Typography,”<http://printingcode.runemadsen.com/lecture-typography/>. 5\nRicard Marxer, “Geomerative Library,”\n<http://www.ricardmarxer.com/geomerative/>. 6 Paul Bourke, “Character\nRepresentation of Greyscale Images,”\n<http://paulbourke.net/dataformats/asciiart/>.\n\n\n# Curves\n\n![c33-fig-5001.jpg](../images/c33-fig-5001.jpg)\n\n**Butt Generator**\n\nWrite a program that uses arcs or Bézier curves to generate images of one or\nmore butts.1\n\n![c33-fig-5002.jpg](../images/c33-fig-5002.jpg)\n\n**Parabola**\n\nA _parabola_ is a curve whose formula is y=ax<sup>2</sup>, where the variable\na is some constant of your choice. Write a program that plots a parabola.\n\n![c33-fig-5003.jpg](../images/c33-fig-5003.jpg)\n\n**One Circle, Three Ways**\n\nPlot a circle (from scratch) three different ways: use the trigonometric\nfunctions sin() and cos() to plot a series of points; approximate a circle\nwith four Bézier curves; and construct a circle with “turtle graphics,” using\na series of alternating forward steps and small rotations.2\n\n![c33-fig-5004.jpg](../images/c33-fig-5004.jpg)\n\n**Continuity of Bézier Curves**\n\nCreate a curve by joining two Bézier curves. Now ensure that the combined\ncurve is “C2 continuous,” with no visual discontinuities at the point where\nits components are joined. In other words, there should be continuity of\nposition, tangent slope, and curvature.\n\n![c33-fig-5005.jpg](../images/c33-fig-5005.jpg)\n\n**Phyllotaxis**\n\nUse turtle graphics to generate a phyllotactic spiral.3 Start your turtle at\nthe center of the canvas. At each step, your turtle should draw a small\nelement; move outwards by a slowly increasing amount; and rotate its\norientation by the “Golden Angle,” ~137.507764°.\n\n![c33-fig-5006.jpg](../images/c33-fig-5006.jpg)\n\n**Lissajous**\n\nLissajous curves are useful parametric functions of the form x=cos(a*t);\ny=sin(b*t);, where a and b are typically small whole numbers. Use the\nmathematics of Lissajous curves to plot the movements of an animated element.\n\n![c33-fig-5007.jpg](../images/c33-fig-5007.jpg)\n\n**Spiral**\n\nWrite a program that draws a spiral. Before you begin, research different\ntypes of spirals, such as Archimedes's spiral (the radius of which grows\narithmetically) and the logarithmic or equiangular spiral (whose radius grows\ngeometrically). Consider different implementations, such as explicitly\nplotting your spiral using polar equations, implicitly rendering it by summing\nsmall differences (e.g., go forward, turn slightly, repeat), or approximating\nit piecewise with circular arcs.\n\n![c33-fig-5008.jpg](../images/c33-fig-5008.jpg)\n\n**Polar Curve**\n\nWrite a program to display a cardioid, epicycloid, hippopede, or other polar\ncurve4 whose equation takes the form r = f(theta). The identities\nx=rcos(theta) and y=rsin(theta) may be helpful. Use the cursor to control\nparameters of the curve in real time.\n\n![c33-fig-5009.jpg](../images/c33-fig-5009.jpg)\n\n**Fourier Synthesis**\n\nIn Fourier synthesis, complex waveforms are made by summing up sine waves of\ndifferent amplitudes and frequencies. For example, a square wave can be\napproximated by the sum: sin(x)+sin(3x)/3+sin(5x)/5+sin(7x)/7…. Generate a\nsquare wave with this technique.\n\n![c33-fig-5010.jpg](../images/c33-fig-5010.jpg)\n\n**Osculating Circle**\n\nEvery point on a curve has a local _radius of curvature_ : the radius of the\nosculating circle that best approximates the curve at that point. Write a\nprogram that displays the osculating circle for points along a mark drawn by\nthe user. You can approximate this by computing a circle from three\nconsecutive points along the mark.\n\n![c33-fig-5011.jpg](../images/c33-fig-5011.jpg)\n\n**Circle Morphing**\n\nCreate a program that interpolates between a circle and a triangle.5\n\n![c33-fig-5012.jpg](../images/c33-fig-5012.jpg)\n\n**Shaping Functions**\n\n“Shaping functions” are indispensable in generating and altering signals for\naesthetic purposes. Also known as tweens, interpolation curves, bias\nfunctions, easing curves, or unit generators, these functions are typically\ndesigned to receive and produce values between 0 and 1. Use a collection of\nshaping functions (such as p5.func by Luke DuBois, or Robert Penner's Easing\nFunctions) to create nonlinear relationships between time and position (to\nmake animations) or between position and color (to make interesting\ngradients).6\n\n## Notes\n\n1 Inspired by Le Wei, “Butt Generator,” accessed April 11, 2020,\n<http://www.buttgenerator.com/>. 2 Yuki Yoshida, “Drawing a Circle Code\nRepository,” accessed April 11, 2020, <https://github.com/yukiy/drawCircle>. 3\nGolan Levin, “Turtle's Phyllotactic Spiral,” accessed April 11, 2020,\n<http://cmuems.com/2015c/deliverables/deliverables-09/#spiral>. 4 Mathworld,\n“Curves,” accessed April 11, 2020,\n<http://mathworld.wolfram.com/topics/Curves.html>. 5 See Dan Shiffman, “Guest\nTutorial #7: Circle Morphing with Golan Levin,” _The Coding Train_ , October\n25, 2017, video, <https://www.youtube.com/watch?v=mvgcNOX8JGQ>. 6 Luke DuBois,\n“p5.func,” accessed July 27, 2020, <https://idmnyu.github.io/p5.js-func/>;\nRobert Penner, “Easing Functions,” accessed July 27, 2020,\n<http://robertpenner.com/easing/>.\n\n\n# Shapes\n\n![c34-fig-5001.jpg](../images/c34-fig-5001.jpg)\n\n**Make a Star**\n\nPlot the vertices of a regular 10-sided polygon, using the trigonometric\nfunctions sin() and cos() in the same way that you might generate a circle.\nModify your code to generate a star by alternating long and short radii.1\n\n![c34-fig-5002.jpg](../images/c34-fig-5002.jpg)\n\n**Random Splat**\n\nPlot the vertices of a many-sided polygon, using the functions sin() and cos()\nin the same way that you might generate a circle, but compute the position of\neach point with a radius that is slightly randomized. How closely can you make\nthe result resemble a drop of ink?2\n\n![c34-fig-5003.jpg](../images/c34-fig-5003.jpg)\n\n**Connect the Dots**\n\nThe numbers below define the vertices of a polygon. Write code that connects\nthe dots to plot the shape. x={81, 83, 83, 83, 83, 82, 79, 77, 80, 83, 84, 85,\n84, 90, 94, 94, 89, 85, 83, 75, 71, 63, 59, 60, 44, 37, 33, 21, 15, 12, 14,\n19, 22, 27, 32, 35, 40, 41, 38, 37, 36, 36, 37, 43, 50, 59, 67, 71}; y={10,\n17, 22, 27, 33, 41, 49, 53, 67, 76, 93, 103, 110, 112, 114, 118, 119, 118,\n121, 121, 118, 119, 119, 122, 122, 118, 113, 108, 100, 92, 88, 90, 95, 99,\n101, 80, 62, 56, 43, 32, 24, 19, 13, 16, 23, 22, 24, 20};\n\n![c34-fig-5004.jpg](../images/c34-fig-5004.jpg)\n\n**Axis-Aligned Minimum Bounding Box**\n\nGiven a 2D shape represented by a set of points, compute and plot its\n“bounding box”: a useful rectangle defined by the leftmost, rightmost,\nhighest, and lowest points on the shape's contour. Create an interaction in\nwhich the shape is displayed differently if the cursor enters its bounding\nbox.\n\n![c34-fig-5005.jpg](../images/c34-fig-5005.jpg)\n\n**Computing the Centroid**\n\nGiven a 2D shape represented by a set of points, compute and plot the\n“centroid” of its outline—the location whose x-coordinate is the average of\nthe x-coordinates of the points, and whose y-coordinate is the average of the\ny-coordinates of the points.\n\n![c34-fig-5006.jpg](../images/c34-fig-5006.jpg)\n\n**Computing the Perimeter**\n\nGiven a 2D shape represented as a set of points, calculate the length (or\n_perimeter_) of its outline. To do this, add up the distances between every\npair of consecutive points along its boundary. Don't forget to include the\ndistance from the last point back to the first one.\n\n![c34-fig-5007.jpg](../images/c34-fig-5007.jpg)\n\n**Computing the Area**\n\nGiven a simple (non-self-intersecting) 2D shape whose points are stored in\narrays x[] and y[], calculate its area. This can be done using Gauss's area\nformula, or “shoelace algorithm,” which sums, for all points i, the products\n((x[i+1] + x[i]) * (y[i+1] - y[i]))/2.0.\n\n![c34-fig-5008.jpg](../images/c34-fig-5008.jpg)\n\n**Shape Metrics: Compactness**\n\nOne of the simplest shape metrics is “compactness” (or isoperimetric\nquotient), which describes the extent to which a shape does or doesn't sprawl.\nOften used in redistricting to avoid gerrymandering, compactness is a\ndimensionless, scale-invariant, and rotation-invariant quantity.3 It is\ncomputed by taking the ratio of a shape's area to its squared perimeter.\nCompute the compactness of the provided shape. Compare this value with the\ncompactness of some other shapes.\n\n![c34-fig-5009.jpg](../images/c34-fig-5009.jpg)\n\n**Detecting Points of High Curvature**\n\nGiven a 2D shape represented as a set of points, we can estimate the “local\ncurvature” along the shape's boundary by computing the angle between\nconsecutive trios of points. Using the provided shape, write code to determine\nwhich points have especially high curvature (i.e. the fingertips) and mark\nthem with a colored dot. _Hint_ : Use the dot product to compute the angles,\nand the cross product to distinguish positive curvature (convexities) from\nnegative curvature (concavities).\n\n![c34-fig-5010.jpg](../images/c34-fig-5010.jpg)\n\n**Hand-Drawn Graphics Library**\n\nCreate a set of functions for rendering basic shapes with a hand-drawn feel.\nAt a minimum, your library should provide functions for drawing line segments,\nellipses, and rectangles.\n\n![c34-fig-5011.jpg](../images/c34-fig-5011.jpg)\n\n**Blob**\n\nUsing any means you prefer, design and animate an expressive 2D blob. For\nexample: you could create a closed loop of Bézier curves; trace the contours\nof metaballs (implicit curves) using Marching Squares; calculate a Cassini\nellipse, cranioid, or other parametric curve; or simulate a smooth ropelike\ncontour using Verlet integration.\n\n## Notes\n\n1 Adapted from Rune Madsen, “Shape: Procedural Shapes,” Programming Design\nSystems, accessed July 6, 2020,\n<https://programmingdesignsystems.com/shape/procedural-shapes/index.html>. 2\nAdapted from Madsen, “Shape: Procedural Shapes.” 3 Wikipedia, s.v.\n“Compactness measure of a shape,” last modified June 22, 2020,\n<https://en.wikipedia.org/wiki/Compactness_measure_of_a_shape>.\n\n\n# Geometry\n\n![c35-fig-5001.jpg](../images/c35-fig-5001.jpg)\n\n**Midpoint of a Line Segment**\n\nWrite a program that draws two random points (A and B) whenever a button is\npressed. Connect these points with a line segment. Calculate the midpoint of\nthis line segment, and place a dot there. Place an additional circle one-third\nof the way from A to B.\n\n![c35-fig-5002.jpg](../images/c35-fig-5002.jpg)\n\n**Intersection of Two Rectangles**\n\nWrite a program that draws two randomly sized, randomly placed rectangles\nwhenever a button is pressed. If these rectangles overlap, locate the new\nrectangle that represents their overlapping region and draw its diagonals.\n\n![c35-fig-5003.jpg](../images/c35-fig-5003.jpg)\n\n**Construction of a Perpendicular**\n\nWrite a program that draws a line from the center of the canvas to the cursor.\nConstruct a second line that starts at the cursor, is 50 pixels long, and is\nperpendicular to the first line.\n\n![c35-fig-5004.jpg](../images/c35-fig-5004.jpg)\n\n**Parallel Polyline (Offset Curve)**\n\nWrite a program that stores cursor points as a user draws. Connect these\npoints with a polyline. Use geometry to calculate another polyline, which is\noffset everywhere from the user's drawing by a distance of 50 pixels.\n\n![c35-fig-5005.jpg](../images/c35-fig-5005.jpg)\n\n**Compass Orientation**\n\nStore the locations of the two most recent mouse clicks and draw a line\nbetween them. Using the atan2() function, compute the angular orientation of\nthis line. Display this angle in degrees, and label it with the nearest\ncompass direction (N, NE, E, SE, S, SW, W, NW).\n\n![c35-fig-5006.jpg](../images/c35-fig-5006.jpg)\n\n**Angle between Three Points**\n\nWrite a program that computes the angle between three points: two randomly\nplaced points, A and B, and the mouse cursor, C. _Hint_ : Use the dot product\nto compute the angles, and use the cross product to distinguish positive from\nnegative curvature.\n\n![c35-fig-5007.jpg](../images/c35-fig-5007.jpg)\n\n**Distance from a Point to a Line**\n\nWrite a program that creates a random line segment whenever a key is pressed.\nCompute and display the shortest distance between this line and the cursor.\nPlace a dot at the point on this line that is closest to the cursor.1\n\n![c35-fig-5008.jpg](../images/c35-fig-5008.jpg)\n\n**Intersection of Two Line Segments**\n\nWrite a program that creates two random line segments whenever a button is\npressed. Calculate the intersection point of these two line segments, and if\nit exists, draw a dot there.2\n\n![c35-fig-5009.jpg](../images/c35-fig-5009.jpg)\n\n**Centroid of a Triangle**\n\nConstruct a triangle from three random points whenever a button is pressed.\nDraw lines (called “medians”) to connect the midpoints of each side to its\nopposite vertex. At the intersection of these medians is the triangle's\ncentroid. Place a dot there.\n\n![c35-fig-5010.jpg](../images/c35-fig-5010.jpg)\n\n**Circle from Three Points (Circumcenter)**\n\nConstruct a random triangle whenever a button is pressed. Generate a circle\nthat passes precisely through all three of its vertices. The center of this\ncircle is called the _circumcenter_ of the triangle. Place a dot there. Note:\nthe circumcenter is not always inside the triangle.3\n\n![c35-fig-5011.jpg](../images/c35-fig-5011.jpg)\n\n**Orthocenter of a Triangle**\n\nThe “altitude” of a triangle is a line that is perpendicular to a given side\nand passes through the opposite vertex. Write a program that constructs a\ntriangle from three random points whenever a button is pressed. Calculate and\ndisplay the triangle's three altitudes. The _orthocenter_ of this triangle is\nlocated at the intersection of its altitudes. Place a dot there. Note: for\nmost triangles, the altitudes will not pass through the midpoints of the\nsides.\n\n![c35-fig-5012.jpg](../images/c35-fig-5012.jpg)\n\n**Incenter of a Triangle**\n\nThe _incenter_ of a triangle is located at the intersection of the angle\nbisectors of a triangle's three corners. Write a program that generates a\nrandom triangle whenever a button is pressed and places a dot at its incenter.\n(The incenter also happens to be the center of a circle inscribed inside the\ntriangle, called the incircle; its radius is obtained by dropping a\nperpendicular line from the incenter to any of the triangle's sides. If you\ncan, draw the triangle's incircle.)\n\n## Notes\n\n1 Paul Bourke, “Points, Lines, and Planes,” 1988–2013,\n<http://paulbourke.net/geometry/pointlineplane/>. 2 Bourke, “Points, Lines,\nand Planes.” 3 Paul Bourke, “Equation of a Circle from 3 Points (2\nDimensions),” January 1990, <http://paulbourke.net/geometry/circlesphere/>.\n\n\n# Image\n\n![c36-fig-5001.jpg](../images/c36-fig-5001.jpg)\n\n**Collage Machine**\n\nCollect a directory of images. Write a program that uses these to generate\ncollages of images. Include some unpredicability within it so that it\ngenerates a different collage each time it runs.\n\n![c36-fig-5002.jpg](../images/c36-fig-5002.jpg)\n\n**Color of a Pixel**\n\nDisplay an image. Create an interactive system that continually fetches the\ncolor of the pixel under the mouse as it passes over this image. Use this\ncolor to fill a shape drawn to the screen as the mouse is moved around.\n\n![c36-fig-5003.jpg](../images/c36-fig-5003.jpg)\n\n**Subsample and Downsample**\n\nWrite a program that pixelates an image to produce a low-resolution version.\nBegin by _subsampling_ the image (destination pixels are selected from the\noriginal). Then, downsample the image (destination pixels are local averages).\n\n![c36-fig-5004.jpg](../images/c36-fig-5004.jpg)\n\n**Random Dot Dithering**\n\nLoad an image into memory and fetch the brightness of a pixel in a random\nlocation. Generate a random number between 0 and 255; if it is greater than\nthe brightness of the pixel, draw a black dot at a corresponding location on\nthe canvas. Repeat as the picture emerges.\n\n![c36-fig-5005.jpg](../images/c36-fig-5005.jpg)\n\n**Searching for the Brightest Point**\n\nDisplay the live video feed from your webcam to the screen and process the\npixels to find the brightest point in the image. Place an indicator at the\nlocation of this pixel.\n\n![c36-fig-5006.jpg](../images/c36-fig-5006.jpg)\n\n**Image Averaging**\n\nCollect ten or more images labeled with the same word or concept, such as\n“apple,” “sunset,” or “clock.” Ensure that all of the images have the same\npixel dimensions. Compute a new image that is the pixelwise average of the\ncollection, as a way to visualize regularities in the representation of that\nconcept.\n\n![c36-fig-5007.jpg](../images/c36-fig-5007.jpg)\n\n**Edge Detector (Sobel Filter)**\n\nWrite a program that uses a Sobel filter to detect and display the edges in an\nimage. If you can, compute the strength and orientation of these edges.\n\n![c36-fig-5008.jpg](../images/c36-fig-5008.jpg)\n\n**Pixel Sort**\n\nSort the pixels of an image by brightness. Then repeat the exercise, but sort\nby hue. For tips, see Dan Shiffman's “Coding Challenge #47: Pixel Sorting in\nProcessing” in his _Coding Train_ video of December 21, 2016.\n\n\n# Visualization\n\n![c37-fig-5001.jpg](../images/c37-fig-5001.jpg)\n\n**Text Message Isotype**\n\nCollect the text messages that you sent and received over the past week. Count\nand categorize these messages according to person, topic, or mood. Create an\nisotype visualization to represent this data pictorially.1\n\n![c37-fig-5002.jpg](../images/c37-fig-5002.jpg)\n\n**Temperature Timeline**\n\nDownload a dataset of the average monthly temperatures where you live.\nVisualize this data in a chart showing time on the x-axis and temperature on\nthe y-axis. Make three versions of the graph, showing the data as points, as\nvertical bars, and as data points connected with a curved line.2\n\n![c37-fig-5003.jpg](../images/c37-fig-5003.jpg)\n\n**Pie Chart**\n\nWrite code to generate a pie chart that displays what you spent on breakfast,\nlunch, dinner, and coffee yesterday. You will need to use a function that\nrenders a filled arc. Load the data from an external file in a well-structured\nformat like JSON or CSV. Make sure your chart has a key.\n\n![c37-fig-5004.jpg](../images/c37-fig-5004.jpg)\n\n**Small Multiple of Radar Charts**\n\nTake a “Big Five” personality quiz to (ostensibly) quantify your\nconscientiousness, agreeableness, neuroticism, openness, and extraversion.3\nGenerate a radar chart to visualize this multivariate data. Ask some friends\nto take the quiz, and create a trellis plot (or “small multiple”) that\npresents their personality charts side by side.\n\n![c37-fig-5005.jpg](../images/c37-fig-5005.jpg)\n\n**Path Plotting I**\n\nUsing a smartphone app such as OpenPaths or SensorLog, record your GPS\nlocation data for a duration of at least one week. Export the latitude and\nlongitude data to a CSV file. Write code to load and visualize the paths of\nyour daily journeys. What observations can you make?\n\n![c37-fig-5006.jpg](../images/c37-fig-5006.jpg)\n\n**Path Plotting II**\n\nReuse the location dataset from the previous exercise and visualize it in a\nway that is _not_ a map. Consider displaying information such as path lengths\nor speed, frequency of visits to a particular location, distance traveled per\nday, etc.\n\n![c37-fig-5007.jpg](../images/c37-fig-5007.jpg)\n\n**Dot Map**\n\nThe “Rat Sightings” dataset from NYC OpenData lists all of the complaints\nabout rats made to New York City's 311 response center since 2010. Write a\nprogram to map this geolocated data by representing each complaint as a single\ndot.4\n\n![c37-fig-5008.jpg](../images/c37-fig-5008.jpg)\n\n**Heat Map**\n\nWrite a program to display the Rat Sightings data as a spatial heat map. This\npresents two challenges, as you will need to: design a density function that\nrepresents the density of dots in a map as a continuous field, and select a\ncolor palette that represents the intensity of this field.\n\n![c37-fig-5009.jpg](../images/c37-fig-5009.jpg)\n\n**Social Network Graph**\n\nDevelop a “force directed graph” to visualize a social network, in which\nindividuals are represented as nodes and their relations as links, and the\nlayout is governed by a simulated particle system. Use this to visualize the\ncommunity of 62 bottlenose dolphins living off Doubtful Sound, New Zealand, as\ncompiled by Lusseau et al.\n\n![c37-fig-5010.jpg](../images/c37-fig-5010.jpg)\n\n**Social Network Matrix**\n\nA _symmetric adjacency matrix_ visualizes social interactions in a group.\nIndividuals are listed along a table's rows and columns, and interactions\nbetween each pair are quantified in the cells. Create such a visualization for\nthe family of wild monkeys observed by Linda Wolfe as they sported by a river\nin Ocala, Florida.\n\n![c37-fig-5011.jpg](../images/c37-fig-5011.jpg)\n\n**Real-Time Data Display**\n\nLocate a real-time Internet data stream such as that published by a sensor or\nan API (weather, stock prices, satellite locations). Create a program that\ndisplays this data in an evocative way. The example here shows the location of\nthe International Space Station, available from [open-notify.org](http://open-\nnotify.org).5\n\n![c37-fig-5012.jpg](../images/c37-fig-5012.jpg)\n\n**Web Scraping**\n\nWrite a program to exhaustively “scrape” a collection of online images. For\nexample, you might automate a process to download images of every product\noffered by a certain vendor. Write a second program to display your images in\na grid. How else could you organize the collection?6\n\n![c37-fig-5013.jpg](../images/c37-fig-5013.jpg)\n\n**One Dataset, Four Ways**\n\nVisualize the Slate USA Gun Deaths dataset (2013) in four different ways: a\nmap, a timeline, a bar chart based on age, and a fourth method of your choice.\nIf you can, provide an interaction that allows a user to zoom, sort, filter,\nor query the data. Write about how the different displays provide different\ninsights.7\n\n## Notes\n\n1 Otto Neurath, _International Picture Language: The First Rules of Isotype_\n(London: K. Paul, Trench, Trubner & Company, Limited, 1936). 2 Ben Fry, “Time\nSeries,” _Visualizing Data: Exploring and Explaining Data with the Processing\nEnvironment,_ chap. 4 (Cambridge, MA: O’Reilly Media, Inc., 2008). 3 For\nexample: Open-Source Psychometrics Project, “Big Five Personality Test,”\n<https://openpsychometrics.org/tests/IPIP-BFFM/>. 4 The Rat Sightings dataset\nis available online at <https://data.cityofnewyork.us/Social-Services/Rat-\nSightings/3q43-55fe>. See also Fry, _Visualizing Data_ , chap. 6. 5\n[OpenNotify.org](http://OpenNotify.org), International Space Station Current\nLocation, <http://open-notify.org/Open-Notify-API/ISS-Location-Now/>. Also see\nDan Shiffman's sequence of video tutorials, _Working with Data and APIs in\nJavaScript,_ last updated on July 8, 2019,\n<https://www.youtube.com/playlist?list=PLRqwX-V7Uu6YxDKpFzf_2D84p0cyk4T7X>. 6\nSee Sam Lavigne's sequence of tutorials, _Scrapism,_ last updated on May 27,\n2020, <https://scrapism.lav.io/>. 7 Dataset available at\n<http://www.slate.com/articles/news_and_politics/crime/2012/12/gun_death_tally_every_american_gun_death_since_newtown_sandy_hook_shooting.html>.\n\n\n# Text and Language\n\n![c38-fig-5001.jpg](../images/c38-fig-5001.jpg)\n\n**String Search**\n\nFind a table of color names and their corresponding RGB values. Create an\ninteraction in which a square becomes colored when a user types a known color\nname. (Is your program case-insensitive?)\n\n![c38-fig-5002.jpg](../images/c38-fig-5002.jpg)\n\n**Nonsense Words**\n\nFind some lists of common English prefixes, word roots, and suffixes. Select a\nrandom item from each list and combine them in a simple syntax\n(prefix+root+suffix) to generate plausible nonsense words. What might these\nwords mean?\n\n![c38-fig-5003.jpg](../images/c38-fig-5003.jpg)\n\n**Letter Frequency**\n\nWrite a program to calculate the frequencies of the letters in a provided\ntext. (Be careful to make your program “case-insensitive.”) Write code to\ngenerate a visualization (such as a bar chart or pie chart) of the letters’\nfrequencies.\n\n![c38-fig-5004.jpg](../images/c38-fig-5004.jpg)\n\n**Letter-Pair Frequency**\n\nWrite a program to calculate the frequencies of letter pairs (character\n2-grams, such as “aa,” “ab,” “ac”) in a large text source. Plot the\nfrequencies in a 26x26 matrix.\n\n![c38-fig-5005.jpg](../images/c38-fig-5005.jpg)\n\n**Average Word Length**\n\nWrite a program that calculates the average word length of a provided text.\nThis is a useful approximation of a text's “reading level.” Run your program\non several different source materials.\n\n![c38-fig-5006.jpg](../images/c38-fig-5006.jpg)\n\n**Sorting Words**\n\nLoad a document and display its words (a) sorted alphabetically, (b) sorted by\ntheir length, and (c) sorted according to their frequency in the text.\n\n![c38-fig-5007.jpg](../images/c38-fig-5007.jpg)\n\n**Cut-Up Machine**\n\nIn the _Dada Manifesto_ , Tristan Tzara describes using a newspaper, scissors,\nand some gentle shaking to generate irrational poetry. Do the same with code.\nWrite a program that randomizes the lines or sentences of a newspaper article\nto make a Dada-style poem.\n\n![c38-fig-5008.jpg](../images/c38-fig-5008.jpg)\n\n**Bigram Calculator**\n\nWrite a program to calculate the frequency of all bigrams (word-pairs) in a\ndocument. _Advanced students:_ develop a program that judges the similarity of\ntwo text files based on how many bigrams they have in common.\n\n![c38-fig-5009.jpg](../images/c38-fig-5009.jpg)\n\n**Dammit Jim**\n\nFind a list of occupations. Use these in a generative grammar that produces\nsentences in this format: “Dammit, Jim, I'm an X, not a Y!” (popularized by\nthe sci-fi TV show _Star Trek_). Be sure to write “an X” if X begins with a\nvowel and “a X” if it begins with a consonant.1\n\n![c38-fig-5010.jpg](../images/c38-fig-5010.jpg)\n\n**Knock-Knock Joke Generator**\n\nCreate a program that generates knock-knock jokes. At a miminum, your program\nshould select a random word as a response to “Who's there?” and add to this\nword to create the final line of the joke. Generate ten jokes.\n\n![c38-fig-5011.jpg](../images/c38-fig-5011.jpg)\n\n**Pig Latin Translator**\n\nCreate a program that translates provided text into Pig Latin. In this playful\nscheme, the initial consonant (or consonant cluster) of each word is\ntransferred to the end of that word, after which the syllable “ay” is added.\n\n![c38-fig-5012.jpg](../images/c38-fig-5012.jpg)\n\n**Argots and Language Games**\n\nInvestigate argots, secret languages, and word games like Ubbi Dubbi, Tutnese,\nPirate English, and Dizzouble Dizzutch. Select one and write a program that\ntranslates a provided text into it.\n\n![c38-fig-5013.jpg](../images/c38-fig-5013.jpg)\n\n**Noun Swizzler**\n\nLoad a text, and replace each noun with a randomly selected noun taken from a\nsecond text. You may need to use a “part-of-speech tagger” to identify the\nnouns. In your substitutions, try to match the use of plural and proper nouns\nin the first text.\n\n![c38-fig-5014.jpg](../images/c38-fig-5014.jpg)\n\n**Rhyming Couplets**\n\nSelect and load a large expository text. Create a program that finds rhyming\ncouplets within it, in order to make new poems. You may need to use an\nadditional library (such as RiTa) to tell you what the words sound like.2\n\n![c38-fig-5015.jpg](../images/c38-fig-5015.jpg)\n\n**Haiku Finder**\n\nWrite a program that automatically discovers “inadvertent haikus”: sentences\nin a chosen text whose words happen to fall in groups of five, seven, and five\nsyllables. A basic solution will discover haikus with awkward breaks; add\nheuristics to improve the quality of your results.\n\n![c38-fig-5016.jpg](../images/c38-fig-5016.jpg)\n\n**Markov Text Generator**\n\nIn a Markov chain, a dataset of letter-pair, bigram, or n-gram frequencies is\nused as a “probability transition matrix” to synthesize new text that\nstatistically resembles the dataset's source. Build a Markov generator using\nthe data you collected earlier.3\n\n![c38-fig-5017.jpg](../images/c38-fig-5017.jpg)\n\n**Keyword Extraction with TF-IDF**\n\nObtain a collection of related documents, such as poems, recipes, or\nobituaries. Write a program that uses the TF-IDF (“Term Frequency - Inverse\nDocument Frequency”) algorithm to determine the keywords that best\ncharacterize each document.4\n\n![c38-fig-5018.jpg](../images/c38-fig-5018.jpg)\n\n**Limerick Generator**\n\nLimerick poems have five lines in an AABBA rhyming pattern. The building block\nof these lines is the _anapest_ : a foot of verse consisting of three\nsyllables, the third of which is accented: _da-da-DA._ Lines 1, 2, and 5\nconsist of three anapests; they end with a similar phoneme in order to create\nthe rhyme. Lines 3 and 4 also rhyme with each other, but are shorter,\nconsisting of two anapests each. Write a program to generate limericks. Use a\ncode library such as RiTa to help evaluate your words’ rhymes, syllables, and\nstress patterns.5\n\n## Notes\n\nExercises in this section were contributed by Allison Parrish. 1 See Darius\nKazemi's repository of structured corpora:\n<https://github.com/dariusk/corpora>. Also consider code libraries for\nproducing grammar structures, like Tracery by Kate Compton,\n<http://www.crystalcodepalace.com/tracery.html>. 2 For Java and JavaScript,\nsee RiTa by Daniel Howe, “RiTa, a Software Toolkit for Computational\nLiterature,” <https://rednoise.org/rita/>. For Python, see NLTK, “Natural\nLanguage Toolkit,” <https://www.nltk.org/>. 3 For Python see: “Markovify,”\n<https://github.com/jsvine/markovify>. 4 See Dan Shiffman, “Coding Challenge\n#40.3: TF-IDF,” _The Coding Train_ , October 12, 2016, video,\n<https://www.youtube.com/watch?v=RPMYV-eb6lI>. 5 Howe, “RiTa”; see “The CMU\nPronouncing Dictionary,” <http://www.speech.cs.cmu.edu/cgi-bin/cmudict>.\n\n\n# Simulation\n\n![c39-fig-5001.jpg](../images/c39-fig-5001.jpg)\n\n**Recursive Tree**\n\nUse recursion to create a tree. Begin with a symmetrical design that\nrepeatedly bifurcates. Introduce a variable that proportionally reduces the\nlength of each iteration's branches, and another to alter their orientation.\nExplore your tree's possibilities by controlling these variables with the\ncursor.1\n\n![c39-fig-5002.jpg](../images/c39-fig-5002.jpg)\n\n**Fireworks (Particle Shower)**\n\nCreate a particle class that stores a speck's 2D position and velocity. Add\nmethods to give the particle an initial, randomly generated velocity and\nconstant acceleration. Create an array of particles to simulate a firework.\nEach element of the array will need to start from the same position.\n\n![c39-fig-5003.jpg](../images/c39-fig-5003.jpg)\n\n**Flocking**\n\nCreate a two-dimensional flock of creatures, modeling your animals as\nparticles that exert (and are affected by) forces of mutual _separation_ (to\nprevent collision), _cohesion_ (to stick together as a group), and _alignment_\n(to orient toward similar directions as their neighbors). Connect your\ngoverning parameters to sliders or other UI controls, and observe how changing\nthe relative strengths of these forces alters the behavior of the flock or\nswarm. Include other forces, such as the desire to flee from a predator, the\nhunger to hunt for food, etc. Be sure to represent your creatures with a\ngraphic that makes their orientation visible.2\n\n![c39-fig-5004.jpg](../images/c39-fig-5004.jpg)\n\n**Braitenberg Vehicles**\n\nA Braitenberg vehicle is an autonomous agent that moves and steers based on\nsensor inputs. It measures a stimulus at its location; depending how this\nsignal is mapped to each wheel's power, the vehicle can appear to exhibit\ndifferent goals and behaviors. Implement vehicles that steer toward or away\nfrom the cursor.\n\n![c39-fig-5005.jpg](../images/c39-fig-5005.jpg)\n\n**Cursor-Sensitive Particles**\n\nCreate a particle class that stores a speck's 2D position and velocity. Add a\nmethod that enables a particle's motion to be affected by simulated forces.\n(See Euler integration and Newton's 2nd Law.) Create an array of particles\nthat are attracted to or repelled by the cursor.\n\n![c39-fig-5006.jpg](../images/c39-fig-5006.jpg)\n\n**Flow Field**\n\nUse 2D Perlin noise to compute a flow field, such that every location on the\ncanvas has an associated x-force and y-force. Place particles into this field,\nimpelling them with the forces at their locations. Make recordings of their\ntraces. Confine the particles with periodic boundaries.\n\n![c39-fig-5007.jpg](../images/c39-fig-5007.jpg)\n\n**Spring**\n\nModel a bouncy spring as a particle with a current position P, velocity V, and\nrest position R. When moved from R, a restorative force F proportional to this\ndisplacement pushes it back. Update the particle using Euler integration: add\nF into V, reduce V by a percentage due to damping, and add V into P.\n\n![c39-fig-5008.jpg](../images/c39-fig-5008.jpg)\n\n**Circle Packing**\n\nGenerate a “circle packing”: an arrangement of circles such that none overlap,\nand some (or all) are mutually tangent. In one approach, randomly placed\ncircles are added to unclaimed regions of the canvas and grow until they\ncollide with any previous circles.3\n\n![c39-fig-5009.jpg](../images/c39-fig-5009.jpg)\n\n**Conway's Game of Life**\n\nCreate an implementation of “Conway's Game of Life.” This classic cellular\nautomaton is one of the simplest and best illustrations of the way in which\ncomplex patterns of self-organization can emerge from simple rules.4\n\n![c39-fig-5010.jpg](../images/c39-fig-5010.jpg)\n\n**Diffusion-Limited Aggregation**\n\nImplement and explore the coral forms that arise from diffusion-limited\naggregation (DLA): a simulation in which meandering particles become fixed in\nplace when they collide with previously fixed particles (or an initial\n“seed”).5\n\n![c39-fig-5011.jpg](../images/c39-fig-5011.jpg)\n\n**Snowflake Generator**\n\nSnowflakes are thought to form through a DLA-like process. To make a snowflake\ngenerator, modify your DLA simulation from the previous exercise so that it\nhas dihexagonal symmetry.\n\n![c39-fig-5012.jpg](../images/c39-fig-5012.jpg)\n\n**Space Colonization**\n\nImplement _space colonization_ : an iterative algorithm, first described by\nAdam Runions, for growing networks of branching line structures based on the\nlocations of “growth hormone” sources to which the lines are attracted.6\n\n![c39-fig-5013.jpg](../images/c39-fig-5013.jpg)\n\n**Differential Growth**\n\nImplement _differential growth_ , in which a chain of connected nodes\n(represented by a curve or polyline) uses simple rules like attraction,\nrepulsion, and alignment to produce meandering shapes. The algorithm uses\n_adaptive subdivision_ to interpose new nodes when two adjacent nodes get too\nfar apart.6\n\n## Notes\n\n1 Dan Shiffman, “Coding Challenge #14: Fractal Trees - Recursive,” _The Coding\nTrain_ , May 30, 2016, video, 15.52,\n<https://www.youtube.com/watch?v=0jjeOYMjmDU>. 2 “6.1: Autonomous Agents and\nSteering - The Nature of Code,” _The Coding Train_ , August 8, 2015, video,\n14.28, <https://www.youtube.com/watch?v=JIz2L4tn5kM>. Also see Craig\nReynolds's research into steering behaviors and “boids.” 3 “Coding Challenge\n#50.1: Animated Circle Packing - Part 1,” _The Coding Train_ , January 9,\n2017, video, 28.31, <https://www.youtube.com/watch?v=QHEQuoIKgNE>. 4 “7.3: The\nGame of Life - The Nature of Code,” _The Coding Train_ , August 10, 2015,\nvideo, 16.03, <https://www.youtube.com/watch?v=tENSCEO-LEc>. 5 “Coding\nChallenge #34: Diffusion-Limited Aggregation,” _The Coding Train_ , August 18,\n2016, video, 47.06, <https://www.youtube.com/watch?v=Cl_Gjj80gPE>. 6 Jason\nWebb, “Morphogenesis Resources,” 2020,\n<https://github.com/jasonwebb/morphogenesis-resources>.\n\n\n# Machine Learning\n\n![c40-fig-5001.jpg](../images/c40-fig-5001.jpg)\n\n**Dataset History Audit**\n\nIdentify a widely used dataset, such as ImageNet, MNIST, or LFW. Research its\ncontent and genesis. Who made it, when, and how? Who uses it, and for what?\n(What biases might this dataset have?) Describe your findings in one or two\nparagraphs.\n\n![c40-fig-5002.jpg](../images/c40-fig-5002.jpg)\n\n**Comparing Models**\n\nUse two different image analysis or classification tools to interpret the same\nimage. Write a paragraph that describes and compares their results.\n\n![c40-fig-5003.jpg](../images/c40-fig-5003.jpg)\n\n**Shopping List**\n\nUse an object recognition library or a classifier to store a list of all the\nobjects it sees. Use it to make a shopping list of everything in your fridge.1\n\n![c40-fig-5004.jpg](../images/c40-fig-5004.jpg)\n\n**What Do You See?**\n\nCombine an object recognition classifier with a text-to-speech library. Write\na computer program that narrates what it sees through the webcam.\n\n![c40-fig-5005.jpg](../images/c40-fig-5005.jpg)\n\n**Don't Touch Your Face**\n\nTouching your face can spread disease. Train a webcam classifier to detect\nwhen you touch your face. Write a program that sounds an alarm if you do.\n(Image: Isaac Blankensmith's ANTI-FACE-TOUCHING MACHINE™, an influential\nimplementation of this concept.)2\n\n![c40-fig-5006.jpg](../images/c40-fig-5006.jpg)\n\n**Emoji Translator**\n\nUsing an image classifier and your computer's camera, train a system that\ndetects your facial expressions and displays corresponding emojis.3\n\n![c40-fig-5007.jpg](../images/c40-fig-5007.jpg)\n\n**Body as Game Controller**\n\nTrain an image classifier to determine whether you have raised your left or\nright hand. Using a “webdriver” (also called a mouse/keyboard automator),\nwrite a program in which your classifier controls a classic arcade game, such\nas Space Invaders, by spoofing presses of the WASD or arrow keys. Examples of\nwebdrivers include the Java Robot class, JavascriptExecutor, and Selenium\nBrowser Automation Project.\n\n![c40-fig-5008.jpg](../images/c40-fig-5008.jpg)\n\n**Draw with Your Nose**\n\nUse a pose classifier or face tracker to create a program that lets you draw\nwith your nose.4\n\n![c40-fig-5009.jpg](../images/c40-fig-5009.jpg)\n\n**Hand Puppet**\n\nTrain a webcam regressor to produce a number between zero and one, according\nto the closing and opening of your hand. (Your regressor could use hand pose\ndata from a tracking library, or it could process the camera's pixels\ndirectly.)5 Use this number to puppeteer the mouth of a simple cartoon face.\n\n![c40-fig-5010.jpg](../images/c40-fig-5010.jpg)\n\n**Environmental Sound Clock**\n\nCollect some audio recordings of the ambient sound in your room in the\nmorning, at midday, in the evening, and at night. Train a classifier or\nregressor with these sounds. Use this system to display an approximate\nestimate of the time.\n\n![c40-fig-5011.jpg](../images/c40-fig-5011.jpg)\n\n**Sentiment Analysis**\n\nCollect the last ten text messages that you sent. Use a sentiment analysis\ntool to assess the mood of each message.6\n\n![c40-fig-5012.jpg](../images/c40-fig-5012.jpg)\n\n**More Like This, Please**\n\nCreate or download a collection of about a thousand images representing a\nnarrow category of subject matter (cats, flowers, yearbook photos). Using a\nGenerative Adversarial Network (GAN), synthesize new images that appear to\nbelong to this dataset.7 (Image: “This Foot Does Not Exist” by the MSCHF\ncollective.)\n\n![c40-fig-5013.jpg](../images/c40-fig-5013.jpg)\n\n**Clustering a Collection of Images**\n\nGenerate a 2D map that reveals similarities between images in a set. Begin\nwith an image dataset that interests you. Using some sort of image analysis\nlibrary, such as a convolutional neural network, calculate high-dimensional\nnumeric descriptions for each image. Simplify these description vectors to\njust two dimensions using a dimensionality reduction algorithm, such as UMAP\nor t-SNE. Plot your images in the (x,y) locations produced by this algorithm.\nDiscuss the clusters you observe. (Image: a UMAP plot by Christopher Pietsch\nshowing the OpenMoji emoji collection.)\n\n## Additional References\n\n  1. Yining Shi, syllabus for Machine Learning for the Web (NYU ITP, Fall 2018 and Spring 2019), <https://github.com/yining1023/machine-learning-for-the-web>.\n  2. Gene Kogan, “Machine Learning for Artists,” GitHub, accessed April 11, 2020, <https://ml4a.github.io/>.\n  3. See RunwayML and their learning section that includes libraries for common creative coding environments like Processing. “Learn,” RunwayML, accessed April 11, 2020, <https://learn.runwayml.com/#/networking/examples?id=processing>.\n\n## Notes\n\n1 See the image classification example in the ml5.js library:\n“imageClassifer(),” accessed April 11, 2020, <https://ml5js.org/reference/api-\nImageClassifier/>. 2 See the ml5.js and Teachable Machine projects for in-\nbrowser tools to train a model. “Image Classifier,” accessed July 11, 2020,\n<https://learn.ml5js.org/docs/#/reference/image-classifier>; “Teachable\nMachine,” accessed April 11, 2020, <https://teachablemachine.withgoogle.com/>;\nand Isaac Blankensmith's “ANTI-FACE-TOUCHING MACHINE,”\n<https://twitter.com/Blankensmith/status/1234603129443962880>. 3 Hayk\nMikayelyan, “Webcam2Emoji,” last modified September 16, 2020,\n<https://github.com/mikahayk/ml4w-homework2>. 4 Dan Shiffman, “ml5.js Pose\nEstimation with PoseNet,” _The Coding Train_ , January 9, 2020, video, 14.24,\n<https://www.youtube.com/watch?v=OIo-DIOkNVg>. 5 See the featureExtractor\nregressor in the ml5.js library, and hand trackers such as Ultraleap and\nGoogle's MediaPipe HandPose library. 6 See the sentiment model in the ml5.js\nlibrary: “Sentiment,” accessed July 11, 2020,\n<https://learn.ml5js.org/docs/#/reference/sentiment>. 7 See, for example,\nRunwayML, or the DCGAN tools in ml5.js.\n\n\n# Sound\n\n![c41-fig-5001.jpg](../images/c41-fig-5001.jpg)\n\n**Theremin**\n\nA _theremin_ is a monophonic instrument whose volume and pitch are controlled\nby the positions of a player's hands, relative to two antennae. Using the x\nand y positions of the cursor, create a simple theremin in code. You may not\nuse mouse clicks or the keyboard.\n\n![c41-fig-5002.jpg](../images/c41-fig-5002.jpg)\n\n**Sequencer I**\n\nCreate a sequencer that plays a fixed, looping phrase or a collection of\nsounds. You will need to use arrays to store values that are interpreted as\nmusical notes, chords, rhythms, or other musical parameters. Advanced\nstudents: consider how your sequencer's “score” is displayed.\n\n![c41-fig-5003.jpg](../images/c41-fig-5003.jpg)\n\n**Sequencer II**\n\nCreate a sequencer that allows a performer to record and play back a looping\nmusical phrase. Devise a visual interface so the performer can alter its\nplayback: for example, changing its speed, transposing it, playing it\nbackwards, reordering its notes, or altering its timbre.\n\n![c41-fig-5004.jpg](../images/c41-fig-5004.jpg)\n\n**Sampler**\n\nA _sampler_ is an electronic instrument that triggers sound recordings. Create\na sampler that triggers sounds with key presses. Develop a method for altering\nthe nature of the playback in a way that is coupled to the x and y position of\nthe cursor.\n\n![c41-fig-5005.jpg](../images/c41-fig-5005.jpg)\n\n**Modulation**\n\nCreate a project that uses oscillators and envelopes to continuously modify\nsome parameter of a sound. For example, you might implement _tremolo_\n(periodic amplitude modulation), _vibra_ to (periodic frequency modulation),\nor wah-wah (periodic modulation of a resonant filter).\n\n![c41-fig-5006.jpg](../images/c41-fig-5006.jpg)\n\n**Filter**\n\nCreate a simple interactive program that permits the user to filter a sound or\nnoise source, using the _x_ and _y_ positions of a cursor to control the\nfilter. Experiment with different types of filters and their parameters.\n\n![c41-fig-5007.jpg](../images/c41-fig-5007.jpg)\n\n**Polyphony**\n\nCreate a sonic experience that explores polyphony. Scaffold this on a visual\nsystem (such as a simulation or game) where the pitch of each voice responds\nto the speed, position, or orientation of a corresponding particle or mob. For\nsimplicity, restrict your polyphony to multiples of the same instrument or\nsound.\n\n![c41-fig-5008.jpg](../images/c41-fig-5008.jpg)\n\n**Visualizer**\n\nCreate a music visualizer for a specific musical recording. Your display\nshould respond to the dynamics (volume changes) and, if possible, the\nfrequency content of your chosen music. For simplicity, be sure to select a\nrecording without speech or lyrics.\n\n![c41-fig-5009.jpg](../images/c41-fig-5009.jpg)\n\n**Data Sonification**\n\nLocate a real-time Internet data stream, such as one published by a sensor or\nan API (monitoring weather, stock prices, satellite locations, tweets, seismic\nevents). Create a program that sonifies this data or uses it to control a\nmusical parameter for a generative composition.\n\n![c41-fig-5010.jpg](../images/c41-fig-5010.jpg)\n\n**Delay Line Effect**\n\nUsing a _delay line_ (a collection of time-delayed copies of a sound), create\na program that transforms a specific sound in a customized way. Popular delay\nline effects include echo and reverb but also flangers, chorus, and\nharmonization, when governed by an oscillator.\n\n![c41-fig-5011.jpg](../images/c41-fig-5011.jpg)\n\n**Physics-Based Modulation**\n\nCreate or repurpose some code for a real-time physical simulation, such as a\nparticle system or spring mesh. Use time-varying quantities from the physical\nsimulation to govern continuous parameters of an audio synthesizer. Consider\nusing statistical properties of the simulation as the basis for audio\nmappings. For example, the average horizontal position of a collection of\nparticles could be mapped to the stereo position of a sound, while the\nparticles’ mean velocity could be mapped to pitch. Which mappings produce the\nmost compelling results? Do you note a tight connection between the\nvisualization and sonification of the simulation?\n\n![c41-fig-5012.jpg](../images/c41-fig-5012.jpg)\n\n**Whistle Cursor**\n\nFind a tool to estimate the pitch of a real-time monophonic audio signal, such\nas the fzero~ object (Max/MSP/Jitter), sigmund~ (Pure Data), or ofxAubio\n(openFrameworks). Create an interactive game in which the pitch of the user's\nwhistling is used to steer a spaceship up or down.\n\n![c41-fig-5013.jpg](../images/c41-fig-5013.jpg)\n\n**Synthetic Voice**\n\nBuild a DIY speech synthesizer. Use any audio techniques you wish, except for\npreexisting speech samples or a text-to-speech system. You could synthesize\ndifferent vowels with formant synthesis and consonants with gated noise. What\nword does your machine pronounce best?\n\n![c41-fig-5014.jpg](../images/c41-fig-5014.jpg)\n\n**Sound to Physical Action**\n\nUse a microphone to control an actuator in a sculpture, environment, or\nproduct prototype. Consider how mic placement affects the sounds you'll\ncollect. An example: collect door-knocking sounds with a contact mic; use\nthese to govern a solenoid that makes tapping sounds far away.\n\n## Note\n\nExercises in this section were contributed by R. Luke DuBois.\n\n\n# Games\n\n![c42-fig-5001.jpg](../images/c42-fig-5001.jpg)\n\n**Immaterial Game**\n\nDesign a game that requires no materials, such as one you might play on a car\nride or on a long walk. Your game should require at least two players.1\n\n![c42-fig-5002.jpg](../images/c42-fig-5002.jpg)\n\n**Collision Detection**\n\nWrite code that moves two circles around the screen in an unpredictable way.\n(Consider using something like Perlin noise.) When they overlap, have them\nindicate this by changing color.\n\n![c42-fig-5003.jpg](../images/c42-fig-5003.jpg)\n\n**Whack-a-Mole**\n\nCreate a game in which animals or objects appear and disappear abruptly on the\nscreen, and a player has to click on them to score points.2\n\n![c42-fig-5004.jpg](../images/c42-fig-5004.jpg)\n\n**WASD Navigation**\n\nImplement a control system for the movement of an avatar using the WASD\nkeyboard keys.\n\n![c42-fig-5005.jpg](../images/c42-fig-5005.jpg)\n\n**Recoding a Classic**\n\nRecreate a classic arcade game such as Pong, Snake, Tetris, or Space Invaders.\nAlternatively, _modify_ a classic arcade game (code for which is readily\nfound); your mod should introduce a twist or subvert the game in some way.3\n\n![c42-fig-5006.jpg](../images/c42-fig-5006.jpg)\n\n**Physics Fun**\n\nCreate or borrow the code for an organic-looking simulation, such as a\nbouncing ball or a springy particle system. Now turn it into a game.4\n\n![c42-fig-5007.jpg](../images/c42-fig-5007.jpg)\n\n**Level Designer**\n\nDesign a data format to represent maps in a dungeon game. Use a system of\ntiles so that each map combines tiles of different kinds (e.g., walls, paths,\nwater, treasure). Create a “level design tool” that allows a user to choose,\nplace, and save map data.5\n\n## Notes\n\n1 Dushko Petrovich and Roger White, eds., _Draw It with Your Eyes Closed: The\nArt of the Art Assignment_ (Paper Monument, 2012), 28. 2, 3, 4, 5 Contributed\nby Paolo Pedercini.\n\n\n# Teaching Programming to Artists and Designers\n\n> _What's different about teaching artists and designers to code, in contrast\n> with teaching programming in a computer science context?_\n\nEducator Leah Buechley has observed that artists and designers are accustomed\nto (1) learning by making concrete examples, rather than by studying abstract\nprinciples; (2) working in ways that are improvisational, rather than planned;\nand (3) creating things that are expressive, rather than utilitarian. Taken\ntogether, Buechley's observations delineate some of the key ways in which the\nassumptions, cultures, and presumed objectives of traditional computer science\neducation often fail for artists and designers. In this section, educators\nelaborate on the special conditions and considerations for teaching a “studio\nart course in computer science.”\n\n**Heather Dewey-Hagborg**\n\nI think it's important to teach everyone to code. I think it's a fundamental\nliteracy that we need to be advocating for across the board. But for artists\nin particular, it has to be _project-based._ I think that the only way you\nreally get artists motivated is by having them work on their own thing,\nsomething that they are actually excited about doing. In my experience, giving\na bunch of assignments that they don't care about is kind of useless. Instead,\nif they are trying to synthesize things that they have read or seen a video\nabout into something that they actually want to make, then they will learn it.\nI think that's maybe more important than what the project looks like, or what\nlanguage it is written in, or whether it is something visual or not. At the\nend of the day, people will want to work on all different kinds of things and\nI really do think that once they learn one language well, they can just pick\nup any other one. Really, the aim is to make programming a tool for artists to\nuse of their own volition and with their own agency, to make what they really\nwant to make.\n\n**Daniel Shiffman**\n\nUltimately people are people, so you can teach classes any number of ways to\ndifferent groups. However, I think that the idea of _sketching with code_ is\nreally important as it implies a lightness in trying things out. I've had this\ndiscussion with Lauren McCarthy, who has a really formal computer science\nbackground. She was telling me about how hard it was for her to get used to\nteaching in a creative context like NYU ITP [the Interactive\nTelecommunications Program at NYU Tisch] because in computer science, there is\nthis feeling of having to know everything, how everything works and of being\nable to reproduce everything from scratch in a memorized and almost blue-book\ntest kind of way. In contrast, teaching in a creative environment or in an\narts environment is about being able to embrace uncertainty—where students\ndon't necessarily know everything but they should try it anyway, where\nmistakes might actually lead to exciting ideas, and where there doesn't have\nto have to be a thing that is solved with an exact answer. In a way, it's\nalmost like stream-of-consciousness coding. I don't want to portray the\ncomputer science class in a negative way, but by contrast it's based on\nlearning something exactly and precisely.\n\nTo be honest, it's hard for the students. Every semester I have a student say,\n“Okay, I really feel like I'm not getting this because the thing you showed in\nclass, I can't sit down by myself from a blank sketch and like, write it\nagain.” But nobody can do that, really! Everybody who's programming something\nis programming it because they did something similar before. Or it's built on\ntop of a library and they started with the example, and that's a totally valid\nway to learn.\n\n**Lauren McCarthy**\n\nAmongst CS people, I think there is this feeling that the code itself is the\nart, and that there are all of these ideas and techniques you have to\nunderstand first, like managing complexity and modularity, before you can get\nto really make things. That was really hard for me when I started teaching.\nI'd think, “Oh, we're just showing the students how to make shapes on the\nscreen; they don't understand what programming is about.” But actually that is\ntotally wrong because programming can be about a lot of different things.\nUsually when teaching artists, some of them will immediately fall in love with\nthe logic of it, but for a lot of them, that's not why they're excited.\nTypically they have other goals. If they progress far enough into it, some\nmight start to feel curious about the higher-order stuff of programming, or\nthey may not. As for me, I wasn't interested at all until I realized I could\nmake art with [code], so I think teaching programming to artists is a lot\nabout expanding purist ideas of what programming might be.\n\n**Phœnix Perry**\n\nI think you need to assume no skill. I think it is really important to make\nsure that students have the basics right. I think that you also need to review\nfundamental mathematics in a way that is connected to a visual output. A lot\nof my students were taught math without it having any meaning; they get taught\nsine and cosine but they never learned these ideas in terms of animation and\noscillation.\n\n**_Golan Levin: I can relate. Sometimes I have studio art sophomores who don't\nknow the Pythagorean theorem. I'm like, “Here's a right triangle; let's talk\nabout it.” And when they struggle with it I have to remind them, “Folks,\nc'mon. This is ancient Greek technology. It's the 21st century; you need to\nknow this stuff.”_**\n\nBut it's mathematics, and it's been strangely segmented from the arts. If you\nidentify as a creative person, it's often really hard to see yourself as a\nmathematician or as a scientist, or that you would even have the confidence to\nbe good at those things, right? So I think you have to really reintroduce\nsubjects in a language the artists understand, which is visual and auditory.\nAnd if you can do that, they'll often get it all of a sudden.\n\n**Zach Lieberman**\n\nI think one of the biggest challenges to respect when teaching artists to code\nis that this mode of working tends to be pretty solitary. You're in front of a\ncomputer screen. It tends to be a time-consuming and solitary experience and\nthat can be frustrating, especially if you're used to practices that are more\nphysical, or group-based, or based on discussion. Instead of talking to people\nyou are talking with a compiler. You're in a conversation mediated through a\ncomputer screen, and I notice students who get frustrated with that.\n\n**De Angela Duff**\n\nI've actually taught both computer science and art students, because in our\ndigital media department we have both. Computer science students have a\ntotally different mentality from the art/design mindset and one of the reasons\nwhy they take our class is because they do want to be “creative.” Of course\nyou can't just typecast them…[but] the ones who might be perceived as more\ntraditional computer science students, they don't consider themselves as being\ncreative or having visual art skills.\n\nI have the class post their projects to\n[OpenProcessing.org](http://OpenProcessing.org), so everybody can see\neverybody else's work, and a lot of times those students [will] be amazed at\nwhat other students came up with, and they realize that their drawings weren't\nas ambitious. It's really important that they can expand upon it and remix it\nor whatever you want to call it, as we work with this drawing for a long time.\nSo I give them one week to revise. One of my former coworkers used to say that\nstudents rise to the water level that you give them and usually that water\nlevel is set by somebody in the class. If one of the students does this\namazing thing, then the students look at what they've done and they think, “I\ncan put a little bit more time or effort into this.” I hear a lot of, “I can't\ndraw,” and it's not about that; it's about creative expression and using these\nbasic building blocks like circles, squares, triangles, lines, and points. Yet\nthis actually is sometimes one of the hardest assignments for a lot of the\nstudents.\n\n**Rune Madsen**\n\nOne major difference with teaching designers is that we don't really care\nabout whether the work is done in a “bad” way or if it's implemented in the\nway engineers would do it. We really just care about making the thing blink,\nmaking the thing functional. And then after a long time, the students will\nlearn how to do things properly…but at least when I teach, I'm not that\nconcerned with that in the beginning. In my classes, there is a lot of\ndeconstruction happening that is about technique, but the instruction goes\nfrom the artwork _backwards_ to the algorithms and the systems, and finding\nsystems in things that the students normally don't think about as having\nsystems. So we look at graphic designers from the ‘50s and ‘60s, and things\nlike printed posters that were done before the computer. And then try to\ndeconstruct them into, “So this designer sat down and had to do this thing by\nhand. But how? What was their system”? And then we take that system and then\nwe go backwards, and think, “Okay, how can we do that in code?” So there is a\nlot of luring the students—like we start with the end goal and then show them\nthe way and the reason why code is important.\n\n**Winnie Soon**\n\nFrom my experience, art and design students have been exposed to artworks in\nwhich programming is used in different ways, and so they are not just focused\non what works functionally, but are more interested in the thinking behind it.\nHowever, programming is still historically and culturally rooted in cultures\nwith high standards of professionalism and instrumentalization; it's seen as a\nutilitarian skill. When teaching art and design students, I observe that they\nare very uncomfortable with calling themselves coders or developers. It's like\neven though you might know how to write, it doesn't mean you would call\nyourself a “writer.”\n\n**Allison Parrish**\n\nWhat's great about teaching art students or teaching creative writing students\nis it's rarely vocationally regulated. Nobody is trying to take my class to\nsee if they can get a better job, or very few students are….But it's kind of a\nrelief and a blessing as an instructor to have students who are very eager to\nlearn what I have to say so that they can apply it in their own arts practice.\nWhenever I say something or whenever I teach them something, you can just see\na light go on in their head and they're like, “This is going to let me do this\nother thing that's a part of my own practice.” It never feels like I have to\nmake the class goal-oriented; it never feels like I have to make the class\nabout working towards something other than our interests as artists. For me,\nthe main difference is that there's a little bit of wiggle room. I can say,\n“Let's appreciate this process for its aesthetic effect on us,” instead of\nhaving to ask, “Does this meet these particular criteria in the curriculum?”\nor “Is this going to prepare students to succeed in the tech industry?”\n\n\n# The Bimodal Classroom\n\n> _How do you manage novice and experienced students simultaneously?_\n\nAs programming skills gain wider public adoption, it has become increasingly\ncommon to have classes in which experienced and inexperienced programmers are\nmixed together. This can be particularly vexing in technically oriented\ncontexts, in which the absolute beginners feel lost, while students who are\nalready familiar with the material are bored.\n\n**Taeyoon Choi**\n\nFor me, the best way [to manage differing skill levels in a classroom] is to\ngive two different versions of the same assignment, especially for my\nelectronics classes, where there's often different ways of doing the same\nthing. I like to give challenges to the experienced people who have done it\nonce, and for the beginners, give them enough time to learn. I want to avoid\nhaving one person done in ten minutes while there is another person taking an\nhour to do it. A more hands-on approach we use at SFPC [the School for Poetic\nComputation] has been to encourage the advanced students to mentor the\nbeginner students—not in a hierarchical way, but to have them solve the\nproblem together. That has mixed success; it works for some assignments and\nnot for others. Something that I learned from other teachers at SFPC is to\nencourage the more advanced students to build teaching materials. They may be\nable to solve a problem quickly, but for them to come up with new assignments\nfrom that code or new assignments for another toolset is a more conceptual\nchallenge. It's also important to understand that even the most advanced\ntechnical students are probably not experienced teachers or artists, and are\nprobably not confident explaining or helping other people, so there's a lot\nfor them to learn from that practice.\n\n**Daniel Shiffman**\n\nFor a beginner, I try to have an approach of looking at a particular example\nin incredible detail—so, line by line, making sure every little piece is sort\nof talked about and understood, and building up this example. In theory I\nmight do that for half the time and then the other half of the time I look at\nan example from a higher level, talking about the concepts and demoing it in a\nway that we can have a more free-form conversation, where it's okay if the\nstudents don't suddenly know every part of how the code is working. In some\nways I oscillate between shooting for the low end and shooting for the high\nend, as for me, the place in the middle is sometimes problematic. Like, “I'm\nkind of showing you the code, but not really” or “I'm just giving you some\nthings but not all of them.” This can get confusing. I feel like it's helpful\nto either go through the process step by step or to just run an example and\ntalk about what it is and why it's meaningful and important. Then students can\nlook at the code later.\n\nIn some ways a lot of what I do is creating a feeling of ease in the\nclassroom. I often joke that I don't know if I'm teaching anybody or if I'm\njust giving people the feeling that they're learning. Ultimately students have\nto learn it on their own when they go and try the stuff, and so it's important\nto have people feel empowered and feel that they can do it.\n\n**Rune Madsen**\n\nI teach a class on graphic design and code. What I say in the beginning is,\n“Some of you are very experienced programmers, but you may not have the\nexperience in the visual arts that some other people have, and that is a skill\nyou will have to acquire. So, it's as hard and as important for you to work on\nthose things. And equally, if you have never programmed before, you might have\nskills in other places.” So it's not a situation in which students who can\nalready program are considered amazing, but students who are maybe design-\ndriven but have never programmed are regarded as not knowing anything. The\nother consideration that I have is the pace in the class. I try to never shoot\nfor the lowest common denominator in my class because it does a disservice to\nall the other students. So I set the bar pretty high. But then in the\nclassroom, I talk a lot about fundamentals that everyone can do. And then if I\nwant, I can be very clear about saying, “Now we're going to talk about\nsomething that might be a little hard for some of you. That's completely okay.\nIf you want to, you can play around with this in the meantime, but if you're\ninterested, feel free to follow along.” And then I talk about something that\nmight be a little bit more advanced. Or I'll say, “So everybody practice this\nnow, and I'll go around and help you. If you're really up for a challenge, do\nthis other thing.” So I try to have multiple tiers always going at the same\ntime.\n\n**Winnie Soon**\n\n99% of my students are beginners with no programming experience, but after the\nfirst few weeks you do start to see who is really into programming, as these\nstudents spend hours and hours on it. Then you see different skill levels, so\nthis issue emerges. I alway emphasize that my course is “low floor, high\nceiling,” which is discussed by Seymour Papert and Paul Wang in their work on\ncomputational thinking. When you introduce programming it has to be with a low\nbarrier; it has to feel accessible so that you can get started. But at the\nsame time it has to feel like there is no limit, no ending, and it's possible\nto go deeper and deeper. So every assignment I set, even if you are a beginner\nin terms of coding, you are still able to submit something—with references,\nwith sample code, with tinkering. My assignments aren't computer science\nassignments where there is a correct answer. Instead, they are conceptual, so\nthat the more advanced students can go really deep and apply more complex\nprogramming skills.\n\n**Phœnix Perry**\n\nUsually what I do is I pull the experienced people aside after class and be\nlike, “Whoa, you're awesome, why don't you go and check out this book. You can\nignore me when I talk, if you show me what you can do with this.” I give them\nstuff that's outside their skillset, that's actually really hard material, and\nit's like a puzzle and then they sort of ignore you in class, and at the end\nthey come back with something. The other thing you can do is turn them into\nTAs, but the danger with those students is that their knowledge is usually\npatchy in that they think they know more than they actually know. I've seen\nthose students become the worst in the class because they get lapped by the\nothers who are paying attention and are systematic in a pedagogical way.\n\n**Jer Thorp**\n\nI've learned to not be very rigid in the way that I define projects, and to\nallow people to operate within those projects at a comfort level that works\nfor them. The first time I taught my class, I defined the projects so rigidly\nthat the difference between the good coders and the bad coders would really\nstand out, but by defining the projects less rigidly, it allows people to play\nto their strengths more. That's something that took me a long time to really\nembrace. My projects are now about a theme and not about how I insist that you\nwork on that theme.\n\n**Lauren McCarthy**\n\nI really try to push that it's not just about wanting to program. A lot of\ntimes, if you're a very good programmer, that probably means that you haven't\nspent a lot of time thinking about art or design or user interaction or any of\nthese things. So I just try to emphasize that, but also when we are giving\nfeedback, I try to call on people who have these other skills to make everyone\nrealize that these are just as important and relevant. Setting expectations in\nthe beginning is helpful. I let the advanced students know that technically\n[the class] might feel a little bit slow, but that gives them time to explore\nsome of their other ideas. And also vice versa—I try to help the students who\nare beginners understand that some [of their classmates] have done this\nbefore, but [their inexperience] doesn't mean that they are bad at it. I\nencourage them to not feel intimidated, to survey their skills to see what\nthey are missing, and to ask themselves how they might work on those skills.\n\n**Zach Lieberman**\n\nI deal with this a lot as most of the classes I teach tend to be mixed…I think\nhaving classrooms where there are people with different skill sets is really\nvaluable, specifically in terms of the community. What you want to do is\ncreate conversation, create modes in which beginners are asking questions of\nexperts and [there is] a chain of learning. When everyone is in the same place\nit's much harder to do. What I try to do for the experts is give them prompts\nwhere they feel like their input and time in the classroom is both useful and\nvaluable. I often ask them, “Can you take the things that we're talking about\nand create tools and techniques to teach this better?” I try to turn them into\nteachers in some way and I try to give them prompts that push them to have\nthat sort of mindset. There are also ways of making everybody a beginner,\nbecause everybody is a beginner in something. So in a classroom where there\nare both beginners and experts, can you figure out ways to push the people who\nthink they know the material and flip what they are doing so that they realize\nthat there is a lot that they don't know? I like to show them the problem set\nin a different way, like in a different language or with different\nconstraints.\n\nAt some point in a bimodal classroom, I also think there is value in pulling\nout the beginners or the people who feel like they are lost, and creating a\nsafe place where this small subgroup can meet. So in a classroom of 15 you\nmight have 4 or 5 students who meet privately and you try to raise their\ncomfort level and confidence. They often need a safer space to ask questions\nthey might not ask in class because they are shy about asking.\n\n**Luke DuBois**\n\nI have to do the “one-room schoolhouse” thing a lot. Sixty percent of my\nstudents are products of the New York City public school system because we're\nNYU's last commuter school and unfortunately, most of my students didn't have\nthe chance to go to Bronx Science or Brooklyn Tech. They went to places where\ntheir computing class was maybe how to use Dreamweaver. Some have done CS and\nthey have some math, but nobody ever thought to take them to MoMA or the\nGuggenheim. One of the things that's really nice about my little proxy for an\nart program inside an engineering school is that in an engineering school\nyou're encouraged to assign lots of group design work. And so I can figure out\nthese orthogonalities between the students pretty quickly and pair them in\nways so that they match each other and so that they have to teach each other\nstuff. I try to do that as much as possible. It's hard—like this year it's\ngotten really hard because we started admitting new graduate students coming\nfrom other parts of NYU that cover recorded music and undergrad photo. And\nthose kids have a very high cultural fluency, but they don't have any of the\ncomputer chops. Or if they do it's like they know how to use a crack of\nAbleton really well but they don't know how to code. In response, I made every\nassignment on how to use photography in some way, but everybody had to have a\nsoundtrack. I “de-oculocentrized” the curriculum and everything had to have\nmusic, no matter what.\n\n**De Angela Duff**\n\nI'm a huge believer in pair-programming, or peer programming. I usually pair a\nmore seasoned programmer with one who is less seasoned or a novice, and I have\nthe novice do the programming and have the more experienced person be the\nobserver. I also encourage the students to form external study groups, as\nsometimes I find that the students who already have experience programming\nactually really want to help the other students, and that one of the ways they\ncan do this is by spearheading a study group outside of class. I might also\nhave a student explain a programming concept using different language than\nwhat I'm using.\n\nI always tell students to use multiple resources when they learn how to\nprogram. I always assign two different textbooks and then tell them to try to\nget a third resource if they can. They all have different examples and explain\ncertain concepts differently, and I think the more the merrier when they're\ntrying to understand something for the first time.\n\n**_Tega Brain: You mentioned that you sometimes teach multiple languages in\nparallel._**\n\nI always do. I don't want students thinking “I can only program in X,” because\nI teach programming as a tool and I want my students to be able to program in\nany programming environment. If they know what a loop is, or what an array is,\nor what an object is as a concept, then they just need to figure out the\nsyntax. I teach three programming languages at the same time only because they\nusually drop one. I don't tell them two is fine but they typically gravitate\ntowards two out of three. I structure the class so that we're learning the\nbasic concepts two-thirds of the way in all three languages, and in the last\nthird of the semester they select their own project. For the week-by-week\ndeliverables they have to write them in all the different languages, and in\nthe final third of the semester they can select one of those languages for the\nfinal project. I give them extra credit if they build it in two languages.\nMostly they don't like it at first, but I do find that students rise to the\nwater level that you expect of them. Hopefully it helps them learn new\nlanguages when they need to, without being afraid.\n\n**Allison Parrish**\n\nTeaching technology with the arts in mind kind of gives you a leg up on that\nproblem automatically. If you consider my class as primarily a programming\nclass—which it isn't, but if you look at it from that point of view—it's the\nconceptual part that levels the students to some extent because there are some\nstudents who, despite being beginning programmers, have amazingly strong\nartistic points of view and are aware of the history of procedural writing or\nare into conceptual writing in general. Even with the very rudimentary tools\nthat I'm teaching at the beginning, they're able to make something that\naesthetically stands up to whatever the advanced programmers are making off\nthe bat. I often teach in Python, and Python used to be a language that not a\nlot of people knew, and so that was another equalizer for the class. That's\nbeen less true more recently in the last couple of years as I get more\nprogrammers coming into my class who have a deeper understanding of Python.\nBut then the subject area [of creative writing and computational poetry] helps\nwith that problem. Not a lot of computer programmers know how to do that, so a\nlittle bit of every tutorial is going to be new to everyone regardless of what\ntheir preexisting programming skill is.\n\n\n# Encouraging a Point of View\n\n> _How do you encourage meaning-making, criticality, perspective, and heart in\n> otherwise technoformal education?_\n\n“More poetry, less demo.” This is the motto of the School for Poetic\nComputation (SFPC) in New York City, an artist-run school co-directed by\nTaeyoon Choi, Zach Lieberman, and Lauren Gardner. The motto encapsulates an\nexpanded ideal for the education of computational literacy: one that frames\n“learning to code” not as mere vocational training, but as central to\ncultivating a student's critical perspective and expressive voice. In this\nscheme, “creative coding,” by analogy to creative writing, is an essential\npractice. Simultaneously, this motto also critiques a prevailing technophilic\nattitude: one characterized by technological solutionism, a seemingly\napolitical preoccupation with “disruptive innovation,” and the narrow\nobjectification of programming education as an economic stimulant. The SFPC\nmotto is also, in its own modest way, an aspirational rebuttal to older\ncritiques of computer art as hollow, impersonal, and instrumental. In this\nsection, we ask educators how they work to ensure their students’ focus on\nmaking meaning, in spite of technoformalist tendencies in programming\neducation.\n\n**Taeyoon Choi**\n\nThat is a really good question because it's easy to retreat to this mode like,\n“We are going to help you become a coder; we're going to help you become an\nexpert.” But to me that sounds as thin as, “We are going to help you become\ngood at Photoshop.” I think the real question is about finding a desire within\nstudents to master the language of the technology. It's code and electronics\nand all of that, but it's really about expressive literacy. I want them to\napproach code and technology as an artistic medium, so they can not only\novercome the psychological barrier of “I can't do this” or “this is not for\nme,” but also be creative with the medium. One thing I like to do is give\nexercises that are as simple as possible. For example, one button, one output,\nlike pressing a button that lights up an LED, and putting delays in between. I\nlike to show that you can make a lot of meaning with very little technology.\n\n**Zach Lieberman**\n\nI try to bring in as many outside examples of things that I think are\nimportant, even if they are not necessarily made with code, but artworks or\nvisual ideas or projects that inspire me….A lot of teaching is about the\npublic articulation of values in a classroom, and that helps students\nunderstand your value system. Maybe they have their own value systems or\ninterests, but seeing your curiosity, and what you think has heart, helps them\nbring that into their work.\n\n**Jer Thorp**\n\nI've been lucky to teach programming in environments where people want to find\nmeaning in the work. I throw a lot of theory and a lot of reading and a lot of\nweird stuff at the students. Often I assign a novel that they have to read\nduring the class. In the past I've assigned Gary Shteyngart's _Super Sad True\nLove Story_ and normally, the response is overwhelmingly “Oh, that was great!\nGive me more of that.” Almost nobody says, “Please teach me more technical\nthings.” The response is almost always, “Let's get into the weird stuff more.”\nI would encourage people to think about ways to do that, even in a basic\nprogramming class. Give small readings. There are some great Pynchon passages\nthat are so good for this type of thing. Ask them to make something in\nresponse, to remind them that it's not all about the computer.\n\n**Lauren McCarthy**\n\nI like to show a lot of references and artworks that have nothing to do with\ncode, especially when I'm giving assignments. I ask students, “What would this\nbe like if we were doing it with the tools that we're using? How would this\nidea manifest?” I also put a really big emphasis on context, especially as we\nget closer to the final project. For example, they are required to come up\nwith several examples of influences or other things that inspired their work\nbeyond the “I learned how to make a for-loop” or “I went to office hours” type\nresponse. This is often really hard for some people at first and when I ask\nfor their inspiration, their response is literally “office hours with the\nresidents.” So I always say: “Keep looking!”\n\nThe Web is also a really ideal social context for sharing and publishing work.\nWe taught our intro class with the Web this semester and it was cool because\npeople are already so familiar with that space…everything from websites, to\nnews, to ads, to videos, to art pieces. I also try to bring in news and\ncurrent affairs, like, “Hey, this lawsuit is happening, or this new regulation\ngot approved. What does that mean for us?”\n\n**Tatsuo Sugimoto**\n\nIt's important to balance the technical and the conceptual, but developing\nconceptual thinking can be very hard, especially in Japan where the art and\ntechnology scene is very commercial. Japan is very different from the USA or\nEurope—of course there are some artists using technology, but most of the work\nis coming from designers and from commercial studios. My students know less\nabout artists and art practices, but are very familiar with commercial work. I\nthink art is an important space for being critical of society, but many\nJapanese young people don't often think this way. For example, in Japan,\ndrones are popular in entertainment and live performance, but on the other\nhand they are also used for military purposes and for surveillance. People\ndon't think enough about this. I don't have the answers, but I show a lot of\nexamples of international art projects to try to draw my students into\nthinking about the politics of the tools they are using.\n\n**Daniel Shiffman**\n\nEarly in the semester, I tell them, don't worry about making a _magnum opus,_\nyou don't have to make a really important, meaningful thing: just play and\nexplore. Let your work wander, and in that sense, have a free spirit while\nyou're doing your assignments. It's okay to focus on, “I just want to try this\nthing with a for-loop, because that's what we're learning and I don't want to\nget lost in my own head about having to solve this climate change crisis; I\njust want to play around here with a for-loop.” So it's OK to free yourself of\nthe ideas at some point, and then—and this often happens at the end of the\nsemester—flip it and free yourself of the technological requirements. I always\noffer to students, “You don't have to use code for your final project,” but\nnobody ever really takes me up on this. I say, “If you have a project that\ncame out of the ideas we're talking about and you're excited enough to make it\nwithout code, go for it and explain why.” I'm sure that the process of a\nstudent explaining why they're not using code would show there's probably a\ngood reason.\n\nThe other thing students get really lost in is, “But I think it's so simple, I\nfeel like it needs more.” It isn't bad if it's simple; that can be good. Worry\nabout your idea or what you're trying to communicate at that point and forget\nabout the technology.\n\n**Heather Dewey-Hagborg**\n\nIn my Intro to BioArt course…I'm trying to teach technical skills but I'm also\ntrying to teach something that students don't really know anything about, and\ntrying to bring in critical and ethical questions along with that. The way\nthat I structured the BioArt class is around four projects: three topical\nprojects that each involved several sets of skills, and then one final\nproject. Each project is a module, and then each module had a set of critical\nreadings to frame the conversation for which the students had to write a page.\nBefore we came into class they would discuss the readings and I had them all\nsubmit a very short essay the day before, just to make sure they actually read\nit and thought about it.Then in class I went through their writing and pulled\nout one thing from each student's writing and tried to bring that into the\nconversation. So before we actually started working hands-on with the\nmaterials and the methods of the technology, I tried to frame it with those\ncritical questions in mind. Then when they started on their final projects, I\nalso tried to bring those questions from the readings they critiqued back into\nthe discussion of the project. It was very natural this time because the\nstudents had already been thinking about these issues a lot and so as they\nstarted proposing projects or sharing their work, the questions naturally came\ninto the critique from all directions and not just from me. I feel like I‘ve\ngrappled with this question for a long time when teaching programming and even\nduring the last semester with the BioArt class. This was the first time I\nthink I've ever come kind of close to getting it right.\n\n**Luke DuBois**\n\nI mostly have engineering students, and the hardest part of teaching engineers\nis to get them to know that they are allowed to ask questions too. That they\nare allowed to draw from their own lived experience and problematize a\nsituation. These things are not normalized in their educations so they're\nflying blind when you ask them to do that. We have hybrid classes, such as one\naround developing assistive technologies. It's an undergraduate disabilities\nstudies class that requires client-centered design and we do it in partnership\nwith United Cerebral Palsy of New York. We put two undergraduate engineers on\na client, so it could be computer science students, it could be civil\nengineering students, or others. And the first thing they have to do is make a\ndocumentary film about their client that helps teach empathy, and then they\nhave to make a design intervention.\n\nA problem with engineers is that they're trained to always go for the most\ngeneral solution of a specific problem. But that doesn't really work in this\ncontext, so instead of saying, “You're going to make an adaptive wheelchair\nthing and we are going to patent it and we are going to get it approved by the\nFDA,” instead, it's more like: this is Steve. Steve has cerebral palsy, Steve\nhas residual motion in two fingers on his right hand, Steve needs an umbrella.\nNow design that for Steve. That's an actual example of a real case, and they\ncreated this mechanized umbrella. In the Ability Lab, we often do “design for\none.”\n\nWith societal-scale problems, solutionism falls apart really fast and that's a\nreally useful lesson for undergrads—to understand that you can't necessarily\nfix things by building a better widget. So we encourage “meaning-making” with\nour engineers by giving them problems that don't or can't have exclusively\ntechnological solutions. I'm not as good at this as my colleague Dana Karwas,\nwho is really good at pushing them to reveal aspects of their lived experience\nand to riff on that. Dana does a lot of activities with her students to help\nthem address things that have happened in their lives, often bad things, and\nit's a way of distilling art out of shit.\n\n**Winnie Soon**\n\nMy course Aesthetic Programming is different from a typical coding course in\nthat every week we have one or two conceptual texts the students have to read.\nFor example, in the first week we teach shapes, like drawing rectangles or\nellipses, so I usually give them something to read on representational\npolitics, like that of emoticons, or something about the politics of geometry.\nSo it's about shapes, but also how can we think about shapes differently, or\nhow when we use emoticons, how colors matter. How we assume one emoji is\nrendered the same on every device, but this universality is not the case. I\nhave them think through the problems that arise because of this kind of\ntechnical infrastructure.\n\nIn terms of assignments, each week my students make a RUNME and a README. A\nRUNME is technical, like writing a program. At the same time they need to\ncontextualize it through the README using the text they are given to read.\nThis forces them to not only code, but also think with and through coding and\nthe text. I provide a brief for both the RUNME and the README. For example,\nthey may have to redesign a progress bar for the RUNME. I give guiding\nquestions for the README like: what does a progress bar hide or show us? How\ndoes it link with the text, to help us think about temporality? The README is\nlike a description of the program, but also a discussion on why you think you\nare making this. It forces them to think beyond, “I just want to make a game.”\n\nMy course is not about making something that looks cool; it's about the\nprocess of understanding software systems as cultural phenomena. It is about\nusing code to increase your sensitivity to tools like Facebook or other\nplatforms. If you know how data capture works, then you can reflect on how you\nuse Instagram or other tools.\n\n**Allison Parrish**\n\nI am personally very excited by systems and by rules and by the act of putting\nthings together programmatically, sometimes like a poet but first and foremost\nlike a computer programmer. When I'm teaching classes it's similar in that my\ncourses are about making computer programs produce creative text towards an\naesthetic end…I have to focus a lot on the technical aspects and say, “We're\ngoing to learn this thing that is applicable to all this conceptual stuff, but\nwe're also going to learn how to make a dictionary or learn how to parse text\ninto parts of speech.” …Of course I would love to be able to teach a class\nwhere every single student in the class is completely devoted to exactly the\nsame concepts as I am and is motivated by exactly the same things that I am,\nbut that would actually probably be boring. I really try to keep it half my\nweird conceptual ideas and half programming tutorials. The conceptual stuff is\nwhat allows the programming stuff to be applied in a way that's not just a\ntechnical worksheet that demonstrates mastery.\n\n**_Tega Brain: You also require your students to perform their work in public.\nWhy do you do that?_**\n\nThat's for a couple of different reasons. The first is that text as a medium\nhas all of these affordances…it can sit on a screen, it can be on a printed\npage, [or] it can be transmitted into language and read out loud. I think it\nwould be a disservice in a class that's about language to only use the one\naffordance of text on the screen. People don't usually think about procedural\ntext as something that you can read out loud, but when I have the students\nread their work out loud they learn new things about the text. It's like\nhaving another window into the language because when you read text out loud\nyou find out the places where you have to breathe, what sequences of words\nsound really good, what sequences of words trip you up, how long it takes to\nread.\n\nWhen you're reading out loud you also become hyper-aware of other people's\nreactions. You can literally see when people get bored with what you're saying\nand use that in the feedback loop to bring that back in and ask, “How am I\ngoing to compute a textual artifact that has a different curve when it comes\nto how people are paying attention to it?”…I also find that it keeps students\na little bit on task and focused because it means that students take a little\nmore responsibility for their work. They know it's not just something that's\ngonna sit in a blog post until the end of time. They know that for two to five\nminutes of in-class time they're actually going to have to be there, in their\nbody, allowing their body to speak this text out loud. I find that that makes\nthe work a little bit more thoughtful and a little bit more interesting.\n\n\n# The First Day\n\n> _What do you do on the first day of class?_\n\nIt's the night before the semester begins and you can't sleep for worrying\nabout tomorrow's class. The first day is often nerve-racking; it's a critical\nmoment to set the tone for the semester and shape student expectations. In\nthis section, our respondents discuss their different approaches to teaching\nthat first class of the term.\n\n**Zach Lieberman**\n\nOn the first day of the School for Poetic Computation we start with questions,\nand it is really fascinating. We teachers come in and explain a little bit\nabout what we do and I ask students to take 20–30 minutes by themselves and\nwrite down every single question that they have. And this creates a really\ninteresting and really strange moment. I want to know whatever has brought\nthem into the classroom and what questions they had in mind. These questions\nbecome signposts or markers for what we talk about. So someone will say, “I\nwant to learn about X, Y, Z,” and someone else will put up a response saying,\n“Come talk to me.” Some questions are unanswerable, like really deep and\nprofound questions, and I love that some students will try and cross questions\noff the list as the class progresses. I think it's important grounding. A lot\nof times you come into the classroom and the professor gives you a syllabus\nand it's like: here's where you're going to go, here's the journey we are\ngoing to take, we're going to cover these things to get to that point. So it's\nnice to start on the first day and say: “What we are going to do here is\nreally _driven by questions,_ driven by a collective exploration of\nquestioning.”\n\n**Daniel Shiffman**\n\nOn the first day, I always feel very anxious all morning before class. I\nwonder, “What am I going to do for two and a half hours? I mean, that's so\nmuch time!” And then I over-prepare and inevitably only get to like a tenth of\nwhat I meant to. It's different in different contexts, but one of the things\nthat I do if possible is to not look at any code until the last five minutes\nof class, if at all. In the past I've done a conditional drawing exercise,\nwhere the students pair up and one writes instructions that the other has to\ndraw.\n\nAnother thing that I also do is try to have a discussion about historical\ncontext, programming languages, and what it means to program. Like, why should\nyou program? If I'm in a class of true beginners, I like to talk about what\nprogramming languages the students have heard of and what they think people do\nwith them. I ground everybody in a larger landscape, which is a nice thing to\ndo.\n\nWhat I also find is useful is to show work people have made in previous years\nand to really focus on projects that do some social good, or projects that\njust have this totally nonsensical, playful quality with no practical value\nwhatsoever. It's easy to imagine certain kinds of interactive exhibits or\ncertain kinds of games, so I try to showcase projects that are a bit\ndifferent. One thing that I have become much more conscious about recently is\nmaking sure that…I have a diverse set of people who've made the projects: from\ndifferent communities, different genders.\n\n**Lauren McCarthy**\n\nI like to tell a lot of jokes that nobody gets, and if I keep it up, by Week 7\nI might at least get a pity laugh. In the introductory class we do a lot of\nborrowed things, like “Conditional Design” drawing exercises, which is also\nsomething Casey Reas does. If I'm teaching JavaScript, we pull up an example\nonline and I show them how to open the console and start messing with the\nJavaScript or CSS that's running to get to the idea that it is all hackable. I\nalso try to ask who's feeling nervous or scared or unhappy or thinks they're\ngoing to be bad at this class. I think a lot of times everyone comes in\nthinking that they're the worst student in the class or that they're the only\none who is not going to get it, and sometimes I think that it is reassuring to\nsee that everyone is terrified.\n\nOur Social Hacking class [taught with Kyle McDonald] has special requirements.\nOn the very first day, we have students sign a contract that says a few\nthings. Firstly, that we are asking them to experiment but that they\nacknowledge that the experiments are theirs so we're not liable for their\ndecisions. Also that they take into consideration that they are doing work\nthat involves other people and that just because these may be art projects,\nthat doesn't give license to not respect a person. Also that they acknowledge\nthat we are asking them to take risks and they must be willing to do that or\notherwise they should drop the class. Then lastly, that as everyone is taking\nrisks, we need to respect that and not shut anyone down for doing so and\nrather we each need to respond to this. In this class everyone puts themselves\nout there, and so we all need to put ourselves out there by giving feedback\nabout how other people's work makes us feel.\n\n**Winnie Soon**\n\nI usually explain the intention of why I choose to work with particular tools.\nWe talk about how there are different coding languages and different coding\neditors and I will talk about why I chose GitLab to host my syllabus. Why did\nI choose to work with p5.js? What are the priorities of this software and how\nare values embedded in software? In a way, choosing a tool is a kind of\npolitics and I want to set the scene that the course is not about picking\nsomething because it is efficient or good and then just using it. I want to\nask why you are using it and what kind of values you are subscribing to. I\nthink that is quite important.\n\nThe second thing is more about creating a space for them to speak. A lot of\nthem have no programming experience and they usually come with a lot of fear\nand anxiety. I am very particular and I ask them to say something about their\nfeelings about code. I want to focus on feelings and the emotions attached to\ncoding, rather than on “why I want to learn C++” and so on. At the end of the\ncourse in the year before, I always ask the students for a sentence on a Post-\nIt note on what they want to channel to the students in the coming year. How\ndo you want them to prepare for this course? Usually there are comments like,\n“Coding can be queer.” “Be open.” “Coding can be something very fun and\nconceptual.” “Don't expect code to always work.” They see these Post-its with\ntheir peers’ words and it's much more convincing than me saying these things.\nI usually stick them on the wall and ask them to come and take a look. I want\nto set the scene that this is a rigorous course, it's difficult, but you are\nnot alone. If we work together, you can pass through this just like these\nstudents who are able to give you this advice.\n\nThe third thing I find very useful is to ask them to think about a reason to\nattend this course, beyond that it is mandatory. As coders and teachers, we\nknow that this stuff is not easy. If they have zero coding experience, it's\ngoing to be difficult to sustain their motivation throughout the entire\ncourse. So I want them to think in the first class of why this is beneficial\nfor them, e.g., I can get a better job, I can communicate between programmers\nand business leaders, or I can understand the black box of computing culture.\nWhatever it is. They need to have a reason. I then emphasize this in a\nlighthearted way. I tell them: remember what you have written down and keep\nthis motivation. If you feel fear or anxiety about your work, get these words\nout again and look at them and think about why you want to learn and why you\nwere once motivated. This will be a helpful tool to pass through the difficult\ntimes.\n\nI try very hard to prepare them psychologically for this challenging class and\nremind them that we will work together. This is the difference between an\nengineering class and a non-engineering class. For engineering and CS\nstudents, they have the motivation to learn coding; they want to be\nprogrammers. But for arts and humanities students, they are scared of this\nweird language. And you have to find a way to bridge that.\n\n**Heather Dewey-Hagborg**\n\nWe talk about conceptual art and code as an instruction-based medium. Then I\nhave one student stand at the board with a marker, and then the other students\ninstruct them how to draw something…we go around the classroom and the\nstudents each give an instruction to the person. It's fun and it helps them\nthink about how specific or not specific their instructions are.\n\n**Jer Thorp**\n\nI'm really aggressive on that first day and we do a lot of things. At that\npoint, everyone's brains are so pliable because it's all foreign…. I've found\nthat the longer I wait to get to the trickier stuff, the harder a time they\nhave with it. Things start to calcify in their brains, and that flexibility\nthey had on the first day disappears because they are building these\nconstructs. If I wait until the fourth session to talk about what a class\nmight look like or what an object is, by that time they are like, “Wait, wait,\nwait, no, this doesn't mesh with the construct that I've built in my brain\naround this.” So if you put it in the first day, you don't have to teach it\nreally heavily and they are like “Oh. Okay, that's how this works.”\n\n**_Golan Levin: What about the first day of an intermediate studio in\ninformation visualization? What does that look like for you?_**\n\nMy information visualization class is half theory and half practice, and there\nwe take a pretty different tack. On the first day in the first half of the\nclass, I write the word “data” up on the board and we talk about what that\nword means, which is actually an incredible little rabbit hole. Everyone\nthinks they know what it means, but nobody actually knows what it means. Then\nwe really talk about “Where does data come from? What does that process mean?\nWhat is it? How does it manifest? What do we do with it?” What I'm trying to\ndo is I'm trying to get them to define what I think of as the data pipeline.\nData is the result of measurement, and we then parse that data using computers\nin some way, and then there's a representation step. I want them to make that\nmap for me, to start identifying interesting places to intervene. For me, a\ndata class is not about that representation piece; it's about the other pieces\nbecause I think they're so much more interesting.\n\nIn the second half of that first day, I try to get the students to create a\ndataset on the fly and think about what that means. A simple one is asking\nstudents to say what they think their level of programming skill is between 0\nand 10. Then we take those 16 answers and do a very simple plot of them, and\nthen we come back and say, “What would have happened if I would have asked\nthis question differently? What would have happened if I would have given you\na clearer way to bound [your answer]? What would have happened if I allowed\nyou to share, if the first person read their answer aloud, and then the second\nperson had to base their answer on that person?” This gives the idea that\nthese numbers already carry a fantastic amount of bias. Even in a really\nsimple exercise like that, you can't really do it the “right way.” We can talk\nall we want about how to represent that data, but actually there are a million\nthings that have happened in producing it that have as much of an effect, if\nnot more of an effect, on what end the reader will see.\n\n**Taeyoon Choi**\n\nI usually prepare a pretty long lecture. This gets the students who are into\nit excited, and also shows what the class is about to the ones who are not\ninto it so that they can see my lecture and leave. I think this is a really\ngood thing. Also, when I write the course description, it's usually like four\nmonths before the course actually starts and my idea of the class was\ncompletely different [then]. This then becomes a chance for me to realign what\nI want to do and what students think the class is. I rarely teach technical\ncourses now, but when I do, it's usually contextualized, so it has some\nconceptual arc. I explain what I can teach and what students need to learn on\ntheir own. I try to make it clear that learning technique is really hard, and\nit's dependent on where you're coming from. I don't try to give them the\nexpectation that they'll be good at technical stuff by the end of the course.\n\n**De Angela Duff**\n\nI try to demystify their fears, and let them know that it's not magical and\nthat it takes what I call “butt-in-seat time.” You just have to really spend\nthe time to either watch Dan Shiffman's videos, or read out of the _Learning\nProcessing_ book. I cover more of the philosophy behind programming. I also\nsort of quiz them and ask them why are they here, beyond the class being a\nrequirement. I want to know their expectations, and I try to get a sense of\nwhat their programming background is. We don't talk about code or anything\nuntil probably the third class. The very first assignment I give is actually\nhaving the students create directions for something that they do in life, just\nwritten instructions….I make them do that before we even start talking about\ncode to draw upon the point that the computer only does what you tell it to\ndo….They then have another classmate execute those instructions, but they\ncan't discuss it. Like the person writing the rules can't talk to the person\nexecuting them. The person who wrote the rules can observe and then modify the\nwritten rules to improve the outcome. I find that a very useful prompt for\ngetting people used to the idea that if you're not very direct and literal in\nyour language, then the program won't execute, and it's usually because you\nhaven't broken it down in enough steps.\n\n**Tatsuo Sugimoto**\n\nDon't be a maniac. The beginning of a class is very important. The students’\nfirst touch and first experience with code makes a big impression. Although as\na teacher you want to show your students that you have a deep knowledge of the\nmaterial, it's more important to go slowly and casually, and stay open to\nwhere they are at.\n\n**Rune Madsen**\n\nI remind them that they are in the beginning of a semester course. They are\nbasically safe from thinking too much because I'm going to take away, I'm\ngoing to put so many constraints on them in the assignments in the beginning\nthat their creative freedom is very small. So the first assignment we do is at\nthe end of the very first class. I tell them to design an ice cream cone using\na triangle, an ellipse, and a rectangle, in black and white only. If you're\nyoung and you're a student and you've never designed before, which many of my\nstudents haven't, the whole palette of everything that can be done is just way\ntoo big. So I try to impose core constraints and have them work on very\nspecific things. Like, how to position three basic shapes. How do I make it\nsay “ice cream cone”? How can I make it say “sad” or “happy”? So the first\nclass is very much about, “You're here. Don't feel bad about not knowing\nthings, because I will make the assignments so simple, that at least in the\nbeginning there's nothing to worry about.”\n\n**Allison Parrish**\n\nIn my text processing classes, I take them straight into the UNIX command line\ntools. I do this because the people who take my classes usually have an OSX\ncomputer, so they have all these tools already built-in and the UNIX command\nline has this reputation of being super forbidding and difficult to get into,\nor for hackers only….Once they know how to open this imposing terminal window\nand then type something into it and make something happen, it instantly feels\ngood to be able to add that to your repertoire. I focus on the command-line\ntext processing tools, tools like _grep,_ _head,_ and _tail_ and other\ncommands that are for transforming text in addition to tools for searching. I\nrelate that back to a conceptual presentation, where we talk about work from\nthe Dada movement, conceptual writers, and poetry from _L=A=N=G=U=A=G=E_ [an\navant-garde poetry magazine, published between 1978 and 1981]—here's how you\ncan either repeat these methodologies or their aesthetics using these tools\nthat are already on your computer and that really are wonderful to use. Of\ncourse, I also cover what the class is going to be about, but [learning about\nthe UNIX command line] makes them feel powerful and like they have dug into\nsomething that's unique to this particular class.\n\n\n# Favorite Assignment\n\n> _What's your favorite assignment for computational arts students?_\n\nLike the riddles of the Sphinx, or the labors of Hercules, good assignments\nhave folkloric qualities. As in any oral tradition, the most memorable\nassignments are passed from teacher to student, often with small changes. They\nresist change, yet allow personalization. We asked our respondents to describe\nthe assignments that are closest to their hearts, or that yield especially\ngood classroom experiences.\n\n**Daniel Shiffman**\n\nLet me preface this by saying that sometimes I feel like assignments are my\nweakness. The tricky thing is to balance a feeling of open-endedness with\nconstraints, so that students can feel creative and make their own thing, but\nthe assignment is not impossible.\n\nI have two assignments to share. One is from the Nature of Code course\nmaterials and it's for more of an advanced classroom, for people who've\nalready taken a full semester of programming and who are now launching into\nlearning about motion, simulation, nature, and physics. The assignment is to\nbuild your own ecosystem. It's really not a single assignment but a project\nyou might do over a long period of time. It starts by learning to make this\none little thing move around the screen, and then later figuring out how to\nmake ten of those move around the screen, and then how to make those ten\nthings see an obstacle in the environment, see each other, and bounce and\ninteract with each other. So [students create] a whole ecosystem out of little\nminiature parts. I really enjoy seeing what kind of strange worlds people will\ncreate, that either mirror things in our real world or are fantastical\ninventions.\n\nThe other thing I really love doing in an intro class is anything that fosters\ncollaboration. It's really hard to do and is a much easier thing to pull off\nin a physical computing class, although I don't actually teach that. But what\nI've observed is that when you're building something physical, seeing where\nthe collaboration comes in is more obvious. When learning programming,\nstudents tend to think they have to work solo on code projects, when actually,\nlarge pieces of software are built by teams of people. One of the assignments\nI really like is to randomly partner students and have them exchange bits of\ncode and you get these kind of Frankenstein monsters, like, “I made the sun\nrise,” and “I made a fish swimming,” and now they have a fish swimming through\na sunrise.\n\n**_Golan Levin: That reminds me of an assignment that John Maeda gave a long\ntime ago, which was to take someone else's assignment from last week and\n“improve” it or modify it._**\n\nRight. And where this can also work well with is when teaching object-oriented\nprogramming. I really like to say, “Make your class and then give it to\nsomebody else to make objects from inside of their world.” What I love about\nthat is not just the collaboration and having to talk to somebody, but that it\nalso teaches about open-source development and making libraries. Students\ncan't just give somebody else their code—they also have to explain what all\nthe functions do and invent their own documentation, whether that's just\nexplaining it in an email or with good code comments.\n\nObviously if you were teaching a more advanced class, you might have the class\nuse GitHub or whatever, and create a documentation page. I like trying to keep\nthat spirit of collaboration, learning how to exchange code and also learning\nabout object-oriented programming. All of that works well together.\n\n**Jer Thorp**\n\nOne that works the best for me is the “drawing tool” assignment and I often\ngive it on the first day. There's something really rewarding about making a\ndrawing tool because you can quickly get at what makes computational\nassistance powerful, and students also get something that they can share very\nquickly. One of my big strategies for teaching designers and artists how to\ncode is to get them making as soon as possible, and associated with that is\nsharing as soon as possible. I want them to have something that they can post\non their feeds at the end of three hours and because they are proud of it,\nthey get that little buzz. They get to say, “Check out what I'm making,” and\ntheir friends are like, “This is great! How did you do it?”\n\nOne of the other reasons I think that [this assignment] works so well at the\nbeginning is that it gets people into this idea of the modularity of\nprogramming. I say to them, “Let's look at the command to draw a line and the\ncommand to draw a rectangle.” Even though one command draws a line and the\nother draws a rectangle, they both take four numbers and so they're actually\ninterchangeable. I can just drop the arguments for one into the other. In the\nbeginning of my first two or three classes, the big focus is to get people to\nrepeatedly ask, “Every time you see a number, what would happen if I put\nanother number in there?” and “Every time you see a method, what would happen\nif I put another method in there?” Because that's where this stuff becomes\nexciting….I want to get them into that Lego method of programming as soon as\npossible, and to see programs not as things that are Krazy-Glued together, but\nas things that can be taken apart and reassembled.\n\n**_Golan Levin: Let me tweak the question. What are some of your favorite\nprompts, specifically in the field of information visualization—and maybe\nprompts that are not necessarily for beginners, but that can be approached by\nanyone at different levels, including advanced or intermediate students?_**\n\nIn an information visualization context, the one that works the best is to\ntake location-based data and ask students to do something with it that is not\nallowed to be on a map. I could build a whole course around this assignment.\nIn my data class, students take a big data set of something that is primarily\nlatitude and longitude but as they can't plot it on a map, they have to plot\nthem in some other way. It's really nice because it kind of frees you from\nconstraint. It's a different take on the exact same thing that we were talking\nabout before: I want people to understand that there's no rule that says that\nyou have to plot longitude as a line along a horizontal axis. In binding my\nstudent's arms a little and saying “You are not allowed to put this on a map,”\nit forces them to see it in a different way….More than any other assignment,\nit gets them thinking about our mental constructs that seem to force us\ntowards making one type of thing with certain types of data.\n\n**Tatsuo Sugimoto**\n\nI am often teaching students who have never been involved in programming, so I\nfocus on getting rid of their fear. In a recent class on data visualization,\nmy first assignment used hand drawing based on personal data and took\ninspiration from a project called _Dear Data_ by Giorgia Lupi and Stefanie\nPosavec. First of all, I had the students discuss what they wanted to record\nin their everyday lives; they chose to look at the dishes and cutlery they use\nfor each meal. So for a week they recorded what items they used to eat with in\na spreadsheet and then they had to convert this data to hand drawings. I think\nit's important to start without coding, but then afterwards I have them\ninterpret their hand drawings in JavaScript.\n\n**Lauren McCarthy**\n\nIn my introductory classes, one of my favorite assignments is for learning\nparameters and variables. The assignment is to create a sketch where there's a\nchange of perspective as you move your mouse across the screen. The question\nis, how can you subvert the viewer's expectations when they move the mouse? I\noften show some examples of this from other areas of art, such a video\nperformance work by Anya Liftig and Caitlin Berrigan called _Adoring\nAppetite_. In this piece, the two artists pushed strollers around NYC,\nsnuggling and kissing their babies. At some point, the kissing turns to biting\nand eating, and they chew through the babies’ heads, which turn out to be made\nof sugar and filled with red jelly. I ask my students, “What would it look\nlike to create an experience this affecting with code?”\n\nI also like the assignments for my more advanced Social Hacking course. My\nfavorite one is where you have to create an API for an aspect of yourself or\nyour life. Students have to pick something and make it controllable somehow by\nsomeone else. This is done either using a data feed or by opening up a\nquestion to the public; usually it's the second thing. Another one that can go\nanywhere is I instruct them to create something such as a browser extension,\nor app or whatever, but they have to make it for one specific person. Students\nthen must start with the person rather than the idea, and I think this helps\nbecause it's a different design process. When you are just making something\nfor yourself, it's easy to not be thinking really clearly about why you're\nmaking [certain] decisions, but if you are thinking about someone else, then\nyou're forced to imagine the user experience a little more.\n\nThe last exercise I like to do is have students write down what they are going\nto do that week, usually the steps they'll take when working towards a final\nproject. I ask them to write down exactly what tasks they are going to\naccomplish, and a time estimate for each one. Then throughout the week they\nare asked to time each task. We then have a debrief afterwards, and they're\nlike: “Well, I thought I was just going to make a simple data viz connecting\nonly one stream of information.” And we find these buzzwords: if you use the\nword “just,” multiply your estimate by two; if you say “simple,” multiply by\nfour. It's really common that people do not easily anticipate how long things\ntake.\n\n**Phœnix Perry**\n\nI like to get people to think about the kinds of things they can do with\ngaming—and not just video games, but gaming more broadly. For example, maybe\nyou want to use a sensor and you want to track someone's position. I'll show\nthem the games I've made—for example, where you can knock things with your\nhead, or when you scream and something happens. I try to expand the\npossibilities and get them to just dream up crazy games. If the sky's the\nlimit, what are some things that you would like to try and do? Around the same\ntime, I introduce the idea of pervasive gaming [games in which the play\nexperience is extended into the physical world], and you can get them thinking\nabout play outside of the computer. That really works well if you have a mix\nof artists and game designers, because they can synthesize their interests in\na really fascinating way and make games that have nothing to do with the\ncomputer and that have very simple rules. I think that that's a really\nempowering experience that isn't tied to a prior skillset.\n\n**Rune Madsen**\n\nMy midterm assignment is always to make a dynamic logo that never looks the\nsame when you run the code. I think that's a very typical assignment for me.\nBut for my “favorite,” I think I'll go with another assignment, which is my\nweek on computational typography. Before every semester I always consider\ncutting this week out entirely, because typography is such a handmade thing;\nwith typefaces, you really need that finesse of tweaking everything. I go to\nthe students and ask them to build a typeface (or just design a typeface for a\nspecific word so they don't have to do the whole alphabet) that has to come\nfrom a core set of rules. That means it has to be better done in code than by\nhand. And that is as broad as it can be: Make a typeface that is better done\nin code than by hand. For example, each letter in the typeface can be defined\nas an object in an array. I need to be able to loop over it and use the same\nprocess in the for-loop to draw each letter.\n\nThis was inspired by John Maeda. [He] has this example in one of his early\nbooks, I think it's a pie typeface, two pie charts are kind of overlapping to\nmake an alphabet. [_Note: Madsen is referring to “Type Me, Type Me Not” by\nPeter Cho, shown in our Modular Alphabet assignment._] And my students just\nalways surprise me, making crazy things like sine and cosine fonts. The\ncreativity of the students always amazes me in that assignment.\n\n**Heather Dewey-Hagborg**\n\nIn my BioArt class, the first assignment starts by looking at the future of\nsynthetic biology, genetic engineering, and design futuring. I use a\nspeculative design approach, and the assignment is to design a product or\nservice that anticipates where genetic engineering or synthetic biology is\ngoing in, say, the next 100 years. The students then come up with a sketch of\nthe product or service and also write a page on the future they have\nenvisioned and how that product has an impact on society.\n\n**_Tega Brain: So it is a critical project that they don't actually have to\nbuild in a working form? It can remain entirely speculative so long as it has\na criticality to it?_**\n\nExactly. In that module we also perform genetic engineering experiments, so\nthe students have some hands-on engagement with these processes and so they\nunderstand some of the limits of what's possible. But this assignment\nencourages them to think beyond what they are capable of actually doing. A\nsemester is not enough time for them to really do anything significant with\nsomething like genetic engineering. This way, they get some experience in a\nlab where they learn what the processes look like, and then I'm asking them to\nthink about where this work is going and what future we are building.\n\n**Zach Lieberman**\n\nThe best assignment that I have is to study and recreate work from an artist\nfrom the past. A common example is where I ask students to take the work of\nJames and John Whitney as a starting point and [have them] approach it in two\nways. First is to make something inspired by their work—to take a look at\ntheir body of work and add a comment to it. [Next I have them] focus on\nreplication and come up with a faithful copy. The ReCode Project by Matthew\nEpler does a great job at presenting both of these approaches side by side.\nThis assignment can then also be used as a method to talk about how people\nwere working in the ‘50s, ‘60s, ‘70s and to talk about different computational\napproaches. I like that it allows you to talk about the past and history and\nnot just the technology and code. Taking an in-depth look at the work of\nanother artist, rather than responding to an open-ended prompt, pushes\nstudents to produce better results and to take the work they are doing more\nseriously.\n\n**Luke DuBois**\n\nIf I'm teaching an audio class with Max/MSP, I'll have everybody build a\nflanger. A flanger plays two copies of the same sound simultaneously, but\nslightly offset in time.\n\nEvery audio effect on the planet can be demonstrated using the guitar solo\nfrom Jimi Hendrix's version of Bob Dylan's “All Along the Watchtower.”\nHendrix's secret weapon was this guy Roger Mayer, who built guitar pedals for\nhim that nobody else had. He had the first wah-wah and the first pedal\nflanger. The flanger originally was a studio technique because the flange\nrefers to manipulating the inner reel of the tape and so it had to be done in\nthe studio. But Mayer figured out how to do it with a capacitor in his delays.\nThe ”All Along the Watchtower” solo has Hendrix switching pickups on his\nguitar, going from picks to fingering to slide, plus a distortion pedal, an\necho pedal, a core, a flanger, and a wah-wah. In that moment in between the\nsecond and third verse, you can learn pretty much everything you need to know\nabout audio signal processing….I usually start by playing that and then I show\nthem how to make a delay line and add feedback. I give them the assignment of\nfiguring out how to tweak all the values to make a flanger in Max or something\nsimilar, and then they have to bring in a record that uses a flanger and a\nrecording of them copying it. Like bring in a Siouxsie and the Banshees tune,\nor Kanye West. Kanye uses flangers all over the place. I have my students do\nthis because it's a really simple Max patch to build.\n\n**De Angela Duff**\n\nI have the students draw something either by hand or in Adobe Illustrator\nusing only basic shapes: triangles, squares, circles, and lines. Then I ask\nthem to recreate that drawing using code. It's really important that it's\nsomething that they draw themselves instead of recreating a painting, because\nthere are a lot of examples of that online from which they could just copy the\ncode.\n\n**_Tega Brain: I've always wanted to try to start the term with a figure\ndrawing class, but using Processing—hire a nude model to sit for my first\nprogramming class and have students represent him or her in code._**\n\nRight! I think drawing is useful because some people aren't practiced in it.\nThey think they can't draw and just to have them engaged with that creative\nexercise is really important, even if they are just drawing stick figures.\n\n**Winnie Soon**\n\nMy favorite assignment for design students is about rule-based systems in\nrelation to generativity, generative art, and emergence. In this assignment, I\nask them to start with a paper and pen and write down two or three\ninstructions. You can't have your outcome in mind first; you need to just\nstart with rules and from there you start to program these rules and they\nunfold over time. I usually introduce this in week six or seven and it is\nfascinating for me because in the first six classes they are given more direct\ntasks—like they have to make a flow bar, an icon, or an emoticon. They are\nable to visualise what they want to make before they code it. But then when\nyou suddenly introduce a rule-based system they are like “WHAT!? WHAT are you\ntalking about?” It's very difficult for them as designers to comprehend this\nconcept. Like 10 PRINT or Game of Life, it's just a different way of thinking.\nIt allows me to talk about things like chaos, noise, ordering, simulations,\nauthorship—whether the machine is co-creating with you. It also lets me\nintroduce conceptual art and conceptual thinking and focus on process rather\nthan the end result. I'm really able to contemplate how things unfold over\ntime, which I think is a really important perspective from which to think\nabout programming.\n\n\n# When Things Go Wrong\n\n> _What happens on your worst day?_\n\nCode is a brittle medium. There's a steep cliff of failure; a simple, easy-to-\nmiss syntax error will often prevent a program from compiling altogether. All\nsoftware educators will experience this publicly at some stage and find\nthemselves sweating in front of a room full of impatient students while\ndesperately debugging. Teaching media arts also demands that instructors\nconstantly keep abreast of ever-changing development environments and new\ntechnologies, many of which are likely to be peripheral to their primary field\nof expertise. Meanwhile, operating systems update themselves, familiar tools\nabruptly become obsolete or incompatible, and cherished references\nunexpectedly disappear from the Internet. For these reasons the creative\nprogramming teacher is particularly prone to having to work through mistakes\nand errors live in the classroom, often while attempting to maintain the\ninterest of skeptical and apprehensive pupils. Our respondents discuss the\ndays when stuff breaks and nothing goes right.\n\n**Daniel Shiffman**\n\nI have moments where nothing is working and I can't figure it out, but\napparently these moments are useful. People tell me this about the [_Coding\nTrain_] videos all the time. Like, “My favorite part was when you couldn't\nfigure it out and you got stuck for like ten minutes, because I like to see\nhow that happens to everybody,” or “I like to see the way you tried to fix\nit.” Even so, these moments can be really bad and stressful. There've been\ntimes when I get completely tripped up in my own head, trying to explain\nsomething like a Markov chain and just feeling like, “That was the worst\nexplanation ever; it made no sense! I should have practiced that.”\n\n**_Golan Levin: It's interesting how you do practice your explanations. It's\nvery obvious in your case, because you live this out more publicly than the\nrest of us_ _through your video channel. It's quite clear that all of your\nexplanations are very patiently refined and revised through practice._**\n\nI've been doing the videos for a while, but recently they clicked for me as I\nwas just more mentally focused on them. I used to do the videos to get ready\nfor class, but now I use the class to practice for the videos. I'm like those\ncomedians who go to comedy clubs to try material out before their TV specials.\n\nAlso, if I have the same class that meets two days in a row, I can never have\nthem both go well. It's either that the first day is amazing, and then I try\nto force that to happen again the second day and it fails—or the first day\nkind of goes haywire, and it motivates me to fix it all the next day.\n\n**Winnie Soon**\n\nI just had one two weeks ago. The worst day is where nothing works. You think\nit works and then when everyone runs it you find all these different problems\nand you have to follow up and figure out if it's their system, version, or\nbrowser, and you even don't know why it's not working. And then you need to\npretend it's OK and actually in your heart you're like ARRGH, _how come it's\nnot working?_ Maybe it's good to have this kind of disruption because it\nallows the students to see the imperfect aspect of programming, which is also\nthe reality, but at the same time you need to be mentally strong to handle\nthis situation. You also have to know how to turn it around and make up\nsomething on the fly so that you can still cover the content you have to\ncover.\n\n**Luke DuBois**\n\nI've had classes where things just fail. My first attempt at setting up a\ncloud-like Node server was the perfect storm clusterfuck. I wanted to show how\nto do a socket IO thing where we could all kick something up to a cloud and\nget a response. And that fucker just didn't want to work. I had some problem\nwith my .ssh directory that wouldn't allow me to log into the thing and it\ntook me 45 minutes to debug. Well, it felt like 45 minutes. It was probably\nonly 15 minutes, but it was right in front of the students. Everything was a\nmess. It was bad.\n\nOn the music side I once taught a terrible, terrible class involving a\nDisklavier [digital piano]. Its default is that there's always a half-second\ndelay on the song, and there's a safety switch that you flip off, and I could\nnot for the life of me figure this out. They built this into the hardware to\nprevent feedback, to prevent you from hitting a key and then getting drilled\ndown and grinding the whole thing and breaking it. I had this beautiful piece\nof repertoire by Kaija Saariaho, who is the composer laureate of Finland and\nhas a really great Disklavier piece. And I, Luke DuBois, with four years of\npiano and graduate studies, can actually play this piece, as it's not hard and\nit's part of an undergraduate pedagogy study she wrote in the ‘90s. But I\ncouldn't get this fucking 500-millisecond thing off and I went ballistic on\nthe piano. I went into this psychotic cursing fit, where there was no stopping\nme. I sounded like fucking Don Rickles at one of those comedy roasts in the\n‘70s. I was like, “You lousy fucking half-assed goddamned piece of shit, I'm\ngoing to fly to Tokyo tonight and single-handedly beat the shit out of the\nentire drum, guitar, and motorcycle division of Yamaha for foisting this piece\nof garbage on us.” That was my worst day of teaching.\n\n**De Angela Duff**\n\nThis is not necessarily my worst day but more a realization of something that\nwas not working. I used to go through a lot of code in class, like line by\nline, and I would dissect code and explain it. When you know how to code, it's\nclear as day, but it's like noise for the newbies. Many of them create this\nnarrative in their head that they're not good with programming or with math.\nThen I'd say to somebody in the class, “Tell me what I just said,” and they\ncan't regurgitate it even one minute after I said it. I realized that actually\nexplaining code line by line is such a waste of time. So I guess my worst days\nwere those days when I thought I was teaching something but I wasn't—it was\nall just noise.\n\n**Allison Parrish**\n\nI think the worst days for me were the days when I was teaching a programming\nclass in the English department. I had lost half the students, like half of\nthe students were not really keeping up with the content of the class, but I\nstill had this very strict schedule that I wanted to follow so that the class\ncould reach its conclusion conceptually. So there were one or two days when I\nwas just like gritting my teeth and doing the tutorial even though I knew that\nhalf the students weren't able to follow along.\n\nMuch of my teaching experience has been in grad school environments at schools\nlike ITP, where the students are so internally motivated to keep up and\nwanting to extract value from the class no matter how poorly it's coming.\nUndergrads, on the other hand, will check out if you are not connecting with\nthem. So having that experience in the English department and having students\ncheck out on you for the first time kind of sucked a lot. But I learned from\nthat experience.\n\n**Lauren McCarthy**\n\nI think it is worth saying that as a teacher, you are there to motivate and\nencourage everyone, and sometimes that's hard. It's tiring, and it doesn't\nalways feel OK to admit that. In other jobs somehow it feels easier to talk\nabout how it's hard, and I guess we do that too sometimes with fellow\nteachers. But often it feels like there is a pressure to always be like:\n“Hurrah! Everything is great!” Maybe that's a buzzkill, but I think it's good\nto say sometimes.\n\n**Jer Thorp**\n\nI taught a programming class for a few years at Vancouver Film School. It was\na required class, and that sucked, as half the students didn't want to be\nthere. You're always able to recruit some of them, but some of them don't\ncare. This is a school where most of the parents pay the tuition and throw\nthem in there because the kids don't know what they want to do. Being in a\nroomful of students who don't care is really hard, even though I think I'm\npretty good at getting people who might not otherwise care to care.\n\n**Taeyoon Choi**\n\nI once brought resistors to class to make a flip-flop memory unit. It's made\nfrom just two transistors, two resistors, and one LED; it's a really simple\ncomponent. Some of the resistors are colored brown, but they can look red when\nit's dark. I think I might be a little bit colorblind or maybe I was in a\nhurry, so I accidentally brought the brown one, which has a different value\nthan the red resistor, and none of the examples worked in the classroom. I was\ngoing nuts because I tested it so many times. It was so frustrating and I was\njust sweating like crazy and I was so embarrassed. Not only did it make me\nlook bad, but [I feel like a public mistake like that] discourages students\nfrom exploring electronics and technology.\n\nAt the start, students need to have immediate feedback, like, _the thing\nshould work_. Luckily I figured it out at the end of the class and some\nstudents had resistors of the right value and we switched them and everything\nworked. But I was definitely underprepared and got totally lost, and was\nhating myself for not being prepared.\n\n\n# Most Memorable Response\n\n> _What was the most memorable response to an assignment you've given?_\n\nThere may be no better proof of an assignment's educational potential than a\nsurprising student response. Here, we ask educators to reminisce about\nstudents who responded to assignments in ways that were subversive, or\nunexpected, or indicative of significant learning. These memories reveal how\nassignments are not only a fulcrum for student growth, but can lead to greater\nunderstanding for the instructor themselves.\n\n**Taeyoon Choi**\n\nThe most heartwarming response I've had was when Andy Clymer from the first\nSchool for Poetic Computation class saw me two years after he finished the\nprogram and said, “I'm still working on your homework.” He's a fantastic type\ndesigner who does generative fonts and I helped get him excited about\nelectronics. He got really into synthesizers and robotics from some of the\nworkshops that I did. It's funny because the workshop itself was not that\nsuccessful; it was one of my first classes teaching technical things and I\ndon't think I was really ready to be the teacher. Still, we tried to adopt a\ncollaborative approach where the students took the lead at the end of the\ncourse and told us what they learned, and hearing that he continued from where\nwe left off probably made this one of my best teaching experiences.\n\n**Winnie Soon**\n\nWell, this was a response to the whole semester and not just one assignment.\nOne of the students wrote down in the course evaluation that she realized that\ncoding is not just a gentleman's club. And just with that line I felt like I\nactually achieved something. I know that the NYC coding scene is much more\nfocused on diversity, but in many parts of the world, gender is still very\nproblematic.\n\n**Phœnix Perry**\n\nDon't be afraid to be a bitch. Don't be afraid to tear somebody down. Some of\nthe most rewarding responses I've had have come from students to whom I gave\nvery negative feedback early on about a bad idea. And they've taken my\nfeedback as kind of fuel for the fire and they've either proven me totally\nwrong, or they've improved their concept significantly by the time I see it\nagain. For example, I had one student who wanted to make an Oculus Rift VR\ngame and he told me his original idea, and I was like, “This is total shit. I\ndon't like it at all. It's not interesting, it's not new, I don't want you to\njust make a space shooter. Why use the Oculus Rift to make a space shooter?\nWhat are you bringing to this?” So what he decided to do was make a rhythm-\nbased space shooter and he used the diegesis of the cockpit to be the control\nsystem for the game. You had to look toward the rhythm of the beat, and the\nbeat got stratified. So when the tom drum beat, one kind of object would shoot\nout, and you would have to go look at that location to stop it. If you heard a\nbass drum beat, you would have to look over at the other location. And doing\nthis would destroy the object that was making the bass beat. It was really fun\nbecause then the act of turning your head, looking up and down and around,\nbecame satisfying because you were doing it on rhythm and at a pulse.\n\nAnother student project that really broke my brain, and I've never actually\nseen anyone do anything like this before, was by a kid named James Cameron. I\ngave a game assignment, and he wanted to make a horror game that connected\nthrough to the real world. He made this horror video game, and it's a really\nterrible scenario, like a murder has occurred, and all of a sudden you see a\nphone in the VR, and when you touch the phone your actual cell phone rings.\nThen the characters start calling you and leaving you messages during the day\nand they're prompting you to kind of play out this story. That was really\namazing, even though his original idea was not spectacular. I was really hard\non this kid in his exams—like he missed a semicolon on something and I drew a\npirate and wrote, “Now walk the plank.” I think sometimes teachers want to\nmake students feel really good and sometimes that's not the goal. Sometimes\nyou want to make them question if something is worth their time and how they\ncan bring something new to it.\n\n**Lauren McCarthy**\n\nFor me, the things that are the most memorable are usually less about the\noutput and more about the process. Like seeing that student who is really\nstruggling in the first few weeks, and then every week, rather than just doing\nthe assignment, she does five versions of the assignment. Then by the end\nshe's a great programmer with a really strong concept in her final project.\nThat's the sort of thing that really sticks with me. In terms of specific\nresponses, one of my students did a project when she was just learning to code\nwhere she would go up and ask people for their Processing sketches, like the\ncode itself. She would then try to draw by hand what the outcome would be and\nafter would run the sketch to see how close she could get.\n\n**_Tega Brain: That's novel, and courageous for a beginner._**\n\nYeah. Let's see, there's another one that is not such a crazy concept itself,\nbut I really liked where the two students were coming from. The project was\ncalled “Not Lost in Translation” and they basically made a chat app for\nthemselves. They both spoke English but with a strong accent. And they felt\nlike people didn't understand them a lot. They first just wanted to make\nsomething that would translate into the opposite language, but then when they\nhit some technical barriers, they changed their concept so that you would\nspeak, and you could choose English, or Hindu, or Korean, or whatever language\nyou are speaking. And then it would do a Google Translate of that, and then a\nGoogle image search. It would just show the other person a pictogram of what\nthey had said, and then the person would respond by speaking and it would show\nanother pictogram. So the result is totally nonsense, but it was nicely\nexecuted.\n\nThen the last one I remember was my student Ben Kauffman's response to the API\nassignment, where you have to make some aspect of your life controllable. His\nproject was super simple: if you messaged him with the hashtag _#brainstamp_ ,\nhe would pull a postcard out of his pocket, mark down whatever he was thinking\nat that moment, and drop it in the next mailbox that he saw.\n\n**Zach Lieberman**\n\nMy department head asked me to teach a class on artistic data visualization,\nand I'm a fan of _artistic_ data work but I'm not a fan of much data\nvisualization, and so I was conflicted the whole time I was teaching it, and I\nreally brought that energy into the classroom. But in that class there were\nfolks like Evan Roth and Christine Sugrue. They took these prompts and really\ncreated artworks out of them—things like Evan Roth's project _Explicit Content\nOnly,_ a record composed entirely of swear words extracted from a rap album.\nThey took the prompts that I was giving them and created things that would\nstand in a gallery or would be written about in a blog. For me it's really\nexciting as a teacher when your students create work that is noteworthy, work\nthat gets people writing or talking about it, because you start to see that\nfeedback loop with culture, which is great. As a practitioner there is a\nfeedback loop of making something, reaching an audience, and getting feedback\nfrom it. [Without that cycle,] it's otherwise really hard to understand the\nvalue of what you do. For me, the most exciting prompts are the ones that\nresult in work that makes it out there in the world.\n\n**Luke DuBois**\n\nIn my creative coding class, I do an assignment where I show the floor of the\nAlhambra Palace and explain how it's a Lindenmayer system, how it can be re-\ncreated with an L-system. I then give them a basic turtle graphic sketch in\np5.js and give them a couple ideas for how to do Lindenmayer systems and a\nbunch of links, and tell them to figure it out. I had a student L-system his\nway toward a perfect replica of this mandala that's in some crazy third-\ncentury CE Hindustani temple in his hometown—Calcutta, if I remember. He\nremembered it, like he knew it visually, and he was like, “There's got to be a\nway I can make this thing.” It's kind of like a space-filling curve, like a\nKoch curve, so it's not too hard to do. Most of the kids made these weird\nturtle graphics things—mazes or Sierpinski triangles or some of those bullshit\nfractal 101 things. But this kid showed a Google Earth photo of the temple, an\nimage of its ceiling; he hit play on the sketch and walked away. What really\nstood out was the personal connection he had to the form, which clearly\nmotivated his craft. All the other kids were just like, “ _Fuuuuck_.”\n\n**Allison Parrish**\n\nIt was by my student Susan Stock, and it's a program that produces a poem.\nIt's based on a poem that she wrote about a friend of hers. She passed the\npoem through this procedure that randomly repeated small segments of the text,\nso that it feels like her record is skipping. It keeps on moving back into the\ntext so that it has to kinda catch up to itself. It's called “Susan's\nScratch.” And she gave this reading of it in class and it was really\naffecting, because it was personal. I think that with procedural art in\ngeneral it's harder, or seen as less desirable, to get at the lyrical, the\nemotional, or the personal…. It was clear that the procedure brought out\nsomething in the original text that wasn't there to begin with.\n\nI think [this] was for my midterm assignment, which is to invent a new form of\npoetry and then write a computer program that attempts to write poems in that\nform. The thing that I remember is that once Susan presented this, it was like\na switch flipped in my head. I realized I can teach my students how to make\njokes with this, I can teach my students how to do data analysis with this\nstuff, I can teach my students how to be postmodern jerks with procedural\ntext. “Susan's Scratch” just made me think: Oh, wait a second, the emotional\nrange of this is way, way larger than I had originally conceived.\n\n**_Tega Brain: And of course computers are rarely framed as emotional or for\nexploring emotion; they are not perceived as having enough ambiguity or\nunpredictability._**\n\nAbsolutely. I think that's understandable considering the history of the\nmedium. People in general don't think that systems can be expressive. They're\nnot perceived as having intentionality. People often don't see the act of\nmaking rules as being the same thing as the act of writing a poem. And this is\nsort of what I am trying to attack in my teaching—to say that actually, the\nprocess of designing a computer program, designing a poetic form, or designing\na game are all processes, and the system that creates these artifacts is\nitself a thing that can contain lyricism and emotion and a personal point of\nview. In fact, it _has_ to have a personal point of view and we shouldn't\nignore the personal aspect of that kind of making.\n\n\n# Advice for New Educators\n\n> _What's your advice for educators teaching arts-programming for the first\n> time?_\n\nIn teaching media artists and computational designers, we need to cultivate\nmultiple skills: a sharp eye, a critical perspective, technological craft, and\npassion for the field. And we must support the student's ability to connect\nthese capabilities in practice. The following conversations navigate this\nterrain, outlining a range of tactics, tips, and lessons learned the hard way.\n\n**Jer Thorp**\n\nWhen teaching programming to non-programmers, and specifically to designers\nand artists, I think it's really important to say in the beginning, “My intent\nhere is not to make you into a programmer. I think you should continue being a\nprintmaker, or a typographer or whatever you are. My intent is to give you\nsome computational tools that will help you.” Also, get your students making\nthings as soon as you can. For me that has to be ten minutes into the class.\nWe write a really simple four-line Processing sketch that does something\nreally easy and gets them there right away. Don't stop to talk about what the\nIDE is and what syntax looks like, or what a semicolon means. That stuff\nsucks. You need it but it sucks. Do the making first, then use that to go back\nand talk about it after.\n\n**Zach Lieberman**\n\nEvery time I give out homework, I always say that every homework assignment is\nan opportunity for genius. The most important advice is to have that sort of\noptimism, to have that sort of energy for your students with every prompt and\nevery time you ask them to do something or engage in something. Learning code\ncan be frustrating and it requires a lot of time and a lot of failure. Time\nand failure and misunderstanding. To imbue a sense of optimism here is so\nimportant—to celebrate this as a new mode of working and to help students to\nrealize that there's all these untapped ideas out there. Also, anything that\ncan turn the classroom into a mini film festival is really great. I think it's\nimportant as a teacher to show what you're curious about, to show what\ninspires you, and to be able to talk elegantly or passionately about what\nmoves you, as that helps students articulate what moves them. By showing these\nreferences and using them as a context to talk about code, it can help\nstudents translate and articulate what they care about.\n\n**Lauren McCarthy**\n\nSomething that I read that really stuck with me is that as soon as you figure\nout something that was confusing you, your brain immediately forgets what it\nfeels like to be confused. Your synapses will fire in a way so that you're not\nconfused anymore. So you learn to code and it clicks. You ride a bike, and\nthen you can't remember how to not ride a bike and for that reason, it's\nsometimes really hard to remember that feeling of confusion. So…before office\nhours, I try to think about something that's really confusing, or I try to\nimagine myself back in the lab as a student. I really try to tap into that\nfeeling of frustration, that confusion when things don't just click….By trying\nto take yourself back there as a teacher, particularly if there's a lot of\nstudents you need to help, it helps you connect with them more.\n\n**_Tega Brain: That's fascinating. There have been times when teaching a class\nfor the first time where I've received some unexpectedly good results and\nreviews, especially for material slightly outside my skill set. Then once I've\ntaught it a few more times and I know the material really well, I think it\ngets harder to empathize. After those first couple of times I often feel like\nI'm actually getting worse at teaching it, which is really not intuitive. So\nas first-time educators your inexperience can actually be a positive thing,\nbecause you are more likely to relate to students and understand what they are\ngoing through._**\n\nTotally. I think the other thing is just modeling belief, as this can change a\nstudent's world. I think probably everyone who has ever done something or been\nsuccessful has had someone who believed in them and made that clear, and\nhelped the student understand how to believe in themselves. I think about that\na lot.\n\n**Rune Madsen**\n\nDon't be afraid that you don't know enough. I was so afraid that I would get\nquestions I didn't know how to answer. And that's not really what makes you a\ngood teacher. Some of the best teachers I know learned programming at ITP and\nstarted teaching programming just a few years after. Because they were so\nclose to the material, they understood how to explain concepts—they remember\nhow it felt not knowing. Something that comes with being afraid when you teach\nthe first time is that you over-prepare, you rush through materials. You're\nafraid of not teaching enough, so you squeeze every last bit of information\ninto this long lecture. And then you and all the students in the class say,\n“What the hell was that?” I would say, _take your time_. Breathe.\n\nThat, and maintaining a good balance between talking and doing. It's okay to\nswitch activities a lot and be clear about, “OK, now I'm going to talk, so\nplease put your laptops down. I'm going to explain things. You don't need to\ncopy my code. You just need to look at it. I'll leave the code on the screen,\nand then in 15 minutes from now open your laptops and let's try to work\ntogether.” So you have to be vocal about what you're doing in the classroom.\nAnd this mix of lecture and hands-on doing is a style that has worked really\nwell for me.\n\n**Daniel Shiffman**\n\nWell, I continually make the same mistakes over and over again, and one piece\nof advice I have is to do a lot less than you think you can do. As a long-time\nneurotic, I am a way over-preparer. For example, I was teaching a new class\nthis semester and I ended up making 30 examples when there was only time to\nlook at like two or three in class. One thing I've often done is to assume\nthat because I made all this stuff, I have to get to it all. Then when there's\nonly ten minutes left of class, I would try to rush through the rest of it. In\nmy opinion, it's definitely much better to slow down and leave stuff out. You\ncan always get to it later, or you can send out an email, or not do it at all.\n\nObviously, I've been doing a lot with videos [_The Coding Train_ on YouTube]\nand that's part of an attempt to create an environment where there is some\nquality of self-paced learning. If you can do this or foster some\ncollaborative learning where the students are working in smaller groups or\nindividually in a kind of workshop setting, it's a really good thing. Much\nbetter than rushing through a lecture and showing 500 examples in 15 minutes.\n\nAlso, don't overlook teaching students how to ask for help. That's a huge part\nof the learning process. You can't just teach the programming; you've also got\nto teach how to get help, like: “How do you ask? What's the right question to\nask? When do you write, how do you debug?” All of that type of stuff can\neasily be lost in the “here's the lesson”-type approach.\n\n**De Angela Duff**\n\nI would recommend that first-time educators check out Daniel Shiffman's\nvideos. I love these videos. Some of my students don't like them because they\ndon't think they're serious enough. They think somehow that they shouldn't be\nhaving fun learning, which sort of blows my mind. But I think it's seriously\nimportant for first-time teachers to witness the enthusiasm that Dan Shiffman\nhas about teaching programming.\n\nI would also recommend doing something similar to what I tell my students to\ndo, which is to look at multiple books to see which book they prefer. Look at\nas many syllabi as you can find. There's a lot on GitHub. I don't recommend\nfollowing what someone has done before because I think that teaching should be\na creative process; crafting your syllabus should also be a creative process,\nand you shouldn't just be following someone else's syllabus. However, it's a\ngood way to get assignment ideas and to get to know how certain assignments\nare just sort of classics.\n\nOne thing I do with my students is called “ticket to leave.” Towards the end\nof class, I give out sticky notes or pieces of paper (but it could be done\nonline). And I tell students to write down three questions that they might\nhave about any of the material that was covered and also to list three key\npoints that they got from what we did in class that day. That way, I can find\nout what stuck and what the problems were. I think that's a really awesome\ntool, because then at the beginning of the next class I answer the questions\nand maybe see something that didn't stick overall. Then I'll go over it again.\n\n**Heather Dewey-Hagborg**\n\nI would tell first-time educators to try to teach what they are enthusiastic\nabout, what they actually care about. If the technical aspects matter less\nthan their enthusiasm for the subject, then they shouldn't feel like they have\nto get up there and lecture about code. It's possible to engage students in\nlots of different ways. The most important thing is to share what is exciting\nto you about the practice of programming.\n\nWhen I started out as a freshman, I really didn't have any kind of interaction\nwith technology whatsoever; in fact, I was probably pretty anti-technology.\nThen in my first year I took a conceptual art class that included a lot of\nreferences to media art, the beginnings of Dada and Fluxus and installation\nart, and that got me excited about learning some of the tools of new media,\nwhich led me into doing some work with sound and video. Then as I started\nworking more with sound in particular, I felt very distant from the medium.\nHaving come from a more materially engaged practice like sculpture and\ninstallation, I felt like when I was working with sound I was removed from the\nmaterial. I was using a software interface but I felt distant from it, so I\nsigned up for an introductory programming class, a Python class. In part, I\nsigned up for it because of the title, which was Thinking with Objects. I\nliked that because I thought it sounded very physical and visual. Of course, I\ndidn't know it just meant object-oriented code, but luckily the professor was\nreally fascinating—a brilliant, brilliant man who, even in that intro class,\nstarted tying the code into ideas about neural networking and genetic\nalgorithms.\n\nEven though we were total beginners and we couldn't quite understand what he\nwas saying, he provided these very visual explanations of neural networks and\nof organisms and I found that really exciting. Again, I probably would have\nnever continued with it except that he specifically came to me and said, “You\nshould consider taking the Artificial Intelligence class.” If he hadn't gone\nout of his way to invite me to take this class, I would have thought I wasn't\ngood enough to do it, but because he did I became curious about it, and then I\ntook it and really loved it. This kind of faith in students can make a huge\ndifference. It's what launched me into this whole algorithmic direction.\n\n**Taeyoon Choi**\n\nTeaching a very small group of students, like maybe less than five people, can\nbe really instructive…you get much higher-resolution feedback on how the\nmaterial is being received. I also think drawing is really helpful. I draw a\nlot before the class and during the class and I sometimes draw the same thing\nover and over again. The idea is that I'm performing a drawing and it gives\nthe students the time to think with me about how knowledge is processed. I\nencourage them to make drawings in their sketchbook as well and then they end\nup with their own textbook in a way—an explanatory text that they processed\nthemselves.\n\nIt's also helpful to understand that not all students are going to appreciate\nwhat you teach. If I get twenty percent of the students excited about what I\ndo, I call it a good day. Teaching is a really difficult thing to do.\n\n**Winnie Soon**\n\nI really want to emphasize this notion of _care_ , care in a lot of different\nways. Care in terms of whether you can create and sustain a motivated and\npositive space for learning and discussion and for just saying vulnerable\nstuff like something is not working. Also care for a diverse range of\nresponses, because as a teacher you see some work that is technically strong\nand some [that shows] the students are struggling—like where they are just\nchanging the values of the parameters. But still they need encouragement, they\nneed appreciation. I think you need to be really sensitive to those students\nwho are unmotivated and falling behind, who have fear and stress. You need to\nthink about how fast you speak, how much repetition you need to have in order\nto adjust the energy of the classroom.\n\n**Phœnix Perry**\n\nMy advice would be to discourage students from collaborating with peers they\nknow. Try and get people to work in groups where they might be exposed to new\nideas or new kinds of things.\n\nThe other piece of advice I would give is to be really careful when you start\nseeing “bro” culture emerge—when you start seeing the classroom segregate and\nthe women are fetching coffee and the guys are doing the code, or where the\nwomen are doing all the “art.” Remain very cognizant that it can happen.\n\n**Luke DuBois**\n\nIn our creative-coding curriculum [at NYU IDM], we have four sections that are\nall different. I teach one that focuses on music. Allison Parrish teaches one\nthat focuses on text, Kevin Siwoff on graphics and 3D, and Katherine Bennett\non physical computing. You choose one to be your home section, but you can\nfloat between the others as the classes don't happen simultaneously. You can\nget reinforcement by going to the other sections….In an ideal world, we would\nfigure out a way to teach all four of those things in each section, and the\ninstructors would rotate, but there's not enough time.\n\nMy advice is, don't just make it a graphics class….Teach that first to get it\nthe fuck out of the way and then talk about text, sound, and hardware. Or the\nWeb. Talk about Rest APIs or about all those pet peeves you have about the\ndot-com fetish of the day. Last year, I had this kid who had hacked his entire\nhome in LA so his mom could hit the snooze button and it would turn on the\ncoffee maker and then when she picked up the coffee [pot], the shower would\nturn on. Really great stuff! Around that time we'd gotten a beta release of\none of those stupid Nest things and I told him to hack it and figure out how\nto get it to do something useful. Originally, he was like, “I'm going to make\na robotic arm for the equipment room,” but by the end of it he had made this\nweird garden of little automatic bleepy bloopy things that look like bombs. He\nmade this great little art installation out of hacking this piece of horrible\ncommerce tech. That was cool.\n\n**Allison Parrish**\n\nI had the pleasure of taking several classes from Marina Zurkow at ITP and she\nwas really mean on the first day of class. She was really strict and she just\nprojected this persona of being very exacting. I found that super refreshing\nin the context of the rest of ITP, where teachers tend to be a tiny bit\nlackadaisical. I've tried to adopt Marina's approach at least for that first\nday: be strict up front and don't give any ground when it comes to the idea\nthat you're the one teaching, and that the class that you've designed or the\nclass that you're teaching is a serious thing that deserves the students’\nattention. Otherwise they might not apply themselves to the same degree. It's\nimportant they know that you really care and that you're not going to accept\nwork or behavior that doesn't live up to a particular standard, even if you\nbecome a super softie as the class evolves. I found that to be successful.\n\n\n# Classroom Techniques\n\nManaging the dynamics of any classroom can take practice. Computational arts\ncourses present special challenges, requiring educators who can both oversee\nan art or design critique and debug a student's code—sometimes simultaneously.\nIn this section we present a selection of tips and tricks to manage a healthy\nclassroom community, organize feedback to and from students, and open up\nchannels for communication.\n\nRespect and Accommodation\n\n_Find out more._ Distribute non-anonymous questionnaires on the first day of\nclass to get information about each student's background, goals, interests,\nskill level, and concerns. This can be a way to learn each student's name and\npronouns and identify any learning difficulties early on. It can also provide\nhelpful context when working with students from diverse disciplines or\nsocioeconomic backgrounds. Follow up by having one-on-one conversations with\nstudents who have concerns about the class.2\n\n* * *\n\n_Inclusivity is not just about the diversity of people but a habitat of\nlearning that is inclusive and empowering for people._\n\n—Taeyoon Choi1\n\n* * *\n\n_Use a specific code of conduct._ Many schools now require educators to\ninclude a code of conduct in their syllabus. Typically, this prohibits\nharassment and other discriminatory, aggressive, oppressive, or suppressive\nbehavior. Classrooms that cover technical concepts often have additional\nneeds. The Hacker School provides key examples of social rules that address\nthese particular issues: “No feigning surprise” (e.g., “What?! I can't believe\nyou don't know what _the terminal_ is!”), “no “well-actually's” (when a minor\nand often irrelevant correction is made in a conversation, as in “well,\n_actually…_ ”), and “no backseat driving” (when someone overhears people\nworking through a problem and interjects without invitation). These rules are\n“designed to curtail specific behavior…found to be destructive to a\nsupportive, productive, and fun learning environment.” 3\n\n_Consider your language._ Try to not say phrases like “this is easy” when\npresenting technical concepts. Instead, say, “You can do this.” This helps to\nnot alienate students who might be struggling to learn introductory concepts.5\n\n* * *\n\n_It is imperative that we engineer robust participation of people from a broad\nset of communities, identity groups, value systems, and fields of knowledge in\nthis emerging media landscape, in all roles and levels of power. This will\nhelp to mitigate the pitfalls of disruption and potentially usher in a change\nthat has justice and equity as core values._\n\n—Kamal Sinclair4\n\n* * *\n\nDebugging in the Classroom\n\n_We all have bugs._ Don't be afraid to debug your code at the lectern. Ask\nyour students for their eyes on the problem. Allowing your students to see you\nsay “I don't know” can help diminish their own impostor syndrome.\n\n_Code isn't precious._ Deliberately break your code in front of students. In\nrepairing it, narrate your steps out loud.6\n\n* * *\n\n_The only skill you need is to know (1) how to identify what you don't know /\nwhen you don't know something; and (2) how to look things up, how to read\ndocumentation, how to try & try & try and keep trying while things fail, until\nthey work. That's literally the job of writing code._\n\n—Jen Simmons7\n\n* * *\n\n_Program in pairs._ When giving exercises in class, direct your students to\ncollaborate in pairs on one computer. One student should do the typing while\nthe other observes, comments, and makes suggestions. Have students switch\nroles for each exercise.\n\n_Teach how to ask for help._ Don't assume that your students will know how to\nask for help. Lauren McCarthy provides her students with example questions to\nask when they are confused: “Will you repeat that last thing you said? Could\nyou do another example? Could you go through that again, slower? Will you\nexplain that a different way? Can you explain that word you said? Can you\nplease speak a little slower?” 8\n\n_No typing._ When a student asks for help, resist the urge to repair their\ncode directly; they need the firsthand experience of resolving the issue\nthemselves. Francis Hunger advises: “Never touch the keyboard, mouse or\ntrackpad of a student's computer. Just tell [them] what needs to be done—they\nown the keyboard, they own the problem.” 9\n\n_Adopt a “three before me” policy._ Taking time to debug one student's\nproblematic code can interrupt the classroom flow for the others. When a\nstudent encounters a bug in their code, require them to seek help from three\nof their peers before coming to the educator for assistance. The “three before\nme” classroom policy also helps instill an atmosphere of collaboration and\ncomradery during studio time.\n\n_Use paper._ Require students to bring a sketchbook to class. This can be an\nessential aid for rapid problem-solving, brainstorming, and paper\nprototyping.10 It can also help support a laptop-free lecture environment—as\nresearch shows that writing things down improves student recall and\nunderstanding.11\n\nTeaching Critique\n\n_Follow a structure for critique._ Students often struggle to give each other\nmeaningful feedback on creative work, lacking a vocabulary or template for\ndoing so. A variety of educators, educational theorists, and critics have\noutlined steps to help students better engage with each other's work. A common\npattern asks students to start with description (“What do you see?”), followed\nby analysis (“How is it made? What does it make you think about or feel?”),\ninterpretation (“What is it about? What is the main idea being explored?”),\nand finally, evaluation (“Is it successful? Does it explore the prompt in a\ncompelling, interesting or unique way?”)12 This last step will typically\nrequire a conversation on what criteria are appropriate for judging the work.\n\n_Use collaborative notepads for critiques._ Critiques in studio classrooms of\n12–20 students can be impractical and awkward—both because of the total time\nrequired to discuss every student project, and because of the social dynamics\nof groups this size. For a more efficient critique, have each student briefly\npresent their project at the lectern; meanwhile, during their presentation,\ndirect the presenter's classmates to type comments into an online\ncollaborative real-time text editor (such as a Google Doc or Etherpad). This\nhas several advantages: peer feedback is instantly captured and organized;\nanonymous editing can encourage more honest contributions; shy students can\noffer feedback more easily; and the group is spared the repetition of\nstatements like “I agree with what everyone else has already said.” For\nlaptop-free classrooms, students can instead provide feedback to their peers\non sticky notes.\n\nPromoting Research\n\n_Encourage weekly journaling._ Help students become familiar with the field\nthrough regular online research. Golan, for example, requires his students to\nbrowse specific blogs and video-sharing sites and then write weekly “Looking\nOutwards” reports: lightweight essays about a specific artwork or other\nproject they've discovered. In a Looking Outwards report, the student should\nexplain what the project is and how it operates, explain what inspires them\nabout it, research the project's chain of influences, and critique the project\nby discussing the possibilities it suggests or the opportunities it missed.\nLooking Outwards reports can be productively constrained in a variety of ways,\nsuch as by restricting them to projects that use certain media, or to work by\nspecific artists.\n\n_Stage research sprints._ Don't exclusively allocate studio time to technical\ncontent. To reinforce the principle that code always exists in a cultural\ncontext, arrange 15–20 minute “research sprints” in which groups of students\nare asked to quickly compile links to projects that exemplify a particular\ntype of practice. Create an editable slide deck and have each group contribute\none slide.13\n\nGetting Feedback on Your Teaching\n\n_Allow time for questions._ Allocate time throughout the term for students to\nask questions on anything they are confused about, safely and without\njudgment. Many educators recommend asking students, “What questions do you\nhave?” rather than, “Do you have any questions?” in order to elicit more\nresponses.14 Post the questions somewhere visible, and over the following\nsessions, address each one in a class discussion.\n\n_Request exit tickets._ At the end of a class, ask students to submit one or\ntwo questions about the content of the day's lesson. This can be done via\nhandwritten notes, an online messaging tool, or a structured electronic\nsurvey. Such “exit tickets” are a way of getting instant feedback on your\nteaching and can help you understand your students’ comprehension.15 They can\nalso function as an attendance record.\n\n* * *\n\n_Students will remember your kindness a lot longer than they'll remember any\nparticular homework assignment._\n\n—Holly Ordway16\n\n* * *\n\n## Notes\n\n1 Taeyoon Choi. “Worms, Butterflies, and Dandelions: Open Source Tools for the\nArts.” [Medium.com](http://Medium.com), June 20, 2018,\n<https://medium.com/@tchoi8/worms-butterflies-and-dandelions-open-source-\ntools-for-the-arts-9b4dcd76a1f2>. 2 Rebecca Fiebrink (@RebeccaFiebrink),\nTwitter, August 8, 2019, 7:18 AM,\n<https://twitter.com/RebeccaFiebrink/status/1159423540392812546>. 3 “User's\nManual,” The Recurse Center, last modified July 26, 2019,\n<https://www.recurse.com/manual>. 4 Kamal Sinclair, “The High Stakes of\nLimited Inclusion,” Making a New Reality, November 29, 2017,\n<https://makinganewreality.org>; quoted in Cara Mertes, “Now Is the Time for\nSocial Justice Philanthropy to Invest in Emerging Media,” _Equals Change Blog_\n, June 22, 2018, <https://www.fordfoundation.org/ideas/equals-change-\nblog/posts/now-is-the-time-for-social-justice-philanthropy-to-invest-in-\nemerging-media>. 5 Luca Damasco (@Lucapodular), Twitter, August 8, 2019, 8:14\nAM, <https://twitter.com/Lucapodular/status/1159437696663789569>. 6 Douglas E.\nStanley (@abstractmachine), “Break code, then try to find your way back,\nasking students for help,” Twitter, August 8, 2019, 6:10 AM,\n<https://twitter.com/abstractmachine/status/1159406642947121152>. 7 Jen\nSimmons (@jensimmons), Twitter, July 26, 2018, 1:20 PM,\n<https://twitter.com/jensimmons/status/1022532183733481472>. 8 Lauren\nMcCarthy, “Are You All In?” (lecture, Learning to Teach, Teaching to Learn II,\nPostlight, NY, January 2017), video, 1:12:35,\n<https://www.youtube.com/watch?v=D7-m6NJ90RE>. 9 Francis Hunger\n(@databaseculture), Twitter, August 7, 2019, 4:35 PM,\n<https://twitter.com/databaseculture/status/1159201338112319491>. 10 Rebecca\nFiebrink (@RebeccaFiebrink), Twitter, August 8, 2019, 7:16 AM,\n<https://twitter.com/RebeccaFiebrink/status/1159423246745382912>. 11 Pam A.\nMueller and Daniel M. Oppenheimer, “The Pen Is Mightier than the Keyboard:\nAdvantages of Longhand over Laptop Note Taking,” _Psychological Science_ 25,\nno. 6 (April 23, 2014): 1159–1168, doi:10.1177/0956797614524581. 12 Terry\nBarrett, _CRITS: A Student Manual_ (London: Bloomsbury Visual Arts, 2018),\n69–154 and _Criticizing Art: Understanding the Contemporary_ (Mountain View,\nCA: Mayfield Publishing Company, 1994). 13 Mitchell Whitelaw (@mtchl),\nTwitter, August 7, 2019, 5:23 PM,\n<https://twitter.com/mtchl/status/1159213387458347009>. 14 Cris Tovani, “Let's\nSwitch Questioning Around,” _Educational Leadership_ 73, no. 1 (2015): 30–35.\n15 Elizabeth F. Barkley, K. Patricia Cross, and Claire Howell Major,\n_Collaborative Learning Techniques: A Handbook for College Faculty_ (San\nFrancisco: Jossey-Bass, 2014), 35. 16 Holly Ordway (@HollyOrdway), Twitter,\nMarch 13, 2020, 5:47 PM,\n<https://twitter.com/HollyOrdway/status/1238582577461702657> and thread at\n<https://twitter.com/HollyOrdway/status/1238576343840968710>.\n\n\n# Provenance\n\nLike the Brothers Grimm, who compiled fairy tales from interviews with\ngrandmothers, or Dushko Petrovich and Roger White, who collected accounts of\nmemorable art assignments from artists and teachers alike, we have assembled\nfavorite computational art and design prompts from friends, colleagues,\nmentors, and students, documenting the strategies and pedagogies of a\ncommunity teaching creative visual production through code. Petrovich and\nWhite remind us that legendary assignments can stay with you for life, often\nresurfacing when one reenters the classroom as a teacher: “Most artists, when\nthey begin to teach, will pass along—consciously or not—assignments they\nthemselves were once given.” 1 Assignments are fodder for adaptation and are\ncontinually shared, forked, recontextualized, subverted, and renewed, a\nprocess that often makes their authorship plural and ambiguous.\n\nPublic records documenting art and design pedagogy are scarce and poorly\nmaintained, especially in contrast to notable works of art and design, which\nare discussed in critical texts and collected by museums. In their\nextraordinary compilation of graphic design assignments, _Taking a Line for a\nWalk: Assignments in Design Education,_ Nina Paim, Emilia Bergmark, and\nCorinne Gisel lament that, in design education, “the layer of language that\nruns alongside this process is often neglected. Words fly out of a teacher's\nor a student's mouth and quickly disappear into thin air. Instructions and\nspecifications, corrections and questions, fuse with practical work. And\nassignments, if written down at all, are rarely considered something worth\nsaving.” 2 In the realm of computational media arts and design, it has been\nmore common for classroom syllabi and curricula to appear online; these\nmaterials, however, are subject to the uniquely digital vagaries of data\npreservation: link rot and bit rot. Web servers for old courses are rarely\nconsidered something worth maintaining; the “walled gardens” of many\ncourseware systems restrict public access; and computer arts courses prior to\n1994 precede the World Wide Web altogether, eluding search engines. The\nInternet is astonishingly fragile, and even within the few years we have spent\nwriting this book, links to many noteworthy resources have gone dark, and\nprojects have been quietly retired from creator portfolios—a process that is\nproducing an undeniable amnesia throughout the field.\n\nIn this section, we lay out the sources from which we encountered or developed\nthe assignments in this collection. We cannot and do not claim that the\ninformation here is definitive. To trace the origins, history, and provenance\nof these assignments would require extensive oral history research, and\nremains a worthy challenge for art and education historians of the future. In\naddition to the gaps in our knowledge due to the absence or loss of\ndocumentation, we also acknowledge that the information below contains\noversights and blind spots owing to our lack of familiarity with non-English-\nspeaking art and technology educational communities, and we fully acknowledge\nthat our perspective is not representative of the rest of the world.\n\nWhile many of the assignments in this book have been adapted from the syllabi\nof our teachers and peers, a few were primarily inspired by projects that we\nadmire. In discussing the provenance of the assignments in their book, Paim,\nBergmark, and Gisel call this process of back-formation “reconstruction,” and\nwe choose similar vocabulary here. In particular, our assignments Personal\nProsthetic, Parametric Object, and Virtual Public Sculpture were inspired by\nthe works that illustrate their respective modules, and are not otherwise\ndiscussed below.\n\nIn other cases, our assignments rely on technologies like speech recognition,\n3D printing, augmented reality, or machine learning that have only recently\nbecome widely accessible. As educators and practitioners have only had a\nrelatively brief time to experiment with these tools, methods for how to teach\nthe creative use of these technologies within art and design contexts are\nstill (quickly) emerging. In such cases, we have taken the liberty to devise\nthe assignment briefs ourselves. This is the case for assignments including\nVoice Machine, Bot, Personal Prosthetic, Parametric Object, and Virtual Public\nSculpture.\n\nFinally, some of the educational approaches documented here spread from the\ncommunity of John Maeda's Aesthetics + Computation Group at the MIT Media\nLaboratory, where one of the authors of this book (Golan Levin) was a graduate\nstudent alongside Casey Reas and Ben Fry (who co-founded the Processing\ninitiative). With this context in mind, it is important to note that some of\nthe assignments presented in this book have been drawn directly from personal\nexperience and firsthand accounts from our peers, and some have been plucked\nfrom a buzzing zeitgeist—distilled from the research, educational materials,\nand social media posts of a highly interconnected community of artists,\nteachers, and students.\n\nIterative Pattern\n\nThe Iterative Pattern exercise extends from the earliest practices in plotter-\nbased computer art—the first university courses for which arose in the early-\nto-mid 1970s. Writing in 1977, Grace C. Hertlein, a professor of computer\nscience at California State University at Chico, details a list of notable\n“computer art systems” actively used in higher education at the time: “in\nJerusalem, by Vladimir Bonacic; Reiner Schneeberger (University of Munich);\nJean Bevis (Georgia State University); Grace C. Hertlein (California State\nUniversity system); John Skelton (University of Denver); Katherine Nash and\nRichard Williams (University of Minnesota at Minneapolis).” 3\n\nAn example of work by one of Reiner Schneeberger's students, Robert Stoiber,\nis shown below, a grid of nested squares.4 In this picture, in which “the\nmiddle point of each square was obtained by chance,” it is clear that students\nwere directed to explore iterative loops and randomness. In a 1976 article,\nSchneeberger describes the context in which this work was produced, a summer\ncourse taught in collaboration with Professor Hans Daucher of the Department\nof Art: “This is a report of the first computer graphics course for students\nof art at the University of Munich. […] A further objective to be realized was\nfor every student to be able to generate aesthetically appealing computer\ngraphics after only the first lecture period.” Schneeberger mentions that the\nart students experienced greater than normal hardship, as all of the computer\nprogramming work had to be performed “locally at the Computer Center, some ten\nkilometers distant from the instructional site.” 5\n\nIterative patternmaking is now a standard exercise in texts on creative\ncoding, especially where iteration techniques are introduced. Examples include\nCasey Reas and Chandler McWilliams's _Form+Code in Design, Art, and\nArchitecture_ (2010)6 and Hartmut Bohnacker et al.'s _Generative Design:\nVisualize, Program, and Create with Processing_ (2012).7\n\n![b2-fig-5001.jpg](../images/b2-fig-5001.jpg)\n\nFace Generator\n\nIn his book _Design as Art_ (1966), Bruno Munari presents the results of a\nchallenge he gave himself: how many different ways could he draw the human\nface?8 Mark Wilson later transposed this assignment to the realm of computer\narts education in _Drawing with Computers_ (1985). Wilson describes a\nhypothetical face-generation software program called METAFACE, analogous to\nDonald Knuth's METAFONT (1977), that his reader is encouraged to develop:\n\nThe human face could be schematically rendered with a sparse set of lines and\ncircles similar to the minimal description of the alphabet. It would be\npossible to write a program—let's call it METAFACE—that would emulate some of\nthe extraordinary variations of the face. The parameters for the various\nvisual descriptions of the face would be given to the program: the size of the\neyes, the location of the eyes, and so forth. Depending on the ambitiousness\nof the programmer, the program could become exceedingly complex.9\n\nWilson also illustrated the variety of faces such a program might produce:\n\n![b2-fig-5002.jpg](../images/b2-fig-5002.jpg)\n\nLorenzo Bravi, an educator at the design department of IUAV of Venice and\nlater at the ISIA of Urbino, gave an influential generative face assignment\ncalled “Parametric Mask” in 2010. Computational face designs by Bravi's\nstudents were used to create _Bla Bla Bla_ , a sound-reactive application for\niPhone and iPad.10 In a 2011 brief, Casey Reas, acknowledging both Munari and\nBravi, invited his students at UCLA to make microphone-reactive faces.11 Face\ngenerator exercises have since become commonplace in introductory creative\ntechnology courses; dozens of examples can be found at\n[OpenProcessing.org](http://OpenProcessing.org), written by educators such as\nJulia Pierre, Steffen Klaue, Rich Pell, Isaac Muro, and Anna Mª del Corral.\n\nClock\n\n“The Clock” is an evergreen creative coding assignment, and was the original\ninspiration for this book. An August 2019 survey of 847 classrooms on\n[OpenProcessing.org](http://OpenProcessing.org) revealed scores of clock\nprojects—assigned by a bevy of international educators including Amy\nCartwright, Sheng-Fen Nik Chien, Tomi Dufva, Scott Fitzgerald, June-Hao Hou,\nCedric Kiefer, Michael Kontopoulos, Brian Lucid, Monica Monin, Matti\nNiinimäki, Ben Norskov, Paolo Pedercini, Rich Pell, Julia Pierre, Rusty\nRobison, Lynn Tomaszewski, Andreas Wanner, Mitsuya Watanabe, and Michael\nZöllner. The particular text of the clock assignment presented in this volume\nis most closely adapted from Golan's version, “Abstract Clock: A diurnally-\ncyclic dynamic display,” which he assigned in his Fall 2004 Interactive Image\ncourse at Carnegie Mellon.12\n\nThe graphic representation of time has long figured into both analog and\ndigital design education. An assignment to devise a graphic system that\ndisplays and contrasts the rhythms of sixteen different calendars (including\nAztec, Chinese, Gregorian, etc.) was given at the Yale School of Art in 1982\nby Greer Allen, Alvin Eisenman, and Jane Greenfield.13 The first\n_computational_ clock assignment that we know of was assigned by John Maeda in\nhis Fall 1999 Organic Form course at the MIT Media Lab. This course—whose\nstudents included Golan as well as future computational media educators like\nElise Co, Ben Fry, Aisling Kelliher, Axel Kilian, Casey Reas, and Tom White —\nexamined “the nature of symbolic descriptions that are creatively coerced into\nrepresentations that react to both internal changes in state and external\nchanges in environment.” In his clock assignment, shown below, Maeda asked\nstudents to use the DBN programming environment to “create a display of time\nthat does not necessarily depict the exact progress of time, but rather the\nabstract concept of time.” 14\n\n![b2-fig-5003.jpg](../images/b2-fig-5003.jpg)\n\nMaeda's assignment extended from his own artistic exploration of time displays\nin his 1996 _12 O’Clocks_ project. Introducing a simplified clock project in\nhis 1999 book, _Design by Numbers_ , Maeda writes that “time is the most\nrelevant subject to depict by means of a dynamically changing form.[…] Given\nthe ability to computationally observe the progress of time, a form that can\nreflect the time is easily constructed.” 15\n\nThe computational clock assignment spread quickly to other universities in the\nearly 2000s. The Computer-Related Design graduate course at Royal College of\nArt, London, was an early such center for creative coding education.16 Citing\nMaeda's clocks as an inspiration, RCA instructors Rory Hamilton and Dominic\nRobson asked graduate students to design a “timepiece” in an interaction\ndesign course in February 2002. Hamilton and Robson's conceptually oriented\n“pressure project” is agnostic as to medium:\n\n> This simple brief is to look at the nature of clocks and other time\n> measuring devices. How do we use them? Why do we use them? What is their\n> meaning? Restyling of clocks are numerous: but we ask you not to restyle but\n> to rethink. Not to reskin existing clocks but to come up with a completely\n> new way of looking at time. Your design should be beautiful, engaging, and\n> work. Whatever medium you choose we should all be able to understand and use\n> your system.17\n\nA 2005 article by a group of Georgia Tech faculty highlights the clock as a\nkey assignment in Computing as an Expressive Medium, a core graduate level\ncourse taught by Michael Mateas. For Mateas, who asks students to “display the\nprogress of time in a non-traditional way,” the clock is not an exercise in\nutilitarian design, but an “expressive project” whose “goal is to start\nstudents thinking about the procedural generation of imagery as well as\nresponsiveness to input, in this case both the system clock, and potentially,\nmouse input.” 18\n\nIn his Fall 2008 Comparative Media Studies workshop at MIT, Nick Montfort\nasked students to develop clocks (“a computer program that visually indicates\nthe current hour, minute, and second”).19 Casey Reas introduced the clock\nassignment to UCLA in Spring 2011. Reas's version leaves open the question of\nwhether the students’ clock should be literal or abstract, and instead places\na primary emphasis on an iterative design process of sketching and ideation.\nAsking students to “create a ‘time visualization,’ aka a clock,” Reas requires\nstudents to bring “at least five different ideas, each with six drawings to\nshow how the clock changes in time.” 20\n\nIn September 2017, NYU ITP educator Dan Shiffman canonized the clock\nassignment for a wide audience in his popular _Coding Train_ video channel on\nYouTube. Citing Maeda's _12 O’Clocks_ and Golan Levin's Fall 2016 course\nmaterials, Shiffman's “Coding Challenge #74: Clock with p5.js” video has\naccumulated (as of May 2020) more than 350,000 views.21\n\nGenerative Landscape\n\nAs a 2006 survey by George Kelly and Hugh McCabe shows, the challenge of\nprocedurally generating landscapes and terrains has been a fixture in game\ndesign and computer graphics literature since the mid-1980s.22 Whereas most\nearly computer graphics research was concerned with achieving realism, the\nintroduction of software development environments into art schools in the\nearly 2000s created a context in which the problem could be imaginatively\naddressed by students steeped in the traditions of conceptual art,\nperformance, film, and art history. The language of the Generative Landscape\nassignment presented here (populated with “body parts, hairs, seaweed, space\njunk, [or] zombies”) is adapted from a prompt given by Golan in his Fall 2005\nInteractive Image course.23\n\nNick Montfort's Comparative Media Studies workshop in 2008 also featured a\n“Generated Landscape” assignment. Navigability is a key requirement of\nMontfort's assignment, which stipulates that a user be able to “move around a\nlarge virtual space, seeing one window of this space at a time.” 24 This\nassignment is included and discussed in Montfort's 2016 book, _Exploratory\nProgramming for the Arts and Humanities_ , which provides sample code for\nreaders to “create a virtual, navigable space.” 25 A more tightly constrained\n“Noisy Landscapes” assignment also appears in Bohnacker et al., _Generative\nDesign_.26\n\nVirtual Creature\n\nJohn Maeda asked students to create a virtual creature in his Fall 1999\ncourse, Fundamentals of Computational Media Design (MAS.110) at MIT. “Inspired\nby a diagram of a cell in _The Biology Coloring Book_ , I asked my class to\nre-interpret the canonical drawing of an amoeba in a medium of their choice.”\n27 Influenced by this prompt, Golan initiated the\n[Singlecell.org](http://Singlecell.org) project in January 2001, inviting\ncreators like Lia, Marius Watz, Casey Reas, and Martin Wattenberg to create\ninteractive creatures for an “online bestiary of online life-forms reared by a\ndiverse group of computational artists and designers.” 28 An even more open-\nended virtual creature assignment was presented by Lukas Vojir's influential\n_Processing Monsters_ project in 2008, which collected Processing-based code\nsketches from scores of contributors around the world:29\n\n![b2-fig-5004.jpg](../images/b2-fig-5004.jpg)\n\n> I'm trying to get as much people as possible, to create simple b/w [black\n> and white] monster in Processing, […] while the bottom line is to encourage\n> other people to learn Processing by showing the source code. So if you feel\n> like you can make one too and be part of it, the rules are simple: Strictly\n> black and white + mouse reactive.30\n\n“Creature” assignments in courses taught by Chandler McWilliams and John Houck\nat UCLA between 2007 and 2009 made explicit mention of Vojir's _Processing\nMonsters_ , as well as animistic simulation models like Valentino\nBraitenberg's vehicles. The UCLA assignments emphasized the use of Java\nclasses and object-oriented programming to develop virtual creatures with\nparameterized appearances and behaviors.31 More recently,\n[OpenProcessing.org](http://OpenProcessing.org) has come to host creature\nassignments by educators including Tifanie Bouchara, Margaretha Haughwout,\nCaroline Kassimo-Zahnd, Cedric Kiefer, Rose Marshack, Matt Richard, Matt\nRobinett, and Kevin Siwoff. Some of these assignments invite students to use\nflocking behaviors explained by Craig Reynolds in his influential 1999\n“Steering Behaviors For Autonomous Characters” 32 and popularized in Dan\nShiffman's _Nature of Code_ book and associated videos.33\n\nCustom Pixel\n\nThe concept of custom picture elements in computer arts extends from\nexperiments in the 1960s by Leon Harmon and Ken Knowlton at Bell Labs; Danny\nRozin's renowned _Wooden Mirror_ (1999) and other interactive mirror\nsculptures; and photomosaic work by Joseph Francis and Rob Silvers, among\nothers. The ability of Processing and other creative coding toolkits to\nprovide straightforward access to image pixel data means that a “custom pixel”\nassignment can be a productive way to support instruction in introductory\nimage processing. Exercises of this sort appear in Casey Reas and Ben Fry's\n_Processing: A Programming Handbook for Visual Designers and Artists_\n(2007);34 in Ira Greenberg's _Processing: Creative Coding and Computational\nArt_ (2007);35 in Dan Shiffman's _Learning Processing_ (2008);36 in Andrew\nGlassner's _Processing for Visual Artists: How to Create Expressive Images and\nInteractive Art_ (2010);37 in Reas and McWilliams's _Form+Code_ (2010);38 and\nin Bohnacker et al.'s _Generative Design_ (2012)39.\n\nThe assignment as presented here derives from Golan's Fall 2004 Interactive\nImage course. He asked students to “create a ‘custom picture element’ with\nwhich to render your image. At no time should the original image be seen\ndirectly.” 40\n\nDrawing Machine\n\nOur Drawing Machine assignment extends from a tradition of experimental\ninteractive paint programs developed by artists during the late 1980s and\n1990s, such as Paul Haeberli's _DynaDraw_ , Scott Snibbe's _Motion Sketch_ and\n_Bubble Harp_ , Toshio Iwai's _Music Insects_ , and John Maeda's conceptually\noriented drawing tools, _Radial Paint_ , _Time Paint_ , and _A-Paint_ (or\n_Alive-Paint_).41 By 1999, Maeda had formalized this type of inquiry as an\neducational assignment. In his book _Design by Numbers_ , he presents an\nexercise involving the construction of an ultra-minimal paint program: a loop\nthat continually sets a canvas pixel to black, at the location of the cursor.\nIn a section entitled “Special Brushes,” Maeda writes: “Perhaps the most\nentertaining exercise in studying digital paint is the process of designing\nyour own special paintbrush. There really is no limit to the kind of brush you\ncan create, ranging from the most straightforward to the completely\nnonsensical. How you approach this creative endeavor is up to you.” 42 Maeda\noffers some potential responses to this assignment, including a pen whose ink\nchanges color over time; a calligraphy brush with a diagonal tip; and a vector\ndrawing tool for polylines. Exemplary student responses to this prompt were\npublished by JT Nimoy in 2001 (then an undergraduate intern in Maeda's group\nat MIT) as a collection of twenty interactive “Scribble Variations,” 43 and by\nZach Lieberman in his 2002 master's thesis at Parsons School of Design,\n“Gesture Machines.” Lieberman's interactive thesis projects were published at\nthe now-defunct Remedi Project online gallery44 and were revived for a lecture\npresentation recorded at the 2015 Eyeo Festival.45\n\nVariants of the drawing tool assignment proliferated in the early 2000s. In a\nFall 2003 course at UCLA, Casey Reas asked students to develop a “mouse-based\ndrawing machine.” 46 In a Spring 2004 course, he asked students to\n\n> Have a concrete idea about the type of images your machine will construct\n> and be prepared to explain this idea during the critique. Build your project\n> to have a large range in the quality of drawings generated and to have a\n> large range of formal contrasts. It's very difficult to program\n> representational images, so it's advised to focus on constructing abstract\n> drawings. Do not use any random values, but instead rely on other sources of\n> data […] as the generators of form and motion.47\n\nGolan's 2000 master's thesis, “Painterly Interfaces for Audiovisual\nPerformance,” developed under John Maeda's supervision at MIT, presented a\ncollection of five interactive software programs for the gestural creation and\nperformance of dynamic imagery and sound. Based on insights from this work,\nGolan assigned a “custom drawing program” in his Fall 2004 course at CMU,\nIntroduction to Interactive Graphics.48 In Spring 2005, he refined this\ndrawing program to one in which “the user's drawings come to life” by making\n“a drawing program which augments the user's gesture in an engaging manner.”\n49\n\nIn a 2005 article, Michael Mateas describes a “drawing tool” assignment given\nin his graduate-level course at Georgia Tech, Computing as an Expressive\nMedium. Mateas asks students to\n\n> Create your own drawing tool, emphasizing algorithmic generation /\n> modification / manipulation. […] The goal of this project is to explore the\n> notion of a tool. Tools are not neutral, but rather bear the marks of the\n> historical process of their creation, literally encoding the biases, dreams,\n> and political realities of its creators, offering affordances for some\n> interactions while making other interactions difficult or impossible to\n> perform or even conceive.50\n\nIn his Winter 2007 and Spring 2008 Interactivity courses at UCLA, Chandler\nMcWilliams gave assignments that explored the nuanced differences between a\nmouse-based “drawing machine” and a “drawing tool.” 51 Since that time, a\npanoply of drawing tool assignments have been published online at\n[OpenProcessing.org](http://OpenProcessing.org), in online classrooms taught\nby Antonio Belluscio, Tifanie Bouchara, Tomi Dufva, Briag Dupont, Rachel\nFlorman, Erik Harloff, Cedric Kiefer, Steffen Klaue, Andreas Koller, Brian\nLucid, Yasushi Noguchi, Paolo Pedercini, Julia Pierre, Ben Schulz, Devon\nScott-Tunkin, and Bridget Sitkoff, among others. The assignment has also\nfeatured in books on introductory creative coding, such as Bohnacker et al.'s\n_Generative Design_ (2012).52\n\nModular Alphabet\n\nThe design principles of parameterization and modularity are foundational\nconsiderations in traditional typography education, quite apart from the use\nof code and digital techniques. A 1982 pencil-and-paper assignment by Hans-\nRudolf Lutz at Ohio State University, for example, asked students to render an\nalphabet with five stages of transitions between consecutive letters.53 An\nassignment by Laura Meseguer, given in 2013 at the Escuela Universitaria de\nDiseño e Ingeniería de Barcelona, asked students to design a modular typeface\nfrom a small set of hand-drawn graphical elements. According to Meseguer,\n“working with this modular method will help you to understand the architecture\nof letterforms and modularity, coherence, and harmony inherent to type\ndesign.” 54\n\nAs a creative coding assignment, our Modular Alphabet project descends most\ndirectly from prompts and student work developed in John Maeda's 1997 Digital\nTypography course at the MIT Media Lab, which focused on the “algorithmic\nmanipulation of type as word, symbol, and form.” 55 Maeda assigned “Pliant\nType” and “Unstable Type” projects that asked students (working in Java) to\n“design a vector-based typeface that transforms well” and to “design a\nparameterized typeface with inherently unstable properties.” 56 Peter Cho's\nproject _Type Me, Type Me Not_ emerged from Maeda's prompts, and established\nthe template for the assignment as reconstructed here.57 Cho's writeup for the\ncourse gallery page explains that “at some point in the class I became fixated\nwith the idea of constructing letters from only circular pie pieces, using the\nfillArc method of the Java graphics class. […] Since each letter is devised\nfrom two filled arcs, it is easy to make transitions between letters in a\nsmooth way.” 58 The text of the assignment in the present volume is adapted\nfrom Golan's “Intermorphable Alphabet: A custom graphic alphabet,” which he\nassigned in his 2004 Interactive Image course.59\n\nComputational “parametric type” assignments, extending from the logic of\nKnuth's METAFONT (1977), focus on the use of parameters to control continuous\nproperties of a typeface. An assignment that illustrates this is “Varying the\nFont Outline (P 3.2.2)” in _Generative Design_ ,60 in which the reader is\nprompted to explore how a code variable may govern a property such as the\ntypeface's overall slant, or something more unconventional, like wiggliness.\n\nData Self-Portrait\n\nDuring the first decade of the 2000s, Judith Donath and her graduate students\nin the MIT Media Lab's Sociable Media Group created “data portraits,”\nformulating principles for producing and understanding media objects that\ndepict their subjects’ accumulated data rather than their faces. Donath argued\nthat “calling these representations ‘portraits’ rather than ‘visualizations’\nshifts the way we think about them.” 61 Concurrently, Nick Felton's diaristic,\nassignment-like “Annual Report” information visualizations, which he published\nfrom 2005 through 2014 and are now part of MoMA's permanent collection, were\nhighly influential in establishing a precedent for computationally designed,\ndata-driven self-portraiture.\n\nBy the early 2010s, the use of personal fitness trackers and smartphone\nselfies had become widespread, pushing an evolution in how data self-\nportraiture was both implemented and described. The wording of the assignment\nin this book is adapted from an exercise in Golan's Spring 2014 Interactive\nArt & Computational Design course at CMU, which asked students to “develop a\nvisualization which offers insights into some data you care about. This\nproject will probably take the form of a “Quantified Selfie”: a computational\nself-portrait developed from any (one or more) of the data-streams you\nproduce.” 62\n\nAugmented Projection\n\nOur assignment is inspired by installation works by artists including Michael\nNaimark, Krystof Wodiczo, Christopher Baker, Andreas Gysin + Sidi Vanetti,\nHeHe (Helen Evans and Heiko Hansen), Jillian Mayer, Joreg Djerzinski, and\nPablo Valbuena. Valbuena's computationally generative _Augmented Sculpture_\nartwork, presented at the 2007 Ars Electronica Festival and widely viewed\nonline,63 was also particularly influential to creative technologists, and\nspurred the development of commercial projection mapping software like\nMadMapper and Millumin. With the help of these tools, projection mapping has\nbecome a staple in progressive theater scenography, and is taught in many\ngraduate programs in video and media design.\n\nGolan gave a version of this assignment (“a poetic gesture projected on a\nwall”) in fall 2013. Students were asked to write code in Processing, using\nthe Box2D physics library, in order to generate real-time animated graphics\nthat related both visually and conceptually to wall features like power\noutlets and doorknobs.64\n\nOne-Button Game\n\nThe first public competition to create “a strictly one-button game”—that we\nknow of—was held in April 2005 by Retro Remakes, an online community of\nindependent game developers.65 Soon after, in a Gamasutra article that would\nbe widely cited in subsequent game design syllabi, Berbank Green discussed in-\ndepth design issues in the low-level mechanics of one-button games.66\n\nIn December of 2009, the experimental game design collective Kokoromi\nannounced the GAMMA IV competition for one-button games, whose winners were\npresented to a large audience at the March 2010 Game Developers Conference.\nResponding to an efflorescence of “high-tech” game controllers that had just\nbeen released to the market, with interfaces like “gestural controls, multi-\ntouch surfaces, musical instruments, voice recognition—even brain control,”\nKokoromi proposed “that game developers can still find beauty in absolute\nsimplicity.” 67 In August of 2009, the wildly popular one-button game\n_Canabalt_ was released online.68 For his Winter 2009 Interactivity course at\nUCLA, Chandler McWilliams introduced the one-button game as a classroom\nexercise:\n\n> Develop a simple one-button game, meaning the interface is a single button.\n> Focus your ideas on expressing a theme while making an interesting\n> experience. Do not be overly concerned with how the game behaves\n> technically, great games can be made very simply. Instead consider how\n> common video and board games operate and what features of these artifacts\n> you can reinterpret in interesting ways. The game need not involve scores or\n> levels; it can be a playful experience. Remember that conceptual and visual\n> development is as important as technical achievement.69\n\nPaolo Pedercini also mentions one-button games (and “no-button games”) in a\nCMU Experimental Game Design syllabus from Fall 2010.70\n\nOne-button games are now a popular and well-established genre. As of August\n2019, the indie game distribution website Itch.io, for example, reported\nhosting over 2800 unique one-button games.\n\nExperimental Chat\n\nOur assignment is influenced by mid-1990s telematic media artworks, such as\nPaul Sermon's _Telematic Dreaming_ (1992), Scott Snibbe's _Motion Phone_\n(1995), and Rafael Lozano-Hemmer's _The Trace_ (1995). This assignment also\nhas roots in the “Shared Paper” exercise that John Maeda discusses in _Design\nby Numbers._ (1999).71 Maeda describes a publicly accessible web server that\nstores 1000 numbers, whose values could be read and modified by user code.\nBuilding on this ultra-simple platform, Maeda presents a “Collaborative\nDrawing” project, in which mouse coordinates are shared from one person to\nanother,72 and “Primitive Chat,” in which a single letter is sent at a time.73\n\nMaeda elaborates on this assignment in _Creative Code_ (2004):\n\n> The standard final assignment I used to set my class [in 1997] was to\n> transmute a stream of data communication. The communication stream in\n> question was the server, which used to pass messages from any connected\n> client to all connected clients simultaneously in the way that Internet-\n> based chat systems work. I stopped setting this assignment because of the\n> technical difficulties of keeping the server running reliably.74\n\nBrowser Extension\n\nA precursor to the Browser Extension assignment was Alex Galloway's\n_Carnivore_ project (2000–2001), in which he invited 15 artists to contribute\ncustom software clients that visualized network traffic captured by a packet\nsniffer. Each “Carnivore Client” provided a different lens on internet\ncommunication. In 2008–2009, Google and Mozilla introduced add-on ecosystems\nfor their Chrome and Firefox web browsers, kicking off a wave of shareable\nexperimentation. The creation of commercial browser extensions as a mode of\nart practice arose at that time, boosted by the popular success of Steve\nLambert's _Add Art_ , a browser plugin that automatically replaces online\nadvertisements with artworks. Lambert's project informs our assignment, and we\ntook additional inspiration from artworks by Allison Burtch, Julian Oliver,\nLauren McCarthy, and others.\n\nCreative Cryptography\n\nThe 2013 Snowden revelations catalyzed artistic engagement with issues\nsurrounding digital privacy, security, surveillance, anonymity, and\ncryptographic technologies.75 Keeping in mind certain cultural practices and\nartworks that problematized these themes, Tega developed this prompt for\nSocial Software, a 2015 class at Purchase College. It draws on research and\nprojects by practitioners like Addie Wagenecht, Julian Oliver, Adam Harvey,\nDavid Huerta and many others.\n\nA variety of mathematics educators have also written about the pedagogic value\nof cryptography for engaging students from non-technical backgrounds,\nincluding Brian Winkel, Neal Koblitz, Manmohan Kaur, and Lorelei Koss.76 These\nauthors discuss cryptography in the context of both mathematics education and\ngeneral education, and although they aim to cultivate mathematical\ncompetencies, rather than designerly or artistic skill sets, they each observe\nthat the study of cryptography brings together the political, social and\ntechnical dimensions of math and computation.\n\nVoice Machine\n\nSpeech-based human-computer interaction in real time became a practical\nreality for creative experimentation in the late 2010s, as we were writing\nthis book. In 2017, Google's Creative Lab division, seeking to encourage\ndevelopers to adopt their Google Home platform and Dialogflow speech-to-text\ntoolkit, sponsored exploratory investigations into “what's possible when you\nbring open-ended, natural conversation into games, music, storytelling, and\nmore.” 77 Among those supported by this “Voice Experiments” initiative was\nprogrammer and artist Nicole He, who created projects like _Mystery Animal_ ,\na game in which the computer pretends to be an animal and users have to guess\nwhat it is by asking spoken questions. In Fall 2018, He taught a course at NYU\nITP entitled Hello, Computer: Unconventional Uses of Voice Technology. The\ncourse objective was to\n\n> give students the technical ability to imagine and build more creative uses\n> of voice technology. Students will be encouraged to examine and play with\n> the ways in which this emerging field is still broken and strange. We will\n> develop interactions, performances, artworks or apps exploring the unique\n> experience of human-computer conversation.78\n\nOur Voice Machine assignment draws inspiration from He's syllabus and projects\nby her students;79 from Google's Voice Experiments program; and from\npioneering early works of speech-based interactive media art, such as David\nRokeby's _The Giver of Names_ (1990).\n\nMeasuring Device\n\nData collection is a starting point for education in a wide range of fields,\nwhether in engineering, the natural sciences, social sciences, communication\ndesign (e.g., in information visualization), or contemporary art (in critical\ncultural practices). Several of our peers have given assignments that ask\ncreative coding students to collect data using an API (Jer Thorp at NYU ITP)80\nor that ask them to develop “scrapers” for computationally harvesting\ninformation from the Internet (Sam Lavigne at NYU ITP and the School for\nPoetic Computation).81 The assignment we present here, however, specifically\nrequires students to develop custom hardware to collect measurements from some\ndynamic system in the physical world.\n\nThe premise of data-collection-as-art comes from approaches used by many\nartists, including Hans Haacke, Mark Lombardi, On Kawara, Natalie Jeremijenko,\nBeatriz da Costa, Brooke Singer, Catherine D’Ignazio, Eric Paulos, Amy Balkin,\nand Kate Rich. It also draws on citizen science projects like Safecast, the\nAir Quality Egg, Smart Citizen Platform, Pachube, and the many instructables\nand online tutorials for DIY data collection tools. Our assignment was adapted\nfrom an assignment in Golan's Electronic Media Studio 2 syllabus, given to art\nstudents at CMU in 2013.82\n\nExtrapolated Body\n\nThe Extrapolated Body assignment is inspired by interactive artworks like\nMyron Krueger's _Videoplace_ (~1974–1989) and the long history of expressive\nuses of offline motion capture in Hollywood computer graphics. Casey Reas's\nUCLA syllabi from the mid-2000s are some of the earliest extant records of\nassignments for the computational, real-time augmentation of bodies captured\nwith cameras and computer vision; an exercise from Winter 2004, for example,\nasks students to use “the unencumbered body as the interface to interacting\nwith a piece of software. Develop and implement an idea for the interaction\nbetween two people via a projection, camera, and computer. Remember that\nthrough processing the camera data, it is possible to track the body, track\ncolors, determine the direction of motion, read gestures, etc.” 83 A related\nassignment from Winter 2006 asks students to “Develop a concept for a video\nmirror which utilizes techniques of computer vision.” 84\n\nFrom Lauren McCarthy's “Mask” assignment, we have borrowed the premise of\nasking students to not only develop body-responsive software, but\nsimultaneously craft a performance that uses that software. For her Winter\n2019 Interactivity course at UCLA, McCarthy writes:\n\n> Write or select a short text (one paragraph or less) that you will\n> read/perform for the class. Based on the text, design and build a virtual\n> mask that you will use to perform the text. Using the provided code\n> template, make your mask react to audio, changing as the volume of your\n> voice changes. This project will be evaluated based on how the face relates\n> to the text, the variation of the mask (how much it changes), the design of\n> the mask, and your performance of it.85\n\nSynesthetic Instrument\n\nThe Synesthetic Instrument assignment asks students to develop a tool for the\nsimultaneous performance of sound and image. Our formulation extends from\nGolan's master's thesis work at the MIT Media Lab (1998–2000), which itself\ntook inspiration from 1990s audiovisual performance instruments by Toshio Iwai\nespecially. In April 2002, in his Audiovisual Systems and Machines (“Avsys”)\ngraduate course at the Parsons School of Design, Golan translated this\nresearch problem into an assignment in “Simultaneous real-time graphics and\nsound”:\n\n> Your assignment is to develop a system which responds to some kind of input\n> (for example, the mouse, the keyboard, some kind of real-time data-stream,\n> etc.) through the real-time generation of synthetic sound and graphics. […]\n> Your system must not use canned (pre-prepared) audio fragments or samples.\n> Therefore it will be necessary for you to code your own digital synthesizer,\n> from scratch. To create a relationship between the image and sound, it is\n> presumed that you will also need to appropriately map the data which\n> describes your visual simulation, to the inputs of your sound synthesizer.\n> When creating your system, consider some of the following possible issues,\n> to which there are no ‘correct’ answers: are the sound and image\n> commensurately plastic, or is one more malleable than the other? Are the\n> sound and image tightly related, or indirectly linked? What is the quality\n> of your use of negative space, in the sound as well as the image? Are\n> rhythms evident in either image or sound, or both?86\n\n## Notes\n\n1 Dushko Petrovich and Roger White, eds., _Draw It with Your Eyes Closed: The\nArt of the Art Assignment_ (Paper Monument, 2012), 122. 2 Nina Paim, Emilia\nBergmark, and Corinne Gisel, eds., _Taking a Line for a Walk: Assignments in\nDesign Education_ (Leipzig, Germany: Spector Books, 2016), 3. 3 Grace C.\nHertlein, “Design Techniques and Art Materials in Computer Art,” _Computer\nGraphics and Art_ 2, no. 3 (August 1977): 27, <http://dada.compart-\nbremen.de/docUploads/COMPUTER_GRAPHICS_AND_ART_Aug1977.pdf>. 4 Reiner\nSchneeberger, “Computer Graphics at the University of Munich (West Germany),”\n_Computer Graphics and Art_ 1, no. 4 (November 1976): 28,\n<http://dada.compart-\nbremen.de/docUploads/COMPUTER_GRAPHICS_AND_ART_Nov1976.pdf>. 5 Schneeberger,\n“Computer Graphics,” 28. 6 Casey Reas and Chandler McWilliams, _Form+Code in\nDesign, Art, and Architecture_ (New York: Princeton Architectural Press,\n2010), 64. See the exercise “Code Examples: Embedded Iteration.” 7 Hartmut\nBohnacker, Benedikt Groß, and Julia Laub, _Generative Design: Visualize,\nProgram, and Create with Processing_ , ed. Claudius Lazzeroni (Hudson, NY:\nPrinceton Architectural Press, 2012), 214–217. See the exercise “Complex\nModules in a Grid.” 8 Bruno Munari, “Variations on the Theme of the Human\nFace,” in _Design as Art_ , trans. Patrick Creagh (London: Pelican Books,\n1971), n.p. Originally published in Italian as _Arte come mestiere_ (Editori\nLaterza, 1966). 9 Mark Wilson, _Drawing with Computers_ (New York: Perigee\nBooks, 1985), 18, <http://mgwilson.com/Drawing%20with%20Computers.pdf>. 10\nFilip Visnjic, “Bla Bla Bla,” Creative Applications Network, April 26, 2011,\n<https://www.creativeapplications.net/processing/bla-bla-bla-iphone-of-\nprocessing-sound/>. 11 Casey Reas, syllabus for Interactivity (UCLA, Spring\n2011 and Fall 2011),\n<http://classes.design.ucla.edu/Spring11/28/exercises.html> and\n<http://classes.design.ucla.edu/Fall11/28/projects.html>. 12\n<https://web.archive.org/web/20060519010135/http://artscool.cfa.cmu.edu/~levin/courses/dmc/iig_04f/ejercicio.php>.\n13 Paim, Bergmark, and Gisel, _Taking a Line for a Walk_ , 135. 14 John Maeda,\nsyllabus for MAS.961: Organic Form (MIT Media Lab, Fall 1999),\n<https://web.archive.org/web/20000901042632/http://acg.media.mit.edu/courses/organic/>,\naccessed January 18, 2000. 15 John Maeda, _Design by Numbers_ (New York:\nRizzoli, 1999), 208. 16 Gillian Crampton-Smith, “Computer-Related Design at\nthe Royal College of Art,” _Interactions_ 4, no. 6 (November 1997): 27–33,\n<https://doi.org/10.1145/267505.267511>. 17\n<https://joelgethinlewis.com/oldrcasite/clock.html>. 18 Ian Bogost, Michael\nMateas, Janet Murray, and Michael Nitsche, “Asking What Is Possible: The\nGeorgia Tech Approach to Game Research and Education,” _iDMAa Journal_ 2, no.\n1 (Spring 2005): 59–68. 19 Nick Montfort, syllabus for CMS.950: Comparative\nMedia Studies Workshop 1 (MIT, Fall 2008),\n<http://nickm.com/classes/cms_workshop_i/2008_fall/>. 20\n<http://classes.design.ucla.edu/Spring11/28/exercises.html>. 21\n<https://www.youtube.com/watch?v=E4RyStef-gY>. 22 George Kelly and Hugh\nMcCabe, “A Survey of Procedural Techniques for City Generation,” _The ITB\nJournal_ 7, no. 2 (January 2006), doi:10.21427/D76M9P. Available at:\n<https://arrow.dit.ie/itbj/vol7/iss2/5>. 23\n<https://web.archive.org/web/20060617092315/http://artscool.cfa.cmu.edu/~levin/courses/dmc/iig_05f/ejercicio.php>.\n24 <http://nickm.com/classes/cms_workshop_i/2008_fall/>. 25 Nick Montfort,\n_Exploratory Programming for the Arts and Humanities_ (Cambridge, MA: MIT\nPress, 2016), 260. 26 Bohnacker, Groß, and Laub, _Generative Design_ , 330. 27\nJohn Maeda, _Creative Code: Aesthetics + Computation_ (London: Thames &\nHudson, 2004), 53. 28 Golan Levin (editor),\n<http://singlecell.org/singlecell.html>, 2001. 29\n<https://web.archive.org/web/20090304170310/http://rmx.cz/monsters/>. 30 Marc\nDe Vinck, “Processing Monsters by Lukas Vojir,” _Make:magazine_ (blog),\nNovember 11, 2008, <https://makezine.com/2008/11/11/processing-monsters-by-\nlu/>, accessed August 9, 2019. 31 Chandler McWilliams, syllabus for\nInteractivity (UCLA, Winter 2007 and Spring 2008),\n<http://classes.dma.ucla.edu/Winter07/28/> and\n<http://classes.dma.ucla.edu/Spring08/28/>; John Houck, syllabus for\nInteractivity (UCLA, Spring 2009),\n<http://classes.dma.ucla.edu/Spring09/28/exercises/>. 32 Craig W. Reynolds,\n“Steering Behaviors For Autonomous Characters,” _Proceedings of Game\nDevelopers Conference 1999, San Jose, California_ (San Francisco, CA: Miller\nFreeman Game Group, 1999), 763–782. 33 Daniel Shiffman, _The Nature of Code_\n(2012); _The Coding Train_ (YouTube channel). 34 Casey Reas and Ben Fry,\n“Image as Data,” in _Processing: A Programming Handbook_ (Cambridge, MA: MIT\nPress, 2007), 364. Examples include “Convert pixel values into a circle's\ndiameter” and “Convert the red values of pixels to line lengths.” 35 Ira\nGreenberg, _Processing: Creative Coding and Computational Art_ (New York:\nFriends of Ed Publishing, 2007), 441–451. See the “Pixilate” and “Pixel Array\nMask” examples. 36 Dan Shiffman, _Learning Processing: A Beginner's Guide to\nProgramming Images, Animation, and Interaction_ (Burlington, MA: Morgan\nKaufmann Publishers, Inc., 2008), 324–327. See exercises like Example 15-4,\n“Pointillism” and Example 16-10, “The Scribbler Mirror.” 37 Andrew Glassner,\n_Processing for Visual Artists: How to Create Expressive Images and\nInteractive Art_ (Boca Raton, FL: A K Peters/CRC Press, 2010), 468–470. 38\nReas and McWilliams, _Form+Code in Design, Art, and Architecture_ (New York:\nPrinceton Architectural Press, 2010), 90. See the exercise “Transcoded\nLandscape.” 39 Bohnacker, Groß, and Laub, _Generative Design_ , 302–317. See\nexercises like “Graphic from pixel values,” “Type from pixel values,” and\n“Real-time pixel values.” 40\n<https://web.archive.org/web/20060519010135/http://artscool.cfa.cmu.edu/~levin/courses/dmc/iig_04f/ejercicio.php>.\n41 John Maeda, _Maeda@Media_ (New York: Rizzoli, 2000), 94–99. 42 Maeda,\n_Design by Numbers_ , 166–169. 43 <https://github.com/jtnimoy/scribble-\nvariations>. 44\n<http://www.theremediproject.com/projects/issue12/systemisgesture/>. 45 Zach\nLieberman, “From Point A to Point B” (lecture, Eyeo Festival, Minneapolis, MN,\nJune 2015), <https://vimeo.com/135073747>. 46 Casey Reas, syllabus for Design\nfor Interactive Media (UCLA, Fall 2003),\n<http://classes.dma.ucla.edu/Fall03/157A/exercises.html>. 47 Casey Reas,\nsyllabus for Programming Media (UCLA, Spring 2004)\n<http://classes.design.ucla.edu/Spring04/160-2/exercises.html>. 48\n<https://web.archive.org/web/20060519010135/http://artscool.cfa.cmu.edu/~levin/courses/dmc/iig_04f/ejercicio.php>.\n49 Golan Levin, syllabus for The Interactive Image (CMU, Spring 2005),\n<https://web.archive.org/web/20060518224636/http://artscool.cfa.cmu.edu/~levin/courses/dmc/iig_05s/ejercicio.php>.\n50 Ian Bogost et al., “Asking What Is Possible,” 59–68. 51\n<http://classes.dma.ucla.edu/Winter07/28/>;\n<http://classes.dma.ucla.edu/Spring08/28/>. 52 Bohnacker, Groß, and Laub,\n_Generative Design_ , 236–245. See exercises including “Drawing with Animated\nBrushes” and “Drawing with Dynamic Brushes.” 53 Paim, Bergmark, and Gisel,\n_Taking a Line for a Walk_ , 51. 54 Paim, Bergmark, and Gisel, _Taking a Line\nfor a Walk_ , 41. 55 John Maeda, course description for MAS.962: Digital\nTypography (MIT Media Lab, Fall 1997), <https://ocw.mit.edu/courses/media-\narts-and-sciences/mas-962-digital-typography-fall-1997/>. 56 Maeda,\n_Maeda@Media_. See also\n<https://web.archive.org/web/20010124052200/https://acg.media.mit.edu/courses/mas962/>.\n57 Cho's project appears in print in Maeda, _Maeda@Media_ , 436. 58 The\nMAS.962 course gallery page is no longer available. See also Peter Cho,\n“Computational Models for Expressive Dimensional Typography” (master's thesis,\nMIT, 1999), 34, <https://acg.media.mit.edu/people/pcho/thesis/pchothesis.pdf>.\n59\n<https://web.archive.org/web/20060519010135/http://artscool.cfa.cmu.edu/~levin/courses/dmc/iig_04f/ejercicio.php>.\n60 Bohnacker, Groß, and Laub, _Generative Design_ , 276–285. 61 Judith Donath\net al., “Data Portraits” (lecture, SIGGRAPH ‘10, Los Angeles, CA, July 2010),\n<https://smg.media.mit.edu/papers/Donath/DataPortraits.Siggraph.final.graphics.pdf>.\n62 <http://golancourses.net/2014/assignments/project-3/>. 63\n<http://www.pablovalbuena.com/augmented/>. 64 Golan Levin, syllabus for\n60-210: Electronic Media Studio 2 (CMU, Fall 2013),\n<http://cmuems.com/2013/a/assignments/assignment-07/>. 65 Barrie Ellis,\n“Physical Barriers in Video Games,”\n[OneSwitch.org.uk](http://OneSwitch.org.uk), March 20, 2006,\n<http://www.oneswitch.org.uk/OS-REPOSITORY/ARTICLES/Physical_Barriers.doc>,\naccessed September 28, 2010. 66 Berbank Green, “One Button Games,”\n[Gamasutra.com](http://Gamasutra.com), June 2, 2005,\n<https://web.archive.org/web/20050822041906/http://www.gamasutra.com/features/20050602/green_01.shtml>.\n67 <http://web.archive.org/web/20091204142734/http://www.kokoromi.org/gamma4>.\n68 <http://adamatomic.com/canabalt/>. 69\n<http://classes.dma.ucla.edu/Winter09/28/exercises/>. 70\n<http://gamedesign.molleindustria.org/2010/>. 71 Maeda, _Design by Numbers_ ,\n204. 72 Maeda, _Design by Numbers_ , 207. 73 Maeda, _Design by Numbers_ , 213.\n74 Maeda, _Creative Code_ , 106. 75 See events like the Prism Breakup\nconference and exhibition at New York's Eyebeam Center in 2013\n(<https://www.eyebeam.org/events/prism-break-up/>) as well as numerous\ncryptoparties (<https://www.cryptoparty.in/>) and Art Hack Days\n(<http://arthackday.net/>) that brought together artists and technologists. 76\nSee Brian Winkel, “Lessons Learned from a Mathematical Cryptology Course,”\n_Cryptologia_ 32, no. 1 (January 2008): 45–55; Neal Koblitz “Cryptography as a\nTeaching Tool,” _Cryptologia_ 21, no. 4 (June 1997): 317–326; Manmohan Kaur,\n“Cryptography as a Pedagogical Tool,” _Primus_ 18, no. 2 (March 2008):\n198–206; Lorelei Koss, “Writing and Information Literacy in a Cryptology\nFirst-Year Seminar,” _Cryptologia_ 38, no. 3 (June 2014): 223–231. 77\n<https://experiments.withgoogle.com/collection/voice>. 78\n<https://nicolehe.github.io/schedule>, last modified October 23, 2018. 79\n<https://medium.com/@nicolehe/fifteen-unconventional-uses-of-voice-technology-\nfa1b749c14bf>. 80 Jer Thorp, syllabus for Data Art (NYU ITP, Spring 2016),\n<https://github.com/blprnt/dataart2017a>. 81 Sam Lavigne, syllabus for\nScrapism (NYU ITP, Fall 2018), <https://github.com/antiboredom/sfpc-scrapism>.\n82 Golan Levin, syllabus for Electronic Media Studio 2 (CMU, Fall 2013),\n<http://cmuems.com/2013/a/assignments/assignment-08/>, last modified December\n2013. 83 Casey Reas, syllabus for Interactive Environments (UCLA, Winter\n2004), <http://classes.design.ucla.edu/Winter04/256/exercises.html#B>. 84\nCasey Reas, syllabus for Interactive Environments (UCLA, Winter 2006),\n<http://classes.design.ucla.edu/Winter06/256/exercises.html#A>. 85 Lauren\nMcCarthy, syllabus for Interactivity (UCLA, Winter 2019),\n<http://classes.dma.ucla.edu/Winter19/28/#projects>. 86 Golan Levin, syllabus\nfor Audiovisual Systems and Machines (Parsons School of Design MFADT Program,\nSpring 2002,\n<https://web.archive.org/web/20020802181442/http://a.parsons.edu/~avsys/homework7/index.html>.\n\n\n# Authors and Contributors\n\n**Tega Brain**\n\n![b3-fig-5001.jpg](../images/b3-fig-5001.jpg)\n\nTega Brain is an Australian-born artist, environmental engineer, and educator.\nHer work examines issues of ecology, data systems, and infrastructure. Her\nwork has been shown in the Vienna Biennale for Change, the Guangzhou\nTriennial, and in institutions like the Haus der Kulturen der Welt and the New\nMuseum, among others. She is Assistant Professor of Integrated Digital Media\nat New York University (NYU) and works with the Processing Foundation on the\nLearning to Teach conference series and p5.js project.\n\n**Golan Levin**\n\n![b3-fig-5002.jpg](../images/b3-fig-5002.jpg)\n\nGolan Levin is Professor of Electronic Art at Carnegie Mellon University,\nwhere he also holds courtesy appointments in the School of Computer Science,\nthe School of Design, the School of Architecture, and the Entertainment\nTechnology Center. As an educator, Golan's pedagogy is concerned with\nreclaiming computation as a medium of personal expression. He teaches “studio\nart courses in computer science,” on themes like interactive art, generative\nform, and information visualization. Since 2009, Golan has also served as\nDirector of CMU's Frank-Ratchye STUDIO for Creative Inquiry, a laboratory for\natypical and anti-disciplinary research across the arts, science, technology,\nand culture.\n\n**Taeyoon Choi**\n\n![b3-fig-5003.jpg](../images/b3-fig-5003.jpg)\n\nTaeyoon Choi is an artist and educator based in New York City and Seoul. He is\nthe co-founder of the School for Poetic Computation and is a faculty\nresearcher at NYU's Interactive Telecommunications Program (ITP). Choi has\nextensive experience teaching art and technology to youth and communities\nthrough his Making Lab and Poetic Science Fair initiatives. He has held artist\nresidencies at Eyebeam Art and Technology Center, Lower Manhattan Cultural\nCouncil, the Frank-Ratchye STUDIO for Creative Inquiry at CMU, and the Art +\nTechnology Lab at the Los Angeles County Museum of Art. His collaboration with\nChristine Sun Kim was presented at the Whitney Museum of American Art.\n\n**Heather Dewey-Hagborg**\n\n![b3-fig-5004.jpg](../images/b3-fig-5004.jpg)\n\nHeather Dewey-Hagborg is a transdisciplinary artist and educator who is\ninterested in art as research and critical practice. She has shown work\ninternationally at events and venues including the World Economic Forum,\nShenzhen Urbanism and Architecture Biennale, the New Museum, and MoMA PS1. Her\nwork has been widely discussed in the media, from the _New York Times_ and the\nBBC to TED and _WIRED_. She is Visiting Assistant Professor of Interactive\nMedia at NYU Abu Dhabi and is co-founder of REFRESH, an inclusive and\npolitically engaged collaborative platform at the intersection of art,\nscience, and technology.\n\n**R. Luke DuBois**\n\n![b3-fig-5005.jpg](../images/b3-fig-5005.jpg)\n\nR. Luke DuBois is a composer, artist, and performer who explores the temporal,\nverbal, and visual structures of cultural and personal ephemera. He holds a\ndoctorate in music composition from Columbia University, and has lectured and\ntaught worldwide on interactive sound and video performance. An active visual\nand musical collaborator, DuBois is the co-author of Jitter, a software suite\nfor the real-time manipulation of matrix data developed by San Francisco-based\nsoftware company Cycling’74. DuBois is the director of the Brooklyn\nExperimental Media Center at the NYU Tandon School of Engineering, and is on\nthe Board of Directors of the ISSUE Project Room.\n\n**De Angela L. Duff**\n\n![b3-fig-5006.jpg](../images/b3-fig-5006.jpg)\n\nDe Angela L. Duff is Industry Professor at NYU Tandon School of Engineering\nand an Associate Vice Provost at NYU. Teaching in higher education since 1999,\nshe is passionate about educating students at the intersection of design, art,\nand technology. She was acknowledged for this passion by being awarded the NYU\nTandon School's 2018 Distinguished Teaching Award. Duff holds an MFA in Studio\nArt (Photography) from MiCA, a BFA in Graphic Design from Georgia State\nUniversity, and a BS in Textiles from Georgia Tech.\n\n**Minsun Eo**\n\n![b3-fig-5007.jpg](../images/b3-fig-5007.jpg)\n\nMinsun Eo is a professor of graphic design at the Maryland Institute College\nof Art (MICA), where he is a recipient of the Trustees Award for Excellence in\nTeaching; previously, Eo was adjunct professor at the City University of New\nYork (CUNY) Queens College. His New York-based design-research studio focuses\non practices that create integrated knowledge, systems, and experiences for\nthe art, technology, architecture, fashion, and education sectors. Eo holds an\nMFA in Graphic Design from Rhode Island School of Design and a BFA in Visual\nCommunication Design from Kookmin University, Seoul. Eo has worked at 2x4 New\nYork under the guidance of Michael Rock (2013–15) and is a member of the\nKorean Society of Typography (KST).\n\n**Zachary Lieberman**\n\n![b3-fig-5008.jpg](../images/b3-fig-5008.jpg)\n\nZachary Lieberman is an artist, researcher, educator, and hacker with a simple\ngoal: he wants you surprised. He creates performances and installations that\ntake human gesture as input and amplify it in different ways—making drawings\ncome to life, imagining what the voice would look like, transforming\nsilhouettes into music. He's been listed as one of Fast Company's Most\nCreative People, and his work has been awarded the Golden Nica from Ars\nElectronica and the Interactive Design of the Year award from Design Museum\nLondon. He creates artwork through writing software and is a co-creator of\nopenFrameworks, an open-source C++ toolkit for creative coding. Lieberman is\nco-founder of the School for Poetic Computation, a school examining the\nlyrical possibilities of code, and is also Adjunct Associate Professor of\nMedia Arts and Sciences at the MIT Media Laboratory, where he directs the\nFuture Sketches research group.\n\n**Rune Madsen**\n\n![b3-fig-5009.jpg](../images/b3-fig-5009.jpg)\n\nRune Madsen is a designer, artist, and educator who explores code as a design\nmaterial. As a co-founder of Design Systems International, a design studio\nthat explores systems in graphic design and digital media, he specializes in\nnon-trivial interfaces, brand systems, and custom design tools. He is the\nauthor of _Programming Design Systems_ , a free online book that teaches a\npractical introduction to the new foundations of graphic design. Rune has\npreviously worked for the _New York Times_ , O’Reilly Media, and as an\nAssistant Arts Professor at New York University Shanghai. Rune holds a BA from\nthe University of Copenhagen and a master's degree from NYU ITP.\n\n**Lauren Lee McCarthy**\n\n![b3-fig-5010.jpg](../images/b3-fig-5010.jpg)\n\nLauren Lee McCarthy is an Los Angeles-based artist examining social relationships in the midst of surveillance, automation, and algorithmic living. She is the creator of p5.js and Co-Director of the Processing Foundation. Lauren's work has been exhibited internationally, at places such as The Barbican Centre, Ars Electronica, Fotomuseum Winterthur, Haus der elektronischen Künste, SIGGRAPH, Onassis Cultural Center, IDFA DocLab, and Seoul Museum of Art. She has received numerous honors including a Creative Capital Award, a Sundance Fellowship, an Eyebeam Residency, and grants from the Knight Foundation, Mozilla Foundation, Google, and Rhizome. Lauren is Associate Professor at UCLA Design | Media Arts.\n\n**Allison Parrish**\n\n![b3-fig-5011.jpg](../images/b3-fig-5011.jpg)\n\nAllison Parrish is a computer programmer, poet, educator, and game designer\nwhose teaching and practice address the unusual phenomena that blossom when\nlanguage and computers meet, with a focus on artificial intelligence and\ncomputational creativity. She is Assistant Arts Professor at NYU ITP, where\nshe earned her master's degree in 2008. Named “Best Maker of Poetry Bots” by\n_The Village Voice_ in 2016, she is also the author of _@Everyword: The Book_\n(Instar, 2015), which collects the output of her popular long-term automated\nwriting project that tweeted every word in the English language—attracting\nover 100,000 followers along the way. Her first full-length book of computer-\ngenerated poetry, _Articulations_ , was published by Counterpath in 2018.\n\n**Phœnix Perry**\n\n![b3-fig-5012.jpg](../images/b3-fig-5012.jpg)\n\nPhœnix Perry creates embodied games and installations. Her work brings people\ntogether to explore their impact on each other and the environment. As an\nadvocate for women in game development, she founded the Code Liberation\nFoundation. Presently, she leads an MSc in Creative Computing at University of\nthe Arts London's Creative Coding Institute. Since 1996, she has exhibited in\na range of cultural venues and game events including Somerset House, Wellcome\nCollection, Lincoln Center, GDC, A Maze, and Indiecade. She owned Devotion\nGallery in Brooklyn, NY from 2009–2014. Devotion generated dialogue between\nart, technology, and scientific research.\n\n**Casey Reas**\n\n![b3-fig-5013.jpg](../images/b3-fig-5013.jpg)\n\nCasey Reas is a professor of Design Media Arts at the University of\nCalifornia, Los Angeles, where he is co-founder of the UCLA Arts Conditional\nStudio. Reas’ creative work builds upon concrete art, conceptual art,\nexperimental animation, and drawing; his projects range from generative prints\nto urban-scale installations, solo projects in studio to collaborations with\narchitects and musicians. With Ben Fry, Reas is renowned for his development\nof Processing, an open-source, flexible software sketchbook and language for\nlearning how to code within the context of the visual arts. Reas is also co-\nauthor of _Form+Code in Design, Art, and Architecture_ (Princeton\nArchitectural Press, 2010), a non-technical introduction to the history and\npractice of software in the visual arts, and _Processing: A Programming\nHandbook for Visual Designers and Artists_ (MIT Press, 2007/2014).\n\n**Daniel Shiffman**\n\n![b3-fig-5014.jpg](../images/b3-fig-5014.jpg)\n\nDaniel Shiffman is Associate Arts Professor at NYU ITP. In his YouTube\nchannel, _The Coding Train_ , he publishes tutorials with subjects ranging\nfrom the basics of programming languages to generative algorithms like physics\nsimulation, computer vision, and data visualization. Shiffman is a director of\nthe Processing Foundation and the author of _Learning Processing: A Beginner's\nGuide to Programming Images, Animation, and Interaction_ and _The Nature of\nCode: Simulating Natural Systems with Processing_ , an open-source book about\nsimulating natural phenomenon with code.\n\n**Kyuha (Q) Shim**\n\n![b3-fig-5015.jpg](../images/b3-fig-5015.jpg)\n\nKyuha (Q) Shim is a computational designer and researcher based in Pittsburgh\nand Seoul. He is Assistant Professor in the School of Design at Carnegie\nMellon University, where he is also the director of Type Lab. Prior to CMU, he\nhad worked as a researcher at Jan van Eyck Academie and MIT's SENSEable City\nLaboratory, and had been awarded residencies and fellowships at Frans Masereel\nCentrum and Facebook Analog Research Lab. His work has been exhibited\ninternationally, at the Cooper Hewitt, Smithsonian Design Museum; Museu\nNacional da República; National Museum of Modern and Contemporary Art, Korea;\nand ggg Gallery, Tokyo. He has also been featured in design festivals such as\nAGI Open, Beijing Design Week, and London Design Festival. Q is the editor of\n_GRAPHIC #37: Introduction to Computation_ (Propaganda, 2016) and is working\non a forthcoming book entitled _Computational Making in Graphic Design_.\n\n**Winnie Soon**\n\n![b3-fig-5016.jpg](../images/b3-fig-5016.jpg)\n\nDenmark-based, Hong Kong-born artist-researcher Winnie Soon is interested in\nthe cultural implications of technologies, specifically concerning internet\ncensorship, data politics, real-time processing/liveness, invisible\ninfrastructure, and the culture of code practice. Her current research focuses\non critical technical and feminist practice, and she is working on two\nforthcoming books entitled _Aesthetic Programming: A Handbook of Software\nStudies_ (with Geoff Cox) and _Fix My Code_ (with Cornelia Sollfrank). She is\nAssistant Professor at Aarhus University.\n\n**Tatsuo Sugimoto**\n\n![b3-fig-5017.jpg](../images/b3-fig-5017.jpg)\n\nTatsuo Sugimoto works across multiple fields, including information design,\nmedia art, and media studies. A member of the faculty in the Graduate School\nof Systems Design at Tokyo Metropolitan University, Sugimoto has participated\nin exhibitions such as Picture Book Museum and the Sapporo International Art\nFestival, and has won awards from the Japan Media Arts Festival and the\nExploratory IT Human Resources Project. He is co-author of the textbook\nHistory of Media Technology and a co-translator of the Japanese editions of\n_Processing: A Programming Handbook for Visual Designers and Artists_ and\n_Generative Design: Visualize, Program, and Create with Processing_.\n\n**Jer Thorp**\n\n![b3-fig-5018.jpg](../images/b3-fig-5018.jpg)\n\nJer Thorp is an artist, writer, and educator from Vancouver, Canada, currently\nliving in New York. Coming from a background in genetics, his digital art\npractice explores the many-folded boundaries between science, data, art, and\nculture. He is Adjunct Professor at NYU ITP and is the co-founder of The\nOffice for Creative Research. Jer was the _New York Times_ 's first Data\nArtist-in-Residence, and in 2017 and 2018 served as the Innovator in Residence\nat the Library of Congress. He is a National Geographic Explorer and a\nRockefeller Foundation Fellow. In 2015, _Canadian Geographic_ named Jer one of\nCanada's Greatest Explorers.\n\n\n# Notes on Computational Book Design\n\nKyuha Shim\n\nComputation is a primary medium in my design practice, so when Golan and Tega\ninvited me to design their book computationally, I was thrilled by both my\nrole in what would be the first publication of its kind by MIT Press and the\nprospect of demonstrating computational book design in the larger context of\ngraphic design. In my studio, I write programs to design dynamic visual\nformations, generate variable end results informed by data, and discover new\ncreative opportunities in graphic design. I also teach in the School of Design\nat Carnegie Mellon University, where I have developed courses that introduce\ncomputation as a creative medium. This project was as refreshing as it was\nchallenging, because it required new ways of working—both in my design process\nand in my collaboration with the authors.\n\nFor the design of the book, the functions handling data were built by Golan\nand Tega, and I made the graphic design decisions with the help of Minsun Eo,\nwho specializes in typography. In addition to designing visual systems, my\nmain role was writing code in Basil.js that intertwined the data and form.\nBasil.js enables designers to go back and forth between designing and\nprogramming, providing “a bridge between generating through code and adjusting\nby mouse.” 1 Thinking about how CSS provides the stylistic specifications in\nwebsite design, I used InDesign files to save and load the typographic styles\n(i.e., character and paragraph styles) and then used Basil.js code to\nconditionally apply them. This enabled me to work systematically by adjusting\ngenerated outcomes through character and paragraph styles. In building this\nworkflow, I needed to work with the rawer computational medium, which enabled\nme to overcome technical obstacles and boundaries set by software.\n\nWhile Golan and Tega worked on their content, sharing it as markdown files and\nparsers on GitHub, I was able to simultaneously build the algorithms\ngenerating the book with data pulled from the online repository. We were all\nable to see and reflect on the designs rendered with the most up-to-date\ncontent and work side by side in real time, which would be unthinkable in a\ntypical publication process—usually book designers only begin work once the\nauthors have handed over the finalized content. What I realized during this\nproject is that the programming aspect of my practice is about generating not\nonly a creative end result, but also a bespoke process. While this book was\npredominantly designed with code, there were certain tasks that I completed by\nhand—such as handling orphans and widows—since my intent in using computation\nwas to make my design process more intelligent rather than to solve\nprogramming problems.\n\nWhen thinking about the larger implications of using computation in this\nproject, I recall a conversation I had with graphic design educator and\ntheorist Ellen Lupton. She speculated that making templates will be one of\ngraphic designers’ primary roles in the future.2 But what if more designers\nuse code to write their own systems? Instead of a scenario in which designers’\nroles are diminished, the computational medium would extend and enhance their\nroles. As computation shifts emphasis from crafting an individual form to\nbuilding the method from which forms emerge, programming is more than ever\nrelevant to design innovation.\n\n## Notes\n\n1 Ludwig Zeller, Benedikt Groß, and Ted Davis, “basil.js – Bridging Mouse and\nCode Based Design Strategies,” in _Proceedings of the Third International\nConference, DUXU 2014_ , ed. Aaron Marcus (Cham, Switzerland: Springer, 2014):\n686–696. 2 Ellen Lupton, “Conversation: Ellen Lupton,” _GRAPHIC_ 37 (2016):\n158–167.\n\n\n# Acknowledgments\n\nIt's humbling to think we began this project eight years ago, intending to\nself-publish it as a short ‘zine or guidebook. As we compiled assignments,\nreferences, and exercises, it grew into what you hold in your hands. Like most\nendeavors, it has been made possible by the generous efforts of so many\npeople—more than we can name here. Many of our colleagues and peers have made\ndirect contributions, and many more have lit our path with their work in the\nfield.\n\nWe are indebted to Casey Reas for writing the Foreword. His unparalleled\nperspective on the field emerges from his tremendously important work creating\nProcessing with Ben Fry, as well as from his teaching and art making. We also\nwish to acknowledge Casey, Ben, and Golan's mentor John Maeda, whose\nfoundational work and influential guidance still shape the field today. Along\nwith John, we are grateful to Christiane Paul, Ellen Lupton, and Chris\nColeman, whose panoptic understandings of new media arts and design gives such\ngravity to their kind endorsements.\n\nThank you to our book production team: our copy editor and savior Shannon Fry,\nwhose wizardry is woven through so many of the publications on our shelves,\nand our designers Kyuha (Q) Shim and Minsun Eo, who have pulled off a unique\nand remarkable achievement in using computational techniques to design this\nbook. These are gorgeous pages. At MIT Press, we are grateful to editors Doug\nSery, Noah Springer, and Gita Manaktala, and to designers Yasuyo Iguchi and\nEmily Gutheinz, and production coordinator Jay McNair, for their stewardship\nand accommodation of this unusual project.\n\nThank you to all of our interviewees—Taeyoon Choi, Heather Dewey-Hagborg, Luke\nDuBois, De Angela Duff, Zach Lieberman, Rune Madsen, Lauren McCarthy, Allison\nParrish, Phœnix Perry, Dan Shiffman, Winnie Soon, Tatsuo Sugimoto, and Jer\nThorp—for generously sharing their time, perspectives, and energies. Thanks\nalso to our colleagues who provided material and gave feedback and advice on\nour manuscript: Daniel Cardoso-Llach, Matt Deslauriers, Benedikt Groß, Jon\nIppolito, Sam Lavigne, Joel Gethin Lewis, Ramsey Nasser, Allison Parrish,\nPaolo Pedercini, Caroline Record, Tom White, and our anonymous reviewers. We\nsincerely appreciate the many artists, designers, researchers, and former\nstudents who graciously gave their permissions for us to share their works in\nthese pages.\n\nSo many of the projects featured in this book were nurtured and made possible\nby creative coding communities and the open-source toolkits they develop. We\nexpress deep gratitude to the Processing Foundation, the Processing community,\nand the p5.js community. Lauren McCarthy, Dorothy Santos, and Johanna Hedva,\nyour commitment to diversity, access, and community building has set the\nstandard for us all. Thanks also to Zach Lieberman, Theo Watson, Arturo\nCastro, Kyle McDonald, the members of the openFrameworks community, and to the\nmany other open-source contributors who spend late nights tending to bugs,\nissues, and the maintenance of the creative coding toolkits that make this\nfield possible. Thanks also to our many teaching role models in these\ncommunities: Dan Shiffman, for his profound public contributions as a new\nmedia arts educator; Taeyoon Choi and the School for Poetic Computation, for\nsharing a practice of institution building in its most compassionate form; and\nChris Coleman of the University of Denver, whose new Clinic for Open Source\nArts holds great promise for the continued support of our field.\n\nThis book was conceived at the 2013 Eyeo Festival Code+Education Summit. We\nwish to thank the directors of the Eyeo Festival—Jer Thorp, Dave Schroeder,\nWes Grubbs, and Caitlin Rae Hargarten—for creating the conditions for this\nexchange and for gathering, galvanizing, and supporting our creative\ncommunity. Similarly, we thank Filip Visnjic, former curator of the Resonate\nFestival and co-curator, with Greg J. Smith, of the\n[CreativeApplications.net](http://CreativeApplications.net) blog, which has\nprovided such a tremendously valuable platform to bring the efforts of our\ncommunity to a broader public.\n\nThis project was funded in part by ArtWorks grant #1855045–34–19 from the\nMedia Arts Program of the National Endowment for the Arts. We thank the NEA\nand its reviewers, and especially Media Arts Director Jax Deluca for her\nleadership in sharing these precious public funds. This book was also\nsupported by a grant from the Frank-Ratchye Fund for Art at the Frontier,\nadministered by the Frank-Ratchye STUDIO for Creative Inquiry at Carnegie\nMellon University; we express our deep gratitude to Edward H. Frank and Sarah\nRatchye for their generosity. Additional support for this book was provided\nthrough graduate assistantships from the Integrative Digital Media program at\nthe Tandon School of Engineering, New York University.\n\nThis book was realized through the logistical and administrative support of\nmany dedicated staff at CMU's Frank-Ratchye STUDIO for Creative Inquiry:\nThomas Hughes, Linda Hager, Carol Hernandez, and Bill Rodgers. We are also\nindebted to the staff of the CMU College of Fine Arts Sponsored Projects\nOffice, Jenn Joy Wilson and January Johnson, for their assistance in\nbureaucratic wayfinding. We thank our student research assistants both at CMU\nand NYU: Sarah Keeling, who tirelessly compiled thousands of assignments from\naround the Web, and Najma Dawood-McCarthy, Chloé Desaulles, Cassidy Haney,\nAndrew Lau, Tatyana Mustakos, Cassie Scheirer, Xinyi (Joyce) Wang, and T.\nJames Yurek, who helped us with many critical tasks including preparing and\nporting code samples, formatting citations, and securing image permissions.\n\nWe acknowledge that we are following in the footsteps of our peers, the fellow\neducators who have written excellent books on computer art and design\neducation. These include John Maeda; Casey Reas and Ben Fry; Daniel Shiffman;\nLauren McCarthy; Andrew Blauvelt and Koert van Mensvoort; Greg Borenstein;\nAndrew Glassner; Nikolaus Gradwohl; Ira Greenberg; Benedikt Groß, Hartmut\nBohnacker, Julia Laub, and Claudius Lazzeroni; Carl Lostritto; Rune Madsen;\nNick Montfort; Joshua Noble; Kostas Terzidis; Jan Vantomme; Mitchell Whitelaw;\nMark Wilson; and Chandler McWilliams. Many of the exercises in this book were\nadapted from projects posted to the p5.js Web Editor and public online\nclassrooms on the [OpenProcessing.org](http://OpenProcessing.org) repository,\nand so we are especially grateful to Cassie Tarakajian and Sinan Ascioglu,\nrespectively, for creating and maintaining these critical community resources.\n\nThanks also to the many educators, students, and friends—a wider set of\npeers—who, knowingly or otherwise, contributed their creative energies to our\nvolume, including: Alba G. Corral, Andreas Koller, Art Simon, Arthur Violy,\nBarton Poulson, Bea Alvarez, Ben Chun, Ben Norskov, Brian Lucid, Caitlin\nMorris, Caroline KZ, Cedric Kiefer, Charlotte Stiles, Chris Sugrue, Chris G.\nTodd, Christophe Lemaitre, Christopher Warnow, Claire Hentschker, Clement\nValla, Connie Ye, Felix Worseck, Florian Jenett, Francisco Zamorano, Gabriel\nDunne, Gene Kogan, Herbert Spencer, Hyeoncheol Kim, Isaac Muro, Joan Roca\nGipuzkoa, Jeremy Rotsztain, Jim Roberts, Joey K. Lee, John Simon, Juan Patino,\nJuseung Stephen Lee, Kasper Kamperman, Kate Hollenbach, Kenneth Roraback, Lali\nBarriere, Lingdong Huang, Luiz Ernesto Merkle, Manolo Gamboa Naon, Marius\nWatz, Marty Altman, Matt Richard, Michael Kontopoulos, Monica Monin, Nick Fox-\nGieg, Nick Senske, Nidhi Malhotra, Ozge Samanci, Paul Ruvolo, Pinar Yoldas,\nRose Marshack, Ryan D’Orazi, Seb Lee-Delisle, Sheng-Fen Nik Chien, Stanislav\nRoudavski, Steffen Fiedler, Steffen Klaue, Tami Evnin, Thomas O. Fredericks,\nand Winterstein / Riekoff.\n\nFinally, we thank our families, partners, and close friends, especially Andrea\nBoykowycz and Sam Lavigne, who have supported us with their feedback,\nencouragement, and unwavering patience throughout this seemingly never-ending\nproject.\n\n\n# Related Resources\n\nThis book is a handbook for teaching and learning computational art and design\nand there are many related texts that complement the topics we cover. This\nlist includes some that we have found to be particularly helpful.\n\n## Compendia of Art Assignments\n\n  1. Bayerdörfer, Mirjam, and Rosalie Schweiker, eds. _Teaching for People Who Prefer Not to Teach._ London: AND Publishing, 2018.\n  2. Blauvelt, Andrew, and Koert van Mensvoort. _Conditional Design: Workbook._ Amsterdam: Valiz, 2013.\n  3. Cardoso Llach, Daniel. _Exploring Algorithmic Tectonics: A Course on Creative Computing in Architecture and Design._ State College, PA: The Design Ecologies Laboratory at the Stuckeman Center for Design Computing, The Pennsylvania State University College of Arts and Architecture, 2015.\n  4. Fulford, Jason, and Gregory Halpern, eds. _The Photographer's Playbook: 307 Assignments and Ideas._ New York: Aperture, 2014.\n  5. Garfinkel, Harold, Daniel Birnbaum, Hans Ulrich Obrist, and Lee Lozano et al. _Do It._ St. Louis, MO: Turtleback, 2005.\n  6. Heijnen, Emiel, and Melissa Bremmer, eds. _Wicked Arts Assignments: Practising Creativity in Contemporary Arts Education._ Amsterdam: Valiz, 2020.\n  7. Johnson, Jason S., and Joshua Vermillion, eds. _Digital Design Exercises for Architecture Students._ London: Routledge, 2016.\n  8. Ono, Yoko. _Grapefruit._ London: Simon & Schuster, 2000.\n  9. Paim, Nina, Emilia Bergmark, and Corinne Gisel. _Taking a Line for a Walk: Assignments in Design Education._ Leipzig, Germany: Spector, 2016.\n  10. Petrovich, Dushko, and Roger White. _Draw It with Your Eyes Closed: The Art of the Art Assignment._ Paper Monument, 2012.\n  11. Smith, Keri. _How to Be an Explorer of the World: Portable Life Museum._ New York: Penguin Books, 2008.\n\n## Computational Art and Design History\n\n  1. Allahyari, Morehshin, and Daniel Rourke. _The 3D Additivist Cookbook._ Amsterdam: The Institute of Network Cultures, 2017. Accessed July 20, 2020. <https://additivism.org/cookbook>.\n  2. Armstrong, Helen, ed. _Digital Design Theory: Readings from the Field._ New York: Princeton Architectural Press, 2016.\n  3. Cornell, Lauren, and Ed Halter, eds. _Mass Effect: Art and the Internet in the Twenty-First Century._ Vol. 1. Cambridge, MA: MIT Press, 2015.\n  4. Freyer, Conny, Sebastien Noel, and Eva Rucki. _Digital by Design: Crafting Technology for Products and Environments._ London: Thames & Hudson, 2008.\n  5. Hoy, Meredith. _From Point to Pixel: A Genealogy of Digital Aesthetics._ Hanover, NH: Dartmouth College Press, 2017.\n  6. Klanten, Robert, Sven Ehmann, and Lukas Feireiss, eds. _A Touch of Code: Interactive Installations and Experiences._ New York: Die Gestalten Verlag, 2011.\n  7. Kwastek, Katja. _Aesthetics of Interaction in Digital Art._ Cambridge, MA: MIT Press, 2013.\n  8. Montfort, Nick, Patsy Baudoin, John Bell, Ian Bogost, Jeremy Douglass, Mark C. Marino, Michael Mateas, Casey Reas, Mark Sample, and Noah Vawter. _10 PRINT CHR $(205.5+ RND (1));: GOTO 10._ Cambridge, MA: MIT Press, 2012.\n  9. Paul, Christiane. _A Companion to Digital Art._ Hoboken, NJ: John Wiley & Sons, 2016.\n  10. Paul, Christiane. _Digital Art (World of Art)._ London: Thames & Hudson, 2015.\n  11. Plant, Sadie. _Zeros and Ones: Digital Women and the New Technoculture._ London: Fourth Estate, 1998.\n  12. Reas, Casey, and Chandler McWilliams. _Form+Code in Design, Art, and Architecture._ New York: Princeton Architectural Press, 2011.\n  13. Rosner, Daniela K. _Critical Fabulations: Reworking the Methods and Margins of Design._ Cambridge, MA: MIT Press, 2018.\n  14. Shanken, Edward A. _Art and Electronic Media._ London: Phaidon Press, 2009.\n  15. Taylor, Grant D. _When the Machine Made Art: The Troubled History of Computer Art._ New York: Bloomsbury Academic, 2014.\n  16. Tribe, Mark, Reena Jana, and Uta Grosenick. _New Media Art._ Los Angeles: Taschen, 2006.\n  17. Whitelaw, Mitchell. _Metacreation: Art and Artificial Life._ Cambridge, MA: MIT Press, 2004.\n\n## Art and Design Pedagogy\n\n  1. Barry, Lynda. _Syllabus: Notes from an Accidental Professor._ Montreal: Drawn & Quarterly, 2014.\n  2. Davis, Meredith. _Teaching Design: A Guide to Curriculum and Pedagogy for College Design Faculty and Teachers Who Use Design in Their Classrooms._ New York: Simon and Schuster, 2017.\n  3. Itten, Johannes. _Design and Form: The Basic Course at the Bauhaus and Later._ New York: Van Nostrand Reinhold Company, 1975.\n  4. Jaffe, Nick, Becca Barniskis, and Barbara Hackett Cox. _Teaching Artist Handbook, Volume One: Tools, Techniques, and Ideas to Help Any Artist Teach._ Chicago: University of Chicago Press, 2015.\n  5. Klee, Paul, and Sibyl Moholy-Nagy. _Pedagogical Sketchbook._ London: Faber & Faber, 1953.\n  6. Lostritto, Carl. _Computational Drawing: From Foundational Exercises to Theories of Representation._ San Francisco: ORO Editions / Applied Research + Design, 2019.\n  7. Lupton, Ellen. _The ABC's of Triangle, Circle, Square: The Bauhaus and Design Theory._ New York: Princeton Architectural Press, 2019.\n  8. Schlemmer, Oskar. _Man: Teaching Notes from the Bauhaus._ Cambridge, MA: MIT Press, 1971.\n  9. Tufte, Edward R. _The Visual Display of Quantitative Information_. Cheshire, CT: Graphics Press, 2001.\n  10. Wong, Wucius. _Principles of Two-Dimensional Design_. New York: John Wiley & Sons, 1972.\n\n## Handbooks for Computational Art and Design\n\n  1. Bohnacker, Hartmut, Benedikt Groß, and Julia Laub. _Generative Design: Visualize, Program, and Create with JavaScript in p5.js._ Ed. Claudius Lazzeroni. New York: Princeton Architectural Press, 2018.\n  2. De Byl, Penny. _Creating Procedural Artworks with Processing: A Holistic Guide._ CreateSpace Independent, 2017.\n  3. Fry, Ben. _Visualizing Data: Exploring and Explaining Data with the Processing Environment._ Sebastopol, CA: O’Reilly Media, Inc., 2008.\n  4. Gonzalez-Vivo, Patricio, and Jennifer Lowe. _The Book of Shaders._ Last modified 2015. <https://thebookofshaders.com/>.\n  5. Greenberg, Ira. _Processing: Creative Coding and Computational Art._ New York: Apress, 2007.\n  6. Igoe, Tom. _Making Things Talk: Practical Methods for Connecting Physical Objects._ Cambridge, MA: O’Reilly Media, Inc., 2007.\n  7. Madsen, Rune. _Programming Design Systems._ Accessed July 20, 2020. <https://programmingdesignsystems.com/>.\n  8. Maeda, John. _Design by Numbers._ Cambridge, MA: MIT Press, 2001.\n  9. McCarthy, Lauren, Casey Reas, and Ben Fry. _Getting Started with P5.js: Making Interactive Graphics in JavaScript and Processing._ San Francisco: Maker Media, Inc., 2015.\n  10. Montfort, Nick. _Exploratory Programming for the Arts and Humanities._ Cambridge, MA: MIT Press, 2016.\n  11. Murray, Scott. _Creative Coding and Data Visualization with p5.js: Drawing on the Web with JavaScript._ Sebastopol, CA: O’Reilly Media, Inc., 2017.\n  12. Parrish, Allison, Ben Fry, and Casey Reas. _Getting Started with Processing.py: Making Interactive Graphics with Processing's Python Mode._ San Francisco: Maker Media, Inc., 2016.\n  13. Petzold, Charles. _Code: The Hidden Language of Computer Hardware and Software._ Redmond, WA: Microsoft Press, 2000.\n  14. Reas, Casey, and Ben Fry. _Processing: A Programming Handbook for Visual Designers and Artists._ Cambridge, MA: MIT Press, 2014.\n  15. Shiffman, Daniel. _Learning Processing: A Beginner's Guide to Programming Images, Animation, and Interaction._ San Francisco: Morgan Kaufmann, 2009.\n  16. Shiffman, Daniel. _The Nature of Code: Simulating Natural Systems with Processing._ 2012.\n  17. Wilson, Mark. _Drawing with Computers._ New York: Perigee Books, 1985.\n\n## Commentary on Computational Culture\n\n  1. Baudrillard, Jean. _Simulacra and Simulation._ Ann Arbor: University of Michigan Press, 1994.\n  2. Benjamin, Ruha. _Race after Technology: Abolitionist Tools for the New Jim Code._ New York: Wiley, 2019.\n  3. Bridle, James. _New Dark Age: Technology and the End of the Future._ London: Verso, 2018.\n  4. Browne, Simone. _Dark Matters: On the Surveillance of Blackness._ Durham, NC: Duke University Press, 2015.\n  5. Cox, Geoff, and Alex McLean. _Speaking Code: Coding as Aesthetic and Political Expression._ Cambridge, MA: MIT Press, 2013.\n  6. D’Ignazio, Catherine, and Lauren F. Klein. _Data Feminism._ Cambridge, MA: MIT Press, 2020.\n  7. Haraway, Donna J. _Simians, Cyborgs and Women: The Reinvention of Nature._ New York: Routledge, 1991: 149–181.\n  8. Hayles, N. Katherine. _How We Became Posthuman: Virtual Bodies in Cybernetics, Literature, and Informatics._ Chicago: University of Chicago Press, 1999.\n  9. Kane, Carolyn L. _Chromatic Algorithms: Synthetic Color, Computer Art, and Aesthetics after Code._ Chicago: University of Chicago Press, 2014.\n  10. McNeil, Joanne. _Lurking: How a Person Became a User._ New York: Macmillan, 2020.\n  11. Odell, Jenny. _How to Do Nothing: Resisting the Attention Economy._ London: Melville House, 2019.\n  12. Quaranta, Domenico. _In Your Computer._ LINK Editions, 2011.\n  13. Steyerl, Hito. _Duty Free Art: Art in the Age of Planetary Civil War._ London: Verso, 2017.\n\n\n# Illustration Credits\n\nUnless otherwise noted, all images are courtesy of the artist(s).\n\n## Iterative Pattern\n\n  1. 1\\. Todo. _Spamghetto_. 2010. Wall coverings with computer-generated patterns. <https://flickr.com/photos/todotoit/albums/72157616412434905>.\n  2. 2\\. Pólya, Georg. “Über die Analogie der Kristallsymmetrie in der Ebene.” _Zeitschrift für Kristallographie_ 60 (1924): 278–282.\n  3. 3\\. Alexander, Ian. “Ceramic Tile Tessellations in Marrakech.” 2001. Ceramic tile. <https://en.wikipedia.org/wiki/Tessellation#/media/File:Ceramic_Tile_Tessellations_in_Marrakech.jpg>.\n  4. 4\\. Reas, Casey. _One Non-Narcotic Pill A Day_. 2013. Print, 27 x 48”. <https://paddle8.com/work/casey-reas/27050-one-non-narcotic-pill-a-day>.\n  5. 5\\. Gondek, Alison. _Wallpaper_. 2015. Generative wallpaper design. <http://cmuems.com/2015c/deliverables/deliverables-03/project-03-staff-picks/>.\n  6. 6\\. Molnár, Vera. _Untitled_. 1974. Computer drawing, 51.5 x 36 cm. Courtesy of the Mayor Gallery, London. <https://www.mayorgallery.com/artists/190-vera-molnar/works/10579>. Courtesy of The Mayor Gallery, London.\n  7. 7\\. Buechley, Leah. _Curtain (Computational Design)_. 2017. Lasercut wool felt. <https://handandmachine.cs.unm.edu/index.php/2019/12/02/computational-design/>.\n\n## Face Generator\n\n  1. 8\\. Dörfelt, Matthias. _Weird Faces_. 2012. Archival digital print on paper. <http://www.mokafolio.de/works/Weird-Faces>.\n  2. 9\\. Dewey-Hagborg, Heather. _Stranger Visions_. 2012. 3D-printed full-color portraits. <http://deweyhagborg.com/projects/stranger-visions>.\n  3. 10\\. Chernoff, Herman. “The Use of Faces to Represent Points in K-Dimensional Space Graphically.” _Journal of the American Statistical Association_ 68, no. 342 (June 1973): 361–368. <http://doi.org/b42z6k>.\n  4. 11\\. Compton, Kate. _Evolving Faces with User Input_. 2009. Interactive software. <https://vimeo.com/111667058>.\n  5. 12\\. Pelletier, Mike. _Parametric Expression_. 2013. Video loops. <http://mikepelletier.nl/Parametric-Expression>.\n  6. 13\\. Sobecka, Karolina. _All the Universe Is Full of the Lives of Perfect Creatures_. 2012. Interactive mirror. <http://cargocollective.com/karolinasobecka/All-The-Universe-is-Full-of-The-Lives-of-Perfect-Creatures>.\n  7. 14\\. National Safety Council, Energy BBDO, MssngPeces, Tucker Walsh, Hyphen-Labs, RMI, and Rodrigo Aguirre. _Prescribed to Death_. 2018. Installation wall with machine-carved pills. <http://www.hyphen-labs.com/nsc.html>. Courtesy of National Safety Council and U.S. Justice Department.\n\n## Clock\n\n  1. 15\\. Byron, Lee. _Center Clock_. 2007. Abstract generative clock. <http://leebyron.com/centerclock>.\n  2. 16\\. Ängeslevä, Jussi, and Ross Cooper. _Last Clock_. 2002. Interactive slit-scan clock. <https://lastclock.net>.\n  3. 17\\. Levin, Golan. _Banded Clock_. 1999. Abstract clock. <http://www.flong.com/projects/clock>.\n  4. 18\\. Puckey, Jonathan, and Studio Moniker. _All the Minutes_. 2014. Twitter bot. <https://twitter.com/alltheminutes>.\n  5. 19\\. Formanek, Mark. _Standard Time_. 2003. Video, 24:00:00. <http://www.standard-time.com/index_en.php>.\n  6. 20\\. Diaz, Oscar. _Ink Calendar_. 2009. Paper and ink bottle, 420 x 595 mm. <http://www.oscar-diaz.net/project/inkcalendar>.\n\n## Generative Landscape\n\n  1. 21\\. Brown, Daniel. _Dantilon: The Brutal Deluxe_ , from the series _Travelling by Numbers. Generative Architecture_. 2016. Collection of digital renderings. <http://flic.kr/s/aHskyNR2Tz>.\n  2. 22\\. Solie, Kristyn Janae. _Lonely Planets_. 2013. Stylized 3D terrain. <https://www.instagram.com/kyttenjanae>.\n  3. 23\\. Mandelbrot, Benoît B. _The Fractal Geometry of Nature_. San Francisco: W. H. Freeman, 1982. Image courtesy of Richard F. Voss.\n  4. 24\\. Pipkin, Everest. _Mirror Lake_. 2015. Virtual landscape generator. <https://everestpipkin.itch.io/mirrorlake>.\n  5. 25\\. Tarbell, Jared. _Substrate_. 2003. Virtual landscape generator. <http://www.complexification.net/gallery/machines/substrate>.\n\n## Virtual Creature\n\n  1. 26\\. Watanabe, Brent. _San Andreas Streaming Deer Cam_. 2015–2016. Live video stream of modified game software. <http://bwatanabe.com/GTA_V_WanderingDeer.html>.\n  2. 27\\. Design IO. _Connected Worlds_. 2015. Large-scale interactive projection installation. New York: Great Hall of Science. <http://design-io.com/projects/ConnectedWorlds>.\n  3. 28\\. Walter, William Grey. _Machina Speculatrix_. 1948–1949. Context-responsive wheeled robots. <http://cyberneticzoo.com/cyberneticanimals/w-grey-walter-and-his-tortoises>.\n  4. 29\\. Sims, Karl. _Evolved Virtual Creatures_. 1994. Animated simulated evolution of block creatures. <https://archive.org/details/sims_evolved_virtual_creatures_1994>.\n\n## Custom Pixel\n\n  1. 30\\. Bartholl, Aram. _0,16_. 2009. Light installation, 530 x 280 x 35 cm. <https://arambartholl.com/016>.\n  2. 31\\. Albers, Anni. _South of The Border_. 1958. Cotton and wool weaving, 4 1/8 x 15 1/4”. Baltimore Museum of Art. <https://albersfoundation.org/art/anni-albers/weavings/#slide15>. Image courtesy of Albers Foundation.\n  3. 32\\. Harmon, Leon, and Ken Knowlton. _Studies in Perception #1_. 1966. Print. <https://www.albrightknox.org/artworks/p20142-computer-nude-studies-perception-i>. Image used with permission of Nokia Corporation and AT&T Archives.\n  4. 33\\. Odell, Jenny. _Garbage Selfie_. 2014. Collage. <http://www.jennyodell.com/garbage.html>.\n  5. 34\\. Gaines, Charles. _Numbers and Trees: Central Park Series II: Tree #8, Amelia_. 2016. Black and white photograph, acrylic on plexiglass, 95 x 127 x 6”. <https://vielmetter.com/exhibitions/2016-10-charles-gaines-numbers-and-trees-central-park-series-ii>. Image courtesy of the artist and Hauser & Wirth.\n  6. 35\\. Koblin, Aaron, and Takashi Kawashima. _10,000 Cents_. 2008. Crowdsourced digital artwork. <http://www.aaronkoblin.com/project/10000-cents>.\n  7. 36\\. Rozin, Daniel. _Peg Mirror_. 2007. Interactive sculpture with wood dowels, motors, video camera, and control electronics. <https://www.smoothware.com/danny/pegmirror.html>. Image courtesy of bitforms gallery, New York.\n  8. 37\\. Blake, Scott. _Self-Portrait Made with Lucas Tiles_. 2012. Digital collage. <http://freechuckcloseart.com>.\n\n## Drawing Machine\n\n  1. 38\\. Wagenknecht, Addie. _Alone Together_. 2017–. Mechanically assisted paintings. <http://www.placesiveneverbeen.com/details/alonetogether>.\n  2. 39\\. Chung, Sougwen. _Drawing Operations_. 2015–. Robot-assisted drawings. <https://sougwen.com/project/drawing-operations>.\n  3. 40\\. Knowles, Tim. _Tree Drawings_. 2005. Tree-assisted drawings. <http://www.cabinetmagazine.org/issues/28/knowles.php>.\n  4. 41\\. Front Design. _Sketch Furniture_. 2007. Hand-sketched 3D-printed furniture. <http://www.frontdesign.se/sketch-furniture-performance-design-project>.\n  5. 42\\. Graffiti Research Lab (Evan Roth, James Powderly, Theo Watson et al.). _L.A.S.E.R. Tag_. 2007. System for projecting “graffiti.” <http://www.theowatson.com/site_docs/work.php?id=40>.\n  6. 43\\. Warren, Jonah. _Sloppy Forgeries_. 2018. Painting game. <https://playfulsystems.com/sloppy-forgeries>.\n  7. 44\\. Maire, Julien. _Digit_. 2006. Performance, writing printed text with fingers. <https://www.youtube.com/watch?v=IzDtVR0-0Es>.\n  8. 45\\. Haeberli, Paul. _DynaDraw_. 1989. Computational drawing environment. <http://www.graficaobscura.com/dyna>.\n\n## Modular Alphabet\n\n  1. 46\\. Pashenkov, Nikita. _Alphabot_. 2000. Interactive typographic system. <https://tokyotypedirectorsclub.org/en/award/2001_interactive>.\n  2. 47\\. Huang, Mary. _Typeface: A Typographic Photobooth_. 2010. Interactive type system that translates facial dimensions into type design. <https://mary-huang.com/projects/typeface/typeface.html>.\n  3. 48\\. Lu, David. _Letter 3_. 2002. Interactive typographic system.\n  4. 49\\. Cho, Peter. _Type Me, Type Me Not_. 1997. Interactive typographic system. <https://acg.media.mit.edu/people/pcho/typemenot/info.html>.\n  5. 50\\. Katsumoto, Yuichiro. _Mojigen & Sujigen_. 2016. Robotic typographic system. <http://www.katsumotoy.com/mojisuji>.\n  6. 51\\. Munari, Bruno. _ABC with Imagination_. 1960. Game with plastic letter-composing elements. Corraini Edizioni. <https://www.corraini.com/en/catalogo/scheda_libro/336/Abc-con-fantasia>. Courtesy of The Museum of Modern Art, New York. Digital Image © The Museum of Modern Art/Licensed by SCALA / Art Resource, NY.\n  7. 52\\. Soennecken, Friedrich. _Schriftsystem_. 1887. Modular type system. <http://luc.devroye.org/fonts-49000.html>.\n  8. 53\\. Devroye, Luc. _Fregio Mecano_. 1920s. Modular font. <http://luc.devroye.org/fonts-58232.html>.\n  9. 54\\. Popp, Julius. _bit.fall_. 2001–2016. Physical typographic installation. <https://www.youtube.com/watch?v=AICq53U3dl8>. Photograph: Rosa Menkman.\n\n## Data Self-Portrait\n\n  1. 55\\. Huang, Shan. _Favicon Diary_. 2014. Browser extension. <http://golancourses.net/2014/shan/03/06/project-3-shan-browser-history-visualization>.\n  2. 56\\. Lupi, Giorgia, and Stephanie Posavec. _Dear Data_. 2016. Analog data drawing project. <http://dear-data.com>.\n  3. 57\\. Viégas, Fernanda. _Themail_ (2006). Email visualization software. <https://web.archive.org/web/20111112164734/http://www.fernandaviegas.com/themail/>.\n  4. 58\\. Emin, Tracey. _Everyone I Have Ever Slept With 1963–1995_. 1995. Appliquéd tent, mattress, and light, 122 x 245 x 214 cm. <https://en.wikipedia.org/wiki/Everyone_I_Have_Ever_Slept_With_1963%E2%80%931995>. Image courtesy of Artists Rights Society, NY.\n  5. 59\\. Rapoport, Sonya. _Biorhythm_. 1981. Interactive computer-mediated participation performance. <http://www.sonyarapoport.org/portfolio/biorhythm>. Image courtesy of the Estate of Sonya Rapoport.\n  6. 60\\. Elahi, Hasan. _Stay_. 2011. C-print, 30 x 40”. <https://elahi.gmu.edu/elahi_stay.php>.\n\n## Augmented Projection\n\n  1. 61\\. Wodiczko, Krzysztof. _Warsaw Projection_. 2005. Public video projection. Zachęta National Gallery of Art, Warsaw. <http://www.art21.org/artists/krzysztof-wodiczko>. © Krzysztof Wodiczko. Photograph: Sebastian Madejski, Zachęta National Gallery of Art. Image courtesy of Galerie Lelong & Co., New York.\n  2. 62\\. Naimark, Michael. _Displacements_. 1980. Rotating projector in exhibition space. Art Center College of Design, Pasadena, CA. <http://http://www.naimark.net/projects/displacements.html>.\n  3. 63\\. McKay, Joe. _Sunset Solitaire_. 2007. Custom software. <http://www.joemckaystudio.com/sunset.php>.\n  4. 64\\. HeHe. _Nuage Vert_. 2008. Laser projection on vapor cloud. Salmisaari power plant, Helsinki. <https://vimeo.com/17350218>.\n  5. 65\\. Mayer, Jillian. _Scenic Jogging_. 2010. Video. The Solomon R. Guggenheim Museum, New York City. <https://youtu.be/uMq9Th3NgGk>. Image courtesy of David Castillo Gallery.\n  6. 66\\. Obermaier, Klaus, and Ars Electronica Futurelab. _Apparition_. 2004. <http://www.exile.at/apparition>.\n  7. 67\\. Peyton, Miles Hiroo. _Keyfleas_. 2013. Interactive augmented projection. Carnegie Mellon University, Pittsburgh. <https://vimeo.com/151334392>.\n  8. 68\\. Valbuena, Pablo. _Augmented Sculpture_. 2007. Virtual projection on physical base. Medialab Prado, Madrid. <http://www.pablovalbuena.com/augmented>. Courtesy of Ars Electronica.\n  9. 69\\. Sugrue, Christine. _Delicate Boundaries_. 2006. Interactive projection. Medialab Prado, Madrid. <http://csugrue.com/delicateboundaries>. Courtesy of the Science Gallery Dublin.\n  10. 70\\. Sobecka, Karolina. _Wildlife_. 2006. Public projection from car. ZeroOne ISEA2006, San Jose, CA. <http://cargocollective.com/karolinasobecka/filter/interactive-installation/Wildlife>.\n\n## One-Button Game\n\n  1. 71\\. Nguyen, Dong. _Flappy Bird._ 2013\\. Mobile game. <https://flappybird.io>.\n  2. 72\\. Rozendaal, Rafaël, and Dirk van Oosterbosch. _Finger Battle._ 2011\\. Mobile game. <https://www.newrafael.com/new-iphone-app-finger-battle>.\n  3. 73\\. Hummel, Benedikt, and Marius Winter (Major Bueno). _Moon Waltz._ 2016\\. Video game. <http://www.majorbueno.com/moon-waltz>.\n  4. 74\\. Rubock, Jonathan. _Nipple Golf._ 2016\\. Online game. <https://jrap.itch.io/obng>.\n  5. 75\\. Abe, Kaho. _Hit Me!_ 2011\\. Two-player physical game. <http://kahoabe.net/portfolio/hit-me>. Photograph: Shalin Scupham.\n  6. 76\\. Bieg, Kurt, and Ramsey Nasser. _Sword Fight._ 2012\\. Two-player physical game. <https://swordfightgame.tumblr.com>.\n\n## Bot\n\n  1. 77\\. Thompson, Jeff. _Art Assignment Bot_. 2013. Twitter bot. <https://twitter.com/artassignbot>.\n  2. 78\\. Parrish, Allison. _Ephemerides_. 2015. Twitter bot. <https://twitter.com/the_ephemerides>.\n  3. 79\\. Pipkin, Everest, and Loren Schmidt. _Moth Generator_. 2015. Twitter bot. <http://everest-pipkin.com/#projects/bots.html>.\n  4. 80\\. Kazemi, Darius. _Reverse OCR_. 2014. Tumblr bot. <http://reverseocr.tumblr.com>.\n  5. 81\\. !Mediengruppe Bitnik. _Random Darknet Shopper_. 2014. Bot. <https://bitnik.org/r>.\n  6. 82\\. Lavigne, Sam. _CSPAN 5_. 2015. YouTube bot. <https://twitter.com/CSPANFive>.\n\n## Collective Memory\n\n  1. 83\\. McDonald, Kyle. _Exhausting a Crowd_. 2015. Video with crowdsourced annotations. <https://www.exhaustingacrowd.com>.\n  2. 84\\. Klajban, Michal. _Rock Cairn at Cairn Sasunnaich, Scotland_. 2019. Photograph. <https://commons.wikimedia.org/wiki/File:Rock_cairn_at_Cairn_Sasunnaich>,_Scotland.jpg.\n  3. 85\\. Goldberg, Ken, and Santarromana, Joseph. _Telegarden_. 1995. Collaborative garden with industrial robot arm. Ars Electronica Museum, Linz, Austria. <http://ieor.berkeley.edu/~goldberg/garden/Ars>.\n  4. 86\\. Vasudevan, Roopa. _Sluts across America_. 2012. Digital map with user input. <http://www.slutsacrossamerica.org>.\n  5. 87\\. Bartholl, Aram. _Dead Drops_. 2010. <http://deaddrops.com/>.\n  6. 88\\. Studio Moniker. _Do Not Touch_. 2013. Interactive crowdsourced music video. <https://studiomoniker.com/projects/do-not-touch>.\n  7. 89\\. Davis, Kevan. _Typophile: The Smaller Picture._ 2002\\. Collaborative pixel art gallery. <https://kevan.org/smaller.cgi>.\n  8. 90\\. Reddit. _/r/place_. 2017. Crowdsourced pixel art. <https://reddit.com/r/place>.\n  9. 91\\. Asega, Salome, and Ayodamola Okunseinde. _Iyapo Repository_. 2015. <http://www.salome.zone/iyapo-repository>.\n\n## Experimental Chat\n\n  1. 92\\. Galloway, Kit, and Sherrie Rabinowitz. _Hole in Space_. 1980. Public communication sculpture. <http://y2u.be/SyIJJr6Ldg8>. Courtesy of the 18th St Arts Center.\n  2. 93\\. Lozano-Hemmer, Rafael. _The Trace_. 1995. Telepresence installation. <http://www.lozano-hemmer.com/the_trace.php>.\n  3. 94\\. Horvitz, David. _The Space Between Us_. 2015. App. <https://rhizome.org/editorial/2015/dec/09/space-between-us>.\n  4. 95\\. Snibbe, Scott. _Motion Phone_. 1995. Interactive software for abstract visual communication. <https://www.snibbe.com/projects/interactive/motionphone>.\n  5. 96\\. Varner, Maddy. _Poop Chat Pro_. 2016. Chatroom. <https://cargocollective.com/maddyv/POOPCHAT-PRO>. Photograph: Thomas Dunlap.\n  6. 97\\. Fong-Adwent, Jen, and Soledad Penadés. _Meatspace_. 2013. Ephemeral chatroom with animated GIFs. <https://chat.meatspac.es/>.\n  7. 98\\. Pedercini, Paolo. _Online Museum of Multiplayer Art_. 2020. Collection of chatrooms with interaction constraints. <https://likelike2.glitch.me/?room=firstFloor>.\n  8. 99\\. Artist, American. _Sandy Speaks_. 2017. Chat platform based on video archive. <https://americanartist.us/works/sandy-speaks>.\n\n## Browser Extension\n\n  1. 100\\. Hoff, Melanie. _Decodelia_. 2016. Browser extension. <https://melaniehoff.github.io/DECODELIA/>.\n  2. 101\\. Lund, Jonas. _We See in Every Direction_. 2013. Web browser. Mac OS X 10.7.5 or later. <http://ineverydirection.net/>\n  3. 102\\. Lambert, Steve. _Add Art_. 2008. Browser extension. <http://add-art.org/>\n  4. 103\\. Oliver, Julian, and Daniil (Danja) Vasiliev. _Newstweek_. 2011. Custom internet router. <https://julianoliver.com/output/newstweek>.\n  5. 104\\. McCarthy, Lauren, and Kyle McDonald. _Us+_. 2013. Google Hangout video chat app. <http://lauren-mccarthy.com/us>.\n\n## Creative Cryptography\n\n  1. 105\\. Wu, Amy Suo. _Thunderclap_. 2017. Steganographic zine. <http://amysuowu.net/content/thunderclap>.\n  2. 106\\. Sherman, William H. “How to Make Anything Signify Anything: William F. Friedman and the Birth of Modern Cryptanalysis.” _Cabinet_ 40 (Winter 2010–2011): n.p. <http://www.cabinetmagazine.org/issues/40/sherman.php>. Courtesy of New York Public Library.\n  3. 107\\. Varner, Maddy. _KARDASHIAN KRYPT_. 2014. Browser extension. <https://cargocollective.com/maddyv/KARDASHIAN-KRYPT>.\n  4. 108\\. Dörfelt, Matthias. _Block Bills_. 2017. Digital print on paper, 5.9 x 3.3”. <https://www.mokafolio.de/works/BlockBills>.\n  5. 109\\. Plummer-Fernández, Matthew. _Disarming Corruptor._ 2013\\. Encryption software. <https://www.plummerfernandez.com/works/disarming-corruptor>.\n  6. 110\\. Tremmel, Georg, and Shiho Fukuhara. _Biopresence_. 2005. Trees transcoded with human DNA. <https://bcl.io/project/biopresence>.\n  7. 111\\. Katchadourian, Nina. _Talking Popcorn_. 2001. Sound sculpture. <http://www.ninakatchadourian.com/languagetranslation/talkingpopcorn.php>. Courtesy of the artist, Catharine Clark Gallery, and Pace Gallery.\n  8. 112\\. Kenyon, Matt, and Douglas Easterly. _Notepad._ 2007\\. Microprinted ink on paper. <http://www.swamp.nu/projects/notepad>.\n\n## Voice Machine\n\n  1. 113\\. Leeson, Lynn Hershman. _DiNA, Artificial Intelligent Agent Installation_. 2002–2004. Interactive network-based multimedia installation. Civic Radar, ZKM Museum of Contemporary Art, Karlsruhe. <https://www.lynnhershman.com/project/artificial-intelligence>. Programming: Lynn Hershman Leeson and Colin Klingman. Courtesy of Yerba Buena Center for the Arts. Photograph: Charlie Villyard.\n  2. 114\\. Dinkins, Stephanie. _Conversations with Bina48_. 2014–. Ongoing effort to establish a social relationship with a robot built by Terasem Movement Foundation. <https://www.stephaniedinkins.com/conversations-with-bina48.html>.\n  3. 115\\. Everybody House Games. _Hey Robot_. 2019. A party game involving smart speakers. <https://everybodyhousegames.com/heyrobot.html>.\n  4. 116\\. Rokeby, David. _The Giver of Names_. 1990–. Computer system for naming objects. Kiasma Museum of Contemporary Art, Helsinki. <http://www.davidrokeby.com/gon.html>. Photograph: Tiffany Lam.\n  5. 117\\. Lev, Roi. _When Things Talk Back. An AR Experience._ 2018\\. Mobile augmented reality artificial intelligence app. <http://www.roilev.com/when-things-talk-back-an-ar-experience>.\n  6. 118\\. Lublin, David. _Game of Phones._ 2012–. Social game of telephone transcription. <http://www.davidlubl.in/game-of-phones>.\n  7. 119\\. Thapen, Neil. _Pink Trombone_. 2017. Interactive articulatory speech synthesizer. <https://experiments.withgoogle.com/pink-trombone>.\n  8. 120\\. Dobson, Kelly. _Blendie_. 2003–2004. Interactive voice-controlled blender. <https://web.media.mit.edu/~monster/blendie>.\n  9. 121\\. He, Nicole. _ENHANCE.COMPUTER_. 2018. Interactive speech-driven browser game. <https://www.enhance.computer>.\n\n## Measuring Device\n\n  1. 122\\. Jeremijenko, Natalie, and Kate Rich (Bureau of Inverse Technology). _Suicide Box_. 1996. Camera, video, and custom software. <http://www.bureauit.org/sbox>.\n  2. 123\\. Oliver, Julian, Bengt Sjölen, and Danja Vasiliev (Critical Engineering Working Group). _The Deep Sweep_. 2016. Weather balloon, embedded computer, and RF equipment. <https://criticalengineering.org/projects/deep-sweep>.\n  3. 124\\. Varner, Maddy. _This or That_. 2013. Interactive poster. <https://www.youtube.com/watch?v=HDWxq1v6A2k>.\n  4. 125\\. Ma, Michelle. _Revolving Games_. 2014. Location-based game and public intervention. <https://vimeo.com/83068752>.\n  5. 126\\. D’Ignazio, Catherine. _Babbling Brook_. 2014. Water sensor with voice interface. <http://www.kanarinka.com/project/the-babbling-brook/>.\n  6. 127\\. Sobecka, Karolina, and Christopher Baker. _Picture Sky_. 2018. Participatory photography event. <http://cargocollective.com/karolinasobecka/Picture-Sky-Zagreb>.\n  7. 128\\. Onuoha, Mimi. _Library of Missing Datasets_. 2016. Mixed media installation. <http://mimionuoha.com/the-library-of-missing-datasets>. Photograph: Brandon Schulman Photography.\n\n## Personal Prosthetic\n\n  1. 129\\. Clark, Lygia. _Dialogue Goggles_. 1968. Wearable device. <http://www.laboralcentrodearte.org/en/recursos/obras/dialogue-goggles-dialogo-oculos-1968>. Image courtesy Associação Cultural O Mundo de Lygia Clark.\n  2. 130\\. Sputniko!. _Menstruation Machine – Takashi's Take_. 2010. Installation with video and wearable device. Scai the Bathhouse, Tokyo. <https://sputniko.com/Menstruation-Machine>.\n  3. 131\\. Dobson, Kelly. _ScreamBody_. 1998–2004. Wearable device. MIT Media Lab, Cambridge. <http://web.media.mit.edu/~monster/screambody>. Photograph: Toshihiro Komatsu.\n  4. 132\\. Ross, Sarah. _Archisuits_. 2005–2006. Wearable soft sculpture. Los Angeles. <https://www.insecurespaces.net/archisuits.html>.\n  5. 133\\. Woebken, Chris, and Kenichi Okada. _Animal Superpowers_. 2008–2015. Series of wearable devices. <https://chriswoebken.com/Animal-Superpowers>. Photograph: Haeyoon Yoo.\n  6. 134\\. Hendren, Sara, and Caitrin Lynch. _Engineering at Home_. 2016. Documentation and discussion of adapted household implements. Olin College of Engineering, Needham, MA. <http://engineeringathome.org>.\n  7. 135\\. Montinar, Steven. 2019. _Entry Holes and Exit Wounds_. Performance with wearable electronics. Carnegie Mellon University, Pittsburgh. <https://www.youtube.com/watch?v=KBsTpQgyvhk>.\n  8. 136\\. McDermott, Kathleen. _Urban Armor #7: The Social Escape Dress_. 2016. Bio-responsive electromechanical garment. <http://www.kthartic.com/index.php?/class/urban-armor-7>.\n  9. 137\\. Okunseinde, Ayodamola. _The Rift: An Afronaut's Journey_. 2015. Afronaut suit. <http://www.ayo.io/rift.html>.\n\n## Parametric Object\n\n  1. 138\\. Desbiens Design Research. _Fahz_. 2015. System for rendering profiles as 3D-printed vases. <http://www.fahzface.com/>. Photograph: Nicholas Desbiens.\n  2. 139\\. Nervous System. _Kinematic Dress_. 2014. System for 3D-printing custom one-piece dresses. <https://n-e-r-v-o-u-s.com/projects/sets/kinematics-dress>. Photograph: Steve Marsel Studio.\n  3. 140\\. Eisenmann, Jonathan A. _Interactive Evolutionary Design with Region-of-Interest Selection for Spatiotemporal Ideation & Generation_. 2014. Ph.D. defense slides, Ohio State University. <https://slides.com/jeisenma/defense#> and <https://etd.ohiolink.edu/pg_10?0::NO:10:P10_ACCESSION_NUM:osu1405610355>.\n  4. 141\\. Epler, Matthew. _Grand Old Party_. 2013. Political polling data visualized as silicone butt plugs. <https://mepler.com/Grand-Old-Party>.\n  5. 142\\. Lia. _Filament Sculptures_. 2014. Computational and organically formed filament sculptures. <https://www.liaworks.com/theprojects/filament-sculptures>.\n  6. 143\\. Csuri, Charles A. _Numeric Milling_. 1968. Computational wood sculpture created with punch cards, an IBM 7094, and a 3-axis milling machine. <https://csuriproject.osu.edu/index.php/Detail/objects/769>.\n  7. 144\\. Ijeoma, Ekene. _Wage Islands_. 2015. Interactive installation and data visualization. New York. <https://studioijeoma.com/Wage-Islands>.\n  8. 145\\. Segal, Adrien. _Wildfire Progression Series_ , 2017. Data-driven sculpture. <https://www.adriensegal.com/wildfire-progression>.\n  9. 146\\. Ghassaei, Amanda. _3D Printed Record_. 2012. System for rendering audio files as 3D-printed 33RPM records. <https://www.instructables.com/id/3D-Printed-Record>.\n  10. 147\\. Binx, Rachel, and Sha Huang. _Meshu_. 2012. System for rendering geodata as 3D-printed accessories. <http://www.meshu.io/about>.\n  11. 148\\. Chung, Lisa Kori, and Kyle McDonald. _Open Fit Lab_. 2013. Performance producing custom-tailored pants for audience members. <http://openfitlab.com>.\n  12. 149\\. Allahyari, Morehshin. _Material Speculation: ISIS; King Uthal_. 2015–2016. 3D-printed resin and electronic components. 12 x 4 x 3.5” (30.5 x 10.2 x 8.9 cm). <http://www.morehshin.com/material-speculation-isis/>.\n\n## Virtual Public Sculpture\n\n  1. 150\\. Matsuda, Keiichi. _HYPER-REALITY_. 2016. Augmented reality futuristic cityscape. <http://km.cx/projects/hyper-reality>.\n  2. 151\\. Shaw, Jeffrey. _Golden Calf_. 1994. Augmented reality idol with custom hardware. <https://www.jeffreyshawcompendium.com/portfolio/golden-calf>. Image: Ars Electronica ‘94, Design Center Linz, Linz, Austria, 1994.\n  3. 152\\. Bailey, Jeremy. _Nail Art Museum_. 2014. Augmented reality miniature museum. <https://www.jeremybailey.net/products/nail-art-museum>.\n  4. 153\\. Y&R New York. _The Whole Story_. 2017. Augmented reality public statuary. https://play.google.com/store/apps/details?id=ca.currentstudios.thewholestory.\n  5. 154\\. Skwarek, Mark, and Joseph Hocking. _The Leak In Your Hometown_. 2010. Augmented reality protest app triggered by BP's logo. <https://theleakinyourhometown.wordpress.com>.\n  6. 155\\. Shafer, Nathan, and the Institute for Speculative Media. _The Exit Glacier Augmented Reality Terminus Project_. 2012. Augmented reality climate data visualization app. Kenai Fjords National Park. <http://nshafer.com/exitglacier>.\n  7. 156\\. Raupach, Anna Madeleine. _Augmented Nature_. 2019. Augmented reality project series using natural objects with data visualization. <http://www.annamadeleine.com/augmented-nature>.\n\n## Extrapolated Body\n\n  1. 157\\. Jones, Bill T., and Google Creative Lab. _Body, Movement, Language_. 2019. Pose model experiments. <https://experiments.withgoogle.com/billtjonesai>. Image courtesy of Google Creative Lab.\n  2. 158\\. Akten, Memo, and Davide Quayola. _Forms_. 2012. Digital renderings. <https://vimeo.com/38017188>. This project was commissioned by the National Media Museum, with the support of imove, part of the Cultural Olympiad programme. Produced by Nexus Interactive Arts.\n  3. 159\\. Universal Everything. _Walking City_. 2014. Video sculpture. <https://vimeo.com/85596568>.\n  4. 160\\. Groupierre, Karleen, Adrien Mazaud, and Sophie Daste. _Miroir_. 2011. Interactive augmented reality installation. <http://vimeo.com/20891308>.\n  5. 161\\. YesYesNo. _Más Que la Cara_. 2016. Interactive installation. <https://www.instagram.com/p/BDxsVZ0JNpm/>. Discussed in Lieberman, Zach. “Más Que la Cara Overview.” Medium, posted April 3, 2017. <https://medium.com/@zachlieberman/m%C3%A1s-que-la-cara-overview-48331a0202c0>.\n\n## Synesthetic Instrument\n\n  1. 162\\. Pereira, Luisa, Yotam Mann, and Kevin Siwoff. _In C_. 2015. Evolving web-based interactive audiovisual score and performance. <http://www.luisapereira.net/projects/project/in-c>.\n  2. 163\\. Pitaru, Amit. _Sonic Wire Sculptor_. 2003. Audiovisual composition and performance instrument. <https://www.youtube.com/watch?v=ji4VHWTk8TQ>.\n  3. 164\\. Van Gelder, Pia. _Psychic Synth_. 2014. Interactive audiovisual installation powered by brain waves. <https://piavangelder.com/psychicsynth>.\n  4. 165\\. Brandel, Jono, and Lullatone. _Patatap_. 2012. Audiovisual composition and performance instrument. <https://works.jonobr1.com/Patatap>.\n  5. 166\\. Clayton, Jace. _Sufi Plug Ins_. 2012. Suite of music-making apps with poetic interface. <http://www.beyond-digital.org/sufiplugins>.\n  6. 167\\. Hundred Rabbits (Rekka Bellum and Devine Lu Linvega). _ORCA._ 2018–2020.\n\n## Exercises\n\n  * (p. 152) Aliyu, Zainab. _p5.js Self-Portrait_. Student project from 15-104: Computation for Creative Practices, Carnegie Mellon University, 2015. <http://cmuems.com/2015c/>.\n  * (p. 178) Blankensmith, Isaac. _ANTI-FACE-TOUCHING MACHINE_. Interactive software presented on Twitter, March 2, 2020. <https://twitter.com/Blankensmith/status/1234603129443962880>.\n  * (p. 179) MSCHF (Gabriel Whaley et al.). _MSCHF Drop #12: This Foot Does Not Exist_. Software and text message service. 2020. <https://thisfootdoesnotexist.com/>.\n  * (p. 179) Pietsch, Christopher. _UMAP Plot of the OpenMoji Emoji Collection_. Presented on Twitter, April 15, 2020. <https://twitter.com/chrispiecom/status/1250404420644454406/>\n\n## Provenance\n\n  * (p. 239) Stoiber, Robert. Student artwork. In Schneeberger, Reiner. “Computer Graphics at the University of Munich (West Germany),” _Computer Graphics and Art_ 1, no. 4 (November 1976): 28, <http://dada.compart-bremen.de/docUploads/COMPUTER_> GRAPHICS_AND_ART_Nov1976.pdf.\n  * (p. 240) Wilson, Mark. “METAFACE”. Generated face designs. In _Drawing with Computers_ (New York: Perigee Books, 1985), 18. <http://mgwilson.com/Drawing%20with%20Computers.pdf>.\n  * (p. 241) Maeda, John. “Problem 2A — Time Display”. Assignment from _MAS.961: Organic Form_ (MIT Media Lab, fall 1999). <https://web.archive.org/> web/20000901042632/<http://acg.media>. [mit.edu/courses/organic/](http://mit.edu/courses/organic/), accessed January 18, 2000. Screenshot courtesy Casey Reas.\n  * (p. 242) Vojir, Lukas. _Processing Monsters_. 2008. Website with contributed interactive sketches. <https://web.archive.org/web/20090304170310/http://rmx.cz/monsters/>.\n\n\n# Name Index\n\n  * !Mediengruppe Bitnik, 80\n\n  * Abe, Kaho, 74, 76\n  * Adam, Jack, 141\n  * Aesthetics + Computation Group, MIT Media Lab, vi, 239\n  * Akay, 53\n  * Akten, Memo, 38, 141\n  * Albaugh, Lea, 121\n  * Albers, Anni, 47\n  * Albers, Josef, 156\n  * Aliyu, Zainab, 152\n  * Allahyari, Morehshin, 129, 130\n  * Allen, Rebecca, 141\n  * Anatsui, El, 44, 47\n  * Andersen, Emily, 71\n  * Anderson, Tim, 107\n  * Anderson, Todd, 96\n  * Andrasek, Alisa, 129\n  * Ängeslevä, Jussi, 32\n  * AntiVJ, 71\n  * Anyasi, Hayden, 139, 141\n  * Archer, Agyei, 59\n  * Arduino, 3, 7, 116, 121, 147\n  * Arnall, Timo, 113\n  * Ars Electronica, 13, 71, 91, 245\n  * Artist, American, 80, 91, 96\n  * Asai, Nobumichi, 141\n  * Asega, Salome, 85\n  * Atari, 76\n  * Auber, Olivier, 85, 91\n  * Awkward Silence Ltd., 136\n\n  * Baas, Maarten, 32\n  * Badr, Andrew, 85\n  * Bailey, Jeremy, 136, 141\n  * Baker, Chris, 71, 113\n  * Baker, Christopher, 113\n  * Balkin, Amy, 247\n  * Banksy, 132, 136\n  * Bartholl, Aram, 47, 85, 101, 136\n  * Beddard, Tom, 38\n  * Bell Labs, 5, 243\n  * Benjamin, Ruha, 7, 13, 139, 142\n  * Bennett, Katherine, 228\n  * Berdugo, Liat, 101\n  * Berio, Daniel, 38\n  * Bernhardsson, Erik, 59\n  * Berrigan, Caitlin, 211\n  * Betts, Tom, 38\n  * Bhatnagar, Ranjit, 80\n  * Bieg, Kurt, 76\n  * Biegert, Marco, 32\n  * Biľak, Peter, 56\n  * Bilal, Wafaa, 91\n  * Binx, Rachel, 65, 129\n  * Black Socialists of America, 91\n  * Blake, Scott, 47\n  * Blank, Marc, 107\n  * Blankensmith, Isaac, 107, 178, 180\n  * Blas, Zach, 27, 141\n  * Bodge, Mike, 107\n  * Bohnacker, Hartmut, 239, 242, 243, 249, 250\n  * Bollinger, Dave, 21\n  * Brain, Tega, 13, 32, 80, 91, 113, 247\n  * Braitenberg, Valentino, 42\n  * Brand, Stewart, 5, 13\n  * Brandel, Jono, 147, 161\n  * Bravi, Lorenzo, 27, 240\n  * Bridle, James, 80\n  * Brown, Daniel, 38\n  * Brucker-Cohen, Jonah, 91\n  * Buckenham, George, 80\n  * Buechley, Leah, 2, 13, 21, 186\n  * Bulloch, Angela, 47\n  * Buolamwini, Joy, 27, 142\n  * Bura, Stéphane, 76\n  * Burrington, Ingrid, 101, 129\n  * Burtch, Allison, 96, 247\n  * Byron, Lee, 32\n\n  * Calver, Peter, 76\n  * Campbell, Jim, 32, 47\n  * Cannon, Bruce, 32\n  * Cantu, Aaron, 101\n  * Cardiff, Janet, 136\n  * Carnegie Mellon University (CMU), 13, 54, 65, 76, 240, 244–247, 250\n  * Castel, Louis-Bertrand, 147\n  * Castro, Arturo, 27, 260\n  * Cave, Nick, 141\n  * Centre for Genomic Gastronomy, 113\n  * Chang, Liu, 21\n  * Charity, Mitchell N., 32\n  * Chen, Adrian, 91\n  * Chen, Alex, 147\n  * Cheng, Ian, 38, 42\n  * Cheng, Siew Ming, 121\n  * Cheung, Jian, 113\n  * Cho, Peter, 59, 212, 245, 250\n  * Choi, Taeyoon, 11, 32, 191, 196, 197, 206, 219, 221, 227, 232, 236\n  * ChucK, 144\n  * Chun, Wendy Hui Kyong, 94\n  * Chung, Lisa Kori, 129\n  * Chung, Sougwen, 53\n  * Cinder, 3, 68, 144\n  * Clark, Lygia, 121\n  * Clayton, Jace, 147\n  * Clifton, Brian, 101\n  * Clymer, Andy, 221\n  * Co, Elise, 240\n  * Coding Train, The, 165, 170, 177, 180, 217, 226, 242, 249\n  * Cohen, Joshua, 91\n  * Cohen, Revital, 32\n  * Collishaw, Mat, 129\n  * Compton, Kate, 27, 36, 38, 80, 175\n  * Conway, John, 42, 150, 177\n  * Cooper, Ross, 32\n  * Crespo, Sofia, 42\n  * Critical Engineering Working Group, 113, 123\n  * Croshere, Skot, 32\n  * Crupi, Jennifer, 121\n  * Csuri, Charles, 124, 129\n\n  * D’Alessio, Stefano, 141\n  * D’Ignazio, Catherine, 13, 110, 113, 247\n  * da Costa, Beatriz, 65, 247\n  * Dameron, David, 129\n  * Daniels, Bruce, 107\n  * Darke, A. M., 141\n  * Daste, Sophie, 141\n  * Davies, Char, 38\n  * Davis, Kevan, 82, 85\n  * Davis, Douglas, 85\n  * Davis, Joe, 101\n  * Davis, Joshua, 21\n  * Davis, George, 113\n  * Davis, Leslye, 141\n  * de Nijs, Marnix, 141\n  * de Vaucanson, Jacques, 42\n  * Delvoye, Wim, 42\n  * Depoorter, Dries, 91\n  * Des Des Res, 129\n  * Design IO, 42\n  * Design Media Arts, UCLA, 92, 240, 241, 243, 244, 246, 248, 249\n  * Deswaef, Julien, 129\n  * Devendorf, Laura, 6, 13, 124, 129\n  * Devroye, Luc, 59\n  * Dewey-Hagborg, Heather, 5, 13, 27, 101, 187, 198, 205, 212, 227\n  * Diaz, Oscar, 32\n  * Dinahmoe, 91\n  * Dinkins, Stephanie, 107, 108\n  * Djerzinski, Joreg, 245\n  * Dobson, Kelly, 107, 121\n  * Donath, Judith, 65, 245, 250\n  * Dörfelt, Matthias, 27, 101\n  * Dove, Toni, 71\n  * Driessens, Erwin, 129\n  * Du Bois, W. E. B., 65\n  * Duarte, Daniel, 32\n  * DuBois, R. Luke, 65, 165, 182, 193, 199, 213, 217, 218, 223, 228\n  * Duc, Hang Do Thi, 65\n  * Dudley, Homer, 107\n  * Dudums, Voldemars, 80\n  * Duff, De Angela, 185–229\n  * Dullaart, Constant, 80\n\n  * Easterly, Douglas, 101\n  * Edmark, John, 129\n  * Edmunds, Peter, 53, 85\n  * Eisenman, Jonathan, 129\n  * Ekman, Paul, 24, 27\n  * Elahi, Hasan, 65\n  * Elsenaar, Arthur, 141\n  * Emin, Tracey, 65\n  * Engelbart, Douglas, 88\n  * Eo, Minsun, 258, 260, 279\n  * Epler, Matthew, 129, 212\n  * Escuela Universitaria de Diseño e Ingeniería de Barcelona, 244\n  * Etani, Takehito, 65\n  * Evans, Helen, 245\n  * Everybody House Games, 107\n  * Evil Mad Scientist Laboratories, 47\n  * Ewan, Ruth, 32\n  * Exonemo, 91\n  * Eyeo Festival, vi, 13, 38, 54, 80, 243, 250\n  * Eyl, Frédéric, 47\n\n  * Feingold, Ken, 107\n  * Fels, Sidney, 107\n  * Felsenstein, Lee, 85\n  * Felton, Nick, 65, 245\n  * Fetter, William, 141\n  * Fiebrink, Rebecca, 145, 147, 236\n  * Field.io, 38\n  * Fletcher, Harrell, 85\n  * Flückiger, Michael, 59\n  * Folger Shakespeare Library, 101\n  * Fong-Adwent, Jen, 91\n  * Forman, Eric, 71\n  * Formanek, Mark, 32\n  * Forsythe, William, 53, 141\n  * Fountain, Henry, 101\n  * Francis, Joseph, 243\n  * Franke, Daniel, 141\n  * Free Art and Technology (F.A.T.) Lab, 53\n  * Freeke, Saskia, 21\n  * Frick, Laurie, 65\n  * Friedman, William and Elizebeth, 101\n  * Friesen, Wallace, 24, 27\n  * Front Design, 53\n  * Frutiger, Adrian, 56\n  * Fry, Ben, vi, 3, 13, 53, 65, 163, 172, 239, 240, 243, 249\n  * Fuchs, Jochen, 121\n  * Fukuhara, Shiho, 101\n  * Fuller, Matthew, 94\n  * Funk, Andreas, 32\n\n  * Gabriel, Ulrike, 42\n  * Gadani, Amisha, 121\n  * Gage, Zach, 91\n  * Gaines, Charles, 47\n  * Galloway, Kit, 91\n  * Galloway, Alex, 94, 96, 246\n  * Gamboa Naon, Manolo, 21, 279\n  * Game Developers Conference (GDC), 246, 249\n  * GAMMA IV, 76, 246\n  * Gannis, Carla, 136\n  * Gannon, Madeline, 129\n  * Gardner, Howard, 13\n  * Gardner, Lauren, 196\n  * Gaskins, Nettrice, 8, 13\n  * Gaulon, Benjamin, 71\n  * Gees, Johannes, 53\n  * Geilfus, Simon, 38\n  * Gelitin, 129\n  * Georgia Tech, 244, 249\n  * Germanidis, Anastasis, 107\n  * Gesture Recognition Toolkit, 145\n  * Ghassaei, Amanda, 129\n  * Giffen, Daniel Craig, 32\n  * Gillian, Nick, 145\n  * Gingold, Chaim, 38, 42\n  * Ginsberg, Alexandra Daisy, 42\n  * Glassner, Andrew, 249\n  * Glenney, Brian, 136\n  * Glow, Beatrice, 38\n  * Goatly, Wesley, 107\n  * Gobeille, Emily, 42\n  * Goldberg, Ken, 85, 91\n  * Gommel, Mattias, 121\n  * Gondek, Alison, 21\n  * Gondry, Michel, 38\n  * Google, 5, 96, 107, 180, 222, 223, 246\n  * Google Creative Lab, 141, 147, 247\n  * Graffiti Research Lab, 53\n  * Gramazio, Holly, 53\n  * Green, Gunnar, 47\n  * Greenberg, Ira, 249\n  * Gremmler, Tobias, 141\n  * Groß, Benedikt, 249, 250\n  * Groupierre, Karleen, 141\n  * Guidetti, Michael, 71\n  * Gysin, Andreas, 71, 245\n\n  * Ha, David, 53, 107, 247\n  * Haacke, Hans, 113, 247\n  * Haeberli, Paul, 53, 243\n  * Haidary, Nadeem, 129\n  * Hallock-Greenewalt, Mary Elizabeth, 147\n  * Hamilton, Rory, 241\n  * Hansen, Heiko, 71, 245\n  * Hansen, Mark, 107\n  * Hapooje, Terike, 113\n  * Harbisson, Neil, 121\n  * Harel, Idit, 7, 13\n  * Harmon, Leon, 47, 243\n  * Hart, Vi, 38\n  * Hartley, Paddy, 141\n  * Hartman, Kate, 121\n  * Harvey, Adam, 27, 139, 141, 247\n  * Hawkins, Max, 91, 141\n  * He, Nicole, 107, 108, 247\n  * Heap, Imogen, 147\n  * Heaton, Kelly, 47\n  * Hello Games, 38\n  * Hendren, Sara, 13, 116, 121, 122, 136\n  * Henry, Desmond Paul, 53\n  * Hertlein, Grace C., 239, 249\n  * Hill, Kashmir, 96\n  * Hillis, Danny, 32\n  * Hinterding, Joyce, 113\n  * Hiromura, Masaaki, 32\n  * Hocking, Joseph, 132, 136\n  * Hodgin, Robert, 38\n  * Hoff, Anders, 38\n  * Hoff, Melanie, 96\n  * holloway, shawné michaelain, 80\n  * hooks, bell, 9, 13\n  * Horn, Rebecca, 121\n  * Horvitz, David, 91\n  * Houck, John, 243, 249\n  * House, Brian, 65, 96\n  * Howe, Daniel C., 96, 175\n  * Hsieh, Tehching, 32\n  * Huang, Mary, 59, 139\n  * Huang, Sha, 65, 129\n  * Huang, Shan, 65\n  * Huang, Lingdong, 141\n  * Huerta, David, 247\n  * Humans since 1982, 32\n  * Hundred Rabbits, 147\n  * Hyphen-Labs, 27\n\n  * Ihnatowicz, Edward, 42\n  * Ijeoma, Ekene, 129\n  * Illiger, Andreas, 76\n  * Integrated Digital Media (IDM), NYU, 165, 228\n  * Interactive Telecommunica-tions Program (ITP), NYU, 180, 187, 218, 226, 229, 242, 247, 250\n  * Itten, Johannes, vii\n  * Iwai, Toshio, 147, 243, 248\n\n  * Jeremijenko, Natalie, 32, 110, 113, 247\n  * Johnson, Ian, 53\n  * Jones, Bill T., 141\n  * Jongejan, Jonas, 53\n  * Jordà, Sergi, 147\n  * Jordan, Chris, 44, 47\n  * July, Miranda, 85, 91\n\n  * Kac, Eduardo, 101\n  * Kalish, Jack, 139, 141\n  * Kang, E Roon, 32\n  * Kanno, So, 53\n  * Kastner, Frederic, 147\n  * Katchadourian, Nina, 101\n  * Katsumoto, Yuichiro, 59\n  * Kaur, Manmohan, 101, 247, 250\n  * Kawara, On, 247\n  * Kawashima, Takashi, 47\n  * Kazemi, Darius, 80, 91, 96, 175\n  * Kedia, Harsh, 53\n  * Kenyon, Matt, 101\n  * Khan, Osman, 113\n  * Kiefer, Cedric, 141, 240, 243, 244\n  * Kite, Suzanne, 107\n  * Klein, Lauren, 13, 110, 113\n  * Klingemann, Mario, 27\n  * Kneupfel, Mike, 129\n  * Knoth, Christoph, 59\n  * Knowles, Tim, 53\n  * Knowlton, Ken, 47, 243\n  * Knuth, Donald, 56, 59, 240, 245\n  * Knutsen, Jørn, 113\n  * Koblin, Aaron, 47\n  * Koblitz, Neal, 101, 247, 250\n  * Koch, Rafael, 59\n  * Koelman, Zelf, 32\n  * Kokoromi, 76, 246\n  * Konami, 76\n  * Konzen, Neil, 76\n  * Koons, Jeff, 136\n  * Koss, Lorelei, 247, 250\n  * Kronick, Sam, 101\n  * Krueger, Myron, 88, 91, 248\n  * Kunz, Nicolas, 59\n  * Kuo, Molmol, 136\n  * Kurant, Agnieszka, 85\n  * Kurenniemi, Erkki, 147\n  * Kusama, Yayoi, 82, 85\n  * Kuwahara, Hiroto, 141\n  * Kuznetsov, Stacey, 113\n  * Kwon, Soonho, 53\n\n  * Lacroix, Paul, 141\n  * Lafuente, Keith, 141\n  * Lambert, Steve, 96, 246\n  * LaPlace, Jules, 27\n  * Latham, William, 42\n  * Latter, Louise, 53\n  * Laub, Julia, 249, 250\n  * Lavigne, Sam, 80, 91, 101, 172, 247, 250\n  * Lebling, Dave, 107\n  * Leeson, Lynn Hershman, 107\n  * Lehni, Jürg, 53, 59, 107\n  * Lehni, Urs, 59\n  * Lemercier, Joanie, 38\n  * Lev, Roi, 107\n  * Levin, Golan, 13, 27, 32, 33, 42, 53, 54, 107, 129, 130, 147, 148, 165, 239, 242, 249, 250\n  * Lewandowski, David, 141\n  * Lewis, John, 91\n  * LeWitt, Sol, 150, 151\n  * Lia, 21, 101, 129, 242\n  * Lieberman, Zachary (Zach), 4, 27, 53, 54, 107, 136, 141, 147, 188, 193, 196, 197, 203, 212, 222, 225, 243, 250\n  * Liftig, Anya, 211\n  * Lily & Honglei, 136\n  * Lipkin, Efrem, 85\n  * Lippmann, Holger, 21\n  * Liu, Fei, 42\n  * Lobser, David, 136\n  * Lombardi, Mark, 247\n  * Lotan, Ben, 101\n  * Louis-Rosenberg, Jesse, 129\n  * Lowe, Jen, 65\n  * Lozano-Hemmer, Rafael, 32, 47, 91, 107, 113, 141, 246\n  * Lu, David, 59, 107\n  * Lublin, David, 107\n  * Lund, Jonas, 96\n  * Lupi, Giorgia, 65, 210\n  * Lupton, Ellen, 59, 259, 265\n  * Lüsebrink, Dirk, 141\n  * Lutz, Hans-Rudolf, 244\n  * Lynch, Caitrin, 121\n\n  * Ma, Michelle, 113\n  * Macawnivore, 27\n  * Maciunas, George, 32\n  * Madsen, Rune, 59, 156, 163, 167, 189, 191, 206, 212, 226\n  * Maeda, John, vi, 6, 13, 32, 53, 59, 151, 209, 212, 239, 240, 242, 243, 244, 246, 249, 250\n  * Mainstone, Di, 121\n  * Maire, Julien, 53\n  * Maizland, Lindsay, 101\n  * Major Bueno, 76\n  * Makey Makey, 74\n  * Manabe, Daito, 121, 142\n  * Mandelbrot, Benoît, 38\n  * Mann, Yotam, 147\n  * Marclay, Christian, 32\n  * Martinussen, Einar Sneve, 113\n  * Mateas, Michael, 38, 244, 249\n  * Matsuda, Keiichi, 136\n  * Mattu, Surya, 80, 96\n  * Maurer, Luna, 150, 151\n  * Max/MSP/Jitter, 3, 144, 182, 213\n  * Mayer, Jillian, 27, 71, 91, 245\n  * Mazaud, Adrien, 141\n  * McCabe, Jonathan, 21\n  * McCarthy, Lauren, 11, 91, 92, 96, 107, 121, 141, 187, 192, 197, 203, 211, 218, 222, 225, 233, 236, 247, 248, 250\n  * McCormack, Jon, 38, 42\n  * McCurdy, Katie, 65\n  * McDermott, Kathleen, 121\n  * McDonald, Kyle, 27, 53, 80, 91, 129, 141, 142, 204\n  * McKay, Joe, 38, 71\n  * McLuhan, Marshall, 5, 13, 44, 116\n  * McNeil, Joanne, 92, 96\n  * McWilliams, Chandler, 239, 243, 244, 246, 249\n  * Menegon, Martina, 141\n  * Meseguer, Laura, 244\n  * Mets, Matt, 53\n  * Meyer-Brandis, Agnes, 113\n  * Miebach, Nathalie, 124, 129\n  * Mignonneau, Laurent, 42\n  * Miharbi, Ali, 32\n  * Miller, George Bures, 136\n  * Mir, Regina Flores, 65\n  * MIT AgeLab, 121\n  * MIT Media Lab, vi, 5, 13, 239, 245\n  * ml5.js, 138, 180\n  * Moeller, Christian, 139, 141\n  * Moholy-Nagy, László, 91\n  * Mojoptix, 32\n  * Moll, Joana, 92, 96, 113\n  * Molnár, Vera, 21, 38, 159\n  * Mondrian, Piet, 152\n  * Montfort, Nick, 159, 241, 242, 249\n  * Montinar, Steven, 121\n  * Monty Python, 104\n  * Morse, Brandon, 42\n  * Morzier, Eric, 32\n  * MSCHF, 179\n  * Mulder, Sander, 32\n  * Müller, Alexander, 121\n  * Munari, Bruno, 27, 32, 59, 240, 249\n  * Muniz, Vik, 47\n  * Munkowitz, Bradley G., 71\n  * Museum of Modern Art, The (MoMA) 136, 193, 245\n  * Mustakos, Tatyana, 158\n  * Mutiti, Nontsikelelo, 21, 91\n\n  * Naimark, Michael, 6, 13, 71, 245\n  * Nakamura, Yugo, 32\n  * Napier, Mark, 85\n  * Nasser, Ramsey, 74, 76, 80\n  * Navarro, Adrià, 42\n  * Nees, Georg, 154\n  * Nervous System, 21, 129\n  * Nexus Studio, 141\n  * Nguyen, Dong, 76\n  * Nimoy, JT, 53, 59, 147, 243\n  * Nordmeyer, Sascha, 121\n\n  * Obermaier, Klaus, 71, 141\n  * Odell, Jenny, 44, 47, 136\n  * Ohio State University, 244\n  * Okada, Kenichi, 121\n  * Okunseinde, Ayodamola, 85, 121\n  * Oliver, Julian, 96, 101, 113, 136, 247\n  * Ono, Yoko, 85, 150\n  * Onuoha, Mimi, 3, 13, 110, 113\n  * Opara, Anastasia, 38\n  * openFrameworks, 3, 68, 182\n  * Oram, Daphne, 53, 147\n  * Orlan, 27, 141\n  * Oxman, Neri, 129\n\n  * p5.js, 3, 21, 65, 147, 163, 204, 223\n  * Paine, Roxy, 124, 129\n  * Papert, Seymour, 7, 13, 192\n  * Parrish, Allison, 5, 78, 80, 147, 175, 189, 194, 200, 207, 218, 223, 229\n  * Parsons School of Design, 243, 248\n  * Pashenkov, Nikita, 59\n  * Pask, Gordon, 147\n  * Paterson, James, 53\n  * Paterson, Katie, 32\n  * Patten, James, 147\n  * Paulos, Eric, 113, 247\n  * Pedercini, Paolo, 7, 13, 38, 76, 91, 183, 240, 244, 246\n  * Pelletier, Mike, 27\n  * Penner, Robert, 165\n  * Perec, Georges, 85\n  * Pereira, Luisa, 147\n  * Perini, Julie, 5, 13\n  * Perlin, Ken, 27\n  * Perry, Phœnix, 188, 192, 211, 221, 228\n  * Peyton, Miles Hiroo, 71\n  * Phiffer, Dan, 96\n  * Picasso, Pablo, 53\n  * Pietsch, Christopher, 179\n  * Pipkin, Everest, 38, 47, 80, 101\n  * Pita, Damjan, 136\n  * Pitaru, Amit, 53, 147\n  * Planetside Software, 38\n  * Plummer-Fernández, Matthew, 101, 129\n  * Pólya, Georg, 21\n  * Popp, Julius, 59\n  * Posavec, Stephanie, 65\n  * Prakash, Akshat, 53\n  * Processing, vi, 3, 13, 21, 42, 65, 68, 96, 144, 163, 170, 172, 180, 206, 222, 225, 239, 242, 243, 244\n  * Procrustes, 163\n  * Prophet, Jane, 42\n  * Purchase College, State University of New York, 247\n  * Pure Data, 144, 182\n  * Pyke, Matt, 42\n\n  * Quayola, Davide, 38, 141\n\n  * Rabinowitz, Sherrie, 88, 91\n  * Radical Software Group, 96\n  * Random International, 32\n  * Rapoport, Sonya, 65\n  * Raupach, Anna Madeleine, 136\n  * Reas, Casey, 3, 13, 21, 38, 92, 151, 163, 203, 239, 240, 241, 242, 243, 244, 248, 249, 250\n  * Remedi Project, 243\n  * Resnick, Mitchel, 7, 13, 54\n  * Reyes-Araos, Gonzalo, 47\n  * Reynolds, Craig, 243, 249\n  * Ribas, Moon, 113, 121\n  * Rich, Elliat, 47\n  * Rich, Kate, 113, 247\n  * Robson, Dominic, 241\n  * Rohrer, Jason, 91\n  * Rokeby, David, 107, 147, 247\n  * Röpke, Konrad, 121\n  * Rosenbaum, Eric, 54\n  * Rosenkrantz, Jessica, 129\n  * Ross, Sarah, 121\n  * Roth, Evan, 85, 222\n  * Rothberg, Sara, 96\n  * Rothenberg, Stephanie, 129\n  * Royal College of Art, 241, 249\n  * Rozendaal, Rafaël, 76, 96\n  * Rozin, Daniel, 47\n  * Rubin, Ben, 107\n  * Rubock, Jonathan, 76\n  * Rule, Alix, 91\n\n  * Sabin, Jenny, 129\n  * Sala, Anri, 113\n  * Salavon, Jason, 129\n  * Saltsman, Adam, 74, 76\n  * Santarromana, Joseph, 85, 91\n  * Saqoosha, 32\n  * Sareen, Harpreet, 107\n  * Sarin, Helena, 21\n  * Sauter, Joachim, 141\n  * Schachman, Toby, 53\n  * Schlemmer, Oskar, 141\n  * Schmidt, Loren, 80\n  * Schmitz, Michael, 59\n  * Schneeberger, Reiner, 239\n  * School for Poetic Computation (SFPC), 191, 196, 203, 221, 247\n  * Schulte, Jamie, 65\n  * Scott, James C., 110, 113\n  * Segal, Adrien, 129\n  * Selley, Gordon, 42\n  * Sermon, Paul, 91, 246\n  * Shafer, Nathan, 136\n  * Shaw, Jeffrey, 136\n  * Shi, Tara, 101\n  * Shiffman, Daniel (Dan), 11, 42, 163, 165, 170, 172, 175, 180, 187, 198, 203, 209, 217, 226, 249\n  * Shim, Kyuha, 59, 258, 260, 279\n  * Shoup, Dick, 54, 258\n  * Silver, Jay, 54\n  * Silvers, Rob, 47, 243\n  * Simon, Joel, 96\n  * Sims, Karl, 42\n  * Sinclair, Kamal, 8, 13, 232, 236\n  * Singer, Brooke, 65, 247\n  * Sinha, Shreeya, 141\n  * Siwoff, Kevin, 147, 243\n  * Sjölen, Bengt, 113\n  * Skwarek, Mark, 132, 136\n  * SMG Studio, 76\n  * Smigla-Bobinski, Karina, 53\n  * Smith, Alvy Ray, 54\n  * Smithson, Robert, 132\n  * Snibbe, Scott S., 5, 13, 53, 54, 91, 142, 246\n  * Snowden, Edward, 98, 247\n  * Soares, Susana, 42\n  * Sobecka, Karolina, 27, 71, 113, 141\n  * Sociable Media Group, MIT Media Lab, 245\n  * Soennecken, Friedrich, 59\n  * Sokpop Collective, 91\n  * Solie, Kristyn Janae, 38\n  * Solt, Mary Ellen, 21\n  * Sommerer, Christa, 42\n  * Soon, Winnie, 189, 192, 199, 204, 213, 217, 221, 228\n  * Spelman, Elliott, 141\n  * Spiegel, Laurie, 54, 147\n  * Sputniko!, 121\n  * Steinkamp, Jennifer, 21\n  * Stelarc, 121\n  * Stewart, Damian, 54\n  * Stock, Susan, 223\n  * Stollenmayer, Phillipp, 76\n  * Studio Moniker, 32, 71, 85\n  * Su, Peiqi, 47\n  * Sugimoto, Tatsuo, 185–229\n  * Sugrue, Christine, 54\n  * Sunlight Foundation, 96\n  * SuperCollider, 144\n  * Surprenant, David, 91\n  * Sutherland, Ivan, 50, 54\n  * Szpakowski, Mark, 85\n\n  * Takahashi, Keijiro, 141\n  * Tale of Tales, 91\n  * Tarbell, Jared, 38\n  * Tardy, Romain, 71\n  * TenthBit Inc, 91\n  * Thapen, Neil, 107\n  * Thayer, Pall, 80\n  * The Yes Men, 96\n  * Thompson, Jeff, 80\n  * Thorp, Jer, 80, 192, 197, 205, 209, 218, 225, 250\n  * Thricedotted, 80\n  * Todo, 21\n  * Tracery, 3, 80, 175\n  * Tremmel, Georg, 101\n  * Trochut, Joan, 59\n  * Tseng, Francis, 42\n  * Tseng, Yen-Wen, 32\n  * Turkle, Sherry, 7\n  * Tzara, Tristan, 173\n\n  * Ulanovsky, Julieta, 59\n  * Unity (game engine), 3\n  * Universal Everything, 42, 141\n  * Urbonas, Julijonas, 113\n  * Utterback, Camille, 141\n\n  * Valbuena, Pablo, 71, 245\n  * Valla, Clement, 54\n  * Van Balen, Tuur, 32\n  * Van Gelder, Pia, 147\n  * Vanetti, Sidi, 71, 245\n  * Vaquié, Thomas, 71\n  * Varner, Maddy, 91, 101, 113\n  * Vasarely, Victor, 21\n  * Vasiliev, Daniel (Danja), 96, 113\n  * Vasudevan, Roopa, 85\n  * Verstappen, Maria, 129\n  * Viégas, Fernanda, 65\n  * Vo, Lam Thuy, 65\n  * Vojir, Lukas, 42, 242, 249\n  * Voss, Richard F., 38\n\n  * Wagenknecht, Addie, 53\n  * Walter, William Grey, 42\n  * Wang, Wen, 129\n  * Warren, Jonah, 53\n  * Watanabe, Brent, 42\n  * Watson, Theo, 42, 141\n  * Wattenberg, Martin, 242\n  * Watz, Marius, 21, 242\n  * Weinberg, Tali, 47\n  * Weinkle, Ari, 141\n  * Weizenbaum, Joseph, 107\n  * Wekinator, 145, 147\n  * Whitelaw, Mitchell, 5, 13, 42, 129, 236\n  * Wiesner, Jerome B., 6\n  * Wilk, Elvia, 6, 13\n  * Willmott, Laurence, 32\n  * Wilson, Mark, 240, 249\n  * Wodiczko, Krzysztof, 71\n  * Woebken, Chris, 121\n  * Wolfram, Stephen, 65\n  * Wolpert, Jesse, 121\n  * Wood, Jeremy, 54\n  * Wood, McKinnon, 147\n  * Woodgate, Agustina, 32\n  * Wright, Will, 42\n  * Wu, Amy Suo, 101\n\n  * Xenakis, Iannis, 54, 147\n  * Xerox Palo Alto Research Center (PARC), 5\n\n  * Y&R New York, 136\n  * Yablonina, Maria, 129\n  * Yamaguchi, Takahiro, 53\n  * Yao, Lining, 129\n  * YesYesNo, 71, 141\n  * Yoshihara, Jirō, 85\n\n  * Zawada, Jonathan, 38\n  * Zer-Aviv, Mushon, 96\n  * Zhang, Jia, 80\n  * Zhu, Jingwen, 91\n  * Zimbardi, Flavia, 59\n  * Zurkow, Marina, 229\n\n\n# Subject Index\n\n  * Abstraction, 2, 4, 17–21, 43, 124, 133, 139, 141, 161, 240, 241, 244\n  * Activist art, 8, 80, 94, 98, 110, 132\n  * Afrofuturism, 85, 120–121\n  * Analog (without a computer), 58, 150, 183\n  * Animation, 24, 36, 74, 138, 139, 160, 161, 162, 163\n  * API (application programming interface), 78, 80, 94, 182, 211, 222, 228, 247\n  * Architecture, 67–71\n  * Artificial intelligence, 53, 82, 88, 91, 104–108, 227\n  * Arts or cultural literacy, 6, 193, 194, 196–201\n  * Assistive technology, 115–122, 199\n  * Audiovisual, 143–148, 161, 181–182\n  * Augmented reality, 68, 130–135\n  * Automation, 53, 85, 103–108, 178. _See also_ Bot\n\n  * Bauhaus movement, vii, 47\n  * Beginners, 188–193, 201–205, 209–211, 224–229, 232–233\n  * BioArt, 198, 212\n  * Biometrics, 24, 116, 137–142, 178, 179\n  * Blockchain, 101\n  * Body, 40, 50, 53, 65, 68, 74, 88, 115–122, 125, 137–142, 144, 178, 201, 248\n  * Bot, 75–78, 88, 91, 103–108. _See also_ Automation\n  * Browser, 82, 93–96, 178, 211, 246\n\n  * CAD, 101, 124, 129\n  * Challenges or difficulties when teaching, 216–218\n  * Chance. _See_ Randomness\n  * Children's media, 98\n  * Circles, 155, 156, 161, 164–166, 169, 171, 183\n  * Classroom, 8–9, 190, 231–236\n  * Climate change, 136, 198\n  * Collaboration, 9, 50, 81–85, 150, 194, 209, 228\n  * Color, 18, 24, 44, 96, 145, 153, 155–156, 170, 171, 173, 199\n  * Community building, 3, 4, 9, 193, 232\n  * Computational art, history, 150, 154, 159, 212, 238–250\n  * Computational literacy, 4, 187, 196\n  * Computer science, 187\n  * Computer science pedagogy, 2–3, 188\n  * Conditional design, 150\n  * Conditional testing, 157\n  * Connectivity, 10. _See also_ Networks\n  * Constructivism, 7\n  * Creative coding, 3, 4, 196, 228, 237–250\n  * Critical design, 5, 94, 212\n  * Critical engineering, 5, 113\n  * Critique, 5, 6, 94, 110, 132, 196, 233–234\n  * Crowdsourcing, 82\n  * Curves, 152, 163–165, 168\n  * Cybernetics, 21, 42, 122\n\n  * Dada, 173, 207, 227\n  * Dance, 137–139, 222–223\n  * Data collection, sensing, 109–113, 116, 144, 247\n  * Data visualization, 24, 30, 61–65, 124, 138, 167, 170, 171–172, 173, 175, 179, 205, 210, 245\n  * Datasets, 155, 156, 170, 171–172, 178, 179, 175, 173, 205\n  * Debugging, 217, 226, 232–233\n  * Defamiliarization, 7, 50, 94, 117\n  * Détournement, 94, 96, 104\n  * Diversity, inclusivity,8, 221, 232\n  * Drawing, 24, 49–54, 150, 152, 158, 167, 188, 210, 211, 213, 243, 244\n\n  * Electronics. _See_ Physical computing\n  * Emergence, 40, 81–85, 150, 176–177, 213\n  * Encoding, 97–101, 151. _See also_ Transcoding\n  * Environmental art, 47, 71, 110, 113, 136\n  * Excitement, 9, 187, 227\n\n  * Fabrication, 18, 116, 123–130\n  * Feminism, as a theme, 53, 85, 121, 136\n  * Fluxus, 227\n\n  * Games, 73–76, 132, 150, 183, 211, 245\n  * Generativity, 9, 23–27, 35–38, 123–130, 164, 174, 175, 213, 242\n  * Geolocation, 132, 171–172\n  * Geometry, 168, 169, 210\n  * Graphic design, 238, 244, 258–259, 17–21, 30, 55–59\n\n  * Hacking, 5, 13, 116, 163, 204, 211, 229\n\n  * Images, 170, 179\n  * Installation, 68, 132\n  * Interactivity, 9, 24, 44, 50, 74, 82, 88, 94, 104, 116, 138, 144, 153, 155, 157, 160, 161, 179, 245, 247\n  * Interface, 50, 74, 82, 88, 94, 104, 144, 145\n  * Internet art, 77–80, 81–85, 88, 93–96\n  * Intervention (public), 67–71, 78, 94, 110, 131–146\n  * Iteration, 17–21, 153, 159, 160, 170, 239\n\n  * Labor, 44, 47, 53, 82\n  * Landscape, 35–38\n  * Language, 88, 200, 206, 222, 232, 238\n\n  * Machine learning, 178, 179\n  * Metadesign, designing systems, 18, 36, 40, 50, 56, 82, 124, 223\n  * Military technology, 124, 139, 198\n  * Mobile media, 24, 36, 56, 124, 132\n  * Movement, motion, 40, 137–142, 158, 176–177, 183\n\n  * Natural language processing, 96, 98, 103–108, 173, 174, 200, 207\n  * Networks, 77–80, 81–85, 87–92, 93–96, 246\n  * Noise, 36, 158–159, 176, 183\n\n  * Parametric design, 24, 56, 123–130, 158, 212, 240, 244\n  * Participation, 81–85\n  * Particle system, 158, 172, 176–177, 181, 182\n  * Pattern, 17–21, 152, 154, 159, 177, 239\n  * Performance, 116, 138, 144, 200, 248\n  * Physical computing, 110, 116, 182\n  * Pixels, 43–47, 150, 170\n  * Portraiture, 61–65, 152\n  * Programming language, vii, 7, 11, 12, 47 147, 173–175, 187, 193, 194, 203, 204\n  * Project-based learning, 6\n  * Projection, 67–71, 245\n  * Public art, 78, 110, 131–134\n  * Public space, 121, 131–136\n\n  * Randomness, 36, 40, 158, 159, 170\n  * Recursion, 176, 276\n  * Robotics, 40, 42, 53, 59, 85, 107, 116\n\n  * Scale, 44, 68\n  * Sculpture, 124–130, 131–136\n  * Search, 160, 173, 170, 153, 207\n  * Shape, 164–165, 162–163, 152, 159\n  * Silicon Valley, 5\n  * Simulation, 24, 36, 40, 176\n  * Social justice, as a theme, 27, 91, 110, 113, 236\n  * Social media, 5, 62, 77–80, 88, 239\n  * Sorting, 160, 170, 173\n  * Sound, 104, 138, 143–148, 151, 179, 180–181, 161, 213, 248\n  * Speculative design, 5, 85, 117, 120, 128, 134, 141, 212\n  * Surveillance, 62, 94, 98, 139\n  * System design. _See_ Metadesign\n\n  * Textiles, 18, 47\n  * Time, 29–33, 161, 211\n  * Tools, 49–54, 87–92, 93–96, 97–101, 109–112, 115–122, 204, 209, 243\n  * Transcoding, 9, 182, 182, 181, 172. _See also_ Encoding\n  * Typography, 55–59, 162–163, 212, 244\n\n  * Virtual reality, 38, 141\n  * Virtuality, also corporeality, 10, 67–72, 115–122, 131–136, 136–142\n  * Visualization. _See_ Data visualization\n  * Voice, 52, 103–108, 182, 247\n\n  * Wearables, 113–120, 138\n  * Wi–Fi, 65, 96, 113, 136\n\n\nO gracious reader, wash your hands and touch the book only when you have\ncoerced those who represent you to dismantle the police and decarbonize our\nsocieties, else there will be no future for computational art and design, nor\nlife on Earth as we know it.\n\nThe attractive parts of this book were designed by Kyuha (Q) Shim and Minsun\nEo. Any unattractive parts are wholly the result of interference and\nquestionable decisions by Golan Levin and Tega Brain. The majority of this\nbook's layout was computationally generated using Basil.js.\n\nThe artworks on the front and back covers of this book were generated in\nProcessing by Manolo Gamboa Naon and are excerpted from _Vitamina_ (2019) and\n_Fils_ (2020), respectively. The illustrations in the Exercises section were\ndesigned using Processing and p5.js.\n\nThe book was set in Atlas Grotesk and Atlas Typewriter, designed in 2012 by\nSusana Carvalho, Kai Bernau, Christian Schwartz, and Ilya Ruderman.\n\nA repository of code for this book, including sample solutions for Exercises,\nis located at: <https://github.com/CodeAsCreativeMedium>.\n\n\n# Part One: Assignments\n\n\n# Part Two: Exercises\n\n\n# Part Three: Interviews\n\n\n# Appendices\n\n\n# Bibliographies\n\n\n# Indexes\n\n",
    "book_id": "code_as_creative_medium",
    "book_title": "Code as Creative Medium",
    "book_author": "Golan Levin",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 0
  },
  {
    "chunk_full": "![image](../images/9781616897840.jpg)\n\n\n![image](../images/f0001-01.jpg)\n\n\n![image](../images/f0002-01.jpg)\n\n\n## I.0 Preface\n\nGenerative design has long ceased to be a trade secret among design students;\nin some universities, it is now firmly integrated into the curriculum. From\ninfographics to the visualization of sound, from the fine arts to\narchitecture, and especially in the realm of communication design and media\ninstallations, generative design allows for dynamic, stunning, and fascinating\napplications.\n\nProcessing and vvvv have for many years been the programming environments of\nchoice for artists and designers. However, more recently there has been a\nshift toward more web-centric applications, giving rise to new coding\nenvironments such as p5.js, a JavaScript library that is especially programmed\nfor and by artists, designers, and other web users.\n\nThe first edition of _Generative Design_ was written almost a decade ago, and\nits acclaimed underlying teaching methods are still unrivaled. In this updated\nedition, the authors create an even more accessible and up-to-date entry point\ninto creative coding with the use of JavaScript. In the spirit of the first\nedition of _Generative Design_ , the goal is to remove any fear of programming\nand demonstrate how existing program snippets can be manipulated and tweaked\nto achieve amazing results almost at the click of a button.\n\nGenerative design fundamentally changes the design process: the designer\nshifts from being a performer of tasks to being a conductor, effectively\norchestrating the decision-making process of the computer. This is what\ngenerative design is all about: iteratively developing different processes and\nthen selecting those that produce the most visually compelling results.\nDesigners and artists no longer have to use the tools dictated by computers\nand powerful but prescriptive design software and can now create their own\ntools, which generate amazing results independently, as many of the examples\nin the book demonstrate.\n\nIn four simple lessons, Color, Shape, Type, and Image, users learn to\ninfluence their results and to improve them by either varying parameters—as\nexplained in detail in each step—or by changing entire algorithms. The\nexplanations are easy to understand, and their execution requires little or\neven no programing; with p5.js and its rapidly growing community, it is\nbecoming easier to lay the groundwork for advanced technologies and trends,\nfrom 3D to augmented reality. The p5.js community is very active and\nconstantly provides new updates and plug-ins for extending the functionality\nof p5.js. This book shows what can be done with this knowledge and how to dive\ndeeper into generative design and its community. After experimenting with the\nsketches in the book and completing initial tasks with the online p5.js\neditor, users can venture forth independently and explore and expand on the\ncreative output of the p5.js community and beyond.\n\nWith the success of _Generative Design_ , which has been translated into\nseveral languages, the authors realized that the key to teaching artists and\ndesigners how to code was to empower them through simple but satisfying\nincremental successes. Students could then build increasing complexity into\ntheir work based on these basic principles.\n\nThe book is supplemented by a website where users can download all of the\nprograms (sketches) for free and start experimenting immediately. After\ncompleting the four tutorials, you will be able to visualize data, create\ninfographics, visualize text analyses, and much more.\n\nHave fun with creative coding.\n\nKarin and Bertram Schmidt-Friderichs\n\n\n## I.1 Contents\n\n**[I Introduction](03-part01.xhtml#int) [3–41](03-part01.xhtml#int)**\n\n**[I.0 Preface](04-part01ch00.xhtml#i.0) [3](04-part01ch00.xhtml#i.0)**\n\n**[I.1 Contents](05-part01ch01.xhtml#i.1) [6](05-part01ch01.xhtml#i.1)**\n\n**[I.2 Website](06-part01ch02.xhtml#i.2) [8](06-part01ch02.xhtml#i.2)**\n\n**[I.3 Projects](07-part01ch03.xhtml#i.3) [10](07-part01ch03.xhtml#i.3)**\n\n[01: 2016/17 Daily Sketches](07-part01ch03.xhtml#lev1)\n[11](07-part01ch03.xhtml#lev1)\n\n[02: 2017 Planck](07-part01ch03.xhtml#lev2) [15](07-part01ch03.xhtml#lev2)\n\n[03: 2015 Abstract](07-part01ch03.xhtml#lev3) [17](07-part01ch03.xhtml#lev3)\n\n[04: 2016 Rottlace – Björk](07-part01ch03.xhtml#lev4)\n[18](07-part01ch03.xhtml#lev4)\n\n[05: 2016 VOID VIII 01](07-part01ch03.xhtml#lev5)\n[20](07-part01ch03.xhtml#lev5)\n\n[06: 2017 Monotype: Type Reinvented](07-part01ch03.xhtml#lev6)\n[23](07-part01ch03.xhtml#lev6)\n\n[07: 2016 Nike Strike Series FA16](07-part01ch03.xhtml#lev7)\n[25](07-part01ch03.xhtml#lev7)\n\n[08: 2016 VoxelChair v.1.0](07-part01ch03.xhtml#lev8)\n[27](07-part01ch03.xhtml#lev8)\n\n[09: 2016 Collide: synaesthetic art installation](07-part01ch03.xhtml#lev9)\n[31](07-part01ch03.xhtml#lev9)\n\n[10: 2017 Block Bills](07-part01ch03.xhtml#lev10)\n[32](07-part01ch03.xhtml#lev10)\n\n[11: 2015 Roads to Rome](07-part01ch03.xhtml#lev11)\n[35](07-part01ch03.xhtml#lev11)\n\n[12: 2015 Jller](07-part01ch03.xhtml#lev12) [36](07-part01ch03.xhtml#lev12)\n\n[13: 2016 Aerial Bold](07-part01ch03.xhtml#lev13)\n[40](07-part01ch03.xhtml#lev13)\n\n**[P Basic Principles](08-part02ch00.xhtml#part1)\n[42–225](08-part02ch00.xhtml#part1)**\n\n**[P.0 Introduction to p5.js](08-part02ch00.xhtml#p.0)\n[42](08-part02ch00.xhtml#p.0)**\n\n[P.0.0 p5.js, JavaScript, and Processing](08-part02ch00.xhtml#p.0.0)\n[44](08-part02ch00.xhtml#p.0.0)\n\n[P.0.1 The development environment](08-part02ch00.xhtml#p.0.1)\n[46](08-part02ch00.xhtml#p.0.1)\n\n[P.0.2 Language elements](08-part02ch00.xhtml#p.0.2)\n[48](08-part02ch00.xhtml#p.0.2)\n\n[P.0.3 Programming beautifully](08-part02ch00.xhtml#p.0.3)\n[56](08-part02ch00.xhtml#p.0.3)\n\n**[P.1 Color](09-part02ch01.xhtml#p.1) [58](09-part02ch01.xhtml#p.1)**\n\n[P.1.0 Hello, color](09-part02ch01.xhtml#p.1.0)\n[60](09-part02ch01.xhtml#p.1.0)\n\n[P.1.1 Color spectrum](09-part02ch01.xhtml#p.1.1)\n[62](09-part02ch01.xhtml#p.1.1)\n\n[P.1.1.1 Color spectrum in a grid](09-part02ch01.xhtml#p.1.1.1)\n[62](09-part02ch01.xhtml#p.1.1.1)\n\n[P.1.1.2 Color spectrum in a circle](09-part02ch01.xhtml#p.1.1.2)\n[64](09-part02ch01.xhtml#p.1.1.2)\n\n[P.1.2 Color palettes](09-part02ch01.xhtml#p.1.2)\n[66](09-part02ch01.xhtml#p.1.2)\n\n[P.1.2.1 Color palettes through interpolation](09-part02ch01.xhtml#p.1.2.1)\n[66](09-part02ch01.xhtml#p.1.2.1)\n\n[P.1.2.2 Color palettes from images](09-part02ch01.xhtml#p.1.2.2)\n[68](09-part02ch01.xhtml#p.1.2.2)\n\n[P.1.2.3 Color palettes from rules](09-part02ch01.xhtml#p.1.2.3)\n[72](09-part02ch01.xhtml#p.1.2.3)\n\n**[P.2 Shape](10-part02ch02.xhtml#p.2) [78](10-part02ch02.xhtml#p.2)**\n\n[P.2.0 Hello, shape](10-part02ch02.xhtml#p.2.0)\n[80](10-part02ch02.xhtml#p.2.0)\n\n[P.2.1 Grid](10-part02ch02.xhtml#p.2.1) [82](10-part02ch02.xhtml#p.2.1)\n\n[P.2.1.1 Alignment in a grid](10-part02ch02.xhtml#p.2.1.1)\n[82](10-part02ch02.xhtml#p.2.1.1)\n\n[P.2.1.2 Movement in a grid](10-part02ch02.xhtml#p.2.1.2)\n[86](10-part02ch02.xhtml#p.2.1.2)\n\n[P.2.1.3 Complex modules in a grid](10-part02ch02.xhtml#p.2.1.3)\n[90](10-part02ch02.xhtml#p.2.1.3)\n\n[P.2.1.4 Checkboxes in a grid](10-part02ch02.xhtml#p.2.1.4)\n[94](10-part02ch02.xhtml#p.2.1.4)\n\n[P.2.1.5 From grid to moiré](10-part02ch02.xhtml#p.2.1.5)\n[98](10-part02ch02.xhtml#p.2.1.5)\n\n[P.2.2 Agents](10-part02ch02.xhtml#p.2.2) [102](10-part02ch02.xhtml#p.2.2)\n\n[P.2.2.1 Dumb agents](10-part02ch02.xhtml#p.2.2.1)\n[102](10-part02ch02.xhtml#p.2.2.1)\n\n[P.2.2.2 Intelligent agents](10-part02ch02.xhtml#p.2.2.2)\n[104](10-part02ch02.xhtml#p.2.2.2)\n\n[P.2.2.3 Shapes from agents](10-part02ch02.xhtml#p.2.2.3)\n[108](10-part02ch02.xhtml#p.2.2.3)\n\n[P.2.2.4 Growth structure from agents](10-part02ch02.xhtml#p.2.2.4)\n[112](10-part02ch02.xhtml#p.2.2.4)\n\n[P.2.2.5 Structural density from agents](10-part02ch02.xhtml#p.2.2.5)\n[116](10-part02ch02.xhtml#p.2.2.5)\n\n[P.2.2.6 Agents on a pendulum](10-part02ch02.xhtml#p.2.2.6)\n[120](10-part02ch02.xhtml#p.2.2.6)\n\n[P.2.3 Drawing](10-part02ch02.xhtml#p.2.3) [126](10-part02ch02.xhtml#p.2.3)\n\n[P.2.3.1 Drawing with animated brushes](10-part02ch02.xhtml#p.2.3.1)\n[126](10-part02ch02.xhtml#p.2.3.1)\n\n[P.2.3.2 Relation and distance in drawing](10-part02ch02.xhtml#p.2.3.2)\n[130](10-part02ch02.xhtml#p.2.3.2)\n\n[P.2.3.3 Drawing with type](10-part02ch02.xhtml#p.2.3.3)\n[132](10-part02ch02.xhtml#p.2.3.3)\n\n[P.2.3.4 Drawing with dynamic brushes](10-part02ch02.xhtml#p.2.3.4)\n[134](10-part02ch02.xhtml#p.2.3.4)\n\n[P.2.3.5 Drawing with the pen tablet](10-part02ch02.xhtml#p.2.3.5)\n[138](10-part02ch02.xhtml#p.2.3.5)\n\n[P.2.3.6 Drawing with complex modules](10-part02ch02.xhtml#p.2.3.6)\n[142](10-part02ch02.xhtml#p.2.3.6)\n\n[P.2.3.7 Drawing with multiple brushes](10-part02ch02.xhtml#p.2.3.7)\n[146](10-part02ch02.xhtml#p.2.3.7)\n\n**[P.3 Type](11-part02ch03.xhtml#p.3) [150](11-part02ch03.xhtml#p.3)**\n\n[P.3.0 Hello, type](11-part02ch03.xhtml#p.3.0)\n[152](11-part02ch03.xhtml#p.3.0)\n\n[P.3.1 Text](11-part02ch03.xhtml#p.3.1) [154](11-part02ch03.xhtml#p.3.1)\n\n[P.3.1.1 Writing time-based text](11-part02ch03.xhtml#p.3.1.1)\n[154](11-part02ch03.xhtml#p.3.1.1)\n\n[P.3.1.2 Text as blueprint](11-part02ch03.xhtml#p.3.1.2)\n[156](11-part02ch03.xhtml#p.3.1.2)\n\n[P.3.1.3 Text image](11-part02ch03.xhtml#p.3.1.3)\n[160](11-part02ch03.xhtml#p.3.1.3)\n\n[P.3.1.4 Text diagram](11-part02ch03.xhtml#p.3.1.4)\n[166](11-part02ch03.xhtml#p.3.1.4)\n\n[P.3.2 Font outline](11-part02ch03.xhtml#p.3.2)\n[170](11-part02ch03.xhtml#p.3.2)\n\n[P.3.2.1 Dissolving the font outline](11-part02ch03.xhtml#p.3.2.1)\n[170](11-part02ch03.xhtml#p.3.2.1)\n\n[P.3.2.2 Varying the font outline](11-part02ch03.xhtml#p.3.2.2)\n[174](11-part02ch03.xhtml#p.3.2.2)\n\n[P.3.2.3 Font outline from agents](11-part02ch03.xhtml#p.3.2.3)\n[178](11-part02ch03.xhtml#p.3.2.3)\n\n[P.3.2.4 Parallel font outlines](11-part02ch03.xhtml#p.3.2.4)\n[180](11-part02ch03.xhtml#p.3.2.4)\n\n[P.3.2.5 Kinetic font](11-part02ch03.xhtml#p.3.2.5)\n[184](11-part02ch03.xhtml#p.3.2.5)\n\n**[P.4 Image](12-part02ch04.xhtml#p.4) [188](12-part02ch04.xhtml#p.4)**\n\n[P.4.0 Hello, image](12-part02ch04.xhtml#p.4.0)\n[190](12-part02ch04.xhtml#p.4.0)\n\n[P.4.1 Image cutouts](12-part02ch04.xhtml#p.4.1)\n[192](12-part02ch04.xhtml#p.4.1)\n\n[P.4.1.1 Image cutouts in a grid](12-part02ch04.xhtml#p.4.1.1)\n[192](12-part02ch04.xhtml#p.4.1.1)\n\n[P.4.1.2 Feedback of image cutouts](12-part02ch04.xhtml#p.4.1.2)\n[196](12-part02ch04.xhtml#p.4.1.2)\n\n[P.4.2 Image collection](12-part02ch04.xhtml#p.4.2)\n[198](12-part02ch04.xhtml#p.4.2)\n\n[P.4.2.1 Collage from image collection](12-part02ch04.xhtml#p.4.2.1)\n[198](12-part02ch04.xhtml#p.4.2.1)\n\n[P.4.2.2 Time-based image collection](12-part02ch04.xhtml#p.4.2.2)\n[202](12-part02ch04.xhtml#p.4.2.2)\n\n[P.4.3 Pixel values](12-part02ch04.xhtml#p.4.3)\n[204](12-part02ch04.xhtml#p.4.3)\n\n[P.4.3.1 Graphics from pixel values](12-part02ch04.xhtml#p.4.3.1)\n[204](12-part02ch04.xhtml#p.4.3.1)\n\n[P.4.3.2 Type from pixel values](12-part02ch04.xhtml#p.4.3.2)\n[210](12-part02ch04.xhtml#p.4.3.2)\n\n[P.4.3.3 Real-time pixel values](12-part02ch04.xhtml#p.4.3.3)\n[214](12-part02ch04.xhtml#p.4.3.3)\n\n[P.4.3.4 Emojis from pixel values](12-part02ch04.xhtml#p.4.3.4)\n[220](12-part02ch04.xhtml#p.4.3.4)\n\n**[A Appendix](13-appendix.xhtml#app) [226–256](13-appendix.xhtml#app)**\n\n[A.1 Looking ahead](14-appendix01.xhtml#a.1) [228](14-appendix01.xhtml#a.1)\n\n[A.2 Reflection](15-appendix02.xhtml#a.2) [244](15-appendix02.xhtml#a.2)\n\n[A.3 Bibliography](16-appendix03.xhtml#a.3) [250](16-appendix03.xhtml#a.3)\n\n[A.4 The authors](17-appendix04.xhtml#a.4) [252](17-appendix04.xhtml#a.4)\n\n[A.5 We thank](18-appendix05.xhtml#a.5) [253](18-appendix05.xhtml#a.5)\n\n[A.6 Copyright](19-appendix06.xhtml#a.6) [254](19-appendix06.xhtml#a.6)\n\n[A.7 Farewll](20-appendix07.xhtml#a.7) [256](20-appendix07.xhtml#a.7)\n\n\n## I.2 [www.generative-gestaltung.de](http://www.generative-gestaltung.de)\n\n_Generative Design: Visualize, Program, and Create with Javascript in p5.js_\nis a tried and tested tutorial that allows people to design with p5.js. It is\nnot necessary to type in any code: all the programs in the book, called\n“sketches,” can be downloaded for free from the book’s website for\nexperimentation. This symbol **→** indicates the name of a sketch in the\ndownload package.\n\nThe code summary page shows the main features of the code and how it affects\nthe program output. The book explains how the parameters of the code impact\nthe outcome and how users can interact with the sketch to develop their own\nvisual solutions.\n\n![image](../images/f0008-01.jpg)\n\n![image](../images/f0009-01.jpg)\n\nHello and welcome to Generative Design. Here, you will find all of the\nsketches from the book and their associated code. Run the sketches directly in\nthe browser with the p5.js-web-editor or locally on your machine by\ndownloading the code package below.\n\n**![image](../images/arrowdown.jpg) Download Code Package**\n\n## Library\n\n**P.1. Color**\n\n![image](../images/f0009-02.jpg)\n\n**![image](../images/arrowright.jpg)P_1_0_01**\n\n![image](../images/f0009-03.jpg)\n\n**![image](../images/arrowright.jpg)P_1_1_1_01**\n\n![image](../images/f0009-04.jpg)\n\n**![image](../images/arrowright.jpg)P_1_1_2_01**\n\n\n## I.3 Projects\n\nThese thirteen works by various media artists, designers, and architects\nactive in the field of generative design are intended to serve as a\nrepresentative overview of the topic and a source of inspiration.\n\n### 01\n\n#### 2016/17 **Daily Sketches**\n\nZach Lieberman\n\n![image](../images/f0011-01.jpg)\n\n**Daily Sketches** is a series of short generative animations shared daily\nwith the world by Zach Lieberman on social media for fast feedback. The\nsketches show the process of exploring new visual concepts using geometry,\nanimation, gesture, form, and code. Lieberman describes his sketches of the\nday as follows: “A lot of times, as artists, we feel like we’re struggling to\nfind our frequencies and what resonates with the frequencies of the world.\nThis act of sketching is a kind of tuning of these frequencies.”\n\n![image](../images/f0012-01.jpg)\n\n![image](../images/f0013-01.jpg)\n\n![image](../images/f0014-01.jpg)\n\n**Design and Development**  \nShota Matsuda (Takram)\n\n**Photo Credits**  \nKoki Nagahama\n\n**Planck** is a web browser–based framework developed by Takram for the\nvisualization of large geographical data sets. The framework is designed to\nachieve both analytical and immersive visual experience by using various\ntechniques, such as parallel projection and depth of field. Three\nvisualizations were created for Media Ambition Tokyo 2017, presenting data on\nJapan’s estimated population in 2050, the languages people have tweeted in,\nand world air traffic.\n\n### 02\n\n#### 2017 **Planck**\n\nTakram\n\n![image](../images/f0015-01.jpg)\n\n![image](../images/f0016-01.jpg)\n\n**Interaction Design**  \nBjørn Karmann\n\n**Fashion Design**  \nJulie Helles\n\n**Textile Design**  \nKristine Boesen\n\n**Bachelor‘s Degree Graduation Project**  \nKolding Design School, DK\n\nFashion has always been a means of self-expression, but **Abstract_** takes\nindividuality a step further by enabling a customer to write themselves into a\npiece of clothing. Abstract_’s interactive platform prompts the customer to\nwrite a personal story and uses the computer’s camera to capture facial\nexpressions. Data collected from the story, the rhythm of the keystrokes, and\nthe customer’s expression are then transformed into a visual representation\nand mapped onto a textile for clothing.\n\n### 03\n\n#### 2015 **Abstract_**\n\nBjørn Karmann  \nJulie Helles  \nKristine Boesen\n\n![image](../images/f0017-01.jpg)\n\n### 04\n\n#### 2016 **Rottlace – Björk**\n\nMIT Media Matter Group  \nChristoph Bader  \nDominik Kolb  \nProf. Neri Oxman  \nStratasys Ltd.\n\n![image](../images/f0018-01.jpg)\n\n![image](../images/f0019-01.jpg)\n\n**Photo Credits**  \nSantiago Felipe\n\n**Rottlace** is part of a family of masks designed for the Icelandic singer-\nsongwriter Björk. The design is informed by the geometric and material logic\nthat underlies the human musculoskeletal system. The masks can be understood\nas “muscle textile”: bundled, continuous multimaterial structures providing\nformal and structural integrity as well as movement to the face and neck,\nresulting in an object that is designed as a synthetic whole without parts.\n\n### 05\n\n#### 2016 **VOID VIII 01**\n\nAndreas Nicolas Fischer\n\n![image](../images/f0020-01.jpg)\n\n![image](../images/f0021-01.jpg)\n\n**Primary Programming**  \nAndreas Nicolas Fischer and Benjamin Maus\n\n**Additional Programming**  \nAbraham Pazos Solatie\n\n**VOID** is a series of images created with custom generative software. A\nswarm of particles governed by an algorithm moves over a surface, leaving\nbehind colorful traces. Over time, this results in an abstract composition\nthat develops in unpredictable ways.\n\n![image](../images/f0022-01.jpg)\n\nCommissioned by and in collaboration with Monotype\n\n**Monotype: Type Reinvented** is a series of three typographic installations\nthat reflect how type can become “smart, dynamic, and emotional” through new\ndigital approaches using interactivity, generative design, and data input.\nPopular Monotype typefaces are recontextualized and reimagined in new spaces\nand materials.\n\n### 06\n\n#### 2017 **Monotype: Type Reinvented**\n\nField\n\n![image](../images/f0023-01.jpg)\n\n![image](../images/f0024-01.jpg)\n\n**Client**  \nNike Global Football\n\n**Nike Strike Series FA16** is a series of short films and still imagery\ncomposed of inspiring 3D renderings that exemplify the power and precision of\nworld-class athletes. Full-body scans were taken of professional football\nplayers to create specialized 3D models that manifest the true essence of\nthese athletes. The data of their distinctive characteristics, such as speed,\nvelocity, and physical skills, defined the visual execution of the players.\n\n### 07\n\n#### 2016 **Nike Strike Series FA16**\n\nOnformative\n\n![image](../images/f0025-01.jpg)\n\n![image](../images/f0026-01.jpg)\n\nUCL Design Computational Lab\n\n**Design**  \nManuel Jimenez García and Gilles Retsin\n\n**Fabrication Support**  \nNagami Design, Vicente Soler\n\n**Team**  \nManuel Jimenez García, Miguel Angel Jimenez García, Ignacio Viguera Ochoa,\nGilles Retsin, Vicente Soler\n\n**VoxelChair** is the first prototype resulting from new and ongoing software\ninnovation and development for robotic 3D printing. The software behind the\nVoxelChair draws on methodologies ranging from computational architecture to\nmedical imaging to create new opportunities for designing and printing 3D\ncompositions and structures. The creators suggest that “instead of designing\nthe form of the chair, designers should design the behaviors and properties of\nthe material directly.”\n\n### 08\n\n#### 2016 **VoxelChair v.1.0**\n\nManuel Jimenez García  \nGilles Retsin  \nNagami Design  \nVicente Soler  \nUCL Design Computational Lab\n\n![image](../images/f0027-01.jpg)\n\n![image](../images/f0028-01.jpg)\n\n![image](../images/f0030-01.jpg)\n\n**Client**  \nDolby Laboratories\n\n**Collide** is an art installation that transforms recorded motion data into\nabstract graphics and sound. Surreal imagery and an engaging sound-scape\ncreate an immersive space that captures the essence of motion, color, and\nsound to represent the experience of “letting go.” Inspired by the phenomenon\nof synesthesia, the union of the senses, Collide creates a new language by\ncombining original chamber music and painterly visuals.\n\n### 09\n\n#### 2016 **Collide: synaesthetic art installation**\n\nOnformative\n\n![image](../images/f0031-01.jpg)\n\n### 10\n\n#### 2017 **Block Bills**\n\nMatthias Dörfelt\n\n![image](../images/f0032-01.jpg)\n\n![image](../images/f0033-01.jpg)\n\nArchival digital print on paper, 3.3 x 5.9 in.\n\n**Block Bills** is a series of sixty-four generated banknotes representing\nblocks in a Bitcoin blockchain. A visual system defined by the artist Matthias\nDörfelt uses both randomness and the unique aspects of each block in the\nselected blockchain to affect every aspect of each bill in the series. The\nbills are allusions to Dörfelt’s ambivalence toward Bitcoin and\ncryptocurrencies; they bring these otherwise intangible and invisible\nconstructions into a visually and emotionally tangible context.\n\n![image](../images/f0034-01.jpg)\n\n**Map and Road Network Data**  \nOpenStreetMap\n\n**Routing Engine**  \nGraphHopper\n\n**Roads to Rome** is a data visualization project that explores the idiom “All\nroads lead to Rome.” The outcome is both information visualization and data\nart and unveils mobility patterns at a very large scale. The visualizations\nwere created using routing algorithms on existing street infrastructure\n(OpenStreetMap) from the city to continent scale. In addition to their\naesthetic quality, the resulting images bring insights into the ways in which\nroad infrastructures reflect regional, political, and geographical situations.\n\n### 11\n\n#### 2015 **Roads to Rome**\n\nBenedikt Groß  \nPhilipp Schmitt  \nRaphael Reimann  \nmoovel lab\n\n![image](../images/f0035-01.jpg)\n\n### 12\n\n#### 2015 **Jller**\n\nBenjamin Maus  \nProkop Bartoníček\n\n![image](../images/f0036-01.jpg)\n\n![image](../images/f0037-01.jpg)\n\n**Additional Mechanics and Electronics**  \nTomislav Arnaudov\n\n**Developed at**  \npebe/lab (Prague) and FELD (Berlin)\n\n**Jller** is part of an ongoing research project in the fields of industrial\nautomation and historical geology. This apparatus uses computer vision to sort\npebbles from a specific river by their geologic age. The installation displays\nnatural geological history in an automated sorting process that constitutes a\nperformance in its own right.\n\n![image](../images/f0038-01.jpg)\n\n### 13\n\n#### 2016 **Aerial Bold**\n\nBenedikt Groß  \nJoey Lee\n\n![image](../images/f0040-01.jpg)\n\n![image](../images/f0041-01.jpg)\n\n**Font Design**  \nMarco Berends\n\n**Machine Learning MSc Thesis**  \nAnkita Agrawal, Institute for Artificial Intelligence, HS Ravensburg-\nWeingarten\n\n**Funding and Support**  \nKickstarter Backers\n\n**Aerial Imagery**  \nUSGS (United States Geological Survey)\n\nContrary to popular belief, much of the world has yet to be fully mapped.\nEvery day, satellites orbit the earth, taking countless pictures, yet there is\nvery limited knowledge about what exactly was captured among the unique things\nin these pictures and how they can be found and classified. **Aerial Bold** —a\nplanet-wide letter search mission—questions this and draws attention to this\npotential.\n\n\n![image](../images/f0042-01.jpg)\n\n# P.0 Introduction to p5.js\n\n[**P.0 Introduction to p5.js**](08-part02ch00.xhtml#p.0)\n[42](08-part02ch00.xhtml#p.0)\n\n[P.0.0 p5.js, JavaScript, and Processing](08-part02ch00.xhtml#p.0.0)\n[44](08-part02ch00.xhtml#p.0.0)\n\n[P.0.1 The development environment](08-part02ch00.xhtml#p.0.1)\n[46](08-part02ch00.xhtml#p.0.1)\n\n[P.0.2 Language elements](08-part02ch00.xhtml#p.0.2)\n[48](08-part02ch00.xhtml#p.0.2)\n\n[P.0.3 Programming beautifully](08-part02ch00.xhtml#p.0.3)\n[56](08-part02ch00.xhtml#p.0.3)\n\n## **P.0.0 p5.js, JavaScript, and Processing**\n\nThe JavaScript library p5.js is used in this book to introduce the concepts of\ngenerative design. Presented on the following pages is an overview of the\nbasic elements and functions of JavaScript and a brief outline of the history\nof Processing and p5.js.\n\nThe project p5.js was started in August 2013 by Lauren McCarthy, an artist and\nassistant professor in UCLA’s Design Media Arts program. To explain the idea\nbehind p5.js, it is useful to start by referencing the much older Processing\nproject.\n\nThe programming language Processing was initiated in the spring of 2001 by Ben\nFry and Casey Reas with a small group of assistants. The main aim of\nProcessing was and still is to give visually oriented people easy access to\nprogramming. Processing is a tool or development environment for the quick\ncreation of digital, programmed sketches (a Processing program is called a\n“sketch”).1 There is a large, vibrant community surrounding Processing that\ngreatly simplifies collaborative learning. There are many examples, videos,\ntutorials, et cetera, available online. Processing has become standard for\nprogramming in design, infographics, architecture, and art.\n\n**[processingfoundation.org](http://processingfoundation.org)**\n\n**Casey Reas and Ben Fry**\n\np5.js pursues the same goal: making it as easy as possible to program\ngraphics. The individual commands of p5.js and Processing are extremely\nsimilar. The main difference is that unlike Processing, p5.js is based not on\nJava programming but on JavaScript. This is technically significant as well as\nimportant for users because JavaScript is the programming language that runs\nin web browsers and makes websites dynamic and interactive. The programs\ncreated with the help of p5.js can be used directly on the web.\n\np5.js is an open-source project; it can be downloaded for free and used for\nindependent projects. Since p5.js is a JavaScript library and JavaScript is\nthe most widely used programming language of the internet age, it is easy to\ntranslate the code examples to most other development environments. JavaScript\nis cross-platform, so the same source code can be used on all operating\nsystems for which a modern browser or a corresponding runtime environment\nexists—not only on computers but also on smartphones and tablets.\n\nAdditionally, there is an enormous online community surrounding p5.js and, to\nan even greater extent, JavaScript. There is an online answer to almost any\nJavaScript-related question or problem. The internet is full of other\nJavaScript libraries, which can usually be combined easily with p5.js.\n\nFor the programs in this book, we use our own program library, which can be\nfound on the book’s website; we also have provided a list of external\nlibraries. For example, you can use the generative-design-library to work on\nyour graphics tablet or create color palettes in Adobe ASE format.\n\n**[generative-gestaltung.de](http://generative-gestaltung.de)**\n\n**generative-design-library**\n\n**Palette exporting P.1.2**\n\nColor palettes\n\n**Writing Programs** p5.js uses the JavaScript programming language, but it is\nsimpler than pure JavaScript. The most important commands, especially those\nfor graphical output (such as drawing a circle), are very easy to use. The\nbeginner is not burdened with additional constructions that would otherwise be\nnecessary to display graphic elements in the browser window.\n\nOn the following pages we will demonstrate how to set up a work space—the\ndevelopment environment—and introduce the most important language elements and\nprogramming concepts of p5.js. You can start immediately by typing the\ncommands presented and seeing what those commands generate.\n\n## **P.0.1 The development environment**\n\nItching to get started? Good! Here’s how to try out our programs, expand on\nthem, or create completely new ones. Just download our code package with all\nthe programs of the book. The link can be found at [generative-\ngestaltung.de](http://generative-gestaltung.de).\n\n**Variation 1: The p5.js Web Editor** This is the easiest way to start. All\nyou need is a browser and access to the internet.\n\n**1** | Open the online editor of p5.js. This provides a comfortable environment in which to try something quickly. In the browser window, there is an area to edit the code on the left side. There are play and stop buttons to start and stop the program; a canvas on the right where your sketch will be rendered; and, at the bottom, the console, which will display any error messages or other text output from the program. **[editor.p5js.org](http://editor.p5js.org)**  \n---|---  \n**2** | Upload the book’s programs to the online editor, copy and paste the text, or copy one of the sketches from the book’s collection.  \n  \n![image](../images/f0046-01.jpg)\n\nCode editor and output are united in the browser window. Several other online\ncode editors, such as Codepen.io or jsfiddle.net, are similar to the p5.js web\neditor, but they are more generic solutions and are not optimized for p5.js.\n\n**Variation 2: Code Editor and Browser** The web editor is a great way to\ninitially experiment with p5.js; in the long term, however, we recommend that\nyou set up a local working environment, as described below.\n\n**1** | Above all, a good code editor, such as Sublime Text, Atom, Brackets, or Coda, is essential.  \n---|---  \n**2** | Download our code package. The link can be found on the book’s website. **[generative-gestaltung.de](http://generative-gestaltung.de)**  \n**3** | You can now run the program by opening the index.html from one of the sketch folders (e.g., P_1_0_01) in the browser.  \n**4** | To modify a program, open the sketch.js file in a text editor, make some edits, and open the corresponding index.html in the browser. Any changes made in sketch.js must be saved before the browser content is updated. Reload the browser to view the changes.  \n| A more detailed guide to working with p5.js can be found on the p5.js\nwebsite. **[p5js.org/get-started](http://p5js.org/get-started)**  \n**!** | Some of our code samples need to be run on a server to work. These sketches use external resources such as a webcam or external files and must be executed by a web server (the URL of the sketch must start with “http” or the browser will prohibit the use of these resources). Detailed explanations of how to use them can be found on the book’s website.  \n  \n![image](../images/f0047-01.jpg)\n\nA code editor, for example Sublime Text. The editor is a separate program and\nis not integrated in the browser.\n\n![image](../images/f0047-02.jpg)\n\nThe file index.html, and thus the sketch, is opened directly in the browser\nfrom the hard drive. The URL therefore also starts with “file.”\n\n## **P.0.2 Language elements**\n\nTo be able to give instructions to a computer, you will need to speak its\nlanguage. In the following section, the most important functions and control\nstructures of JavaScript and p5.js will be introduced.\n\n**Hello, Ellipse** A first program. Open the sketch.js file (located in the\nempty-example folder), enter the following line within the draw() function,\nand open the associated index.html in a browser.\n\n**1**\n\nfunction draw() {\n\nellipse(50, 50, 80, 80);\n\n}\n\nThere are commands that draw, such as ellipse, rect, line, et cetera, and\ncommands that specify how the graphic that follows should be drawn, such as\nstroke, strokeWeight, noStroke, fill, noFill, et cetera. Once a drawing mode\nhas been configured, it will apply for all additional drawing commands until\nthe drawing mode is reconfigured. Most drawing commands require one or more\nparameters that indicate where something should be drawn and at what size. The\nunit of measure for this is the pixel. The origin of the coordinate system is\nlocated in the upper left corner of the drawing canvas.\n\n**1** | A circle is drawn on the canvas. A function is created by declaring the word “function” followed by the function name and a set of parentheses (). What the function should do—for example, draw a circle—is defined between the curly brackets {...}.  \n---|---  \n  \n**2**\n\npoint(60, 50);\n\nThe pixel generated by the command point(60, 50) is thus drawn sixty pixels\nfrom the left edge and fifty pixels from the upper edge.\n\n**2** | ![image](../images/f0048-01.jpg)  \n---|---  \n  \nOf course, it is possible to have more than one line of code. The individual\nlines are then processed sequentially from top to bottom. The following\ncommand lines are thus read by p5.js:\n\n**3**\n\nfill(128);\n\nstrokeWeight(1);\n\nellipse(40, 50, 60, 60);\n\nrect(50, 50, 40, 20);\n\nWith the commands, it is important to pay attention to the exact spelling,\nincluding uppercase and lowercase letters. For example, the strokeWeight()\ncommand would not be recognized if it were written as strokeweight() or\nStrokeWeight().\n\n**3** | Set the fill color to a medium gray. Set the line width to 1 pixel. Draw an ellipse at the point (40, 50) with width and height 60 pixels. Draw a rectangle to the coordinates (50, 50) with width 40 and height 20.  \n---|---  \n| ![image](../images/f0048-02.jpg)  \n---|---  \n  \n**Setup, Draw, and Preload** p5.js provides various functions that are called\nautomatically. The two most important are setup() and draw().\n\nThe draw() function is called in every drawing step and each command line is\nexecuted each time.\n\n**4**\n\nfunction draw() { }\n\n**4** | Although this draw() function contains no commands, it keeps the program running.  \n---|---  \n  \n**5**\n\nfunction draw() {\n\nconsole.log(frameCount);\n\n}\n\nThe draw() function is displayed with a preset frequency that specifies how\nmany images are shown per second. The standard number is set at sixty images\nper second, but this can be reset using the command frameRate().\n\n**5** | A draw() function with one command. The command console. log() writes text to the console. In this case, it displays the continually increasing number of the actual frame.  \n---|---  \n  \n**6**\n\nframeRate(30);\n\nHowever, if the computational effort per frame becomes so high that the\nbrowser is unable to execute it within the preset time, the set frame rate\ndecreases automatically.\n\nSome actions should only be performed once when the program is started and not\nrepeatedly in each frame. The setup() function is used for this.\n\n**6** | This drawing speed is set at 30 images per second.  \n---|---  \n  \n**7**\n\nfunction setup() {\n\nframeRate(30);\n\n}\n\nThe preload() function ensures that additional data are fully loaded when the\nprogram starts.\n\n**7** | These commands, which should be executed just once when the program is started, appear in the setup() function.  \n---|---  \n  \n**8**\n\nfunction preload(){\n\nimg = loadImage(\"data/pic1.jpg\");\n\n}\n\n**8** | In preload() the instructions for loading data are inserted; in this example an image should be loaded.  \n---|---  \n  \n**Drawing Canvas and Renderer** A drawing canvas is created within the browser\nwindow. This is the display window for the program’s visual output and can be\nscaled to any size.\n\n**9**\n\ncreateCanvas(640, 480);\n\nIn addition to the parameters’ width and height, using the command\ncreateCanvas() makes it possible to enter which renderer should be used for\ndisplaying the image. The renderer is responsible for how the different\ngraphic commands are actually translated into pixels. The following rendering\noptions are available:\n\n**9** | The drawing canvas is now 640 pixels wide and 480 pixels high.  \n---|---  \n  \n**10**\n\ncreateCanvas(640, 480, P2D);\n\n**11**\n\ncreateCanvas(640, 480, WEBGL);\n\n**10** | The standard renderer: this is used if nothing else has been specified.  \n---|---  \n**11** | Renderer for hardware-accelerated 3D graphics in the web browser.  \n  \n**Transformations** One of the strengths of p5.js is that it can move, rotate,\nand scale the coordinate system. All graphic commands thereafter refer to this\naltered coordinate system.\n\n**12**\n\ntranslate(40, 20);\n\nrotate(0.5);\n\nscale(1.5);\n\nIn this example, the coordinate system is moved 40 pixels to the right and 20\npixels downward, then rotated 0.5 radians (about 30°), and finally scaled by a\nfactor of 1.5.\n\nIn Processing, angles are generally shown in radians, in which 180° equals the\nnumber pi (≈ 3.14) and the rotation is in a clockwise direction.\n\n**12** | ![image](../images/f0050-01.jpg)  \n---|---  \n  \n**Variables and Data Types** In a program, information is stored in variables\nso that it is available to other parts of the program. These variables can\nhave any name other than the keywords for JavaScript keywords. Keywords can be\nrecognized by their special color in the editor.\n\n**13**\n\nvar myVariable;\n\nmyVariable = 5;\n\n**13** | The variable myVariable is created. The value 5 is then saved there.  \n---|---  \n  \nVariables can contain different data types. Unlike most other programming\nlanguages, JavaScript does not have to take this into account when creating a\nvariable. Nevertheless, it is important to keep track of which variables store\nwhich type of value. For example, data types can be:\n\n**14**\n\nvar myBooleanvalue = true:\n\n**15**\n\nvar myInteger = 7:\n\n**16**\n\nvar myFloatingPointNumber = -3.219:\n\n**17**\n\nvar myCharacter = \"A\":\n\n**18**\n\nvar myString = \"This is a text\":\n\n**14** | Logic values (Boolean values): true or false  \n---|---  \n**15** | Integers: e.g., 50, -532.  \n**16** | Floating point value: e.g., 0.02, -73.1.  \n**17** | A single character: e.g., “a”, “A”, “9”, “&”.  \n**18** | Character string/text: e.g., “Hello, world”.  \n  \n**Arrays** When working with a large number of values, it is inconvenient to\nhave to create a variable for each value. An array allows a list of values to\nbe managed.\n\n**19**\n\nvar planets = [\"Mercury\", \"Venus\", \"Earth\", \"Mars\".\n\n\"Jupiter\", \"Saturn\", \"Uranus\", \"Neptune\"]:\n\n**19** | Arrays are defined by their square brackets. Within these, any number of values (here the eight planet names) can be listed, separated by commas.  \n---|---  \n  \n**20**\n\nconsole.log(planet[0]):\n\n**20** | Values can be accessed by entering an index number in the square brackets following the variable name. The index 0 points to the first entry in the array.  \n---|---  \n  \nIf values are not immediately assigned to an array, the array can be created\nfirst and filled later:\n\n**21**\n\nvar planetDiameter = [];\n\nplanetDiameter[0] = 4879;\n\nplanetDiameter[1] = 12104;\n\nplanetDiameter[2] = 12756;\n\nplanetDiameter[3] = 6794;\n\n...\n\nArrays have many other functions for processing elements. In this book we\nmainly use <yourarray>.push() to add new elements to the end of an array.\n\n**21** | The square brackets initialize an empty array. Values are then assigned to the array. This could also happen later while the program is running.  \n---|---  \n  \n**22**\n\nplanet.push (\"Pluto\");\n\n**22** | Although Pluto is no longer considered a planet, its name can be added to the array planet in this way.  \n---|---  \n  \n**Objects** In addition to arrays, many pieces of information can be stored in\nobjects at one time. The difference is that individual values are not accessed\nvia a numeric index but via a key.\n\n**23**\n\nvar planet = {name: \"Saturn\", mass: 5.685e26,\n\ntemperature: 134};\n\nplanet.diameter = 120536;\n\nconsole.log(\"mass in kg: \" + planet.mass);\n\nAlternatively, a value can be accessed as follows. This is necessary if the\nkeys are not fixed from the beginning but are dynamically generated or read\nout.\n\n**23** | JavaScript objects are created by enclosing a set of key/value pairs within a set of curly brackets {}. Here an object with three key/value pairs is generated. Another key/value pair is added to the existing object, and one of the values is accessed and logged to the console.  \n---|---  \n  \n**24**\n\nvar k = \"mass\";\n\nconsole.log(\"mass in kg: \" + planet[k]);\n\n**24** | If the name of the key is stored in a variable, then the value can be accessed using that variable as a reference to that key.  \n---|---  \n  \n**Operators and Mathematics** Naturally, calculations can also be performed in\nProcessing. This is possible with simple numbers . . .\n\n**25**\n\nvar a = (4 + 2.3) / 7;\n\nwith strings ...\n\n**25** | After execution the value 0.9 is saved in a.  \n---|---  \n  \n**26**\n\nvar s = \"circumference of Jupiter: \" + (142984*PI) + \"km\";\n\n**26** | The variable s then contains the string “circumference of Jupiter: 449197.5 km”.  \n---|---  \n  \nand with variables:\n\n**27**\n\nvar i = myVariable * 50;\n\n**27** | The value in myVariable is multiplied by 50 and the result saved in i.  \n---|---  \n  \nThe following computational operators are available: +, -, *, /, %. A whole\nseries of mathematical functions are also available. Here are a few:\n\n**28**\n\nvar converted aValue = map(aValue, 10, 20, 0, 1);\n\n**29**\n\nvar roundedValue = round(2.67);\n\n**30**\n\nvar randomValue = random(-5, 5);\n\n**31**\n\nvar cosineValue = cos(angle);\n\n**28** | The value in aValue is converted from a number between 10 and 20 to a number between 0 and 1.  \n---|---  \n**29** | Round: roundedValue is 3.  \n**30** | A random number between -5 and 5.  \n**31** | Calculates the cosine of the angle.  \n  \n**Mouse and Keyboard** There are several ways to access information using the\nmouse and keyboard as input devices. One option is to query the variables\navailable in p5.js.\n\n**32**\n\nfunction draw() {\n\nconsole.log(\"mouseposition:\" + mouseX \\+ \",\" + mouseY);\n\nconsole.log(\"mousekey pressed?:\" + mouseIsPressed);\n\nconsole.log(\"key pressed?:\" + keyIsPressed);\n\nconsole.log(\"last pressed key:\" + key); }\n\nAnother possibility is to implement one of the event handlers. An event\nhandler is called when the corresponding event occurs—i.e., when a mouse\nbutton or key on the keyboard is pressed.\n\n**32** | The Processing variables mouseX and mouseY always contain the actual position of the mouse; mouseIsPressed is true if one of the mouse buttons is pressed in that moment. For the keyboard, keyIsPressed indicates if a key is pressed; the key last pressed appears in key.  \n---|---  \n  \n**33**\n\nfunction mouseReleased() {\n\nconsole.log(\"The mouse key was released\");\n\n}\n\nAdditional event handlers include mousePressed(), mouseMoved(), keyPressed(),\nand keyReleased().\n\n**33** | The function mouseReleased() is called when the left mouse button is released.  \n---|---  \n  \n**Conditions** It is often necessary to execute lines of code only when\ncertain conditions are met. The if statement is used to do this.\n\n**34**\n\nif (aNumber == 3) {\n\nfill(255, 0, 0);\n\nellipse(50, 50, 80, 80);\n\n}\n\n**35**\n\nif (aNumber == 3) {\n\nfill(255, 0, 0);\n\n} else {\n\nfill(0, 255, 0);\n\n}\n\n**36**\n\nif (aNumber == 3) fill(255, 0, 0);\n\nelse fill(0, 255, 0);\n\nDifferentiating between multiple values of a variable can usually be achieved\nby the switch command.\n\n**34** | The two lines of code inside the curly brackets are executed only when the condition inside the if statement is met—i.e., when the value of the variable aNumber is 3.  \n---|---  \n**35** | With else, the if condition is expanded by one code snippet that is executed when the specified if condition is not met.  \n**36** | If a code snippet consists of only one line, as in the previous example, the curly brackets can be eliminated.  \n  \n**37**\n\nswitch (aNumber) {\n\ncase 1:\n\nrect(20, 20, 80, 80);\n\nbreak;\n\ncase 2:\n\nellipse(50, 50, 80, 80);\n\nbreak;\n\ndefault:\n\nline(20, 20, 80, 80);\n\n}\n\n**37** | The switch command tests if the value of the variable aNumber corresponds to one of the values listed in the case lines, jumps there if it does, and continues code execution until the following break statement.  \n---|---  \n| If no corresponding value exists, program execution continues at default.  \n  \n**Functions** There are often parts of a program that appear in a similar way\nin different places. In many such cases it is wise to encapsulate these parts\nin a function. For example:\n\n**38**\n\nfunction draw() {\n\ntranslate(40, 15);\n\nline(0, -10, 0, 10);\n\nline(-8, -5, 8, 5);\n\nline(-8, 5, 8, -5);\n\ntranslate(20, 50);\n\nline(0, -10, 0, 10);\n\nline(-8, -5, 8, 5);\n\nline(-8, 5, 8, -5);\n\n}\n\nTo change the star’s appearance in this program, the drawing commands have to\nbe changed in two places. It is therefore better to store the corresponding\nlines in a function.\n\n**38** | The coordinate system is moved to another location, where a star is made of three lines. The coordinate system is then moved again, and a star is drawn using the same commands at the new location.  \n---|---  \n  \n**39**\n\nfunction draw() {\n\ntranslate(40, 15);\n\ndrawStar();\n\ntranslate(20, 50);\n\ndrawStar();\n\n}\n\nfunction drawStar() {\n\nline(0, -10, 0, 10);\n\nline(-8, -5, 8, 5);\n\nline(-8, 5, 8, -5);\n\n}\n\nValues can be passed in functions, which can also return a value as a result.\n\n**39** | The function drawStar() now contains the drawing commands. This function can be called from any other point in the program to execute the code contained there.  \n---|---  \n  \n**40**\n\nfunction setup() {\n\nconsole.log(\"The faculty of 5 is 1*2*3*4*5 = \"\n\n\\+ faculty(5));\n\n}\n\nfunction faculty(theValue) {\n\nvar result = 1;\n\nfor (var i = 1; i <= theValue; i++) {\n\nresult = result * i;\n\n}\n\nreturn result;\n\n}\n\nIn JavaScript, it is particularly common for functions to accept as one of its\narguments another function called a “callback function.” Callback functions\nallow actions to occur when triggered by an event.\n\n**40** | A function faculty() is defined here, in which a value is passed, theValue.  \n---|---  \n| When the function is called, the parameter, 5 in this case, is passed in the\nvariable theValue.  \n| In the function, various calculations are executed with this value, and the\nresult is returned using the command return.  \n  \n**41**\n\nloadJSON(\"myData.json\", callback);\n\nfunction callback(data) {\n\nconsole.log(data);\n\n}\n\n**41** | When the p5.js function loadJSON() is used in this way, the program flow is not blocked by the loading process but runs in the background (asynchronous loading). When the file is fully loaded, the callback() function is called.  \n---|---  \n  \n**Loops** Loops are used to execute a particular command several times within\na program. You can program loops in several ways:\n\nThe for loop is used to loop a code snippet for a specific number of\nrepetitions.\n\n**42**\n\nfor (var i = 0; i <= 5; i++) {\n\nline(0, 0, i * 20, 100);\n\nline(100, 0, i * 20, 100);\n\n}\n\n**42** | The two lines of code inside the curly brackets are executed exactly six times. First the variable i is set to the value 0, then increased by 1 (i++) after each cycle, as long as the value is 5 or less.  \n---|---  \n  \n**43**\n\nvar myvalue = 0;\n\nwhile (myValue < 100) {\n\nmyValue = myValue + random(5);\n\nconsole.log(\"The value of myValue is \" + myValue); }\n\n**43** | This while loop will continue as long as the value of the variable myValue is less than 100. A random value between 0 and 5 is added in each new loop cycle.  \n---|---  \n  \nA while loop will continue as long as a certain condition is fulfilled.\n\nThere are several ways to go through all the values in an array. Some of the\nusual methods are shown here:\n\nvar planets = [\"Mercury\", \"Venus\", \"Earth\", \"Mars\",\n\n\"Jupiter\", \"Saturn\", \"Uranus\", \"Neptune\"];\n\n**44**\n\nfor (var i = 0; i < planets.length; i++) {\n\nconsole.log(planets[i]);\n\n}\n\n**45**\n\nplanet.forEach(function(planet) {\n\nconsole.log(planet);\n\n});\n\n**44** | Variation 1: The for loop runs if the variable i is smaller than the length of the array.  \n---|---  \n**45** | Variation 2: The array function forEach() is used here. This requires a callback function, which is called in every iteration of the loop. The variable planet contains the individual values of the array successively.  \n  \nIn order to traverse all key/value pairs of an object, the following variation\nof the for loop is suitable:\n\nvar planet = {name: \"Saturn\", mass: 5.685e26,\n\ntemperature: 134, diameter: 120536};\n\n**46**\n\nfor (k in planet) {\n\nconsole.log(\"Key: \" + k + \", Value: \" + planet[k]);\n\n}\n\n**46** | The variable k contains each key of the object in succession. This can be used to access the associated values.  \n---|---  \n  \n## **P.0.3 Programming beautifully**\n\nIn programming there are usually many ways to attain a desired result. The\nfact that a program works, however, is just one part of the solution, albeit a\nvery important one; there are other aspects you should consider.\n\n**Comments** The trickier and more complex a program is, the harder it is for\nothers—and for you, too, after a short time—to understand it. A program that\nis not accessible cannot be modified or updated. Comments in the program help\nto clarify the code.\n\n**1**\n\n// calculate distance between actual and previous mouse\n\n// position, which represents the speed of the mouse\n\nvar speed = dist(mouseX, mouseY, pmouseX, pmouseY);\n\nThe p5.js community is spread across the entire world. It is therefore\nadvisable to write comments and variable names in English. This makes it\neasier for you and others to look for and find suggestions and answers to\nproblems on internet forums.\n\n**1** | Text that follows two forward slashes is ignored and can be used for comments.  \n---|---  \n  \n**Useful Names and Clear Structures** In addition to the use of comments,\nsensibly selected variable and function names help users to maintain an\noverview and to understand what a specific program does.\n\n**2**\n\nfunction mixer(apples, oranges) {\n\nvar juice = (apples + oranges) / 2;\n\nreturn juice;\n\n}\n\nTo this end, it is necessary to structure the program into smaller, logical\nsections with the help of functions and classes. This encapsulation of\nfunctionality also means that the program can be quickly modified and\nextended.\n\n**2** | In this example it is difficult to recognize just what the function calculates. The fact that the average of two numbers is calculated can only be guessed from the formula.  \n---|---  \n  \n**Performance** Even though computers are becoming faster and faster, it still\ntakes time to execute a command. Even short periods of time add up noticeably\nwhen a command is executed often. Try not to burden the program with\nunnecessary work.\n\n**3**\n\nfor (var i = 0; i < 10000000; i++) {\n\nvar speed = dist(mouseX, mouseY, pmouseX, pmouseY);\n\ndoSomethingWithMouseSpeed(speed);\n\n}\n\n**4**\n\nvar speed = dist(mouseX, mouseY, pmouseX, pmouseY);\n\nfor (var i = 0; i < 10000000; i++) {\n\ndoSomethingWithMouseSpeed(speed);\n\n}\n\nWith the programs in this book, we have tried to uphold the aforementioned\nprinciples. Sometimes, however, clarity and performance contradict each other,\nmeaning that program code optimized for performance can be more difficult to\nunderstand. In such cases we have opted for clearer structures.\n\n**3** | Here mouseSpeed is calculated in all ten million loops even though the mouse position cannot be changed during this process.  \n---|---  \n**4** | Performance is therefore improved enormously when the mouse speed is calculated outside the loop.  \n  \n1 A very good and detailed introduction to programming with Processing can be\nfound in the book _Processing: A Programming Handbook for Visual Designers and\nArtists_.\n\n\n![image](../images/f0058-01.jpg)\n\n# P.1 Color\n\nIn contrast to this book, the colors of which we perceive through reflected\nand filtered light, computers are true quantum catapults. When we look at a\nscreen, light is sent directly into our eyes in various wavelengths and can be\nmanipulated in real time. The following examples provide users with the most\nimportant technical features for working with color and introduce several ways\nto use color when designing on the screen.\n\n[**P.1 Color**](09-part02ch01.xhtml#p.1) [58](09-part02ch01.xhtml#p.1)\n\n[P.1.0 Hello, color](09-part02ch01.xhtml#p.1.0)\n[60](09-part02ch01.xhtml#p.1.0)\n\n[P.1.1 Color spectrum](09-part02ch01.xhtml#p.1.1)\n[62](09-part02ch01.xhtml#p.1.1)\n\n[P.1.1.1 Color spectrum in a grid](09-part02ch01.xhtml#p.1.1.1)\n[62](09-part02ch01.xhtml#p.1.1.1)\n\n[P.1.1.2 Color spectrum in a circle](09-part02ch01.xhtml#p.1.1.2)\n[64](09-part02ch01.xhtml#p.1.1.2)\n\n[P.1.2 Color palettes](09-part02ch01.xhtml#p.1.2)\n[66](09-part02ch01.xhtml#p.1.2)\n\n[P.1.2.1 Color palettes through interpolation](09-part02ch01.xhtml#p.1.2.1)\n[66](09-part02ch01.xhtml#p.1.2.1)\n\n[P.1.2.2 Color palettes from images](09-part02ch01.xhtml#p.1.2.2)\n[68](09-part02ch01.xhtml#p.1.2.2)\n\n[P.1.2.3 Color palettes from rules](09-part02ch01.xhtml#p.1.2.3)\n[72](09-part02ch01.xhtml#p.1.2.3)\n\n## P.1.0 Hello, color\n\nThe ability to directly influence 16,777,216 colors gives users an amazing\nfreedom. Simultaneous contrast—without which it would be impossible to\nperceive colors—is illustrated here by juxtaposing a number of color\ncombinations. Our perception of a color is affected by its neighboring color\nand the shifting proportions of that color to its background.\n\n**→ P_1_0_01**\n\nThe horizontal position of the mouse controls the size of the color field.\nStarting in the center, the colored area is depicted with a height and width\nof 1 to 720 pixels. The vertical mouse position controls the hue. The\nbackground passes through the color spectrum from 0 to 360, while the color\nfield passes through the spectrum in the opposite direction, from 360 to 0.\n\n![image](../images/f0060-01.jpg)\n\n**1**\n\nfunction setup() {\n\ncreateCanvas(720, 720);\n\nnoCursor();\n\n**2**\n\ncolorMode(HSB, 360, 100, 100);\n\nrectMode(CENTER);\n\nnoStroke();\n\n}\n\n**1** | The setup() function sets the size of the display window and makes the cursor invisible.  \n---|---  \n**2** | The colors should pass through the hue spectrum in this program. For this reason, colorMode() allows users to change the way color value is interpreted. HSB specifies the color model, and the three values following it specify the respective range. Hue, for example, can only be specified by values between 0 and 360.  \n  \nfunction draw() {\n\n**3**\n\nbackground(mouseY / 2, 100, 100);\n\n**4**\n\nfill(360 - mouseY / 2, 100, 100);\n\n**5**\n\nrect(360, 360, mouseX + 1, mouseX \\+ 1);\n\n}\n\n**3** | The y-value of the mouse position is divided by 2 to get values from 0 to 360 on the color wheel.  \n---|---  \n**4** | The halved y-value of the mouse position is subtracted from 360, creating values from 360 to 0.  \n**5** | The size of the color field changes relative to the x-value of the mouse position, with a side length between 1 and 720 pixels.  \nMouse: | Position x: Size of rectangle  \n---|---  \n| Position y: Hue  \nKeys: | S: Save image  \n  \n![image](../images/f0061-01.jpg)\n\n**→ P_1_0_01** The x-value of the mouse position defines the size of the\ninside color field, the y-value the hue.\n\n## P.1.1.1 Color spectrum in a grid\n\nThis color spectrum is composed of colored rectangles. Each tile is assigned a\nhue on the horizontal axis and a saturation value on the vertical. The color\nresolution can be reduced by enlarging the rectangles so that the primary\ncolors in the spectrum become clearer.\n\n**→ P_1_1_1_01**\n\nThe grid is created by two nested for loops. In the outer loop, the y-position\nis increased, step by step. The inner loop then draws a line by increasing the\nvalue for the rectangle’s x-position, step by step, until the entire width is\nprocessed. The step size is set by the value of the mouse position and is\nlocated in the variables stepX and stepY. It also determines the length and\nwidth of the rectangles.\n\n![image](../images/f0062-01.jpg)\n\n![image](../images/f0062-02.jpg)\n\n**→ P_1_1_1_01** Although the distance of the color values is the same between\nall the color tiles, the contrasts are perceived as greater in some positions\nthan in others.\n\nvar stepX;\n\nvar stepY;\n\nfunction setup() {\n\n**1**\n\ncreateCanvas(800, 400);\n\nnoStroke();\n\n**2**\n\ncolorMode(HSB, width, height, 100);\n\n}\n\n**1** | The display size is set using createCanvas(). The values defined here can be retrieved at any time using the system variables width and height.  \n---|---  \n**2** | The value range for hue and saturation is set at 800 or 400 using the command colorMode(). Hue is no longer defined as a number between 0 and 360 but rather as one between 0 and 800. The same is true of the saturation value.  \n  \nfunction draw() {\n\n**3**\n\nstepX = mouseX \\+ 2;\n\nstepY = mouseY \\+ 2;\n\n**4**\n\nfor (var gridY = 0; gridY < height; gridY += stepY) {\n\nfor (var gridX = 0; gridX < width; gridX += stepX) {\n\n**5**\n\nfill(gridX, height \\- gridY, 100);\n\nrect(gridX, gridY, stepX, stepY);\n\n}\n\n}\n\n}\n\n**3** | The addition of 2 prevents stepX or stepY from being too small, which would lead to longer display times.  \n---|---  \n**4** | With the help of two nested loops, all the positions in the grid will now be processed. The y-position of the rectangle to be drawn is defined by gridY in the outer loop. The value increases only when the inner loop has been processed (i.e., once a complete row of rectangles has been drawn).  \n**5** | The variables gridX and gridY are used not only to position the tile but also to define the fill color. The hue is determined by gridX. The saturation value decreases proportionally to increases in the value gridY.  \nMouse: | Position x/y: Grid resolution  \n---|---  \nKeys: | S: Save image  \n  \n![image](../images/f0063-01.jpg)\n\n**→ P_1_1_1_01** A soft rainbow effect occurs at full resolution.\n\n![image](../images/f0063-02.jpg)\n\n**→ P_1_1_1_01** The primary colors of a computer monitor—red, green, and\nblue—in various gradations.\n\n![image](../images/f0063-03.jpg)\n\n**→ P_1_1_1_01** The secondary colors are also visible at this resolution.\n\n## P.1.1.2 Color spectrum in a circle\n\nThere are numerous models for organizing colors. The color spectrum arranged\nin a circle (a color wheel) is a popular model for comparing harmonies,\ncontrasts, and color tones. You can control the number of circle segments, as\nwell as their brightness and saturation values, allowing you to better\nunderstand the color arrangement in HSB mode.\n\n**→ P_1_1_2_01**\n\nThe color-wheel segments are arranged in the shape of a fan. The individual\nvertices are computed from the cosine and sine values of the corresponding\nangle. A Processing method mode can be used that makes it especially easy to\ncreate the wheel segments. The individual points have to be set in the\nfollowing order: first the middle point, and then the outer ones sequentially.\n\n![image](../images/f0064-01.jpg)\n\n![image](../images/f0064-02.jpg)\n\n**→ P_1_1_2_01** Segment count 45, key 2.\n\n![image](../images/f0064-03.jpg)\n\n**→ P_1_1_2_01** Segment count 12, key 4.\n\n![image](../images/f0064-04.jpg)\n\n**→ P_1_1_2_01** Segment count 6, key 5.\n\n**1**\n\nfunction draw() {\n\ncolorMode(HSB, 360, width, height);\n\nbackground(360, 0, height);\n\n**2**\n\nvar angleStep = 360 / segmentCount;\n\nbeginShape(TRIANGLE_FAN);\n\n**3**\n\nvertex(width / 2, height / 2);\n\nfor (var angle = 0; angle <= 360; angle += angleStep) {\n\n**4**\n\nvar vx = width / 2 + cos(radians(angle)) * radius;\n\nvar vy = height / 2 + sin(radians(angle)) * radius;\n\nvertex(vx, vy);\n\n**5**\n\nfill(angle, mouseX, mouseY);\n\n}\n\n**6**\n\nendShape();\n\n}\n\n**1** | The ranges of values for saturation and brightness are adjusted in such a way that mouse coordinates can be taken as their values.  \n---|---  \n**2** | The angle increment angleStep depends on how many segments are to be drawn (segmentCount).  \n**3** | The first vertex point is in the middle of the drawing canvas.  \n**4** | For the other vertices, angle has to be converted from degrees (0–360) to radians (0–2π) because the functions sin() and cos() require the angle be input this way. This conversion is done by using radians().  \n**5** | The fill color for the next segment is defined: the value of angle as hue, mouseX as saturation, and mouseY as brightness.  \n**6** | The construction of the color segment is ended with endShape().  \n  \nfunction keyPressed() {\n\n...\n\n**7**\n\nswitch (key) {\n\ncase '1':\n\nsegmentCount = 360;\n\nbreak;\n\ncase '2':\n\nsegmentCount = 45;\n\nbreak;\n\ncase '3':\n\n**8**\n\nsegmentCount = 24;\n\nbreak;\n\ncase '4':\n\nsegmentCount = 12;\n\nbreak;\n\ncase '5':\n\nsegmentCount = 6;\n\nbreak;\n\n}\n\n}\n\n**7** | The switch() command checks the last key pressed, which enables easy switching between different cases.  \n---|---  \n**8** | If the key 3 was pressed, e.g., then segmentCount is set to the value 24.  \nMouse: Keys: | Position x: Saturation Position y: Brightness 1–5: Segment count S: Save image  \n---|---  \n  \n## P.1.2.1 Color palettes through interpolation\n\nIn each color model, colors have their clearly defined place. The direct path\nfrom one color to another always has precisely definable gradations, which\nwill vary depending on the specific model. Using this interpolation you can\ncreate color groups in every gradation, as well as locate individual\nintermediate nuances.\n\n**→ P_1_2_1_01**\n\nBecause a color is not defined by a single number but by several values, it is\nnecessary to interpolate between these values. Depending on the chosen color\nmodel, RGB or HSB, the same color is defined by different values, thereby\ncausing the path from one to another to lead past different colors. In the HSB\ncolor model, for instance, a detour is made past the color wheel. This\ndifference is due to the characteristics of the color models, both of which\ncan be very useful, depending on the situation. It is therefore important to\nchoose the appropriate color model to solve a specific problem.\n\n![image](../images/f0066-01.jpg)\n\n![image](../images/f0066-02.jpg)\n\n**→ P_1_2_1_01** Interpolation in the RGB color space. In each row, two colors\nare interpolated in nine steps.\n\nfunction draw() {\n\n**1**\n\ntileCountX = int(map(mouseX, 0, width, 2, 100));\n\ntileCountY = int(map(mouseY, 0, height, 2, 10));\n\nvar tileWidth = width / tileCountX;\n\nvar tileHeight = height / tileCountY;\n\nvar interCol;\n\n...\n\n**2**\n\nfor (var gridY = 0; gridY < tileCountY; gridY++) {\n\n**3**\n\nvar col1 = colorsLeft[gridY];\n\nvar col2 = colorsRight[gridY];\n\nfor (var gridX = 0; gridX < tileCountX; gridX++) {\n\nvar amount = map(gridX, 0, tileCountX - 1, 0, 1);\n\n...\n\n**4**\n\ninterCol = lerpColor(col1, col2, amount);\n\n...\n\nfill(interCol);\n\nvar posX = tileWidth * gridX;\n\nvar posY = tileHeight * gridY;\n\nrect(posX, posY, tileWidth, tileHeight);\n\n...\n\n}\n\n}\n\n}\n\n**1** | The number of color gradations tileCountX and the number of rows tileCountY are determined by the position of the mouse.  \n---|---  \n**2** | Drawing the grid row by row.  \n**3** | The colors for the left and right columns are set in the arrays colorsLeft and colorsRight.  \n**4** | The intermediary colors are calculated with lerpColor(). This function performs the interpolation between the individual color values. The variable amount, a value between 0 and 1, specifies the position between the start and end color.  \n**!** | In all programs that are described in the color palette chapters, a color palette in ASE format for Adobe applications can be saved using the C key.  \nMouse: Keys: | Left click: New random color set Position x: Resolution Position y: Number of rows 1–2: Interpolation style S: Save image C: Save ASE palette  \n---|---  \n  \n![image](../images/f0067-01.jpg)\n\n**→ P_1_2_1_01** Interpolation in the HSB color space. Although the colors at\nthe beginning and end of the rows are the same as those in the image opposite,\ncompletely different intermediary color steps are generated.\n\n## P.1.2.2 Color palettes from images\n\nWe constantly are surrounded by color palettes; we need only to record and\nevaluate them. The colors obtained from the photograph of one individual’s\nwardrobe—a very personal color palette—are selected and sorted using the\nfollowing program. You can export the resulting color collection and use it as\nan inspirational color palette.\n\n**→ P_1_2_2_01**\n\nThe pixels of a loaded image are scanned using the mouse position in a\nspecific grid spacing, one by one and row for row, in order to define the\nrespective color value. These values are stored in an array and can be sorted\nby hue, saturation, brightness, or gray value.\n\n![image](../images/f0068-01.jpg)\n\n![image](../images/f0068-02.jpg)\n\n**→ Fotografie: Stefan Eigner** Original image: Subway tunnel.\n\n![image](../images/f0068-03.jpg)\n\n**→ P_1_2_2_01** Pixels arranged according to hue.\n\n![image](../images/f0068-04.jpg)\n\nPixels arranged according to saturation.\n\n![image](../images/f0068-05.jpg)\n\nPixels arranged according to brightness.\n\nvar img;\n\n**1**\n\nvar colors = [];\n\nvar sortMode = null;\n\n**1** | The currently selected sorting mode is always stored in the variable sortMode. The default is to not sort, and the value is therefore set at null (undefined).  \n---|---  \n  \nfunction preload(){\n\n**2**\n\nimg = loadImage(\"data/pic1.jpg\");\n\n}\n\n**2** | Before the program starts to run, the image is loaded and stored into the variable called img.  \n---|---  \n  \nfunction draw() {\n\n**3**\n\nvar tileCount = floor(width / max(mouseX, 5));\n\n**4**\n\nvar rectSize = width / tileCount;\n\n**3** | The number of rows and columns in the grid tileCount depends on the x-value of the mouse. The function max() selects the larger of the two given values.  \n---|---  \n**4** | The grid resolution just calculated is now used to define the size of the tiles, rectSize.  \n  \n**5**\n\nimg.loadPixels();\n\ncolors = [];\n\nfor (var gridY = 0; gridY < tileCount; gridY++) {\n\nfor (var gridX = 0; gridX < tileCount; gridX++) {\n\n**6**\n\nvar px = int(gridX * rectSize);\n\nvar py = int(gridY * rectSize);\n\nvar i = (py * img.width \\+ px) * 4;\n\nvar c = color(img.pixels[i], img.pixels[i+1],\n\nimg.pixels[i+2], img.pixels[i+3]);\n\ncolors.push(c);\n\n}\n\n}\n\n**7**\n\ngd.sortColors(colors, sortMode);\n\nvar i = 0;\n\nfor (var gridY = 0; gridY < tileCount; gridY++) {\n\nfor (var gridX = 0; gridX < tileCount; gridX++) {\n\n**8**\n\nfill(colors[i]);\n\nrect(gridX*rectSize, gridY*rectSize,\n\nrectSize, rectSize);\n\ni++;\n\n}\n\n}\n\n}\n\n**5** | Only after calling loadPixels() can the individual pixels of the image be accessed.  \n---|---  \n**6** | Now the picture is scanned line by line in the previously calculated grid spacing, rectSize. The pixels are stored in the pixels[] array as a long list of values. Therefore, from px and py, the corresponding index i must be calculated.  \n**7** | The colors are sorted using the sortColors() function. This function must pass the array colors and sort mode sortMode.  \n**8** | In order to draw the palette, the grid is processed again. The fill colors for the tiles are taken, value by value, from the array colors.  \n  \nfunction keyReleased(){\n\nif (key == 'c' || key == 'C') writeFile(\n\n**9**\n\n[gd.ase.encode(colors)],\n\ngd.timestamp(), 'ase');\n\n...\n\n**10**\n\nif (key == '5') sortMode = null;\n\nif (key == '6') sortMode = gd.HUE;\n\nif (key == '7') sortMode = gd.SATURATION;\n\nif (key == '8') sortMode = gd.BRIGHTNESS;\n\nif (key == '9') sortMode = gd.GRAYSCALE;\n\n}\n\n**9** | The function ase.encode() allows an array of colors to be saved as an Adobe Swatch Exchange (ASE) file. The palette can then be loaded as a color swatch library, e.g., in Adobe Illustrator.  \n---|---  \n**10** | The keys 5 to 9 control what sorting function is applied to colors. For this the sortMode is set at null (no sorting) or at one of the constants, HUE, SATURATION, BRIGHTNESS, or GRAYSCALE, from the Generative Design library.  \nMouse: Keys: | Position x: Resolution 1–4: Change example image 5–9: Change sort mode S: Save image C: Save ASE palette  \n---|---  \n  \n![image](../images/f0070-01.jpg)\n\n**→ P_1_2_2_01** “pic4.jpg” with sorting by gray value.\n\n![image](../images/f0070-02.jpg)\n\n**→** Photograph: **Steffen Knöll.**\n\n![image](../images/f0071-01.jpg)\n\n**→ P_1_2_2_01** “pic4.jpg” with sorting by hue.\n\n## P.1.2.3 Color palettes from rules\n\nAll colors are made up of three components: hue, saturation, and brightness.\nThe values for these color components are defined using a set of rules. By\nusing controlled random functions, you can quickly create different palettes\nin specific color nuances.\n\n**→ P_1_2_3_01**\n\nValues for hue, saturation, and brightness are randomly selected from\npredefined ranges of values. This combination of rule sets—the definition of\nvalue ranges—and random functions means that new palettes are continually\ncreated and that they always produce specific color nuances.\n\nBecause the perception of color depends on context, the produced colors are\ndrawn in an interactive grid. Even the color nuances emerge more distinctly.\n\n![image](../images/f0072-01.jpg)\n\n**1**\n\nvar hueValues = [];\n\nvar saturationValues = [];\n\nvar brightnessValues = [];\n\n**1** | An individual array is used to save hue, saturation, and brightness. Depending on what key is pressed (0–9), the arrays are filled according to different rules.  \n---|---  \n  \nfunction draw() {\n\n...\n\n**2**\n\nvar index = counter % currentTileCountX;\n\nfill(hueValues[index],\n\nsaturationValues[index],\n\nbrightnessValues[index]);\n\nrect(posX, posY, tileWidth, tileHeight);\n\ncounter++;\n\n...\n\n}\n\n**2** | When the grid is drawn, the colors are selected from the arrays, one by one. The continually incrementing variable counter starts to cycle through the same values because of the modulo operator, %. When currentTileCountX is 3, e.g., index will consecutively hold the values 0, 1, 2, 0, 1, 2. . . . This means only the first colors in the arrays are used in the grid.  \n---|---  \n  \n**3**\n\nif (key == '1') {\n\nfor (var i = 0; i < tileCountX; i++) {\n\nhueValues[i] = int(random(0, 360));\n\nsaturationValues[i] = int(random(0, 100));\n\nbrightnessValues[i] = int(random(0, 100));\n\n}\n\n}\n\n**3** | When key 1 is pressed, the three arrays are filled with random values from the complete ranges of values. This means any color can appear in the palette. ![image](../images/f0073-01.jpg)  \n---|---  \n  \n**4**\n\nif (key == '2') {\n\nfor (var i = 0; i < tileCountX; i++) {\n\nhueValues[i] = int(random(0, 360));\n\nsaturationValues[i] = int(random(0, 100));\n\nbrightnessValues[i] = 100;\n\n}\n\n}\n\n**4** | Here brightness is always set at the value 100. The result is a palette dominated by bright colors. ![image](../images/f0073-02.jpg)  \n---|---  \n  \n**5**\n\nif (key == '3') {\n\nfor (var i = 0; i < tileCountX; i++) {\n\nhueValues[i] = int(random(0, 360));\n\nsaturationValues[i] = 100;\n\nbrightnessValues[i] = int(random(0, 100));\n\n}\n\n}\n\n**5** | When the saturation value is set at 100, no pastel tones are created. ![image](../images/f0073-03.jpg)  \n---|---  \n  \n**6**\n\nif (key == '7') {\n\nfor (var i = 0; i < tileCountX; i++) {\n\nhueValues[i] = int(random(0, 180));\n\nsaturationValues[i] = int(random(80, 100));\n\nbrightnessValues[i] = int(random(50, 90));\n\n}\n\n}\n\n**6** | Here a restriction occurs in all color components; e.g., the hues are only selected from the first half of the color wheel, creating warmer colors. ![image](../images/f0073-04.jpg)  \n---|---  \n  \n**7**\n\nif (key == '9') {\n\nfor (var i = 0; i < tileCountX; i++) {\n\nif (i % 2 == 0) {\n\nhueValues[i] = int(random(0, 360));\n\nsaturationValues[i] = 100;\n\nbrightnessValues[i] = int(random(0, 100));\n\n**8**\n\n} else {\n\nhueValues[i] = 195;\n\nsaturationValues[i] = int(random(0, 100));\n\nbrightnessValues[i] = 100;\n\n}\n\n}\n\n}\n\n**7** | It is also possible to mix two color palettes. The expression i % 2 produces alternately the numbers 0 and 1. When the result is 0, a darker, more saturated color is saved in the array.  \n---|---  \n**8** | Otherwise the second rule is applied, and hue and brightness are set to fixed values. These values produce bright blue tones. ![image](../images/f0073-05.jpg)  \nMouse: Keys: | Position x/y: Grid resolution 0–9: Change color palette S: Save image C: Save ASE palette  \n---|---  \n  \n![image](../images/f0074-01.jpg)\n\n**→ P_1_2_3_01** The 0 key creates a color palette in which two preset hues\n(cyan and violet) alternate. Saturation and brightness are varied randomly.\n\n![image](../images/f0075-01.jpg)\n\n**→ P_1_2_3_02** Randomness plays a larger role here. The rows are divided\ninto tiles of different widths. It is randomly decided which tiles are divided\nagain.\n\n![image](../images/f0076-01.jpg)\n\n**→ P_1_2_3_03** If the tiles are slightly transparent and overlapping,\nanother series of colors is created in addition to those in the color palette.\n\n![image](../images/f0077-01.jpg)\n\n**→ P_1_2_3_05** The main difference between this image and the previous one\nis that here about half of the tiles are simply not drawn.\n\n\n![image](../images/f0078-01.jpg)\n\n# P.2 Shape\n\nThe previous chapter was primarily devoted to color, with shape playing a\nlesser role. Since it is possible to control every element of an image, we are\nable to access, modularize, and automate an enormous variety of not only\ncolors but also shapes. A dialog with form emerges.\n\n[**P.2 Shape**](10-part02ch02.xhtml#p.2) [78](10-part02ch02.xhtml#p.2)\n\n[P.2.0 Hello, shape](10-part02ch02.xhtml#p.2.0)\n[80](10-part02ch02.xhtml#p.2.0)\n\n[P.2.1 Grid](10-part02ch02.xhtml#p.2.1) [82](10-part02ch02.xhtml#p.2.1)\n\n[P.2.1.1 Alignment in a grid](10-part02ch02.xhtml#p.2.1.1)\n[82](10-part02ch02.xhtml#p.2.1.1)\n\n[P.2.1.2 Movement in a grid](10-part02ch02.xhtml#p.2.1.2)\n[86](10-part02ch02.xhtml#p.2.1.2)\n\n[P.2.1.3 Complex modules in a grid](10-part02ch02.xhtml#p.2.1.3)\n[90](10-part02ch02.xhtml#p.2.1.3)\n\n[P.2.1.4 Checkboxes in a grid](10-part02ch02.xhtml#p.2.1.4)\n[94](10-part02ch02.xhtml#p.2.1.4)\n\n[P.2.1.5 From grid to moiré](10-part02ch02.xhtml#p.2.1.5)\n[98](10-part02ch02.xhtml#p.2.1.5)\n\n[P.2.2 Agents](10-part02ch02.xhtml#p.2.2) [102](10-part02ch02.xhtml#p.2.2)\n\n[P.2.2.1 Dumb agents](10-part02ch02.xhtml#p.2.2.1)\n[102](10-part02ch02.xhtml#p.2.2.1)\n\n[P.2.2.2 Intelligent agents](10-part02ch02.xhtml#p.2.2.2)\n[104](10-part02ch02.xhtml#p.2.2.2)\n\n[P.2.2.3 Shapes from agents](10-part02ch02.xhtml#p.2.2.3)\n[108](10-part02ch02.xhtml#p.2.2.3)\n\n[P.2.2.4 Growth structure from agents](10-part02ch02.xhtml#p.2.2.4)\n[112](10-part02ch02.xhtml#p.2.2.4)\n\n[P.2.2.5 Structural density from agents](10-part02ch02.xhtml#p.2.2.5)\n[116](10-part02ch02.xhtml#p.2.2.5)\n\n[P.2.2.6 Agents on a pendulum](10-part02ch02.xhtml#p.2.2.6)\n[120](10-part02ch02.xhtml#p.2.2.6)\n\n[P.2.3 Drawing](10-part02ch02.xhtml#p.2.3) [126](10-part02ch02.xhtml#p.2.3)\n\n[P.2.3.1 Drawing with animated brushes](10-part02ch02.xhtml#p.2.3.1)\n[126](10-part02ch02.xhtml#p.2.3.1)\n\n[P.2.3.2 Relation and distance in drawing](10-part02ch02.xhtml#p.2.3.2)\n[130](10-part02ch02.xhtml#p.2.3.2)\n\n[P.2.3.3 Drawing with type](10-part02ch02.xhtml#p.2.3.3)\n[132](10-part02ch02.xhtml#p.2.3.3)\n\n[P.2.3.4 Drawing with dynamic brushes](10-part02ch02.xhtml#p.2.3.4)\n[134](10-part02ch02.xhtml#p.2.3.4)\n\n[P.2.3.5 Drawing with the pen tablet](10-part02ch02.xhtml#p.2.3.5)\n[138](10-part02ch02.xhtml#p.2.3.5)\n\n[P.2.3.6 Drawing with complex modules](10-part02ch02.xhtml#p.2.3.6)\n[142](10-part02ch02.xhtml#p.2.3.6)\n\n[P.2.3.7 Drawing with multiple brushes](10-part02ch02.xhtml#p.2.3.7)\n[146](10-part02ch02.xhtml#p.2.3.7)\n\n## P.2.0 Hello, shape\n\nAre point, line, and plane still the holy trinity of every form? Wassily\nKandinsky’s determination of these basic elements is more relevant than ever\nwhen viewed in the context of generative design. In keeping with this\napproach, the pixel is the origin of the small black circle in the image\nbelow. Lines are created by a series of pixels while planes are created by a\ncollection of connected lines.\n\n**→ P_2_0_01**\n\nThe shape is reduced to one pixel when the cursor is located in the middle of\nthe upper border of the display window.\n\nThe x-value of the mouse position sets the length of the lines. The y-value\nsets the value and number of the lines.\n\n![image](../images/f0080-01.jpg)\n\nfunction draw() {\n\nbackground(255);\n\n**1**\n\ntranslate(width / 2, height / 2);\n\n**2**\n\nvar circleResolution = map(mouseY, 0, height, 2, 80);\n\n**3**\n\nvar radius = mouseX \\- width / 2 \\+ 0.5;\n\n**4**\n\nvar angle = TWO_PI / circleResolution;\n\nstrokeWeight(mouseY / 20);\n\nbeginShape();\n\nfor (var i = 0; i <= circleResolution; i++) {\n\nvar x = cos(angle * i) * radius;\n\nvar y = sin(angle * i) * radius;\n\nline(0, 0, x, y);\n\n**5**\n\n// vertex(x, y);\n\n}\n\nendShape(CLOSE);\n\n}\n\n**1** | The origin of the coordinate system is moved to the center of the drawing canvas.  \n---|---  \n**2** | The function map() converts the y-value of the mouse position (a number between 0 and height) to a value between 2 and 80.  \n**3** | By subtracting half of the display’s width from the x-value of the mouse position, the radius becomes increasingly smaller the more the mouse is moved toward the center. Adding 0.5 to the x-value ensures the diameter of the circle is at least 1.  \n**4** | The increment angle is calculated as the full circle, TWO_PI, divided by the number of lines, circleResolution.  \n**5** | The end points of the line can be connected as a closed vertex by deleting the comment characters //.  \nMouse: Keys: | Position x: Line length Position y: Value and number of lines S: Save image  \n---|---  \n  \n![image](../images/f0081-01.jpg)\n\n**→ P_2_0_02 and → P_2_0_03** In this variation, the end points of the star\nforms are connected to the closed polygons. In addition, when drawing with the\nmouse, the transformation tracks are retained because the background is not\ngiven a new color.\n\n## **P.2.1.1 Alignment in a grid**\n\nHow can a diagonal with only two possible directions use the strict order of a\ngrid to make complex structures? The direction of the diagonals in every grid\nis decided randomly. Overlapping, resulting from changes in line width,\ncreates new forms, connections, and interstices.\n\n**→ P_2_1_1_01**\n\nIn a grid, either a line A is drawn from the upper-left to the lower-right\ncorner or a line B is drawn from the lower-left to the upper-right corner. The\ndirection of the diagonals is selected randomly.\n\n![image](../images/f0082-01.jpg)\n\n![image](../images/f0082-02.jpg)\n\n**→ P_2_1_1_01** The line values of the diagonals are linked to the mouse\nposition, and the types of line endings are switched using keys 1 to 3.\n\n**1**\n\nvar tileCount = 20;\n\n**1** | The value of the variable tileCount defines the grid resolution.  \n---|---  \n  \nfunction draw() {\n\n...\n\nstrokeCap(actStrokeCap);\n\n...\n\nfor (var gridY = 0; gridY < tileCount; gridY++) {\n\nfor (var gridX = 0; gridX < tileCount; gridX++) {\n\nvar posX = width / tileCount * gridX;\n\nvar posY = height / tileCount * gridY;\n\n**2**\n\nvar toggle = int(random(0, 2));\n\n**3**\n\nif (toggle == 0) {\n\n**4**\n\nstrokeWeight(mouseX / 20);\n\nline(posX, posY, posX + width / tileCount,\n\nposY + height / tileCount);\n\n}\n\n**5**\n\nif (toggle == 1) {\n\nstrokeWeight(mouseY / 20);\n\nline(posX, posY + width / tileCount,\n\nposX + height / tileCount, posY);\n\n}\n\n...\n\n**2** | The command random(0,2) creates a random number between 0.000 and 1.999. When converting this into an integer using int, it is rounded off and the variable toggle becomes either 0 or 1.  \n---|---  \n**3** | In this if query, the value of the variable toggle is compared to 0. When this statement is true, the next two lines of code are executed, drawing line A.  \n**4** | The value of the current mouse position on the x-axis defines the stroke value of line A. It is divided by 20 so the stroke value does not become too large.  \n**5** | The same applies to line B.  \n  \n**6**\n\nfunction keyReleased() {\n\n...\n\n**7**\n\nif (key == '1') actStrokeCap = ROUND;\n\n**8**\n\nif (key == '2') actStrokeCap = SQUARE;\n\n**9**\n\nif (key == '3') actStrokeCap = PROJECT;\n\n}\n\n**6** | Pressing one of the keys 1 to 3 sets the variable actStrokeCap to one of the Processing constants: ROUND, SQUARE, or PROJECT. This parameter can then be used to set how the ends of the lines are drawn using the function strokeCap().  \n---|---  \n**7** | strokeCap(ROUND) ![image](../images/f0083-02.jpg)  \n**8** | strokeCap(SQUARE) ![image](../images/f0083-03.jpg)  \n**9** | strokeCap(PROJECT) ![image](../images/f0083-04.jpg)  \nMouse: | Position x: Line value of left diagonals Position y: Line value of right diagonals Left click: New random value  \n---|---  \nKeys: | 1–3: Change line endings S: Save image  \n  \n![image](../images/f0083-01.jpg)\n\n**→ P_2_1_1_02** In this variation, additional colors and transparencies are\nused for both types of diagonals.\n\n![image](../images/f0084-01.jpg)\n\n**→ P_2_1_1_04** Here the elements in the grid are loaded out of the data file\nSVG graphics, which are always turned toward the mouse position. Using the key\nC, it is possible to switch between different color modes. The transparency of\nan element depends on its distance from the mouse position.\n\n![image](../images/f0084-02.jpg)\n\n**→ P_2_1_1_04** Using key D, it is possible to select whether the elements\nbecome smaller or larger as they near the mouse.\n\n![image](../images/f0085-01.jpg)\n\n**→ P_2_1_1_04** Additional SVG modules can be selected with the keys 1 to 7.\nWhen the SVG graphics are decentralized in position, the grid appears to\ndissolve, although the modules are drawn strictly within the grid.\n\n![image](../images/f0085-02.jpg)\n\n**→ P_2_1_1_04** The elements can also be rotated (left- and right-arrow\nkeys).\n\n## **P.2.1.2 Movement in a grid**\n\nTension is highest when order borders on chaos. Individual forms abandon their\nstrict arrangement in the dynamic grid and submit to random configurations.\nElements inclined to the grid and those adverse to it fight for visual\nsupremacy. It is the moments of transition that are important.\n\n**→ P_2_1_2_01**\n\nA fixed number of circles are drawn to the display one by one, line by line.\nRandom values are added to the grid position of a circle, forcing it to move\nalong the x- and y-axes. The farther the mouse is moved to the right, the\ngreater the circle’s movement.\n\n![image](../images/f0086-01.jpg)\n\nfunction draw() {\n\n**1**\n\ntranslate(width / tileCount / 2, height / tileCount / 2);\n\n...\n\nstrokeWeight(mouseY / 60);\n\nfor (var gridY = 0; gridY < tileCount; gridY++) {\n\nfor (var gridX = 0; gridX < tileCount; gridX++) {\n\nvar posX = width / tileCount * gridX;\n\nvar posY = height / tileCount * gridY;\n\n**2**\n\nvar shiftX = random(-mouseX, mouseX) / 20;\n\nvar shiftY = random(-mouseX, mouseX) / 20;\n\n**3**\n\nellipse(posX + shiftX, posY + shiftY,\n\nmouseY / 15, mouseY / 15);\n\n}\n\n}\n\n}\n\n**1** | The origin of the coordinate system is shifted by half a tile width and height to the right and down so the circles are located in the center of their tiles.  \n---|---  \n**2** | The higher the x value of the mouse position mouseX, the greater the range of values for the random numbers.  \n**3** | Adding the values shiftX and shiftY to the grid coordinates posX and posY results in the final shifted positions of the circles.  \nMouse: | Position x: Circle position Position y: Circle size Left click: New random values of circle position  \n---|---  \nKeys: | S: Save image  \n  \n![image](../images/f0087-01.jpg)\n\n**→ P_2_1_2_01** Transparent circles move in the horizontal grid according to\nthe mouse position. The vertical mouse position determines the size of the\ncircles.\n\n![image](../images/f0087-02.jpg)\n\n**→ P_2_1_2_02** The shifting black circles are situated behind a dense grid\nof white circles.\n\n![image](../images/f0088-01.jpg)\n\n**→ P_2_1_2_02** A color variation with transparent circles.\n\n![image](../images/f0088-02.jpg)\n\n**→ P_2_1_2_02** In this row, only the violet circles are offset, while the\npink ones remain fixed to the grid.\n\n![image](../images/f0089-01.jpg)\n\n**→ P_2_1_2_04** Only the corners of the elements are shifted here, not the\nelements themselves.\n\n![image](../images/f0089-02.jpg)\n\n**→ P_2_1_2_04** The overlaps and gaps here reveal the grid before it\ngradually becomes unrecognizable.\n\n## **P.2.1.3 Complex modules in a grid**\n\nEven more interesting is the nesting of several forms into a complex module.\nIn this illustration, four different clusters of ellipses increase in size in\nopposite directions and are positioned alternately on the grid. The varied\ndiameters and transparencies of the individual ellipses create an illusion of\ndepth.\n\n**→ P_2_1_3_01**\n\nThe module filling the grid consists of a stack of circles that become smaller\nand smaller and move in one direction (top, bottom, right, or left) that is\ndecided randomly. The number of circles corresponds to mouse position x, the\nmovement to mouse position y.\n\n![image](../images/f0090-01.jpg)\n\n![image](../images/f0090-02.jpg)\n\n**→ P_2_1_3_01** Each module in the grid consists of several circles. The\nmouse position defines their number, size, and position.\n\nfunction draw() {\n\n...\n\n**1**\n\ncircleCount = mouseX / 30 + 1;\n\nendSize = map(mouseX, 0, max(width, mouseX),\n\ntileWidth / 2, 0);\n\nendOffset = map(mouseY, 0, max(height, mouseY),\n\n0, (tileWidth - endSize) / 2);\n\nfor (var gridY = 0; gridY <= tileCountY; gridY++) {\n\nfor (var gridX = 0; gridX <= tileCountX; gridX++) {\n\n**2**\n\npush();\n\ntranslate(tileWidth * gridX, tileHeight * gridY);\n\nscale(1, tileHeight / tileWidth);\n\n**3**\n\nvar toggle = int(random(0, 4));\n\nif (toggle == 0) rotate(-HALF_PI);\n\nif (toggle == 1) rotate(0);\n\nif (toggle == 2) rotate(HALF_PI);\n\nif (toggle == 3) rotate(PI);\n\n// draw module\n\nfor (var i = 0; i < circleCount; i++) {\n\n**4**\n\nvar diameter = map(i, 0, circleCount, tileWidth, endSize);\n\nvar offset = map(i, 0, circleCount, 0, endOffset);\n\nellipse(offset, 0, diameter, diameter);\n\n}\n\n**5**\n\npop();\n\n}\n\n}\n\n}\n\n**1** | The mouse position defines circleCount in a module, the size of the circles, and the offset of the last circle.  \n---|---  \n**2** | The varying rotation of the modules is easier to manage when, before drawing a module, the origin of the coordinate system is temporarily moved to the position where it will be drawn. The command push() ensures that the current state of the coordinate system is saved before the origin is moved using translate().  \n**3** | The random number toggle decides between the four rotation directions. random(0, 4) generates a number between 0 and 3.999; so, rounded off, the values 0, 1, 2, and 3. The radian angle HALF_PI is a rotation of 90°.  \n**4** | The module is constructed by drawing the circles in succession. The value of diameter ranges between tileWidth and the previously calculated endSize. The value offset comprises the shift from the center. The circles become smaller and smaller, thereby shifting farther and farther to the right.  \n**5** | Finally, the previously saved state of the coordinate system is restored using pop().  \nMouse: | Position x: Number and size of circles Position y: Position of circles Left click: New random value for position  \n---|---  \nKeys: | S: Save image  \n  \n![image](../images/f0091-01.jpg)\n\n**→ P_2_1_3_02** Complex modules made of straight lines. The consolidation of\nthe lines generates new forms.\n\n![image](../images/f0091-02.jpg)\n\n**→ P_2_1_3_02** The point toward which the lines converge can be shifted\ndiagonally with the mouse.\n\n![image](../images/f0091-03.jpg)\n\n**→ P_2_1_3_02** Consolidation easily creates a 3D effect.\n\n![image](../images/f0092-01.jpg)\n\n**→ P_2_1_3_05** This module consists of increasingly smaller and darker\ncircles.\n\n![image](../images/f0093-01.jpg)\n\n**→ P_2_1_3_04** This module displays a similar principle to the one opposite\nbut with rotating squares. The squares are hardly recognizable as colored and\ntransparent.\n\n## **P.2.1.4 Checkboxes in a grid**\n\nThis is where the controls’ wildest dreams come true: they finally become\nvisual design elements. Buttons, checkboxes, and sliders are used as raster\nelements in the browser, serving as pure visualization elements that become\nintriguing when grouped and arranged together.\n\n**→ P_2_1_4_01**\n\nEach pixel of an input image is analyzed and displayed as a checkbox. The\nbrightness of the pixel determines whether it is checked off or not. If the\nbrightness is above a specified threshold, the checkmark will not be set.\nThose checkboxes therefore will appear brighter.\n\n![image](../images/f0094-01.jpg)\n\n![image](../images/f0094-02.jpg)\n\n**→ P_2_1_4_01** The appearance of the checkboxes varies from browser to\nbrowser but can be adjusted via style sheets.\n\n<html>\n\n...\n\n<body>\n\n**1**\n\n<div id=\"container\"></div>\n\n...\n\n</body>\n\n</html>\n\n**1** | The index.html document provides a <div> element with the id container, to which the checkboxes will be added later.  \n---|---  \n  \nfunction setup() {\n\n**2**\n\nnoCanvas();\n\n...\n\nfor (var y = 0; y < rows; y++) {\n\nfor (var x = 0; x < cols; x++) {\n\n**3**\n\nvar box = createCheckbox();\n\nbox.style('display', 'inline');\n\nbox.parent('container');\n\nboxes.push(box);\n\n}\n\n**4**\n\nvar linebreak = createSpan('<br/>');\n\nlinebreak.parent('mirror');\n\n}\n\n**5**\n\nslider = createSlider(0, 255, 0);\n\n}\n\n**2** | Since the elements land directly in the HTML document, the standard drawing canvas can be removed.  \n---|---  \n**3** | Create checkbox with createCheckbox(). These can be positioned with style() style properties, and parent() inserts it into the HTML element container.  \n**4** | A line break is added at the end of each line.  \n**5** | The slider can later be used to control the brightness level threshold. A range of values from 0 to 255 is therefore useful. The starting value is 0.  \n  \nfunction draw() {\n\nfor (var y = 0; y < img.height; y++) {\n\nfor (var x = 0; x < img.height; x++) {\n\n**6**\n\nvar c = color(img.get(x, y));\n\nvar bright = (red(c) + green(c) + blue(c)) / 3;\n\n**7**\n\nvar threshold = slider.value();\n\n**8**\n\nvar checkIndex = x + y * cols;\n\n**9**\n\nif (bright > threshold) {\n\nboxes[checkIndex].checked(false);\n\n} else {\n\nboxes[checkIndex].checked(true);\n\n}\n\n}\n\n}\n\n}\n\n**6** | When converting the image into checkboxes, the individual pixels of the image must be converted into a value, in this case, bright. For grayscale images, a simple mean value calculation is sufficient. For color images, a slightly more complex calculation should be used.  \n**[→ P.4.3.1 Graphics from pixel values](12-part02ch04.xhtml#p.4.3.1)**  \n---|---  \n**7** | The value() function retrieves the value currently set by the slider.  \n**8** | The index of the checkbox associated with this pixel is calculated from x, y, and the number of columns, cols.  \n**9** | If the brightness of the pixel is above the set threshold, the check mark is removed; otherwise it is set.  \nKeys: | 1–3: Change image  \n---|---  \nSlider: | Set grayscale threshold  \n  \n![image](../images/f0096-01.jpg)\n\n**→ P_2_1_4_03** Many sliders arranged geometrically.\n\n![image](../images/f0097-01.jpg)\n\n**→ P_2_1_4_04** These sliders automatically stay in motion.\n\n## **P.2.1.5 From grid to moiré**\n\nA moiré, which is usually considered a mistake in printing technology, is\ndesirable here. By laying one graphic grid over another identical grid and\nmoving it, you can generate unexpected optical illusions that you can change\nin real time.\n\n**→ P_2_1_5_01**\n\nThe moiré effect occurs when two or more fine grid structures overlap to form\na coarser pattern. This happens because the overall picture shines brighter at\nall crossing points, because more of the white background can be seen next to\nthese intersections.\n\n![image](../images/f0098-01.jpg)\n\n![image](../images/f0098-02.jpg)\n\n**→ P_2_1_5_01** Curved raster elements usually create particularly\ninteresting overlays.\n\nfunction draw() {\n\n...\n\nstrokeWeight(3);\n\n**1**\n\noverlay();\n\n**2**\n\nvar x = map(mouseX, 0, width, -50, 50);\n\nvar a = map(mouseX, 0, width, -0.5, 0.5);\n\nvar s = map(mouseY, 0, height, 0.7, 1);\n\n**3**\n\nif (drawMode == 2) translate(x, 0);\n\nif (drawMode == 1) rotate(a);\n\nscale(s);\n\n**4**\n\nstrokeWeight(2);\n\noverlay();\n\n}\n\n**1** | The background graphic is drawn. Since this graphic must be generated a second time, this process has been swapped out to the overlay() function.  \n---|---  \n**2** | The mouse position is used to calculate the corresponding values for the translation x, the rotation a, and the scaling s.  \n**3** | Depending on the drawMode, the coordinate system is either moved horizontally or rotated. Scaling is done in both cases.  \n**4** | The overlay graphic is drawn with a slightly different stroke width.  \n  \n**5**\n\nfunction overlay() {\n\nvar w = width \\- 100;\n\nvar h = height \\- 100;\n\n**6**\n\nif (drawMode == 1) {\n\nfor (var i = -w / 2; i < w / 2; i += 5) {\n\nline(i, -h / 2, i, h / 2);\n\n}\n\n}\n\n**7**\n\nif (drawMode == 2) {\n\nfor (var i = 0; i < w; i += 10) {\n\nellipse(0, 0, i);\n\n}\n\n}\n\n}\n\n**5** | The graphic is created in the function overlay().  \n---|---  \n**6** | In drawMode 1, a series of lines.  \n**7** | In drawMode 2, gradually increasing circles.  \nMouse: | Position x: Rotate or move overlay Position y: Scale overlay  \n---|---  \nKeys: | 1–2: Change drawMode S: Save image  \n  \n![image](../images/f0099-01.jpg)\n\n**→ P_2_1_5_01** The moiré effect becomes stronger as the two grids get closer\nin terms of rotation and scaling.\n\n![image](../images/f0100-01.jpg)\n\n**→ P_2_1_5_04** Freely drawn lines, drawn numerous times, parallel to each\nother and overlaid.\n\n## **P.2.2.1 Dumb agents**\n\nInstead of being rigidly embedded in a grid, the pixel now becomes an agent\nand can move freely based on different behavioral patterns. With each step the\nagent advances according to one of eight directions, leaving a trail behind\nit. It pursues its mission and never gives up.\n\n**→ P_2_2_1_01**\n\nDuring each drawing operation, one of the eight possible directions is\nrandomly selected for the next step. Steps are made by adding or subtracting a\npredetermined value (step size) to the current position’s coordinates. The\ncircle is finally drawn at the new position.\n\n![image](../images/f0102-01.jpg)\n\n![image](../images/f0102-02.jpg)\n\n**→ P_2_2_1_01** The longer the agents migrate, the denser the cloud structure\nbecomes.\n\n![image](../images/f0102-03.jpg)\n\n**→ P_2_2_1_02** The agents’ direction of movement is restricted here.\n\n![image](../images/f0102-04.jpg)\n\n**→ P_2_2_1_02** In this variation, the diameters of the circles are larger\nand sometimes colored blue.\n\n**1**\n\nvar NORTH = 0;\n\nvar NORTHEAST = 1;\n\nvar EAST = 2;\n\nvar SOUTHEAST = 3;\n\nvar SOUTH = 4;\n\nvar SOUTHWEST = 5;\n\nvar WEST = 6;\n\nvar NORTHWEST = 7;\n\n...\n\n**2**\n\nvar stepSize = 1;\n\nvar diameter = 1;\n\n**1** | Eight constants, each with a different numerical value, are defined.  \n---|---  \n**2** | The step size and diameter of the agents can be set by changing the values of stepSize and diameter.  \n  \nfunction draw() {\n\nfor (var i = 0; i <= mouseX; i++) {\n\n**3**\n\ndirection = int(random(0, 8));\n\n**4**\n\nif (direction == NORTH) {\n\nposY -= stepSize;\n\n**5**\n\n} else if (direction == NORTHEAST) {\n\nposX += stepSize;\n\nposY -= stepSize;\n\n**6**\n\n} else if (direction == EAST) {\n\nposX += stepSize;\n\n} else if (direction == SOUTHEAST) {\n\nposX += stepSize;\n\nposY += stepSize;\n\n} else if (direction == SOUTH) {\n\nposY += stepSize;\n\n} else if (direction == SOUTHWEST) {\n\nposX -= stepSize;\n\nposY += stepSize;\n\n} else if (direction == WEST) {\n\nposX -= stepSize;\n\n} else if (direction == NORTHWEST) {\n\nposX -= stepSize;\n\nposY -= stepSize;\n\n}\n\n**7**\n\nif (posX > width) posX = 0;\n\nif (posX < 0) posX = width;\n\nif (posY < 0) posY = height;\n\nif (posY > height) posY = 0;\n\n**8**\n\nellipse(posX + stepSize / 2, posY + stepSize / 2,\n\ndiameter, diameter);\n\n}\n\n}\n\n**3** | The command random(0,8) creates a random number between 0.000 and 7.999. When rounded off, this results in a value between 0 and 7. This, in turn, is stored in direction and determines the next step.  \n---|---  \n**4** | ![image](../images/f0103-01.jpg)  \n**5** | ![image](../images/f0103-02.jpg)  \n**6** | ![image](../images/f0103-03.jpg)  \n**7** | When the agent’s current position extends beyond the right border of the display window, posX is set at 0. This causes the agent to continue its path on the opposite side.  \n**8** | A transparent circle is drawn in the new position. Half the increment size, stepSize/2, is added to this so the circle is not cut off at the border of the display window.  \nMouse: | Position x: Speed of the image construction  \n---|---  \nKeys: | DEL: Clear drawing canvas S: Save image  \n  \n## **P.2.2.2 Intelligent agents**\n\nBehavioral patterns are now far more complex and subject to precise\nconditions. The agent easily goes astray when it crosses its own path. When it\nreaches the border of the display window, it changes direction. The straight\nlines, which are drawn between two points of intersection, change their color\nand stroke value depending on the traveled distance.\n\n**→ P_2_2_2_01**\n\nThe agent always moves in one of the cardinal directions: north, east, south,\nor west. It can, however, choose from several possible angles, whereby right\nangles are not possible. When the agent reaches the border of the drawing\ncanvas, it turns around and randomly selects one of the possible angles. When\nit crosses its own path, it maintains its general direction but selects a new\nangle.\n\n![image](../images/f0104-01.jpg)\n\n![image](../images/f0104-02.jpg)\n\n**→ P_2_2_2_01** By pressing key R, lines are drawn without tracks in a PDF.\nThe process is ended by pressing key E.\n\n![image](../images/f0104-03.jpg)\n\n**→ P_2_2_2_02** A line’s value depends on its distance.\n\n![image](../images/f0104-04.jpg)\n\n**→ P_2_2_2_02** The lines here are colored according to their length (key 2).\n\nfunction draw() {\n\nvar speed = int(map(mouseX, 0, width, 0, 20));\n\nfor (var i=0; i<=speed; i++) {\n\nstrokeWeight(1);\n\nstroke(180, 0, 0);\n\n**1**\n\npoint(posX, posY);\n\n**2**\n\nposX += cos(radians(angle)) * stepSize;\n\nposY += sin(radians(angle)) * stepSize;\n\nreachedBorder = false;\n\n**3**\n\nif (posY <= 5) {\n\ndirection = SOUTH;\n\nreachedBorder = true;\n\n} else if (posX >= width \\- 5) {\n\ndirection = WEST;\n\nreachedBorder = true;\n\n}\n\n...\n\nloadPixels();\n\n**4**\n\nvar currentPixel = get(floor(posX), floor(posY));\n\nif ((currentPixel[0]!=255 && currentPixel[1]!=255 &&\n\ncurrentPixel[2]!=255) || reachedBorder) {\n\nangle = getRandomAngle(direction);\n\n**5**\n\nvar distance = dist(posX, posY,\n\nposXcross, posYcross);\n\nif (distance >= minLength) {\n\nstrokeWeight(3);\n\nstroke(0, 0, 0);\n\nline(posX, posY, posXcross, posYcross);\n\n}\n\n**6**\n\nposXcross = posX;\n\nposYcross = posY;\n\n}\n\n}\n\n}\n\n**1** | A point is drawn on the agent’s current position (posX, posY). The point can become almost (or completely) invisible when its color matches the background color.  \n---|---  \n**2** | The agent takes a step. In doing so, its position is updated: angle defines the direction and stepSize the length of the step.  \n**3** | Now the agent is checked to determine if it has reached a border of the display window. If, e.g., it is only five pixels or fewer away from the upper border, the direction is changed to SOUTH and the variable reachedBorder set to true.  \n**4** | The function get() checks every time the agent moves to determine whether it is on a pixel that is not white. If this is the case or if the variable reachedBorder is true, then a new random angle step increment in the main direction is selected using the function getRandomAngle().  \n**5** | When changing direction, a line is only drawn when the position of the last direction change (posXcross, posYcross) is at least as far away as the distance defined by minLength.  \n**6** | Finally, the current position is saved in the variables posXcross and posYcross.  \n  \n**7**\n\nfunction getRandomAngle(currentDirection) {\n\nvar a = (floor(random(-angleCount, angleCount)) +\n\n0.5) * 90 / angleCount;\n\nif (currentDirection == NORTH) return a - 90;\n\nif (currentDirection == EAST) return a;\n\nif (currentDirection == SOUTH) return a + 90;\n\nif (currentDirection == WEST) return a + 180;\n\nreturn 0;\n\n}\n\n**7** | The function getRandomAngle() randomly selects and returns one of the possible angles, which is dependent on the passed main direction, currentDirection. When angleCount, e.g., is 3 (i.e., three directions per quadrant) and currentDirection is SOUTH, then one of these angles is returned.  \n---|---  \n  \n![image](../images/f0105-01.jpg)\n\nMouse: | Position x: Speed of the image construction  \n---|---  \nKeys: | DEL: Clear canvas S: Save image  \n  \n![image](../images/f0106-01.jpg)\n\n**→ P_2_2_2_02** Longer lines are opaque in appearance, shorter ones\ntransparent.\n\n![image](../images/f0107-01.jpg)\n\n**→ P_2_2_2_02** Fixed values are set for hue and saturation; brightness\ndepends on the length of the lines.\n\n## **P.2.2.3 Shapes from agents**\n\nIt is only through cooperative efforts that dumb agents become strong. A\ncircle is the source shape in this example. The permutation rule is that every\npoint on the circle is represented by a dumb agent and that movement of the\npoints will cause the circle to gradually change its shape. This produces a\nsurprisingly large repertoire of forms.\n\n**→ P_2_2_3_01**\n\nThe calculation of the points on the circle produces the starting positions of\nthe agents. A flexible curve firmly connects each agent with both its\nneighbors. The farther away the dumb agents move from their original\npositions, the more the circular organization dissolves.\n\n![image](../images/f0108-01.jpg)\n\n![image](../images/f0108-02.jpg)\n\n**→ P_2_2_3_01** The constantly changing shape always moves toward the mouse\nand can thereby be controlled by it.\n\n![image](../images/f0108-03.jpg)\n\n**→ P_2_2_3_01** The fill mode can be set using key 2 so that the form is\ncolored with a random and changing gray value.\n\nfunction setup() {\n\n...\n\n**1**\n\ncenterX = width / 2;\n\ncenterY = height / 2;\n\nvar angle = radians(360 / formResolution);\n\nfor (var i = 0; i < formResolution; i++) {\n\n**2**\n\nx.push(cos(angle * i) * initRadius);\n\ny.push(sin(angle * i) * initRadius);\n\n}\n\n...\n\n}\n\n**1** | The agents are drawn later around the position (centerX, centerY). The initial starting point is the center of the display.  \n---|---  \n**2** | The agents’ starting positions are calculated as points on the circle and saved in the arrays x and y by the array function push().  \n**[→ P.1.1.2 Color spectrum in a circle](09-part02ch01.xhtml#p.1.1.2)**  \n  \nfunction draw() {\n\n**3**\n\ncenterX += (mouseX \\- centerX) * 0.01;\n\ncenterY += (mouseY \\- centerY) * 0.01;\n\nfor (var i = 0; i < formResolution; i++) {\n\n**4**\n\nx[i] += random(-stepSize, stepSize);\n\ny[i] += random(-stepSize, stepSize);\n\n**5**\n\n// ellipse(x[i] + centerX, y[i] + centerY, 5, 5);\n\n}\n\n...\n\nbeginShape();\n\n**6**\n\ncurveVertex(x[formResolution - 1] + centerX,\n\ny[formResolution - 1] + centerY);\n\nfor (var i = 0; i < formResolution; i++) {\n\ncurveVertex(x[i] + centerX, y[i] + centerY);\n\n}\n\ncurveVertex(x[0] + centerX, y[0] + centerY);\n\n**7**\n\ncurveVertex(x[1] + centerX, y[1] + centerY);\n\nendShape();\n\n}\n\n**3** | The mouse follows the position (centerX, centerY). With every frame the difference between the agent position and the mouse position is calculated, multiplied by a small value, and added again to this position.  \n---|---  \n**4** | The addition of random values between \\- stepSize and stepSize to the agents’ current positions results in an up and down movement.  \n**5** | The agents’ positions can also be visualized by adding the ellipse command.  \n**6** | Note that when drawing the form, the first and last points set with the curveVertex() serve as control points and are not drawn. These two control points ensure the circle is completed without any bends so the final circle is smooth. ![image](../images/f0109-01.jpg)  \n**7** | If, e.g., curveVertex is replaced with vertex, straight connections are produced. Experiments with the fill color fill also result in interesting variations.  \nMouse: | Left click: New circle Position x/y: Movement direction  \n---|---  \nKeys: | 1–2: Fill mode S: Save image  \n  \n![image](../images/f0110-01.jpg)\n\n**→ P_2_2_3_02** In the second version of the program, a drawMode can be\nselected that initially creates a straight line, which then gradually deforms.\n\n![image](../images/f0111-01.jpg)\n\n**→ P_2_2_3_02** Drawing with a changing form. When you click, a slightly\ndeformed circular geometry appears. As you move the mouse, the form follows\nthe position of the cursor and the distortion becomes increasingly apparent.\n\n## **P.2.2.4 Growth structure from agents**\n\nA stable structure results from the convergence of multiple agents based on\nsimple rules. Complex shapes are created out of simple propagation patterns\nsuch as: draw a new circle and position it as close as possible to its nearest\nneighbor. These types of algorithms also describe growth processes in plants\nand minerals.\n\n**→ P_2_2_4_01**\n\nIn each frame, a new circle is generated at a random position and with a\nrandom radius (dashed circles). It is then determined which of the existing\ncircles lies nearest to the new one. In the final step, the new circle docks\nwith its closest neighbor via the shortest path.\n\n![image](../images/f0112-01.jpg)\n\n![image](../images/f0112-02.jpg)\n\n**→ P_2_2_4_01** Circles increasingly fill the area and an organic structure\nevolves.\n\nfunction draw() {\n\nbackground(255);\n\n**1**\n\nvar newR = random(1, 7);\n\nvar newX = random(newR, width \\- newR);\n\nvar newY = random(newR, height \\- newR);\n\nvar closestDist = Number.MAX_VALUE;\n\nvar closestIndex = 0;\n\n**2**\n\nfor (var i = 0; i < currentCount; i++) {\n\nvar newDist = dist(newX, newY, x[i], y[i]);\n\nif (newDist < closestDist) {\n\nclosestDist = newDist;\n\nclosestIndex = i;\n\n}\n\n}\n\n**3**\n\n// fill(230);\n\n// ellipse(newX, newY, newR * 2, newR * 2);\n\n// line(newX, newY, x[closestIndex], y[closestIndex]);\n\n**4**\n\nvar angle = atan2(newY - y[closestIndex],\n\nnewX - x[closestIndex]);\n\nx[currentCount] = x[closestIndex] + cos(angle) *\n\n(r[closestIndex] + newR);\n\ny[currentCount] = y[closestIndex] + sin(angle) *\n\n(r[closestIndex] + newR);\n\nr[currentCount] = newR;\n\ncurrentCount++;\n\nfor (var i = 0; i < currentCount; i++) {\n\nfill(50);\n\n**5**\n\nellipse(x[i], y[i], r[i] * 2, r[i] * 2);\n\n}\n\n**6**\n\nif (currentCount >= maxCount) noLoop();\n\n}\n\n**1** | The radius newR and the position (newX, newY) for the circle are defined randomly.  \n---|---  \n**2** | The closest neighbor is searched in the for loop. All circles are processed one by one and their distance to the new circle is calculated. If this distance is smaller than all previous distances, a reference to this circle is saved in the variable closestIndex. ![image](../images/f0113-01.jpg)  \n**3** | By drawing the starting position of the new circle and the connecting line to the circle closest to it, the process just described can be visualized using these three lines of code.  \n**4** | By calculating the angle to the closest neighbor, the new circle can be positioned so the two circles touch.  \n**[→ Kap.P.1.1.2 Color spectrum in a circle](09-part02ch01.xhtml#p.1.1.2)**\n![image](../images/f0113-02.jpg)  \n**5** | The circles are drawn.  \n**6** | When currentCount reaches the defined upper limit, the program is stopped using the function noLoop().  \nKeys: | S: Save image  \n---|---  \n  \n![image](../images/f0114-01.jpg)\n\n**→ P_2_2_4_01** This sequence demonstrates how the structure grows gradually\nbut continually.\n\n![image](../images/f0115-01.jpg)\n\n**→ P_2_2_4_02** When the source circle is particularly large, the structure\ngrows from the outside in. In addition, the initial positions of the circles\nand connecting lines are drawn here to their new position (Key 1).\n\n## **P.2.2.5 Structural density from agents**\n\nIn this example an iterative process again serves as a shape-giver: Generate a\nnew circle. If this circle does not intersect with any other circle in the\ndisplay, make it as large as possible; if it intersects with another circle,\nstart over. The aim of this algorithm is to pack the circles so densely that\neventually even the smallest gaps are closed.\n\n**→ P_2_2_5_01**\n\nHere, too, a new circle (shown by a dashed yellow outline) with a random\nposition and size is generated in each frame. When this intersects with a\npreexisting circle, the algorithm starts over.\n\nOtherwise, it locates the closest circle. The distance to this circle and its\nradius now defines how large the new circle has to be drawn so that it touches\nits neighbor and the circles can be packed densely.\n\n![image](../images/f0116-01.jpg)\n\n![image](../images/f0116-02.jpg)\n\n**→ P_2_2_5_01** The algorithm fills the area with circles that become smaller\nand smaller. The green lines show on which circles new growths have docked.\n\nfunction draw() {\n\nbackground(255);\n\n**1**\n\nvar newX = random(maxRadius, width - maxRadius);\n\nvar newY = random(maxRadius, height - maxRadius);\n\nif (mouseIsPressed && mouseButton == LEFT) {\n\n**2**\n\nnewX = random(mouseX - mouseRect, mouseX + mouseRect);\n\nnewY = random(mouseY - mouseRect, mouseY + mouseRect);\n\n}\n\nvar intersection = false;\n\n**3**\n\nfor (var newR = maxRadius; newR >= minRadius; newR--) {\n\nfor (var i = 0; i < circles.length; i++) {\n\nvar d = dist(newX, newY, circles[i].x, circles[i].y);\n\n**4**\n\nintersection = d < circles[i].r + newR;\n\nif (intersection) {\n\nbreak;\n\n}\n\n}\n\n**5**\n\nif (!intersection) {\n\ncircles.push(new Circle(newX, newY, newR));\n\nbreak;\n\n}\n\n}\n\nfor (var i = 0; i < circles.length; i++) {\n\n**6**\n\nif (showLine) {\n\nvar closestCircle;\n\nfor (var j = 0; j < circles.length; j++) {\n\nvar d = dist(circles[i].x, circles[i].y,\n\ncircles[j].x, circles[j].y);\n\nif (d <= circles[i].r + circles[j].r + 1) {\n\nclosestCircle = circles[j];\n\nbreak;\n\n}\n\n}\n\nif (closestCircle) {\n\nstroke(100, 230, 100);\n\nstrokeWeight(0.75);\n\nline(circles[i].x, circles[i].y,\n\nclosestCircle.x, closestCircle.y);\n\n}\n\n}\n\n**7**\n\nif (showCircle) circles[i].draw();\n\n}\n\n...\n\n}\n\n**1** | Create a position and radius for a new circle. ![image](../images/f0117-01.jpg)  \n---|---  \n**2** | By holding down the mouse button, the range for random values is limited, allowing new circles to be positioned selectively and generating an interactive drawing tool.  \n**3** | The radius of the new circle must now be determined. For this, newR is set to the maximum radius, maxRadius, and counted down.  \n**4** | All existing circles are compared to the new one. When an intersection exists (e.g., when the distance is smaller than the sum of both radii), then the variable intersection is set to true.  \n**5** | If there is no overlap, a new instance of the Circles class is created and saved. ![image](../images/f0117-02.jpg)  \n**6** | If the showLine option is selected, each circle is compared to all others to find an adjoining circle, closestCircle.  \n**7** | If showCircle is true, the circle is also drawn.  \nMouse: | Drag: Targeted placement of the circles  \n---|---  \nKeys: | 1: Show/hide circles 2: Show/hide lines Arrow ↓/↑: Change area to be drawn S: Save image  \n  \n![image](../images/f0118-01.jpg)\n\n![image](../images/f0119-01.jpg)\n\n**→ P_2_2_5_02** By loading SVG modules, very different images are created.\nUsing keys 1 to 3, it is possible to specify which elements are to be visible:\nthe SVGs, the connecting lines, or the circles.\n\n## **P.2.2.6 Agents on a pendulum**\n\nThese double agents are changeable and chained together. The agents move along\nsets of interrelated pendulums, leaving behind complex traces that are\nreminiscent of the drawings of a Spirograph. Over time, their secret mission\nbecomes visible.\n\n**→ P_2_2_6_01**\n\nThe agents form a chain of pendulums. To determine the individual positions of\nthe agents, we begin at the center. The angle of rotation and the length of\nthe pendulum determine the position of the first agent. This is also the\nrotation center for the next pendulum. The farther out in the chain, the\nshorter the pendulum becomes and the faster it turns. Every other pendulum\nturns in the opposite direction.\n\n![image](../images/f0120-01.jpg)\n\n![image](../images/f0120-02.jpg)\n\n**→ P_2_2_6_01** Shapes with, e.g., three, six, or four symmetry axes are\nproduced with different settings for the ratio of the rotational speeds.\n\nfunction draw() {\n\nbackground(0, 0, 100);\n\n**1**\n\nangle += speed;\n\nif (angle <= maxAngle + speed) {\n\n**2**\n\nvar pos = center.copy();\n\nfor (var i = 0; i < joints; i++) {\n\n**3**\n\nvar a = angle * pow(speedRelation, i);\n\nif (i % 2 == 1) a = -a;\n\n**4**\n\nvar nextPos = p5.Vector.fromAngle(radians(a));\n\nnextPos.setMag((joints - i) / joints * lineLength);\n\nnextPos.add(pos);\n\n**5**\n\nif (showPendulum) {\n\nnoStroke();\n\nfill(0, 10);\n\nellipse(pos.x, pos.y, 4, 4);\n\nnoFill();\n\nstroke(0, 10);\n\nline(pos.x, pos.y, nextPos.x, nextPos.y);\n\n}\n\n**6**\n\npendulumPath[i].push(nextPos);\n\npos = nextPos;\n\n}\n\n}\n\nif (showPendulumPath) {\n\nstrokeWeight(1.6);\n\nfor (var i = 0; i < pendulumPath.length; i++) {\n\nvar path = pendulumPath[i];\n\nbeginShape();\n\n**7**\n\nvar hue = map(i, 0, joints, 120, 360);\n\nstroke(hue, 80, 60, 50);\n\nfor (var j = 0; j < path.length; j++) {\n\nvertex(path[j].x, path[j].y);\n\n}\n\nendShape();\n\n}\n\n}\n\n}\n\n**1** | The variable angle contains the angle for the first pendulum. In each frame, the angle is incremented slightly.  \n---|---  \n**2** | The variable center is copied to pos. It contains a value from p5. Vector. Thus, only one variable is needed to store the x and y coordinates of a point. In addition, the p5. Vector object offers much more practical computing functionalities.  \n**3** | The angle a for a pendulum is calculated from the base angle and a factor, which increases in size the farther the pendulum is from the center; e.g., for the value 2 for speedRelation, the factor for the pendulum with index 3 is, in this case, 8 (2 to the power of 3). For every other angle, the direction of rotation is reversed.  \n**4** | To reach the position of the pendulum end, use the fromAngle() function to generate a unit vector (a vector of length 1) from the angle just calculated. setMag() extends it and adds the current position pos.  \n**5** | If the pendulums are to be displayed, a circle and a line are drawn.  \n**6** | The new calculated position nextPos is added to the path of the pendulum and is used for the next iteration as start position pos.  \n**7** | The paths are allotted individual colors. Depending on their index i, a hue between 120° (green) and 360° (red) is determined.  \nKeys: | 1: Toggle display of pendulum off/on 2: Display toggle path off/on DEL: Clear canvas –/+: Jog velocity ratio –/+ Arrow↓/↑: Decrease/increase line length –/+ Arrow ←/→: Jog rotational velocity –/+ S: Save image  \n---|---  \n  \n![image](../images/f0122-01.jpg)\n\n**→ P_2_2_6_02** In other variations of the program, pendulums move along a\npath drawn with the mouse.\n\n![image](../images/f0122-02.jpg)\n\n**→ P_2_2_6_03** Marks like these occur when a pendulum no longer completely\norbits its center but—as its name suggests—swings back and forth.\n\n![image](../images/f0122-03.jpg)\n\n**→ P_2_2_6_04** If the pendulums form a branched tree structure, then there\nwill be multiple endpoints.\n\n![image](../images/f0123-01.jpg)\n\n**→ P_2_2_6_03** In this drawing mode, the background is not deleted. The\nanimation of the pendulum is continuous and thus becomes visible.\n\n![image](../images/f0124-01.jpg)\n\n**→ P_2_2_6_04** Here the end points of the tree structure are connected to\nform a transparent polygon.\n\n## **P.2.3.1 Drawing with animated brushes**\n\nIn previous chapters, agents moved autonomously according to predefined rules.\nIn this example, users will interact with an agent, creating an experimental\ndrawing tool that follows its own set of rules. What makes animated brushes so\nunusual is their ability to be inspired by their own behavior while drawing.\nThis process offers a much greater range of expression, as the act of drawing\nbecomes like a ballroom dance with a partner.\n\n**→ P_2_3_1_01**\n\n![image](../images/f0126-01.jpg)\n\nThe first drawing tool is an excellent demonstration of how much visual\npotential can be contained in very simple principles. A line rotates around\nthe mouse position. The lines consolidate in different ways depending on the\nspeed and direction of the mouse’s movements. The line changes its color and\nlength with each mouse click. The rotation speed can be set with the right or\nleft arrow keys.\n\n![image](../images/f0126-02.jpg)\n\n**→ P_2_3_1_01** The mouse was only moved back and forth along the middle of\nthe horizontal line. The animated brush thereby generated different levels of\ndensity.\n\nfunction draw() {\n\n**1**\n\nif (mouseIsPressed && mouseButton == LEFT) {\n\npush();\n\n**2**\n\ntranslate(mouseX, mouseY);\n\nrotate(radians(angle));\n\nstroke(c);\n\n**3**\n\nline(0, 0, lineLength, 0);\n\npop();\n\n**4**\n\nangle += angleSpeed;\n\n}\n\n}\n\n**1** | Only when the left mouse button is pressed will something be drawn.  \n---|---  \n**2** | The line is supposed to rotate around the mouse position. Therefore, the origin of the coordinate system must first be moved to the mouse position using the translate() function. The coordinate system is then rotated with the rotate() function.  \n**3** | The horizontal line drawn now becomes a rotating brush.  \n**4** | The rotation angle is increased by a value for the rotation speed.  \n  \n**5**\n\nfunction mousePressed() {\n\nlineLength = random(70, 200);\n\n}\n\n**5** | The length of the line changes with each click.  \n---|---  \nMouse: | Drag: Draw  \n---|---  \nKeys: | 1–4: Change color settings Space: New random color DEL: Clear canvas D: Change direction and mirror angle Arrow ↓/↑: Line length - /+ Arrow ←/→: Rotation speed - /+ S: Save image  \n  \n![image](../images/f0127-01.jpg)\n\n**→ P_2_3_1_02** In addition to the mouse movement and the different colors,\nthe step size of the rotating lines is also changed.\n\n![image](../images/f0128-01.jpg)\n\n**→ P_2_3_1_02** Lines of random colors rotate around the mouse position when\nthe mouse button is held down. Even without moving the mouse, a wealth of\nparameters such as line‘s length, color, line shape, and rotation speed can be\ninfluenced with the keys, thereby generating a variety of images.\n\n![image](../images/f0129-01.jpg)\n\n## **P.2.3.2** **Relation and distance in drawing**\n\nSince the relation of individual elements to one another is crucial to an\noverall image, it is important for users to be able to control individual\nparameters such as distance and angle; this program provides the necessary\nknowledge.\n\n**→ P_2_3_2_01**\n\nIn the previous example, a new element was drawn on the drawing canvas in each\nframe when the mouse button was held down. This function is now restricted: a\nnew element is positioned only when it stays a minimum distance from the\npreviously drawn element. There are several ways to do this:\n\nVariation 1: The new element is not placed directly at the mouse position but\nrather at the exact specified minimum distance from the last element.\n\nVariation 2: The new element is placed at the mouse position, and the minimum\ndistance only serves as a threshold value.\n\n![image](../images/f0130-01.jpg)\n\n![image](../images/f0130-02.jpg)\n\n**→ P_2_3_2_01** The more quickly the mouse is moved during the drawing\nprocess, the longer the lines become.\n\nfunction draw() {\n\n**1**\n\nif (mouseIsPressed && mouseButton == LEFT) {\n\nvar d = dist(x, y, mouseX, mouseY);\n\n**2**\n\nif (d > stepSize) {\n\nvar angle = atan2(mouseY \\- y, mouseX \\- x);\n\npush();\n\ntranslate(x, y);\n\nrotate(angle);\n\nstroke(col);\n\n**3**\n\nif (frameCount % 2 == 0) stroke(150);\n\n**4**\n\nline(0, 0, 0, lineLength * random(0.95, 1.0) * d/10);\n\npop();\n\n**5**\n\nif (drawMode == 1) {\n\nx = x + cos(angle) * stepSize;\n\ny = y + sin(angle) * stepSize;\n\n}\n\nelse {\n\nx = mouseX;\n\ny = mouseY;\n\n}\n\n}\n\n}\n\n}\n\n**1** | Holding down the mouse button (i.e., when you want to draw) calculates the distance from the last drawing position (x,y) to the current mouse position.  \n---|---  \n**2** | If this distance is greater than stepSize, a new point is drawn. To do this, the angle to the previous drawing position has to be calculated. This is easily accomplished with the function atan2(), which requires two parameters: the vertical distance between the two points mouseY-y and the horizontal distance between the two points mouseX-x.  \n| ![image](../images/f0131-01.jpg)  \n**3** | The lines are drawn alternately in different colors: the randomly selected color (col) or a medium gray.  \n**4** | A vertical line is drawn. Since the coordinate system was previously rotated by angle, the line is now perpendicular to the drawing path. The line’s length results from the basic length lineLength, a random factor that varies the length slightly, by the factor d/10. This means that the greater the distance between the old and new points, the longer the line is drawn, thereby reflecting the speed of the mouse.  \n**5** | In version 1 (drawMode==1), the new point is placed at the distance stepSize from the old position. In version 2, the mouse determines the new position.  \nMouse: | Drag: Draw  \n---|---  \nKeys: | 1–2: Drawing mode DEL: Clear canvas Arrow ↓/↑: Line length -/+ S: Save image  \n  \n![image](../images/f0131-02.jpg)\n\n**→ P_2_3_2_01 → Illustration: Victor Juarez Hernandez** The multiple\nsuperimposition of the lines can be used to create fine shading.\n\n## **P.2.3.3 Drawing with type**\n\nWho doesn’t like to switch brushes while painting? In this application, the\nposition and size of characters are constantly transformed according to the\nposition and speed of the brush. The user can paint random series of letters\nor even whole novels.\n\n**→ P_2_3_3_01**\n\nAlong the mouse’s drawing line, a text appears that is defined in the program\nand is drawn larger or smaller depending on mouse speed.\n\n![image](../images/f0132-01.jpg)\n\nfunction draw() {\n\nif (mouseIsPressed && mouseButton == LEFT) {\n\n**1**\n\nvar d = dist(x, y, mouseX, mouseY);\n\ntextSize(fontSizeMin \\+ d / 2);\n\n**2**\n\nvar newLetter = letters.charAt(counter);\n\nstepSize = textWidth(newLetter);\n\n**3**\n\nif (d > stepSize) {\n\nvar angle = atan2(mouseY \\- y, mouseX \\- x);\n\npush();\n\ntranslate(x, y);\n\nrotate(angle + random(angleDistortion));\n\ntext(newLetter, 0, 0);\n\npop();\n\n**4**\n\ncounter++;\n\nif (counter >= letters.length) counter = 0;\n\nx = x + cos(angle) * stepSize;\n\ny = y + sin(angle) * stepSize;\n\n}\n\n}\n\n}\n\n**1** | The distance between the mouse and the current writing position (x,y) is calculated. This, in turn, determines the font size for the next character. The value in fontSizeMin ensures that the font will not be smaller than a specified size.  \n---|---  \n**2** | To check whether a new letter can be written, the next character is selected from the string letters and the variable stepSize set to the character’s width.  \n**3** | When there is enough space between the mouse and the current writing position, the new letter is written.  \n**4** | The variable counter counts how many letters have already been drawn. This value is used to read the letters in succession from the specified text letters. When counter is greater than the number of letters in the original text, it is reset to 0.  \nMouse: | Drag: Draw text  \n---|---  \nKeys: | DEL: Clear canvas Arrow ↓/↑: Distortion angle -/+ S: Save image  \n  \n![image](../images/f0133-01.jpg)\n\n**→ P_2_3_3_01_TABLET → Illustration: Pau Domingo** In the tablet version, the\npressure on the pen modulates the size of the text. An image can be placed as\na template in the background and can be shown or hidden while drawing.\n\n## **P.2.3.4 Drawing with dynamic brushes**\n\nA virtual rubber band becomes a dynamic paintbrush as arbitrary basic elements\nare strung like pearls on a string between brushstrokes and a lazy agent. The\ntension between the two poles defines the size and position of the elements\nwhen drawing.\n\n**→ P_2_3_4_01**\n\nA graphic element is dragged on one end by the mouse. The opposite end moves\nsluggishly in the direction of the mouse. Depending on the drawing speed and\nset inertia, the original elements are depicted as stretched to their limits\nor virtually unchanged. The width of the element remains the same.\n\n![image](../images/f0134-01.jpg)\n\nfunction draw() {\n\nif (mouseIsPressed && mouseButton == LEFT) {\n\n**1**\n\nvar d = dist(x, y, mouseX, mouseY);\n\nif (d > stepSize) {\n\nvar angle = atan2(mouseY \\- y, mouseX \\- x);\n\npush();\n\n**2**\n\ntranslate(mouseX, mouseY);\n\nrotate(angle + PI);\n\nimage(lineModule, 0, 0, d, moduleSize);\n\npop();\n\n**3**\n\nx = x + cos(angle) * stepSize;\n\ny = y + sin(angle) * stepSize;\n\n}\n\n}\n\n}\n\n**1** | In this program, the variables x and y are used for the position of the opposite pole. The distance between these variables and the mouse position determines how much the character element is stretched.  \n---|---  \n**2** | The coordinate system is moved to the mouse position with translate() and turned so it faces the opposite pole; the element is then drawn. The additional angle, PI here, can vary depending on the SVG module. Its length is set at d (the distance to the mouse), the width to moduleSize.  \n**3** | The position of the opposite pole is moved by the value stepSize at every step. Since this movement is usually much slower than that of the mouse, a rubber-band effect is produced.  \nMouse: | Drag: Draw  \n---|---  \nKeys: | 1–9: Change module DEL: Clear canvas Arrow ↓/↑: Module size - /+ Arrow ←/→: Step size - /+ S: Save image  \n  \n![image](../images/f0135-01.jpg)\n\n**→ P_2_3_4_01 → Illustrationen: Pau Domingo** A very subdued opponent to the\nfast-moving mouse.\n\n![image](../images/f0136-01.jpg)\n\n**→ P_2_3_4_01 → Illustration: Pau Domingo** Quick, regular movements with the\nmouse generate flowing forms.\n\n![image](../images/f0137-01.jpg)\n\n**→ P_2_3_4_01 → Illustration: Pau Domingo** Different movement speeds of the\nmouse also generate different structures.\n\n## **P.2.3.5 Drawing with the pen tablet**\n\nCompared to a conventional mouse, a pen tablet puts several more parameters at\na user’s disposal, including pressure and position. When drawing, the pen\nmovement can be better recorded and interpreted so that the movement of your\nhand can be reproduced more accurately. When you use a pen tablet you are\ncloser to the generative process.\n\n**→ P_2_3_5_01_TABLET**\n\nThe additional parameters provided by a pen tablet are transferred to a basic\nelement (in this case a pen). The azimuth, pressure, and inclination define\nthe rotation, saturation, and length, respectively, of the element. The\nparameters are read using the tablet classes of the book’s Generative Design\nlibrary. The form is not loaded from an SVG file but rather drawn as a curve,\nenabling the individual curve points to be manipulated more easily.\n\n![image](../images/f0138-01.jpg)\n\n![image](../images/f0138-02.jpg)\n\n**→ P_2_3_5_01_TABLET → Illustration: Jana-Lina Berkenbusch** The new elements\nare always superimposed on the old ones, thereby making it necessary to work\nfrom the background to the foreground.\n\nfunction draw() {\n\nvar tabletValues = tablet.values();\n\n**1**\n\nvar pressure = gamma(tabletValues.pressure, 2.5);\n\nvar angle = tabletValues.azimuth;\n\nvar penLength = cos(tabletValues.inclination);\n\n**2**\n\nif (pressure > 0 && penLength > 0) {\n\npush();\n\ntranslate(mouseX, mouseY);\n\nrotate(angle);\n\n**3**\n\nvar elementLength = penLength * 250;\n\nvar h1 = random(10) * (1.2 + penLength);\n\nvar h2 = (-10 + random(10)) * (1.2 + penLength);\n\n...\n\n**4**\n\npointsX = [];\n\npointsY = [];\n\npointsX[0] = 0;\n\npointsY[0] = 0;\n\npointsX[1] = elementLength * 0.77;\n\npointsY[1] = h1;\n\npointsX[2] = elementLength;\n\npointsY[2] = 0;\n\npointsX[3] = elementLength * 0.77;\n\npointsY[3] = h2;\n\npointsX[4] = 0;\n\npointsY[4] = -5;\n\n**5**\n\nbeginShape();\n\ncurveVertex(pointsX[3], pointsY[3]);\n\nfor (var i = 0; i < pointsX.length; i++) {\n\ncurveVertex(pointsX[i], pointsY[i]);\n\n}\n\ncurveVertex(pointsX[1], pointsY[1]);\n\nendShape(CLOSE);\n\npop();\n\n}\n\n}\n\n**1** | The tablet captures many characteristics of the pen. These three queries get pressure and angle information.  \n---|---  \n**2** | The elements should only be drawn if the pen is not completely perpendicular to the tablet and the pressure is greater than zero.  \n**3** | Declaration of variables that are needed to describe the shape. Using the random() function, the shapes are varied slightly in their length and width when calculated.  \n**4** | Two arrays, pointsX and pointsY, are created for the shape’s contour points and then filled with values.  \n**5** | The shape is drawn.  \n**!** | For drawing the closed curve, see: **→P.2.2.3 Shapes from agents**  \nMouse: | Drag: Draw  \n---|---  \nTablet: | Pressure: Saturation Azimuth: Rotation Inclination: Length  \nKeys: | 1–3: Drawing mode 6–0: Colors DEL: Clear canvas S: Save image  \n  \n![image](../images/f0140-01.jpg)\n\n**→ P_2_3_5_01_TABLET → Illustration: Pau Domingo** The basic forms, which are\nsuperimposed on each other in rapid succession, lead to the creation of\ncomplex organic forms.\n\n![image](../images/f0141-01.jpg)\n\n**→ P_2_3_5_01_TABLET → Illustrations: Pau Domingo, Franz Stämmele, and Jana-\nLina Berkenbusch** From birds to mountain ranges to abstract organic forms,\nthe pen tablet inspires the creation of unique illustrations.\n\n## P.2.3.6 Drawing with complex modules\n\nTogether they are strong: simple modules become supercharacters. Using\ncombinatorics and complex modules as paintbrushes, each module is defined in\nrelation to its four neighbors, generating supercharacters. Depending on the\nmodule repertoire and constellation, many different character groups can be\ngenerated.\n\n**→ P_2_3_6_01**\n\nWith the mouse, the user draws a grid and positions various SVG modules in its\nfields. The only information that exists for each field is whether it is empty\nor filled. When the grid is displayed, the filled or empty state of each\nfield’s neighbors becomes apparent; according to this, a specific graphic\nmodule is selected.\n\nThe sixteen possible states of the four neighbors can be easily summarized in\na four-digit binary code. The correct SVG module can be determined simply by\nconverting the binary code into a decimal number, without complicated\ncombinatorics.\n\n![image](../images/f0142-01.jpg)\n\nfunction draw() {\n\nbackground(255);\n\n**1**\n\nif (mouseIsPressed) {\n\nif (mouseButton == LEFT) setTile();\n\nif (mouseButton == RIGHT) unsetTile();\n\n}\n\n**2**\n\nif (doDrawGrid) drawGrid();\n\ndrawModules();\n\n}\n\n**1** | Using the functions setTile() and unsetTile(), the state of a field in the grid is set to 1 or 0.  \n---|---  \n**2** | Using the function drawModules(), all SVG modules are displayed. The underlying grid can be drawn with the function drawGrid().  \n  \nfunction setTile() {\n\n**3**\n\nvar gridX = floor(mouseX / tileSize) + 1;\n\ngridX = constrain(gridX, 1, gridResolutionX - 2);\n\nvar gridY = floor(mouseY / tileSize) + 1;\n\ngridY = constrain(gridY, 1, gridResolutionY - 2);\n\n**4**\n\ntiles[gridX][gridY] = 1;\n\n}\n\n**3** | The mouse position is transferred to the corresponding grid field square in the grid (gridX, gridY).  \n---|---  \n**4** | The state of all the fields in the grid is saved in the two-dimensional tiles array. The clicked-on field is set to the value 1.  \n  \nfunction drawModules() {\n\nfor (var gridX=0; gridX<gridResolutionX-1; gridX++) {\n\nfor (var gridY=0; gridY<gridResolutionY-1; gridY++) {\n\n**5**\n\nif (tiles[gridX][gridY] == 1) {\n\nvar NORTH = str(tiles[gridX][gridY - 1]);\n\nvar WEST = str(tiles[gridX - 1][gridY]);\n\nvar SOUTH = str(tiles[gridX][gridY + 1]);\n\nvar EAST = str(tiles[gridX + 1][gridY]);\n\n**6**\n\nvar binaryResult = NORTH + WEST + SOUTH + EAST;\n\n**7**\n\nvar decimalResult = parseInt(binaryResult, 2);\n\nvar posX = tileSize * gridX - tileSize / 2;\n\nvar posY = tileSize * gridY - tileSize / 2;\n\n**8**\n\nimage(modules[decimalResult], posX, posY,\n\ntileSize, tileSize);\n\n...\n\n}\n\n}\n\n}\n\n}\n\n**5** | All tiles are processed. Only fields whose state is 1 (i.e., filled ones) are taken into account.  \n---|---  \n**6** | The state of the four neighbors is queried, converted into strings, and concatenated. Consequently, binaryResult contains a sequence of four zeros or ones.  \n**7** | The coded states of the binary representations are converted into a decimal number using the function parseInt().  \n**8** | The corresponding SVG module is selected with decimalResult and drawn on the drawing canvas.  \n  \nMouse: | Drag: Draw modules  \n---|---  \nDrag right mouse button: Delete modules  \nKeys: | DEL: Clear canvas  \n| G: Show grid on/off  \n| D: Show module values on/off  \n| S: Save image  \n  \n![image](../images/f0144-01.jpg)\n\n**→ P_2_3_6_02** Complete characters can be easily drawn with the tiles; the\npossibilities are endless, since new tile sets can always be generated in a\nvector program.\n\n![image](../images/f0145-01.jpg)\n\n**→ P_2_3_6_02 → Illustration: Pau Domingo** Patterns are created by filling\nin adjacent grids.\n\n![image](../images/f0145-02.jpg)\n\n**→ P_2_3_6_02 → Illustration: Cedric Kiefer** The tiles here are used as\nornamental structures.\n\n![image](../images/f0145-03.jpg)\n\n**→ P_2_3_6_02 → Illustration: Cedric Kiefer** Symmetrical masklike forms.\n\n## P.2.3.7 Drawing with multiple brushes\n\nThe arrangement of the different brushes, all of which simultaneously react to\nthe mouse position, gives the impression of drawing in a hall of mirrors. The\nhorizontal and vertical mirroring of each brush movement generates\nkaleidoscopic results.\n\n**→ P_2_3_7_01**\n\nCreating a paint program is very simple: in each frame, a line is drawn\nbetween the current mouse position and the one in the preceding frame. This\nsmall line can easily be drawn several times, either offset as exact copies\nhorizontally and vertically or as reflections: horizontal, vertical, or\ndiagonal—or both offset and reflected at the same time.\n\n![image](../images/f0146-01.jpg)\n\n![image](../images/f0146-02.jpg)\n\n**→ P_2_3_7_01** Three different ways to use mirror axes and repetitions.\n\nfunction setup() {\n\n**1**\n\ncanvasElement = createCanvas(800, 800);\n\n...\n\n**2**\n\nimg = createGraphics(width, height);\n\n...\n\n}\n\n**1** | This sketch should be used only on a square drawing canvas. Otherwise, the diagonal reflections will cause problems.  \n---|---  \n**2** | The function createGraphics() creates an image that will later be drawn. This allows the mirror axes to be displayed and hidden again.  \n  \nfunction draw() {\n\nbackground(255);\n\n**3**\n\nimage(img, 0, 0);\n\n...\n\nif (mouseIsPressed && mouseButton == LEFT) {\n\n**4**\n\nvar w = width / penCount;\n\nvar h = height / penCount;\n\n**5**\n\nvar x = mouseX % w;\n\nvar y = mouseY % h;\n\n**6**\n\nvar px = x - (mouseX \\- pmouseX);\n\nvar py = y - (mouseY \\- pmouseY);\n\nfor (var i = 0; i < penCount; i++) {\n\nfor (var j = 0; j < penCount; j++) {\n\n**7**\n\nvar ox = i * w;\n\nvar oy = j * h;\n\n**8**\n\nimg.line(x+ox, y+oy, px+ox, py+oy);\n\n**9**\n\nif (mh || md2 && md1 && mv)\n\nimg.line(w-x+ox, y+oy, w-px+ox, py+oy);\n\nif (mv || md2 && md1 && mh)\n\nimg.line(x+ox, h-y+oy, px+ox, h-py+oy);\n\nif (mv && mh || md2 && md1)\n\nimg.line(w-x+ox, h-y+oy, w-px+ox, h-py+oy);\n\n**10**\n\nif (md1 || md2 && mv && mh)\n\nimg.line(y+ox, x+oy, py+ox, px+oy);\n\nif (md1 && mh || md2 && mv)\n\nimg.line(y+ox, w-x+oy, py+ox, w-px+oy);\n\nif (md1 && mv || md2 && mh)\n\nimg.line(h-y+ox, x+oy, h-py+ox, px+oy);\n\nif (md1 && mv && mh || md2)\n\nimg.line(h-y+ox, w-x+oy, h-py+ox, w-px+oy);\n\n}\n\n}\n\n...\n\n}\n\n}\n\n**3** | In each frame, the drawn picture is first copied to the drawing canvas.  \n---|---  \n**4** | The penCount variable determines how many virtual brushes are generated in the x and y directions. A value of three produces nine brushes. The variables w and h are the width and height of a tile in which a brush moves.  \n**5** | The coordinate x, y is the position of the brush in the upper left tile.  \n**6** | The variables pmouseX and pmouseY are provided by p5.js and always contain the mouse position in the previous frame.  \n**7** | The values ox and oy represent the x and y offset for a brush.  \n**8** | In all cases, a line is drawn between the current and previous mouse positions.  \n**9** | If mh is true in value (it should be mirrored horizontally), the x position is subtracted from the tile width. Incidentally, a horizontal reflection also appears when mirroring vertically and around both diagonal axes.  \n**10** | One-diagonal reflection is most easily generated by swapping x and y.  \n  \nMouse: | Drag: Draw  \n---|---  \nKeys: | 1–4: Various reflections on/off  \n| 5–0: Colors  \n| Arrow ↓/↑: Line width –/+  \n| Arrow ←/→: Number of brushes –/+  \n| DEL: Clear canvas  \n| D: Display mirror axes on/off  \n| G: GIF recording start/stop  \n| S: Save image  \n  \n![image](../images/f0148-01.jpg)\n\n**→ P_2_3_7_02_TABLET** The number of tiles was constantly changed for this\npicture, and the graphics were often painted over with black, green, and\nwhite.\n\n![image](../images/f0149-01.jpg)\n\n**→ P_2_3_7_02_TABLET** In the version for the pen tablet, the line weight\nresponds to the pen’s pressure.\n\n\n![image](../images/f0150-01.jpg)\n\n# P.3 Type\n\nIn the Shape chapter, we demonstrated how shapes can be generated using the\nprinciples of repetition (grid), iteration (agents), and interaction\n(drawing). This chapter is devoted to a special kind of form that is also\nextremely important in design: typography. Using various methods—from the\nvisual analysis of a text to the outlines of a character—typography will be\nviewed in the following examples in the context of generative design.\n\n[P.3 Type](11-part02ch03.xhtml#p.3) [150](11-part02ch03.xhtml#p.3)\n\n[P.3.0 Hello, type](11-part02ch03.xhtml#p.3.0)\n[152](11-part02ch03.xhtml#p.3.0)\n\n[P.3.1 Text](11-part02ch03.xhtml#p.3.1) [154](11-part02ch03.xhtml#p.3.1)\n\n[P.3.1.1 Writing time-based text](11-part02ch03.xhtml#p.3.1.1)\n[154](11-part02ch03.xhtml#p.3.1.1)\n\n[P.3.1.2 Text as blueprintn](11-part02ch03.xhtml#p.3.1.2)\n[156](11-part02ch03.xhtml#p.3.1.2)\n\n[P.3.1.3 Text image](11-part02ch03.xhtml#p.3.1.3)\n[160](11-part02ch03.xhtml#p.3.1.3)\n\n[P.3.1.4 Text diagram](11-part02ch03.xhtml#p.3.1.4)\n[166](11-part02ch03.xhtml#p.3.1.4)\n\n[P.3.2 Font outline](11-part02ch03.xhtml#p.3.2)\n[170](11-part02ch03.xhtml#p.3.2)\n\n[P.3.2.1 Dissolving the font outline](11-part02ch03.xhtml#p.3.2.1)\n[170](11-part02ch03.xhtml#p.3.2.1)\n\n[P.3.2.2 Varying the font outline](11-part02ch03.xhtml#p.3.2.2)\n[174](11-part02ch03.xhtml#p.3.2.2)\n\n[P.3.2.3 Font outline from agents](11-part02ch03.xhtml#p.3.2.3)\n[178](11-part02ch03.xhtml#p.3.2.3)\n\n[P.3.2.4 Parallel font outlines](11-part02ch03.xhtml#p.3.2.4)\n[180](11-part02ch03.xhtml#p.3.2.4)\n\n[P.3.2.5 Kinetic font](11-part02ch03.xhtml#p.3.2.5)\n[184](11-part02ch03.xhtml#p.3.2.5)\n\n## P.3.0 Hello, type\n\nLetters become spaces. In generating a vector-based font, you can directly\ninfluence numerous parameters and design with letters in time and space.\nTraces of the emergence of the character and the interactive manipulation of\nits size and position can be made visible.\n\n**→ P_3_0_01**\n\nThe size of the letter is controlled with the horizontal movement of the\nmouse, and its vertical movement moves the letters up and down. The letter\nleaves a trail when the mouse button is held down.\n\n![image](../images/f0152-01.jpg)\n\n![image](../images/f0152-02.jpg)\n\n**→ P_3_0_01** The letter leaves the tracks of its changes, then becomes\nunrecognizable and generates new forms.\n\n**1**\n\nvar font = \"sans-serif\";\n\nvar letter = \"A\";\n\n**1** | The name of the font to use is saved in the variable font.  \n---|---  \n  \nfunction setup() {\n\ncreateCanvas(windowWidth, windowHeight);\n\nbackground(255);\n\nfill(0);\n\n**2**\n\ntextFont(font);\n\ntextAlign(CENTER, CENTER);\n\n}\n\n**2** | The function textFont() makes it the current font. The horizontal and vertical alignment can be specified with textAlign().  \n---|---  \n  \nfunction mouseMoved() {\n\nclear();\n\n**3**\n\ntextSize((mouseX \\- width / 2) * 5 + 1);\n\ntext(letter, width / 2, mouseY);\n\n}\n\n**3** | When the mouse is moved, the letter size changes according to the value of the horizontal mouse position. The letter is positioned horizontally in the middle of the display window width / 2, vertically in the position mouseY, and displayed using the text() command.  \n---|---  \n  \n**4**\n\nfunction mouseDragged() {\n\ntextSize((mouseX \\- width / 2) * 5 + 1);\n\ntext(letter, width / 2, mouseY);\n\n}\n\n**4** | This also occurs when the mouse is moved with the mouse button held down, but now the background does not get a new color and the letter leaves a trail.  \n---|---  \n  \nMouse: | Position x: Size  \n---|---  \n| Position y: Position  \n| Drag: Draw  \nKeys: | A–Z: Letter selection  \n| CTRL: Save image  \n  \n![image](../images/f0153-01.jpg)\n\n## P.3.1.1 Writing time-based text\n\nComposing text with automatic line breaks is nothing new. But when the\nvertical mouse position is responsible for the leading (the space between\nlines), and the elapsed time before entering each letter determines its size,\nthen the rhythm of writing begins to interact with the text.\n\n**→ P_3_1_1_01**\n\nWhen typing, the virtual “pen tip” moves from left to right over the drawing\ncanvas. When it reaches the right border, it starts again from the beginning\nand skips a line. Leading is defined by the vertical mouse position. The time\nbetween the individual keystrokes is measured. The greater the time interval,\nthe larger the entered letter.\n\n![image](../images/f0154-01.jpg)\n\n![image](../images/f0154-02.jpg)\n\n**→ P_3_1_1_01** The vertical mouse position defines the leading (line\nspacing). Different levels of legibility result.\n\nfunction draw() {\n\n...\n\nspacing = map(mouseY, 0, height, 0, 120);\n\ntranslate(0, 200 + spacing);\n\nvar x = 0;\n\nvar y = 0;\n\nvar fontSize = 20;\n\n**1**\n\nfor (var i = 0; i < textTyped.length; i++) {\n\n**2**\n\nfontSize = fontSizes[i];\n\ntextFont(font, fontSize);\n\n**3**\n\nvar letter = textTyped.charAt(i);\n\nvar letterWidth = textWidth(letter) \\+ tracking;\n\n**4**\n\nif (x + letterWidth > width) {\n\nx = 0;\n\ny += spacing;\n\n}\n\n**5**\n\ntext(letter, x, y);\n\nx += letterWidth;\n\n}\n\n**6**\n\nvar timeDelta = millis() - pMillis;\n\nnewFontSize = map(timeDelta, 0, maxTimeDelta,\n\nminFontSize, maxFontSize);\n\nnewFontSize = min(newFontSize, maxFontSize);\n\nfill(200, 30, 40);\n\nif (int(frameCount / 10) % 2 == 0) fill(255);\n\n**7**\n\nrect(x, y, newFontSize / 2, newFontSize / 20);\n\n}\n\n**1** | The variable textTyped contains the entered text. This is now processed letter by letter.  \n---|---  \n**2** | The fontSize is taken from the array fontSizes[] from index i, and the font is set to this size.  \n**3** | Now the letter at index i is selected from the string textTyped and saved in letter. In addition, the letter width textWidth(letter) is increased by the value tracking.  \n**4** | When the current position plus the character width exceeds the width of the drawing canvas, a line break occurs. Thus x is reset to 0 and the vertical position y is increased by leading.  \n**5** | The letter is drawn at the position x, y.  \n**6** | After all letters have been drawn, a blinking cursor is displayed. Over time, this is supposed to grow, so timeDelta—the time that has elapsed since the last typing operation—must be determined. The current time is obtained in milliseconds with the millis() function. The value pMillis, which was saved with the last keystroke, is subtracted from this. This time difference can now be converted to the range minFontSize to maxFontSize.  \n**7** | This value is used to draw a rectangle at the current drawing position.  \n  \nfunction keyTyped(){\n\nif(keyCode >= 32){\n\n**8**\n\ntextTyped += key;\n\nfontSizes.push(newFontSize);\n\n}\n\n}\n\n**8** | By pressing a key, the typed letter of the character string textTyped is attached and the array fontSizes[] is grown by appending the new letter size in newFontSize.  \n---|---  \n  \nfunction keyPressed() {\n\n...\n\n**9**\n\npMillis = millis();\n\n}\n\n**9** | The current time is saved in pMillis so that the time of the last keystroke is always available.  \n---|---  \n  \nMouse: | Position x: Size  \n---|---  \n| Position y: Position  \nKeys: | A–Z: Letter selection  \n| CTRL: Save image  \n  \n## P.3.1.2 Text as blueprint\n\nIn this example, time no longer determines letter size. Rather, certain\nletters modify, for instance, the text orientation. In this program, every\ncharacter is translated using a visual rule. The source text thus becomes the\nblueprint for the composition.\n\n**→ P_3_1_2_01**\n\nThe user can freely enter the text using the keyboard. Every character is\ntranslated by a fixed rule in the program that specifies what will be drawn\nand how position and size are to be altered.\n\nEvery entry can be undone using the backspace or delete key.\n\nIn this example, some of the characters are replaced by loaded SVG modules.\n\n![image](../images/f0156-01.jpg)\n\n![image](../images/f0156-02.jpg)\n\n**→ P_3_1_2_01** The program interprets a text (here the lyrics of the song\n_Taschenrechner_ by Kraftwerk) as a floor plan. Using the ALT key, different\nRandomSeeds can be generated that constantly give the text a new appearance,\nsince there are two possible random directions for the space key.\n\nfunction draw() {\n\n...\n\n**1**\n\ntranslate(centerX, centerY);\n\nscale(zoom);\n\n**2**\n\nfor (var i = 0; i < textTyped.length; i++) {\n\nvar letter = textTyped.charAt(i);\n\n**3**\n\nvar letterWidth = textWidth(letter);\n\n**4**\n\nswitch (letter) {\n\n**5**\n\ncase ' ':\n\nvar dir = floor(random(0, 2));\n\nif (dir == 0) {\n\nimage(shapeSpace, 1, -15);\n\ntranslate(4, 1);\n\nrotate(QUARTER_PI);\n\n}\n\nif (dir == 1) {\n\nimage(shapeSpace2, 1, -15);\n\ntranslate(14, -5);\n\nrotate(-QUARTER_PI);\n\n}\n\nbreak;\n\n**6**\n\ncase ',':\n\nimage(shapeComma, 1, -15);\n\ntranslate(35, 15);\n\nrotate(QUARTER_PI);\n\nbreak;\n\n...\n\n**7**\n\ndefault:\n\nfill(0);\n\ntext(letter, 0, 0);\n\ntranslate(letterWidth, 0);\n\n}\n\n}\n\n// blinking cursor after text\n\nfill(0);\n\n**8**\n\nif (int(frameCount / 6) % 2 == 0) rect(0, 0, 15, 2);\n\n}\n\n**1** | The origin of the coordinate system is moved to the point (centerX, centerY) before the text is displayed. This flexible definition allows the point to be moved via mouse interaction.  \n---|---  \n**2** | All typed letters are processed sequentially.  \n**3** | The character width of each letter is calculated so that the writing position can later be offset to the correct distance.  \n**4** | The heart of the program is the set of rules that specifies how the individual characters impact the image and writing behavior. For this purpose, the current letter is distinguished using the switch() command.  \n**5** | A space is translated as follows: depending on the random value dir, one of the two loaded SVG modules shapeSpace or shapeSpace2 is drawn; the writing position is adjusted using translate(); and the writing direction is rotated 45° to either the left or right using rotate().  \n**6** | For other special characters (in this case, the comma), SVGs are also loaded in variables. When one of these characters appears, the corresponding module is drawn, and the writing position and direction are adjusted.  \n**7** | With all other characters the letter is drawn and the writing position shifted by the letterWidth.  \n**8** | Display of the blinking cursor: this construction uses the p5.js variable frameCount, which automatically increments in each frame, and the modulo operator % to produce alternately the values 0 and 1. This is used to fade the cursor in and out.  \n  \nMouse: | Drag: Move drawing canvas  \n---|---  \nKeys: | A–Z: Input text  \n| Punctuation mark: Curves  \n| Space bar: Curve with random position  \n| DEL: Delete letters  \n| Arrow ↓/↑: Zoom into the drawing canvas  \n| ALT: New random layout  \n| CTRL: Save image  \n  \n![image](../images/f0158-01.jpg)\n\n**→ P_3_1_2_02 → Illustration: Cedric Kiefer**  \nTyped letters are replaced by various elements—e.g., ENTER = “begin new line.”\nThe text is completely transformed into an image.\n\n## P.3.1.3 Text image\n\nWhich characters appear, and how often? The properties of an analyzed text can\ngenerate images. The number of appearances in a text is calculated for each\ncharacter and determines its appearance. The color of the letters, for\ninstance, corresponds to how frequently they appear. Ultimately we do not even\nneed the letters anymore.\n\n**→ P_3_1_3_01**\n\nA text is processed character by character, and each character’s corresponding\ncounter is advanced. That results in a list of counters representing the\nfrequency of each character. These values can now be used as parameters for\ndisplaying the text.\n\n![image](../images/f0160-01.jpg)\n\n...\n\n**1**\n\nvar alphabet = \"ABCDEFGHIJKLMNORSTUVWYZÄÖÜß,.;!? \";\n\nvar counters = [];\n\n...\n\n**1** | The character string alphabet determines which characters are to be counted. The counters array is initialized with the string’s length and thereby provides each character with a counter.  \n---|---  \n  \nfunction preload() {\n\n**2**\n\njoinedText = loadStrings(\"data/faust_kurz.txt\");\n\n}\n\n**2** | The text to be analyzed is loaded with the loadStrings() function.  \n---|---  \n  \nfunction setup() {\n\ncreateCanvas(620, windowHeight);\n\n...\n\n**3**\n\njoinedText = joinedText.join(\" \");\n\nfor (var i = 0; i < alphabet.length; i++) {\n\ncounters[i] = 0;\n\n}\n\n**4**\n\ncountCharacters();\n\n}\n\n**3** | The individual texts are now located in an array of strings. Since it is more practical to work with a continuous text, the lines are joined together using the join() function.  \n---|---  \n**4** | The countCharacters() function is called to determine the frequency.  \n  \nfunction countCharacters() {\n\nfor (var i = 0; i < joinedText.length; i++) {\n\n**5**\n\nvar c = joinedText.charAt(i);\n\nvar upperCaseChar = c.toUpperCase();\n\n**6**\n\nvar index = alphabet.indexOf(upperCaseChar);\n\nif (index >= 0) counters[index]++;\n\n}\n\n}\n\n**5** | The text is processed: a character is taken from the text using charAt() and converted into an uppercase letter using the toUpperCase() function.  \n---|---  \n**6** | The function indexOf() is used to find where each letter from the given character string is positioned in the alphabet. If the letter is found, the index for the corresponding letter is incremented.  \n  \nfunction draw() {\n\n...\n\n**7**\n\nposX = 20;\n\nposY = 40;\n\nfor (var i = 0; i < joinedText.length; i++) {\n\nvar upperCaseChar=joinedText.charAt(i).toUpperCase();\n\n**8**\n\nvar index = alphabet.indexOf(upperCaseChar);\n\nif (index < 0) continue;\n\n**9**\n\nif (drawAlpha) {\n\nfill(87, 35, 129, counters[index] * 3);\n\n} else {\n\nfill(87, 35, 129);\n\n}\n\n**10**\n\nvar sortY = index * 20 + 40;\n\nvar m = map(mouseX, 50, width \\- 50, 0, 1);\n\n**11**\n\nm = constrain(m, 0, 1);\n\nvar interY = lerp(posY, sortY, m);\n\ntext(joinedText.charAt(i), posX, interY);\n\n**12**\n\nposX += textWidth(joinedText.charAt(i));\n\nif (posX >= width \\- 200 && upperCaseChar == \" \") {\n\nposY += 30;\n\nposX = 20;\n\n}\n\n}\n\n}\n\n**7** | The variables posX and posY are initialized with the values at the start.  \n---|---  \n**8** | When drawing, the text is processed from the beginning each time, and, just as in the countCharacters() function, the index of the current character is determined for the counter array. If the character is not found—index < 0—the drawing process for this character is canceled using the continue command.  \n**9** | When the drawing mode drawAlpha is engaged, the transparency of the fill color is dependent on the frequency of the character and set with counters[index].  \n**10** | The variable sortY is introduced to sort the characters. This represents the y-coordinate of the line to which the character should go or migrate. For A, it is 40. For B, the value is 60, etc.  \n**11** | The mouse position is converted into a number m between 0 and 1 that will serve as an interpolation variable. Using the lerp() function, an interpolation between posY and sortY is carried out, and the calculated value interY is used to position the character.  \n**12** | Now only the writing position has to be updated. The value posX is increased by the character width; if this value approximates the right border of the display and the current character is a space, then the line break takes place. The value posY is increased by the line spacing and posX set back to the left.  \nMouse: | Position x: Survivors from the normal and sorted text  \n---|---  \nKeys: | 1: Switch alpha mode  \n| S: Save image  \n  \n![image](../images/f0161-01.jpg)\n\n**→ P_3_1_3_01** The horizontal mouse position controls whether the letters\nare displayed in the normal text position or sorted.\n\n![image](../images/f0162-01.jpg)\n\n![image](../images/f0163-01.jpg)\n\n**→ P_3_1_3_03** The frequency of the letters in a text are coded here several\ntimes and in various combinations: by the size and transparency of the green\ncircles, by the length of the purple lines, and by the opacity of the letters\nthemselves.\n\n![image](../images/f0164-01.jpg)\n\n**→ P_3_1_3_04** Identical letters are connected by colored lines; the colors\nof the lines pass through the color wheel once in alphabetical order. The\nletters are sorted by their frequency when the mouse is moved to the right.\n\nAny kind of letter can be individually switched on and off, allowing, e.g.,\nthe frequency of the vowels to be observed separately. The gray line (see\nillustration on the right), which can be turned on and off using key 2,\nconnects each letter with the letter directly following it. Although hardly\nvisible in the normal text, when arranged by letter, these gray lines create a\nstriking network structure.\n\n![image](../images/f0165-01.jpg)\n\n## P.3.1.4 Text diagram\n\nWhat were Jane Austen’s favorite words? The possibility of mechanically\nreading and processing large amounts of text provides considerable room for\nexperimentation. All the words in _Pride and Prejudice_ , for instance, can be\ncounted and their frequency represented by elements (in this case, rectangles)\nof varying sizes to create diagrams that function as static literary\ncriticism.\n\n**→ P_3_1_4_01**\n\nThe aim of a so-called treemap is to divide a rectangle into smaller\nrectangles according to the frequency with which each word is used in a text.\nThe complete text of Jane Austen’s _Pride and Prejudice_ is read from a text\nfile and transferred to the Treemap class for visualization. Individual\nparameters for the TreeMap algorithm can be keyed in.\n\n![image](../images/f0166-01.jpg)\n\n![image](../images/f0166-02.jpg)\n\n**→ P_3_1_4_01** Section of a treemap arranged according to frequency, in\nwhich only horizontal elements are permitted.\n\n**1**\n\nvar mapData = {};\n\n...\n\n**2**\n\nvar doSort = true;\n\nvar rowDirection = \"both\";\n\n**1** | The data container mapData will later be filled with the loaded text’s words and their numbers.  \n---|---  \n**2** | The doSort and rowDirection variables control the various layouts of the TreeMap.  \n  \nfunction setup() {\n\n...\n\njoinedText = joinedText.join(\" \");\n\n**3**\n\nvar words = joinedText.match(/\\w+/g);\n\n**4**\n\ntreemap = new gd.Treemap(1, 1, width - 3, height - 3,\n\n{sort:doSort, direction:rowDirection});\n\nfor (var i = 0; i < words.length; i++) {\n\n**5**\n\nvar w = words[i].toLowerCase();\n\ntreemap.addData(w);\n\n}\n\ntreemap.calculate();\n\n}\n\n**3** | The complete text is split into individual words using match(). The parameter for this function is a regular expression. With its help, very complex text searches can be performed. The one used here is relatively simple: one or more consecutive (+) word characters (\\w) are to be searched. The g at the end means “global,” i.e., the search should not stop after the first occurrence.  \n---|---  \n**4** | The TreeMap is generated. The first four parameters specify position (x, y), width, and height. In addition, an object is swapped for the display options.  \n**5** | The array with the words is processed. Each word is initially converted into lowercase letters so that different capitalizations of the same word (e.g., when it is used as the first word of a sentence) are still recognized. Then, addData() adds the word to the treemap and starts the tally with calculate().  \n  \nfunction draw() {\n\nbackground(255);\n\ntextAlign(CENTER, BASELINE);\n\n**6**\n\nfor (var i = 0; i < treemap.items.length; i++) {\n\nvar item = treemap.items[i];\n\nfill(255);\n\nstroke(0);\n\nstrokeWeight(1);\n\n**7**\n\nrect(item.x, item.y, item.w, item.h);\n\nvar word = item.data;\n\ntextFont(font, 100);\n\n**8**\n\nvar textW = textWidth(word);\n\nvar fontSize = 100 * (item.w * 0.9) / textW;\n\n**9**\n\nfontSize = min(fontSize, (item.h * 0.9));\n\ntextFont(font, fontSize);\n\nfill(0);\n\nnoStroke();\n\n**10**\n\ntext(word, item.x + item.w/2, item.y + item.h*0.8);\n\n}\n\n...\n\n}\n\n**6** | The Treemap class was used to calculate a treemap element for each word, all of which are now available as items in the array.  \n---|---  \n**7** | Position, width, and height of the element are automatically available in the variables x,y and w and h and can be used to draw the element’s frame.  \n**8** | Then the font size is set to 100 to determine the width of the word. Using a rule of thirds, the font size can be calculated so that the word fits within the width of the item.w rectangle.  \n**9** | The word, however, may not be taller than the rectangle.  \n**10** | The word is written in the rectangle.  \nKeys: | R: Random on/off H: Horizontal arrangement V: Vertical arrangement B: Arrangement in both directions S: Save image  \n---|---  \n  \n![image](../images/f0168-01.jpg)\n\n**→ P_3_1_4_01** The frequency of all the words in Jane Austen’s _Pride and\nPrejudice_ as a treemap. The algorithm tries to keep the rectangles as square\nas possible in this arrangement.\n\n![image](../images/f0169-01.jpg)\n\n**→ P_3_1_4_02** In the second version of the program, words are grouped\naccording to the number of letters. You can use the number keys (1–9) to show\nor hide individual groups.\n\n## P.3.2.1 Dissolving the font outline\n\nA text is made up of characters. A character, in turn, is shaped by its\noutlines. In the following chapters, this outline dissipates into a multitude\nof points and will establish the basis for generative font manipulation.\nIndividual points are replaced by other elements, thereby disguising the\noriginal font.\n\n**→ P_3_2_1_01**\n\nThe starting point is a text and a font file. The opentype.js Library,\ndesigned by Frederik De Bleser, generates a multitude of points onto the font\noutline. This information can then be used to give the characters new visual\nidentities.\n\n![image](../images/f0170-01.jpg)\n\n![image](../images/f0170-02.jpg)\n\n**→ P_3_2_1_02** SVGs are loaded and placed on the character outline in this\nversion of the program. Rotation angles and scaling can be controlled with the\nmouse.\n\n**1**\n\nfunction setup() {\n\n...\n\nopentype.load('data/FreeSans.otf', function(err, f) {\n\nfont = f;\n\nloop();\n\n});\n\n}\n\n**1** | The opentype.js library loads the font file and stores it in the font variable.  \n---|---  \n  \nfunction draw() {\n\n...\n\nif (textTyped.length > 0) {\n\n**2**\n\nvar fontPath = font.getPath(textTyped, 0, 0, 200);\n\n**3**\n\nvar path = new g.Path(fontPath.commands);\n\n**4**\n\npath = g.resampleByLength(path, 11);\n\nstroke(181, 157, 0);\n\nstrokeWeight(1.0);\n\nvar l = 5;\n\n**5**\n\nfor (var i = 0; i < path.commands.length; i++) {\n\nvar pnt = path.commands[i];\n\nline(pnt.x \\- l, pnt.y \\- l, pnt.x + l, pnt.y + l);\n\n}\n\nfill(0);\n\nnoStroke();\n\nvar diameter = 7;\n\nfor (var i = 0; i < path.commands.length; i++) {\n\nvar pnt = path.commands[i];\n\n**6**\n\nif (i % 2 == 0) {\n\nellipse(pnt.x, pnt.y, diameter, diameter);\n\n}\n\n}\n\n}\n\n...\n\n}\n\n**2** | There are three steps to extract the points: first, the getPath() function of the opentype.js library converts the characters to path.  \n---|---  \n**3** | This is then converted to a g.Path object (g.js is another library).  \n**4** | The command resampleByLength() divides this path into sections of equal length (here eleven pixels).  \n**5** | The dots are processed. Short, angled lines are first drawn on their positions. ![image](../images/f0171-01.jpg)  \n**6** | Next, black circles are processed. In this run, only every second position is used. ![image](../images/f0171-02.jpg)  \nKeys: | Keyboard: Text input DEL: Delete letters CTRL: Save image  \n---|---  \n  \n![image](../images/f0171-03.jpg)\n\n**→ P_3_2_1_01** Graphic elements are placed on character outlines.\n\n![image](../images/f0172-01.jpg)\n\n**→ P_3_2_1_02** The appearance of the generated character forms can be\nregulated by the scaling and rotation of the elements on the character\noutlines.\n\n## **P.3.2.2 Varying the font outline**\n\nIf the font outline is not made up of straight lines or curves but\ncontrollable elements instead, then we increasingly free ourselves from the\nunderlying font. All existing base points are joined together with specially\nformed Bézier curves. This is just one of the myriad ways of quickly creating\ndozens of new fonts.\n\n**→ P_3_2_2_01**\n\nThe dots on the text outline are connected with Bézier curves. The shape of\nthe curves can be controlled interactively with the mouse.\n\n![image](../images/f0174-01.jpg)\n\n![image](../images/f0174-02.jpg)\n\n**→ P_3_2_2_01** Different settings for the height and rotation of the Bézier\ncurves.\n\nfunction draw() {\n\n...\n\nif (textTyped.length() > 0) {\n\n...\n\n**1**\n\nvar addToAngle = map(mouseX, 0, width, -PI, +PI);\n\nvar curveHeight = map(mouseY, 0, height, 0.1, 2);\n\n**2**\n\nfor (var i = 0; i < path.commands.length-1; i++) {\n\nvar pnt0 = path.commands[i];\n\nvar pnt1 = path.commands[i+1];\n\nvar d = dist(pnt0.x, pnt0.y, pnt1.x, pnt1.y);\n\n**3**\n\nif (d > 20) continue;\n\n**4**\n\nvar stepper = map(i%2, 0, 1, -1, 1);\n\nvar angle = atan2(pnt1.y-pnt0.y, pnt1.x-pnt0.x);\n\nangle = angle + addToAngle;\n\nvar cx = pnt0.x+cos(angle*stepper)*d*4*curveHeight;\n\nvar cy = pnt0.y+sin(angle*stepper)*d*3*curveHeight;\n\n**5**\n\nbezier(pnt0.x,pnt0.y, cx,cy, cx,cy, pnt1.x,pnt1.y);\n\n}\n\n}\n\n}\n\n**1** | The variables addToAngle and curveHeight result from the x- and y-coordinates of the mouse position and control the rotation and height of the Bézier curves.  \n---|---  \n**2** | The dots are processed from the first to the next-to-last. The distance from the current dot to the next one is calculated each time.  \n**3** | When the distance is greater than fifty, this loop is aborted and no line is drawn. Since the opentype.js library provides the dots for the entire text as a chain of dots, it ensures that the individual letters are not connected.  \n**4** | For the variable stepper, the values –1 and 1 are alternately produced. These are used to calculate the control points for the Bézier curve cx, cy in order to move the curve up and down.  \n**5** | Four points have to be defined when drawing the Bézier curve: the beginning point, the end point, and two control points. The control point just calculated is used twice here.  \nMouse: | Position x: Rotation of curve Position y: Height of curve  \n---|---  \nKeys: | Keyboard: Input text DEL: Delete letters ALT: Change filling mode CTRL: Save image  \n  \n![image](../images/f0175-01.jpg)\n\nA new outline is produced and moves up and down around the original outline;\nit consists of individual Bézier curves that are defined by the beginning and\nend points and the two control points, which in this case have the same value:\nc of p[0] = c of p[1].\n\n![image](../images/f0176-01.jpg)\n\n**→ P_3_2_2_01** The mouse position determines the form of the Bézier curves.\nIt is possible to switch to filled curves by using the ALT key.\n\n## **P.3.2.3 Font outline from agents**\n\nHow long is a letter recognizable as such? In this example, the outlines of a\nletter serve as the source shape. Each individual nodal point moves like a\ndumb agent. Over time, the letter becomes illegible and is transformed into\nsomething new.\n\n**→ P_3_2_3_01**\n\nPoints are again generated from a font outline. Each point becomes an\nindependent dumb agent but remains connected to its neighbor.\n\n![image](../images/f0178-01.jpg)\n\n![image](../images/f0178-02.jpg)\n\n![image](../images/f0179-01.jpg)\n\n**→ P_3_2_3_01** The more time that passes without a key being pressed, the\nmore a character becomes deformed.\n\nfunction draw() {\n\n...\n\n**1**\n\ntranslate(letterX, letterY);\n\ndanceFactor = 1;\n\n**2**\n\nif (mouseIsPressed && mouseButton == LEFT)\n\ndanceFactor = map(mouseX, 0, width, 0, 3);\n\nif (pnts.length > 0) {\n\nfor (var i = 0; i < pnts.length; i++) {\n\n**3**\n\npnts[i].x += random(-stepSize, stepSize)\n\n* danceFactor;\n\npnts[i].y += random(-stepSize, stepSize)\n\n* danceFactor;\n\n}\n\nstrokeWeight(0.1);\n\nstroke(0);\n\nbeginShape();\n\nfor (var i = 0; i < pnts.length; i++) {\n\n**4**\n\nvertex(pnts[i].x, pnts[i].y);\n\nellipse(pnts[i].x, pnts[i].y, 7, 7);\n\n}\n\n**5**\n\nvertex(pnts[0].x, pnts[0].y);\n\nendShape();\n\n}\n\npop();\n\n}\n\n**1** | The origin of the coordinate system is moved to the current writing position before a letter is written.  \n---|---  \n**2** | By keeping the mouse button pressed down, the variable danceFactor is set to a value, which increases proportionally to the value of the mouse’s x-coordinate.  \n**3** | Random values are added to a point’s position in every iteration. The value danceFactor increases the speed of the movement.  \n**4** | Lines connect the dots.  \n**5** | Finally, another line is drawn to the first point, closing the outline.  \nMouse: | Left click + position x: Deformation speed  \n---|---  \nKeys: | Keyboard: Input text SHIFT: Movement start/stop DEL: Clear canvas CTRL: Save image  \n  \n## **P.3.2.4 Parallel font outlines**\n\nUsing the moiré effect of overlapping grid structures, you can create optical\nillusions that affect font outlines and change the impression of font volumes.\nEventually forms emerge that detach themselves from the font and lead a life\nof their own.\n\n**→ P_3_2_4_01**\n\nThe starting point is the font contour of a letter. For each of the many short\nsections that make up the font’s outline, the same calculation procedure is\nused: the section is rotated 90° and set to the correct length. The result is\na path that runs parallel to the original path. The grid structure arises when\nthis is repeated several times with ever-increasing distances.\n\n![image](../images/f0180-01.jpg)\n\n![image](../images/f0180-04.jpg)\n\n**→ P_3_2_4_01** The lowercase letter “a” shown in three variations. The font\noutline here, however, has been increasingly simplified.\n\n**1**\n\nfunction createLetters() {\n\nletters = [];\n\n**2**\n\nvar chars = textTyped.split('');\n\nvar x = 0;\n\nfor (var i = 0; i < chars.length; i++) {\n\nif (i > 0) {\n\n**3**\n\nvar charsBefore = textTyped.substring(0, i);\n\nx = font.textBounds(charsBefore, 0, 0, fontSize).w;\n\n}\n\n**4**\n\nvar newLetter = new Letter(chars[i], x, 0);\n\nletters.push(newLetter);\n\n}\n\n}\n\n**1** | When the program starts, or whenever the entered text changes, the createLetters() function is called.  \n---|---  \n**2** | There, the input text with split() generates an array of single letters, chars.  \n**3** | To determine the x-position of a letter, use substring() to remove the substring up to the current character and use the textBounds() function to calculate its width, w.  \n**4** | For each letter a new instance of the letter class is created and added to the array letters.  \n  \nfunction Letter(char, x, y) {\n\nthis.char = char;\n\nthis.x = x;\n\nthis.y = y;\n\n**5**\n\nLetter.prototype.draw = function() {\n\n**6**\n\nvar path = font.textToPoints(\n\nthis.char, this.x, this.y, fontSize,\n\n{sampleFactor: pathSampleFactor});\n\nstroke(shapeColor);\n\n**7**\n\nfor (var d = 0; d < ribbonWidth; d += density) {\n\nbeginShape();\n\nfor (var i = 0; i < path.length; i++) {\n\nvar pos = path[i];\n\n**8**\n\nvar nextPos = path[i + 1];\n\n**9**\n\nif (nextPos) {\n\nvar p0 = createVector(pos.x, pos.y);\n\nvar p1 = createVector(nextPos.x, nextPos.y);\n\n**10**\n\nvar v = p5.Vector.sub(p1, p0);\n\n**11**\n\nv.normalize();\n\nv.rotate(HALF_PI);\n\nv.mult(d);\n\n**12**\n\nvar pneu = p5.Vector.add(p0, v);\n\ncurveVertex(pneu.x, pneu.y);\n\n}\n\n}\n\nendShape(CLOSE);\n\n}\n\n}\n\n}\n\n**5** | The letter class has a draw() function called by the main program in each frame. There, the font outline is moved farther and farther inward.  \n---|---  \n**6** | The textToPoints() function turns the char character into an array of points.  \n**7** | This loop draws the individual paths. In each loop, the variable d contains the distance of the path to be drawn from the original path.  \n**8** | Two consecutive positions are taken from the array path.  \n**9** | If nextPos is not empty (i.e., the end of the path has not yet been reached), the two positions are converted to values of type p5.Vector with createVector().  \n**10** | sub() calculates the difference between the two points and stores them in v.  \n**11** | The vector v is moved to length 1 with normalize(), rotated 90° with rotate(), and then multiplied by d.  \n**12** | The position on the offset path is determined by adding v to p0.  \nMouse: | Position x: Simplification of font outline Position y: Width of ribbon outline  \n---|---  \nKeys: | Arrow ←/→: Change line density Arrow ↓/↑: Change font size  \n  \n![image](../images/f0181-01.jpg)\n\n**→ P_3_2_4_01** Four characters: percent, plus, star, and paragraph. The\noutline was greatly simplified. This results in ornate figures.\n\n## **P.3.2.5 Kinetic font**\n\nHere the font outline may do as it wishes. Ignoring legibility, it transforms\ninto patterns and leaves no formal gimmick untried. In constant motion, these\nmetamorphoses remain alive and make us wonder: When does writing become a form\nof its own?\n\n**→ P_3_2_5_01**\n\nNormally, it is good if every newly generated random number is truly random.\nWhen creating animations, however, this usually leads to the image flickering.\nThe use of Perlin noise prevents this. This method of calculating random\nnumbers generates values where the difference from one to the next is never\nvery large.\n\n![image](../images/f0184-01.jpg)\n\n![image](../images/f0184-02.jpg)\n\n**→ P_3_2_5_01** Rotating lines—sometimes densely arranged, sometimes with a\ngreater distance between them.\n\n**1**\n\nfunction setupText() {\n\ntextImg = createGraphics(width, height);\n\ntextImg.pixelDensity(1);\n\ntextImg.background(255);\n\ntextImg.textFont(font);\n\ntextImg.textSize(fontSize)\n\n**2**\n\ntextImg.text(textTyped, 100, fontSize + 50);\n\n**3**\n\ntextImg.loadPixels();\n\n}\n\n**1** | Each time the text is changed, the setupText() function is called. This creates a so-called off-screen graphic using createGraphics(). This is an image that is not visible but exists only in memory.  \n---|---  \n**2** | The entered text, textTyped, is written in this image in the previously set font and size.  \n**3** | Call loadPixels() to be able to read the individual pixel values later.  \n  \nfunction draw() {\n\nbackground(255);\n\nnOff++;\n\nfor (var x = 0; x < textImg.width; x+=pointDensity) {\n\nfor (var y = 0; y < textImg.height; y+=pointDensity)\n\n{\n\n**4**\n\nvar index = (x \\+ y * textImg.width) * 4;\n\n**5**\n\nvar r = textImg.pixels[index];\n\nif (r < 128) {\n\nif(drawMode == 1){\n\nstrokeWeight(1);\n\nvar noiseFac = map(mouseX, 0,width, 0,1);\n\nvar lengthFac = map(mouseY, 0,height, 0.01,1);\n\n**6**\n\nvar num = noise((x+nOff) * noiseFac,\n\ny * noiseFac);\n\n**7**\n\nif (num < 0.6) {\n\nstroke(colors[0]);\n\n} else if (num < 0.7) {\n\nstroke(colors[1]);\n\n} else {\n\nstroke(colors[2]);\n\n}\n\npush();\n\ntranslate(x, y);\n\nrotate(radians(frameCount));\n\n**8**\n\nline(0, 0, fontSize * lengthFactor, 0);\n\npop();\n\n}\n\n...\n\n}\n\n}\n\n}\n\n}\n\n**4** | The color values of the image are stored as a long string of values. Therefore, to get the color value of a pixel, an index must be calculated from x and y. The factor 4 is necessary because one pixel consists of four separately stored values (one each for red, green, blue, and transparency).  \n---|---  \n**5** | The image with the text consists only of black, white, and a few gray pixels. Therefore, it is sufficient to check only if the red value r is below a certain limit, in which case it is a dark pixel.  \n**6** | A random value is required to color the lines. To avoid flickering, noise() is preferable to the random() function. This produces random numbers, similar to a mountainous landscape. The function noise() is called with two parameters here, the first dependent on x, the second on y. The variable nOff is incremented continuously, thus ensuring an animation of the random numbers.  \n**7** | Depending on num, one of the three predefined colors will be selected.  \n**8** | A horizontal line is drawn in a previously shifted and rotated coordinate system.  \nMouse: | Position x/y: Different parameters (depending on drawing mode)  \n---|---  \nKeys: | Keyboard: Text input Arrow ←/→: Change drawing mode Arrow ↓/↑: Change point density DEL: Clear canvas CTRL: Save image  \n  \n![image](../images/f0186-01.jpg)\n\n**→ P_3_2_5_01 to → P_3_2_5_03** Different parameter settings and results from\nall three versions of the program. The letter shapes are generated in\ndifferent ways: from the pixels (version 01), entirely programmed (version\n02), or from the font contours (version 03).\n\n\n![image](../images/f0188-01.jpg)\n\n# P.4 Image\n\nIn the last chapter, we saw how text can be dissolved and how the resulting\nelements—words, letters, and even dots on contours—can be used for\nexperimentation. Similarly, images can be manipulated: details can be copied,\ncollages can be produced, and pixels—the digital image’s smallest units of\ninformation—can become the basis of a new visual world.\n\n[P.4 Image](12-part02ch04.xhtml#p.4) [188](12-part02ch04.xhtml#p.4)\n\n[P.4.0 Hello, image](12-part02ch04.xhtml#p.4.0)\n[190](12-part02ch04.xhtml#p.4.0)\n\n[P.4.1 Image cutouts](12-part02ch04.xhtml#p.4.1)\n[192](12-part02ch04.xhtml#p.4.1)\n\n[P.4.1.1 Image cutouts in a grid](12-part02ch04.xhtml#p.4.1.1)\n[192](12-part02ch04.xhtml#p.4.1.1)\n\n[P.4.1.2 Feedback of image cutouts](12-part02ch04.xhtml#p.4.1.2)\n[196](12-part02ch04.xhtml#p.4.1.2)\n\n[P.4.2 Image collection](12-part02ch04.xhtml#p.4.2)\n[198](12-part02ch04.xhtml#p.4.2)\n\n[P.4.2.1 Collage from image collection](12-part02ch04.xhtml#p.4.2.1)\n[198](12-part02ch04.xhtml#p.4.2.1)\n\n[P.4.2.2 Time-based image collection](12-part02ch04.xhtml#p.4.2.2)\n[202](12-part02ch04.xhtml#p.4.2.2)\n\n[P.4.3 Pixel values](12-part02ch04.xhtml#p.4.3)\n[204](12-part02ch04.xhtml#p.4.3)\n\n[P.4.3.1 Graphics from pixel values](12-part02ch04.xhtml#p.4.3.1)\n[204](12-part02ch04.xhtml#p.4.3.1)\n\n[P.4.3.2 Type from pixel values](12-part02ch04.xhtml#p.4.3.2)\n[210](12-part02ch04.xhtml#p.4.3.2)\n\n[P.4.3.3 Real-time pixel values 214](12-part02ch04.xhtml#p.4.3.3)\n\n[P.4.3.4 Emojis from pixel values](12-part02ch04.xhtml#p.4.3.4)\n[220](12-part02ch04.xhtml#p.4.3.3)\n\n## **P.4.0 Hello, image**\n\nA digital image is a mosaic of small color tiles. Dynamic access to these tiny\nelements allows for the generation of new compositions. It is possible to\ncreate your own collection of image tools with the following programs.\n\n**→ P_4_0_01**\n\nAn image is loaded and displayed in a grid defined by the mouse. Each tile in\nthe grid is filled with a scaled copy of the source image.\n\n![image](../images/f0190-01.jpg)\n\n![image](../images/f0190-02.jpg)\n\noriginal image\n\n![image](../images/f0190-03.jpg)\n\n**→ P_4_0_01** Abstract images are created through the repeated copying and\nextreme scaling of the source image.\n\n**1**\n\nvar img;\n\nfunction preload() {\n\nimg = loadImage('data/image.jpg');\n\n}\n\n**1** | The image is loaded in the preload() function. This ensures that the loading process is completed before calling the setup() and draw() functions.  \n---|---  \n  \n**2**\n\nfunction draw() {\n\nvar tileCountX = mouseX / 3 + 1;\n\nvar tileCountY = mouseY / 3 + 1;\n\nvar stepX = width / tileCountX;\n\nvar stepY = height / tileCountY;\n\nfor (var gridY = 0; gridY < height; gridY += stepY) {\n\nfor (var gridX = 0; gridX < width; gridX += stepX) {\n\n**3**\n\nimage(img, gridX, gridY, stepX, stepY);\n\n}\n\n}\n\n}\n\n**2** | The mouse position determines tileCountX and tileCountY and, thereby, their width stepX and height stepY.  \n---|---  \n**3** | The image is drawn using the function image(). The upper-left corner of the image is located in the grid (gridX, gridY); width and height are determined by tile width stepX and tile height stepY.  \nMouse: | Position x: Number of horizontal tiles Position y: Number of vertical tiles  \n---|---  \nKeys: | S: Save image  \n  \n![image](../images/f0191-01.jpg)\n\n## **P.4.1.1 Image cutouts in a grid**\n\nThe principle illustrated below is almost the same as the one in the previous\nexample, and yet a whole new world of images emerges. An image’s details and\nfine structures become pattern generators when only a portion of it is\nselected and configured into tiles. The results are even more interesting if\nthese sections are randomly selected.\n\n**→ P_4_1_1_01**\n\nUsing the mouse, a section of the image is selected in the display window.\nAfter releasing the mouse button, several copies of this section are stored in\nan array and organized in a grid. The program offers two variations. In\nvariation one, all copies are made from the exact same section. In variation\ntwo, the section is shifted slightly at random each time.\n\n![image](../images/f0192-01.jpg)\n\n![image](../images/f0192-02.jpg)\n\noriginal image\n\n![image](../images/f0192-03.jpg)\n\n**→ P_4_1_1_01** By repeatedly copying and moving image sections, abstract\nimages are created.\n\n**1**\n\nfunction cropTiles() {\n\ntileWidth = width / tileCountY;\n\ntileHeight = height / tileCountX;\n\n**2**\n\nimgTiles = [];\n\nfor (var gridY = 0; gridY < tileCountY; gridY++) {\n\nfor (var gridX = 0; gridX < tileCountX; gridX++) {\n\n**3**\n\nif (randomMode) {\n\ncropX = int(random(mouseX \\- tileWidth / 2,\n\nmouseX \\+ tileWidth / 2));\n\ncropY = int(random(mouseY \\- tileHeight / 2,\n\nmouseY \\+ tileHeight / 2));\n\n}\n\n**4**\n\ncropX = constrain(cropX, 0, width \\- tileWidth);\n\ncropY = constrain(cropY, 0, height \\- tileHeight);\n\n**5**\n\nimgTiles.push(img.get(cropX, cropY,\n\ntileWidth, tileHeight));\n\n}\n\n}\n\n}\n\n**1** | The core of the program is the cropTiles() function. Here the image is fragmented and the copies of the sections are stored in an array.  \n---|---  \n**2** | The image framing array imgTiles is cleared.  \n**3** | When version two comes into play (randomMode is true), all the values for cropX and cropY are randomly selected from a value range around the mouse position.  \n**4** | The constrain() function ensures that the cutout section does not extend beyond the image boundaries.  \n**5** | Finally, the section is copied from the image img using get() and stored in the array.  \nMouse: | Position x/y: Detail positioning Left click: Copy detail  \n---|---  \nKeys: | 1–3: Change detail size R: Random on/off S: Save image  \n  \n![image](../images/f0193-01.jpg)\n\n![image](../images/f0194-01.jpg)\n\n**→ P_4_1_1_01** The multiplication of small image sections creates rhythmic\nstructures that are only recognizable as image sections at a second glance.\n\n![image](../images/f0195-01.jpg)\n\n**→ P_4_1_1_01** Using keys 1 to 3, selections can be made among different\nportions of the image sections. The motifs are still recognizable in these\nlarge, detail-rich excerpts but now have an unsettling perspective.\n\n## **P.4.1.2 Feedback of image cutouts**\n\nA familiar example of feedback: a video camera is directed at a television\nscreen that displays the image taken by the camera. After a short time, the\ntelevision screen depicts an ever-recurring and distorted image. When this\nphenomenon is simulated, an image’s level of complexity is increased. This\nrepeated overlaying leads to a fragmentary composition.\n\n**→ P_4_1_2_01**\n\nFirst, the image is loaded and shown in the display. A section of the image is\ncopied to a new randomly selected position with each iteration step. The\nresulting image now serves as the basis for the next step—the principle of\neach and every feedback.\n\n![image](../images/f0196-01.jpg)\n\n![image](../images/f0196-02.jpg)\n\n**→ Fotografie: Stefan Eigner** original image: subway tunnel\n\n![image](../images/f0196-03.jpg)\n\n**→ P_4_1_2_01** Right after the program starts, the motif is easily\nrecognizable. It then dissolves more and more through the overlapping of\ncopied image strips.\n\n**1**\n\nfunction setup() {\n\ncreateCanvas(1024, 780);\n\nimage(img, 0, 100);\n\n}\n\n**1** | When the program is started, the loaded image is offset one hundred pixels downward using the image() command and positioned on the drawing canvas.  \n---|---  \n  \n**2**\n\nfunction draw() {\n\nvar x1 = floor(random(width));\n\nvar y1 = 0;\n\nvar x2 = round(x1 + random(-7, 7));\n\nvar y2 = round(random(-5, 5));\n\nvar w = floor(random(10, 40));\n\nvar h = height;\n\n**3**\n\nset(x2, y2, get(x1, y1, w, h));\n\n}\n\n**2** | The x-position of the detail to be copied (x1), the target position (x2, y2), and its width (w) are all determined randomly.  \n---|---  \n**3** | Using the function get(), some of the canvas content is copied and then pasted into the new position (x2, y2) using set().  \nKeys: | DEL: Delete display S: Save image  \n---|---  \n  \n![image](../images/f0197-01.jpg)\n\n## **P.4.2.1 Collage from image collection**\n\nYour archive of photographs now becomes artistic material. This program\nassembles a collage from a folder of images. The cropping, cutting, and\nsorting of the source images are especially important, since only picture\nfragments are recombined in the collage.\n\n**→ P_4_2_1_01**\n\nAll the pictures in a folder are read dynamically and assigned to one of\nseveral layers. This allows the semantic groups to be treated differently. The\nindividual layers also have room for experimentation with rotation, position,\nand size when constructing the collage. Note the layer order; the first level\nis drawn first and is thus in the background.\n\n![image](../images/f0198-01.jpg)\n\n![image](../images/f0198-02.jpg)\n\n**→ P_4_2_1_01 Illustration: Andrea von Danwitz** The image consists of three\nlevels: scraps of paper are on layer 1, cutouts of the sky on layer 2, and\nplants and street elements on layer 3.\n\n![image](../images/f0198-03.jpg)\n\nA new composition is immediately created when the images on a level are\nswitched or the parameters are changed.\n\n**1**\n\nvar layer1Images = [];\n\nvar layer2Images = [];\n\nvar layer3Images = [];\n\nvar layer1Items = [];\n\nvar layer2Items = [];\n\nvar layer3Items = [];\n\n**1** | Several arrays are needed to load the images and arrange the layout of the collage pieces. In layer1Images, e.g., the loaded images are saved for the first layer in order to be used later to create the collage items.  \n---|---  \n  \nfunction setup() {\n\n...\n\n**2**\n\nlayer1Items = generateCollageItems(\n\nlayer1Images, 100, width / 2, height / 2,\n\nwidth, height, 0.1, 0.5, 0, 0);\n\nlayer2Items = generateCollageItems(\n\nlayer2Images, 150, width / 2, height / 2,\n\nwidth, height, 0.1, 0.3, -HALF_PI, HALF_PI);\n\nlayer3Items = generateCollageItems(\n\nlayer3Images, 110, width / 2, height / 2,\n\nwidth, height, 0.1, 0.4, 0, 0);\n\n**3**\n\ndrawCollageItems(layer1Items);\n\ndrawCollageItems(layer2Items);\n\ndrawCollageItems(layer3Items);\n\n}\n\n**2** | The function generateCollageItems() fills the array layers (layer1Items,...) with collage items. The parameters determine which loaded images are to be used and how many items are to be created, and they specify value ranges for positions, scattering, scaling, and rotation. In this example, images in the array layer1Items are used. All instances are placed in the position (width/2, height/2) with a scattering of width and height. The scaling varies from 0.1 to 0.5 and no rotation is used.  \n---|---  \n**3** | Each time the drawCollageItems() function is run, one of the layers is drawn. The order of the layers’ invocation determines the construction of the final composition. The images from layer1Items are located in the background, those from layer3Items in the foreground.  \n  \n**4**\n\nfunction CollageItem(image) {\n\nthis.image = image;\n\nthis.x = 0;\n\nthis.y = 0;\n\nthis.rotation = 0;\n\nthis.scaling = 1;\n\n}\n\n**4** | All features of a collage item are summarized in the class CollageItem.  \n---|---  \nKeys: | 1–3: New random arrangement for one of the three levels S: Save image  \n---|---  \n  \n![image](../images/f0200-01.jpg)\n\n**→ P_4_2_1_02 → Illustration: Andrea von Danwitz** A second version of the\nprogram allows the image details to be arranged radially around a particular\ncenter. The angle at which the images gather and their distance from the\ncenter can be specified for each layer.\n\n## **P.4.2.2 Time-based image collection**\n\nIn this example, the inner structures of moving images are visualized. After\nextracting individual images from a video file, this program arranges the\nimages in defined and regular time intervals in a grid. This grid depicts a\ncompacted version of the entire video file and represents the rhythm of its\ncuts and frames.\n\n**→ P_4_2_2_01**\n\nTo fill the grid, individual still images are extracted at regular intervals\nfrom the entire length of a video. Accordingly, a sixty-second video and a\ngrid with twenty tiles results in three-second intervals.\n\n![image](../images/f0202-01.jpg)\n\n**1**\n\nfunction draw() {\n\nif(movie.elt.readyState == 4) {\n\nvar posX = tileWidth * gridX;\n\nvar posY = tileHeight * gridY;\n\nimage(movie, posX, posY, tileWidth, tileHeight);\n\ncurrentImage++;\n\n**2**\n\nvar nextTime = map(currentImage, 0, imageCount,\n\n0, movie.duration());\n\n**3**\n\nmovie.time(nextTime);\n\n**4**\n\ngridX++;\n\nif (gridX >= tileCountX) {\n\ngridX = 0;\n\ngridY++;\n\n}\n\n**5**\n\nif (currentImage >= imageCount) noLoop();\n\n}\n\n}\n\n**1** | Every time the draw() function is run, an image is selected from the video and depicted in the grid. The first image at time 0 can be placed immediately.  \n---|---  \n**2** | The next time in the video (nextTime) is calculated. The variable currentImage (a number between 0 and imageCount) is converted to a second value between 0 and the entire playing time of the video.  \n**3** | Using the time() function, the program jumps to the newly calculated time.  \n**4** | To define the next tile, gridX is increased by 1. If the end of the line has been reached, the program jumps to the first image of the next line by setting gridX to 0 and increasing gridY incrementally.  \n**5** | The end of the program is reached when all the tiles are filled with images.  \nKeys: | S: Save image  \n---|---  \n  \n![image](../images/f0203-01.jpg)\n\n**→ P_4_2_2_01** Fifty-five images from a two-and-a-half-minute video clip,\nfilmed on the way to the main train station in Stuttgart, Germany.\n\n## **P.4.3.1 Graphics from pixel values**\n\nPixels, the smallest elements of an image, can serve as the starting point for\nthe composition of portraits. In this example, each pixel is reduced to its\ncolor value. These values modulate design parameters such as rotation, width,\nheight, and area. The pixel is completely replaced by a new graphic\nrepresentation, and the portrait becomes somewhat abstract.\n\n**→ P_4_3_1_01**\n\nThe pixels of an image are analyzed sequentially and transformed into other\ngraphic elements. The key to this is the conversion of the color values of\npixels (RGB) into the corresponding gray values, because—in contrast to the\npure RGB values—these can be practically applied to design aspects such as\nline width. It is advisable to reduce the resolution of the source image\nfirst.\n\n![image](../images/f0204-01.jpg)\n\n![image](../images/f0204-02.jpg)\n\n**→ P_4_3_1_01 → Photograph: Tom Ziora** The gray value of each pixel defines\nthe size of its diameter; the pixels’ original color values are kept.\n\n**1**\n\nfor (var gridX=0; gridX<img.width; gridX++) {\n\nfor (var gridY=0; gridY<img.height; gridY++) {\n\nvar tileWidth = width / img.width;\n\nvar tileHeight = height / img.height;\n\nvar posX = tileWidth * gridX;\n\nvar posY = tileHeight * gridY;\n\nimg.loadPixels();\n\n**2**\n\nvar c = color(img.get(gridX, gridY));\n\n**3**\n\nvar grayscale = round(red(c) * 0.222 +\n\ngreen(c) * 0.707 \\+ blue(c) * 0.071);\n\n**4**\n\nswitch (drawMode) {\n\ncase 1:\n\nvar w1 = map(grayscale, 0, 255, 15, 0.1);\n\nstroke(0);\n\nstrokeWeight(w1 * mouseXFactor);\n\nline(posX, posY, posX + 5, posY + 5);\n\nbreak;\n\ncase 2:\n\n...\n\n}\n\n}\n\n}\n\n**1** | The width and height of the original image determines the resolution of the grid.  \n---|---  \n**2** | The color of the pixels at the current grid position (and thus the image) is defined.  \n**3** | In calculating the gray value, the values for red, green, and blue are weighted differently, whereby there are no absolutely correct weights since colors are both displayed and perceived differently. This gray value is used later to control individual parameters.  \n**4** | The program provides several drawing modes, drawMode, which can also be influenced by the horizontal mouse position. These were previously converted into a value between 0.05 and 1 and are available in the variable mouseXFactor.  \nMouse: | Position x/y: Different parameters  \n(dependent on drawing mode)  \n---|---  \nKeys: | S: Save image  \n  \n![image](../images/f0205-01.jpg)\n\n![image](../images/f0206-01.jpg)\n\n**→ P_4_3_1_01** The gray value defines the size, stroke value, rotation, and\nposition of the elements.\n\n![image](../images/f0206-02.jpg)\n\n**→ P_4_3_1_01** In this drawing mode (key 9), each pixel is represented by\nseveral color-distorting elements.\n\n![image](../images/f0207-01.jpg)\n\n![image](../images/f0208-01.jpg)\n\n**→ P_4_3_1_02** Pixels of various brightness are replaced here by SVG units.\nThe SVG files have been sorted according to brightness using a supplementary\nprogram. Note that the files have been renamed (the brightness value forms the\nbeginning of the file name).\n\n## **P.4.3.2 Type from pixel values**\n\nThe following text image is ambiguous. It can be read for its meaning, or\nviewed at a distance and perceived as a picture. The pixels from the image\ncontrol the configuration of the letters. The size of each letter depends on\nthe gray values of the pixels in the original image and thereby creates an\nadditional message.\n\n**→ P_4_3_2_01**\n\nA character string is processed letter by letter → P.3.1.1/P.3.1.2 and\nconstructed row by row in the normal writing direction. Before a character is\ndrawn, its position in display coordinates is matched to the corresponding\nposition in the original image in pixel coordinates. Only a subset of the\noriginal pixels is used—merely those for which a corresponding character\nposition exists. The color of the selected pixel can now be converted into its\ngray value and the gray value used to modulate the font size, for example.\n\n![image](../images/f0210-01.jpg)\n\n![image](../images/f0210-02.jpg)\n\n**→ P_4_3_2_01** The color of a pixel can define the size or color of the\nletters, or both.\n\nfunction draw() {\n\n...\n\nvar x = 0;\n\nvar y = 10;\n\nvar counter = 0;\n\n**1**\n\nwhile (y < height) {\n\nimg.loadPixels();\n\n**2**\n\nvar imgX = round(map(x, 0, width, 0, img.width))\n\nvar imgY = round(map(y, 0, height, 0, img.height))\n\nvar c = color(img.get(imgX, imgY));\n\nvar grayscale = round(red(c) * 0.222 +\n\ngreen(c) * 0.707 +\n\nblue(c) * 0.071);\n\npush();\n\ntranslate(x, y);\n\n**3**\n\nif (fontSizeStatic) {\n\ntextSize(fontSizeMax);\n\nif (blackAndWhite) fill(grayscale);\n\nelse fill(c);\n\n} else {\n\nvar fontSize = map(grayscale, 0, 255,\n\nfontSizeMax, fontSizeMin);\n\n**4**\n\nfontSize = max(fontSize, 1);\n\ntextSize(fontSize);\n\nif (blackAndWhite) fill(0);\n\nelse fill(c);\n\n}\n\nvar letter = inputText.charAt(counter);\n\ntext(letter, 0, 0);\n\nvar letterWidth = textWidth(letter) + kerning;\n\n**5**\n\nx += letterWidth;\n\npop();\n\n**6**\n\nif (x + letterWidth >= width) {\n\nx = 0;\n\ny += spacing;\n\n}\n\ncounter++;\n\nif (counter >= inputText.length) {\n\ncounter = 0;\n\n}\n\n}\n\nnoLoop();\n\n}\n\n**1** | The writing process continues as long as the y-coordinate of the current writing position y is still less than the height of the display.  \n---|---  \n**2** | Using the map() function, the display coordinates are converted into image coordinates; e.g., the x-coordinate x is proportionally converted from a value between 0 and the display width to a corresponding value between 0 and the width of the image img.width.  \n**3** | Depending on the selected mode fontSizeStatic (key 1 or 2), the font size is set to a fixed value fontSizeMax or is varied by the gray value.  \n**4** | The value fontSize cannot be zero or negative, as this would cause problems. Therefore, the function max() ensures this value is at least 1.  \n**5** | The value of the variable x is increased by the character width.  \n**6** | If x is greater than or equal to the width of the drawing canvas, the line is wrapped. The y value then increases by the line spacing and x restarts from 0 on the far left of the drawing canvas.  \nKeys: | 1: Switch character size mode 2: Switch character color mode Arrow ↓/↑: Maximum character size –/+ Arrow ←/→: Minimum character size –/+ S: Save image  \n---|---  \n  \n![image](../images/f0212-01.jpg)\n\n**→ P_4_3_2_01** The size and color of the characters are defined by an\nunderlying image.\n\n![image](../images/f0213-01.jpg)\n\n**→ P_4_3_2_01** Here the gray value of the pixels determines font size.\n\n## **P.4.3.3 Real-time pixel values**\n\nThe color values of pixels can again be translated into graphic elements but\nwith two important differences: first, the pixels are constantly changing\nbecause the images come from a video camera, and second, pixels are translated\nsequentially by dumb agents that are constantly in motion rather than all at\nonce. The motion captured by the camera and the migration of the agents thus\ncan paint a picture right before our eyes.\n\n**→ P_4_3_3_01**\n\nA dumb agent moves over the drawing canvas. The color value of the current\nreal-time video image is analyzed at each position and serves as a parameter\nfor each color and stroke value. The mouse position defines the stroke length\nand the speed of the agent.\n\n![image](../images/f0214-01.jpg)\n\n**→P.2.2.1 Dumb agents**\n\n![image](../images/f0214-02.jpg)\n\n**→ P_4_3_3_0 1** The agent’s path gradually creates an image.\n\nfunction setup() {\n\n...\n\n**1**\n\nvideo = createCapture(VIDEO, function(){\n\nstreamReady = true\n\n});\n\n**2**\n\nvideo.size(width, height);\n\n**3**\n\nvideo.hide();\n\n...\n\n}\n\n**1** | The function createCapture() creates a video element to access images on the webcam.  \n---|---  \n**2** | The live video images from the connected video camera are reduced to the size of the drawing canvas.  \n**3** | The hide() command prevents the video image from being automatically displayed on the drawing canvas.  \n  \nfunction draw() {\n\n**4**\n\nif(streamReady) {\n\nfor (var j = 0; j <= mouseX / 50; j++) {\n\nvideo.loadPixels();\n\n**5**\n\nvar pixelIndex = (((video.width \\- 1 - x)\n\n\\+ y * video.width) * 4);\n\nvar c = color(video.pixels[pixelIndex],\n\nvideo.pixels[pixelIndex + 1],\n\nvideo.pixels[pixelIndex + 2]);\n\n**6**\n\nvar cHSV = chroma(red(c), green(c), blue(c));\n\nstrokeWeight(cHSV.get('hsv.h') / 50);\n\nstroke(c);\n\ndiffusion = map(mouseY, 0, height, 5, 100);\n\nbeginShape();\n\n**7**\n\ncurveVertex(x, y);\n\ncurveVertex(x, y);\n\n**8**\n\nfor (var i = 0; i < pointCount; i++) {\n\nvar rx = int( random(-diffusion, diffusion));\n\ncurvePointX = constrain(x + rx, 0, width \\- 1);\n\nvar ry = int( random(-diffusion, diffusion));\n\ncurvePointY = constrain(y + ry, 0, height \\- 1);\n\ncurveVertex(curvePointX, curvePointY);\n\n}\n\ncurveVertex(curvePointX, curvePointY);\n\nendShape();\n\n**9**\n\nx = curvePointX;\n\ny = curvePointY;\n\n}\n\n}\n\n}\n\n**4** | If the video signal is available, the pixels of the current video image are loaded.  \n---|---  \n**5** | As in a static pixel image, the pixels in a video image are also numbered row by row. Therefore, the pixel index has to be calculated from the current writing position (x, y). When using webcams directed at the user, it is useful to mirror the video image horizontally using the calculation video.width-1-x.  \n**6** | The stroke value is set so it is defined by the hue of the pixel. The chroma.js library helps to convert RGB to HSV values.  \n**7** | The line element can now be drawn. The first curve point is placed on the current drawing position. This is done twice because the first and last points are not drawn when drawing lines with curveVertex().  \n**8** | The variable pointCount now specifies how many curve points are to be drawn. The default value is 1, so only one line is drawn. The curve points are placed in random positions around the drawing position. The value diffusion specifies how large this area is.  \n**9** | The last curve point is specified as the new drawing position.  \nMouse: | Position x: Drawing speed Position y: Direction  \n---|---  \nKeys: | Arrow ↓/↑: Number of curve points –/+ Q: Stop drawing W: Continue drawing Arrow ←/→: Minimum font size –/+ S: Save image  \n  \n![image](../images/f0216-01.jpg)\n\n![image](../images/f0217-01.jpg)\n\n**→ P_4_3_3_01** The people leave tracks in the image with their movements.\nThe length of the lines varies throughout the drawing process, whereby the\nimage is sometimes more detailed and sometimes more abstract.\n\n![image](../images/f0218-01.jpg)\n\n**→ P_4_3_3_02** Three agents move around the display in this version of the\nprogram. The first agent’s stroke value is defined by the pixel’s hue; the\nsecond’s by the pixel’s saturation; and the third’s by the pixel’s brightness.\n\n![image](../images/f0219-01.jpg)\n\n**→ P_4_3_3_02** When a subject moves in front of the camera, a collection of\nseemingly random scribbles come together to represent the subject’s form.\n\n## **P.4.3.4 Emojis from pixel values**\n\nAn emotional transformation from a raster element to a symbol: here the things\nthat visualize feelings in an SMS become a small part of a greater whole. Any\ncollection of thumbnails in this program can be the source material, and the\npixel values determine which can join.\n\n**→ P_4_3_4_01**\n\nColor values can be interpreted as points in 3D space. For this program, it is\nnecessary to find the shortest distance from a color value to a set of other\ncolor values, here the average colors of the individual emojis. There are\nvarious mathematical methods for this search. Particularly fast and\ncomparatively easy to use is the search in a so-called k-dimensional tree. The\nfunctionality for this is provided by the library kdTree.js.\n\n![image](../images/f0220-01.jpg)\n\n**1**\n\n<script src=\"../../libraries/kd-tree/kdTree.js\"\n\ntype=\"text/javascript\"></script>\n\n**2**\n\n<script src=\"data/emoji-average-colors.js\"\n\ntype=\"text/javascript\"></script>\n\n**1** | Two additional scripts are required for this program. These are loaded in index.html.  \n---|---  \n**2** | Information about the average colors of emoji files and their respective file names is contained in emoji-average-colors.js. The calculation of the average colors takes place in an additional program: **→ P_4_3_4_emoji_color_analyser.**  \n  \nvar emojis = {\n\n**3**\n\n\"1f4a3\": {\"averageColor\": {\"r\":57, \"g\":57, \"b\":52}},\n\n**4**\n\n\"1f43b\": {\"averageColor\": {\"r\":187, \"g\":111,\"b\":88}},\n\n**5**\n\n\"1f600\": {\"averageColor\": {\"r\":227, \"g\":181, \"b\":70}},\n\n...\n\n}\n\n**3** | file 1f4a3.png: ![image](../images/i220-1.jpg)  \n---|---  \n**4** | file 1f43b.png: ![image](../images/i220-2.jpg)  \n**5** | file 1f600.png: ![image](../images/i220-3.jpg)  \n  \nfunction preload() {\n\nimg = loadImage(\"data/pic.png\");\n\n**6**\n\nicons = {};\n\nfor (var name in emojis) {\n\nicons[name] = loadImage(emojisPath + \"36x36/\" +\n\nname \\+ \".png\");\n\n}\n\n}\n\n**6** | In the variable icons, all images of the emojis are loaded and can be called later using their names (name).  \n---|---  \n  \nfunction setup(){\n\n...\n\n**7**\n\nvar colors = [];\n\nfor (var name in emojis) {\n\nvar col = emojis[name].averageColor;\n\ncol.name = name;\n\ncolors.push(col);\n\n}\n\n**8**\n\nvar distance = function(a, b){\n\nreturn pow(a.r - b.r, 2) +\n\npow(a.g - b.g, 2) +\n\npow(a.b - b.b, 2);\n\n}\n\n**9**\n\ntree = new kdTree(colors, distance, [\"r\", \"g\", \"b\"]);\n\n}\n\n**7** | The search for the closest color begins here. Two things must be created: first, an array of points and their corresponding r, g, and b colors. Each entry in the colors array is also assigned the name name, in order to display the appropriate image files later.  \n---|---  \n**8** | Second, a function is defined that specifies how the distance between two points should be calculated. Typically, this is the length of the diagonal from color a to color b in the RGB color space.  \n**9** | With the help of the kdTree library, a so-called k-dimensional tree is created. For this, the newly created point list colors and the distance function distance are passed as parameters. In addition, a list of dimensions must be passed.  \n  \nfunction draw() {\n\nbackground(255);\n\nfor (var gridX = 0; gridX < img.width; gridX++) {\n\nfor(var gridY = 0; gridY < img.height; gridY++) {\n\nvar posX = tileWidth * gridX;\n\nvar posY = tileHeight * gridY;\n\n**10**\n\nvar c = color(img.get(gridX, gridY));\n\n**11**\n\nvar nearest = tree.nearest(\n\n{r:red(c), g:green(c), b:blue(c)},\n\n1);\n\n**12**\n\nvar name = nearest[0][0].name;\n\nimage(icons[name], posX, posY,\n\ntileWidth, tileHeight);\n\n}\n\n}\n\nnoLoop();\n\n}\n\n**10** | The image, img, to be displayed is scanned and the pixel color is stored in c.  \n---|---  \n**11** | The nearest() function in the kdTree library looks for the closest points to the one just passed. The last parameter indicates how many results should be returned; only one is needed here.  \n**12** | The search result is an array with the closest points. Each entry in it is itself an array with two elements: the point itself and its distance to the point passed in nearest(). In both arrays the first entry is required: [0] [0]. The image of the emoji can now be placed on the drawing canvas via the name.  \nKeys: | S: Save image  \n---|---  \n  \n![image](../images/f0222-01.jpg)\n\n**→ P_4_3_4_01** Every pixel becomes an emoji. Any image can be used, as long\nas it is large enough to include many color values.\n\n![image](../images/f0222-02.jpg)\n\noriginal photo\n\n![image](../images/f0223-01.jpg)\n\n![image](../images/f0223-02.jpg)\n\noriginal photo\n\n![image](../images/f0224-01.jpg)\n\n![image](../images/f0225-01.jpg)\n\n**→ P_4_3_4_02** An image from a webcam could also be the basis of a\nrasterization in emojis.\n\n\n![image](../images/f0226-01.jpg)\n\n# Appendix\n\n**Appendix**\n\n[A.1 Looking ahead](14-appendix01.xhtml#a.1)\n\n[A.2 Reflection](15-appendix02.xhtml#a.2)\n\n[A.3 Bibliography](16-appendix03.xhtml#a.3)\n\n[A.4 The authors](17-appendix04.xhtml#a.4)\n\n[A.5 We thank](18-appendix05.xhtml#a.5)\n\n[A.6 Copyright](19-appendix06.xhtml#a.6)\n\n[A.7 Farewell](20-appendix07.xhtml#a.7)\n\n\n## **A.1 Looking ahead**\n\nWould you like to dive deeper into generative design? We hope so, because\nthere is still much to discover! This chapter gives an overview on complex\nmethods that are the basis for data visualization, 3D shape generation, wild\nparticle animation, et cetera. You can find the code in the additional\nsketches on our website and our GitHub channel (links available on\n[generative-gestaltung.de](http://generative-gestaltung.de)). After all the\npractical making of the previous chapters, this chapter is an invitation to\nreflect on what you’ve learned and to explore theoretical and conceptual\nconnections in the context of generative design.\n\n![image](../images/f0229-01.jpg)\n\n![image](../images/f0230-01.jpg)\n\n**→ M_1_5_02** A typical image that arises when noise-generated values control\nthe directional movements of a swarm of agents. The agents are represented as\ndots and leave a trail as they are drawn continuously over the old image.\n\n![image](../images/f0231-01.jpg)\n\n![image](../images/f0232-01.jpg)\n\n![image](../images/f0234-01.jpg)\n\n![image](../images/f0235-01.jpg)\n\n**→ M_2_5_02** The superimposition of different sinusoids results in Lissajous\nfigures, named after the person who researched them, Jules Antoine Lissajous.\nIf you connect the generated points not in sequence but rather each point with\nall the others, exciting netlike structures arise.\n\n![image](../images/f0236-01.jpg)\n\n![image](../images/f0237-01.jpg)\n\n![image](../images/f0238-01.jpg)\n\n![image](../images/f0239-01.jpg)\n\n**→ M_4_3_01** The points of regularly extending lines were gradually deformed\nby attractors, programmed virtual magnets that attract and repel.\n\n![image](../images/f0240-01.jpg)\n\n![image](../images/f0241-01.jpg)\n\n**→ M_4_3_01** A data visualization of folders and files presented in the form\nof a sunburst diagram. The darker the circle segments, the longer the files in\nthe folders have remained unchanged. Any folder and file structures can be\nloaded and visualized from a hard disk.\n\n![image](../images/f0242-01.jpg)\n\n**→ M_5_4_01** Data visualization of linking structures of Wikipedia articles.\nThe size of each circle represents the length of an article, while the color\nsignifies its theme. By experimenting with the sample programs, it is possible\nto gain a sense of what generative design is all about. On the following\npages, we reflect on the ideas behind the sample programs and put what has\nbeen learned into context. This part also has a referential character—it\nidentifies related topics and connections.\n\n![image](../images/f0243-01.jpg)\n\n\n## **A.2 Reflection**\n\nThis book shows how imagery can be generated through code. By experimenting\nwith the sample programs, you can gain a sense of what generative design is\nall about. On the following pages, we reflect on the ideas behind the sample\nprograms and put what we have learned into context. This section also\nidentifies related topics and connections.\n\n**Status Quo** Digitization has become an integral part of the discipline of\ndesign, and all of us use computers for our daily work. The best-known tools\nin the design sector, however, such as the Adobe Creative Cloud products\n(Photoshop, Illustrator, et cetera) have only an illustrative and imaging\nfunction. Existing tools, such as brushes, scissors, or photo labs, can be\nmade virtual and more efficient, allowing us to get results faster and work\nmore comfortably. Yet the design process has not evolved with these\ntools.**1** Whether an image has been produced with a mouse or a paintbrush,\nthe concept of creating a line, for instance, remains the same. Generative\ndesign is different from conventional methods; the design process is unique\nand by its nature results in new possibilities.\n\n**The New Design Process?** The main change in the design process achieved by\nusing generative design is that traditional craftsmanship recedes into the\nbackground, and abstraction and information become the new principal elements.\nThe relevant question is no longer “How do I draw?” but rather “How do I\nabstract?” This is because the process that leads from the idea to the final\nimage can only take place using an algorithm—a sequence of rules—that the\ncomputer interprets and processes. Before appearing in the display, each\ngenerated image must first be described completely using a set of rules. This\nposes two challenges for the designer: how to abstract a vague idea, and how\nto enter an idea into the computer in a formalized way. No set rules exist for\nhow to abstract an idea; for a complex idea to be implemented, the problem\nmust be broken down into smaller chunks. This problem-solving strategy is also\nknown as “divide and conquer.” **2** For instance, a surface is to be filled\nwith as many circles with random diameters as possible without any overlaps.\nThe first step is to convert the vague idea into a concrete and simple\n“recipe.” Draw a new circle. If this circle does not intersect with any other\non the display, make this circle as large as possible. If the circle\nintersects another, start over. **[→ P.2.2.5](10-part02ch02.xhtml#p.2.2.5)**\nOnly through this decomposition is it possible to formulate the individual\nsteps in a programming language, thereby making it executable for the\ncomputer. A programming language offers the elementary building blocks with\nwhich to do this, such as _repetition, randomness_ , and _logic_ , which will\nbe explained more precisely in the following pages.\n\n**Repetition** allows us to let the computer work on a task until it is solved\nand gives us the ability to manipulate a huge number of objects. The\nimportance of repetition is reflected by how often the for-loop is used in the\nprograms in this book or by the fact that the draw-loop is the central\nfunction of p5.js.\n\n**Randomness** is used to create variations to break up the strict regularity\nof the computer. True randomness rarely produces compositionally interesting\nresults. These are created instead when randomness is limited and applied in\nmeasured doses. In p5.js randomness is represented by the keywords “random”\nand “noise.” **[→ P.3.2.5](11-part02ch03.xhtml#p.3.2.5)**\n\n![image](../images/f0246-01.jpg)\n\n**→ P_3_2_5_01 bis → P_3_2_5_0**  \nText in motion\n\nWe use **logic** as a kind of control structure to steer the generative\nprocess. Here we can set up conditions for directing the program flow into\ndifferent branches. For example, the chapter “Text as a Blueprint” **[→\nP.3.1.2](11-part02ch03.xhtml#p.3.1.2)** contains another switch structure we\ncan use to execute different parts of the program depending on the entered\ntext: each letter is written out normally, but every time a punctuation mark\nappears, the writing direction changes and the punctuation mark is replaced by\na curved element. The most common keywords for control structures are “if,”\n“else,” “switch,” and “case.”\n\n![image](../images/f0246-02.jpg)\n\n**→ P.3.1.2**  \nText as a blueprint\n\nOnce a design idea has been translated into code that can be interpreted by\nthe computer, it is possible to generate many image possibilities, without the\nhand ever drawing a line. However, the first results are almost never 100\npercent satisfactory. These results must be evaluated; this evaluation serves\nas the basis for improving the next iteration. In contrast to the conventional\napproach, we do not directly put our hands on the image but instead change the\nunderlying abstraction or individual parameters in the program and continue to\nrefine each iteration until the desired end result is achieved. Interaction\nhelps accelerate this reciprocal effect. A generative system in and of itself\nis not necessarily interactive, and this book does not deal explicitly with\ninteraction. The extra effort involved in installing control elements\n(buttons, sliders, et cetera) is worth it, however. These allow us to track\nand control every parameter change in real time. Since all the programs in\nthis book run in the web browser, it is very easy to enhance them with\nstandard HTML elements such as buttons, sliders, et cetera. **[→\nP.2.1.4](10-part02ch02.xhtml#p.2.1.4)**\n\n![image](../images/f0247-01.jpg)\n\n**→ P.2.1.4**  \nCheckboxes in a grid\n\n### Design process: analog and digital\n\n![image](../images/f0245-01.jpg)\n\nThe idea for an output is manually implemented, step by step, in a “visual\nflight rule” fashion. Designers can evaluate and intervene directly in each\nstep as they draw on paper, create keyframes for animation, et cetera. If the\nidea is implemented digitally, tools like brushes, scissors, or photo labs\nbecome virtual and therefore more efficient. Although this makes everything\nfaster and easier, the design process remains the same.\n\n### Design process: generative\n\n![image](../images/f0245-02.jpg)\n\n![image](../images/f0245-03.jpg)\n\n**P.2.2.5**  \nStructural density from agents.\n\n**New Possibilities in Design** The integration of programming into the design\nprocess greatly increases the possibilities for designers. Conceptual\ncompetence still lies with the designer; the computer assumes only the role of\nthe tireless helper. It is a given in today’s automated and digital world that\ndesigns can be executed quickly and easily and that it is possible to generate\na composition with thousands of elements. But it is more interesting to look\nat the new possibilities of generative design in terms of _emergence,\nsimulation_ , and _tools_.\n\nIn the context of generative design, processes are emergent when their results\nare not predetermined and when the interaction of all the elements leads to\nmore than is obvious from their individual properties. A common example of\n_emergence_ is the flocking behavior of birds: simple rules result in highly\ncomplex, unpredictable behavior. In the section “Growth structure from\nagents,” **[→ P.2.2.4](10-part02ch02.xhtml#p.2.2.4)** a very simple algorithm\nthat consists of only two steps results in completely unexpected organic\nstructures. (Boids and automata are other examples.)\n\n![image](../images/f0247-02.jpg)\n\n**→P.2.2.4**  \nGrowth structure from agents\n\nThe **simulation** of natural processes is another important method in\ngenerative design. For example, in the chapter “Agents on a pendulum,” **[→\nP.2.2.6](10-part02ch02.xhtml#p.2.2.6)** a structure of several connected\npendulums is created. The movement of a single pendulum is simple: it circles\nits point of articulation. However, if several of them are linked to form a\nchain, then the outermost ones move in a complex manner. And, as is so often\nthe case, the transfer of knowledge between fields leads to amazing results;\ncertainly there are many more models in nature that could potentially be\ntransferred to a generative system.\n\n![image](../images/f0247-03.jpg)\n\n**→ P.2.2.6**  \nAgents on the pendulum\n\nPerhaps the most important aspect of this increase in possibilities is that\nthe designer is now the creator of individualized tools, since each generative\nprogram is also a customized software tool. This allows the designer to\nexplore new paths that would not have been available using existing software,\nleading to a wider range of visual design mediums. It is astonishing how much\ndesigners can accomplish with tools they develop themselves, as is evident in\n“Drawing.” **[→ P.2.3](10-part02ch02.xhtml#p.2.3)** Even these simple tools\ngreatly increase a designer’s options, and, because such tools can be refined\nconstantly, they become perfectly tailored work instruments. In addition, it\nis usually not a huge leap from a generative program to developing one’s own\napp.\n\n![image](../images/f0248-01.jpg)\n\n**→ P.2.3**  \nDrawing\n\n**Looking ahead** The world of generative design has significantly evolved\nsince the first edition of this book. Generative design—also referred to as\ncreative coding—has established itself in the design world. Working with\ngenerative design has become common practice, as the basis of data\nvisualizations, artwork, media installations, architectural models, video\nclips, fonts, dynamic appearances—all the way to customized mass production. A\nlook into the future clearly shows that the use of generative design will\ncontinue to increase. This is indicated by the following favorable factors:\n\nThe amount of information with which we are confronted daily is rapidly\nincreasing thanks to ubiquitously accessible computer networks. Helping\nsociety deal with this flood of information through data visualization is an\nimportant task for designers.**3** Generative design will be indispensable in\ncoping with this challenge.\n\nThe expansion of technological possibilities is a constant source of\nstimulation for generative design. Just a few years ago, for example, it was\nnearly impossible to generate complex, 3D worlds or to make them tangible with\nvirtual reality and augmented reality—something that is now possible even on a\nsmartphone. This technical potential will continue to be a driving force in\ngenerative design.\n\nEqually important is the community of collaborators, which in almost no other\ndesign field is so vital. It is astounding how much the internet-based\ncommunities that have arisen around Processing, p5.js, vvvv, openFrameworks,\nNodeBox, or Basil.js have contributed to libraries, tutorials, sample\nprograms, articles, forum contributions, wikis, et cetera. This is certainly\nin part because creating code—generative design’s underlying design medium—is\nwell suited to teamwork, as software development has demonstrated. One\nadvantage of code is that it can easily be exchanged and spread, in contrast\nto other media such as video.\n\nNevertheless, a designer who has programming skills is still the exception\nrather than the rule today. This has historical and cultural reasons:\ngenerally designers have been forced to decide to be either artists or\ntechnicians. To be both in perfect union is rarely an option, for example, in\nuniversity curricula, although this distinction has begun to blur in recent\nyears. In addition, learning this new design process—using mathematical-\nanalytical source code to transform an idea into a visual result—feels to many\nlike an insurmountable obstacle. Overcoming this obstacle is one of this\nbook’s main objectives.\n\nAn understanding of generative design will lead to a new matter-of-factness in\ntaking advantange of the computer’s potential. Programming, and thus\ngenerative design, is rapidly becoming a universal cultural asset and\ntechnical tool, just as photography and film were in the last century. The\npossibilities are already there; we just need to use them.\n\n**Georg Trogemann, Jochen Viehoff**  \n _CodeArt_ , foreword\n\n**1** “Painting with a mouse on the computer screen has a high entertainment\nvalue, but [. . .] drawing a stroke with a pen is no different from drawing a\nstroke with a mouse. The real challenge is to discover the intrinsic\nproperties of the new medium and to find out how the stroke you draw via\ncomputation is one you could never draw, or even imagine, without\ncomputation.”\n\n**John Maeda**  \n _Design by Numbers_\n\n**2** “In addition to ‘divide and conquer,’ there are many other problem-\nsolving strategies, such as top-down and bottom-up, et cetera. In this\ncontext, the ‘pattern’ idea (analyzing problems in recurring patterns and\nsolving these systematically) can be very helpful.”\n\n**Christopher Alexander**  \n _A Pattern Language_\n\n**3** “The ability to take data—to be able to understand it, to process it, to\nextract value from it, to visualize it, to communicate it—that’s going to be a\nhugely important skill in the next decades, not only at the professional level\nbut even at the educational level for elementary school kids, for high school\nkids, for college kids. Because now we really do have essentially free and\nubiquitous data. So the complimentary scarce factor is the ability to\nunderstand that data and extract value from it.”\n\n**Hal Varian**  \nProfessor at the University of California, Berkeley\n\n\n## **A.3 Bibliography**\n\n### General\n\n**Armstrong, Helen.** _Digital Design Theory: Readings from the Field_. New\nYork: Princeton Architectural Press, 2016.  \n_Collection of new and old essays on generative design and related fields_.\n\n**Flake, Gary William.** _The Computational Beauty of Nature: Computer\nExplorations of Fractals, Chaos, Complex Systems, and Adaptation_. Cambridge,\nMA: MIT Press, 1998.  \n_Generative design looked at from a (very) mathematical viewpoint_.\n\n**Reas, Casey, Chandler McWilliams, and Jeroen Barendse.** _Form+Code in\nDesign, Art, and Architecture_. New York: Princeton Architectural Press, 2010.\n\n### p5.js and Processing\n\n**McCarthy, Lauren, Ben Fry, and Casey Reas.** _Make: Getting Started with\np5.js: Making Interactive Graphics in JavaScript and Processing_. San\nFrancisco: Maker Media, 2016.\n\n**Reas, Casey, and Ben Fry.** _Processing: A Programming Handbook for Visual\nDesigners and Artists_ , second edition. Cambridge, MA: MIT Press, 2014.\n\n**Shiffman, Daniel.** [TheCodingTrain.com](http://TheCodingTrain.com).  \n_Great video tutorials on p5.js, processing, and generative design_.\n\n**Shiffman, Daniel.** _The Nature of Code: Simulating Natural Systems with\nProcessing_. Self-published, 2012.\n\n### Data graphics\n\n**Cairo, Alberto.** _The Truthful Art: Data, Charts, and Maps for\nCommunication_. San Francisco: New Riders, 2016.\n\n**Fry, Ben.** _Visualizing Data: Exploring and Explaining Data with the\nProcessing Environment_. Sebastopol, CA: O’Reilly Media, 2008.\n\n**Klanten, Robert, Sven Ehmann, Thibaud Tissot, and Nicolas Bourquin.** _Data\nFlow 2: Visualizing Information in Graphic Design_. Berlin: Gestalten, 2010.  \n_Illustrated book covering advanced data graphic programming with Processing\nand presenting a wide range of data graphics projects_\n\n**Lupi, Giorgia, and Stefanie Posavec.** _Dear Data_. New York: Princeton\nArchitectural Press, 2016.\n\n**Munzner, Tamara.** _Visualization Analysis and Design_. Boca Raton, FL: CRC\nPress, 2014.\n\n**Tufte, Edward R.** _Envisioning Information_. Cheshire, CT: Graphics Press,\n1990.\n\n**Tufte, Edward R.** _The Visual Display of Quantitative Information_.\nCheshire, CT: Graphics Press, 2001.\n\n### Aesthetics\n\n**Gerstner, Karl.** _Designing Programmes_. Zurich: Lars Müller, 2007.\n_Originally published in 1964; its title is somewhat misleading in our\ncomputer age—if it were published for the first time today, it would be called\nsomething like_ Inventing Design Rules.\n\n**Kandinsky, Wassily.** _Point and Line to Plane_. Edited by Hilla Rebay.\nTranslated by Howard Dearstyne and Hilla Rebay. New York: Dover Publications,\n1979.  \n_Originally published in 1925; theoretical musings on abstract painting by the\nBauhaus master_.\n\n**Leborg, Christian.** _Visual Grammar_. Translated by Diane Oatley. New York:\nPrinceton Architectural Press, 2006.  \n_Visual syntax abstract in overview; very clear and well illustrated_ Studio\nMoniker. _Conditional Design Workbook_. Amsterdam: Valiz, 2013.\n\n### History and context\n\n**Brown, Paul, Charlie Gere, Nicholas Lambert, and Catherine Mason, eds.**\n_White Heat Cold Logic: British Computer Art, 1960–1980_. Cambridge, MA: MIT\nPress, 2008.\n\n**Burnham, Jack.** _Software: Information Technology; Its New Meaning for\nArt_. New York: Jewish Museum, 1970.\n\n**Leavitt, Ruth, ed.** _Artist and Computer_. New York: Harmony Books, 1976.\n\n**Lee, Pamela M.** _Chronophobia: On Time in the Art of the 1960s_. Cambridge,\nMA: MIT Press, 2004.\n\n**Maeda, John.** _Design by Numbers_. Cambridge, MA: MIT Press, 2001.  \n_First popular book that showed the design world how it was done, using the\nprogramming language DBN_.\n\n**Maeda, John.** _Maeda@Media_. New York: Rizzoli, 2000. _Autobiography and\nretrospective of John Maeda’s major contribution to generative design_.\n\n**McCollough, Malcolm.** _Abstracting Craft: The Practiced Digital Hand_.\nCambridge, MA: MIT Press, 1998.\n\n**Paul, Christiane.** _Digital Art_. New York: Thames & Hudson, 2003.\n\n**Reichardt, Jasia.** _Cybernetic Serendipity: The Computer and the Arts_. New\nYork: Frederick A. Praeger, 1968.\n\n_Catalog of the first major exhibition on computer art in London_.\n\n**Rosen, Margit, ed.** _A Little-Known Story about a Movement, a Magazine, and\nthe Computer’s Arrival in Art: New Tendencies and Bit International,\n1961–1973_. Cambridge, MA: MIT Press, 2011.\n\n**Shanken, Edward A.** _Art and Electronic Media_. New York: Phaidon Press,\n2009.\n\n**Wardrip-Fruin, Noah, and Nick Montfort, eds.** _The New Media Reader_.\nCambridge, MA: MIT Press, 2003.\n\n**Whitelaw, Mitchell.** _Metacreation: Art and Artificial Life_. Cambridge,\nMA: MIT Press, 2004.\n\n**Wilson, Mark.** _Drawing with Computers: The Artist’s Guide to Computer\nGraphics_. New York: Perigee Books, 1985.\n\n**Wood, Debora.** _Imaging by Numbers: A Historical View of the Computer\nPrint_. Chicago: Northwestern University Press, 2008.\n\n\n## **A.4 The authors**\n\n**Benedikt Groß** Born in 1980 in Baden-Württemberg. 2002: studied geography\nand computer science. Discontinued those studies and became interested in\ndesign. 2007: completed master’s degree in generative systems with Julia Laub\nat the HfG Schwäbisch Gmünd. Thereafter, employed in higher education and in\nthe field of design. 2011–13: master’s student in design interactions at the\nRoyal College of Art London under Anthony Dunne and Fiona Raby; at the same\ntime, data visualization specialist at the MIT Senseable City Lab in\nCambridge, Massachusetts / Singapore. Since 2013: freelance work in\nspeculative and computational design; various international exhibitions,\nawards, and publications. Since 2017: professor of interaction design at the\nHfG Schwäbisch Gmünd.\n\n**Hartmut Bohnacker** Born in 1972 in Baden-Württemberg. Beginning in 1992:\nstudied mathematics (discontinued) and earned a degree in economics in 1997.\n1998: studied communication design at the HfG Schwäbisch Gmünd. Since\ncompleting studies in 2002: independent designer in Stuttgart, specializing in\nconception, design, and prototypical implementation of projects in the field\nof interface and interaction development; teacher of digital media. Since\n2009: professor of interaction design at the HfG Schwäbisch Gmünd.\n\n**Julia Laub** Born in 1980 in Bavaria. 2003: studied communication design at\nthe HfG Schwäbisch Gmünd. Studied abroad at the HGK Basel. 2007: completed\nmaster’s thesis, “Generative Systems,” with Benedikt Groß. Since 2008:\nindependent graphic designer specializing in book design, corporate design,\nand generative design. 2010: cofounder of the design studio onformative\n(studio for digital art and design) in Berlin with Cedric Kiefer. Teacher and\nleader of workshops at various universities.\n\n**Claudius Lazzeroni** Born in 1965 in Bavaria. 1984: trained to become a\nphotographer with Raoul Manuel Schnell. 1987: tutor at Massachusetts College\nof Art in Boston. 1992: earned degree in media design at the BILDO Academy in\nBerlin. Until 1996: creative director at Pixelpark. Until 2001: founder,\ndirector, and creative director of the design agency IM STALL in Berlin. Since\n1999: professor of interface design at the Folkwang Kunsthochschule in Essen.\nSince 2005: exploration, development, and construction of solographs. Since\n2007: expansion of academic department to include physical computing.\n\n**Niels Poldervaart** Born in 1994 in the Netherlands. 2016: earned bachelor’s\ndegree in communication and multimedia design at the Avans University of\nApplied Sciences’s-Hertogenbosch. Since 2016: web developer and designer, with\na focus on data visualization, e-learning, gaming, and graphic design.\n\n**Joey Lee** Born in 1990 in California. 2012: earned bachelor’s degree in\ngeography from the University of California, Los Angeles, and master’s degree\nin geography from the University of British Columbia, Vancouver. International\nactivities in research and teaching at institutions such as MIT Senseable City\nLab in 2013 and at Mozilla Science Lab in 2015. Various international\nexhibitions, prizes, and publications in the fields of new spatial and digital\nmedia, climate research in urban areas, and open source.\n\n\n## **A.5 We thank**\n\nLauren McCarthy, the initiator of p5.js.\n\nAll those involved at Gold & Wirtschaftswunder: Julia Kühne, Christian\nSchiller, Steffen Knöll, Christian Nicolaus, Andreas Lörinc.\n\nEveryone at the Hermann Schmidt Mainz publishing house (especially Brigitte\nRaab).\n\nSabrina Groß, for her text feedback and advice.\n\nAll the contributors who made small code contributions and all the developers\nof the JavaScript libraries we used in the book (see readme file in the code\npackage).\n\nThe team of twemoji\n([github.com/twitter/twemoji](http://github.com/twitter/twemoji)).\n\nFrederik De Bleser, for his prompt answers to unusual questions about\nOpentype.js and g.js.\n\n\n## **A.6 Copyright**\n\nPublished by  \nPrinceton Architectural Press  \nA McEvoy Group company  \n202 Warren Street  \nHudson, New York 12534  \n[www.papress.com](http://www.papress.com)\n\nPrinceton Architectural Press is a leading publisher in architecture, design,\nphotography, landscape, and visual culture. We create fine books and\nstationery of unsurpassed quality and production values. With more than one\nthousand titles published, we find design everywhere and in the most unlikely\nplaces.\n\nOriginally published by Verlag Hermann Schmidt  \n© 2018 Verlag Hermann Schmidt and the authors  \nAll rights reserved  \nEnglish translation © 2018 Princeton Architectural Press\n\nNo part of this book may be used or reproduced in any manner without written\npermission from the publisher, except in the context of reviews.\n\nEvery reasonable attempt has been made to identify owners of copyright. Errors\nor omissions will be corrected in subsequent editions.\n\nSpecial thanks to: Paula Baver, Janet Behning, Abby Bussel, Benjamin English,\nJan Cigliano Hartman, Susan Hershberg, Kristen Hewitt, Lia Hunt, Valerie\nKamen, Jennifer Lippert, Sara McKay, Parker Menzimer, Eliana Miller, Nina\nPick, Wes Seeley, Rob Shaeffer, Marisa Tesoro, Paul Wagner, and Joseph Weston\nof Princeton Architectural Press —Kevin C. Lippert, publisher\n\n**Library of Congress Cataloging-in-Publication Data**\n\nNames: Bohnacker, Hartmut, 1972– author. | Gross, Benedikt, 1980–author. | Laub, Julia, 1980– author. | Lazzeroni, Claudius, author. | Frohling, Marie, translator.  \nTitle: Generative design : visualize, program, and create with JavaScript in\np5.js / Benedikt Gross, Hartmut Bohnacker, Julia Laub, Claudius Lazzeroni ;\nwith contributions by Joey Lee and Niels Poldervaart ; translated by Marie\nFrohling.  \nOther titles: Generative Gestaltung. English  \nDescription: New York : Princeton Architectural Press, [2018] | Translation of Genertive Gestaltung by Hartmut Bohnacker, Benedikt Gross, and Julia Laub and edited by Claudius Lazzeroni | Includes bibliographical references.  \nIdentifiers: LCCN 2018007216 | ISBN 9781616897581 (pbk. : alk. paper) | ISBN 978-1-61689-784-0 (epub, mobi)  \nSubjects: LCSH: Computer art. | Computer drawing. | Graphic arts—Data processing. | Generative programming (Computer science) | Computer-aided design. | JavaScript (Computer program language)  \nClassification: LCC N7433.8 .B6313 2018 | DDC 776—dc23  \nLC record available at <https://lccn.loc.gov/2018007216>\n\n**Concept**  \nBenedikt Groß, Hartmut Bohnacker, Julia Laub, Claudius Lazzeroni\n\n**Text**  \nHartmut Bohnacker, Benedikt Groß, Claudius Lazzeroni\n\n**Programming**  \nBenedikt Groß, Hartmut Bohnacker, Niels Poldervaart, Joey Lee\n\nAll code in the book has been created using p5.js, version 0.5.11, and is\npublished under the Apache license.\n\n**Illustrations and photography**  \nHartmut Bohnacker, Benedikt Groß, Julia Laub, Claudius Lazzeroni, Jana-Lina\nBerkenbusch, Andrea von Danwitz, Pau Domingo, Stefan Eigner, Victor Juarez\nHernandez, Cedric Kiefer, Steffen Knöll, Andreas Lörinc, Franz Stämmele, Tom\nZiora\n\n**Book design**  \nGold & Wirtschaftswunder Stuttgart, [www.gww-design.de](http://www.gww-\ndesign.de)  \nJulia Kühne & Christian Schiller (art direction), Steffen Knöll, Christian\nNicolaus\n\n**Layout and typesetting**  \nJulia Kühne, Steffen Knöll, Christian Nicolaus, Hartmut Bohnacker, Flore de\nCrombrugghe, Maximilian Semmler\n\n**For Princeton Architectural Press**  \nTranslator: Marie Frohling  \nProject editor: Sara Stemen  \nTypesetting: Tina Henderson\n\n**Typefaces**  \nMaison Neue Book, Maison Neue Bold, Maison Neue Mono  \nFor programming code: FreeSans, Fira Sans, Miso, Noto Sans\n\n**Website**  \n[www.generative-gestaltung.de](http://www.generative-gestaltung.de)  \nConcept and design: Gold & Wirtschaftswunder, Benedikt Groß  \nProgramming and implementation: Niels Poldervaart, Joey Lee, Benedikt Groß\n\n\n## **A.7 Farewell**\n\n**Go on coding!**  \n[www.generative-gestaltung.de](http://www.generative-gestaltung.de)\n\n",
    "book_id": "generative_design",
    "book_title": "Generative Design",
    "book_author": "Benedikt Gross",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 1
  },
  {
    "chunk_full": " \n \n \nintrodução à \neletrônica \npara artistas \n \n \n \n \n \n \nhelder  da  rocha\n",
    "book_id": "introducaoeletronicaartistas",
    "book_title": "Microsoft Word - IntroduçãoEletronicaArtistas.docx",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 2
  },
  {
    "chunk_full": "Introdução a Eletrônica para Artistas \n \n \nc b a 2017 Helder da Rocha \n \n \n2 \n \nTermos de uso \n \nO conteúdo (texto, fotografias, diagramas e outras imagens) deste documento, produzido \nentre janeiro e maio de 2017 por Helder da Rocha, poderá ser reproduzido e utilizado de \nacordo com os termos da licença Creative Commons BY-SA (Attribution-ShareAlike) descrita \nem http://creativecommons.org/licenses/by-sa/3.0/br/legalcode. \n \nAlgumas imagens, diagramas e fotografias foram produzidas por terceiros e têm sua fonte \nindicada. São imagens royalty-free ou imagens de documentos de referência, relacionados a \ncomponentes ou especificações citadas no texto. \n \nVários circuitos foram inspirados ou são modificações de circuitos publicados em livros, \nrevistas e em artigos na Internet. Esses circuitos estão indicados e as fontes originais estão \nlistadas no final deste documento. \n \nO código-fonte dos exemplos em Arduino está disponibilizado em repositório GitHub \nhttps://github.com/helderdarocha/EletronicaParaArtistas e é software livre com licença de \nuso Apache 2.0. \n \nFotografia da capa por Teresa Siewerdt. \n \nDesenho de multímetros usados nos circuitos, por Mike Mitterer (disponibilizado sob licença \nCreative Commons em https://github.com/MikeMitterer). \n \n \n \n \nhttp://www.eletronicaparaartistas.com.br \n \n \n \n \n \n \n \n \n \n \n \nR672i \nRocha, Helder Lima Santos da, 1968- \nIntrodução à Eletrônica para Artistas. \n__pp. 21cm x 29.7cm. PDF. \nDocumento criado em 18 de maio de 2017. \n \n \n1. Eletrônica aplicada (621.381) – Eletrônica (621.38) – Física aplicada (621) \n2. Projeto de circuitos eletrônicos – Eletrônica (537.5) – Física (530). \n3. Aplicações artísticas da eletrônica – Artes em geral (707). \nI. \nTítulo.  \nII. \nApostila de curso livre. \n \nCDD 621.381 \n \n",
    "book_id": "introducaoeletronicaartistas",
    "book_title": "Microsoft Word - IntroduçãoEletronicaArtistas.docx",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 3
  },
  {
    "chunk_full": "Introdução a Eletrônica para Artistas \n \n \nc b a 2017 Helder da Rocha \n \n \n3 \nCONTEÚDO \n \n1. \nINTRODUÇÃO ................................................................................................................................... 8 \n1.1. \nOFICINA: INTRODUÇÃO À ELETRÔNICA PARA ARTISTAS ........................................................................ 8 \n1.1.1. \nEstrutura da oficina ............................................................................................................................... 8 \n1.1.2. \nSobre o laboratório e o material ...................................................................................................... 9 \n1.1.3. \nSobre as baterias e fonte incluídas no kit ..................................................................................... 9 \n1.1.4. \nInfraestrutura para o módulo de Arduino ................................................................................... 9 \n1.1.5. \nExperimentos extras ........................................................................................................................... 10 \n1.2. \nSOBRE ESTA APOSTILA ................................................................................................................................ 10 \n1.2.1. \nSeções de referência ............................................................................................................................ 10 \n1.2.2. \nCircuitos ................................................................................................................................................... 10 \n1.2.3. \nCódigo-fonte ........................................................................................................................................... 10 \n2. \nFUNDAMENTOS DE ELETRÔNICA ............................................................................................ 11 \n2.1. \nCIRCUITOS ..................................................................................................................................................... 11 \n2.2. \nFONTES DE ENERGIA .................................................................................................................................... 11 \n2.2.1. \nTensão ....................................................................................................................................................... 12 \n2.2.2. \nCorrente ................................................................................................................................................... 13 \nExperimento 1 – Medição de tensão de uma bateria ..................................................................................... 14 \nExperimento 2 – Ligando LEDs, cigarras e motores com 1,5 e 3V ........................................................... 16 \nExperimento 3 – Bateria de cobre/zinco com eletrólito de batata ......................................................... 16 \n2.3. \nCONDUÇÃO DE ELETRICIDADE ................................................................................................................... 19 \n2.3.1. \nCircuitos abertos e fechados ............................................................................................................ 19 \n2.3.2. \nChaves ....................................................................................................................................................... 20 \nExperimento 4 – Um circuito usando chaves .................................................................................................... 21 \n2.4. \nRESISTÊNCIA ELÉTRICA ............................................................................................................................... 23 \n2.4.1. \nResistores ................................................................................................................................................. 23 \n2.4.2. \nIdentificação de resistores ............................................................................................................... 23 \nExperimento 5 – Teste de condutividade e medição de resistência ....................................................... 24 \n2.5. \nTEORIA BÁSICA DE CIRCUITOS RESISTIVOS E LEI DE OHM .................................................................... 28 \n2.5.1. \nLei de Ohm ............................................................................................................................................... 28 \n2.5.2. \nCircuitos em série e em paralelo ................................................................................................... 28 \n2.5.3. \nMedição de tensão ............................................................................................................................... 30 \nExperimento  6 – Introdução ao protoboard e divisor de tensão ............................................................ 31 \n2.5.4. \nDivisor de tensão .................................................................................................................................. 32 \n2.5.5. \nMedição de corrente ........................................................................................................................... 33 \n2.5.6. \nDivisor de corrente .............................................................................................................................. 33 \n2.5.7. \nPotência máxima de um componente ......................................................................................... 34 \n2.6. \nSEMICONDUTORES: DIODOS E LEDS ......................................................................................................... 35 \n2.6.1. \nDiodos ........................................................................................................................................................ 35 \n2.6.2. \nLEDs ........................................................................................................................................................... 35 \nExperimento 7 – Acendendo LEDs com 9 e 12v .............................................................................................. 37 \n2.7. \nPOTENCIÔMETROS E SENSORES RESISTIVOS ........................................................................................... 38 \n2.7.1. \nPotenciômetros ..................................................................................................................................... 38 \nExperimento 8 – Variando as cores de um LED RGB ..................................................................................... 38 \n2.7.2. \nLDR – sensor de luz ............................................................................................................................. 39 \n2.7.3. \nTermistor – sensor de temperatura ............................................................................................. 40 \n2.8. \nCAPACITORES ................................................................................................................................................ 40 \nExperimento 9 – Carga e descarga de capacitores ......................................................................................... 41 \nAlteração 9.1 – Usando a carga do capacitor para acender um LED ....................................................... 42 \n2.9. \nCÉLULA PIEZOELÉTRICA .............................................................................................................................. 43 \nExperimento 10 – Gerador piezoelétrico ........................................................................................................... 43 \n",
    "book_id": "introducaoeletronicaartistas",
    "book_title": "Microsoft Word - IntroduçãoEletronicaArtistas.docx",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 4
  },
  {
    "chunk_full": "Introdução a Eletrônica para Artistas \n \n \nc b a 2017 Helder da Rocha \n \n \n4 \n3. \nTRANSISTORES ............................................................................................................................. 45 \n3.1. \nTRANSISTORES BIPOLARES DE JUNÇÃO NPN .......................................................................................... 45 \nExperimento 11 – Transistores: circuito básico .............................................................................................. 46 \nExperimento 12 – Luz de emergência com transistor .................................................................................. 47 \n3.2. \nFOTOTRANSISTOR ........................................................................................................................................ 48 \nExperimento 13 – Fototransistor que desliga a carga ao ser ativado .................................................... 48 \n3.3. \nOSCILADORES ................................................................................................................................................ 50 \nExperimento 14 – Pisca-pisca alternado com LEDs ....................................................................................... 50 \nAlteração 14.1 – Acoplador ótico ........................................................................................................................... 51 \nExperimento 15 – Oscilador sonoro ou sirene ................................................................................................. 52 \nAlteração 15.1 – Um “theremin” sensível a luz ................................................................................................ 52 \nExperimento 16 (extra) – Oscilador sonoro com transistor PNP ............................................................ 53 \n4. \nCIRCUITOS INTEGRADOS ........................................................................................................... 55 \n4.1. \nO CIRCUITO INTEGRADO 555 ..................................................................................................................... 55 \n4.1.1. \n555 em modo biestável ...................................................................................................................... 56 \nExperimento 17 – Disparador acionado por pouca luz ................................................................................ 57 \n4.1.2. \n555 em modo monoestável .............................................................................................................. 58 \nExperimento 18 – Temporizador ........................................................................................................................... 59 \nAlteração 18.1 – Usando um sensor sonoro para disparar o temporizador ....................................... 60 \nAlteração 18.2 – Substituindo o LED por um relé ........................................................................................... 62 \n4.1.3. \n555 em modo astável .......................................................................................................................... 63 \nExperimento 19 – Pisca-pisca com LED usando 555 ..................................................................................... 64 \nExperimento 20 (extra): Mini instrumento musical com 555 ................................................................... 65 \n4.1.4. \nControle da duração dos pulsos ..................................................................................................... 67 \n4.1.5. \nPWM – Pulse Width Modulation .................................................................................................... 68 \nExperimento 21 (extra) – Dimmer usando PWM ........................................................................................... 69 \nVariação 21.1 – Controle de velocidade de motor com PWM .................................................................... 71 \n4.2. \nOUTROS CIRCUITOS INTEGRADOS .............................................................................................................. 72 \n4.2.1. \nContador de década 4017 ................................................................................................................ 72 \nExperimento 22 (extra): sequenciador de LEDs com o 4017 .................................................................... 72 \nAlteração 22.1 – Sequenciador de LEDs automático com 555 e 4017 ................................................... 73 \n4.2.2. \nDecodificador para display de 7 segmentos 4026 ................................................................. 74 \nExperimento 23 (extra) – Contador de 0 até 9 com display de 7 segmentos e 4026 ...................... 74 \n5. \nINTRODUÇÃO AO ARDUINO ...................................................................................................... 76 \n5.1. \nPROJETOS ARTÍSTICOS COM ARDUINO ..................................................................................................... 76 \n5.2. \nARQUITETURA DO ARDUINO ...................................................................................................................... 77 \n5.3. \nPLACAS ARDUINO ......................................................................................................................................... 79 \n5.3.1. \nArduino Nano ......................................................................................................................................... 80 \n5.4. \nPREPARAÇÃO E TESTE DO ARDUINO ......................................................................................................... 81 \n5.5. \nINSTALAÇÃO DO AMBIENTE DE DESENVOLVIMENTO ............................................................................. 82 \n5.5.1. \nInstalação do driver ............................................................................................................................ 82 \nWindows ........................................................................................................................................................................... 82 \nMac ...................................................................................................................................................................................... 82 \nLinux ................................................................................................................................................................................... 82 \n5.5.2. \nInstalação do ambiente de programação (IDE) ..................................................................... 82 \n5.5.3. \nComunicação do Arduino com o computador ......................................................................... 83 \n5.6. \nPROGRAMAÇÃO DO ARDUINO: FUNDAMENTOS ...................................................................................... 84 \n5.6.1. \nEstrutura básica de um sketch ....................................................................................................... 84 \n5.6.2. \nSintaxe das instruções ........................................................................................................................ 85 \nComandos ......................................................................................................................................................................... 85 \nExpressões e variáveis ................................................................................................................................................ 85 \nNomes de variáveis ...................................................................................................................................................... 86 \nComentários .................................................................................................................................................................... 86 \nExperimento 24 – Piscando um LED .................................................................................................................... 87 \n5.6.3. \nPinos digitais e estados HIGH e LOW .......................................................................................... 88 \n",
    "book_id": "introducaoeletronicaartistas",
    "book_title": "Microsoft Word - IntroduçãoEletronicaArtistas.docx",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 5
  },
  {
    "chunk_full": "Introdução a Eletrônica para Artistas \n \n \nc b a 2017 Helder da Rocha \n \n \n5 \nA instrução pinMode() ................................................................................................................................................ 88 \n5.6.4. \nSaída digital ........................................................................................................................................... 88 \n5.6.5. \nVariáveis globais e #define .............................................................................................................. 89 \nVariáveis globais ............................................................................................................................................................ 89 \nOutra forma de declarar uma variável global para um pino ...................................................................... 89 \nAlteração 24.1 – Usando variáveis ......................................................................................................................... 90 \nExperimento 25 – Reagindo ao acionamento de chaves liga-desliga ..................................................... 90 \n5.6.6. \nEntrada digital ...................................................................................................................................... 91 \n5.6.7. \nLógica condicional e bloco if-else ................................................................................................. 91 \nAlteração 25.1 – Invertendo o estado de acionamento ................................................................................ 92 \nExperimento 26 – Entrada com resistores pull-up ........................................................................................ 92 \nAlteração 26.1 – Substituindo uma chave por um sensor ........................................................................... 94 \n5.6.8. \nPWM e analogWrite ............................................................................................................................ 94 \nExperimento 27 – Piscando suavemente ............................................................................................................ 95 \n5.6.9. \nEntrada analógica ............................................................................................................................... 96 \nExperimento 28 – “Theremin” com LDR e potenciômetro ......................................................................... 96 \n5.6.10. \nSerial monitor ..................................................................................................................................... 97 \nExperimento 29 – Termômetro .............................................................................................................................. 98 \nExperimento 30 (extra) – Acelerando e desacelerando o motor com luz ............................................ 99 \nAlteração 30.1 – Usando uma fonte externa para alimentar o motor ................................................. 100 \n5.7. \nPROGRAMAÇÃO DO ARDUINO: FUNÇÕES, LISTAS E BIBLIOTECAS ..................................................... 101 \n5.7.1. \nDeclarando funções ........................................................................................................................... 101 \nExperimento 31 (extra) – Definindo funções para controlar um LED RGB ...................................... 102 \n5.7.2. \nBibliotecas e arquivos .h (arquivos de cabeçalho) .............................................................. 104 \nExperimento 32 (extra) – Usando bibliotecas para produzir notas musicais ................................. 105 \n5.7.3. \nListas ........................................................................................................................................................ 107 \n5.7.4. \nRepetição com for .............................................................................................................................. 107 \nExperimento 33 (extra) – Usando LEDs RGB endereçáveis .................................................................... 108 \n6. \nTÉCNICAS ..................................................................................................................................... 112 \n6.1. \nSOLDAGEM .................................................................................................................................................. 112 \n6.1.1. \nFerramentas para soldagem ......................................................................................................... 112 \n6.1.2. \nSoldagem de terminais no LED endereçável WS2812 ....................................................... 113 \n6.1.3. \nSoldagem de terminais na célula piezoelétrica .................................................................... 115 \n6.1.4. \nSoldagem de terminais mais longos no microfone de eletreto ...................................... 115 \n6.1.5. \nSoldagem de terminais na célula fotovoltaica ...................................................................... 117 \n6.1.6. \nSoldagem de terminais rígidos nos terminais do motor ................................................... 118 \n6.1.7. \nSoldagem de um circuito na placa de fenolite universal .................................................. 118 \n6.2. \nELETRÔNICA PARA VESTIR (WEARABLE ELECTRONICS) .................................................................... 119 \n6.3. \nELETRÔNICA COM OUTROS MATERIAIS ................................................................................................. 119 \n6.3.1. \nMaterial para circuitos de papel ................................................................................................. 119 \n6.3.2. \nMassa condutiva ................................................................................................................................. 119 \n7. \nCOMPONENTES: REFERÊNCIA RÁPIDA .............................................................................. 120 \n7.1. \nCOMPONENTES ELETROMAGNÉTICOS ................................................................................................... 120 \n7.1.1. \nRelé ........................................................................................................................................................... 120 \n7.1.2. \nFonte de alimentação chaveada .................................................................................................. 120 \n7.1.3. \nMotor RF-300CA-09550 .................................................................................................................. 121 \n7.1.4. \nBuzzer (cigarra) ativo 5V ............................................................................................................... 121 \n7.1.5. \nMini microfone de eletreto ............................................................................................................. 121 \n7.1.6. \nMini alto-falante ................................................................................................................................. 121 \n7.2. \nRESISTORES E CAPACITORES ................................................................................................................... 122 \n7.3. \nSEMICONDUTORES .................................................................................................................................... 123 \n7.3.1. \nTransistores bipolares de junção ................................................................................................ 123 \nNPN de propósito geral: BC548/548/549 ou 2N3904/2N2222 ........................................................... 123 \nPNP de propósito geral: BC557/558/559 ou 2N3906 .............................................................................. 123 \n",
    "book_id": "introducaoeletronicaartistas",
    "book_title": "Microsoft Word - IntroduçãoEletronicaArtistas.docx",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 6
  },
  {
    "chunk_full": "Introdução a Eletrônica para Artistas \n \n \nc b a 2017 Helder da Rocha \n \n \n6 \n7.3.2. \nTransistores de efeito de campo (MOSFETs) ......................................................................... 123 \nMOSFET de propósito geral: 2N7000 ................................................................................................................ 123 \nMOSFET de potência: IRF540 ............................................................................................................................... 123 \n7.3.3. \nDiodos ...................................................................................................................................................... 124 \nDiodo de propósito geral: 1N4148 ..................................................................................................................... 124 \nDiodo regulador de tensão: 1N4728A – Diodo Zener ................................................................................ 124 \n7.3.4. \nLEDs ......................................................................................................................................................... 124 \nDisplay de LED com 7 segmentos HS5161AS ................................................................................................ 125 \nLED 5mm de alto-brilho .......................................................................................................................................... 125 \nLED 5mm vermelho difuso .................................................................................................................................... 125 \nLED 5mm RGB de anodo comum ........................................................................................................................ 125 \nLED SMD RGB 5050 ................................................................................................................................................... 125 \nLED 5050 WS2812 (NeoPixel) ............................................................................................................................. 126 \n7.3.5. \nCircuitos integrados e outros semicondutores ...................................................................... 126 \nL7805CV – Regulador de tensão .......................................................................................................................... 126 \nLM 555 CN – Temporizador multiuso de propósito geral ........................................................................ 127 \nCD 4017 BD – Contador de década ..................................................................................................................... 127 \nCD 4026 BE – Contador de década com decodificador para display de 7 segmentos. ................. 127 \n7.4. \nSENSORES ................................................................................................................................................... 128 \n7.4.1. \nLDRs genéricos de 5 e 7mm (valores típicos) ........................................................................ 128 \n7.4.2. \nTermistor genérico NTC 10k típico ............................................................................................ 128 \n7.4.3. \nLM35DZ – Termômetro de precisão .......................................................................................... 128 \n7.4.4. \nFototransistor TIL 78 ....................................................................................................................... 129 \n7.4.5. \nChave/sensor magnético reed ...................................................................................................... 129 \n7.4.6. \nCélula piezoelétrica ........................................................................................................................... 129 \n7.4.7. \nCélula fotovoltaica de 0,5V (silício policristalino) ............................................................... 130 \n7.5. \nIMÃS ............................................................................................................................................................. 130 \n7.5.1. \nImã de neodímio N24 10x4mm .................................................................................................... 130 \n7.5.2. \nImã de ferrite 10x4mm .................................................................................................................... 130 \n7.5.3. \nEletroimã ............................................................................................................................................... 130 \n7.6. \nFERRAMENTAS E ACESSÓRIOS ................................................................................................................. 131 \n7.6.1. \nMultímetro DT830B .......................................................................................................................... 131 \n7.6.2. \nProtoboard de 830 pontos ............................................................................................................. 132 \n7.6.3. \nPlaca de circuito impresso universal ......................................................................................... 133 \n8. \nMINI-REFERÊNCIA DE ARDUINO .......................................................................................... 134 \n8.1. \nPROGRAMAÇÃO .......................................................................................................................................... 134 \n8.1.1. \nSintaxe básica ...................................................................................................................................... 134 \nVariáveis e constantes ............................................................................................................................................. 134 \nAtribuição ...................................................................................................................................................................... 135 \nOperações aritméticas ............................................................................................................................................. 135 \nOperações de lógica relacional e booleana ..................................................................................................... 135 \nEstrutura condicional: if ......................................................................................................................................... 136 \nEstrutura de repetição: for ..................................................................................................................................... 136 \nDefinição de funções ................................................................................................................................................. 136 \nChamada de funções ................................................................................................................................................. 137 \nListas indexadas ......................................................................................................................................................... 137 \nDiretiva #include ........................................................................................................................................................ 137 \nDiretiva #define .......................................................................................................................................................... 138 \n8.1.2. \nVariáveis e constantes do Arduino ............................................................................................. 138 \nEstados lógicos ............................................................................................................................................................ 138 \nFinalidade de um pino (pinMode) ...................................................................................................................... 138 \n8.1.3. \nFunções do Arduino ........................................................................................................................... 138 \nanalogRead(pino-analógico) ................................................................................................................................. 138 \nanalogWrite(pino-digital-pwm, valor) ............................................................................................................. 139 \ndigitalRead(pino-digital) ........................................................................................................................................ 139 \ndigitalWrite(pino-digital, HIGH ou LOW) ........................................................................................................ 139 \n",
    "book_id": "introducaoeletronicaartistas",
    "book_title": "Microsoft Word - IntroduçãoEletronicaArtistas.docx",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 7
  },
  {
    "chunk_full": "Introdução a Eletrônica para Artistas \n \n \nc b a 2017 Helder da Rocha \n \n \n7 \ndelay(tempo-em-milissegundos) ........................................................................................................................ 139 \ntone(pino, frequência, duração) .......................................................................................................................... 139 \nnoTone(pino) ............................................................................................................................................................... 140 \npinMode(pino, função) ............................................................................................................................................ 140 \n8.2. \nPLACA ARDUINO NANO ............................................................................................................................ 140 \n8.2.1. \nEspecificações técnicas .................................................................................................................... 140 \n8.2.2. \nPinagem ................................................................................................................................................. 140 \n9. \nLINKS E REFERÊNCIAS ............................................................................................................. 141 \n9.1. \nREFERÊNCIAS BIBLIOGRÁFICAS .............................................................................................................. 141 \n9.1.1. \nLivros ....................................................................................................................................................... 141 \n9.1.2. \nApostilas e material de cursos ...................................................................................................... 141 \n9.1.3. \nManuais .................................................................................................................................................. 141 \n9.1.4. \nRevistas online, tutoriais, vídeos e blogs .................................................................................. 142 \n9.2. \nSOFTWARE .................................................................................................................................................. 142 \n9.2.1. \nEditores e simuladores de circuitos ........................................................................................... 142 \n9.2.2. \nCalculadoras online .......................................................................................................................... 144 \n9.3. \nFORNECEDORES DE MATERIAL ............................................................................................................... 145 \n10. \nÍNDICE DE EXPERIMENTOS ................................................................................................. 146 \n11. \nMATERIAL USADO NOS EXPERIMENTOS ......................................................................... 147 \n12. \nSOBRE O AUTOR ...................................................................................................................... 149 \n",
    "book_id": "introducaoeletronicaartistas",
    "book_title": "Microsoft Word - IntroduçãoEletronicaArtistas.docx",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 8
  },
  {
    "chunk_full": "Introdução a Eletrônica para Artistas \n \n \nc b a 2017 Helder da Rocha \n \n \n8 \n1. Introdução \nEsta apostila é utilizada como referência e fonte de aprofundamento teórico em vários tópicos que \nsão explorados de forma prática na oficina prática Introdução a Eletrônica para Artistas. Mas o a \nordem e o conteúdo deste material poderá ser diferente dependendo da carga-horária e forma de \nrealização da oficina. Nem todos os assuntos e experimentos abordados nesta apostila serão \nexplorados durante a oficina. \n1.1. Oficina: Introdução à Eletrônica para Artistas \nDurante os próximos encontros iremos explorar a arte e ciência de controlar a eletricidade usando \neletricidade – a eletrônica – um dos conhecimentos mais importantes da humanidade e que teve \ninfluência incontestável na cultura do século 20, sem a qual não existiria o rádio, a TV, as viagens \nespaciais, os computadores, a Internet. A eletrônica está em toda parte e faz parte da natureza da \nnossa civilização. Ao conhecer seus princípios você terá os recursos para usar a eletrônica como \nmatéria-prima em suas obras.  \nA oficina é um curso introdutório projetado para artistas. Durante os encontros nós iremos ligar e \ndesligar coisas, mover motores, criar e detectar som, luz e calor. Vamos também fazer medições, \nalgumas contas e também queimar alguns circuitos. O foco não é a eficiência, mas a investigação de \nusos criativos dos princípios da eletrônica. Para isto precisaremos conhecer algumas leis, princípios, \nregras, saber fazer medições, estimativas, cálculos simples (mesmo que o objetivo seja \nposteriormente quebrar as regras.) O curso é uma oportunidade de experimentação e também de \ntroca com outros artistas. Ao longo do curso e no último dia você terá a oportunidade de usar a \neletrônica em seus projetos, e em colaboração com os outros participantes. \n1.1.1. Estrutura da oficina \nO curso acontecerá durante cinco ou dez encontros e se divide em cinco partes. Os tópicos abaixo \ndescrevem o que esperamos abordar em cada parte, mas o programa pode variar um pouco se \nnecessário. Os próximos cinco capítulos desta apostila refletem essa organização. \nParte 1 – Introdução – Fundamentos de Eletrônica, Resistores, LEDs e Capacitores \nNesta primeira parte faremos uma introdução do curso, apresentar os artistas participantes, conhecer \num pouco da história e alguns exemplos de aplicações criativas da eletrônica. Apresentaremos \ntambém o kit e o laboratório. Em seguida iniciaremos com alguns experimentos: medições de tensão, \ncorrente e resistência, identificação de resistores, como usar o protoboard, como identificar \ncapacitores, e como calcular circuitos com LEDs usando a Lei de Ohm. Construiremos uma bateria \ncom batatas e acenderemos LEDs, ligaremos um motor e uma mini cigarra. \nParte 2 – Transistores \nNesta parte exploraremos circuitos mais complexos usando o transistor – o componente \nsemicondutor de silício que foi o protagonista da revolução eletrônica nos anos 50. Usaremos o \nprotoboard para construir circuitos com transistores e sensores de luz e som, acionando LEDs e relés \ncom presença e ausência de luz, magnetismo e efeitos sonoros. Faremos também um oscilador que irá \npiscar LEDs alternadamente e soar um apito em um alto-falante. Poderemos explorar também alguns \nexperimentos com eletromagnetismo. \nParte 3 – Circutos integrados \nEsta parte será dedicada à aprendizagem do circuito integrado 555, que custa menos que 1 real e \npermite fazer circuitos temporizadores, acionar motores, piscar LEDs, apitar, disparar cronômetros, \nacender luzes em sequência e displays de LED. A temporização é a base de qualquer circuito digital, \ndo mais simples ao mais complexo. Conhecer o 555 é essencial para saber fazer circuitos simples e \nbaratos, e entender melhor o funcionamento de circuitos com resistores e capacitores. Usaremos \ncapacitores para calcular a frequência e o tempo das temporizações.  \n",
    "book_id": "introducaoeletronicaartistas",
    "book_title": "Microsoft Word - IntroduçãoEletronicaArtistas.docx",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 9
  },
  {
    "chunk_full": "Introdução a Eletrônica para Artistas \n \n \nc b a 2017 Helder da Rocha \n \n \n9 \nParte 4 – Introdução ao Arduino \nNesta penúltima parte conheceremos o Arduino, um dos mais populares microcontroladores. Você \nnão precisa mais calcular circuitos de temporização com capacitores: pode escrever um programa \nsimples no computador dizendo quando o LED vai apagar e acender, e fazer muito, muito mais. O \nArduino requer a programação em computador, mas isto é muito mais simples que calcular circuitos. \nFaremos uma série de experimentos que introduzem o essencial do Arduino usando o Arduino Nano, \nincluído no kit. Se também quiser programar, traga seu computador. \nParte 5 – Projeto  \nEsta última parte do curso é livre para que você possa desenvolver e apresentar um projeto usando \neletrônica. Descubra uma forma de aplicar a eletrônica na sua arte, junte-se a outros colegas e \napresente algo no final. Pode ser algo bem simples, por exemplo, fazer um LED piscar na sua bolsa do \ncurso, um alarme acionado por luz ou um carrinho que se move com um grito ou quando a luz acende \n(esses são projetos que você conseguirá fazer ao final do curso). Dependendo de como a oficina for \norganizada, esta parte poderá ocorrer ao longo dos vários dias do curso. \n1.1.2. Sobre o laboratório e o material \nEsta é uma oficina prática. Todos terão uma bolsa com um kit contendo diversos e variados \ncomponentes eletrônicos: resistores, capacitores, transistores, circuitos integrados, sensores de luz e \ncalor, leds, chaves e interruptores, fios, etc. O kit também inclui uma barra de prototipagem (para \nmontagem dos circuitos), um multímetro, uma fonte de 9V, um alicate de bico-fino e uma placa-clone \nArduino Nano. A maioria dos componentes serão usados e demonstrados durante a oficina. Outros \npodem ser usados em seus projetos durante ou depois da oficina. \nUma lista com todos os materiais contidos no kit está em uma seção própria no final da apostila. O \nlaboratório em sala de aula também dispõe de materiais e ferramentas compartilhadas, e inclui:  \n• \nferro de soldar com suporte \n• \nsolda, pasta de solda, sugador de solda e malha de cobre \n• \nsuporte para placa de circuito impresso e terceira mão com lupa \n• \nalicates de corte, de ponta-fina e para desencapar fios \n• \nestiletes e tesouras \n• \nfio esmaltado, fios flexíveis e sólidos \n• \nfita isolante, espaguete termo-retrátil, epóxi, super-bonder, cola quente e silicone \n• \nlupas e calculadoras \n• \nmultímetros, protoboards, placas e componentes extras \n1.1.3. Sobre as baterias e fonte incluídas no kit \nIncluímos 3 tipos de fonte de energia: 1 bateria CR2032, 2 baterias AAA de 1,5V e uma fonte de 9V \n(que será usada para alimentar a maior parte dos circuitos). A fonte de 9V tem a capacidade de \nfornecer corrente para circuitos de até 1 ampere. Ela pode ser usado no lugar de uma bateria de 9V, e \ntambém para alimentar circuitos com Arduino Uno ou Nano. \nEm vez de usar a fonte de 9V você talvez ache mais fácil trabalhar com uma bateria de 9V (que pode \nnão estar incluída no kit). Uma vantagem da bateria é não precisar de tomada. A vantagem da fonte é \nque ela não descarrega e fornece corrente e tensão constante.  \nOutra vantagem da bateria, para iniciantes em eletrônica, é que ela perde carga rapidamente quando \nhá um curto-circuito (assim você pode ganhar algum tempo para desligar o circuito antes de queimar \nalguma coisa.) O ideal é usar uma bateria alcalina. Uma bateria recarregável (Ni-Mh) também pode \nser usada. Mas não use baterias de lítio (Li-ion ou Li-po). Elas esquentam muito e podem explodir em \ncaso de curto-circuito. Baterias baratas de zinco-carbono também podem ser usadas. \n1.1.4. Infraestrutura para o módulo de Arduino  \nNa quarta parte da oficina demonstraremos o uso de Arduino, e se você trouxer seu computador \npodemos tentar instalar o ambiente (IDE) do Arduino nele. Mesmo que você não leve um notebook, \nvocê ainda poderá criar circuitos com o Arduino (teremos um computador disponível para programa-\nlo). Para evitar dificuldades no acesso WiFi (que pode não estar disponível ou ser limitada em alguns \n",
    "book_id": "introducaoeletronicaartistas",
    "book_title": "Microsoft Word - IntroduçãoEletronicaArtistas.docx",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 10
  },
  {
    "chunk_full": "Introdução a Eletrônica para Artistas \n \n \nc b a 2017 Helder da Rocha \n \n \n10 \nlocais de realização da oficina), você pode baixar os drivers e ambiente de desenvolvimento antes da \naula (veja detalhes no capítulo sobre Arduino). \n1.1.5. Experimentos extras \nAlguns experimentos presentes na apostila poderão ser apenas demonstrados durante a oficina (não \nhaverá tempo para que todos possam construí-los juntos durante a aula), mas você pode montá-los \nfora do horário de aula. Eles também podem ser montados por alunos avançados que terminarem os \noutros experimentos antes. \nExperimentos marcados como “extra” não serão realizados durante esta oficina, mas alunos \navançados podem tentar fazê-los fora do horário de aula, discuti-los durante o curso e usá-los em \nseus projetos.  \nTodos os experimentos, inclusive os extras, usam apenas materiais disponíveis no kit. \n1.2. Sobre esta apostila \n1.2.1. Seções de referência \nAlém dos módulos que refletem a estrutura da oficina Introdução à Eletrônica para Artistas, esta \napostila também contém várias seções de referência. \nTécnicas \nEsta seção contém alguns tutoriais sobre técnicas úteis para construir projetos com eletrônica. Como \nsoldar, como costurar circuitos para vestir, como criar circuitos de papel, onde encontrar materiais \ncomo cola condutiva, fitas condutivas e como fazer massa condutiva.  \nLinks e referências \nEsta seção contém uma lista dos livros, artigos, tutoriais e outras referências bibliográficas usadas \npara escrever esta apostila. Também contém links para vários outros recursos, como simuladores de \ncircuitos, software e calculadoras online. \nMaterial usado nos experimentos \nUma lista de todos os materiais, ferramentas e componentes distribuídos no kit da oficina Introdução \nà Eletrônica para Artistas. \nReferência rápida dos componentes \nUma lista ilustrada de todos os principais componentes do kit, com foto do componente, símbolo \nusado nos esquemas, exemplos de uso, especificações de limites de tensão, corrente e potência, links \npara a especificação (datasheet) e informações adicionais sobre forma de uso. \nMini-referência de Arduino \nUma pequena referência rápida contendo os comandos e estruturas da linguagem do Arduino que \nexploramos neste curso, e informações técnicas sobre especificações do Arduino Nano. \n1.2.2. Circuitos \nA maior parte dos circuitos usados nesta apostila foram criados especificamente para a oficina. \nAlguns foram adaptados de circuitos publicados em livros, revistas e sites, revisados e testados. Os \ncircuitos não têm restrição de uso e podem ser usados em outros projetos e adaptados livremente. \n1.2.3. Código-fonte \nOs programas escritos para Arduino estão disponíveis na internet em um repositório GitHub. Você \npode baixá-los em https://github.com/helderdarocha/EletronicaParaArtistas. Eles são software livre \ne também podem ser usados e alterados livremente. \n",
    "book_id": "introducaoeletronicaartistas",
    "book_title": "Microsoft Word - IntroduçãoEletronicaArtistas.docx",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 11
  },
  {
    "chunk_full": "Introdução a Eletrônica para Artistas \n \n \nc b a 2017 Helder da Rocha \n \n \n11 \n2. Fundamentos de Eletrônica \n2.1. Circuitos \nPara usar eletricidade para realizar algo é preciso construir um circuito. Um circuito interliga os \npolos de uma fonte de potencial elétrico (tensão) permitindo o fluxo de corrente. Circuitos elétricos \npodem ser construídos de várias formas: interligando fios metálicos, usando uma placa de \nprototipagem, usando uma placa de circuito impresso, usando caminhos desenhados com tinta \ncondutiva, alinhavados com linha de costura condutiva, até espremidos com massa condutiva. \nUm circuito é representado graficamente através de um esquema. O esquema é um diagrama que \nilustra de forma objetiva os caminhos por onde passa a corrente e os componentes que estão \ninterligados. É como usar um mapa do metrô, que foca nas conexões e omite os detalhes \ndesnecessários. Um mesmo esquema pode representar um circuito montado em uma placa ou \ncosturado em uma roupa. Os componentes eletrônicos são representados por símbolos mais ou \nmenos padronizados. Também é comum indicar do lado de cada símbolo valores ou outras \ninformações necessárias. \nExistem várias formas de desenhar esquemas. Os três diagramas abaixo representam esquemas \nidênticos para ligar uma lâmpada LED a uma fonte de 9 volts. \n            \n \nObservação: o símbolo \n, é o símbolo de terra, ou ground, às vezes abreviado GND. Em circuitos de \nalta tensão ele realmente indica um cabo ligado à terra, que funciona como referência zero de tensão. \nMas esse símbolo também é usado em circuitos de baixa tensão para representar o terminal \nnegativo (que deve ser conectado ao negativo da bateria, não à terra). \nÉ muito importante aprender a ler esquemas eletrônicos. Para montar um circuito é preciso \ntranscrever o esquema para o meio usado na montagem (uma base de protótipos, placa de circuito \nimpresso, fios interligados, linha condutiva costurada em um tecido, etc.) \n2.2. Fontes de energia \nPara funcionar, um circuito necessita de uma fonte de energia elétrica. Todos os circuitos que iremos \nconstruir nesta oficina são circuitos de energia contínua que podem ser fornecidos por uma bateria. \nO símbolo abaixo, usado em esquemas de eletrônica, representa uma bateria. \n",
    "book_id": "introducaoeletronicaartistas",
    "book_title": "Microsoft Word - IntroduçãoEletronicaArtistas.docx",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 12
  },
  {
    "chunk_full": "Introdução a Eletrônica para Artistas \n \n \nc b a 2017 Helder da Rocha \n \n \n12 \nUma bateria é uma fonte química de potencial elétrico. Ela possui um valor nominal de potencial \nelétrico (indicado em volts) e permite o fluxo de uma quantidade máxima de corrente por unidade \nde tempo (indicado em mAh – miliamperes por hora). Para limitar a corrente é preciso que o circuito \natravesse componentes que forneçam resistência à corrente. Esses componentes diminuem o fluxo \nde corrente transformando-a em outro tipo de energia (ex: luz, movimento, calor). Potencial elétrico é \ntensão, fluxo de carga elétrica é corrente. Essas duas propriedades, mais a resistência ao fluxo de \nelétrons do material por onde flui a corrente, serão os valores mais importantes que precisamos \naprender a observar, calcular e medir ao projetar circuitos eletrônicos.  \n2.2.1. Tensão \n1 volt (indicado pelo símbolo V) é a unidade padrão de potencial elétrico, que também é chamado de \ntensão, ou voltagem. O nome é uma homenagem ao cientista italiano Alessandro Volta, o inventor da \npilha elétrica. Baterias geram potencial elétrico (tensão) através de uma reação química.  \nOutra forma comum de gerar eletricidade envolve magnetismo. A eletricidade fornecida pela fonte \nde 9 volts distribuída no kit é obtida da rede elétrica que obtém sua energia de geradores que \ntransformam energia mecânica em elétrica. A fonte retifica essa energia (converte energia alternada \nem contínua), e reduz a tensão de 120V para 9V, para que possa ser usada no lugar da bateria. \nO funcionamento de uma bateria ou fonte de tensão contínua quanto ao seu potencial elétrico pode \nser comparado ao potencial energético um tanque cheio de água. A pressão da água pode ser \nconsiderada análoga à tensão. Enquanto a saída da água estiver fechada, não haverá fluxo, mas a \npressão (potencial energético) existe. Um encanamento com a torneira fechada, que não deixa fluir \nágua, é análogo a um circuito aberto, por onde não flui corrente elétrica.  \n \nPressão e tensão são medidas relativas, e devem ser medidas em relação a um valor de referência \n(pressão do ar, potencial da terra, potencial do polo negativo, etc). Em uma bateria, usamos \nconvencionalmente como referência o polo negativo. A tensão é a diferença de potencial entre polos. \nPortanto, se medimos a tensão no polo negativo, o valor é zero, mas se medirmos a tensão no polo \npositivo (ou seja, entre o polo positivo e o negativo), o valor é o da capacidade da bateria. Esta \nanalogia também pode ser representada pela caixa d’água acima. \nNão existe uma bateria 100% eficiente. Qualquer bateria que permanecer armazenada por muito \ntempo sem uso, também irá descarregar, pois pequenas quantidades de corrente vazam entre os \npolos da bateria. Na analogia acima, representamos o vazamento interno por uma passagem estreita, \npor onde a água vaza lentamente.  \n",
    "book_id": "introducaoeletronicaartistas",
    "book_title": "Microsoft Word - IntroduçãoEletronicaArtistas.docx",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 13
  },
  {
    "chunk_full": "Introdução a Eletrônica para Artistas \n \n \nc b a 2017 Helder da Rocha \n \n \n13 \n2.2.2. Corrente \nCorrente elétrica é a quantidade de elétrons que flui por um condutor a cada segundo. É a corrente \nque efetivamente faz o circuito funcionar. 1 ampere (indicada pelo símbolo A) é a unidade padrão de \ncorrente elétrica. O nome é uma homenagem ao cientista francês Jean-Marie Ampère.  \nPode-se usar a analogia do fluxo de água também para entender o fluxo de corrente elétrica. Não é \numa analogia perfeita, mas ajuda a entender o mecanismo básico da corrente elétrica.  \nConsidere a ilustração abaixo. Na hora que a passagem da caixa d´água é aberta, a água sai pelo cano \n“+” e passa por um caminho que retorna pelo cano “–“ (fazendo analogia aos polos da bateria.) Se \nnada impedir a passagem da água, a pressão cairá drasticamente, e em pouco tempo a caixa d´água \nestará vazia. Na realidade, o cano irá oferecer alguma resistência, já que é uma passagem estreita com \natrito e veremos a pressão diminuir pressão lentamente ao longo do caminho. Mas vamos supor um \ncano ideal, quase sem atrito. Se medirmos a pressão entre dois pontos próximos do cano, teremos um \nvalor tão baixo que será percebido pelo medidor como zero: \n \nA vazão da água pode ser comparada à corrente em um circuito. Fazendo uma analogia com o \ndesenho acima, isto seria um curto-circuito - onde o positivo da bateria é ligado diretamente no \nnegativo. O curto-circuito é um fenômeno destrutivo: um valor máximo de corrente flui entre os \npolos, fazendo a tensão cair drasticamente e a bateria esquentar muito. Em pouco tempo toda a carga \nda bateria é esgotada (se ela não explodir ou se o fio não derreter e se romper antes).  \nCircuitos elétricos projetados para serem úteis buscam controlar o fluxo de corrente através de \ncargas resistivas. Uma carga pode ser uma lâmpada, um motor, ou qualquer dispositivo que \nconsuma corrente transformando-a em algum outro tipo de energia (movimento, luz, calor). Uma \ncarga pode também ser um circuito complexo, com vários caminhos por onde a corrente se divide. \nNa analogia da caixa d’água, a corrente pode ser comparada à medida da quantidade de água que flui \npelo cano em um determinado intervalo de tempo, considerando que os canos estão sempre cheios de \nágua e que não haja vazamentos no encanamento. Essa medida será igual em qualquer ponto, \nmesmo que haja algum tipo de mecanismo retardando o fluxo da água. Se o fluxo diminuir de \nvelocidade, ele diminuirá tanto no início, onde há mais pressão, quanto no final, onde há menos. Esse \nmecanismo é análogo a uma carga resistiva em um circuito elétrico. Esse tipo de carga terá, no ponto \nem que for ligada ao encanamento, uma pressão próxima à pressão da caixa d´água, mas no final, \nquando encontrar novamente o encanamento, uma pressão próxima de zero, pois nada mais impede \na passagem da água até o polo negativo, apenas a resistência do próprio cano. Se medirmos a pressão \nno meio da carga, ela deve estar perto da metade da pressão fornecida.  \n",
    "book_id": "introducaoeletronicaartistas",
    "book_title": "Microsoft Word - IntroduçãoEletronicaArtistas.docx",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 14
  },
  {
    "chunk_full": "Introdução a Eletrônica para Artistas \n \n \nc b a 2017 Helder da Rocha \n \n \n14 \n \nUm circuito elétrico funciona de forma similar. Se apenas uma carga estiver conectada a uma bateria, \na tensão medida (diferença de potencial) entre os terminais da carga será igual à tensão da bateria. \nSe a carga estiver dividida em duas partes iguais (ex: duas lâmpadas ligadas em série), e pudermos \nmedir a tensão no meio dela, encontraremos aproximadamente metade da tensão nesse ponto. Por \nfim, a tensão medida entre o terminal que está ligado ao negativo e o polo negativo da bateria, deverá \nser (praticamente) zero. Já a corrente será igual em todo o circuito. \nPortanto, embora relacionadas, as propriedades tensão e corrente podem aumentar e diminuir de \nforma independente, já que dependem da resistência. Em circuitos alimentados por baterias com a \nmesma carga resistiva, mais tensão garante mais corrente, mas se a resistência diminuir, mesmo a \ntensão permanecendo constante, a corrente irá aumentar. A tensão será (normalmente) limitada pela \ncapacidade de fornecimento da bateria. Já a corrente será geralmente limitada pela resistência da \ncarga. Mesmo com uma tensão baixa você pode ter correntes muito altas, que podem destruir um \ncircuito. Iremos explorar bastante essas propriedades nos próximos experimentos. \nExperimento 1 – Medição de tensão de uma bateria \nTudo ficará mais fácil de entender com alguns experimentos. Neste primeiro experimento vamos \naprender a usar o multímetro e tentar medir o potencial elétrico (a tensão) de várias e diferentes \nfontes de energia elétrica: duas ou três baterias e uma fonte chaveada de 9V ligada na rede elétrica. \nMaterial necessário: \n• \nMultímetro \n• \nBaterias diversas \n• \nFonte de 9V ou 12V com saída em plugue P4 macho \n• \nPlugue/tomada P4 fêmea \n• \nFios/jumpers \nUse o multímetro distribuído no kit. Um multímetro é um medidor multi-função. O modelo usado \n(830B) possui um display numérico e um seletor de função dividido em várias partes, além de \nfunções para medir tensão (voltímetro), corrente (amperímetro) e resistência (ohmímetro). Inclui \ntambém funções para testar continuidade e ganho de transistores. Neste experimento usaremos \napenas a função voltímetro, que mede tensão. O voltímetro possui uma resistência interna muito \nalta. Quando conectado a uma fonte de tensão, ele deixa passar uma quantidade mínima de corrente \n(apenas o suficiente para permitir a medição da tensão da fonte ou bateria). Na analogia da caixa \nd’água, é como se o voltímetro fizesse um pequeno furo na caixa, só para deixar passar uma pequena \nquantidade de água suficiente para medir a pressão. \n",
    "book_id": "introducaoeletronicaartistas",
    "book_title": "Microsoft Word - IntroduçãoEletronicaArtistas.docx",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 15
  },
  {
    "chunk_full": "Introdução a Eletrônica para Artistas \n \n \nc b a 2017 Helder da Rocha \n \n \n15 \nPara medir tensão, o cabo preto deverá ser colocado na tomada preta (a de baixo, marcada COM), e \no cabo vermelho na tomada vermelha do meio. \nGire o seletor para a seção de 5 posições marcada como “V=”, que serve para medir tensão contínua, \nque é o tipo de tensão produzida por baterias. (As duas posições marcadas “V~” servem para medir \ntensão alternada, como a produzida pela rede elétrica.) \n \n(jbriant.eu) \nCada posição do seletor é identificada pelo valor máximo que pode ser medido. A posição 2000 mV \nmede qualquer valor até 2V (2 mil milivolts é a mesma coisa que 2 volts). Se o valor for maior, o \nmultímetro indicará no display um “1___” à esquerda do visor. Se o voltímetro estiver na posição \n200V, medirá até 200 volts (de tensão contínua), mas não terá precisão suficiente para medir valores \npequenos de tensão. Escolha a posição mais adequada girando o seletor dentro da faixa “V=”. O ideal é \niniciar com um valor maior que a tensão a ser medida, e ir baixando até obter uma leitura que tenha \numa precisão razoável. \nMeça as tensões das baterias e fonte contidas no kit. Para medir, encoste as pontas de prova nos \nterminais das baterias.  \nVocê pode tocar os terminais e segurar com a mão se necessário. Não existe risco de choque, já que as \ntensões e correntes usadas na fonte e baterias são muito baixas. Mas não faça isto quando for medir \ntensões altas com o multímetro. Neste curso não mediremos nem faremos circuitos usando alta-\ntensão. \nO kit contém uma mini-bateria CR2032 de 3V, duas pilhas AAA de 1,5V e uma fonte de 9V, que \ndeve ser ligada na tomada. Experimente medir a tensão das pilhas AAA separadamente, e também em \nsérie, posicionando duas delas enfileiradas (positivo com negativo). \nPara medir a tensão da fonte, use um pino \nP4-fêmea (foto ao lado, à esquerda) que \nencaixa no pino P4-macho da fonte, e \nencoste as pontas de prova nos terminais \ndo lado oposto. \nVocê pode também tentar obter a tensão \nda célula fotovoltáica de silício incluída \nno kit. Ela é de vidro, finíssima e muito frágil, portanto \nmanuseie com cuidado. Use a escala de 2000 mV e coloque \numa ponta de prova de cada lado, sobre a trilha branca. O \nvalor máximo da célula fica em torno de 500 mV (milivolts) \nquando exposto à luz direta do sol.  \n",
    "book_id": "introducaoeletronicaartistas",
    "book_title": "Microsoft Word - IntroduçãoEletronicaArtistas.docx",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 16
  },
  {
    "chunk_full": "Introdução a Eletrônica para Artistas \n \n \nc b a 2017 Helder da Rocha \n \n \n16 \nExperimento 2 – Ligando LEDs, cigarras e motores com 1,5 e 3V \nNeste segundo experimento usaremos a mini bateria de 3V para acender vários LEDs, ligar um motor \ne uma cigarra. \n    \n    \n    \n    \n \nMaterial necessário: \n• \n1 bateria CR2032 de 3V \n• \n1 pilha AAA de 1,5V \n• \n1 motor de 3V \n• \nLEDs diversos \n• \nCigarra de 5V \nEscolha alguns LEDs e tente acendê-los na bateria CR2032 de 3V, segurando um terminal de cada \nlado da bateria. Se eles não acenderem em uma posição, inverta os terminais. \nLEDs não são lâmpadas comuns. São componentes eletroluminescentes polarizados que emitem luz \nem um espectro que inclui infravermelho, luz visível e ultravioleta. São muito sensíveis e não podem \nreceber grandes tensões diretamente. A maior parte dos LEDs são alimentados por tensões entre 1,8 \ne 3,5V, e baixas correntes (no máximo 0,02 A). Nenhum LED do kit acende com menos de 1,8V. Alguns \nrequerem até 3V para começar a emitir qualquer luz. \nNão tente ligar LEDs diretamente em baterias de 9V ou na tensão da fonte. Eles irão queimar \nrapidamente. Mas você pode alimentar LEDs por alguns minutos (ou mais, dependendo do LED) \nusando a mini bateria de 3V. LEDs possuem uma polaridade, portanto só irão acender se o terminal \nmenor, o catodo (K), estiver ligado ao negativo da bateria, e o terminal maior, o anodo (A), ao \npositivo. Outra forma de identificar o catodo é procurar o lado que tem um chanfro na base circular \ndo LED. Se o LED for transparente e você conseguir ver o interior dele, o catodo é o terminal que está \nligado ao componente maior.  \nLigar um LED ao contrário com 3V não irá queimá-los, mas ele não vai acender. LEDs queimam com \ntensões reversas maiores que 5V.  \nO motor incluído no kit tem um valor nominal de operação de 3V, mas funciona com tensões entre 1 e \n6V, portanto você vai conseguir fazê-lo girar ligando à bateria de 3V, ou até mesmo com a bateria de \n1,5V. Não ligue o motor diretamente na bateria de 9V. Ele irá rodar mais rápido, mas não vai durar \nmuito tempo. Diferente do LED, o motor funciona com qualquer polaridade. A única diferença é que \nele gira na direção oposta se a polaridade for trocada. \nA cigarra também tem uma polaridade. Ligue o terminal maior no positivo da bateria e o outro no \nnegativo e você ouvirá um apito agudo. Ela também deve ter um “+” impresso na sua embalagem \nindicando o terminal positivo. A cigarra também deve funcionar com 1,5V. \nExperimento 3 – Bateria de cobre/zinco com eletrólito de batata \nMaterial necessário: \n• \n1 batata (ou banana ou limão) \n• \n1 pedaço de cobre (ex: fio de cobre sólido) com 3 a 5 cm de comprimento \n• \n1 pedaço de zinco (ex: prego galvanizado) com 3 a 5 cm de comprimento \n• \nCabos com garras jacaré \n• \nMultímetro, na função Voltímetro \n• \nLED \nA reação química entre os materiais zinco e cobre permite produzir, em condições ideais, baterias \ncom até 1,1 volts de potencial elétrico. A bateria é produzida preenchendo o espaço entre os \nterminais de zinco e cobre com um ácido. Este ácido pode ser um purê de batatas, um suco de limão, \nou mesmo uma batata, limão, kiwi, ou outra fruta ácida. \n",
    "book_id": "introducaoeletronicaartistas",
    "book_title": "Microsoft Word - IntroduçãoEletronicaArtistas.docx",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 17
  },
  {
    "chunk_full": "Introdução a Eletrônica para Artistas \n \n \nc b a 2017 Helder da Rocha \n \n \n17 \n \nPara os terminais da bateria, use um pedaço de zinco (pode ser um prego ou parafuso de zinco \ngalvanizado, desde que não plastificado), e um pedaço de cobre (pode ser um fio rígido grosso). Cada \nterminal deve ter aproximadamente 5cm. Insira na batata até onde for possível, mas sem permitir \nque os terminais toquem um no outro dentro da batata.  \nDepois, use o voltímetro para medir a tensão produzida (coloque na posição 2000 mV). O valor \nmedido deve ser de pouco menos de 1 volt. Observe a polaridade. Se o valor medido for negativo, ela \nestá invertida (o cabo vermelho indica a o positivo). A polaridade será importante para ligar o LED. O \nterminal positivo da batata é o catodo de cobre. O terminal negativo é o anodo de zinco. \n \nPara acender um LED é necessário produzir pelo menos 2 a 3V (e uma corrente de pelo menos 5mA \n– 0,005 A). Como uma única batata fornece corrente insuficiente (2 mili amperes – 2mA – no \nmáximo), será necessário obter mais baterias. Junte sua batata com a de seus colegas usando garras \njacaré e ligando-as em série (cobre com zinco, zinco com cobre) e veja quantas batatas são \nnecessárias para acender um LED. Mais batatas em série aumentam a tensão produzida, e também a \ncapacidade de fornecer corrente. \n \n",
    "book_id": "introducaoeletronicaartistas",
    "book_title": "Microsoft Word - IntroduçãoEletronicaArtistas.docx",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 18
  },
  {
    "chunk_full": "Introdução a Eletrônica para Artistas \n \n \nc b a 2017 Helder da Rocha \n \n \n18 \nBaterias de batata não são muito práticas, mas \nconseguem fornecer energia por um tempo \nrazoável. Quando ela começar a perder carga, você \npode limpar a oxidação dos terminais e reinseri-los. \nA bateria na verdade não é de batata, mas de zinco-\ncobre. O eletrólito pode ser outro, como por \nexemplo, suco de limão, laranja, banana, etc. A \ncombinação zinco-cobre sempre produz em torno \nde 1V por bateria. A foto ao lado ilustra uma bateria \nsimilar feita com limão. \nAs baterias que usamos hoje contém eletrólitos \nsecos de material ácido ou alcalino. As mais \ncomuns fornecem entre 1,2 a 1,6V. Baterias de \nvalores maiores (ex: 4,5V, 9V, 12V) são obtidas \nconectando várias células em série.  \nA relação abaixo descreve algumas das baterias mais comuns usadas atualmente: \n• \nZinco-Carbono (ZnC) – baterias comuns com eletrólito ácido, que produzem 1,5V cada. Baterias \nde 9V de ZnC contém 6 células de 1,5V. \n• \nZinco-Óxido de Manganês (ZnMnO2) – baterias alcalinas modernas não-recarregáveis. Tem alta \ncapacidade de fornecimento de corrente desde que descarreguem lentamente. Fornecem 1,5V. \n• \nNíquel-Cádmio (NiCd) – baterias recarregáveis com eletrólito alcalino, produzindo 1,25V. \n• \nNíquel-Hidreto metálico (NiMH) – baterias recarregáveis populares. Geralmente fornecem \nentre 1,25V por bateria. \n• \nLítio-polímero (Lipo) e Lítio-íon (Li-ion) – mais eficientes – usadas em mini e microbaterias, \nbaterias de celular, etc. Produzem 1,2V e geralmente são distribuídas em pacotes de 3, \nfornecendo 3,7V. Estas baterias tem alta capacidade de fornecimento de corrente. Um curto-\ncircuito em uma delas pode causar incêndios e explosões. \n \nA capacidade de fornecimento de corrente é medida em mAh (miliampere por hora) e varia \ndependendo da bateria e da velocidade da descarga. Sabendo-se o valor de mAh de uma bateria e o \nnível de consumo de corrente de um circuito, pode-se estimar o tempo que um circuito funcionará \nantes que a bateria não possa mais alimentá-lo. Isto é importante para avaliar o custo de um circuito.  \nA lista abaixo contém valores típicos de capacidade em mAh para baterias populares, considerando o \nuso da bateria em condições ideais para a sua estrutura química, e baixo consumo de corrente: \n• \nBateria de 9V. Alcalina: 565, ZnC: 400, Li-ion: 1200, NiMH: 175-300, NiCd: 120, Li-po: 500 \n• \nPilhas botão de lítio: CR2032: 225. SR41: 25. SR44: 110 \n• \nAAA. Alcalina: 1200, ZnC: 540, NiMH: 800-1000, NiZn: 500 \n• \nAA. Alcalina: 2700, ZnC: 1100, Li-FeS2: 3000, NiMH: 1700-2700, NiCd: 600-1000, NiZn: 1500 \nNúmeros maiores nem sempre significam baterias melhores. O comportamento de uma bateria varia \nbastante dependendo da sua composição química. Baterias de NiMH e lítio são muito mais eficientes \nem situações de consumo elevado. Já alcalinas e baterias de zinco-carbono perdem a carga \nrapidamente quando a demanda de corrente é alta (mas podem funcionar por muito tempo se a \ndemanda for baixa.) Os valores nominais das baterias (informado na embalagem) também variam \nconforme o tipo. Alcalinas geralmente informam a tensão máxima fornecida, enquanto que baterias \nde lítio e recarregáveis informam uma tensão média, que é menor que a tensão medida quando a \nbateria está com carga máxima. Portanto, calcular o consumo usando mAh é mais preciso em circuitos \nque usam baterias recarregáveis ou de lítio. \n \n",
    "book_id": "introducaoeletronicaartistas",
    "book_title": "Microsoft Word - IntroduçãoEletronicaArtistas.docx",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 19
  },
  {
    "chunk_full": "Introdução a Eletrônica para Artistas \n \n \nc b a 2017 Helder da Rocha \n \n \n19 \n2.3. Condução de eletricidade \nPara transmitir eletricidade para outras partes do circuito, precisamos construir um “encanamento” \nfeito de condutores elétricos. Condutores são materiais capazes de conduzir corrente elétrica com \num valor desprezível de resistência. Exemplos de condutores são fios, os cabos com garras jacaré, ou \nos terminais metálicos de um LED.  \nIgualmente importantes na construção de circuitos são os materiais isolantes. Eles impedem (ou \ndificultam muito) a passagem de corrente. O ar é um isolante, assim como vários materiais sólidos \ncomo plástico e borracha.  \nO fluxo da corrente é convencionalmente representado do positivo para o negativo da bateria. Ligar \num condutor diretamente entre os dois polos da bateria provoca um curto-circuito, liberando um \ngrande fluxo de corrente em pouco tempo (o máximo que a bateria pode fornecer), o que fará com \nque ela descarregue rapidamente, provavelmente esquente muito ou até mesmo exploda. Para fazer \num circuito útil e consumir a bateria de uma forma sustentável, precisamos que o caminho entre o \npositivo e negativo ofereça alguma resistência à passagem de corrente. Por exemplo, um LED ou um \nmotor já oferecem muito mais resistência que um fio, e limitam a corrente, convertendo parte dela em \nluz ou movimento, e permitindo a descarga lenta da bateria.  \nOs melhores condutores geralmente são metais como cobre, alumínio e prata. Em circuitos práticos é \ncomum a preferência por fios de cobre e caminhos de cobre desenhados em placas de circuito \nimpresso, soldados com prata ou estanho. Em circuitos artísticos, aproveitar a resistência natural dos \nmateriais pode ser algo desejável. Podemos trocar os fios de cobre por linha de costura de aço \ninoxidável, fita adesiva de prata e cobre, tecidos condutivos, massa condutiva, papel laminado, \ncartolina laminada, tinta condutiva, cola condutiva, pó de ferro, líquido condutivo, e outros \ncondutores que não são tão eficientes quanto os fios de cobre e prata, mas que também possibilitam a \ncriação de circuitos eletrônicos. \nAs fotos abaixo ilustram linha de costura condutiva (aço inoxidável), fita de cobre condutiva, fita de \ntecido de prata condutiva e tecido condutivo. \n \n \n2.3.1. Circuitos abertos e fechados \nPor um circuito aberto não passa corrente. Se ele estiver ligado a uma fonte potencial de energia, o \nvalor dela pode ser medido e irá se concentrar entre os dois pontos onde o circuito está aberto. O \nefeito é análogo a medição da pressão concentrada em uma torneira fechada. \n \nUma vez fechado o circuito, esse ponto terá potencial quase zero, pois a corrente irá fluir sem \nimpedimentos (convencionalmente) do positivo ao negativo da fonte de tensão. É análogo à água que \nflui por um encanamento sem o impedimento da torneira fechada.  \n",
    "book_id": "introducaoeletronicaartistas",
    "book_title": "Microsoft Word - IntroduçãoEletronicaArtistas.docx",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 20
  },
  {
    "chunk_full": "Introdução a Eletrônica para Artistas \n \n \nc b a 2017 Helder da Rocha \n \n \n20 \n \n(O circuito acima é ilustrativo. Na prática, todo material tem alguma resistência, o que limita a \ncorrente e garante uma queda de tensão mínima diferente de zero.) \nCircuitos complexos consistem de vários caminhos por onde a corrente se bifurca e possuem trechos \nque são abertos e fechados temporariamente, mudando e desviando o fluxo de corrente e a \ndistribuição de potencial pelo circuito durante sua operação. O simples evento de abrir e fechar \nrapidamente um circuito gera pulsos de corrente que servem para disparar eventos em outras partes \ndo circuito. O controle de comportamentos desse tipo é o objetivo da eletrônica. \n2.3.2. Chaves \nUm circuito pode ser aberto temporariamente através de um interruptor. Um interruptor pode ser \numa chave liga-desliga simples, mas pode ser também uma chave magnética (um reed, ou um relé), \numa chave eletrônica (um transistor, uma porta lógica) ou qualquer tipo de mecanismo que \ninterrompa o fluxo de corrente que atravessa um condutor (um zíper, um botão de roupa, um encaixe \nmetálico, um fio passando através de uma porta que rompe quando alguém passa).  \nInterruptores comerciais típicos têm dois terminais que são ligados ao circuito. Podem ser \ninterruptores de pressão, normalmente abertos, que fecham um circuito apenas quando apertados, \nou interruptores que estacionam em uma das duas posições (ligado ou desligado). Os símbolos abaixo \nsão usados para representar esses interruptores em circuitos:  \n \n \nChaves que possuem três terminais podem também ser usadas como interruptores se apenas dois \ndos seus terminais forem usados. Se os três terminais fizerem parte do circuito elas servem para \ndesviar o fluxo da corrente. Essas chaves sempre têm duas posições inicialmente ligadas entre \nsi. Quando mudam de posição a configuração se inverte: abrem um trecho do circuito, mas fecham \noutro.  \n          \n \nHá também chaves de dois ou mais polos, que podem chavear vários circuitos independentes de \numa só vez. No kit há duas chaves dessas do tipo alavanca, e uma do tipo pressão.  \n    \n       \n    \n \n",
    "book_id": "introducaoeletronicaartistas",
    "book_title": "Microsoft Word - IntroduçãoEletronicaArtistas.docx",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 21
  },
  {
    "chunk_full": "Introdução a Eletrônica para Artistas \n \n \nc b a 2017 Helder da Rocha \n \n \n21 \nNo kit há também foi incluído um relé, que é uma chave acionada eletricamente (o relê do kit tem \ndois polos e duas posições), e um reed, que é uma chave acionada magneticamente (interruptor \nnormalmente aberto de um polo e uma posição).  \n      \n \n \nChaveamento eletrônico é feito através de transistores (que exploraremos mais adiante) que \nfunciona como uma porta lógica, ligando ou desligando a partir de correntes e tensões aplicadas a \num terminal de controle, e circuitos integrados (que internamente são compostos de centenas a \nbilhões de portas lógicas, abrindo e fechando centenas a bilhões de circuitos por segundo). \nExperimento 4 – Um circuito usando chaves \nUsando fios, fitas, linhas, garras jacaré, chaves e alguns componentes eletrônicos, faremos a corrente \nde uma bateria fluir por diversos caminhos ligando e desligando esses componentes. \nMaterial necessário (veja a referência no final da apostila se não souber identificar o componente): \n• \nMultímetro \n• \nCabos com garras jacaré \n• \nLinha de costura condutiva \n• \nChave táctil de pressão \n• \nChave magnética reed \n• \nChave de duas posições \n• \nImã pequeno (ferrite ou neodímio) \n• \nUm LED de qualquer cor \n• \nMotor de 3V \n• \nCigarra \n• \nBateria de 3V \n• \nPegador de roupa (para servir de suporte para a bateria). \nConstrua o circuito abaixo. Faça as conexões usando garras jacaré, pegadores de roupa e nós. Tenha \ncuidado para manter os fios suficiente afastados ou isolados para que não interfiram no circuito, \nprincipalmente a linha condutiva e as garras jacaré conectadas nas chaves, que ficam muito juntas. \nVerifique bem as conexões para evitar maus contatos. \n \n",
    "book_id": "introducaoeletronicaartistas",
    "book_title": "Microsoft Word - IntroduçãoEletronicaArtistas.docx",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 22
  },
  {
    "chunk_full": "Introdução a Eletrônica para Artistas \n \n \nc b a 2017 Helder da Rocha \n \n \n22 \nConfira a polaridade do LED e da cigarra, pois eles não funcionam se ligados ao contrário. A perna \nmaior do LED é seu anodo (A) e deve ser ligada ao positivo. A cigarra tem uma indicação na \nembalagem indicando qual dos terminais é positivo. Os outros componentes não têm polaridade e \npodem ser conectados em qualquer posição. \nA linha condutiva não é um fio elétrico comum. Ela não é isolada e é preciso ter cuidado para que ela \nnão toque em outras partes do circuito, pois pode causar um curto-circuito e descarregar a bateria. \nVocê pode trocá-la por um jumper ou pedaço de fio se quiser. Ela serve para costurar caminhos \ncondutivos em tecido. Você pode testar a condutividade dela com um LED: \n \nÀ medida em que for conectando cada componente, risque a conexão no desenho do circuito acima \npara que fique mais fácil lembrar o que já foi feito. Tenha cuidado ao manusear o reed pois sua \nembalagem de vidro é muito frágil.  \nÀs vezes é mais fácil montar o circuito usando um esquema, que foca no essencial que são as \nconexões. A ilustração abaixo é um esquema deste circuito. Tente prever o que vai acontecer quando \nas chaves mudarem de posição: \n \nAperte o botão e veja o que acontece. Depois mude as posições das chaves e aperte o botão \nnovamente. Aproxime um imã do reed. Experimente com diferentes imãs. O de neodímio consegue \nfechar a chave a uma distância muito maior que o imã de ferrite \nExperimente fazer alterações no circuito. Coloque um LED em série com o motor. O que acontece? \nTroque o interruptor de pressão por outra chave de 2 posições, para selecionar entre o circuito do \nmotor ou o circuito da cigarra e LED. \n",
    "book_id": "introducaoeletronicaartistas",
    "book_title": "Microsoft Word - IntroduçãoEletronicaArtistas.docx",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 23
  },
  {
    "chunk_full": "Introdução a Eletrônica para Artistas \n \n \nc b a 2017 Helder da Rocha \n \n \n23 \n2.4. Resistência elétrica \nA condutividade elétrica de um material é uma medida da sua capacidade de conduzir corrente \nelétrica.  O valor inverso da condutividade é a sua resistência, que mede quanto o material limita a \npassagem de corrente. A resistência elétrica é medida em ohms (Ω) em homenagem ao cientista \nalemão Georg Simon Ohm (1789-1854). Metais como cobre e prata, usados em fios e circuitos \nimpressos, possuem uma resistência muito baixa (da ordem de bilionésimos de ohm).  \n2.4.1. Resistores \nResistores são componentes de dois terminais que \npossuem uma resistência nominal e definida. Podem ser \nusados para controlar o fluxo de corrente e para dividir \ntensão com precisão. Variam em tamanho, tipo de material \nusado na construção, precisão, e capacidade de dissipar \ncalor (sua potência).  \nO kit contém muitos resistores. A maior parte são de \ncarbono, têm precisão de 95% (tolerância de 5%) e \npotência máxima de 0,25 watts. Todos esses têm corpo de cor bege e o mesmo tamanho. Um dos \nresistores disponibilizados no kit é de metal. Tem precisão de 99% (tolerância de 1%), potencia \nmáxima de 1 watt e corpo de cor azulada. \nA tolerância significa que, se um resistor tem valor nominal de 1000Ω, ele pode na verdade ter entre \n990 e 1010 Ω, se a tolerância for de 1%, ou 950 a 1050 se a tolerância for de 5%. \nOs resistores do kit estão na faixa de 100 a 1 milhão de ohms, e são expressos usando os prefixos \npadrão k (x 1000) e M (x 1000000). Portanto 1000Ω é o mesmo que 1k Ω e 1000000Ω é o mesmo \nque 1000kΩ ou 1MΩ. Em resistores comerciais é também comum representar valores fracionados \ncom k ou M no lugar da vírgula, e omitir o símbolo de ohm. Por exemplo, 2M2 e 3k3 significam 2,2MΩ \ne 3,3kΩ. Também se usa R no lugar do símbolo Ω. \nEm esquemas, resistores são representados pelos símbolos abaixo (tanto faz usar um ou outro):  \n                       \n \nA resistência dos resistores pode ser medida no multímetro, movendo o seletor para a seção “Ω”. O \nmultímetro permite medir resistências desde alguns ohms até no máximo 2 mega ohms (2 milhões de \nohms). As 5 posições do seletor são suficientes para medir com uma precisão razoável a maior parte \ndos resistores mais comuns. Este multímetro não tem precisão suficiente para medir valores de \nresistência muito pequenas. \n2.4.2. Identificação de resistores \nResistores comerciais são identificados na sua embalagem por um código de cores. A maioria dos \nfabricantes utiliza um código de 4 faixas pintadas no corpo do resistor. Resistores de alta precisão \nusam um código de 5 faixas. Eles são semelhantes. No código de 4 faixas, as duas primeiras \nrepresentam dígitos, e o terceiro representa um multiplicador. No código de 5 faixas (maior \nprecisão), são três dígitos e um multiplicador. A ultima faixa representa a tolerância (margem de \nerro) do resistor.  \nPor exemplo, um resistor com quatro faixas nas cores Vermelho, Amarelo, Verde, Dourado é de \n2,4MΩ. Vermelho e amarelo são os dígitos 2 e 4, respectivamente. Verde representa 5, mas aqui é um \nmultiplicador, ou seja 105 = 100000. Pode-se também simplesmente adicionar cinco zeros. Logo, o \nvalor do resistor é 2400000 ou 2,4M. Dourado é a tolerância, que de acordo com a tabela é de 5%, ou \nseja, o valor real do resistor é 2,4 +/- 120kΩ. A resistência deve estar entre 2,28MΩ e 2,52MΩ. \nO diagrama abaixo ilustra o uso dos códigos de cores em resistores. Veja na referência de \ncomponentes no final da apostila. Lá estão identificados todos os resistores do kit com seu código de \ncores. Na dúvida meça a resistência usando o multímetro. \n",
    "book_id": "introducaoeletronicaartistas",
    "book_title": "Microsoft Word - IntroduçãoEletronicaArtistas.docx",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 24
  },
  {
    "chunk_full": "Introdução a Eletrônica para Artistas \n \n \nc b a 2017 Helder da Rocha \n \n \n24 \n \n(Wikimedia) \nResistores SMD (Surface-Mounted Device) são resistores minúsculos de 2mm ou menos usados em \ncircuitos modernos. Eles não são identificados por cores mas por três dígitos que representam os \nnúmeros do código. Por exemplo, um resistor com marcação 104 significa dígitos “1”, “0” e \nmultiplicador “4” (+4 zeros), ou seja, 100kΩ. \nExperimento 5 – Teste de condutividade e medição de resistência \nMaterial necessário: \n• \nMultímetro \n• \nResistores diversos \n• \nCondutores diversos: fios, jumpers,  cabos com garras jacaré, um prego, linha de costura \ncondutiva, fitas condutivas (cobre e tecido de prata), papel laminado, lápis grafite \n• \nChaves diversas (reed, táctil de pressão, duas posições) \n• \nFita condutiva \n• \nLinha de costura condutiva \n• \nTraço de grafite \n• \nResistores diversos \n• \nPotenciômetros \n• \nLDRs \n• \nTermistor \nPara usar a função ohmímetro do multímetro 830B, a ponta de prova preta deverá estar na posição \nCOM, e a vermelha na posição VΩmA (mesma posição que usamos para medir tensão). \nGire o seletor do multímetro para uma das 5 posições da função ohmímetro (símbolo Ω).  \nAntes de iniciar, encoste uma ponta na outra. Esta é a resistência do fio, que é muito baixa, \npraticamente zero (muito menos do que o ohmímetro seria capaz de medir). Seria necessário 50 \nmetros de fio de cobre com 1mm de diâmetro para chegar a 1Ω. Com as pontas encostadas o \nmultímetro deve mostrar “000”, mas nas mais baixas pode aparecer um valor pequeno, que não é a \nresistência  do fio mas um erro devido à baixa precisão do ohmímetro.  \nQuando as pontas estão separadas, o valor exibido é sempre “1___” que indica uma resistência maior \ndo que aquela escala é capaz de medir. O ohmímetro mede resistência até 2000k (2MΩ).  \n",
    "book_id": "introducaoeletronicaartistas",
    "book_title": "Microsoft Word - IntroduçãoEletronicaArtistas.docx",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 25
  },
  {
    "chunk_full": "Introdução a Eletrônica para Artistas \n \n \nc b a 2017 Helder da Rocha \n \n \n25 \nA) Teste de condutividade \nPara verificar se um material conduz eletricidade ou não, você pode fazer um circuito mínimo \nconectando uma bateria de 3V e um LED usando o material como caminho para fechar o circuito. É \num teste rápido e pode ser feito segurando a bateria, o LED e o material com os dedos. Nas fotos \nabaixo acendemos um LED passando corrente por cartolina laminada, papel crepom metalizado e fita \nadesiva de cobre: \n  \n  \n \nOutra forma de medir condutividade é usando o multímetro. Gire o seletor para a posição 200k da \nfunção “Ω”. Se as pontas de prova forem encostadas em duas posições de um material condutor, o \nvisor deve mostrar o valor “000” (ou valor próximo). Se for um material isolante, aparecerá um \nnúmero “1____” alinhado à esquerda, indicando a passagem de pouquíssima ou nenhuma corrente. \n (jbriant.eu) \n1. Experimente com um fio. Use o multímetro na posição 2000k coloque uma ponta de prova de \ncada lado do fio. O valor deve ser zero ou quase zero, indicando que o fio não oferece nenhuma ou \npouquíssima resistência à corrente.  \n2. Teste a condutividade de diversos materiais: sua roupa, sua pele, sua língua, um prego, linha de \ncostura condutiva, fita de cobre, fita de prata, lápis grafite, uma linha desenhada em grafite numa \nfolha de papel. Os metais geralmente conduzem melhor. A linha de costura incluída no kit é de \naço inoxidável, e as fitas adesivas são condutivas. \n3. Teste as chaves que você usou no experimento anterior. Prenda, usando garras jacaré, as pontas \nde prova do multímetro aos terminais de uma chave táctil de pressão. O multímetro deve \nindicar que não há condutividade, mas quando você apertar o botão, o visor deve indicar que o \ncircuito está conduzindo. Teste a chave de duas posições e descubra qual dos lados está \ninicialmente fechado, e qual está inicialmente aberto usando o multímetro. Quando você mover a \nalavanca da chave, as posições devem se inverter. Teste a chave magnética reed. Prenda com \ngarras jacaré os dois terminais a pontas de prova do multímetro, que deve indicar um circuito \naberto. Aproxime (com cuidado) um imã de neodímio do reed e veja que a chave se fecha, \nfazendo o circuito conduzir. \n",
    "book_id": "introducaoeletronicaartistas",
    "book_title": "Microsoft Word - IntroduçãoEletronicaArtistas.docx",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 26
  },
  {
    "chunk_full": "Introdução a Eletrônica para Artistas \n \n \nc b a 2017 Helder da Rocha \n \n \n26 \nB) Medição de resistência \nEscolha 5 a 10 resistores e meça sua resistência encostando as pontas de prova nos terminais.  \nSe ao medir a resistência o multímetro exibir um “1___” à esquerda, e a faixa \nselecionada não for 2000k, gire o seletor para uma faixa maior até que \napareça um valor.  Se ainda assim o “1___” for exibido, a resistor tem mais que \n2MΩ. Tente identifica-lo pelo código de cores. \nSe o valor for muito pequeno, quase zero (ex: 001, 002) melhore a precisão \nda leitura girando o seletor para uma faixa menor. Lembre-se que resistências \nmuito pequenas não são medidas com precisão. \nCompare os valores medidos com os valores impressos (através de código de \ncores) em cada resistor. O valor provavelmente não será exato. As variações \nse devem à imprecisão do ohmímetro (principalmente nos valores mais \nbaixos), e à tolerância do resistor (principalmente nos valores mais altos). \nComo vimos, não é possível medir valores baixos demais de resistência \ndevido à baixa precisão do ohmímetro que temos no kit, mas o multímetro é suficiente para medir \nvalores práticos (acima de 10 ohms) que podem afetar o funcionamento de um circuito. Use o \nmultímetro para medir a resistência dos seguintes materiais: \n• \n1 metro de linha condutiva (disponível no kit) \n• \n1 metro de fita de cobre (disponível do kit – tente morder com o jacaré as duas \nextremidades, sem tirar da embalagem) \n• \n0,5 metro de fita de tecido de prata (há duas disponíveis no kit) \nSe tiver um lápis grafite, meça a resistência entre uma ponta e outra.  \nVocê também pode medir a resistência de um desenho. Em uma folha de papel, desenhe uma linha \ngrossa perto da borda da página com uns 5cm de comprimento e 0,5cm de largura. Morda as \nextremidades do desenho com garras jacaré, e conecte as outras pontas do jacaré nas pontas de \nprova. Ajuste o ohmímetro até aparecer a resistência. No desenho abaixo, temos 223kΩ de grafite. \n \nAumente a largura do desenho com o lápis grafite e a resistência deve diminuir, já que a corrente \nencontrará um caminho mais largo por onde passar. Se você usar um lápis grafite mais mole (ex: 6B \nou 9B) poderá desenhar caminhos de poucas centenas de ohms, suficiente para acender um LED. \nMeça a resistência de uma folha de cartolina laminada. Comece com uma distância pequena e \naumente gradualmente deslizando a ponta de prova sobre a folha. \nC) Resistores em série e paralelo \nVocê viu que a resistência de um caminho de grafite desenhado no papel com 1cm de largura tem \naproximadamente metade da resistência de um caminho com apenas 0,5cm de largura. Isto é porque \na corrente tem mais espaço por onde fluir. É análogo a um encanamento conectado em um sistema \nhidráulico de alta pressão. Se você adicionar mais canos, a resistência à pressão vai diminuir. Se os \ncanos tiverem diâmetro maior, também. Por outro lado vimos vários materiais cuja resistência \naumenta com o comprimento. Por exemplo, meio metro de linha condutiva tem metade da \nresistência de um metro de linha condutiva.  \n",
    "book_id": "introducaoeletronicaartistas",
    "book_title": "Microsoft Word - IntroduçãoEletronicaArtistas.docx",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 27
  },
  {
    "chunk_full": "Introdução a Eletrônica para Artistas \n \n \nc b a 2017 Helder da Rocha \n \n \n27 \nUsando resistores podemos representar valores de resistência com precisão, e obter valores \ndiferentes de resistência conectando-os em série (somando suas resistências) ou em paralelo \n(dividindo a resistência proporcionalmente). \nEscolha três resistores de mesmo valor (ex: 100Ω ou 1kΩ). Meça seus valores individualmente e \ndepois e conecte-os como mostrado abaixo.  \n \nO valor deve ser aproximadamente o triplo. Tente outras combinações de resistores e veja que seus \nvalores se somam. \nAgora ponha os resistores em paralelo e veja que o valor de resistência resultante é sempre menor \nque o valor do menor resistor. \n \nVocê pode calcular esses valores. A fórmula para resistores em série é simplesmente a soma de suas \nresistências: \nR = R1 + R2 + R3 + ... \nPor exemplo, se R1 for 100Ω, R2 for 470Ω e R3 for 1kΩ, o valor de R será: \nR = 100 + 470 + 1000 = 1,57kΩ \nA resistência de resistores em paralelo é o seu valor médio. Se forem dois resistores iguais, a \nresistência será metade. Se forem três iguais, a resistência será 1/3. Se forem diferentes, use a \nfórmula: \n1/R = 1/R1 + 1/R2 + 1/R3 + ... \nSe houver apenas dois resistores, você pode usar uma equação mais simples: \nR = (R1 x R2) / (R1 + R2) \nPor exemplo, o valor paralelo de 100Ω, 470Ω e 1kΩ será: \n1/R = 1/100 + 1/470 + 1/1000 = 0,01 + 0,00213 + 0,001 = 0,01313 \nR = 76,16 Ω \n",
    "book_id": "introducaoeletronicaartistas",
    "book_title": "Microsoft Word - IntroduçãoEletronicaArtistas.docx",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 28
  },
  {
    "chunk_full": "Introdução a Eletrônica para Artistas \n \n \nc b a 2017 Helder da Rocha \n \n \n28 \n2.5. Teoria básica de circuitos resistivos e Lei de Ohm \nNesta seção exploraremos um princípio fundamental da eletrônica que é a relação entre corrente, \ntensão e resistência. Iniciaremos com a Lei de Ohm que é uma relação matemática simples (apenas \nmultiplicação e divisão) que permite descobrir um desses valores, tendo-se os outros dois. Depois \nveremos como medir e estimar corrente e tensão em circuitos básicos.  \n2.5.1. Lei de Ohm \nA  Lei de Ohm é uma relação linear entre corrente, tensão e resistência. É uma equação fundamental \npara analisar e projetar circuitos e saber como limitar valores de tensão e corrente. A lei de Ohm é \nexpressa através da fórmula: \nV = R * I \nOnde V é a tensão em volts, R a resistência em ohms, e I é a corrente em amperes.  \nAo fazer os cálculos é importante levar em conta as unidades. Por exemplo, se a corrente estiver em \nmA (miliamperes), multiplique o valor por 1000, para que ela fique em A (amperes), e possa ser \nusada na fórmula. Ou se a resistência estiver em MΩ (megaohms), divida antes por 1000000 para \nconverter o valor para Ω (ohms). \nA Lei de Ohm pode ser usada para calcular o valor de resistência necessária para limitar a corrente \nde um componente, quando se conhece a tensão e corrente sobre ele, usando: \nR = V / I \nPor exemplo, suponha um circuito formado por um resistor alimentado por 9V. Para garantir que \numa corrente 10mA esteja passando por ele, ele deve ter uma resistência de: \nR = 9 / 0,01 = 900 ohms \nFinalmente, para calcular a corrente conhecendo-se os valores da tensão e resistência, use: \nI = V / R \n2.5.2. Circuitos em série e em paralelo \nUm circuito é como um encanamento. Existem canos abertos, outros fechados. Uns estreitos ou \nentupidos que limitam a vazão retendo a pressão da água, outros largos onde a água flui livre. De \nmaneira análoga, em um circuito as correntes se bifurcam por diversos caminhos. Há caminhos de \nalta resistência que provocam queda de tensão e a corrente é muito baixa, e outros onde a corrente \nflui livremente. Componentes que oferecem mais resistência, retém mais tensão, diminuindo a \ncorrente que flui pelo trecho. Os que oferecem pouca ou nenhuma resistência, deixam passar toda a \ncorrente que recebem, e praticamente não retém tensão.  \nPortanto, a tensão varia ao longo do caminho seguido pela corrente, iniciando com o valor da \nbateria, e terminando em zero, mas toda a corrente que entra no circuito pelo terminal positivo da \nbateria, retorna ao negativo, portanto a corrente que entra é sempre a mesma que sai.  \nConsidere o circuito abaixo onde  uma lâmpada é alimentada por uma bateria de 9V.  \n \n",
    "book_id": "introducaoeletronicaartistas",
    "book_title": "Microsoft Word - IntroduçãoEletronicaArtistas.docx",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 29
  },
  {
    "chunk_full": "Introdução a Eletrônica para Artistas \n \n \nc b a 2017 Helder da Rocha \n \n \n29 \nA corrente I que entra no circuito é igual à corrente que sai. Considerando que os fios que ligam a \nbateria à lâmpada sejam ótimos condutores, toda a queda de tensão da bateria estará sobre a \nlâmpada, que oferece uma resistência à passagem de corrente. É a passagem de corrente pelo \nfilamento que faz com que ele fique incandescente gerando a luz. \nAgora vamos conectar um dos terminais da lâmpada a outra lâmpada idêntica, e os terminais \nrestantes nos pontos A e C do circuito acima. Teremos um circuito com duas lâmpadas em série. Como \nas lâmpadas são idênticas, elas têm uma resistência interna igual, e como duas resistências em série \nse somam, a resistência do circuito agora é o dobro do que era antes, fazendo com que a corrente caia \npela metade (e que o brilho das lâmpadas também diminua). \n \nOutra maneira de entender porque as lâmpadas emitem menos luz, é que a diferença de potencial em \ncada uma delas é menor. Embora a corrente seja a mesma em todos os pontos, a tensão se divide \nentre as cargas. Em cada componente há uma “queda de tensão” que é proporcional à sua \nresistência. Um princípio importante é que a soma das quedas de tensão em um circuito equivale ao \ntotal de tensão fornecido pelo circuito, ou seja, à tensão da bateria. \nSe medirmos a queda de tensão em cada uma, veremos que há apenas 4,5V em cada lâmpada. É como \nse cada uma delas estivesse sendo alimentada individualmente por uma bateria com metade da carga. \nMenos tensão gera menos corrente, e consequentemente menos brilho. \nOutra maneira de conectar as lâmpadas na bateria é ligá-las em paralelo. Neste caso, oferecemos dois \ncaminhos para a corrente, e a tensão sobre cada lâmpada é a mesma. A tensão oferecida pela \nbateria corresponde a uma diferença de potencial elétrico fixo. Mas a corrente é a quantidade de \nelétrons circulando durante um intervalo de tempo. Se a bateria for capaz de fornecer mais elétrons \n(assim descarregando mais rapidamente), cada lâmpada poderá ter a mesma corrente que no circuito \ncom uma única lâmpada. Mas para isto, o circuito irá demandar da bateria o dobro da corrente. \n \nNas instalações elétricas residenciais, lâmpadas são instaladas em paralelo. Desta forma, novas \nlâmpadas não interferem no brilho das outras, já que a tensão sobre elas é a mesma, mas aumentam a \ndemanda de corrente (e o consumo residencial). \n",
    "book_id": "introducaoeletronicaartistas",
    "book_title": "Microsoft Word - IntroduçãoEletronicaArtistas.docx",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 30
  },
  {
    "chunk_full": "Introdução a Eletrônica para Artistas \n \n \nc b a 2017 Helder da Rocha \n \n \n30 \nSe for acrescentada uma terceira lâmpada em paralelo, ela demandará a mesma corrente, e a bateria \nprecisará ter capacidade de fornecer três vezes a corrente para o circuito. \n \nNem sempre uma bateria é capaz de fornecer a corrente necessária, e isto irá fazer com que a sua \ntensão caia (é o que acontece com a batata quando tentamos alimentar um LED que demanda mais \ncorrente que uma única batata consegue fornecer). \n2.5.3. Medição de tensão \nPodemos medir a tensão entre dois pontos de um circuito usando o multímetro na função \nVoltímetro, posicionando-o em paralelo com o componente (ou seja, encostando as pontas de prova \nnos terminais do componente, enquanto ele está ligado no circuito). A conexão do multímetro em \nparalelo garante que a mesma tensão que estiver na carga, também estará no multímetro (como a \nresistência interna do multímetro é muito alta, apenas uma minúscula corrente irá circular dentro \ndele, insuficiente para interferir na medição). \nA ilustração abaixo mostra a medição da queda de tensão sobre um resistor. Durante a medição, uma \nminúscula corrente flui pelo multímetro, mas isto na prática não afeta o resultado. \n \nSe um circuito contém apenas uma bateria e uma carga, toda a tensão estará sendo aplicada à carga. \nMas se ele contiver duas cargas interligadas em série (de forma que haja apenas um caminho para a \ncorrente), a queda de tensão será dividida entre os dois, proporcionalmente às suas resistências.  \nNo experimento a seguir mediremos as quedas de tensão sobre duas cargas ligadas em série. As \ncargas são resistores.  \n",
    "book_id": "introducaoeletronicaartistas",
    "book_title": "Microsoft Word - IntroduçãoEletronicaArtistas.docx",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 31
  },
  {
    "chunk_full": "Introdução a Eletrônica para Artistas \n \n \nc b a 2017 Helder da Rocha \n \n \n31 \nExperimento  6 – Introdução ao protoboard e divisor de tensão \nMaterial necessário: \n• \nProtoboard \n• \nMultímetro \n• \n2 resistores de 1k e resistores de 100 ohms, 10k, 100k e 1M (1 de cada) \n• \nFios e jumpers \nA) Introdução ao protoboard \nA partir deste experimento usaremos o protoboard como uma alternativa para montar circuitos. O \nprotoboard (também chamado de breadboard), é uma base de furos interligados usados para \nconstruir protótipos. Ele permite que componentes sejam inseridos e removidos de um circuito com \nfacilidade. Podemos continuar a fazer circuitos simples com garras jacaré, mas à medida em que \ntivermos que realizar mais conexões isto ficará inviável. O protoboard é ideal para experimentar e \ntestar diferentes configurações. Quando você terminar de testar o seu circuito, e ele estiver de acordo \ncom o que você deseja, você poderá montá-lo em algum lugar definitivo.  \nO protoboard incluído no kit possui 830 furos onde são inseridos terminais dos componentes e fios. \nPara construir circuitos com ele é preciso conhecer como estão interligados esses furos internamente. \nNa ilustração abaixo, que mostra o raio-X de meio-protoboard, os retângulos representam condutores \nque interligam os furos. Isto significa que se você inserir o terminal de um componente no furo a3, e o \nterminal de outro no furo e3, eles estarão conectados.   \n \nNas laterais, as colunas marcadas + e – consistem de duas conexões de 15 furos cada. Em protoboards \nde 60 linhas, é comum que as linhas laterais se estendam de uma ponta a outra, mas em alguns \nprotoboards, há uma interrupção no meio (às vezes até mais de uma). Na dúvida, retire o adesivo no \nfundo (ou meça a continuidade com o multímetro) para saber como é a configuração do seu \nprotoboard. \nVeja mais detalhes sobre o protoboard incluído no kit, na referência no final desta apostila. \nB) Medição de tensão \nMonte o circuito representado pelo esquema abaixo. A ilustração à direita mostra uma maneira de \nmontar o circuito usando o protoboard: \n",
    "book_id": "introducaoeletronicaartistas",
    "book_title": "Microsoft Word - IntroduçãoEletronicaArtistas.docx",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 32
  },
  {
    "chunk_full": "Introdução a Eletrônica para Artistas \n \n \nc b a 2017 Helder da Rocha \n \n \n32 \n   \n \nPara fazer as medições, escolha uma posição do voltímetro que seja superior à tensão da bateria ou \nfonte (no nosso caso, escolha a posição 20V). \nMeça primeiro a tensão entre os terminais da bateria. Depois meça as tensões nos terminais de cada \nresistor. Anote os resultados. Como os dois resistores têm o mesmo valor nominal, a tensão sobre \neles deve ser bem próxima.  \nAgora substitua um dos resistores por outro da lista abaixo, e veja como mudam as tensões em cada \nresistor e sobre a bateria.  \nSe os resistores forem diferentes, a tensão sobre eles será diferente. Compare os resultados.  \n2.5.4. Divisor de tensão \nO experimento anterior foi uma demonstração prática do divisor de tensão, um circuito e conceito \nmuito importante na eletrônica. É preciso saber calcular ou medir a queda de tensão sobre um \ncomponente para que você possa saber como controlar a corrente que passa por ele, através do \ncálculo de resistores.  \n \n",
    "book_id": "introducaoeletronicaartistas",
    "book_title": "Microsoft Word - IntroduçãoEletronicaArtistas.docx",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 33
  },
  {
    "chunk_full": "Introdução a Eletrônica para Artistas \n \n \nc b a 2017 Helder da Rocha \n \n \n33 \nEm vez de medir as tensões, você pode usar as fórmulas abaixo para calcular a queda de tensão em \ncada resistor acima. A queda de tensão em R1: \nVAB = 9 x 3k / (7 + 3)k = 27/10 = 2,7 V  \nE a tensão em R2: \nVBC = 9 x 7k / (7 + 3)k = 63/10 = 6,3 V  \nVimos que as resistências em série se somam, portanto os dois circuitos abaixo são equivalentes: \n                                    \n \nAgora deve ser fácil entender que a tensão da bateria se divide igualmente em cada resistor.  \nPodemos usar a Lei de Ohm para calcular a corrente, que é a mesma nos dois circuitos. Para isto \nusamos como valor de tensão V o valor da queda de tensão sobre o componente. Por exemplo, \npara calcular o valor da corrente podemos usar um dos resistores: \nI = VAB / R = 4,5 / 1000 = 0,0045 A = 4,5mA \nOu a soma deles. Tanto faz, já que a corrente é uma só: \nI = VAC / R = 9 / (1000 + 1000) = 9 / 2000 = 4,5mA \nComo exercício, calcule a corrente que flui no circuito mostrado no início desta seção (que tem \nresistores de 3k e 7k em série). \n2.5.5. Medição de corrente \nMedir corrente não é tão simples quanto medir tensão. A corrente é medida incluindo o multímetro \nem série  com o trecho do circuito por onde flui a corrente a ser medida.  É preciso abrir o circuito e \nfazer a corrente fluir por dentro do multímetro. É preciso ter cuidado pois a capacidade de corrente \nmesmo de uma bateria irá superar o limite máximo suportado pelo multímetro se o circuito não \nhouver nenhuma carga ou se ela não oferecer resistência suficiente.  \nO multímetro distribuído no kit possui dois amperímetros (medidores de corrente). Um deles mede \ncorrentes até 200mA. O segundo mede correntes até 10 A. Para selecioná-los, não basta girar o \nseletor. É preciso plugar uma das pontas de prova em local diferente. \nSempre comece configurando o multímetro na posição de máxima corrente (10A), com a ponta de \nprova vermelha plugada no primeiro soquete (indicado 10 A). Apenas se o valor medido for inferior a \n0,1 A, insira o cabo no segundo soquete (mA) com o seletor na posição 200mA, e gradualmente gire \npara valores menores até obter um valor que possa ser medido. Mesmo com essa precaução, motores \ne transformadores podem produzir pulsos curtíssimos de corrente muito intensos quando ligados e \ndesligados, que podem queimar o fusível do multímetro. \nNeste curso introdutório não faremos medição de corrente, embora ela seja uma medida importante \npara estimar o consumo de bateria de um circuito. Mas, conhecendo as tensões e resistências, \npodemos calcular a corrente que flui em um circuito. \n2.5.6. Divisor de corrente \nA corrente que entra em um circuito é sempre a mesma que sai, mas quando o caminho em um \ncircuito se bifurca, a corrente se divide. A ilustração abaixo mostra um circuito com dois resistores \n",
    "book_id": "introducaoeletronicaartistas",
    "book_title": "Microsoft Word - IntroduçãoEletronicaArtistas.docx",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 34
  },
  {
    "chunk_full": "Introdução a Eletrônica para Artistas \n \n \nc b a 2017 Helder da Rocha \n \n \n34 \nem paralelo. A corrente que é medida pelo primeiro amperímetro será o dobro da que é medida pelo \nsegundo, já que a corrente que passa pela bateria contém a soma das correntes que fluem por cada \num dos resistores. A tensão em cada resistor é a mesma. Veja que os pontos A e C de cada resistor \nsão o mesmo ponto. \n \nA quantidade de corrente que passará em cada trecho depende de sua resistência. Se os resistores \nforem diferentes, correntes diferentes passarão por cada trecho, mas a corrente que passa na bateria, \nque é a soma das correntes que passam em cada resistor, será a mesma.  \nOutra forma de analisar o divisor de corrente é considerar o efeito causado por resistores em \nparalelo. Nos dois circuitos abaixo, a corrente que passa pela bateria de 9V é a mesma. \n                        \n \n2.5.7. Potência máxima de um componente \nResistores, diodos e outros componentes têm uma indicação máxima de potência. Esse valor, quando \natribuído a um componente, refere-se à sua capacidade de dissipar calor. É muito importante \nobservar esse valor para não sobrecarregar os limites de dissipação de potência de um componente. A \npotencia é calculada multiplicando a queda de tensão e a corrente sobre um componente: \nP = V x I \nA potencia P é medida em Watts (W) em homenagem ao cientista escocês James Watt (1736-1819). 1 \nWatt é 1 Volt vezes 1 Ampere. Também se usa miliwatts para valores menores e quilowatts e \nmegawatts para grandes valores de potencia. \nPor exemplo, se uma bateria de 9V é ligada a um resistor de 10 ohms, sua corrente, pela Lei de Ohm é: \nI = V / R = 9 / 10 = 0,9 A = 900 mA \nE a potencia dissipada será: \nP = 0,9 x 9 = 8,1W \nO resistor usado deve ser capaz de suportar essa potência, caso contrário irá esquentar e queimar. Os \nresistores que temos no kit são quase todos de ¼ de Watt (0,25 W) e não seriam suficientes. Para este \ncenário, teríamos que usar um resistor de 10W. \n",
    "book_id": "introducaoeletronicaartistas",
    "book_title": "Microsoft Word - IntroduçãoEletronicaArtistas.docx",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 35
  },
  {
    "chunk_full": "Introdução a Eletrônica para Artistas \n \n \nc b a 2017 Helder da Rocha \n \n \n35 \n2.6. Semicondutores: diodos e LEDs \nSemicondutores são materiais que oferecem maior resistência à passagem de eletricidade que os \ncondutores, mas têm propriedades químicas interessantes que os fazem funcionar de forma especial \nem determinados níveis de corrente e tensão.  \nDentre os diversos materiais semicondutores existentes na natureza, o mais importante deles para a \neletrônica moderna é o silício. Os principais componentes que fazem funcionar os computadores, \ncelulares e aparelhos eletrônicos modernos em geral são feitos, na sua maior parte, de silício. \n2.6.1. Diodos \nUm diodo é um componente polarizado que só permite a passagem de \ncorrente em um sentido. Até os anos 50, diodos eram frágeis válvulas de \nvidro contendo filamentos à vácuo (como as lâmpadas incandescentes). Após \nas invenções que deram origem à eletrônica de estado sólido, eles passaram \na ser feitos com materiais semicondutores e ficaram muito menores e muito \nmais robustos. Diodos de silício são compostos por uma junção de dois \nmateriais semicondutores com propriedades elétricas opostas misturados \ncom silício. \nDiodos são muito usados para proteger circuitos contra correntes em sentido contrário (acontece \ncom motores, relés e transformadores) e para retificar corrente alternada (transformar corrente \nalternada em corrente contínua). A maior parte dos diodos emite apenas calor, mas alguns emitem \nluz. \nDiodos têm uma polaridade e funcionam de forma diferente se ela for invertida. O símbolo abaixo é \nusado para representar um diodo: \n \nOs polos são identificados com os mesmos termos usados para identificar terminais de uma bateria \n(anodo - A e catodo - K), mas com polaridade oposta.  \n2.6.2. LEDs \n \nLED significa Light-Emmiting Diode, ou diodo emissor de luz. Um LED é um diodo construído para \nemitir radiação em uma faixa de frequências estreita, e mais alta, que corresponde ao espectro de luz \n(inclusive luz invisível como infravermelho e ultravioleta).  \nO símbolo abaixo é usado para representar um LED. Em geral a embalagem possui um chanfro do lado \ndo catodo, que deve ser ligado ao negativo: \n \n",
    "book_id": "introducaoeletronicaartistas",
    "book_title": "Microsoft Word - IntroduçãoEletronicaArtistas.docx",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 36
  },
  {
    "chunk_full": "Introdução a Eletrônica para Artistas \n \n \nc b a 2017 Helder da Rocha \n \n \n36 \nExistem LEDs de várias cores e tamanhos. As cores do espectro (vermelho, laranja, amarelo, verde, \nazul, violeta) são puras. Branco e cores compostas (rosa, verde-água) são produzidas combinando \nleds diferentes. As cores dos LEDs são identificadas pelo seu comprimento de onda no espectro de \ncores. No kit há vários LEDs vermelhos, e outros com a embalagem transparente que produzem uma \ncor apenas quando são ligados.  \nImportante: no kit há também um fototransistor, que é um sensor de luz e não é um LED, mas a sua \nembalagem é idêntica. Ele tem pernas mais longas. Se você tentar acender um LED do kit, de pernas \nlongas, transparente, e ele não acender, pode ser um fototransistor (mas pode também ser um led \nqueimado). \nLEDs não tem uma relação linear entre corrente e tensão, como os resistores. Os LEDs só acendem \nquando alcançam uma tensão específica (sua tensão direta) que permanece praticamente constante \nenquanto a corrente sobe rapidamente. Portanto, LEDs geralmente não podem ser ligados \ndiretamente em uma bateria, pois queimarão em pouco tempo. É necessário conectar um resistor \npara limitar a corrente em série com o LED. Para calcular o resistor, é preciso saber qual a tensão \ndireta do LED usado. Ela depende principalmente de sua cor. Valores típicos aproximados, na \ncorrente máxima sustentável de operação (20 mA) são: \n (lednique.com) \nObserve o gráfico que relaciona a corrente direta (If) e tensão direta (Vf) de vários LEDs. Eles \ncomeçam a emitir alguma luz com cerca de 5mA. Em 20-25mA LEDs típicos atingem o brilho máximo \nsustentável. É possível fornecer mais corrente, e  fazê-lo brilhar mais intensamente, mas por \nperíodos curtos de tempo. Um pulso de corrente de 100mA ou mais geralmente queima o LED \nimediatamente. Valores menores queimam em alguns segundos a minutos. Fornecer uma tensão \ninversa ao LED nem sempre vai queimá-lo, a menos que ela exceda seu valor máximo suportável, que \npara os LEDs do kit é de aproximadamente 5 volts. \nPara calcular o resistor ideal para um LED, é preciso subtrair a tensão no LED da tensão fornecida ao \ncircuito (ou trecho do circuito), e dividir pela corrente nominal (ex: 20mA), de acordo com a Lei de \nOhm. Por exemplo, para usar um LED vermelho (queda de 2V), na capacidade máxima (20mA) com \numa bateria de 9V, o resistor deve ser de: \nR = V/I = (9 V – 2 V) / 0,02 A = 350Ω \nComo não existem comercialmente resistores de 350Ω, podemos usar um de 390Ω, que é maior e \ngarante uma corrente de um pouco menos que 20mA, ou mesmo de 470Ω, que deixará passar uma \ncorrente de: \nI = V/R = 7 V / 470Ω = 0,0149 ~ 15mA \nNão é o brilho máximo, mas é um valor seguro que permitirá que o LED tenha uma vida longa. Se o \nLED vai ficar aceso por períodos curtos, também é possível passar um pouco de 20mA e usar um \nresistor de 330 Ω. \nO cálculo do resistor limitador do LED é simples, mas se você quiser pode usar sites que fazem \ncálculos para LEDs. Veja alguns links no final desta apostila. \n•\nInfravermelho:1,6V \n•\nAmarelo:2V \n•\nVerde:2V \n•\nVermelho:2V \n•\nAzul:3V \n•\nBranco:3V \n•\nRosa:3V \n•\nUltravioleta: 3,2V \n",
    "book_id": "introducaoeletronicaartistas",
    "book_title": "Microsoft Word - IntroduçãoEletronicaArtistas.docx",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 37
  },
  {
    "chunk_full": "Introdução a Eletrônica para Artistas \n \n \nc b a 2017 Helder da Rocha \n \n \n37 \nVeja também no final da apostila maiores detalhes sobre os diferentes LEDs incluídos no kit. A \nidentificação dos LEDs depende da embalagem. LEDs RGB possuem 4 a 6 terminais, e contém três \nLEDs na mesma embalagem. Cada um deles representando uma cor. Para ligar cada cor, é preciso \nobservar o esquema e saber quem é cada terminal. Se você queimar um LED RGB, é provável que \nalgumas cores ainda funcionem. \nExperimento 7 – Acendendo LEDs com 9 e 12v \nNeste experimento calcularemos resistores para limitar a corrente dos LEDs e permitir que operem \nem brilho máximo sem correr o risco de queimar. \nMaterial necessário: \n• \nLEDs diversos \n• \nResistores de 330 Ω, 470 Ω, 560 Ω,  680 Ω e 1k Ω. \n• \nProtoboard, fios e jumpers \n• \nFonte de 9V ou 12V, ou bateria de 9V \nConstrua um circuito ligando um LED em série com um resistor e uma bateria ou fonte de acordo com \no esquema e ilustração abaixo: \n \nCalcule o resistor ideal para acender o LED de forma que ele funcione com brilho máximo sem risco \nde queimar. Leve em conta os dados a seguir: \n1. A corrente máxima de um LED comum é 20mA. \n2. A tensão que um LED utiliza é fixa (não é linear, como no resistor). Pode ser de 2V (LEDs \nvermelhos, amarelos) a 3V (azuis e brancos). Veja o gráfico e tabela de queda de tensões de \ndiferentes LEDs mostradas acima. \n3. Descubra a tensão no resistor subtraindo a tensão do LED: tensão da fonte – tensão do \nLED = tensão do resistor. \n4. Divida a tensão do resistor por 20mA, para descobrir sua resistência. Use um resistor de \nvalor igual ou maior. \nExperimente usar resistores de valor maior, e veja se o brilho do LED diminui muito.  \nExperimente também ligar mais LEDs em paralelo (catodo com catodo, anodo com anodo), mas em \nsérie com o mesmo resistor (ligado no anodo ou catodo). O que acontece?  \nExperimente ligar mais LEDs em série (bateria+, resistor, LED1, LED2). Qual o máximo de LEDs que \npodem ser colocados em série? \n",
    "book_id": "introducaoeletronicaartistas",
    "book_title": "Microsoft Word - IntroduçãoEletronicaArtistas.docx",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 38
  },
  {
    "chunk_full": "Introdução a Eletrônica para Artistas \n \n \nc b a 2017 Helder da Rocha \n \n \n38 \n2.7. Potenciômetros e sensores resistivos \nNesta seção apresentaremos alguns componentes que possuem resistência que varia. A variação pode \nser controlada manualmente (potenciômetros) ou devido à influência externa, de luz, calor, ou outros \nfatores (sensores).  \nNo kit temos potenciômetros de diversos valores e dois sensores resistivos: O LDR – Light \nDependent Resistor, que varia com a luz, e o termistor, que varia com a temperatura.  \n2.7.1. Potenciômetros \nPotenciômetros são resistores variáveis. Têm três terminais. A resistência entre os dois terminais \nmais distantes é fixa, mas o terminal do meio desliza sobre a resistência interna, permitindo obter um \nvalor variável entre cada terminal.  \nUm potenciômetro pode ser usado como um divisor de tensão variável. Se apenas o terminal do \nmeio e um dos laterais for usado, o potenciômetro se comporta como um resistor variável. Pode ser \nutilizado para controlar volume, fazer dimmers, etc. \n                              \n \nSímbolo de um potenciômetro: \n        \n \nPotenciômetros identificados com a letra B são lineares, ou seja, variam linearmente (em um \npotenciômetro de 100k, 20% é 20k, 40% é 40k). Existem também potenciômetros que variam de \nforma exponencial, que são identificados com a letra A (em um potenciômetro de 100k, um giro de \n50% corresponde a 20k).  \nUsaremos potenciômetros no experimento a seguir para variar a corrente que passa em um LED, \nalterando o seu brilho. \nExperimento 8 – Variando as cores de um LED RGB \nO objetivo é acender um LED RGB (são três LEDs em uma única embalagem). Este experimento \ntambém pode ser feito com três LEDs separados.  \nMaterial: \n• \n1 LED RGB de anodo comum (veja detalhes na referência no final da apostila) \n• \nFonte de 9 (ou 12 V) \n• \n2 resistores de 330 ohms e 1 resistor de 470 Ω (para fonte de 9V), ou 2 resistores de 470 Ω, e \n1 resistor de 560 Ω (para fonte de 12V) \n• \nTrês potenciômetros de 100k Ω (no kit não há três iguais – use um de 100k para o LED \nvermelho, 50k para o LED verde e 20k para o LED azul) \n• \nProtoboard \n• \nFios ou jumpers \n",
    "book_id": "introducaoeletronicaartistas",
    "book_title": "Microsoft Word - IntroduçãoEletronicaArtistas.docx",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 39
  },
  {
    "chunk_full": "Introdução a Eletrônica para Artistas \n \n \nc b a 2017 Helder da Rocha \n \n \n39 \nMonte o esquema abaixo: \n \nO resistor em série com o potenciômetro é importante, pois quando o potenciômetro estiver no valor \nmínimo de resistência (zero ohms), deve ainda haver uma resistência limitando a corrente no LED.  \nA ilustração a seguir contém uma possível montagem do circuito em um protoboard. \n \n2.7.2. LDR – sensor de luz \nNo kit há dois LDRs. Um de 7mm e outro de 5mm. O de 7mm é um pouco mais sensível. Estes LDRs \napresentam baixa resistência em ambientes iluminados (tipicamente 50 a 100 ohms em uma sala \niluminada, ou menos de 50 ohms em luz do sol direta), e alta resistência em ambientes escuros \n(tipicamente 100-500k em uma sombra, a 1M ohm em uma sala escura). \n               \n \n",
    "book_id": "introducaoeletronicaartistas",
    "book_title": "Microsoft Word - IntroduçãoEletronicaArtistas.docx",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 40
  },
  {
    "chunk_full": "Introdução a Eletrônica para Artistas \n \n \nc b a 2017 Helder da Rocha \n \n \n40 \nO LDR pode ser usado como um resistor variável usando os dois terminais, ou como um divisor de \ntensão, escolhendo um resistor fixo para ligar no positivo ou negativo (ilustrado acima). \nAlém do LDR, existem outros componentes no kit que reagem a luz. O foto-transistor TIL-78 tem a \nmesma embalagem que um LED translúcido, e reage à luz visível ou infravermelha aplicada \ndiretamente à sua lente. Ele não altera a resistência, mas se comporta como uma chave liga-desliga. \nA célula fotovoltaica de silício gera até 0,5V de tensão quando recebe luz direta do sol. \nVocê pode substituir os potenciômetros usados no experimento anterior por LDRs, e perceber \nvariação nos LEDs devido à luz ambiente. \n2.7.3. Termistor – sensor de temperatura \nO termistor incluído no kit reduz sua resistência com o aumento da temperatura. \nEle apresenta uma resistência de 10k ohms em temperatura ambiente (25 graus \nCélsius). Veja na referência no final desta apostila para uma estimativa da \nresistência em várias outras temperaturas. \nO termistor pode ser configurado da mesma forma que o LDR, como resistor variável, ou em um \ncircuito divisor de tensão. Ele é mais eficiente para medir temperaturas elevadas. Com 100 graus ele \ntem aproximadamente 500 ohms de resistência. Pode ser usado para acionar circuitos que disparam \numa ação quando a temperatura atinge um certo nível (ex: controlar a fervura de água). \nAlém do termistor, existe um outro componente incluído no kit que mede temperatura com mais \nprecisão: o circuito integrado LM35. Ele tem três terminais e não varia resistência com a \ntemperatura, mas a tensão entre os terminais. Ele será usado em experimentos mais adiante. \n2.8. Capacitores \nCapacitores são componentes que acumulam carga elétrica. Em circuitos onde flui corrente \ncontínua, um capacitor age como um circuito aberto, impedindo a passagem de corrente, mas \ndurante transições (ex: quando o circuito é ligado ou desligado) ou quando flui corrente alternada, o \ncapacitor age como um condutor, deixando passar a corrente.  \nNo esquema, um capacitor é representado por duas placas separadas. Alguns capacitores têm \npolaridade e precisam ser usados no circuito respeitando essa polaridade (o + ligado ao positivo e o – \nligado ao negativo). \nOs símbolos abaixo são usados para capacitores: \n \nA capacidade de carga de um capacitor é medida em farads, em homenagem ao cientista inglês \nMichael Faraday. Em geral usamos bilionésimos ou milionésimos de farad em nossos circuitos, às \nvezes ainda menos que isto. Portanto os capacitores que usaremos são representados em microfarads \n(µF) (1/1000000), nanofarads (nF) (1/109) e picofarads (pF) (1/1012).  \nOs capacitores eletrolíticos são polarizados, e têm valores maiores. A identificação deles é impressa \nna embalagem. Já os capacitores de menor valor, cerâmicos e de poliéster, têm um código para \nrepresentar o valor baseado em pF. O código é semelhante ao dos resistores, mas sem as cores. São \ntrês dígitos. Os dois primeiros representam dígitos do valor, e o terceiro o número de zeros. Por \nexemplo: \n• \n103 = 1, 0, 000 = 10000pF = 10kpF = 10nF \n• \n474 = 4, 7, 0000 = 470000pF = 470kpF = 470nF \n• \n225 = 2, 2, 00000 = 2200000pF = 2200kpF = 2200nF = 2,2 µF \nUm capacitor acumula carga assim que recebe um pulso de corrente elétrica. Se um capacitor está \nligado diretamente a uma fonte de tensão contínua, ele recebe sua carga quase instantaneamente, \n",
    "book_id": "introducaoeletronicaartistas",
    "book_title": "Microsoft Word - IntroduçãoEletronicaArtistas.docx",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 41
  },
  {
    "chunk_full": "Introdução a Eletrônica para Artistas \n \n \nc b a 2017 Helder da Rocha \n \n \n41 \nassim que a bateria for ligada no capacitor. A descarga, curto-circuitando os terminais do capacitor \ntambém é praticamente imediata. \nPara limitar o tempo de carga ou descarga de um capacitor, usa-se um resistor. O valor do resistor \nmultiplicado pelo valor da capacitância corresponde ao tempo em segundos que leva para um \ncapacitor totalmente descarregado atingir 63% de sua carga. Cinco vezes esse tempo corresponde à \ncarga total do capacitor. \nPortanto, a constante de tempo para um circuito formado por um resistor R e um capacitor C é \nt = RC \nE o tempo de carga é: \n5 x RC \nPor exemplo, para um capacitor de 100 µF (0,0001 F) em série com um resistor de 10k ohms tem uma \nconstante de tempo de: \nt = 0,001 x 10000 = 1 segundo \nE o capacitor levará \n5 x 1 = 5 segundos \npara carregar (ou descarregar) completamente. \nExperimento 9 – Carga e descarga de capacitores \nEste experimento demonstra o efeito da carga e descarga em um capacitor. \nMaterial necessário: \n• \n1 capacitor eletrolítico de 100µF \n• \nCapacitores eletrolíticos de 1000µF, 470µF, 47 e 10µF \n• \n2 resistores de 10k Ω \n• \nResistores de 1k e 100k Ω \n• \nVoltímetro \n• \nBateria ou fonte de 9V \n• \nDuas chaves tácteis (botões de pressão) \n• \nProtoboard, jumpers e fios, garras jacaré \nMonte o circuito abaixo. O desenho das chaves no protoboard (de 4 terminais) podem estar diferentes \ndas que você tem no kit (de 2 terminais). Na dúvida use o esquema como referência para as conexões. \n \n \n",
    "book_id": "introducaoeletronicaartistas",
    "book_title": "Microsoft Word - IntroduçãoEletronicaArtistas.docx",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 42
  },
  {
    "chunk_full": "Introdução a Eletrônica para Artistas \n \n \nc b a 2017 Helder da Rocha \n \n \n42 \nAntes de iniciar, meça a tensão da bateria, para saber o valor máximo de tensão que poderá ser \ncarregado pelo capacitor. Depois prenda o multímetro nos terminais do capacitor com garras jacaré, \ntendo o cuidado para não deixar que encostem uma na outra. \nApertando o botão B1 o circuito é fechado e a bateria começa a carregar através do resistor R1. \nSegure o botão por uns 5 segundos ou até que o multímetro indique a tensão da bateria. Agora solte o \nbotão e perceba que a carga diminui muito lentamente (ela está vazando pelo voltímetro, que tem \numa resistência muito alta).  \nAgora aperte o botão B2, que descarrega o capacitor através do resistor R2. Como os resistores são \niguais, o tempo de carga e descarga é semelhante ao da carga. Experimente trocá-los por valores \ndiferentes. Troque também o capacitor de 100µF por capacitores maiores (470 e 1000 µF) e menores \n(47 e 10µF), e observe o resultado.  \nPara carregar ou descarregar rapidamente ligue as chave correspondente diretamente ao positivo ou \nnegativo sem usar resistor, ou use resistores de valores baixos (100 Ω). \nAlteração 9.1 – Usando a carga do capacitor para acender um LED \nMaterial adicional: \n• \nUm LED de qualquer cor \n• \nResistor de 470 Ω \n• \nCapacitor de 2200 µF \nExperimente ligar um LED em paralelo com o capacitor de 1000µF e veja como ele se comporta \ndurante os estágios de carga e descarga (lembre-se que o LED não pode ser ligado diretamente; ele \nsempre precisa ter um resistor em série para limitar a corrente.)  \nVeja uma possível solução no circuito abaixo: \n  \n \nNesta configuração, o botão B2 descarrega o capacitor imediatamente através do fio. Mas ele também \nirá descarregar um pouco mais lentamente através do LED e resistor de 470 Ω. Se você um resistor \nmaior para limitar a corrente do LED (1k Ω) ele brilhará menos, mas ele também permanecerá aceso \npor mais tempo já que a resistência maior irá retardar a descarga do capacitor. \nExperimente trocar o capacitor de 1000µF por um capacitor de 2200µF e troque o resistor R1 por \numa ligação direta (para que a carga do capacitor seja imediata). Experimente colocar os dois \ncapacitores de 1000µF e 2200µF em paralelo. O que acontece? \nCarregue o capacitor totalmente, depois desligue a bateria do circuito. Por quanto tempo o LED ainda \npermanece aceso? \nCapacitores são bastante usados em circuitos eletrônicos, para acumular tensão, gerar pulsos, \nconfigurar temporizadores, retificar corrente alternada, proteger circuitos de sobretensão, isolar \nsinais, etc. Eles serão usados em vários outros experimentos. \n",
    "book_id": "introducaoeletronicaartistas",
    "book_title": "Microsoft Word - IntroduçãoEletronicaArtistas.docx",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 43
  },
  {
    "chunk_full": "Introdução a Eletrônica para Artistas \n \n \nc b a 2017 Helder da Rocha \n \n \n43 \n2.9. Célula piezoelétrica \nA célula piezoelétrica pode ser usada como sensor de impacto e deformação, dentro de um circuito \nelétrico, ou como para geração de energia elétrica. \n \nExperimente ligar um LED entre os terminais da célula piezoelétrica e bater rapidamente no centro \ndela. Isto causa uma mini-deformação na cerâmica que gera energia suficiente para acender o LED \ncom um pulso. Células piezoelétricas são usadas como sensores para construir instrumentos musicais \nsensíveis à velocidade e intensidade do toque. Também são usadas em sapatos para acender LEDs \ncom o impacto, detectar quando alguém bate em uma porta, e até mesmo instalados debaixo do chão \nem locais de grande movimento para gerar e acumular eletricidade. \nExperimento 10 – Gerador piezoelétrico \nEste experimento demonstra a carga de um capacitor e o uso de uma célula piezoelétrica que gera \nenergia através de movimento (deformação e impacto). \nMaterial necessário: \n• \nCélula piezoelétrica (pode ser necessário soldar terminais na placa e sensor; ou prender os \nfios usando mini-pegadores de roupa – não use garras jacaré pois a célula é frágil) \n• \nCapacitor de 100 µF \n• \n4 diodos de propósito geral (1N4148 ou equivalente) \n• \nProtoboard, fios e jumpers \n• \nVoltímetro \nA célula piezoelétrica gera energia alternada: produz corrente em um sentido quando contrai e no \nsentido inverso quando expande. Como o LED é polarizado, ele só acende em um desses pulsos. A \nponte com quatro diodos inverte os pulsos negativos permitindo aproveitar todo o ciclo (o LED \nacende duas vezes mais). O capacitor acumula carga, permitindo que o LED fique aceso sem pulsar, \n(mas no início ele vai demorar para acumular tensão suficiente para acender o LED). Esse tipo de \ncircuito é chamado de retificador, e é usado também em fontes que são ligadas em tensão alternada \npara gerar tensão contínua (como a fonte de 9V usada nos experimentos).  \n \nO capacitor é opcional.  Sem usar um capacitor, o LED brilha imediatamente, a cada impulso de \ncompressão ou expansão gerado pela deformação do sensor (batendo no sensor com o dedo, por \nexemplo, ou apertando-o).  \nUsando um capacitor, é preciso primeiro acumular carga suficiente para que haja tensão suficiente \npara acender o LED. Pode ser necessário espremer muitas vezes o sensor para que o capacitor atinja a \ntensão mínima para acender o LED. Quanto maior o capacitor, mais tempo levará para acumular a \n",
    "book_id": "introducaoeletronicaartistas",
    "book_title": "Microsoft Word - IntroduçãoEletronicaArtistas.docx",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 44
  },
  {
    "chunk_full": "Introdução a Eletrônica para Artistas \n \n \nc b a 2017 Helder da Rocha \n \n \n44 \ncarga (e para descarregar depois). Cada impulso gera um pouco de carga. Assim que o LED acumular \na carga mínima ele irá acender, mas também irá descarregar o capacitor mais rapidamente. \n \nTire o LED do circuito e coloque um voltímetro entre os terminais do capacitor, na posição de 20V. A \ncada impulso você verá que alguns milivolts se acumularem no capacitor. Quando houver \naproximadamente 2 volts, coloque um LED vermelho que ele acenderá por alguns segundos. Você \ntambém pode deixar o LED no circuito enquanto acumula carga. Enquanto não houver tensão direta \nsuficiente para acendê-lo, o consumo de corrente será baixo, e a carga subirá mais rapidamente.  \nUm LED rosa acenderá quando a tensão chegar a 2,5V aproximadamente. Na foto abaixo a luz começa \na aparecer com 2,38V: \n \nDepois de aceso, o LED rapidamente consome a corrente, diminuindo a tensão no capacitor, mas não \ntotalmente. Mais alguns toques e ela sobe de novo. Portanto, se a fonte de pulsos for contínua (ex: se \nos sensores estiverem instalados em um sapato e a pessoa estiver andando, saltando ou dançando), o \ncapacitor manterá uma luz constante no LED. Você pode usar um resistor em série com o LED, que irá \natrasar um pouco a descarga, mas também irá fazer com que ele demore mais a acender.  \nSe você usar várias células fotoelétricas retificadas em paralelo, o capacitor carregará mais rápido, e \no LED brilhará mais forte. Uma possível aplicação é instalar duas ou mais células piezoelétricas em \nsapatos para iluminar roupas e acessórios usados para correr ou dançar. O impacto no chão gera \nenergia, que se acumula, e pode ser usado para acender LEDs distribuídos pelas roupas.  \nVocê também pode preferir usar o circuito sem o capacitor, que acende o LED mais rapidamente e \ngera pulsos curtos (mas intensos) de energia apenas na hora do impacto. \nEste link contém a demonstração de um sapato gerando energia piezoelétrica para acender LEDs: \nhttps://www.youtube.com/watch?v=lDBhMCFZfv0 \n \n",
    "book_id": "introducaoeletronicaartistas",
    "book_title": "Microsoft Word - IntroduçãoEletronicaArtistas.docx",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 45
  },
  {
    "chunk_full": "Introdução a Eletrônica para Artistas \n \n \nc b a 2017 Helder da Rocha \n \n \n45 \n3. Transistores \nTransistores são os componentes eletrônicos de estado sólido responsáveis \npela revolução eletrônica no século 20. Eles substituíram as válvulas (tríodos) \nque antes eram usadas em rádios e TVs para amplificar sinais, e permitiram a \nminiaturização que possibilitaram as viagens espaciais e computadores. \nO transistor foi inventado em 1948 por John Bardeen, Walter Brattain e \nWilliam Shockley, que receberam o prêmio Nobel de física em 1956 pelo \ninvento.  \nAssim como os diodos e LEDs, transistores são feitos de junções de materiais \nsemicondutores. Os primeiros transistores eram feitos de ligas de germânio \n(Ge). Os transistores modernos são feitos principalmente de Silício (Si), que é \num dos minerais mais abundantes na natureza. Os modelos mais populares consistem de duas \njunções de três ligas de Silício, que tem suas propriedades físico-químicas alteradas através da adição \nde impurezas (outros semicondutores). \nHoje existem vários diferentes tipos de transistores. Os modelos mais antigos ainda são usados e são \ngeralmente comercializados em embalagens plásticas ou metálicas de três terminais. Chegam a custar \nem torno de 10 centavos.  Mas nos equipamentos eletrônicos modernos, como smartphones e \ncomputadores,  transistores existem aos bilhões dentro dos chips ou circuitos integrados. Consomem \npouquíssima corrente e são minúsculos (da ordem de milésimos de milímetro). \nUtilizaremos nestes experimentos os transistores bipolares de junção do tipo NPN. Eles são \nantigos, mas são baratos, fáceis de encontrar e mais robustos que os transistores de efeito de \ncampo (MOSFETs), mais modernos e que consomem muito pouca energia. Mas o kit também inclui \nMOSFETS, usados em alguns experimentos com Arduino, e transistores bipolares de junção PNP (que \ntêm polaridade inversa). \nTransistores bipolares têm três terminais que se chamam base (B), emissor (E) e coletor (C). A base \nde um transistor bipolar funciona como uma torneira que controla o fluxo de corrente do coletor \npara o emissor, que geralmente é bem maior (100 vezes ou mais) que a corrente na base. Esse fator \nde amplificação é chamado de ganho do transistor (também chamada de beta –  β ou hFE).  \nExistem duas formas básicas de usar um transistor. Como amplificador ou como chave.  \nComo amplificador ele é configurado para operar em uma faixa que amplia a corrente na base de \nmaneira mais ou menos linear. Se usarmos a analogia da torneira, operando como amplificador ela \nnunca seria totalmente fechada ou aberta, mas operaria apenas controlando a intensidade do fluxo.  \nComo chave, o transistor trabalha em estados extremos, totalmente ligado (em saturação) ou \ndesligado (em corte). A torneira ou está completamente aberta (corrente na base suficiente para \nsaturar o transistor) ou completamente fechada (nenhuma corrente ou corrente negativa na base). \nEm resumo, o transistor se comporta exatamente como uma chave entre seus terminais C e E, que é \naberta ou fechada pela corrente aplicada no terminal B. Quando o transistor está desligado (sem \ncorrente na base), a tensão entre C e E é máxima, e a corrente é zero. Quando ele está saturado a \ntensão entre C e E é zero e a corrente entre C e E é máxima. \nAlém do uso como amplificador e chave, transistores também podem ser usados para construir \nosciladores (geradores de ondas e pulsos) amplificando ciclos de carga e descarga de circuitos RC \n(resistor-capacitor) e realimentando-o na sua entrada. \n3.1. Transistores bipolares de junção NPN \nO funcionamento que descrevemos acima para o transistor refere-se a transistores bipolares de \njunção NPN, que é apenas um dos tipos de transistores usados, mas é o único que iremos explorar \nnesta seção., O símbolo desse tipo de transistor está ilustrado abaixo. É preciso identificar os \nterminais na embalagem plástica do componente. Veja na referência no final desta apostila. Ela varia \nde acordo com o modelo (há muitos) e o fabricante. \n",
    "book_id": "introducaoeletronicaartistas",
    "book_title": "Microsoft Word - IntroduçãoEletronicaArtistas.docx",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 46
  },
  {
    "chunk_full": "Introdução a Eletrônica para Artistas \n \n \nc b a 2017 Helder da Rocha \n \n \n46 \n \nB = base (controle, baixa corrente + ou -), C = coletor (+), E = emissor (-) \nExperimento 11 – Transistores: circuito básico \nPara demonstrar o funcionamento básico do transistor, monte o circuito abaixo. \nMaterial necessário: \n• \n1 transistor BC 549 (ou equivalente) \n• \n1 resistor de 220 Ω (ou 330 Ω, se a fonte for de 12 V) \n• \n1 LED \n• \nProtoboard \n• \nFios e jumpers \n• \nFonte de 9V (ou 12V) \n \nVocê fará a ligação da base com o terminal positivo segurando nas duas pontas dos fios. Como a sua \nresistência é muito alta, a corrente que irá fluir pela base será baixíssima. \nVerifique todas as conexões e ligue a fonte de 9V por último. Os terminais separados não devem \ntocar em hipótese alguma. Mantenha-os separados no protoboard. \n \nAo fechar o circuito, uma pequena corrente (da ordem de microampères) irá circular pela base, \nabrindo (bastante) a “torneira” do transistor e permitindo a passagem de uma grande corrente entre \nC e E, suficiente para acender o LED. Não seria possível acender o LED com a corrente que passa na \nbase apenas. \n \n \n“Feche” o circuito segurando \nestes dois fios. \n \n",
    "book_id": "introducaoeletronicaartistas",
    "book_title": "Microsoft Word - IntroduçãoEletronicaArtistas.docx",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 47
  },
  {
    "chunk_full": "Introdução a Eletrônica para Artistas \n \n \nc b a 2017 Helder da Rocha \n \n \n47 \nNa verdade, o circuito é tão sensível que o LED poderá acender até mesmo antes que você toque nos \nfios. Normalmente usamos circuitos com um divisor de tensão na base, para que se tenha maior \ncontrole sobre a corrente da base evitando interferências. Se houver interferência, experimente \nconectar um resistor de 10M ou mais ligando a base ao negativo. Neste caso, o LED acenderá quando \na resistência que liga a base ao positivo for maior que a resistência que a liga ao negativo. O próximo \nexperimento usará um divisor de tensão na base. \nExperimento 12 – Luz de emergência com transistor \nEste circuito fará um LED acender quando o ambiente estiver escuro, e apagar quando estiver claro. \nMaterial necessário: \n• \nBateria ou fonte de 9V (ou fonte de 12V) \n• \nLED \n• \nTransistor NPN de propósito geral (BC549 ou equivalente) \n• \nResistor de 220 Ω (ou 330 Ω, se a fonte for de 12V) \n• \nResistor de 47k Ω, 100k Ω ou potenciômetro de 100k Ω + resistor de 10k Ω (valor vai \ndepender da sensibilidade do LDR e luminosidade da sala) \n• \nLDR de 7mm (pode-se usar o de 5mm, mas será preciso ajustar a sensibilidade) \n• \nProtoboard, fios e jumpers \n \nPode ser necessário ajustar o resistor R1, dependendo da sensibilidade do LDR e da luz do ambiente. \nCom a mesma quantidade de luz, o LDR de 7mm terá uma resistência maior que o de 5mm, então se \nfor usado o de 5mm, pode ser necessário uma resistência maior (ex: 100k Ω) para que o circuito \nfuncione igual. No desenho do protoboard está sendo usado um resistor de 100k Ω: \n \n",
    "book_id": "introducaoeletronicaartistas",
    "book_title": "Microsoft Word - IntroduçãoEletronicaArtistas.docx",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 48
  },
  {
    "chunk_full": "Introdução a Eletrônica para Artistas \n \n \nc b a 2017 Helder da Rocha \n \n \n48 \nUma outra alternativa é substituir R1 por um resistor de 10k Ω em série com um potenciômetro de \n100k, para permitir o ajuste fino dependendo das condições de luz do ambiente. \n \n \n3.2. Fototransistor \nO fototransistor é um transistor especial cuja base é ativada por luz. Se não houver luz o transistor \ndesliga. Se houver luz diretamente aplicada na lente, ele liga e deixa passar corrente entre E e C. O \nfototransistor é acionável por luz visível e também por luz infravermelha. Fototransistores são \nfrequentemente usados em controles remotos. \nExperimento 13 – Fototransistor que desliga a carga ao ser ativado \nEste circuito tem um comportamento similar ao circuito com LDR. É mais simples e mais sensível, \nmas o foto-transistor requer que a luz seja aplicada diretamente na sua lente. \n",
    "book_id": "introducaoeletronicaartistas",
    "book_title": "Microsoft Word - IntroduçãoEletronicaArtistas.docx",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 49
  },
  {
    "chunk_full": "Introdução a Eletrônica para Artistas \n \n \nc b a 2017 Helder da Rocha \n \n \n49 \nMaterial necessário: \n• \nFonte de 9V (ou 12V) \n• \nFototransistor TIL 78 (tem a mesma embalagem que um LED – veja referência no final \ndesta apostila para detalhes). \n• \nResistor de 1k Ω \n• \nLED \n• \nProtoboard, fios/jumpers \nA embalagem do fototransistor é transparente e igual à de um LED. A perna mais longa é o emissor \n(E) e a mais curta é o coletor (C). No circuito abaixo o emissor (perna mais longa) deve ser conectada \nao negativo e ao catodo (perna mais curta) do LED.  \n \nQuando o transistor liga, ele faz um curto-circuito nos terminais do LED, apagando-o. Se não houver \nluz no fototransistor, ele se comporta como um circuito aberto, e toda a corrente irá fluir pelo LED. \nPara apagar o LED, aplique luz diretamente sobre a lente do fototransistor (ex: use um outro LED ou \numa lanterna de celular). Teste também apontando um LED infravermelho (não foi incluído no kit) \nque emite luz invisível. \nO LED pode ser substituído por outro circuito, que será desligado quando o fototransistor receber luz. \nNeste circuito, você pode também substituir o LED por uma cigarra (buzzer). \n \n",
    "book_id": "introducaoeletronicaartistas",
    "book_title": "Microsoft Word - IntroduçãoEletronicaArtistas.docx",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 50
  },
  {
    "chunk_full": "Introdução a Eletrônica para Artistas \n \n \nc b a 2017 Helder da Rocha \n \n \n50 \nComo você alteraria este circuito para que ele tenha o comportamento inverso, e acenda o LED (ou \nacione a cigarra) somente quando houver luz? \nNão desmonte este circuito pois utilizaremos em outro experimento. \n3.3. Osciladores \nUm oscilador é um circuito que automaticamente alterna seu estado. É possível usar transistores para \nconstruir um oscilador realimentando a sua entrada com o sinal produzido na saída. Osciladores \npermitem gerar sinais de corrente alternada, produzindo ondas em vários formatos, permitindo a \nconstrução de sirenes e pisca-piscas. Também são usados em circuitos transmissores e receptores de \nsinais de rádio e muitas outras aplicações.  \nExistem várias formas de construir um oscilador. Todos envolvem realimentação de sinais, e não são \nmuito simples de entender, mas são circuitos fundamentais na eletrônica, produzem resultados \ninteressantes e portanto vale a pena construir alguns.  \nExperimento 14 – Pisca-pisca alternado com LEDs \nUsando capacitores e podemos acionar automaticamente a corrente na base dos transistores a partir \nda carga do capacitor.  Combinando dois circuitos desse tipo fazemos um circuito oscilador que é \nchamado de gangorra, ou “multivibrador astável”. É formado por dois circuitos amplificadores \nsimétricos. A entrada (base) de um é realimentada pela saída (coletor) do outro. \nNeste experimento usaremos uma combinação de capacitores e resistores para produzir uma \nfrequência que permite que os LEDs pisquem de forma alternada. \nMaterial necessário: \n• \nBateria de 9V, fonte de 9V (ou de 12V) \n• \n2 resistores de 200 Ω (use 330 Ω, se fonte for de 12V) \n• \n2 LEDs \n• \n2 resistores de 330k Ω \n• \n2 capacitores eletrolíticos de 3,3 µF \n• \n2 transistores NPN de propósito geral (BC 549 ou equivalente) \n• \nProtoboard, fios e jumpers \nMonte o circuito abaixo prestando atenção nas conexões: \n \nA ilustração abaixo mostra uma montagem possível que pode ser feita no protoboard: \n",
    "book_id": "introducaoeletronicaartistas",
    "book_title": "Microsoft Word - IntroduçãoEletronicaArtistas.docx",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 51
  },
  {
    "chunk_full": "Introdução a Eletrônica para Artistas \n \n \nc b a 2017 Helder da Rocha \n \n \n51 \n \nVocê pode variar os valores dos resistores da base (não use menos que 1k Ω) e dos capacitores para \nobter tempos e carga e descarga diferentes.  \nNão se limite ao protoboard! Tente montar o circuito usando o \nesquema. A fotografia ao lado mostra o mesmo circuito oscilador \nexplorado neste experimento construído usando fita de cobre \nsobre uma folha de papel. \nO tempo que cada LED fica aceso pode ser calculado usando a \nfórmula \n0,7 x R x C \nOnde R e C são o resistor e o capacitor ligados a base do \ntransistor. No nosso exemplo os tempos são iguais, com resistor \nde 330k e capacitor de 3,3uF, então cada pulso dura: \n0,7 x 330000 x 0,0000033 = 0,8s \nVeja também as calculadoras online para esse tipo de circuito no \nfinal da apostila. \nAlteração 14.1 – Acoplador ótico  \nExperimente posicionar a saída de um dos LEDs diretamente na lente do fototransistor do circuito do \nexperimento anterior, como mostrado na fotografia abaixo.  \n  \nIsto vai acionar o fototransistor sempre que o LED acender, fazendo com que o LED, que é controlado \npelo do circuito do fototransistor, apague e acenda no mesmo ritmo que os LEDs do oscilador astável. \nOs dois circuitos agora estão sincronizados. O LED e o foto-transistor funcionam nesta configuração \ncomo um acoplador ótico. \n",
    "book_id": "introducaoeletronicaartistas",
    "book_title": "Microsoft Word - IntroduçãoEletronicaArtistas.docx",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 52
  },
  {
    "chunk_full": "Introdução a Eletrônica para Artistas \n \n \nc b a 2017 Helder da Rocha \n \n \n52 \nExperimento 15 – Oscilador sonoro ou sirene \nEsta é uma adaptação do circuito anterior. A estrutura básica é a mesma, mas foram alterados valores \ndos capacitores e resistores para que a oscilação ocorra em uma frequência maior e audível. Usando \nresistores de 100k e capacitores de 10nF, o circuito oscilará em uma frequência próxima de 1500 \nciclos por segundo. Nessa frequência, não será possível ver os LEDs piscarem, mas podemos ouvir a \noscilação. Para isto, acrescentamos um estágio amplificador na saída de um dos transistores, ligado \na um alto-falante. \nMaterial necessário: \n• \nAlto-falante de 8 Ω \n• \n2 resistores de 100k Ω \n• \n2 resistores de 680 Ω \n• \n1 resistor de 47k Ω \n• \n1 resistor de 33k Ω \n• \n1 resistor de 47 Ω \n• \n2 capacitores de 10nF \n• \n1 capacitor eletrolítico de 10 µF \n• \n3 transistores BC 549 ou equivalente \n• \nProtoboard, fios e jumpers \nO circuito está ilustrado abaixo. O módulo amplificador é um circuito separado, que recebe o sinal do \noscilador através do capacitor de 10µF, e o amplifica, ativando o alto-falante. O resultado, ao ligar o \ncircuito, deve ser um apito agudo. \n \nA ilustração abaixo mostra uma possível implementação do circuito usando o protoboard. \n \nAlteração 15.1 – Um “theremin” sensível a luz \nUm theremin é um sintetizador de som que produz tonalidades e timbres diferentes a partir de \ninterferência de sensores externos, geralmente magnéticos. Podemos criar um theremin simples para \nvariar a frequência com a luz incluindo um LDR no oscilador sonoro. \n",
    "book_id": "introducaoeletronicaartistas",
    "book_title": "Microsoft Word - IntroduçãoEletronicaArtistas.docx",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 53
  },
  {
    "chunk_full": "Introdução a Eletrônica para Artistas \n \n \nc b a 2017 Helder da Rocha \n \n \n53 \nMaterial adicional: \n• \n1 LDR (sensor resistivo de luz) \nEscolha um dos lados do circuito e coloque um LDR em série com um resistor de 100k. Isto vai \naumentar a resistência e consequentemente o tempo de descarga de um dos capacitores, diminuindo \na frequência e fazendo o som ficar mais grave. Mas quando houver luz no LDR, a sua resistência irá \ncair fazendo com que a frequência fique próxima à do circuito anterior, e o som mais agudo. \n \nExperimento 16 (extra) – Oscilador sonoro com transistor PNP \nExistem diversas outras formas de construir osciladores com transistores. Nenhum é muito fácil de \ncalcular e controlar. Dependendo da aplicação, o ideal é usar cristais de quartzo (que garante alta \nprecisão e permite frequências elevadas) ou indutores e sistemas que geram ondas naturais \n(senoidais). Para finalizar este capítulo e usar o transistor PNP disponível no kit, incluímos um \ncircuito oscilador clássico usando um transistor PNP e NPN. Ele gera som cuja frequência varia com a \nluz. O transistor PNP funciona com polaridade inversa ao NPN. \nMaterial necessário: \n• \nTransistor NPN de propósito geral (BC 549 ou equivalente) \n• \nTransistor PNP de propósito geral (BC 559 ou equivalente) \n• \nCapacitor de 1nF \n• \nResistor de 1k Ω \n• \nResistor de 3,3M Ω \n• \nResistor de 47 Ω \n• \nLDR \n• \nAlto-falante \n• \nFios, jumpers e protoboard \n• \nFonte de 9V \n",
    "book_id": "introducaoeletronicaartistas",
    "book_title": "Microsoft Word - IntroduçãoEletronicaArtistas.docx",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 54
  },
  {
    "chunk_full": "Introdução a Eletrônica para Artistas \n \n \nc b a 2017 Helder da Rocha \n \n \n54 \n \nVocê pode substituir o LDR por outros sensores (ex: de umidade ou temperatura). Pode também \nvariar a frequência alterando resistores e capacitores (mas este circuito não é tão flexível quanto os \noutros). A ilustração abaixo mostra uma possível implementação com protoboard: \n \n",
    "book_id": "introducaoeletronicaartistas",
    "book_title": "Microsoft Word - IntroduçãoEletronicaArtistas.docx",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 55
  },
  {
    "chunk_full": "Introdução a Eletrônica para Artistas \n \n \nc b a 2017 Helder da Rocha \n \n \n55 \n4. Circuitos integrados \nCircuitos integrados são componentes eletrônicos que contém circuitos inteiros, formados por \ndezenas, centenas, milhares, milhões e até bilhões de transistores. Circuitos integrados também são \nchamados de chips. Geralmente eles têm uma aplicação bem definida e diversos terminais usados \ncomo entradas e saídas e para configuração. Circuitos eletrônicos que usam circuitos integrados são \ngeralmente mais compactos e mais simples do que os que dependem de transistores e diodos \nindividuais. Em geral, uma mesma funcionalidade é mais fácil de implementar, ou fica mais robusta ou \nmais segura usando um circuito integrado em vez de transistores individuais.  \nNesta oficina apresentaremos alguns circuitos integrados baratos e populares. Não se preocupe se \nnão entender completamente o funcionamento deles. É possível construir circuitos e até mesmo fazer \nalterações nos circuitos sem entender todos os detalhes. Monte os experimentos e depois, se desejar, \nleia as explicações do seu funcionamento. \n4.1. O circuito integrado 555 \nO circuito integrado 555 é um componente muito versátil. \nBasicamente funciona como um temporizador, que pode ser \nconfigurado em um circuito simples contendo um capacitor e alguns \nresistores. O chip está contido em uma embalagem de plástico de \noito pinos, alinhados em paralelo (padrão DIP – Dual-In-Line). Dois \npinos são usados para alimentá-lo com uma tensão entre 3 e 15V \n(GND: negativo, e Vcc: positivo). Os outros pinos são usados como \nentrada e saída. \nPinos de circuitos integrados DIP são contados a partir de uma \nmarca que indica o pino 1: um chanfro ou um ponto. O pino 1 está localizado à esquerda da marca, e \na contagem prossegue crescente pelo lado esquerdo, e volta pelo lado direito, de forma que o \nprimeiro e último pinos se localizam na frente do componente, em lados opostos.  \nO diagrama abaixo descreve os pinos do circuito integrado 555. Os pinos vermelhos (1 e 8) são para \nalimentação do componente. Os verdes (2, 4, 5 e 6) são entradas, e os azuis (3 e 7) são saídas. \n \nO 555, e muitos outros circuitos integrados que funcionam com lógica digital, reconhece na entrada e \nproduz na saída valores fixos de tensão que são identificados como níveis lógicos. A saída do 555 \nou é zero (nível lógico BAIXO) ou Vcc (nível lógico ALTO), que normalmente é uma tensão maior \nque zero (tipicamente a mesma tensão usada para alimentá-lo). Os pinos de entrada (em verde) são \ncontrolados aplicando neles valores de tensão relativas à tensão de entrada (1/3 e 2/3 dessa tensão), \npara produzir os valores de tensão 0V (nível lógico baixo) ou Vcc (alto) nas saídas. \nA tabela abaixo descreve em detalhes o funcionamento de cada pino. \nPino \nNome \nFunção \n1 \nGND \n0V. Ligue no polo NEGATIVO da bateria. \n2 \nTRIG \nDisparador. Um intervalo de temporização inicia quando a entrada neste pino cai \nabaixo de ½ do valor em CTRL (1/3 de VCC, se CTRL não estiver sendo usado). Isto \nfaz o valor em OUT ser ALTO. O valor ALTO em OUT será mantido enquanto este pino \nestiver com tensão baixa. \n",
    "book_id": "introducaoeletronicaartistas",
    "book_title": "Microsoft Word - IntroduçãoEletronicaArtistas.docx",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 56
  },
  {
    "chunk_full": "Introdução a Eletrônica para Artistas \n \n \nc b a 2017 Helder da Rocha \n \n \n56 \n3 \nOUT \nSaída. O valor de saída, que pode ser nível lógico ALTO (até 1.7 V abaixo de +VCC) ou \nBAIXO (igual a GND). \n4 \nRESET \nReset. Reinicia o intervalo se ligado em GND. Um novo intervalo só inicia novamente \nse RESET tiver uma tensão de no mínimo 0,7V. \n5 \nCTRL \nTensão de controle. Permite estabelecer a tensão de referência usada para \ndisparar e limitar o temporizador. Normalmente este pino não é usado (e deve ser \nconectado ao GND através de um capacitor de 10nF), e neste caso a tensão de \nreferência será sempre de 2/3 de VCC. \n6 \nTHR \nLimite. Quando o nível de tensão aqui for maior que em CTRL (2/3 de VCC), o valor \nem OUT será reduzido para zero, terminando o ciclo. \n7 \nDIS \nChave de descarga. O pino é ligado temporariamente à GND entre cada intervalo de \ntemporização. Um capacitor conectado aqui será descarregado entre intervalos. O \ninício de novo ciclo fecha a chave que só abre novamente quando o próximo \nintervalo terminar (quando a tensão em OUT tiver nível lógico BAIXO). \n8 \nVCC \nFonte de tensão entre +3 e +15V para alimentar o componente. Ligue este pino no \npolo POSITIVO da bateria. \nOs controles (entradas) disponíveis são 4: CTRL(5), RESET(4), TRIG(2) e THR(6). A entrada RESET \n(pino 4) tem precedência sobre TRIG(pino 2), e TRIG(pino 2) tem precedência sobre THR(pino 6). \nIsto significa que se RESET estiver acionado (ligado no negativo), os valores de TRIG e THR são \nignorados, e se RESET estiver inativo (ligado no positivo), e TRIG for acionado (ligado no negativo), o \nvalor de THR é ignorado. THR só é considerado se RESET e TRIG estiverem ambos inativos (ligados \nno positivo). A entrada CTRL estabelece a referência usada por TRIG e THR. Em circuitos simples, \nCTRL é ligada a GND através de um capacitor de 10nF (para eliminar interferências), fazendo com que \na referência seja considerada igual à tensão de entrada (Vcc). \nAs saídas são duas: OUT(3) e DIS(7). OUT (pino 3) é usado em praticamente todas as aplicações. DIS \n(pino 7)é ligada ao GND (negativo) todas as vezes que um ciclo termina, e geralmente usada para \ndescarregar um capacitor ligado neste pino. \nTudo isto será mais fácil de entender se fizermos alguns circuitos. A seguir estão três circuitos \nessenciais com 555. Em todos eles, o pino VCC(8) liga-se ao positivo, o pino GND(1) ao negativo, e o \npino CTRL(5) ao negativo através de um capacitor de 10nF. A saída é sempre conectada ao pino \nOUT(3), e os outros pinos variam conforme o modo usado. \n4.1.1. 555 em modo biestável \n \nO circuito biestável é um alternador de estado. É uma espécie de memória que guarda um estado \n(ligado/ALTO ou desligado/BAIXO). O estado é uma tensão, portanto o estado ligado pode ser usado \npara acender um LED, e o estado desligado funciona para apagá-lo. Esta tensão irá aparecer na saída \nOUT (pino 3). O disparador do alternador é TRIG (pino 2), que liga o circuito. Para reinicializar o \nprocesso e voltar ao estado anterior, aciona-se o RESET (pino 4) que reinicializa o processo. \n",
    "book_id": "introducaoeletronicaartistas",
    "book_title": "Microsoft Word - IntroduçãoEletronicaArtistas.docx",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 57
  },
  {
    "chunk_full": "Introdução a Eletrônica para Artistas \n \n \nc b a 2017 Helder da Rocha \n \n \n57 \nNeste circuito o pino 7 (DIS) não é usado e o pino 6 (THR) é ligado ao negativo. Somente os pinos 2 \n(disparador) e 4 (reset) controlam o circuito. Os dois devem iniciar em estado ligado, ou ALTO \n(conectados ao positivo).  \nPara iniciar o ciclo (começa com nível ALTO em OUT), o pino 2 precisa ser ligado temporariamente \nao negativo. \nPara terminar o ciclo (causar nível BAIXO em OUT), o pino 4 é ligado temporariamente ao negativo. \nA ligação pode ser feita com chaves, botões, sensores, transistores, etc. \nUma técnica para manter os níveis lógicos em estado ALTO é construir um divisor de tensão (entre os \npolos positivo e negativo da bateria, com o pino ligado no meio e um resistor ligado ao positivo que \ngaranta uma tensão alta o suficiente no pino para mantê-lo desligado (maior que 1/3 de Vcc). Essa \ntécnica é chamada de resistor de pull-up já que ela puxa para o ALTO o estado do pino. O estado do \npino só mudará quando a tensão cair abaixo do valor mínimo. Isto será provocado pelo sensor, chave, \ncapacitor ou componente que deverá conectar o pino ao negativo, quando sua resistência cair a ponto \nde fazer a tensão sobre ele cair abaixo do limite de 1/3 de Vcc.  \nLembre-se: os pinos de entrada 2 e 4 do 555 são disparados com nível lógico BAIXO (negativo, ou \nmenos de 1/3 de VCC), e são inativos com nível lógico ALTO (positivo, mais e 2/3 de VCC). O pino 6 \n(THR) dispara com 2/3 ou mais de VCC. \nO experimento a seguir ilustra esse comportamento usando a lógica inversa (resistor de pull-down): \num sensor para disparar o pino 2 quando estiver escuro. Um botão é usado para reiniciar o ciclo.  \nExperimento 17 – Disparador acionado por pouca luz \nMaterial necessário: \n• \nFonte de 9 ou 12V, ou bateria de 9V \n• \nProtoboard, fios e jumpers \n• \nLDR de 5 ou 7mm \n• \nPotenciômetro de 50k/100k ou resistor de 22k Ω, 33k Ω, 47k Ω ou 100k Ω (de acordo com a \nsensibilidade desejada para o LDR) \n• \nResistor de 10k Ω \n• \nResistor de 470 Ω \n• \nLED \n• \nCapacitor cerâmico de 10nF \n• \nChave táctil tipo push-button \n• \nCircuito integrado 555 \n \n",
    "book_id": "introducaoeletronicaartistas",
    "book_title": "Microsoft Word - IntroduçãoEletronicaArtistas.docx",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 58
  },
  {
    "chunk_full": "Introdução a Eletrônica para Artistas \n \n \nc b a 2017 Helder da Rocha \n \n \n58 \nMonte o circuito acima e experimente em um lugar onde você possa variar a iluminação. Quando a luz \nestiver acesa, a resistência do LDR será baixa (menor que a resistência do potenciômetro que liga o \npino 2 ao negativo), portanto o pino 2 (TRIG) terá tensão bem maior que 3V (1/3 de 9V) e não opera.  \nQuando a luz for apagada, a resistência no LDR ficará muito alta (bem maior que os 10k ohms que \nligam o pino 2 ao negativo), fazendo cair a tensão no resistor para menos de 3V, que dispara o \ntemporizador mudando o nível do pino 3 (OUT) para nível lógico ALTO (9V), e fazendo o LED \nacender. Depois que a luz for acesa, o LED continuará aceso e só apagará se houver um reset \n(apertando o botão, que ligará o pino RESET ao polo negativo, ou seja, tensão 0V).  \nO LED neste circuito representa uma carga. Pode ser substituído por outro circuito ou dispositivo. Por \nexemplo, pode ser substituído por um relé (chave elétrica), para acionar qualquer coisa (um alarme, \num motor, etc.) \nO bom funcionamento do circuito depende da luminosidade do ambiente. Os LDRs de 5mm e 7mm \npodem ter sensibilidades diferentes à luz. Normalmente o de 7mm é mais sensível e o LED só \nacenderá com uma escuridão maior (neste caso, precisará de um resistor menor). O desenho no \nprotoboard abaixo usa um resistor de 47k no lugar do potenciômetro (substitua por um \npotenciômetro de 50k ou 100k para ajustar a sensibilidade se necessário, ou experimente diferentes \nresistores entre 10 e 100k).  \n \n4.1.2. 555 em modo monoestável \n \n",
    "book_id": "introducaoeletronicaartistas",
    "book_title": "Microsoft Word - IntroduçãoEletronicaArtistas.docx",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 59
  },
  {
    "chunk_full": "Introdução a Eletrônica para Artistas \n \n \nc b a 2017 Helder da Rocha \n \n \n59 \nO circuito monoestável encerra o ciclo automaticamente algum tempo depois do disparo. \nFunciona como um temporizador ou cronômetro. A duração desse ciclo é determinada pelos valores \ndo resistor R e do capacitor C. \nNeste circuito, o pino 4 (Reset) é mantido inativo, sempre ligado diretamente em Vcc (8). Os pinos 6 \n(Threshold) e 7 (Descarga) são ligados um ao outro (e têm sempre o mesmo valor). \nO pino 2 (disparador) inicialmente também é ligado ao positivo, mas quando conectado ao \nnegativo, ele dispara e inicia um novo ciclo (fazendo o pino 3 ter valor ALTO).  \nO fim do ciclo acontece automaticamente depois de um determinado tempo provocado pela carga do \ncapacitor, que está ligado aos pinos 6 e 7. O valor de R é calculado para controlar o tempo de carga \ndo capacitor C. Quando a carga do capacitor atingir 2/3 da tensão de referência (6V = 2/3 de 9V), o \npino 6 dispara, causando o fim do ciclo (faz o pino 3 ter nível lógico BAIXO, ou seja, 0V). Na sequência, \no pino 7 descarrega o capacitor. \nA fórmula (aproximada) para calcular o tempo (em segundos) em que a saída OUT permanecerá \nligada é: \nT = 1.1 x R x C \nExperimento 18 – Temporizador \nMaterial necessário: \n• \nFonte de 9 ou 12V, ou bateria de 9V \n• \nProtoboard, fios e jumpers \n• \nCapacitor eletrolítico de 220µF \n• \nResistor de 33k Ω \n• \nResistor de 10k Ω \n• \nResistor de 470 Ω (ou 560 Ω, se fonte de 12V) \n• \nLED \n• \nCapacitor cerâmico de 10nF \n• \nChave táctil tipo push-button \n• \nCircuito integrado 555 \nO circuito abaixo acende um LED por aproximadamente 8 segundos depois que o botão é apertado. \n \nA duração foi calculada usando a fórmula acima: 33000 x 0,00022 x 1,1 = 7,26s. Altere os valores de \nR e C para obter tempos diferentes. \nA ilustração abaixo mostra como o circuito poderia ser montado em um protoboard: \n",
    "book_id": "introducaoeletronicaartistas",
    "book_title": "Microsoft Word - IntroduçãoEletronicaArtistas.docx",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 60
  },
  {
    "chunk_full": "Introdução a Eletrônica para Artistas \n \n \nc b a 2017 Helder da Rocha \n \n \n60 \n \nVeja duas alterações desse circuito a seguir. \nAlteração 18.1 – Usando um sensor sonoro para disparar o temporizador \nMaterial adicional: \n• \nMini-microfone de eletreto \n• \nResistor de 3,3k Ω \n• \nCapacitor de 1µF \nO pulso que dispara o temporizador acontece quando o pino 2 (disparador) estiver em nível lógico \nBAIXO (tensão zero, ou negativa). O acionamento também pode ser causado por um sensor, como um \nLDR (luz), um termistor (temperatura), ou qualquer outro dispositivo ou circuito que abaixe a tensão \nno pino 2 até um valor próximo de zero.  \nNo exemplo abaixo, substituímos o botão por um circuito que amplifica o sinal recebido por um \nmicrofone de eletreto. O resistor de 10k mantém ALTO (positivo) o nível lógico do pino 2, mas \nquando som é captado pelo microfone, um sinal é enviado através do capacitor e o transistor satura, \ncomportando-se como uma chave fechada entre os seus terminais C e E, conectando a entrada do \npino 2 ao negativo (nível lógico baixo), desta forma disparando o temporizador. \n \n",
    "book_id": "introducaoeletronicaartistas",
    "book_title": "Microsoft Word - IntroduçãoEletronicaArtistas.docx",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 61
  },
  {
    "chunk_full": "Introdução a Eletrônica para Artistas \n \n \nc b a 2017 Helder da Rocha \n \n \n61 \nObserve que o microfone tem uma polaridade definida. (Pode ser necessário soldar terminais no \nmicrofone incluído no kit, se eles já não estiverem soldados, ou se forem muito pequenos. Veja o \ntutorial no final da apostila.)   \nExperimente gritar ou bater palmas perto do microfone. O som deve acionar o temporizador e fazer o \nLED acender por alguns segundos.  \nA ilustração abaixo mostra uma montagem possível usando o protoboard: \n \nO capacitor de 1µF usado na base do transistor serve para que o componente de tensão contínua que \nalimenta o microfone não passe para o transistor. O capacitor não deixa passar corrente contínua, \nmas apenas transições e pulsos. Interessa amplificar o sinal recebido pelo microfone, mas ele sempre \nvem sobreposto à tensão do microfone, que produziria corrente para o transistor ligar mesmo não \nhavendo som algum vindo do microfone. Usando o capacitor, apenas o sinal gerado pelo microfone \npassa para a base do transistor. Esse tipo de circuito é usado em muitas situações onde é preciso \namplificar sinais e eliminar a componente contínua: \n \n",
    "book_id": "introducaoeletronicaartistas",
    "book_title": "Microsoft Word - IntroduçãoEletronicaArtistas.docx",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 62
  },
  {
    "chunk_full": "Introdução a Eletrônica para Artistas \n \n \nc b a 2017 Helder da Rocha \n \n \n62 \nAlteração 18.2 – Substituindo o LED por um relé  \nMaterial adicional: \n• \nRelé de 5V, dois polos, duas posições \n• \nDiodo de propósito geral (1N4148 ou equivalente) \n• \nCigarra de 5V \n• \nResistor de 100 Ω (ou 220 Ω se fonte de 12V) \nFinalmente o LED pode ser substituído por qualquer outro circuito. No exemplo abaixo, substituímos \no LED por um relé de duas chaves e duas posições.  \nUm relé é uma chave acionada por um eletroímã e está descrito na referência no final da apostila. Ele \ntem oito terminais e deve ser posicionado no meio do protoboard, como os circuitos integrados. \nInternamente ele possui a seguinte estrutura: \n \nOs pinos 1 e 2 são os terminais do eletroímã. O relé contém duas chaves, representadas pelos pinos \n3-5-7 e 4-6-8. Os pinos 3-5 e 4-6 estão inicialmente conectados. Ao ligar o eletroímã (aplicando uma \ntensão de 4 a 10V nos terminais), os pinos 5 e 6 são desligados e a conexão muda para 3-7 e 4-8. \nNo circuito abaixo, o relé L foi colocado no lugar do LED. São usados os terminais conectados da \nprimeira chave (3-5-7) para manter um LED aceso enquanto OUT estiver em nível lógico BAIXO (e \nconsequentemente o relé estiver desligado). A segunda chave (4-6-8) conecta uma cigarra (buzzer) \naos terminais inicialmente desconectados (4-8), que será ligada durante o período em que OUT \nmudar para nível lógico ALTO (e o eletroímã do relé estiver acionado).  \n \nO diodo D é importante pois indutores em geral (relés, motores, transformadores) armazenam \ncorrente que podem fluir no sentido contrário ao ligar o circuito. Esse pulso de corrente pode ter um \nvalor muito elevado e queimar um componente. O diodo só permite a passagem de corrente em um \nsentido. Colocando-o da forma mostrada no circuito, impede-se que essa corrente reversa flua, \nprotegendo o circuito. Diodos de proteção devem sempre ser usados ao incluir relés, \ntransformadores, motores e outros indutores no circuito. \nA ilustração abaixo mostra uma possível montagem com o protoboard. Observe que a cigarra também \ntem uma polaridade. A cigarra distribuída no kit tem valor nominal de 5V e pode ser usada com \ntensões entre 3 e 8V e corrente de 30mA, por isto incluímos um resistor para reduzir a tensão e a \ncorrente que passa por ele. \n",
    "book_id": "introducaoeletronicaartistas",
    "book_title": "Microsoft Word - IntroduçãoEletronicaArtistas.docx",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 63
  },
  {
    "chunk_full": "Introdução a Eletrônica para Artistas \n \n \nc b a 2017 Helder da Rocha \n \n \n63 \n \n4.1.3. 555 em modo astável \n \nNeste modo, o 555 funciona como um oscilador gerando na saída OUT uma sequência contínua de \npulsos alternados em uma frequência determinada pelos dois resistores (R1 e R2) e capacitor (C), e \npode ser usado para várias aplicações, por exemplo, piscar LEDs, produzir tons em um alto-falante, \ncontrolar a intensidade de LEDs e velocidade de um motor. \nPode-se usar um potenciômetro no lugar de R2 para variar a frequência dos pulsos gerados.  \nO mesmo capacitor é usado para acionar os pinos 6 (THR) e 2 (TRIG). Quando o capacitor carrega via \nR1 e R2 e atinge 2/3 de Vcc, THR (6) dispara e encerra o ciclo, iniciando a descarga via R2 e DIS (pino \n7), e fazendo o pino 3(OUT) = nível lógico BAIXO. Quando a carga do capacitor cai abaixo de 1/3 de \nVcc, TRIG é acionado e OUT passa a ter nível ALTO, fechando a chave DIS e permitindo o reinício da \ncarga do capacitor.   \nA fórmula para determinar a frequência (ciclos por segundo) é \n1,44 / [ C x (R1 + 2 x R2) ] \n",
    "book_id": "introducaoeletronicaartistas",
    "book_title": "Microsoft Word - IntroduçãoEletronicaArtistas.docx",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 64
  },
  {
    "chunk_full": "Introdução a Eletrônica para Artistas \n \n \nc b a 2017 Helder da Rocha \n \n \n64 \nA duração de cada pulso depende principalmente de R2. Para R1 pode-se escolher um valor fixo (ex: \n10k Ω). Fazendo-se o valor de R2 muito maior que R1 garante pulsos positivos e negativos com \nduração semelhante (mas evite valores de R1 muito menores que 1k Ω). A fórmula para a duração de \ncada pulso é: \nALTO = 0,7 x C x (R1+R2) \nBAIXO = 0,7 x C x R2 \nPelas fórmulas, a duração do pulso de nível lógico ALTO (chamado de “ciclo de trabalho” ou “duty-\ncycle”) sempre será um pouco maior que a duração do pulso BAIXO. Para obter pulsos iguais ou \npulsos de nível logico baixo mais longos é preciso configurar o 555 de forma diferente (isto será \nmostrado mais adiante).  \nNo final da apostila estão listados alguns sites que contém calculadoras para astáveis 555, onde você \npode entrar com a frequência desejada, ciclo de trabalho desejado, e obter valores de capacitor e \nresistor. Também existem simuladores onde você pode experimentar alterar valores de capacitores e \nresistores na tela e ver a onda gerada na saída. \nExperimento 19 – Pisca-pisca com LED usando 555 \nMaterial necessário: \n• \nFonte de 9 ou 12V, ou bateria de 9V \n• \nProtoboard, fios e jumpers \n• \nResistor de 10k Ω \n• \nResistor de 680k Ω \n• \nCapacitor de 2,2µF \n• \nResistor de 470 Ω (ou 560 Ω se fonte de 12V) \n• \nLED \n• \nCapacitor cerâmico de 10nF \n• \nCircuito integrado 555 \nO circuito abaixo acende e apaga um LED por tempo determinado. Como R2 é muito maior que R1, é \npossível calcular a duração aproximada de cada pulso (ligado ou desligado) ignorando-o no cálculo e \nusando 0,7 x C x R2. \n \nA ilustração abaixo mostra uma possível montagem no protoboard: \n",
    "book_id": "introducaoeletronicaartistas",
    "book_title": "Microsoft Word - IntroduçãoEletronicaArtistas.docx",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 65
  },
  {
    "chunk_full": "Introdução a Eletrônica para Artistas \n \n \nc b a 2017 Helder da Rocha \n \n \n65 \n \nEscolhendo valores menores de R2 e C, pode-se aumentar a frequência até atingir um valor audível \n(tipicamente entre 30 a 8000 pulsos por segundo, ou Hertz). Por exemplo, para produzir uma \nfrequência de próxima de 440 Hz (frequência do diapasão usado para afinar instrumentos) pode-se \nusar um resistor de 47k Ω e um capacitor de 33nF, e ligar a saída do circuito a um alto-falante. \nExperimente usando os simuladores e calculadoras de astáveis 555 listados no final da apostila. \nExperimento 20 (extra): Mini instrumento musical com 555 \nEste experimento parece complexo, mas é apenas uma variação do anterior. A maior complexidade é \nposicionar os componentes corretamente no protoboard. \nMaterial necessário: \n• \nCircuito integrado 555 \n• \n5 chaves tácteis de pressão \n• \n1 resistor de 100k Ω \n• \n1 resistor de 33k Ω \n• \n2 resistores de 22k Ω \n• \n2 resistores de 10k Ω \n• \n2 resistores de 6,8k Ω \n• \n2 resistores de 4,7k Ω \n• \n2 resistores de 3,3k Ω \n• \n2 capacitores de 10nF \n• \n1 capacitor eletrolítico de 10µF \n• \n1 alto-falante de 8 Ω \n• \nFonte ou bateria de 9V \n• \nFios, jumpers e protoboard \nO circuito abaixo é uma versão do multivibrador astável em frequência audível, similar ao circuito \nque montamos com transistores. Mas criamos 5 trechos RC (resistor-capacitor) separados, variando o \nresistor R1, de tal forma a produzir frequências que correspondem aproximadamente a notas \nmusicais. Para acionar cada trecho é preciso apertar o botão correspondente. \n",
    "book_id": "introducaoeletronicaartistas",
    "book_title": "Microsoft Word - IntroduçãoEletronicaArtistas.docx",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 66
  },
  {
    "chunk_full": "Introdução a Eletrônica para Artistas \n \n \nc b a 2017 Helder da Rocha \n \n \n66 \nEste circuito utiliza vários resistores em série para produzir os valores necessários para obter as \nfrequências desejadas. Todos os resistores usados estão disponíveis no kit. Se você tiver outros \nresistores além dos resistores do kit, poderá usar os dados na tabela abaixo para tentar obter uma \nfrequência mais próxima da esperada, ou usar menos resistores no circuito. \n \nCada botão, quando apertado, combina os resistores abaixo dele (em série) e faz o 555 apitar em uma \nfrequência diferente, aproximadamente um tom ou semitom acima do botão anterior. As frequências \ncalculadas são Lá(440Hz), Si(494Hz), Dó (523Hz), Ré (587Hz) e Mi (659Hz). Elas foram calculadas \nusando a fórmula: \n1,44 / [ C x (R1 + 2 x R2) ] \nAs combinações R1-R2-C foram construídos de acordo com a tabela abaixo. Apenas R1 varia: \nO 555 produz na saída OUT uma onda quadrada e a fórmula usada não garante uma onda com \npulsos de duração igual (o pulso de nível lógico ALTO sempre é igual ou maior que o pulso BAIXO, e \ngeralmente é bem maior), portanto o som não é dos melhores.  \nOutra questão é a precisão dos resistores que é de 95%. Uma variação de 5% pode causar uma \ndiferença de um semitom, que é significativa para a afinação. Mesmo assim,  o som resultante deve \nficar em uma frequência próxima da esperada. Você pode tentar melhorar o circuito incluindo um \najuste fino da afinação usando potenciômetros.  \nNota \nFrequência \nesperada \nR1 (com resistores \nexistentes no kit) \nR2 \nC \nFrequência calculada \nMi (5) \n659 Hz \n19,4k \n(4,7k+4,7k+10k) \n100k \n10nF \n1,44 / (200k + 19,4k) * 10nF =  \n656 Hz \nRé (5) \n587 Hz \n44,7k \n(19,4k+22k+3,3k) \n100k \n10nF \n1,44 / (200k + 44,7k) * 10nF =  \n588 Hz \nDó (5) \n523 Hz \n73,5k \n(44,7+22k+6,8k) \n100k \n10nF \n1,44 / (200k + 73,5k) * 10nF =  \n527 Hz \nSi (4) \n494 Hz \n90,3k \n(73,5k+10k+6,8k) \n100k \n10nF \n1,44 / (200k + 90,3k) * 10nF =  \n496 Hz \nLá (4) \n440 Hz \n126,6k  \n(90,3k+33k+3,3k) \n100k \n10nF \n1,44 / (200k + 126,6k) * 10nF =  \n441 Hz \n",
    "book_id": "introducaoeletronicaartistas",
    "book_title": "Microsoft Word - IntroduçãoEletronicaArtistas.docx",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 67
  },
  {
    "chunk_full": "Introdução a Eletrônica para Artistas \n \n \nc b a 2017 Helder da Rocha \n \n \n67 \nA ilustração abaixo é uma possibilidade de montagem em protoboard: \n \n4.1.4. Controle da duração dos pulsos \nNos exemplos acima, é impossível ter um pulso de nível BAIXO que seja maior que um pulso de nível \nALTO. De acordo com as fórmulas, qualquer aumento de R2 que aumenta a duração do pulso em \nestado BAIXO, também aumenta o pulso em estado ALTO: \nALTO = 0,7 x C x (R1+R2) \nBAIXO = 0,7 x C x R2 \nO pulso em estado ALTO acontece durante a carga do capacitor, com a corrente fluindo pelos dois \nresistores. Já o pulso em estado BAIXO corresponde à descarga do capacitor através do resistor R2 \ne pino 7 (DIS). Uma forma de evitar que R2 influencie no pulso ALTO é fazer com que ele não \nparticipe da carga do capacitor, apenas da descarga. Isto é possível usando um diodo. \nUm diodo só permite a passagem de corrente em um sentido. Colocando um diodo em paralelo com o \nresistor R2, podemos deixar a corrente passar pelo resistor apenas quando ele estiver descarregando. \nDurante a carga o resistor será ignorado porque o diodo se comporta como um circuito fechado. \nO circuito abaixo ilustra essa configuração: \n \nAgora é possível calcular a duração de cada pulso usando apenas um resistor, e assim escolher suas \ndurações de forma independente: \n",
    "book_id": "introducaoeletronicaartistas",
    "book_title": "Microsoft Word - IntroduçãoEletronicaArtistas.docx",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 68
  },
  {
    "chunk_full": "Introdução a Eletrônica para Artistas \n \n \nc b a 2017 Helder da Rocha \n \n \n68 \nALTO  = 0,7 x C x R1 \nBAIXO = 0,7 x C x R2 \nExiste uma variação do circuito acima que é mais comum por permitir variar a largura dos pulsos \nmantendo R1 fixo. Essa configuração é importante para o uso do 555 com PWM – uma técnica \neficiente para simular saída analógica com o 555. Esta variação está mostrada abaixo: \n \nAs fórmulas para calcular a duração de caba pulso são: \nALTO  = 0,7 x C x (R1 + Rc) \nBAIXO = 0,7 x C x Rd \nAs fórmulas são aproximadas e não levam em conta a queda de tensão nos diodos (que é de \naproximadamente 0,7V). \n4.1.5. PWM – Pulse Width Modulation \nPWM significa modulação de largura de pulso. É uma das principais técnicas usadas para simular \numa saída analógica através de um circuito digital. Consiste em gerar pulsos digitais em alta \nfrequência, variando sua largura para produzir uma valor médio de tensão variável. Ou seja, o PWM \né um pisca-pisca que varia os tempos aceso e apagado muito rapidamente, produzindo a ilusão de \nque a intensidade está variando. \nPor exemplo, se um pulso de onda quadrada de 5V fica metade do tempo ligado, e metade do \ntempo desligado (em 0V), o valor médio simulado é de 2,5V. Aplicando um sinal desses a um LED, \nele piscará muito rapidamente ficando aceso metade do tempo, e irá aparentar ter metade do brilho \nque teria com um valor contínuo.  \nSe o pulso ficar 25% do tempo em 5V, o valor médio cai para 1,25V, e se ele ficar 75% do tempo em \n5V e apenas 25% do tempo em 0, o valor médio aumenta para 3,75V. Comparada ao uso de um \npotenciômetro, que consome energia dissipando na forma de calor, PWM é uma técnica mais eficiente \nde variar tensão e corrente, já que a carga fica desligada (consumindo zero amperes) durante o tempo \nque o pulso é BAIXO. \nCom PWM o 555 pode ser usado para implementar aplicações como controles de intensidade \nluminosa (dimmers) e controle de velocidade de motores. É bastante usado em circuitos com Arduino, \nque implementa suas saídas analógicas usando PWM.  \nNem todo circuito funciona com PWM. Dispositivos que têm limite de tensão menor que o valor \nmáximo produzido podem ser danificados com uma tensão simulada baixa (pois ela sempre alterna \nentre valores máximo e mínimo). Nessas situações é preciso construir um circuito retificador para \nconverter as ondas quadradas em um valor contínuo.  \n",
    "book_id": "introducaoeletronicaartistas",
    "book_title": "Microsoft Word - IntroduçãoEletronicaArtistas.docx",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 69
  },
  {
    "chunk_full": "Introdução a Eletrônica para Artistas \n \n \nc b a 2017 Helder da Rocha \n \n \n69 \nOs gráficos a seguir ilustram o funcionamento de PWM descrito acima. \n \nPara que o primeiro pulso possa ser menor que o segundo, é preciso usar a configuração mostrada \nanteriormente (usando dois diodos) para poder controlar isoladamente os tempos de carga e \ndescarga do capacitor.  \nNos experimentos a seguir, implementaremos PWM nessa configuração usando um potenciômetro, \nque irá permitir a variação do equilíbrio de resistências de um divisor de tensão, e desta forma variar \nos tempos de carga e descarga do capacitor, e consequentemente a largura dos pulsos na saída OUT \n(pino 3) do 555. \nExperimento 21 (extra) – Dimmer usando PWM \nMaterial necessário: \n• \nFonte ou bateria de 9V \n• \n1 resistor de 1k Ω \n• \n1 resistor e 680 Ω \n• \n1 potenciômetro de 100k Ω \n• \n2 diodos de propósito geral (1N4148 ou equivalente) \n• \n2 capacitores de 10nF \n• \n1 LED de qualquer cor \n• \n1 circuito integrado 555 \n• \nProtoboard, fios, jumpers \nO circuito abaixo controla a intensidade do brilho do LED de forma eficiente usando PWM, em vez de \ndesperdiçar energia dissipando o excesso de corrente com um resistor limitador (como fizemos no \nexperimento com LED RGB).  \nCom PWM o LED permanece desligado parte do tempo. O LED acende e apaga completamente muito \nrapidamente, portanto a redução de brilho é uma ilusão causada principalmente pela persistência da \nvisão (é o mesmo efeito que ocorre quando assistimos um filme no cinema), embora parte do efeito \ntambém se deva ao fato de que a onda quadrada não é perfeita. \n \n",
    "book_id": "introducaoeletronicaartistas",
    "book_title": "Microsoft Word - IntroduçãoEletronicaArtistas.docx",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 70
  },
  {
    "chunk_full": "Introdução a Eletrônica para Artistas \n \n \nc b a 2017 Helder da Rocha \n \n \n70 \n \nO potenciômetro varia a largura de pulso para mais ou para menos, fazendo com que o brilho \naparente do LED varie. O LED é a carga do circuito, e pode ser substituído por outro dispositivo \nanalógico ou circuito que funcione com PWM. \nA ilustração abaixo é uma possível implementação para o circuito acima. \n \n",
    "book_id": "introducaoeletronicaartistas",
    "book_title": "Microsoft Word - IntroduçãoEletronicaArtistas.docx",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 71
  },
  {
    "chunk_full": "Introdução a Eletrônica para Artistas \n \n \nc b a 2017 Helder da Rocha \n \n \n71 \nVariação 21.1 – Controle de velocidade de motor com PWM \nMaterial adicional necessário: \n• \n1 transistor MOSFET de canal N de propósito geral (2N7000 ou equivalente) \n• \n1 resistor de 47 Ω \n• \n1 diodo Zener de 3,3V e 1W (1N4728 ou equivalente) \n• \n1 diodo de propósito geral (1N4148 ou equivalente) \n• \n1 motor de 3V (opera com 1 a 6V) \nO circuito abaixo é uma adaptação do anterior. Nesta versão PWM é usado para variar a velocidade de \num motor. O circuito possui várias adaptações necessárias para lidar com algumas limitações \nimpostas pelo 555, transistor e pelo motor: \n• \nPulsos de corrente gerados pelo motor: uma bobina (indutor) armazena corrente, que é \nliberada em um pulso durante transições (ao ligar ou desligar a energia sobre ela). Um pulso \nde corrente reversa muito alta pode queimar um componente. Por este motivo usamos um \ndiodo de proteção posicionado de forma reversa e em paralelo com o relé em um \nexperimento anterior, para que esse pulso seja curto-circuitado e não afete o resto do \ncircuito.  O mesmo vale para motores, transformadores e indutores em geral. \n• \nDemanda de corrente do motor: além de pulsos elevados, a demanda de corrente do motor \npode ser maior que a suportada pelo 555 ou Arduino. Nestes casos, precisamos isolar o \ncircuito com um transistor. Pode ser um transistor bipolar, mas a opção mais eficiente neste \ncaso é usar um transistor MOSFET, que fornece um isolamento maior e quase não consome \nenergia quando desligado (ele tem uma resistência interna altíssima na entrada). \n• \nTensão máxima suportada pelo motor: o motor do kit tem uma faixa de operação que \nvaria de 1 a 6V. Deixá-lo funcionando muito tempo em 9V pode queimá-lo, portanto é preciso \ngarantir que a tensão sobre ele não passe de 6V. Não adianta limitar a tensão via PWM, pois \nela é simulada (o pulso sempre irá produzir valores máximos de 9V). Um divisor de tensão \ncom resistores também não garante essa redução, pois a resistência interna do motor varia \ndurante a operação. Uma solução para regular a tensão é usar um diodo Zener. Esse tipo de \ndiodo é construído para manter uma tensão reversa fixa sobre ele, mesmo com grandes \nvariações de corrente. No kit temos o diodo 1N4728, que mantém uma tensão fixa de 3,3V. \nColocado em série com o motor, de forma reversa, ele irá subtrair 3,3V da tensão máxima \nproduzida na saída (ou seja, do pulso máximo), garantindo que o motor nunca receba uma \ntensão maior que 6V (para uma alimentação de 9V). \nTransistores MOSFET não têm terminais base, emissor e coletor, mas Gate (G) – que tem função \nsimilar à base,  Drain (D), similar ao coletor, e Source (S), que tem um papel similar ao emissor. No \ncircuito abaixo, a saída do 555 controla o terminal G que controla a passagem de corrente de D para S \nque aciona o motor.  \nVeja mais informações e links sobre diodos Zener, transistores MOSFET e motores na referência no \nfinal da apostila. \n \n",
    "book_id": "introducaoeletronicaartistas",
    "book_title": "Microsoft Word - IntroduçãoEletronicaArtistas.docx",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 72
  },
  {
    "chunk_full": "Introdução a Eletrônica para Artistas \n \n \nc b a 2017 Helder da Rocha \n \n \n72 \nUma possível implementação do circuito acima em um protoboard está ilustrado abaixo. \n \nPWM funciona no experimento com motor devido à inércia, que impede que o motor pare \ncompletamente quando desligado. Portanto, o motor está sempre acelerando e desacelerando muito \nrapidamente, o que na prática produz uma velocidade constante reduzida. \n4.2. Outros circuitos integrados \nNo kit há dois outros circuitos integrados que funcionam com lógica digital. Os experimentos extras a \nseguir exploram o uso deles. \n4.2.1. Contador de década 4017 \nO circuito integrado 4017 é um contador de década. Basicamente ele sabe contar até 10 e dentre os \nseus 16 pinos possui 10 pinos que produzem uma saída em nível lógico ALTO ou BAIXO, dependendo \ndos controles que disparam sua contagem. Essas saídas podem ser usadas para ligar circuitos, \nacender LEDs, disparar cigarras, etc. A contagem é produzida por um pino de entrada que avança a \ncontagem através do recebimento de um pulso (que pode ser produzido por uma chave, um sensor, \num oscilador de cristal de quartzo ou gerado através de um temporizador como o 555). \nComo o 555, o 4017 também é um circuito integrado muito antigo (tem quase meio-século de \nexistência).  \nO experimento a seguir usa o 4017 para acender uma série de LEDs em sequência a cada pulso \nrecebido. Uma alteração do experimento inclui o 555 para que os pulsos sejam gerados \nperiodicamente. \nExperimento 22 (extra): sequenciador de LEDs com o 4017 \nEste experimento parece complexo apenas porque tem muitos fios, mas na verdade é muito mais \nsimples que os outros experimentos montados anteriormente. A principal complexidade é verificar \ncom cuidado as polaridades e o posicionamento dos fios e componentes. \nMaterial necessário: \n• \n1 circuito integrado 4017 \n• \n10 resistores de 470 Ω \n• \n10 LEDs \n• \n1 resistor de 10k \n• \n1 chave táctil de pressão \n• \nFonte ou bateria de 9V \n• \nFios e jumpers \n",
    "book_id": "introducaoeletronicaartistas",
    "book_title": "Microsoft Word - IntroduçãoEletronicaArtistas.docx",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 73
  },
  {
    "chunk_full": "Introdução a Eletrônica para Artistas \n \n \nc b a 2017 Helder da Rocha \n \n \n73 \nMonte o circuito abaixo. Lembre-se que os pinos são contados em sentido anti-horário a partir do \nchanfro em um dos lados, ou do ponto, que indica o pino 1. Veja o diagrama do 4017 e link para seu \ndatasheet no final desta apostila. \n \nEsta é uma possível implementação com protoboard. \n \nVocê também pode posicionar os LEDs formando um círculo, assim a contagem não acaba nunca e o \nLED fica girando para sempre. Isto pode ser feito em uma placa ou superfície qualquer, soldando (ou \namarrando) os terminais dos LEDs. \nAlteração 22.1 – Sequenciador de LEDs automático com 555 e 4017 \nEste circuito adiciona um gerador de pulsos automático com 555, para o circuito anterior.  \nMaterial adicional necessário \n• \nCircuito integrado 555 \n• \nCapacitor de 10nF \n• \nResistor de 680k Ω \n• \nCapacitor de 100nF (104) \n• \n1 circuito 4026 \nA função do 555 é apenas de produzir pulsos para a entrada do 4017. Você pode alterar os valores do \ncapacitor (de 100nF) e do resistor (de 680k) para variar a frequência e fazer o LED se mover mais ou \nmenos rapidamente. \n",
    "book_id": "introducaoeletronicaartistas",
    "book_title": "Microsoft Word - IntroduçãoEletronicaArtistas.docx",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 74
  },
  {
    "chunk_full": "Introdução a Eletrônica para Artistas \n \n \nc b a 2017 Helder da Rocha \n \n \n74 \n \nAlguns protoboards têm uma interrupção no meio dos condutores laterais. O circuito abaixo assume \nessa possibilidade e faz a ligação entre as duas metades com um fio (na faixa positiva e negativa). \n \n4.2.2. Decodificador para display de 7 segmentos 4026 \nUm decodificador é um circuito que adapta dados de entrada codificados para uma saída esperada \npor um dispositivo. A codificação pode ser uma sequência de pulsos. Assim como o 4017, o 4026 \ntambém sabe contar pulsos, mas em vez de ligar uma dentre 10 saídas, ele liga duas a sete saídas de \numa vez. Essas sete saídas correspondem aos LEDs de um display de sete segmentos, que pode ser \nusado para representar dígitos. \nExperimento 23 (extra) – Contador de 0 até 9 com display de 7 segmentos e 4026 \nEste circuito é ainda mais simples que o anterior (basicamente ligar fios a terminais), e aproveita o \ngerador de pulsos automático criado acima. \nMaterial necessário \n• \nCircuito temporizador 555 (construído na alteração do experimento anterior) \n• \n1 capacitor de 1µF (substituindo o de 100nF, para que a contagem seja mais lenta) \n• \n1 resistor de 10k Ω \n• \n1 resistor de 470 Ω \n• \nChave táctil de pressão \n• \nDisplay de 7-segmentos \n• \nCircuito integrado 4026 \n• \nProtoboard, fios e jumpers \n• \nFonte ou bateria de 9V \nMonte o circuito a seguir. Observe que a parte do 555 é idêntica ao do circuito do experimento \nanterior com exceção do capacitor de 100nF que foi trocado por um de 1µF, para que a contagem seja \nmais lenta. \n",
    "book_id": "introducaoeletronicaartistas",
    "book_title": "Microsoft Word - IntroduçãoEletronicaArtistas.docx",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 75
  },
  {
    "chunk_full": "Introdução a Eletrônica para Artistas \n \n \nc b a 2017 Helder da Rocha \n \n \n75 \n \nVeja detalhes sobre a pinagem do display e do 4026 na referência no final da apostila. Esta é uma \npossível implementação em protoboard. \n \n",
    "book_id": "introducaoeletronicaartistas",
    "book_title": "Microsoft Word - IntroduçãoEletronicaArtistas.docx",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 76
  },
  {
    "chunk_full": "Introdução a Eletrônica para Artistas \n \n \nc b a 2017 Helder da Rocha \n \n \n76 \n5. Introdução ao Arduino \nArduino é o nome de um projeto que consiste na \nespecificação de uma linguagem de programação e de \num circuito baseados em um microcontrolador, \ncomo uma plataforma eletrônica para facilitar a \nconstrução de dispositivos interativos que detectam e \ncontrolam objetos no mundo físico. É um projeto de \nfonte aberta: a especificação do hardware e software \nsão livres, permitindo que qualquer um fabrique e \nvenda placas Arduino sem pagar royalties a ninguém. \nA plataforma original foi criada em Ivrea, na Itália, por \nestudantes italianos e colombianos, com a intenção de \nfacilitar o uso da eletrônica por artistas e designers. \n5.1. Projetos artísticos com Arduino \nArduino é ideal para projetos artísticos, pois facilita muito o projeto e construção de circuitos \neletrônicos, eliminando grande parte da sua complexidade. Muitos projetos que normalmente \nrequerem o cálculo de circuitos elaborados com resistores, capacitores e transistores para usar um \nsensor, podem ser construídos usando apenas este sensor e um Arduino. É necessário, no entanto, \nprogramar o comportamento desejado em uma linguagem de computador. \nArduinos são usados em inúmeros projetos, dos mais simples aos mais complexos. Luzes que piscam \nao ritmo da música, alarmes que disparam quando percebem movimento, robôs que interagem com o \nambiente e controlam máquinas pela Internet, roupas, óculos e bolsas multimídia, instalações visuais, \nsensoriais, cinéticas e sonoras, sistemas de automação residencial, sistemas de irrigação, sistemas de \nrealidade virtual, próteses controladas por voz, jogos, drones, controladores MIDI, impressoras 3D \nsão alguns exemplos de projetos já criados com Arduino. \n \n \nNesta seção faremos uma breve introdução ao Arduino através de alguns circuitos e programas \nsimples que você pode usar como base para projetos mais sofisticados. No final da apostila estão \nlistados alguns sites com tutoriais mais detalhados sobre a linguagem de programação e a construção \nde circuitos com Arduino.  \n",
    "book_id": "introducaoeletronicaartistas",
    "book_title": "Microsoft Word - IntroduçãoEletronicaArtistas.docx",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 77
  },
  {
    "chunk_full": "Introdução a Eletrônica para Artistas \n \n \nc b a 2017 Helder da Rocha \n \n \n77 \n5.2. Arquitetura do Arduino \nO circuito do Arduino é composto de um microcontrolador programável montado em uma placa, onde \né configurado para operar e oferecer acesso seguro aos seus pinos de entrada e saída. Um \nmicrocontrolador é como um mini-computador. Ele tem memória, uma unidade central de \nprocessamento (CPU), entradas e saídas. Os programas gravados na memória de um \nmicrocontrolador controlam os sinais (correntes e tensões) enviadas e recebidas em seus pinos de \nentrada e saída, permitindo receber sinais de sensores externos e controlar dispositivos. \nAs entradas do Arduino recebem dados, que podem ser pulsos, tensões e outros sinais elétricos que \nele interpreta como dados digitais (dois estados lógicos: ligado/ALTO ou desligado/BAIXO) ou \nanalógicos (valores que variam). Os geradores desses sinais podem ser chaves, potenciômetros, \nsensores de luz, som e temperatura, outros circuitos, dispositivos conectados a redes, etc. \nAs saídas também produzem pulsos, tensões e sinais analógicos ou digitais, que podem ser usadas \npara diversas tarefas, como acender um LED, controlar um motor, ligar ou desligar um circuito, \ncontrolar um dispositivo externo, enviar um email.  \nO microcontrolador processa os dados de entrada para gerar os dados de saída. Todo o \nprocessamento é feito via software, ou seja, através de uma linguagem de programação. \n \nUm programa usa instruções para ler o estado (nível de tensão, nível lógico alto ou baixo) em pinos \nde entrada, e produzir saídas (tensão, nível lógico alto ou baixo) nos pinos de saída, que poderão \nligar, desligar ou controlar componentes e dispositivos. O programa é um arquivo de texto digitado \nem um computador, em seguida traduzido para linguagem de máquina e depois transferido para o \nmicrocontrolador (através de um circuito de comunicação que controla o acesso a pinos de \ncomunicação serial), onde será gravado na memória do chip.  \n \nUma vez gravado o programa, o microcontrolador pode ser desconectado do computador e usado em \noutro circuito para usar seus pinos de entrada/saída, ser alimentado por baterias, e funcionar de \nforma independente. \n \n",
    "book_id": "introducaoeletronicaartistas",
    "book_title": "Microsoft Word - IntroduçãoEletronicaArtistas.docx",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 78
  },
  {
    "chunk_full": "Introdução a Eletrônica para Artistas \n \n \nc b a 2017 Helder da Rocha \n \n \n78 \nO microcontrolador usado pelo Arduino é um circuito integrado pertencente à família AVR. \nMicrocontroladores AVR são populares em drones e impressoras 3D. Existem vários diferentes tipos, \ncom diferentes capacidades de processamento e memória. Nos Arduinos, os mais populares são os \nATMega e ATTiny. Eles têm diferentes formatos e tamanhos. A ilustração abaixo contém três \ncircuitos integrados AVR usados em Arduinos (ATMega328 SMD, ATMega168, ATTiny85): \n \nUm circuito mínimo do Arduino é simples e pode ser montado com um ATMega e meio-protoboard. \nBasicamente é um circuito que serve para alimentar o circuito integrado (pinos 7 e 8) e gerar os \npulsos de relógio que o microprocessador precisa para operar. O ATMega328 do esquema abaixo usa \num oscilador de cristal de quartzo nos pinos 9 e 10 para gerar 16 milhões de pulsos por segundo (16 \nmega Hertz). Este microcontrolador possui 14 pinos digitais (0 a 13) e seis analógicos (A0 a A6) que \npodem ser usados tanto como entrada ou saída (a finalidade é determinada via software). \n \nO ATMega precisa ser alimentado por 5V. O circuito acima usa um regulador de tensão L7805 que \npermite alimentar o circuito com 7 a 35V, garantindo que apenas 5V seja enviado ao ATMega. O \ndesenho abaixo ilustra um Arduino montado em protoboard. É igual ao esquema acima, mas \nacrescenta um LED entre 5V e GND que acende quando o Arduino estiver sendo alimentado. \n \n",
    "book_id": "introducaoeletronicaartistas",
    "book_title": "Microsoft Word - IntroduçãoEletronicaArtistas.docx",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 79
  },
  {
    "chunk_full": "Introdução a Eletrônica para Artistas \n \n \nc b a 2017 Helder da Rocha \n \n \n79 \nO circuito acima é apenas uma plataforma básica para operar o Arduino. Ele não faz nada. Para isto \né preciso que tenha na memória um programa contendo instruções dizendo o que deve fazer, e sejam \nconectados sensores e/ou dispositivos a serem controlados aos seus pinos de entrada e saída.  \nA transferência do programa para a memória do ATMega é feita através dos pinos RX0 (entrada) e \nTX1 (saída). Para realizar essa transferência a partir de um computador é preciso conectar esses (e \nalguns outros) pinos a um circuito adaptador USB-Serial. Este circuito pode ser comprado \nseparadamente como uma pequena placa, e às vezes já vem embutido em alguns cabos adaptadores. \nO diagrama abaixo mostra um circuito de Arduino mínimo conectado a uma pequena placa \nadaptadora USB-Serial. Através da placa ele poderá ser conectado a um computador, que também \nfornecerá a alimentação de 5V): \n \nConstruir um Arduino desta forma é bom como exercício didático para entender como funciona, mas \nnormalmente usamos placas prontas, que já contém um adaptador USB embutido e regulação de \ntensão (com saídas fixas de 3,3V e 5V), permitindo alimentar o Arduino com tensões variáveis (de 1,8 \naté 20V, dependendo do modelo). Essas placas vêm em vários tamanhos, são mais práticas, fáceis de \nusar, e às vezes até mais baratas que montar um circuito Arduino como mostrado acima. \n5.3. Placas Arduino \nExistem dezenas de diferentes tipos de placas que podem ser chamadas de “Arduino”. Todas, sejam as \noriginais italianas ou clones, são baseadas nas famílias de chips AVR ATMega/ATTiny ou similar, e \nrodam programas escritos para a plataforma Arduino.  \nO Arduino Uno é um dos mais populares, e ideal para fazer protótipos, experimentar e programar. O \nArduino Mega possui mais pinos, maior capacidade de processamento e memória e é indicado para \nprojetos maiores (ex: impressoras 3D). Existem várias placas minúsculas como o Arduino Pro Mini, \no DigiSpark ATTiny, ou o Arduino Nano (incluído no kit). Algumas não tem entrada USB (para \neconomizar espaço e energia) e precisam de um adaptador USB-Serial para que sejam programadas.  \n \nO Arduino LilyPad é a principal placa usada em eletrônica para vestir (wearables). Nessa linha \nexistem várias placas, a maioria em formato circular com pinos em forma de ilhas que podem ser \namarradas com linha de costura condutiva, e costuradas em tecido. Exemplos incluem Arduino \nGemma, AdaFruit Flora, Digispark LilyTiny.  \n \n",
    "book_id": "introducaoeletronicaartistas",
    "book_title": "Microsoft Word - IntroduçãoEletronicaArtistas.docx",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 80
  },
  {
    "chunk_full": "Introdução a Eletrônica para Artistas \n \n \nc b a 2017 Helder da Rocha \n \n \n80 \n   \n    \n    \n    \n \n5.3.1. Arduino Nano \nO Arduino que usaremos na oficina é um Arduino Nano. \nTem dimensões de 43 x 15 mm. Ele possui uma entrada \nUSB que permite a ligação direta a um computador (não \nprecisa de adaptador), e que também fornece alimentação \nde 5V enquanto estiver conectado. Depois de programado e \ndesconectado do computador, ele pode ser alimentado de \nforma independente por 7 a 12V aplicados nos pinos VIN \n(ligado ao positivo da bateria ou fonte) e GND (ligado ao negativo).  \nO Arduino Nano também possui saídas de tensão reguladas em 3,3 V (Pino 3V3) e 5V (Pino 5V). Os \npinos A0 a A7 são de entrada analógica (recebem valores entre 0 e 5V), e D0 a D13 suportam \nentrada digital (reconhecem dois valores: 0V – nível lógico BAIXO ou 5V – nível lógico ALTO). A \nsaída analógica é simulada via PWM apenas através dos pinos digitais D3, D5, D6, D9, D10 e D11. Os \noutros pinos digitais, e também os pinos A0 a A5, podem operar como saída digital. O diagrama \nabaixo ilustra a pinagem do Arduino Nano: \n \nAs especificações de corrente e tensão referem-se ao clone chinês CH340 do Arduino Nano que está \nincluído no kit, e não ao Arduino Nano original italiano (que são um pouco diferentes). \nAlgumas observações e cuidados importantes: \n• \nOs pinos do Arduino suportam no máximo 40mA (ligar em um circuito que deixa passar \nmais corrente pode queimar o pino). É necessário calcular resistores para limitar a corrente. \n",
    "book_id": "introducaoeletronicaartistas",
    "book_title": "Microsoft Word - IntroduçãoEletronicaArtistas.docx",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 81
  },
  {
    "chunk_full": "Introdução a Eletrônica para Artistas \n \n \nc b a 2017 Helder da Rocha \n \n \n81 \n• \nO Arduino inteiro fornece no máximo 200mA. Mas é possível controlar circuitos que \nconsomem bem mais corrente, desde os sinais enviados e recebidos pelos pinos sejam \nintermediados por circuitos que reduzam as correntes e tensões a níveis suportados. Isto \npode ser feito com resistores, capacitores, transistores, relés e outros dispositivos.  \n• \nTambém é necessário ter cuidado para não curto-circuitar as saídas (5V ou 3V3 ligadas \ndiretamente em GND). Os pinos analógicos e digitais podem ser ligados diretamente em 5V \nou 0V somente se forem usados como entradas. Esses valores são tratados como \ninformação (nível lógico ALTO e BAIXO) pelo Arduino. Para usá-los como saídas, é \nnecessário configurar essa funcionalidade na programação, e ter o mesmo cuidado que as \nsaídas 5V e 3V3 (não ligar diretamente em GND), além de usar resistores para manter o fluxo \nde corrente dentro do limite. \n• \nO pino AREF é usado para ajustar a tensão de referência usada para os pinos analógicos. Ela \nestá internamente conectada ao pino 5V, mas pode ser desligada via programação. Ligar uma \ntensão qualquer neste pino sem primeiro fazer essa alteração via código irá queimar o \nregulador de tensão (e provavelmente a entrada USB). \nUm programa escrito para um tipo de Arduino pode ser usado em outro tipo de Arduino. Pode-se \naproveitar programas prontos e fazer pequenas adaptações sem que seja necessário entender todo o \ncódigo. Portanto, sabendo o mínimo da programação do Arduino, você pode baixar programas da \nInternet e adaptar para seus circuitos. É preciso garantir que os números de pinos, declarados no \ncódigo dos programas, e os pinos reais, usados no circuito estejam de acordo. Em geral qualquer \npino digital ou analógico pode ser usado. Eles podem até ser reprogramados. Alguns pinos têm \ncapacidades especiais. Por exemplo, os pinos digitais 3, 5, 6, 9, 10 e 11, no Arduino Nano, permitem \ngerar saída analógica usando PWM.  \nNão se preocupe se você não entendeu tudo. São muitos conceitos e é sempre mais fácil entender com \num ou mais exemplos. Nas próximas seções mostraremos como instalar e configurar o Arduino, e \ndepois como usá-lo através de vários experimentos. Depois que você fizer os experimentos, releia \nesta seção. Vários conceitos irão ficar mais claros. \n5.4. Preparação e teste do Arduino \nComo vamos construir circuitos, e o Arduino Nano não possui soquetes onde podemos inserir \nterminais de componentes, precisamos usar o protoboard. Encaixe o Arduino com cuidado ocupando \na parte central, de forma que possamos ter acesso a todos os seus pinos através dos pinos laterais. Da \nforma mostrada abaixo, cada pino terá dois a três furos. \n \nO protoboard deve estar livre de outros circuitos (principalmente, não deve haver nenhuma fonte de \nenergia conectado a ele).  \nDepois de encaixado o Arduino, encaixe uma das pontas do cabo USB no Arduino, e a outra em alguma \nsaída USB do seu computador. O LED PWR do Arduino deverá acender, indicando que ele está sendo \nalimentado pela porta USB. Para haver comunicação, no entanto, é preciso instalar o driver. \n",
    "book_id": "introducaoeletronicaartistas",
    "book_title": "Microsoft Word - IntroduçãoEletronicaArtistas.docx",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 82
  },
  {
    "chunk_full": "Introdução a Eletrônica para Artistas \n \n \nc b a 2017 Helder da Rocha \n \n \n82 \n5.5. Instalação do ambiente de desenvolvimento \nPara habilitar um computador para programar o Arduino Nano do kit são necessárias duas etapas: \n1. Instalar o driver (programa que permite a comunicação com o Arduino via porta USB do \ncomputador) do adaptador USB-Serial (embutido no Arduino). \n2. Instalar o programa com o ambiente de programação (Arduino IDE). \nA IDE (aplicação com ambiente gráfico para programação) é distribuída pelo site oficial do Arduino \n(arduino.cc) e existe para Mac, Windows e Linux. Roda de maneira praticamente idêntica nas três \nplataformas.  \nO driver é mais complicado de instalar, e pode variar dependendo do Arduino usado, se é um clone \nou se é um autêntico italiano. O Arduino original (italiano) não requer a instalação de drivers no Mac, \nmas a instalação ainda pode ser necessária em algumas versões de Windows.  \n5.5.1. Instalação do driver \nO Arduino Nano incluído no kit é um clone e usa um adaptador USB-Serial chinês (chip CH341). \nPara que ele seja reconhecido pelo computador, seja Mac, PC ou Linux, ele precisa ter o driver \ninstalado antes. O driver é um programa de instalação que deve ser baixado do site do fabricante e \nexecutado. Ele não faz nada além disso. A instalação termina depois que o computador for reiniciado. \nVeja as instruções abaixo. Elas podem ser diferentes dependendo do sistema que você estiver usando. \nWindows \nSe você usa Windows 10 baixe o arquivo EXE disponível em  \nhttp://www.wch.cn/download/CH341SER_EXE.html  \n(clique no botão de Download), execute-o. Você deve ter as permissões para executar este programa \nno computador, pois ele vai gravar arquivos do sistema. Siga o passo-a-passo (em inglês). Depois é \nnecessário reiniciar o computador para completar a instalação. Quando terminar e reiniciar, pule para \na seção seguinte (IDE) para instalar o ambiente de programação. \nMac \nSe usa Mac OS Sierra (10.12), baixe o arquivo ZIP em  \nhttp://www.wch.cn/download/CH341SER_MAC_ZIP.html  \n(clique no botão de Download) e abra o ZIP. Dentro dele há um arquivo CH34x_Install_V1.4.pkg. \nExecute esse arquivo e siga as instruções (em inglês). Depois é necessário reiniciar o computador \npara completar a instalação. Quando reiniciar, pule para a seção seguinte (IDE) para instalar o \nambiente de programação. \nLinux \nE se você usa Linux baixe o arquivo localizado em \nhttp://www.wch.cn/download/CH341SER_LINUX_ZIP.html  \n(clique no botão de Download). Abra o ZIP em uma pasta. Abra uma janela do terminal e execute as \nlinhas abaixo: \nsudo make \nsudo make load \nDepois siga para a seção seguinte. \n5.5.2. Instalação do ambiente de programação (IDE) \nA programação do Arduino é feito na linguagem Processing, que é baseada na linguagem C e similar a \nlinguagens de programação populares como C# e Java. Embora possam ser escritos programas \nbastante complexos usando essa linguagem, é possível fazer muita coisa escrevendo programas bem \nsimples e fáceis de entender mesmo para quem é leigo em programação. Aprendendo o mínimo, você \n",
    "book_id": "introducaoeletronicaartistas",
    "book_title": "Microsoft Word - IntroduçãoEletronicaArtistas.docx",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 83
  },
  {
    "chunk_full": "Introdução a Eletrônica para Artistas \n \n \nc b a 2017 Helder da Rocha \n \n \n83 \nconseguirá baixar programas disponíveis na Internet e adaptar para rodar com seus circuitos. Para \nisto, precisamos instalar o ambiente de desenvolvimento integrado (IDE – Integrated Development \nEnvironment) do Arduino. Baixe o programa de instalação para o seu sistema operacional (Windows, \nMac ou Linux) na página \n   https://www.arduino.cc/en/Main/Software  \nExecute o instalador e siga o passo-a-passo. Depois rode o programa. Ele deverá abrir a janela abaixo: \n \n5.5.3. Comunicação do Arduino com o computador \nDepois de instalados o driver e o IDE, é preciso ainda selecionar a placa usada e a identificar a \nporta de comunicação onde ela está conectada. Isto só precisa ser feito uma vez para cada placa \ndiferente que você usar, mas requer que o Arduino esteja conectado. Portanto, se você ainda não \nconectou o Arduino a uma porta USB do seu computador, faça isto agora. \nSelecione no menu Ferramentas (Tools), na opção Placa (Board). Na lista há várias placas. Selecione \nArduino Nano. \n \nDepois selecione a porta de comunicação. No Windows deve ser algo como COM4. No Linux e Mac, um \ncaminho que inicia com /dev/cu.wchusbserial (ex: /dev/cu.wchusbserial123456). \n \n",
    "book_id": "introducaoeletronicaartistas",
    "book_title": "Microsoft Word - IntroduçãoEletronicaArtistas.docx",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 84
  },
  {
    "chunk_full": "Introdução a Eletrônica para Artistas \n \n \nc b a 2017 Helder da Rocha \n \n \n84 \nSe você usar outro tipo de Arduino posteriormente, terá que modificar esses parâmetros ou a \ntransferência do programa não será possível (o programa apresentará mensagens de erro \ninformando isto). Alguns clones de Arduino são identificados diferentemente (ex: alguns clones \nchineses de LilyPad são identificados como Arduino Uno) e outros requerem a instalação de \nbibliotecas externas (que podem ser baixadas) para funcionar. \n5.6. Programação do Arduino: fundamentos \nPara fazer qualquer circuito com o Arduino, é preciso primeiro programá-lo. Este é o objetivo desta \nseção onde introduziremos, de forma prática, sua linguagem de programação.  \nPara que esta introdução seja curta, objetiva e mais simples possível, omitiremos detalhes e \nabordaremos conceitos de maneira incompleta e às vezes até imprecisa, para focar apenas no \nessencial necessário para entender os programas que serão construídos.  \n5.6.1. Estrutura básica de um sketch \nUm programa Arduino é chamado de Sketch. Ele consiste de uma sequência de instruções escritas em \numa linguagem chamada Processing. Para ser usado o programa depois precisa ser compilado \n(traduzido para linguagem de máquina) e transferido para o Arduino. Um sketch é também um \narquivo de texto que pode ser gravado no computador, e possui a extensão .ino.  O menor sketch \ncontém no mínimo a seguinte estrutura (que não faz nada): \nvoid setup() { \n \n} \n \nvoid loop() { \n \n} \nSe você está usando um Arduino pela primeira vez, e não sabe que programa está em sua memória, é \numa boa prática transferir o programa acima para ele. Isto garante que ele não executará nenhuma \ntarefa que possa danificá-lo. \nA estrutura acima possui dois blocos, que podemos chamar de bloco setup() e bloco loop(). A \ninstrução void setup() define o bloco setup(), e a instrução void loop() define o bloco loop(). Essas \ninstruções são chamadas automaticamente quando o Arduino estiver executando, e todas as \ninstruções que forem digitadas entre as chaves { } serão executadas. \nNo programa acima, as chaves estão vazias, portanto quando o Arduino chamar setup() e loop(), ele \nnão vai fazer nada.  \nOs blocos setup() e loop() funcionam de forma distinta. O bloco setup() é chamado uma vez só, \nportanto ele deve conter instruções que serão executadas uma única vez.  Já o bloco loop() é \nchamado eternamente, e deve conter instruções que repetem para sempre (até que o Arduino seja \ndesligado ou reiniciado).  \nNormalmente dentro de setup() serão colocadas instruções de configuração (por exemplo, \nespecificar a função que um determinado pino irá assumir – se entrada ou saída). Em loop() ficam as \ninstruções que efetivamente programam o Arduino, por exemplo, mandar nível lógico alto (5V) \npara pino 4, esperar meio segundo, e depois mandar nível lógico baixo (0V), e repetir isto sem parar. \nAntes de programar qualquer coisa, vamos testar a transferência de programas para o Arduino.  \nDigite o programa acima. Para transferir para o Arduino, clique no ícone  \n (ou selecione o menu \nSketch/Upload). Se houver erro, ele aparecerá na caixinha de status na parte inferior do programa, e \na transferência não acontecerá. Um erro comum é esquecer de selecionar o modelo de Arduino e sua \nporta de comunicação (veja a seção 5.5.3). Verifique também se não esqueceu de fechar alguma chave, \nou se digitou algo diferente do que foi listado. Os comandos precisam ser escritos exatamente como \nacima (letras maiúsculas e minúsculas são consideradas diferentes na linguagem do Arduino (ex: \nescrever LOOP() ou Loop() é um erro). \n",
    "book_id": "introducaoeletronicaartistas",
    "book_title": "Microsoft Word - IntroduçãoEletronicaArtistas.docx",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 85
  },
  {
    "chunk_full": "Introdução a Eletrônica para Artistas \n \n \nc b a 2017 Helder da Rocha \n \n \n85 \n5.6.2. Sintaxe das instruções \nAs instruções usadas dentro dos blocos setup() e loop() têm uma sintaxe bem definida. Existem vários \ntipos. Usaremos principalmente instruções de uma linha. Essas instruções sempre terminam em \nponto-e-vírgula e podem ser classificadas como comandos (que mandam o Arduino fazer alguma \ncoisa) ou expressões (que calculam valores, guardam dados, etc.).  \nComandos \nComandos são formalmente chamados de funções ou métodos. Eles fazem parte de uma biblioteca \nque define seus nomes e parâmetros. Os parâmetros são valores passados entre parênteses, às vezes \nentre aspas, e separados por vírgulas depois do nome da função. Há comandos que não têm \nparâmetros (apenas os parênteses vazios). Por exemplo, o comando: \ndelay(500); \nmanda o Arduino esperar meio segundo (500 milissegundos). Há apenas um parâmetro que é o \nnúmero de milissegundos a esperar. Este outro comando: \nanalogWrite(9, 128); \nmanda o Arduino produzir no seu pino digital 9 um sinal analógico de nível 128 (o nível varia de 0 a \n255, e corresponde a valores médios (simulados) de 0 a 5V produzidos com PWM). A instrução, \nportanto, produz um pulso ligado 50% do tempo que resulta em um valor médio de 2,5V no pino 9. \nObserve que as instruções terminam sempre em ponto-e-vírgula. Observe também que o “W” em \nanalogWrite é maiúsculo (e assim deve ser escrito). \nExpressões e variáveis \nPode-se escrever um programa apenas com comandos, mas alguns comandos retornam resultados \nque precisam ser processados. O processamento é feito através de expressões. Existem vários tipos \nde expressões: aritméticas, lógicas, etc. Expressões frequentemente são formadas por operações. Por \nexemplo, esta é uma expressão contendo uma operação de soma e uma operação de atribuição: \nnumero = 3 + 4; \nO Arduino irá somar 3 com 4 e guardar o resultado na variável numero. O sinal de = é usado para \nfazer uma operação de atribuição, isto é, copiar um valor (o resultado da expressão) para uma área \nda memória associada à variável. Variáveis são palavras usadas para identificar, guardar e referenciar \ndados. Elas só guardam dados de um tipo de dados específico. A variável acima precisa declarar o \ntipo de dados que pode armazenar, antes que seja usada. A declaração é também uma expressão. \nEntão algum lugar antes da linha acima, deve haver algo como: \nint numero; \ndeclarando que a variável numero é do tipo int. A palavra int significa inteiro, e é usada para declarar \nvariáveis que só aceitam valores inteiros. Não seria possível, por exemplo, guardar um 3.14 na \nvariável numero. Para isto ela teria que ser declarada como float, que é o nome usado para variáveis \ncom parte decimal. Observe que a declaração da variável também termina em ponto-e-vírgula. \nNos programas em Arduino que faremos nesta introdução, declararemos apenas variáveis do tipo int \ne float. Muitas vezes, a declaração e a atribuição ocorrem na mesma linha, por exemplo: \nint pino = 6;  \nfloat valor = 3.14; \nDepois de declarada uma variável, provavelmente vamos querer usá-la depois. O uso de uma variável \npode ser, por exemplo, a inclusão do valor que ela contém em alguma outra expressão ou comando: \nfloat raio = 9.5; \nfloat area = valor * raio * raio; \nint tempo = 1000; \ndelay( tempo ); \nA última linha acima é um comando que está usando a variável tempo, que contém o valor inteiro \n1000, que é passado como parâmetro da função delay(). \n",
    "book_id": "introducaoeletronicaartistas",
    "book_title": "Microsoft Word - IntroduçãoEletronicaArtistas.docx",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 86
  },
  {
    "chunk_full": "Introdução a Eletrônica para Artistas \n \n \nc b a 2017 Helder da Rocha \n \n \n86 \nAlguns comandos produzem um valor, que geralmente é resultado do processamento executado por \neles. Esse valor geralmente é guardado em uma variável. Por exemplo: \nint duracao = analogRead(2); \nO comando analogRead(2) é executado, e seu resultado é copiado (via operação de atribuição) para a \nvariável duracao. Depois, este valor pode ser usado em outro comando, por exemplo: \ndelay(duracao); \nA instrução analogRead(2) produz um valor (entre 0 e 1023) resultante da leitura do nível da \ntensão no pino A2. Por exemplo, se houver um potenciômetro com os pinos externos ligados entre 0 \ne 5V, e o pino do meio estiver ligado no A2 do Arduino, e este potenciômetro estiver com o seletor \nposicionado exatamente no meio, o valor recebido por analogRead(2) será 1024/2 ou 512. \nNomes de variáveis \nVariáveis não podem ter qualquer nome. Não crie nomes com acentos, hífens, pontos. O ideal é usar \nnomes curtos e explicativos. Se você quer criar uma variável com mais de uma palavra, você pode \ndistinguir as palavras usando maiúsculas, por exemplo: \nint numeroDoPino = 6; \nou sublinhados: \nint NUMERO_DO_PINO = 6; \nVariáveis também não podem usar certas palavras, que são reservadas. Exemplos são as palavras int \ne float, que têm significado especial para o Arduino. É fácil saber quando uma palavra é reservada, \npois ela aparece com uma cor diferente (azulada) no IDE.  \nComentários \nNem tudo o que está escrito em um sketch é enviado para o Arduino. Para que os programas sejam \nmais fáceis de entender pelos humanos que irão lê-lo, é comum que tenham comentários. Os \ncomentários são texto ignorado pelo compilador (mecanismo da IDE que traduz o programa para \nlinguagem de máquina) e devem ser usados para explicar trechos do programa, ou incluir instruções \nde como usá-los. Há dois tipos: comentários de linha e comentários de bloco. \nComentários de linha geralmente aparecem antes de instruções, ou logo depois do ponto-e-vírgula, \nna mesma linha que a instrução. Tudo o que aparece depois do // é considerado um comentário. Por \nexemplo: \nint PINO_DO_LED = 13;   // este é o pino do LED interno \nOutra forma de escrever comentários no programa é usar comentários de bloco, que consiste em \nincluir o texto de uma ou mais linhas entre /* e */. Use esse tipo de comentário se o que você \npretende escrever tem muitas linhas: \n/*  \n   Este programa faz um LED piscar. \n   Ligue o Anodo do LED no pino 6. \n   Ligue o Catodo em um resistor de 470 ohms, ligado a GND.  \n*/ \nvoid setup() { ... } \nComentários também são usados para temporariamente ignorar um trecho de código (que você não \nquer que execute, mas não quer apagar do sketch): \nvoid loop() { \n    // delay(500); \n    delay(1000);    // a linha anterior será ignorada \n} \nIsto é suficiente como uma introdução à linguagem do Arduino. Com o que vimos até aqui já é possível \nfazer um primeiro programa para piscar um LED. \n",
    "book_id": "introducaoeletronicaartistas",
    "book_title": "Microsoft Word - IntroduçãoEletronicaArtistas.docx",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 87
  },
  {
    "chunk_full": "Introdução a Eletrônica para Artistas \n \n \nc b a 2017 Helder da Rocha \n \n \n87 \nExperimento 24 – Piscando um LED \nMaterial necessário: \n• \nArduino Nano + cabo USB + computador \n• \nUm LED de qualquer cor \n• \nResistor de 220 ohms \n• \nProtoboard, fios e jumpers \nPara piscar um LED, é preciso liga-lo em uma saída que alterne entre dois níveis lógicos: alto e baixo, \ncom um intervalo entre eles. Como precisamos de apenas dois estados, podemos usar um pino de \nsaída digital. Há 14 deles. Podemos usar qualquer um. Vamos escolher o pino digital 8, identificado \nna placa do Arduino Nano com a indicação D8. \nO nível lógico ALTO no Arduino é sempre 5 volts. Precisamos de um resistor para limitar a corrente \ndo LED e temos informações suficientes para calcular seu valor. Se for um LED vermelho, com 2V de \nqueda de tensão: \nR = (5V – 2V) / 0,02 A = 150 ohms. \nNão temos 150 ohms no kit, mas podemos usar 220 ohms que consome um pouco menos corrente, ou \naté mesmo arriscar um valor menor (100 ohms) já que ele não vai ficar ligado muito tempo. Podemos \ntambém usar um resistor de 100 ohms e trocar o LED vermelho por um de 3V (azul, rosa ou branco).  \nAntes de montar qualquer circuito, sempre desligue o Arduino do computador (desconecte o cabo \nUSB). Monte o circuito abaixo, verifique as conexões, e depois ligue novamente o Arduino ao \ncomputador.  \nA porta USB é quem irá fornecer corrente para o circuito. Se houver um problema no seu circuito e ele \ntentar puxar corrente demais da porta USB, o computador desligará o acesso e desligará o Arduino \n(você terá que remover o cabo e reinserir novamente, depois de corrigir o problema). \n              \n \nO resistor ligado ao catodo do LED pode ser conectado a qualquer um dos dois pinos GND disponíveis \nno Arduino Nano (há um de cada lado). Internamente eles estão ligados entre si. \nAgora vamos escrever um programa para piscar o LED. Abra um novo sketch (ícone \n ou Menu \nFile/New) e preencha os blocos setup() e loop() com as seguintes instruções: \nvoid setup() \n    pinMode(8, OUTPUT);     // declara pino 8 como uma saída \n} \n \nvoid loop() { \n    digitalWrite(8, HIGH); \n    delay(500); \n    digitalWrite(8, LOW); \n    delay(500); \n} \n",
    "book_id": "introducaoeletronicaartistas",
    "book_title": "Microsoft Word - IntroduçãoEletronicaArtistas.docx",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 88
  },
  {
    "chunk_full": "Introdução a Eletrônica para Artistas \n \n \nc b a 2017 Helder da Rocha \n \n \n88 \nClique no ícone \n para transferir o programa para o Arduino. Em alguns segundos o LED deverá \ncomeçar a piscar ficando meio segundo aceso e meio segundo apagado. Experimente mudar o valor \nde delay() para que ele pisque mais rápido, ou que fique mais tempo aceso que apagado. \nExplicaremos os comandos usados no programa nas seções a seguir. \n5.6.3. Pinos digitais e estados HIGH e LOW \nUm pino digital permite a entrada e saída de valores digitais correspondentes aos níveis lógicos \nligado (5V), ou ALTO, e desligado (0V), ou BAIXO. Esses dois estados são representados na \nlinguagem do Arduino pelas palavras reservadas HIGH (sempre em maiúsculas) e LOW (idem). No \ncontexto do circuito, correspondem aos valores de tensão 5V e GND (0V). \n(Na verdade HIGH e LOW são variáveis pré-definidas que, no contexto do programa, contém os \nnúmeros inteiros 1 e 0, respectivamente.) \nUm componente de dois terminais ligado a um pino de saída deve ter o seu outro terminal ligado a \numa referência de tensão: o pino GND (negativo) ou o pino 5V (positivo). Haverá corrente se houver \ndiferença de potencial entre o pino de saída e a referência. Isto significa que, para que haja corrente \nfluindo por um componente, se ele estiver conectado a um pino que é acionado pelo valor HIGH, o \noutro terminal deve estar conectado a GND (é assim que o LED está configurado no nosso exemplo). \nSe o componente for acionado pelo valor LOW, o outro terminal deve estar conectado a 5V.  \nÉ importante observar a polaridade do componente e posicioná-lo de acordo, e também limitar a \ncorrente. Um pino e saída do Arduino não suporta mais que 40 mA. Ligar um pino de saída \ndiretamente a 5V ou GND sem resistor limitador gera uma corrente muito alta que poderá queimá-lo \nquando houver uma diferença de potencial no pino. \nA instrução pinMode() \nNormalmente os pinos operam como entrada. Para usar um pino como saída digital é preciso \nexecutar uma instrução para declará-lo explicitamente. Isto normalmente é feito dentro do bloco \nsetup() através da instrução pinMode(número-do-pino, função) (observe o “M” maiúsculo da \ninstrução). Os dois parâmetros informam respectivamente o número do pino e o tipo de função que \nele vai exercer (OUTPUT, para a função saída): \nvoid setup() \n    pinMode(8, OUTPUT);     // pino 8* é uma saída \n} \n* Na placa do Arduino Nano os pinos digitais são identificados pelo prefixo D (D0, D1, D2, D3, etc.) e \nos analógicos pelo prefixo A (A0, A1, A2, A3, etc.) No programa, apenas os números dos pinos \ndigitais são usados (ex: 0, 1, 2, 3, etc.) Os pinos analógicos podem ser identificados com ou sem \nprefixo nos comandos que aceitam entradas analógicas. \n5.6.4. Saída digital \nPara produzir uma saída digital em níveis lógicos (HIGH/5V ou LOW/0V, sem valores intermediários) \nusa-se a instrução digitalWrite(número-do-pino, nível-lógico), dentro de setup() (para rodar \napenas uma vez) ou loop() (para rodar repetidas vezes). O número do pino precisa ter sido \npreviamente declarado como OUTPUT através da instrução pinMode().  \nPor exemplo: \ndigitalWrite(8, HIGH); // aplica o valor HIGH (5 volts) no pino 8 \nO comando acima transfere 5V (HIGH) para a saída digital D8. Se no pino D8 houver um LED \n(alimentado entre o pino 8 e GND), ele irá receber 5V, e acender. \nPara o LED piscar é preciso fazer o pino 8 ter valor HIGH, depois esperar algum tempo (mantendo o \npino neste estado) e em seguida fazer o pino 8 ter valor LOW, esperar mais algum tempo (em que o \nLED ficará apagado) e repetir a sequência. A repetição acontece automaticamente para instruções \ndigitadas dentro do bloco loop(), portanto para piscar o LED serão necessárias apenas quatro \ninstruções: \n",
    "book_id": "introducaoeletronicaartistas",
    "book_title": "Microsoft Word - IntroduçãoEletronicaArtistas.docx",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 89
  },
  {
    "chunk_full": "Introdução a Eletrônica para Artistas \n \n \nc b a 2017 Helder da Rocha \n \n \n89 \nvoid loop() { \n    digitalWrite(8, HIGH);    // aplica 5V no LED+resistor \n    delay(500);             \n// mantém em 5V por 0,5 segundos \n    digitalWrite(8, LOW); \n// aplica 0V no LED+resistor \n    delay(500);  \n \n// mantém em 0V por 0,5 segundos \n}  \n// repete tudo ad infinitum \n5.6.5. Variáveis globais e #define \nQuando se tem um programa que usa apenas um ou dois pinos, é fácil lembrar o que está conectado a \ncada um, mas se muitos pinos estiverem sendo usados o programa pode tornar-se difícil de ler e \nentender. E se for necessário mudar um componente para outro pino? O número teria que ser \nalterado em todos os lugares onde foi digitado. Uma solução para este problema é declarar uma \nvariável para identificar o pino:  \nvoid loop() { \n    int PINO_DO_LED = 8;    // Declarando variável PINO_DO_LED contendo 8 \n    digitalWrite(PINO_DO_LED, HIGH); // o primeiro parâmetro recebe 8 \n    delay(500); \n    digitalWrite(PINO_DO_LED, LOW); \n    delay(500); \n} \nVariáveis globais \nUma variável declarada dentro das chaves { ... } de um bloco (ex: setup() ou loop()) é acessível \napenas dentro daquele mesmo bloco. Quando o bloco terminar, ela não poderá mais ser usada \n(causará erro no programa). Mas às vezes criamos uma variável exatamente para poder usá-la em \nblocos diferentes. Por isto é comum que a declaração de algumas variáveis ocorra fora dos blocos \nloop() e setup(). Variáveis declaradas fora dos blocos são chamadas de variáveis globais, porque \nelas podem ser usadas em qualquer um dos dois blocos.  \nNo exemplo abaixo, criamos uma variável global para guardar o pino do LED:. \nint LED = 8; // declarada fora de setup() ou loop() – é global \n \nvoid setup() \n    pinMode(LED, OUTPUT);     // reconhecida dentro de setup() – recebe 8 \n} \n \nvoid loop() { \n    digitalWrite(LED, HIGH);  // reconhecida dentro de loop() – recebe 8 \n    delay(500); \n    digitalWrite(LED, LOW); \n    delay(500); \n} \nOutra forma de declarar uma variável global para um pino \nVocê encontrará alguns programas que declaram variáveis usando o comando #define antes dos \nblocos setup() e loop(). Por exemplo: \n#define LED 8 \nA sintaxe é diferente de uma declaração de variável comum. Não existe o sinal de igual (=) e nem \nponto-e-vírgula (não pode ter ponto-e-vírgula). Na prática o resultado é o mesmo. Usar esta forma \nou a outra é uma questão de estilo. Não vai alterar o funcionamento do programa. Mesmo que você \nescolha usar apenas a outra forma, é importante reconhecer essa sintaxe, pois muitos programadores \npreferem usar #define em vez de declarar variáveis globais.  \nAs declarações #define geralmente aparecem no início do programa. Elas não podem aparecer \ndentro dos blocos setup() ou loop(). \n",
    "book_id": "introducaoeletronicaartistas",
    "book_title": "Microsoft Word - IntroduçãoEletronicaArtistas.docx",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 90
  },
  {
    "chunk_full": "Introdução a Eletrônica para Artistas \n \n \nc b a 2017 Helder da Rocha \n \n \n90 \nAlteração 24.1 – Usando variáveis \nAltere o programa do último experimento substituindo o número do pino por uma variável, e faça o \nupload novamente. Veja que o funcionamento não muda. Agora mude a posição do LED para o pino 7 \nno protoboard. Ele não pisca mais, mas você pode abrir o programa, fazer apenas uma alteração \n(mudando a variável LED para 7) e transferi-lo novamente, que ele voltará a funcionar.  \nExperimento 25 – Reagindo ao acionamento de chaves liga-desliga \nMaterial necessário: \n• \nArduino Nano + cabo USB + computador \n• \nUm LED de qualquer cor \n• \nResistor de 220 ohms \n• \n1 chave táctil de pressão \n• \nProtoboard, fios e jumpers \nEm um circuito Arduino, chaves não são usadas para ligar ou desligar um componente diretamente \n(como em circuitos eletrônicos tradicionais) mas para fornecer um dado de entrada para o \nprograma, que poderá usá-lo para tomar decisões (a decisão pode ser inclusive para ligar ou desligar \no componente.) Portanto, uma chave deve ser ligada diretamente a um pino digital de entrada. \nNão é preciso chamar a instrução pinMode() no setup para configurar entradas (se for usada, deve \ndeclarar a funcionalidade do pino como INPUT.) Nesse modo, quando a chave estiver fechada ela \ndeverá fornecer ou 5V ou 0V para o pino. Quando aberta, o estado do pino é indefinido. \nMonte o circuito abaixo (é o mesmo circuito do experimento anterior, acrescentando a chave): \n    \n \nLigamos uma chave de pressão (normalmente aberta) entre o pino 3 e GND. Quando ela não estiver \npressionada, não haverá sinal algum no pino 3 (estado indefinido), mas quando ela estiver apertada, \nela fará a conexão entre o pino D3 e GND e seu estado será LOW. Usamos uma expressão \ncondicional para testar o estado do pino, com a seguinte regra: se o estado do pino 3 for LOW, o LED \nserá aceso, caso controário (se for qualquer outro estado – indefinido ou HIGH), o LED será apagado: \n#define CHAVE 3 \n#define LED   8 \n \nvoid setup() { \n  pinMode(LED, OUTPUT); \n} \n \nvoid loop() { \n  int estado = digitalRead(CHAVE); \n  delay(10); // espera 10 milissegundos antes de testar \n  if(estado == LOW) { \n    digitalWrite(LED, HIGH); \n  } else { \n    digitalWrite(LED, LOW); \n  } \n} \n",
    "book_id": "introducaoeletronicaartistas",
    "book_title": "Microsoft Word - IntroduçãoEletronicaArtistas.docx",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 91
  },
  {
    "chunk_full": "Introdução a Eletrônica para Artistas \n \n \nc b a 2017 Helder da Rocha \n \n \n91 \nO loop repete continuamente lendo o estado da chave e usando o estado lido para comparar com o \nvalor LOW (0V).  \nSe a chave estiver aberta, o estado não é LOW. É indefinido, portanto, o conteúdo do bloco if é \nignorado, mas o bloco else é executado. A instrução dentro do else mantém o LED apagado, já que \nfornece 0V (LOW) para a saída 8.  \nSe a chave estiver apertada, ela faz uma ligação direta entre o pino 3 e GND, fazendo-o ter o estado \nLOW. Neste caso, a instrução executada muda o estado do pino 8 para HIGH, e o LED acende. Soltando \no botão, ele volta ao estado indefinido, e em pouco mais de 10 milissegundos, o estado do pino 3 será \ntestado de novo, desta vez apagando o LED. Portanto, o LED só acende enquanto o botão estiver \napertado.  \n5.6.6. Entrada digital \nA instrução digitalRead(número-do-pino) serve para ler o nível lógico de um pino de entrada. O \nvalor pode ser ALTO (HIGH) ou BAIXO (LOW). Normalmente HIGH corresponde a 5V e LOW \ncorresponde a 0V (mas na prática o Arduino irá considerar como HIGH qualquer valor de tensão de 3 \nvolts ou mais. Valores abaixo de 3V serão considerados nível lógico LOW.  \nHIGH e LOW são variáveis que guardam valores inteiros (respectivamente 1 e 0), portanto o valor lido \npor digitalRead() deve ser armazenado em uma variável declarada como int: \nint valor = digitalRead(3); \nOs pinos digitais são inicialmente configurados como entradas, portanto não é necessário usar \npinMode() para declará-los como tal. Se for usada deve conter a opção INPUT: \n    pinMode(3, INPUT);      // pino 3 é uma entrada \n5.6.7. Lógica condicional e bloco if-else \n“If” significa “se”. O bloco condicional if(condição) {} recebe entre parênteses uma expressão lógica \nbooleana, e entre as chaves uma lista de instruções que devem ser executadas somente se a \nexpressão for verdadeira.  \nExpressões lógicas podem ser igualdade (operador ==), diferença (operador !=) e desigualdade \n(operadores >, <, >= e <=).  \nObserve que para testar a igualdade usa-se um duplo igual ==, já que o sinal de igual isolado é usado \ncomo operador de atribuição. \nBlocos if() devem ser usados dentro de blocos loop() ou setup(), portanto é uma boa prática, ao \nescrever programas, endentar o conteúdo do bloco para facilitar a leitura do código (ex: digitar \nquatro espaços antes, para cada novo nível de chaves {...}).  \nPor exemplo, as instruções que começam com “int” e “if” abaixo estão dentro de loop() e endentadas 4 \nespaços. A instrução “digitalWrite” está dentro de if, que está dentro de loop, e endentada 8 espaços: \nvoid loop() { \n    int estado = digitalRead(3);   \n    if (estado == HIGH) {         // testa se estado é 5V \n        digitalWrite(8, HIGH);    // “acende” componente que está no pino 8 \n    } \n} \nUm bloco if() pode ser seguido por um bloco else {}, que significa “caso contrário” e executa quando a \ncondição não for verdadeira: \nif (estado == HIGH) {         // somente se o valor de estado for 5V \n   digitalWrite(8, HIGH);     // “acende” componente que está no pino 8 \n} else {                      // caso contrário \n   digitalWrite(10, HIGH);    // acende o componente do pino 10 \n} \n",
    "book_id": "introducaoeletronicaartistas",
    "book_title": "Microsoft Word - IntroduçãoEletronicaArtistas.docx",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 92
  },
  {
    "chunk_full": "Introdução a Eletrônica para Artistas \n \n \nc b a 2017 Helder da Rocha \n \n \n92 \nAlteração 25.1 – Invertendo o estado de acionamento \nAltere o programa do último experimento para que ele acione o LED quando: \n• \nO estado do pino não for LOW (requer apenas alteração no código). \n• \nO estado do pino for HIGH (que alteração precisará ser feita no circuito?) \nExperimento 26 – Entrada com resistores pull-up \nMaterial necessário: \n• \nArduino Nano + cabo USB + computador \n• \nUm LED de qualquer cor \n• \nResistor de 220 ohms \n• \n2 chaves tácteis de pressão \n• \nProtoboard, fios e jumpers \nHá situações em que um programa precisa saber o estado da chave também quando ela não estiver \nsendo apertada. Nesses casos, é preciso conectar um resistor de pull-up ou pull-down entre o pino \ne o estado desejado (oposto ao estado quando a chave estiver acionada) para que o estado inicial \nseja definido. Este resistor liga o pino a um estado inicial. Se o pino for depois ligado diretamente \nou através de uma resistência menor, a 5V ou GND, haverá um caminho mais curto para a corrente e \nseu estado será invertido. \nLigar o pino a 5V ou GND é irrelevante, pois o programa poderá escolher o que fazer em cada caso. \nPor exemplo, se escolhermos ligar uma chave a 5V, o valor no pino será HIGH quando a chave for \npressionada, mas indefinido quando ela estiver aberta (veja ilustração abaixo). Usando um resistor de \npull-down (tipicamente de 10k) ligando o pino inicialmente a GND, garantirá ao pino um estado \ninicial LOW que mudará para HIGH quando a chave for pressionada. \n \nSe usarmos a lógica oposta (conectar com GND/LOW em vez de 5V/HIGH) não precisaremos dos \nresistores, pois o Arduino possui internamente resistores de pull-up (ligados em 5V) para cada \npino. Esta opção pode ser ativado configurando o pinMode() com INPUT_PULLUP: \npinMode(CHAVE, INPUT_PULLUP); \nNessa configuração, o pino terá sempre como estado inicial o nível lógico HIGH. No fechamento da \nchave, o estado mudará para o nível lógico oposto (LOW). O diagrama abaixo ilustra as duas formas \nde configurar entradas digitais em Arduino: \n",
    "book_id": "introducaoeletronicaartistas",
    "book_title": "Microsoft Word - IntroduçãoEletronicaArtistas.docx",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 93
  },
  {
    "chunk_full": "Introdução a Eletrônica para Artistas \n \n \nc b a 2017 Helder da Rocha \n \n \n93 \n \nConstrua o circuito abaixo. Ele apenas acrescenta mais uma chave ao circuito anterior. \n \nUsaremos um programa que depende dos dois estados (ligado e desligado) de cada chave para decidir \nquando acender o LED.  \n#define PINO_LIGAR    3 \n#define PINO_DESLIGAR 4 \n#define LED 8 \n \nvoid setup() { \n  pinMode(LED, OUTPUT); \n  pinMode(PINO_LIGAR, INPUT_PULLUP); \n  pinMode(PINO_DESLIGAR, INPUT_PULLUP); \n} \n \nvoid loop() { \n  int acender = digitalRead(PINO_LIGAR);   \n  int apagar  = digitalRead(PINO_DESLIGAR);   \n    \n  if(acender == LOW) { \n    digitalWrite(LED, HIGH); \n  } \n  if(apagar  == LOW) { \n    digitalWrite(LED, LOW); \n  } \n} \nO loop() é executado repetidas vezes e cada vez: 1) o estado de cada pino é lido, e 2) dois blocos \ncondicionais if são executados. Se a condição testada for verdadeira, o conteúdo é executado. Se não \nfor, o conteúdo é ignorado.  \n",
    "book_id": "introducaoeletronicaartistas",
    "book_title": "Microsoft Word - IntroduçãoEletronicaArtistas.docx",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 94
  },
  {
    "chunk_full": "Introdução a Eletrônica para Artistas \n \n \nc b a 2017 Helder da Rocha \n \n \n94 \nOs blocos condicionais apenas testam se cada botão está em estado LOW, se não estiver, eles são \nignorados. Mas os botões raramente estão no estado LOW. Isto acontece apenas quando forem \napertados. O estado normal de cada botão é HIGH, já que estão conectados via pull-up. Se por uma \nfração de segundo você apertar qualquer um dos botões, seu estado será momentaneamente LOW e o \nbloco será executado. O primeiro if acende o LED, o segundo if apaga. Enquanto nenhum botão está \npressionado, o estado anterior é mantido. \nO circuito, portanto, tem uma memória que representa o estado do último botão apertado no \nacendimento ou apagamento do LED. Funciona como um alternador de estado (faz o mesmo que o \n555 bi-estável do capítulo anterior, mas sem precisar calcular capacitores nem resistores).  \nAlteração 26.1 – Substituindo uma chave por um sensor \nSe você trocar a chave por um sensor que baixe a tensão no pino 3 a um nível abaixo de 3V, você pode \nfazer o LED acender com um evento externo, por exemplo, apagar ou acender uma luz, ou bater \npalmas. O botão em D4 seria usado apenas para apagar o LED. \nA entrada digital não é a ideal para obter dados de sensores (que são dispositivos analógicos), mas \npodemos experimentar enquanto não aprendemos a usar entradas analógicas. \nFaça o teste usando um LDR ou um fototransistor no lugar da chave. O fototransistor (esquema \nabaixo) se comporta como uma chave fechada quando recebe luz (visível ou infravermelha) \ndiretamente na sua lente. Um pulso curto de luz é suficiente para acender o LED. \n \nO circuito sensor de som abaixo irá gerar pulsos altos e baixos. Um pulso que faça o transistor \nconduzir por um instante fará o LED acender. O microfone pode ser alimentado através da saída 5V.  \n \n5.6.8. PWM e analogWrite \nO Arduino Nano não produz sinal analógico verdadeiro, mas apenas os simula através de PWM (Pulse \nWidth Modulation – veja capítulo anterior) através dos pinos 3, 5, 6, 9, 10 e 11. PWM permite simular \nvalores médios de tensão que variam entre os níveis lógicos LOW e HIGH. Os valores são definidos \nno programa dentro de uma escala de 0 a 255 e produzidos com a instrução analogWrite() em \nqualquer pino digital PWM. \nanalogWrite(5, 64);  // envia 5V 25% do tempo 5V para a saída digital 5 \n",
    "book_id": "introducaoeletronicaartistas",
    "book_title": "Microsoft Word - IntroduçãoEletronicaArtistas.docx",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 95
  },
  {
    "chunk_full": "Introdução a Eletrônica para Artistas \n \n \nc b a 2017 Helder da Rocha \n \n \n95 \nExperimento 27 – Piscando suavemente \nMaterial necessário: \n• \nArduino Nano + cabo USB + computador \n• \nLED \n• \nResistor de 220 ohms \n• \nProtoboard, fios e jumpers \nEste é uma variação do primeiro experimento que pisca um LED. O circuito é praticamente o mesmo, \nmas como o pino 8 usado no programa anterior não suporta PWM, transferimos o LED para o pino \ndigital 3 (que é um dos seis pinos que suporta saída analógica).  \n     \n \nO programa também mudou. Em vez de piscar em dois estados digitais, este pisca suavemente, \nvariando entre o totalmente aceso e totalmente apagado. \nint LED = 3; \nint brilho = 255; // inicia com brilho máximo \nint direcao = -1;  // 1 = aumentando, -1 = diminuindo \n \nvoid setup() { \n    pinMode(LED, OUTPUT);  // opcional com analogWrite (mas é boa prática) \n} \n \nvoid loop() { \n    analogWrite(LED, brilho); \n    delay(2); \n    brilho = brilho + direcao; \n    if(brilho <= 0 || brilho >= 255) { \n        direcao = -direcao; \n    } \n} \nO programa define duas variáveis (além do pino): brilho, que guarda o valor do brilho do LED (entre \n0 e 255) e direcao, que contém o número 1 ou -1, que será somado ao brilho cada vez que loop() for \nexecutado, fazendo com que o brilho aumente ou diminua lentamente. \nAssim que loop() inicia, o pino onde está o LED recebe o valor analógico 255 (correspondente a 5V). \nDepois espera 10 milissegundos e executa a instrução \nbrilho = brilho + direcao; \nUma expressão de atribuição primeiro executa a expressão do lado direito do “=”, substituindo as \nvariáveis por seus valores. Portanto, a expressão executada será calculada da seguinte forma: \nbrilho = 255 + (-1) \nque irá gravar o valor 254 na variável brilho.  \n",
    "book_id": "introducaoeletronicaartistas",
    "book_title": "Microsoft Word - IntroduçãoEletronicaArtistas.docx",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 96
  },
  {
    "chunk_full": "Introdução a Eletrônica para Artistas \n \n \nc b a 2017 Helder da Rocha \n \n \n96 \nNo bloco seguinte temos uma expressão condicional if que testa se brilho é maior ou igual a zero OU \nse é maior ou igual a 255. O símbolo || conecta duas expressões através de uma proposição lógica \nOU. Isto significa que a condição do if será verdadeira se uma ou ambas as expressões forem \nverdadeiras. A condição é testada para os valores limite 0 e 255. Quando o valor de brilho chegar a \num desses valores, o sinal da variável direcao é trocado (se era 1, passa a ser -1; se era -1 passa a ser \n1). Assim o brilho que estava diminuindo, passa a gradualmente aumentar, e vice-versa. \n5.6.9. Entrada analógica \nAs entradas analógicas podem ser usadas para ler valores produzidos por sensores e potenciômetros. \nA instrução é analogRead(Número-do-pino). O valor lido é de 0 a 1023 (correspondente a valores \nintermediários de tensão entre 0 e 5V). Os pinos de entrada analógica são A0 – A7. Eles podem ser \nidentificados no programa com ou sem prefixo A (ex: pode-se usar A0 ou simplesmente 0): \nint valor = analogRead(3); // lê do pino A3 \nÉ uma boa prática usar sempre o prefixo A ao declarar pinos analógicos, já que deixa o código mais \nlegível e fácil de entender, evitando confusão com pinos digitais. \nOs pinos de entrada analógica não podem ser usados para saída analógica. Eles não suportam PWM. \nMas os seis primeiros (A0 a A5) podem ser usados para saída digital (digitalWrite), se necessário. \nNeste caso eles podem ser identificados com o nome analógico prefixado (A0, A1, etc.) ou com os \nnúmeros 14 a 19 (que correspondem aos pinos A0 a A5, respectivamente). Ou seja,  \ndigitalWrite(A5, HIGH); \né o mesmo que \ndigitalWrite(19, HIGH); \nExperimento 28 – “Theremin” com LDR e potenciômetro \nMaterial necessário: \n• \nArduino Nano + cabo USB + computador \n• \nPotenciômetro de qualquer valor (ex: 10k, 100k) \n• \nResistor de 1k ohms \n• \nLDR \n• \nAlto-falante \n• \nProtoboard, fios e jumpers \nO circuito abaixo usa duas entradas analógicas. Uma para obter a leitura de luz, e a outra para obter a \nposição do potenciômetro que será usada para especificar um intervalo de tempo. A quantidade de \nluz no LDR irá variar a tensão sobre o resistor de 1k (e consequentemente no pino A0). O mesmo \nocorre no pino A1, que obtém seu valor do divisor de tensão formado pelo potenciômetro.  \nA saída PWM varia de 0 a 255, portanto em um circuito perfeitamente calibrado, dividiríamos o valor \nlido em qualquer uma das entradas analógica por 4 (1024/4 = 256) para usá-lo diretamente na saída. \n \n",
    "book_id": "introducaoeletronicaartistas",
    "book_title": "Microsoft Word - IntroduçãoEletronicaArtistas.docx",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 97
  },
  {
    "chunk_full": "Introdução a Eletrônica para Artistas \n \n \nc b a 2017 Helder da Rocha \n \n \n97 \n \nNo programa abaixo dividimos por 4 o valor lido no potenciômetro. Mas a tensão no pino A1 que \nvaria com a luz aplicada ao LDR tem valor indefinido (que provavelmente nunca será zero ou 1023). \n#define FTE 12 \n#define LDR A0  \n#define POT A1  \n \nvoid setup() {}  // pinMode é opcional com analogRead \n \nvoid loop() { \n  int luz    = analogRead(LDR); \n  int pausa  = analogRead(POT); \n  delay(pausa * .25); \n  int tom = luz * luz * luz / 16;  // experimente outros valores \n  tone(FTE, tom); \n} \nO comando tone() gera uma onda quadrada em frequência especificada como parâmetro. As \nfrequências suportadas (até 65kHz) incluem frequências audíveis (20Hz a 20kHz). O valor \nproporcional ao cubo do valor lido pelo LDR permite que a frequência varie bastante alternando entre \nsons agudos e graves. \n5.6.10. Serial monitor \nO monitor serial é uma tela que aparece no computador quando o Arduino está conectado via USB e \nque permite imprimir texto enviado pelo programa. Ele pode ser usado para depurar programas, ou \nexibir dados lidos por sensores.  \nPara usar o monitor serial, é preciso declarar dentro do bloco setup() a taxa de leitura. Para \ndepuração simples use o comando: \nSerial.begin(9600); \nDentro de loop(), quando quiser imprimir algo no terminal use Serial.print() (que imprime texto ou \nvalor de uma variável) ou Serial.println() (que imprime uma nova linha no final). Para imprimir \ntexto, ele deve ser informado entre aspas. Por exemplo, considere o trecho abaixo: \nvoid loop() { \n    int estado = digitalRead(9);  \n    Serial.print(\"Estado do botão: \"); \n    Serial.println(estado); \n} \n",
    "book_id": "introducaoeletronicaartistas",
    "book_title": "Microsoft Word - IntroduçãoEletronicaArtistas.docx",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 98
  },
  {
    "chunk_full": "Introdução a Eletrônica para Artistas \n \n \nc b a 2017 Helder da Rocha \n \n \n98 \nO monitor serial pode ser aberto clicando no ícone \n ou através do menu Ferramentas (ou Tools) \ndo IDE Arduino, ou. Rodando o programa acima, ele irá imprimir “Estado do botão: 0” se o botão \nestiver no estado LOW, ou “Estado do botão: 1” se ele estiver no estado HIGH. \n \nÉ mais interessante usar o monitor serial para ler dados gerados por dispositivos analógicos. Faremos \nisto no experimento a seguir. \nExperimento 29 – Termômetro \nMaterial necessário: \n• \nArduino Nano + cabo USB + computador \n• \nCircuito termômetro LM35DZ (veja referência no final da apostila) \n• \nProtoboard, fios e jumpers \nNeste experimento conectamos um componente LM35 ao Arduino para obter a temperatura do \nambiente e imprimir o seu valor no monitor serial.  \nO LM35 é um termômetro de precisão. Ele tem a mesma embalagem (TO-92) que um transistor \nBC549. Uma vez alimentado com uma tensão entre 5 e 15 V nos seus terminais externos, o terminal \ncentral apresentará uma tensão relativa ao terminal negativo proporcional à temperatura ambiente. \nAos 25 graus Celsius essa tensão medirá 0,25V, e varia 0,01 volts para cada grau acima ou abaixo com \nmargem de erro de 0,5 graus dentro da faixa 2 a 100 graus Celsius. \nVeja a pinagem do LM35 na referência ao final da apostila. Ligue o seu pino VCC no pino 5V do \nArduino, e o GND do LM35 em qualquer um dos dois GND do Arduino. O pino central OUT fornecerá a \nmedida da tensão e deve ser ligado a qualquer uma das entradas analógicas (A0 a A7) do Arduino. \n \nNo programa abaixo ligamos terminal OUT do LM35 na entrada analógica A1. \n",
    "book_id": "introducaoeletronicaartistas",
    "book_title": "Microsoft Word - IntroduçãoEletronicaArtistas.docx",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 99
  },
  {
    "chunk_full": "Introdução a Eletrônica para Artistas \n \n \nc b a 2017 Helder da Rocha \n \n \n99 \n#define TERMOMETRO A1 \n \nvoid setup() { \n  Serial.begin(9600); \n} \n \nvoid loop() { \n  int leitura = analogRead(TERMOMETRO); \n  float volts = (leitura / 1024.0) * 5.0; \n  float celsius = (volts) * 100.0; \n   \n  Serial.print(\"Temperatura: \"); \n  Serial.println(celsius); \n   \n  delay(2000); \n} \nA leitura analógica do Arduino varia de 0 (0 volts) a 1024 (5 volts). Portanto é preciso dividir por \n1024 e multiplicar por 5 para obter o valor real em volts que está no terminal OUT do LM35. A leitura \nem graus Célsius será este valor multiplicado por 100. \nNeste programa a temperatura é impressa a cada 2 segundos no monitor serial. O circuito funciona \napenas conectado ao computador. Para tornar o termômetro independente do computador teríamos \nque elaborar um circuito de saída capaz de indicar a temperatura. Poderia ser um par de displays de 7 \nsegmentos, um display de cristal líquido, uma série de leds, etc.  \nO próximo experimento combina o uso de entradas e saídas analógicas para controlar a velocidade de \num motor.  \nExperimento 30 (extra) – Acelerando e desacelerando o motor com luz \nMaterial necessário: \n• \nArduino Nano + cabo USB + computador \n• \nMotor de 3V \n• \n1 diodo de propósito geral (1N4148 ou equivalente) \n• \n2 resistores de 10k ohms \n• \nLDR \n• \n1 transistor MOSFET IRL540 \n• \nProtoboard, fios e jumpers \nVariando a tensão média aplicada no motor podemos fazê-lo acelerar ou desacelerar. Já fizemos isto \nusando o circuito integrado 555. Com Arduino o circuito é bem mais simples. Monte o circuito abaixo: \n \n",
    "book_id": "introducaoeletronicaartistas",
    "book_title": "Microsoft Word - IntroduçãoEletronicaArtistas.docx",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 100
  },
  {
    "chunk_full": "Introdução a Eletrônica para Artistas \n \n \nc b a 2017 Helder da Rocha \n \n \n100\n  \nO circuito requer uma entrada analógica que irá selecionar a velocidade desejada para o motor, e uma \nsaída, também analógica (PWM), para variar a intensidade da tensão aplicada no motor e assim fazê-\nlo girar mais rápido ou mais devagar. \nA leitura dos dados de entrada para controlar a velocidade do motor poderia ser feita variando um \npotenciômetro, mas é bem mais interessante usar um sensor de intensidade luminosa como o LDR. O \ncircuito pode ser usado para construir um carrinho que anda quando o ambiente está iluminado, e \nque para quando entra em um ambiente escuro. \nComo um motor pode demandar muito mais corrente que o pino do Arduino é capaz de fornecer, \nprecisamos isolá-lo. Uma forma de fazer isto é através de um transistor. Usaremos neste exemplo um \ntransistor MOSFET de potência, que consome pouca energia mas suporta bem mais corrente direta \nque o motor será capaz de exigir. Motores também causam pulsos reversos de corrente quando o \nmotor liga ou desliga. Protegeremos o circuito desses pulsos com um diodo em paralelo com o motor. \nEste MOSFET que usamos só irá conduzir (entre os seus terminais D e S) quando a tensão no terminal \nde controle (terminal G) tiver mais de 4,5V. Como usamos PWM, o pino 3 irá gerar pulsos de onda \nquadrada em valores absolutos de 0 ou 5V (os valores intermediários são simulados através da \nlargura dos pulsos, ou seja, através de PWM). Assim o MOSFET irá ligar e desligar muito rapidamente. \nQuando o tempo ligado aumentar, o motor irá acelerar. Quando o tempo desligado aumentar, o motor \nirá desacelerar. \nO programa é simples. Consiste na leitura do valor do LDR (0 a 1023) dividido por quatro para que \npossa ser enviado para a saída PWM (0 a 255): \n#define PINO_MOTOR 3; \n#define PINO_LDR A0; \n \nvoid setup() {  \n  pinMode(PINO_MOTOR, OUTPUT); \n}  \n  \nvoid loop() {  \n    int velocidade = analogRead(PINO_LDR) / 4;  \n    analogWrite(PINO_MOTOR, velocidade); \n} \nAlteração 30.1 – Usando uma fonte externa para alimentar o motor \nEste experimento funciona adequadamente com o motor distribuído no kit, que consome muito pouca \ncorrente, mas se você usar um outro motor ele poderá exigir demais do Arduino. Normalmente um \nmotor não deve ser alimentado com a fonte interna de 5V do Arduino. Ele deve usar uma fonte \nexterna (a fonte pode até ser a mesma usada pelo Arduino, se o motor suportar). Isto não impede que \no Arduino continue a controlá-lo. O circuito abaixo ilustra  esta configuração (o Arduino pode ser \nalimentado via USB ou pelo pino Vin): \n",
    "book_id": "introducaoeletronicaartistas",
    "book_title": "Microsoft Word - IntroduçãoEletronicaArtistas.docx",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 101
  },
  {
    "chunk_full": "Introdução a Eletrônica para Artistas \n \n \nc b a 2017 Helder da Rocha \n \n \n101\n \n \n5.7. Programação do Arduino: funções, listas e bibliotecas \nCom o que vimos de programação do Arduino até aqui, que foi basicamente como usar suas entradas \nanalógicas e digitais, já podemos construir todos os circuitos que vimos nas seções anteriores usando \ntransistores e circuitos integrados 555, de forma muito mais simples e sem precisar calcular circuitos \nRC (resistor-capacitor), e nem mesmo medir tensões e correntes. Não será sempre assim. O \nconhecimento de eletrônica básica ainda será importante para fazer projetos mais interessantes, \nporém o Arduino de fato simplifica o uso da eletrônica. \nEsta é uma seção opcional. Aqui exploramos alguns tópicos mais avançados de programação com o \nArduino que permitirão que você use programas escritos por outras pessoas e faça alterações neles \nsem necessariamente entender tudo o que fazem. Se você nunca programou antes os conceitos podem \nparecer complexos. Mas não é preciso entender tudo para fazer os experimentos. Monte os circuitos \n(que são muito simples) e copie os códigos. Você pode deixar para ler a teoria depois, quando já tiver \nmais familiaridade com programas em Arduino.  \n5.7.1. Declarando funções \nOs comandos digitalWrite, analogRead, etc. que chamamos dentro dos blocos setup() e loop() são \nchamadas de funções. Elas foram definidas nas bibliotecas do Arduino. Essas bibliotecas são \nautomaticamente incluídas em todos os programas.  \nUma função, portanto, para que possa ser chamada, precisa ser definida em algum lugar. A chamada \npoderá ser feita dentro do bloco loop() ou setup(). A definição de novas funções poderá ser feita no \npróprio sketch, ao lado dos blocos loop() e setup() que são definições de funções (chamadas \nautomaticamente pelo Arduino). \nPortanto, para definir uma função, escolha qualquer lugar antes ou depois dos blocos loop() e setup(), \ne crie um novo bloco com a estrutura abaixo: \n",
    "book_id": "introducaoeletronicaartistas",
    "book_title": "Microsoft Word - IntroduçãoEletronicaArtistas.docx",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 102
  },
  {
    "chunk_full": "Introdução a Eletrônica para Artistas \n \n \nc b a 2017 Helder da Rocha \n \n \n102\nvoid nomeDaFuncao() { \n    // coloque aqui as instruções que sua função irá executar quando chamada \n} \nVejamos um exemplo. Abaixo definimos uma função chamada piscar(): \nvoid piscar() {  \n    digitalWrite(8, HIGH); \n    delay(500); \n    digitalWrite(8, LOW); \n    delay(500); \n} \nEla tem o mesmo código dentro de loop() do circuito que pisca o LED. Agora podemos chamar a \nfunção que acabamos de definir como se fosse um comando (terminando em ponto-e-vírgula): \nvoid loop() { \n    piscar();   // chama a função piscar() \n} \nO código acima funciona igual ao código original que pisca o LED. A vantagem de definir funções é que \npodemos chamá-las várias vezes, sem precisar repetir o código que ela contém. Isto é mais fácil de \nperceber se definirmos funções com parâmetros. \nQuando definimos uma função, os parâmetros são declarados como variáveis. Como toda variável, o \ncada parâmetro tem um tipo de dados que faz parte da declaração. Essas variáveis declaradas irão \nreceber valores quando a função for chamada, e podem usar esses valores dentro da definição da \nfunção. No exemplo abaixo definimos uma função piscar() contendo dois parâmetros inteiros: \nvoid piscar(int pino, int tempo) { \n    digitalWrite(pino, HIGH); \n    delay(tempo); \n    digitalWrite(pino, LOW); \n    delay(tempo); \n} \nObserve que as variáveis são usadas pelos comandos que estão dentro da função. Para chamar a \nfunção acima precisamos passar para ela dois parâmetros inteiros. O valor do primeiro parâmetro \nserá copiado (atribuído) à variável pino, e o valor do segundo será copiado à variável tempo. Por \nexemplo, veja a chamada abaixo dentro de loop(): \nvoid loop() { \n    piscar(8, 500); \n} \nIsto irá copiar o valor 8 para pino, e o valor 500 para tempo. E dentro da função, esses valores serão \nnovamente copiados para funções do Arduino (digitalWrite e delay). Qual a vantagem disso? Agora \npodemos chamar a mesma função várias vezes, alterando os parâmetros que passamos para ela. Por \nexemplo, podemos fazer um loop() que pisca LEDs em pinos diferentes e em tempos diferentes sem \nprecisar escrever 12 linhas de código: \nvoid loop() { \n    piscar(8, 500); \n    piscar(9, 250); \n    piscar(8, 1000); \n} \nExperimento 31 (extra) – Definindo funções para controlar um LED RGB \nMaterial necessário: \n• \nArduino Nano + cabo USB + computador \n• \nLED RGB de anodo comum (ou três LEDs: um vermelho, um verde e um azul) \n• \nResistor de 220 ohms \n• \nProtoboard, fios e jumpers \n",
    "book_id": "introducaoeletronicaartistas",
    "book_title": "Microsoft Word - IntroduçãoEletronicaArtistas.docx",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 103
  },
  {
    "chunk_full": "Introdução a Eletrônica para Artistas \n \n \nc b a 2017 Helder da Rocha \n \n \n103\n         \n \nO programa abaixo define uma função cor(r,g,b) que permite declarar cores usando comandos RGB \nque são traduzidos em cores de um LED RGB cujos terminais estão conectados a saídas PWM do \nArduino. O circuito pode ser usado com 3 LEDs ou um LED RGB. Como o Led RGB usado tem o anodo \ncomum, ele foi ligado em 5V e analogWrite(pino, 0) provocará o brilho máximo, já que a diferença de \npotencial nesse valor é máxima. Para compensar isto sem modificar o circuito podemos subtrair 255 \ndo valor de cada componente de cor. Isto é feito dentro da função cor(); \n#define LED_VERMELHO 9 \n#define LED_VERDE 10 \n#define LED_AZUL 11 \n \nvoid setup() {} \n \nvoid cor(int r, int g, int b) { \n  analogWrite(LED_VERMELHO, 255 - r); \n  analogWrite(LED_VERDE, 255 - g); \n  analogWrite(LED_AZUL, 255 - b); \n} \n \nvoid acender(int r, int g, int b, int intervalo) { \n  cor(r, g, b); \n  delay(intervalo); \n} \n \nvoid loop() { \n  acender(255,0,0,1000);      // vermelho \n  acender(0,255,0,1000);      // verde \n  acender(0,0,255,1000);      // azul \n  acender(190,255,0,1000);    // amarelo \n  acender(190,0,255,1000);    // magenta \n  acender(0,255,255,1000);    // ciano \n  acender(255,255,255,1000);  // branco \n  acender(0,0,0,1000);        // apagado \n \n  acender(63,0,0,1000);  // vermelho com 1/4 da intensidade \n  acender(0,63,0,1000);  // verde com 1/4 da intensidade \n  acender(0,0,63,1000);  // azul com 1/4 da intensidade \n} \nAlém da função cor(), o programa acima também define uma função acender(r,g,b,duracao), que \nestabelece a cor (chamando a função cor()) e o tempo em que ela ficará acesa. O loop() chama várias \nvezes a função acender() com diferentes parâmetros, para que o circuito exiba uma sequência de \ncores diferentes, mantendo cada uma acesa por um segundo. \n",
    "book_id": "introducaoeletronicaartistas",
    "book_title": "Microsoft Word - IntroduçãoEletronicaArtistas.docx",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 104
  },
  {
    "chunk_full": "Introdução a Eletrônica para Artistas \n \n \nc b a 2017 Helder da Rocha \n \n \n104\n5.7.2. Bibliotecas e arquivos .h (arquivos de cabeçalho) \nSuponha que você pretenda usar LEDs RGB em vários circuitos diferentes e queira reusar as funções \ncriadas no experimento anterior. Uma maneira de fazer isto é recortar e colar o texto no outro \nprograma. Mas há alternativas melhores. Uma delas é armazená-las em um arquivo separado que \npossa ser incluído em outros programas: um arquivo de cabeçalho, que tem a extensão .h \nPara criar um arquivo de cabeçalho no IDE do Arduino, clique no menu que aparece logo abaixo do \nícone do monitor serial, e depois selecione “New Tab”: \n \nDepois escolha um nome para o arquivo. Por exemplo, “ledsrgb.h”, e grave-o. \n \nAgora vamos transferir as funções cor() e acender() para o arquivo ledsrgb.h. Recorte-as do sketch, e \ncole as duas funções abaixo no arquivo .h: \nvoid cor(int r, int g, int b) { \n  analogWrite(LED_VERMELHO, 255 - r); \n  analogWrite(LED_VERDE, 255 - g); \n  analogWrite(LED_AZUL, 255 - b); \n} \n \nvoid acender(int r, int g, int b, int intervalo) { \n  cor(r, g, b); \n  delay(intervalo); \n} \nComo a função cor() depende de variáveis que ainda estão no arquivo original, precisamos redefini-\nlas localmente (no arquivo .h) e depois copiar os valores. Acrescente o seguinte código: \nint pino_R;  // foram declaradas sem valor inicial \nint pino_G; \nint pino_B; \n \nvoid configurarRGB(int pr, int pg, int pb) { \n  pino_R = pr;  // o valor inicial passado quando esta função for chamada \n  pino_G = pg; \n  pino_B = pb; \n} \nE altere a função cor() para que ela use os novos valores: \nvoid cor(int r, int g, int b) { \n  analogWrite(pino_R, 255 - r); \n  analogWrite(pino_G, 255 - g); \n  analogWrite(pino_B, 255 - b); \n} \nA função configurarRGB() será chamada a partir do sketch original, para passar os valores dos \npinos ao arquivo .h. Para incluir o arquivo .h no seu sketch, use uma expressão #include: \n#include \"piscaled.h\" \n",
    "book_id": "introducaoeletronicaartistas",
    "book_title": "Microsoft Word - IntroduçãoEletronicaArtistas.docx",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 105
  },
  {
    "chunk_full": "Introdução a Eletrônica para Artistas \n \n \nc b a 2017 Helder da Rocha \n \n \n105\n(observe que ela não tem ponto e vírgula no final – é uma diretiva como #define). \nDentro do setup() do seu sketch, chame a função configurarRGB() passando as variáveis que contém \nos números dos pinos. Desta forma, os valores serão copiados para as variáveis do arquivo .h: \nvoid setup() { \n    configurarRGB(LED_VERMELHO, LED_VERDE, LED_AZUL); \n} \nAgora é possível rodar o programa alterado, que consiste de dois arquivos, e ele funcionará igual, \nmesmo não contendo a definição de acender(). Se você quiser usar piscaled.h em outro sketch, precisa \napenas copiá-lo para a pasta onde está o arquivo .ino do sketch, declarar o #include e chamar o \nconfigurarRGB(). Depois disso você pode chamar as funções cor() e acender() como se elas tivessem \nsido definidas localmente. \nSe você achar mais fácil, pode baixar o projeto contendo os dois arquivos usados neste exemplo a \npartir do repositório GitHub disponível na Internet. Veja o link na introdução da apostila.  \nExperimento 32 (extra) – Usando bibliotecas para produzir notas musicais \nNeste experimento usaremos uma mini-biblioteca (arquivo .h) de notas musicais para fazer o Arduino \nexecutar uma música. \nMaterial necessário \n• \nArduino Nano + cabo USB + computador com conexão à internet \n• \nAlto-falante \n• \nProtoboard, fios e jumpers \nO comando tone() gera pulsos de onda quadrada na frequência passada como parâmetro. Por \nexemplo, pra produzir um lá central usado para afinação, podemos mandar para um pino PWM que \nesteja ligado a um alto-falante um comando: \ntone(440); \nPara tocar uma música precisamos saber a frequência de cada nota, mas o ideal seria poder \nsimplesmente chamar as notas pelo nome.  \nExiste um arquivo .h para isto. Ele atribui nomes de variáveis para cada uma das 88 notas do piano, \nguardando a sua frequência. É um programa de domínio público que faz parte dos exemplos da IDE \ndo Arduino. Este é um trecho: \n... \n#define NOTE_A4  440 \n#define NOTE_AS4 466 \n#define NOTE_B4  494 \n#define NOTE_C5  523 \n#define NOTE_CS5 554 \n... \nOu seja, usando-o podemos mandar comandos: \ntone(NOTE_A4); \ne fazer soar um lá sem precisar lembrar da sua frequência.  \nNOTE_C3 corresponde ao terceiro Dó do piano. NOTE_A4 ao quarto lá, e assim por diante. O comando \nnoTone(PINO) desliga o sinal no pino (aplica valor LOW, 0V). \nCriaremos o arquivo .h localmente. Abra um sketch novo, e em seguida abra um tab (como mostrado \nacima no exemplo sobre arquivos .h), e cole nele conteúdo do arquivo abaixo: \nhttps://www.arduino.cc/en/Tutorial/ToneMelody?action=sourceblock&num=2 \nGrave o arquivo como notas.h. \nVamos alterar o nome das variáveis para que fiquem mais curtos e seja mais fácil digitar notas. Com o \ntab notas.h aberto, digite Control-F (Command-F no Mac) ou selecione o item de menu Edit/Find. \nAbrirá a janela abaixo.  \n",
    "book_id": "introducaoeletronicaartistas",
    "book_title": "Microsoft Word - IntroduçãoEletronicaArtistas.docx",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 106
  },
  {
    "chunk_full": "Introdução a Eletrônica para Artistas \n \n \nc b a 2017 Helder da Rocha \n \n \n106\n \nNo campo “Find:” digite NOTE_ e clique no botão Replace All. Isto removerá o prefixo NOTE_ de todas \nas notas. O resultado deve ficar da forma abaixo: \n#define B0  31 \n#define C1  33 \n#define CS1 35 \n#define D1  37 \n#define DS1 39 \n#define E1  41 \n... \nAgora acrescente mais uma definição, em uma linha antes de B0: \n#define ZZ 0 \nEsta será a nota de pausa (frequência zero) que não produzirá som algum. \nDigite o programa abaixo. Ele foi adaptado dos exemplos do Arduino (“toneMelody” criado por Tom \nIgoe). Ele usa essa biblioteca de notas musicais para tocar uma pequena música. Nesta versão \ntocamos uma música diferente. As notas da música estão dentro do bloco melodia[], e os tempos \ncorrespondentes de cada nota em durações[]: \n#include \"notas.h\" \n \nint ALTO_FALANTE = 5; // coloque alto-falante ou buzzer entre pino 5 e GND \nint NUM_NOTAS = 8;    // número de notas incluídas em melodia[] \n \nint melodia[] = { \n  G4, G4, G4, DS4, AS4, G4, DS4, AS4, G4, \n  D5, D5, D5, DS5, AS4, FS4, DS4, AS5, G4, \n  G5, G4, G4, G5, FS5, F5, E5, DS5, E5, ZZ,  \n  GS4, CS5, C5, B5, AS5, A5, AS5, ZZ,  \n  DS4, FS4, DS4, AS4, G4, DS4, AS5, G4}; \n \n// duração: 1 = semibreve, 2 = mínima, 4 = seminima, 8 = colcheia, etc.: \nint duracoes[] = {  \n  4, 4, 4, 6, 8, 4, 6, 8, 2, \n  4, 4, 4, 6, 8, 4, 6, 8, 2, \n  4, 6, 8, 4, 6, 8, 16, 16, 8, 8,  \n  8, 4, 6, 8, 16, 16, 8, 8,  \n  8, 4, 6, 8, 4, 6, 8, 2 \n}; \n \nvoid setup() { \n  for (int nota = 0; nota < NUM_NOTAS; nota++) { \n    int duracao = 1000 / duracoes[nota]; // divide 1000 por cada duração \n    tone(ALTO_FALANTE, melodia[nota], duracao); \n    int pausa = duracao * 1.30; \n    delay(pausa); \n    noTone(ALTO_FALANTE); \n  } \n} \nvoid loop() {} \n \n",
    "book_id": "introducaoeletronicaartistas",
    "book_title": "Microsoft Word - IntroduçãoEletronicaArtistas.docx",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 107
  },
  {
    "chunk_full": "Introdução a Eletrônica para Artistas \n \n \nc b a 2017 Helder da Rocha \n \n \n107\nO programa roda apenas uma vez no bloco setup(). Se quiser que ela toque sem parar, você pode \ntransferir todo o código para o bloco loop(), incluindo um delay(1000) no final para que haja um \nintervalo entre as execuções.  \nO programa usa um bloco de repetição for() para executar cada nota (será explicado mais adiante.) \nConstrua o circuito abaixo (que talvez seja o circuito mais simples criado no curso), ligando um dos \nfios do alto-falante no pino 5, e o outro terminal em GND. \n \nLogo após a transferência, a música começará a tocar. Experimente criar outras músicas usando essa \nbiblioteca. \nO código-fonte deste experimento (o sketch e arquivo notas.h já alterado) também está disponível \npara download em repositório GitHub (veja link na introdução da apostila). \n5.7.3. Listas \nO código do programa anterior possui várias estruturas que ainda não vimos. Uma delas é a lista \n(também chamado de array, ou vetor). Há duas listas no código do experimento anterior, \nrepresentadas pelas variáveis durações[] e melodia[]. Listas são declaradas com seus itens entre \nchaves, separados por vírgula. O tipo de dados se refere ao tipo de cada elemento da lista: \nfloat precos = {23.56, 9.99, 11.99, 25.49}; \nCada item da lista é referenciado por um numero que é seu índice. A contagem começa em zero, \nportanto, na lista acima, os índices são 0, 1, 2 e 3. Eles são usados para referenciar itens da lista. Por \nexemplo, precos[1] é 9.99 e precos[3] é 25.49. \nAs duas listas do programa do experimento anterior têm 8 elementos, e seus índices variam de 0 a 7. \n5.7.4. Repetição com for \nO bloco for é uma estrutura de repetição. Consiste de uma declaração entre parênteses e um \nconteúdo (a ser repetido) entre chaves: \nfor( declaração ) { /* conteúdo a ser repetido */ } \nA declaração tem três partes (separadas por ponto e vírgula):  \n",
    "book_id": "introducaoeletronicaartistas",
    "book_title": "Microsoft Word - IntroduçãoEletronicaArtistas.docx",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 108
  },
  {
    "chunk_full": "Introdução a Eletrônica para Artistas \n \n \nc b a 2017 Helder da Rocha \n \n \n108\n• \nO valor inicial de uma variável \n• \nA condição envolvendo a variável que deve ser verdadeira enquanto o bloco é repetido  \n• \nUma instrução para mudar o valor da a variável  \nBlocos de repetição for() são frequentemente usados com listas. Para imprimir no monitor serial cada \num dos itens da lista precos (de 4 elementos) acima, pode-se usar o seguinte for(): \nfor(int i = 0; i < 4; i = i + 1) { \n    Serial.println( precos[i] ); \n} \nA cada passada no for(), o valor de i será diferente. Iniciará em zero, e será 1, depois 2, depois 3. Na \npróxima passada i é 4, a condição se tornará falsa, e o bloco será encerrado.  \nNo código do experimento anterior, o for() incrementa o índice (variável nota) usando nota++. Isto é \no mesmo que fazer: \nnota = nota + 1; \nO for() executa apenas uma vez a primeira parte da declaração (inicialização da variável que \ncontrola a repetição), mas a cada repetição a condição da segunda parte é testada, o bloco entre \nchaves é executado, e por fim a expressão da terceira parte é executada, nesta ordem.  \nÉ importante que o valor da variável mude de forma que faça o for() terminar em algum momento (a \nmenos que você queira um loop infinito – mas para isto você já tem a função loop() que é mais \nsimples). Normalmente as expressões usadas na terceira parte do for() são apenas para incrementar \n(somar um) ou decrementar (subtrair um) da variável de controle. \nExperimento 33 (extra) – Usando LEDs RGB endereçáveis \nNeste último experimento iremos programar o Arduino para que ele execute uma sequência luminosa \nem um LED endereçável WS2812. Baixaremos as bibliotecas e os programas para executar as \nsequências da Internet através da IDE do Arduino.  \nLEDs WS2812 são pixels RGB frequentemente usados em painéis digitais coloridos de alta-definição. \nEles também são muito populares em produtos de wearables (eletrônica para vestir), usados em \nroupas, jóias e calçados. Geralmente eles são distribuídos em conjuntos contendo vários LEDs, \norganizados em matrizes quadradas, sequências circulares, fitas e painéis flexíveis. \nA foto abaixo mostra um circuito usando dois conjuntos de LEDs WS2812 (1 + 16) piscando em uma \nsequência programada no Arduino. No kit foi incluído um LED WS2812. \n \nMaterial necessário \n• \nArduino Nano + cabo USB + computador \n• \nLED RGB endereçável WS2812 \n• \nResistor de 470 ohms \n• \nCapacitor de 100uF \n• \nProtoboard, fios e jumpers \n",
    "book_id": "introducaoeletronicaartistas",
    "book_title": "Microsoft Word - IntroduçãoEletronicaArtistas.docx",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 109
  },
  {
    "chunk_full": "Introdução a Eletrônica para Artistas \n \n \nc b a 2017 Helder da Rocha \n \n \n109\nMonte o circuito abaixo. É necessário soldar terminais no LED WS2812 distribuído no kit. Veja como \nfazer isto no tutorial de soldagem no final da apostila. Você também pode adquirir LEDs WS2812 \nmontados em placas com vários LEDs e adaptados para uso em projetos de eletrônica para vestir.  \n \n \nExistem várias bibliotecas para usar LEDs endereçáveis. As mais populares são as bibliotecas \nNeoPixel e FastLED. Todas têm diversos programas de exemplo que você pode usar imediatamente, \nfazendo poucas alterações. Vamos instalar uma delas e rodar seus exemplos. \nSelecione o menu Sketch/Include Library/Manage Libraries no IDE do Arduino. Você verá a janela \nabaixo. No campo de pesquisa digite FastLED, e a janela filtrará a biblioteca FastLED que usaremos \npara programar os LEDs: \n \n",
    "book_id": "introducaoeletronicaartistas",
    "book_title": "Microsoft Word - IntroduçãoEletronicaArtistas.docx",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 110
  },
  {
    "chunk_full": "Introdução a Eletrônica para Artistas \n \n \nc b a 2017 Helder da Rocha \n \n \n110\nClique no botão Install, que aparece do lado direito. Quando a instalação terminar, você poderá abrir \nos exemplos que usam a biblioteca selecionando o menu File/Examples. No final há uma seção \n“Examples from Custom Libraries” e você encontrará FastLED. \nAbra o exemplo Blink. Verifique que a variável NUM_LEDS contém o valor 1 (um LED), e altere \nDATA_PIN para 4 (que é o pino onde conectamos o LED). Depois faça upload para o Arduino. O LED \ndeverá piscar na cor vermelha. Você pode mudar a cor alterando o código em loop(), por exemplo, \ntroque por: \nleds[0] = CRGB::Blue; \npara piscar na cor azul. \nAbra o exemplo ColorPalette. Este programa realiza uma série de animações passando por várias \ncores. Como temos apenas um LED, mude NUM_LEDS para 1, e LED_PIN para 4. Faça upload para o \nArduino e observe as variações de cor e pulsos que o LED vai fazer. \nÉ muito mais interessante usar esse programa com vários LEDs WS2812. Se você adquirir um outro \nLED endereçável, ou melhor ainda, uma placa com diversos LEDs WS2812 montados, você pode \nconectá-los em sequência ligando o pino Dout de uma placa à entrada Din da seguinte. Você então \ndeve alterar os programas que fazem o sequenciamento de LEDs e informar a quantidade total de \nLEDs que serão controlados. \nPor exemplo, no circuito abaixo adicionamos um anel de 16 LEDs em série com o WS2812 que já \nestava ligado ao circuito, resultando no total de 17 LEDs endereçáveis. Altere a variável NUM_LEDS \nnovamente (para 17) e faça upload para ver os LEDs piscarem e mudarem de cor sequencialmente. \n \nO circuito acima só deve ser usado em Arduino estabelecendo um limite de brilho para os LEDs \n(controlado pelo programa), já que o uso de 17 LEDs em brilho máximo exige muita corrente da saída \n5V do Arduino Nano chinês (limite máximo de 500mA via USB e 800mA via fonte externa conectada a \nVin). O LED com brilho máximo consome 60mA, portanto 17 LEDs com brilho máximo podem \nultrapassar 1 ampere. O ColorPalette usa apenas ¼ do brilho (64) máximo. Há um fusível nos \nreguladores de tensão que desliga temporariamente o fornecimento de energia se ela ultrapassar \nesses limites. \nO ideal é alimentar circuitos que tenham 7 ou mais LEDs WS2812 usando uma fonte externa. Nessa \nconfiguração, o Arduino fornece apenas o sinal de controle, através de seu pino de saída. A fonte \nexterna pode até ser a mesma bateria, se ela fornecer até 5V.   \nO esquema abaixo mostra como construir o mesmo circuito acima de maneira mais segura, usando \numa fonte externa. Com essa configuração, como a corrente não passará por dentro do Arduino, \npodemos acender os LEDs na intensidade máxima e alimentar muito mais LEDs: \n",
    "book_id": "introducaoeletronicaartistas",
    "book_title": "Microsoft Word - IntroduçãoEletronicaArtistas.docx",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 111
  },
  {
    "chunk_full": "Introdução a Eletrônica para Artistas \n \n \nc b a 2017 Helder da Rocha \n \n \n111\n \nNo esquema acima o Arduino pode estar sendo alimentado por USB ou por outra fonte a partir da sua \nentrada Vin. Mas é possível também usar a mesma fonte que fornece energia para os pixels WS2812, \nse ela tiver capacidade de fornecimento de corrente suficiente. Por exemplo, 3 pilhas AAA de 1,5V \ncada poderiam alimentar o Arduino e os LEDs.  \nCom 4,5V não seria possível alimentar o Arduino pelo pino Vin, já que o limite mínimo é de 6V. A \nalimentação do Arduino teria que ser feita diretamente via pino 5V. É preciso tomar cuidado ao \nutilizar esse pino como entrada, já que ele está ligado diretamente ao microcontrolador (sem fusível), \ne poderá queimar o Arduino se a tensão passar de 5,5V. \n \n \n \n",
    "book_id": "introducaoeletronicaartistas",
    "book_title": "Microsoft Word - IntroduçãoEletronicaArtistas.docx",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 112
  },
  {
    "chunk_full": "Introdução a Eletrônica para Artistas \n \n \nc b a 2017 Helder da Rocha \n \n \n112\n6. Técnicas  \nTécnicas, informações e links para construir obras eletrônicas permanentes (sem protoboard). \n6.1. Soldagem \nPara construir circuitos permanentes, a forma mais comum de unir fios e componentes é através da \nsoldagem. Os tutoriais abaixo demonstram a técnica da soldagem através de exemplos usando \ncomponentes do kit. \n6.1.1. Ferramentas para soldagem \nPara soldar circuitos eletrônicos o mínimo necessário é um ferro de soldar de baixa potencia (entre \n20 e 30W) e fio de solda (liga de estanho que será derretida pelo ferro). Além disso, é útil ter também \num pouco de pasta de solda (que ajuda na aderência da solda), um suporte para o ferro de soldar \ncom esponja (para limpar o bico), e um suporte com garras jacaré para fixar componentes e placas, \nque precisam estar firmes na hora da soldagem. Um desses suportes é chamado de terceira mão, e \ngeralmente contém também uma lupa para ampliar os objetos a serem soldados. \nA soldagem consiste no aquecimento da solda junto aos contatos a serem soldados pelo tempo \nmínimo necessário para que a solda derreta, criando a aderência e juntando as partes. Componentes \neletrônicos suportam calor por alguns poucos segundos. Se você errar a soldagem, o ideal é deixar \nesfriar e tentar remover a solda em uma segunda aplicação do ferro, para não danificar os \ncomponentes. Você também pode prender garras jacaré nos terminais durante a soldagem pois eles \nfuncionam como dissipadores e reduzem a quantidade de calor que chega ao componente. \n    \n    \n \nSe você vai soldar componentes mais sensíveis, o ideal é ter pelo menos uma estação de solda com \ncontrole de temperatura (foto abaixo, à esquerda). Um equipamento desses é um pouco mais caro, \nmas é necessário, principalmente se você precisar soldar componentes SMD, que são muito \npequenos e muito mais vulneráveis ao calor. \n                       \n \nSe você precisar soldar muitos componentes SMD em uma placa, o ideal é usar uma estação de \nretrabalho (foto acima, à direita), que contém ferro de soldar com temperatura controlada e \nferramentas de solda a vapor (sopradores térmicos) que permitem colar componentes na placa e \nsoldar com ar quente. Depois disso, ainda existem fornos que são usados para soldar vários \ncomponentes ao mesmo tempo. Uma estação de retrabalho também facilita a remoção de solda, que \nàs vezes é muito mais difícil que a soldagem em si. \n",
    "book_id": "introducaoeletronicaartistas",
    "book_title": "Microsoft Word - IntroduçãoEletronicaArtistas.docx",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 113
  },
  {
    "chunk_full": "Introdução a Eletrônica para Artistas \n \n \nc b a 2017 Helder da Rocha \n \n \n113\nMas existem alternativas mais baratas. Para remover solda aplicada em excesso, você pode passar \numa malha de cobre chamada de malha dessoldadora, que absorve a solda extra. Em circuitos muito \npequenos usar essa malha é inclusive uma técnica de soldagem: a solda é aplicada em excesso, para \nque se espalhe rapidamente, e depois ela é aquecida com a malha para remover o excesso. A solda \nadere apenas ao cobre, então a retirada separa as ilhas unidas pelo excesso de solda. Pode-se usar \ntambém um sugador de solda (abaixo, à direta), que utiliza vácuo para sugar a solda de um circuito \ndepois que ela é aquecida. \n      \n      \n \nPara isolar fios, depois de soldá-los, você pode usar fita isolante. Mas existem alternativas mais \nseguras e limpas. Uma delas é usar espaguete termo-retrátil, que é um tubo de plástico que diminui \nseu diâmetro pela metade quando aquecido. Depois de fazer a ligação, passe o tubo de espaguete \nsobre a conexão, e aqueça rapidamente com um isqueiro. O tubo irá se retrair e isolar a conexão. \n         \n \nAlternativas também incluem cola epóxi (ex: Araldite), cola quente e borracha de silicone, que é \nideal em circuitos flexíveis. Colas “puff” também podem ser usados em circuitos têxteis. \n6.1.2. Soldagem de terminais no LED endereçável WS2812 \nAntes de iniciar, prenda a placa dissipadora do LED em um lugar firme. Na foto abaixo usamos um \nsuporte para placas chamado de “terceira mão”, que tem uma lupa e duas garras jacaré. Mas você \ntambém pode usar outros meios, por exemplo, prender a placa numa mesa com fita crepe (deixando \nlivres as partes que serão soldadas) ou com fita dupla-face do outro lado (protegendo a lente do LED \ncom fita adesiva). Depois é importante fixar o fio a ser soldado sobre a superfície.  \n \nOs pontos de soldagem do LED WS2812 não precisam ser lixados. Eles já têm uma fina camada de \nsolda que irá facilitar a aderência. Primeiro derreta um pouco de solda na ponta do ferro. Depois \nencoste a solda no fio de um lado e o ferro de solda por alguns segundos do outro lado. Assim que \nderreter, afaste o ferro e espere secar. \n",
    "book_id": "introducaoeletronicaartistas",
    "book_title": "Microsoft Word - IntroduçãoEletronicaArtistas.docx",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 114
  },
  {
    "chunk_full": "Introdução a Eletrônica para Artistas \n \n \nc b a 2017 Helder da Rocha \n \n \n114\n \nA foto abaixo ilustra o resultado esperado. \nSe a solda for excessiva, limpe o \nferro de solda e depois encoste-o \nnovamente na junção, que ele \nabsorverá o excesso. \nContinue soldando os outros fios. \nReposicione a placa se necessário. \nPara usar o LED isoladamente só \né necessário fixar três terminais: \nDin, 5V e GND. Se você quiser \nconectar o LED a outros (para \nfazer \numa \nsequência) \nsolde \ntambém um terminal em Dout.  \nAs fotos a seguir ilustram o \nprocesso. \n \nNão se preocupe se você soldar dois terminais por engano. Limpe o ferro de solda e passe-o sobre a \njunção novamente para q ue ele absorva o excesso. Se não for suficiente, você pode aquecer a junção \ncom o ferro e uma malha de cobre dessoldadora para remover o excesso. \n \n",
    "book_id": "introducaoeletronicaartistas",
    "book_title": "Microsoft Word - IntroduçãoEletronicaArtistas.docx",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 115
  },
  {
    "chunk_full": "Introdução a Eletrônica para Artistas \n \n \nc b a 2017 Helder da Rocha \n \n \n115\n6.1.3. Soldagem de terminais na célula piezoelétrica \nA célula piezoelétrica não tem pontos destinados a soldagem. Para que a solda tenha a aderência \nnecessária para grudar na célula é necessário lixar um pouco a superfície. \nEscolha dois pontos na lateral da célula como mostrado abaixo. Prenda a célula em uma superfície \nfirme (ex: usando fita crepe). Lixe a superfície metálica, passe um pouco de pasta de solda e em \nseguida derreta um pouco de solda sobre ela.  \n  \n  \n \nDerreta um pouco de solda também na ponta do fio. Depois, encoste o fio no ponto onde deve ser \nsoldado, e use o ferro de solda para derreter a solda que está no fio e na célula. Assim que derreter, \nafaste o ferro e espere secar. \nFaça o mesmo na parte cerâmica, com mais cuidado pois a fina camada que permite a aderência da \nsolda pode desaparecer facilmente com o calor. \n \nVocê não precisa soldar fios na célula piezo elétrica. Se preferir, pode usar presilhas ou clipes que \nprendam fios firmemente em cada placa (eles devem estar bem isolados), usar fita adesiva de prata \nou cobre, ou usar um soquete especialmente criado para esse tipo de célula. Você pode também \nimprovisar o contato, durante os experimentos, com um pegador de roupa pequeno. \n6.1.4. Soldagem de terminais mais longos no microfone de eletreto \nO microfone de eletreto distribuído no kit tem terminais muito curtos que podem não encaixar muito \nbem no protoboard.  Para alongá-los, prenda o microfone firmemente em uma superfície (pode ser \nusando um suporte de placa (terceira mão) ou com uma fita crepe, perfurando a fita com os dois \nterminais, deixando-os livres para que possam ser soldados. \n   \n   \n \n",
    "book_id": "introducaoeletronicaartistas",
    "book_title": "Microsoft Word - IntroduçãoEletronicaArtistas.docx",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 116
  },
  {
    "chunk_full": "Introdução a Eletrônica para Artistas \n \n \nc b a 2017 Helder da Rocha \n \n \n116\nPasse um pouco de pasta de solda em cada terminal, e depois derreta um pouco de solda em cada um. \nFaça o mesmo na ponta de cada um dos fios que será soldado. \nSegure o fio paralelamente ao terminal a ser soldado. Se necessário, use um alicate ou pinça. Como o \nfio é rígido, você pode tentar prender o fio na mesa e curvá-lo de forma que ele fique na posição \nesperada. Aproxime o ferro de solda e derreta a junção. Se necessário, acrescente um pouco mais de \nsolda. \n     \n    \n \nSe o microfone não tiver terminais, você terá que soldá-los diretamente nas ilhas localizadas na \nparte traseira. Nem sempre a polaridade é indicada, mas se você olhar bem verá que um dos lados \nestá conectado à embalagem metálica do microfone. Este é o negativo. \nPara soldar, usamos o suporte “terceira mão” que possui garras jacaré para sustentar os microfone, \nque é muito pequeno, e também os fios. Escolha cores dos fios que ajudem a identificar a polaridade \n(preto para negativo, e vermelho para positivo.) Prenda um fio preto com a ponta desencapada de \nforma a encostar no terminal negativo. Depois esquente com o ferro de solda e um pouco de solda até \na solda derreter. Quando secar faça o mesmo com o terminal positivo. \n   \n   \n \nOutra maneira de acrescentar terminais longos no microfone é encaixá-lo em uma placa de circuito \nimpresso, e soldar fios mais longos (ou conectores do tipo “header”) para possibilitar o uso no \nprotoboard.  \nCorte um pedaço sua placa de circuito impresso universal contendo duas fileiras. O ideal é usar uma \nmini-serra, mas você pode vincar várias vezes, dos dois lados, com um estilete, e depois usar um \nalicate para cuidadosamente partir em duas metades. \n \nEncaixe o microfone em dois furos que estejam ligados a ilhas separadas, anotando qual deles é o \npositivo e qual o negativo (já que essa informação ficará oculta debaixo da placa). Encaixe fios preto e \nvermelho nas posições correspondentes. Prenda essa estrutura em uma superfície firme (usando fita \ncrepe) ou use um suporte de placa ou terceira mão,  se tiver. \n",
    "book_id": "introducaoeletronicaartistas",
    "book_title": "Microsoft Word - IntroduçãoEletronicaArtistas.docx",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 117
  },
  {
    "chunk_full": "Introdução a Eletrônica para Artistas \n \n \nc b a 2017 Helder da Rocha \n \n \n117\nPasse um pouco de pasta de solda nas conexões, depois derreta solda sobre as ilhas. Não se preocupe \nse ela for excessiva. Você pode sempre remover o excesso usando o ferro ou a malha dessoldadora. \n   \n \n6.1.5. Soldagem de terminais na célula fotovoltaica \nA célula fotovoltaica distribuída no kit não possui terminais soldados. Se você deseja fazer uma célula \nmaior, com 1,5V, 3V ou mais, precisará de células adicionais. Aproveite e adquira também fita de \nestanho (veja foto) que é mais adequada para soldar essas placas que são muito frágeis. \n \nPrenda a célula numa superfície deixando a ilha central exposta. Passe pasta de solda na fita de \nestanho, e posicione-a sobre a ilha na fotocélula, e encoste o ferro de solda por 10 segundos. Se a fita \nnão colar facilmente, derreta um pouco (muito pouco) de solda sobre ela e tente novamente. \nVire a placa ao contrário e faça o mesmo do outro lado. \nPara conectar várias placas em série, solde os terminais de baixo aos terminais colados na parte de \ncima da placa. A foto abaixo mostra uma configuração com três células, que gera até 1,5V. \n \nUsando 6 células, o conjunto é capaz de gerar até 3V (e ligar um motor, acionar uma cigarra ou \nacender um LED). \nAs células são muito frágeis, e a solda que une os contatos não é muito forte. Assim que você decidir \ncomo organizar as células, cole-as em uma superfície mais rígida (ex: cartão, plástico) e cubra com \numa camada de resina epóxi líquida, que servirá para protegê-las e também garantir isolamento. \n",
    "book_id": "introducaoeletronicaartistas",
    "book_title": "Microsoft Word - IntroduçãoEletronicaArtistas.docx",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 118
  },
  {
    "chunk_full": "Introdução a Eletrônica para Artistas \n \n \nc b a 2017 Helder da Rocha \n \n \n118\n6.1.6. Soldagem de terminais rígidos nos terminais do motor \nO motor distribuído no kit vem como fios flexíveis que não são muito fáceis de inserir no protoboard. \nPara torná-lo mais fácil de usar, podemos soldar fios rígidos nas pontas. \nPrimeiro é preciso prender os fios firmemente em uma superfície. Se você tiver um suporte do tipo \n“terceira mão” prenda o fio do motor em um jacaré, e o fio rígido em outro, e posicione-os da forma \nindicada abaixo. \n \nSe você não tiver uma terceira mão, use um cabo de garras jacaré que foram distribuídas no kit. \nDeslize a capa protetora de cada uma delas, para expor o jacaré inteiro, e prenda-os, com os fios a \nserem soldados, numa superfície com fita crepe, de forma que os jacarés fiquem a uma distância de \nalguns centímetros da mesa.  \nPasse um pouco de pasta de solda e depois derreta solda na junção. Se for excessiva, espere um pouco \ne derreta novamente com um ferro de solda limpo para tirar o excesso. A solda deverá ficar lisa e \nbrilhante.  \n \nQuando terminar você deve isolar a conexão. A melhor forma de fazer isto é usando 2cm de espaguete \ntermo-retratil, que é um tubo que encolhe com o calor. Use um tubo de 2mm e deslize sobre a junção, \ndepois aqueça rapidamente com um isqueiro para que o tubo encolha e isole a ligação. \n  \n \nSe você não tiver espaguete termo-retrátil, use fita isolante, cola quente, cola epóxi (Araldite) ou em \núltimo caso, fita adesiva (mas não deixe a junção sem isolamento). \nVocê pode usar o motor nos experimentos com protoboard sem precisar soldar. Prenda garras jacaré \nfirmemente em cada terminal (lembrando de baixar a capa isolante dos jacarés para evitar curtos-\ncircuitos), e na outra ponta prenda jumpers. Agora você pode inserir os jumpers no protoboard. \n6.1.7. Soldagem de um circuito na placa de fenolite universal \nVocê pode usar a placa de fenolite universal para fazer uma versão permanente de um circuito que \nvocê montou no protoboard. A configuração da placa é muito parecida com o protoboard e não é \ndifícil transportar circuitos para a placa. O tutorial abaixo (Sparkfun) mostra como soldar \ncomponentes em uma placa de circuito impresso: \nhttps://learn.sparkfun.com/tutorials/how-to-solder-through-hole-soldering   \n",
    "book_id": "introducaoeletronicaartistas",
    "book_title": "Microsoft Word - IntroduçãoEletronicaArtistas.docx",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 119
  },
  {
    "chunk_full": "Introdução a Eletrônica para Artistas \n \n \nc b a 2017 Helder da Rocha \n \n \n119\n6.2. Eletrônica para vestir (wearable electronics) \nA soldagem é ideal para circuitos rígidos, mas para circuitos têxteis que serão construídos em bolsas, \nsapatos, roupas e acessórios flexíveis, usando condutores como linha de costura condutiva, é preciso \nbuscar técnicas alternativas. Abaixo estão alguns links para tutoriais que demonstram essas técnicas, \ne algumas aplicações criativas. \n• \nComo costurar com linha condutiva (Sparkfun): \nhttps://learn.sparkfun.com/tutorials/lilypad-basics-e-sewing  \n• \nTécnicas de isolamento para circuitos de eletrônica para vestir (Sparkfun):  \nhttps://learn.sparkfun.com/tutorials/insulation-techniques-for-e-textiles  \n• \nPorta-pilhas de feltro: http://ohvillo.blogspot.com.br/p/e-textiles-portapilas.html e \nhttp://cosiriferampolles.blogspot.com.br/2014/10/tutorial-portapilas-de-fieltro.html  \n• \nSensor de pressão: http://www.instructables.com/id/Flexible-Fabric-Pressure-Sensor/  \n• \nJaqueta para ciclistas com LEDs e Arduino:  \nhttp://www.instructables.com/id/turn-signal-biking-jacket/ e \nhttp://www.instructables.com/id/Signaling-Cyclist-Jacket/ \n• \nSoutien Theremin com Arduino: https://www.youtube.com/watch?v=LmZJy8lvj5A \n• \nSapatilhas do Mágico de Oz:  \nhttps://ohvillo.wordpress.com/portfolio/zapatillas-el-mago-de-oz/  \n• \nAlto-falantes de tecido: http://www.kobakant.at/DIY/?p=2936  \n• \nSeção de wearables do site oficial do Arduino:  \nhttps://blog.arduino.cc/category/wearable-computing/ \n• \nSeção de wearables da AdaFruit: https://learn.adafruit.com/category/wearables \n• \nComo preparar um LilyPad:  \nhttps://ohvillo.wordpress.com/portfolio/primeros-pasos-con-el-lilypad-que-es-y-como-prepararlo/  \n6.3. Eletrônica com outros materiais \nOs links abaixo contém informações, produtos e tutoriais úteis para a construção de projetos usando \noutros materiais além de circuitos tradicionais rígidos e tecido. \n6.3.1. Material para circuitos de papel \nCanetas de tinta condutiva:  \n• \nCircuit Scribe. Caneta de tinta condutiva. (https://www.circuitscribe.com/).  \n• \nAgIC Circuit Maker. Caneta de tinta condutiva (https://agic.cc/en/). \nhttps://www.youtube.com/watch?v=4TnkmatWAiM \nhttps://www.youtube.com/watch?v=FAC3kqzWm4g \nAdesivos condutivos:  \n• \nCola epoxi condutiva:  \nhttp://www.mgchemicals.com/products/adhesives/electrically-conductive-adhesives/  \n• \nFita de cobre condutiva: http://produto.mercadolivre.com.br/MLB-788470458-fita-\nadesiva-de-cobre-condutiva-5mm-_JM  \n• \nFita de tecido de prata condutiva: http://produto.mercadolivre.com.br/MLB-800437426-\nfita-adesiva-de-tecido-com-prata-condutiva-10m-x-10mm-_JM  \n• \nStickers condutivos: https://www.crowdsupply.com/chibitronics/circuit-stickers  \n6.3.2. Massa condutiva \nReceitas, materiais e circuitos: \n• \nReceita: http://blog.novaeletronica.com.br/como-fazer-massa-condutiva/ \n• \nCremor de Tártaro (ingrediente usado em massa condutiva) \nhttp://www.americanas.com.br/produto/9781558/cremor-de-tartaro-com-50g-mix \n• \nCircuitos com massa condutiva e massa isolante \nhttp://courseweb.stthomas.edu/apthomas/SquishyCircuits/buildingCircuits.htm  \n",
    "book_id": "introducaoeletronicaartistas",
    "book_title": "Microsoft Word - IntroduçãoEletronicaArtistas.docx",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 120
  },
  {
    "chunk_full": "Introdução a Eletrônica para Artistas \n \n \nc b a 2017 Helder da Rocha \n \n \n120\n7. Componentes: referência rápida \nPara maiores informações, procure o datasheet ou manual na Internet (alguns produtos abaixo \ncontém o link), usando o código do componente na pesquisa. \n7.1. Componentes eletromagnéticos \n7.1.1. Relé \n \nSanyou DSY2Y-S-205L \n• \nDatasheet: http://www.sanyourelay.ca/public/products/pdf/DSY2Y.pdf  \n• \nVoltagem nominal: 5V \n• \nVoltagem mínima de acionamento: 3,75V \n• \nVoltagem de liberação: 4,75V \n• \nVoltagem máxima: 10V (não aplique mais de 10V) \n• \nCorrente de operação: 40mA \n• \nResistência: 125R \n• \nCarga nominal: 1A, 120V.  \n• \nCorrente máxima de chaveamento: 2A. \nEsquema de pinagem e conexões internas: \n \n7.1.2. Fonte de alimentação chaveada \n \n \n• \nTensão fornecida: 9V ou 12V (confira a indicação na parte de baixo da fonte) \n• \nCorrente máxima fornecida: 1 ampere (1 A) \n• \nSaída: plugue P4 (positivo no centro: \n) \n",
    "book_id": "introducaoeletronicaartistas",
    "book_title": "Microsoft Word - IntroduçãoEletronicaArtistas.docx",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 121
  },
  {
    "chunk_full": "Introdução a Eletrônica para Artistas \n \n \nc b a 2017 Helder da Rocha \n \n \n121\n7.1.3. Motor RF-300CA-09550 \n \n• \nFaixa de operação: 1 a 6V (não aplique mais de 6V) \n• \nValor nominal: 3V; velocidade 2700 rpm (sem carga), corrente 12mA \n• \nMáxima eficiência: 2100 rpm com 2,8 g*cm de torque, 60mW, 42mA \n• \nMotor parado: 13 g*cm de torque, 150mA \n• \nDatasheet: http://www.jameco.com/Jameco/Products/ProdDS/238458.pdf  \nEmbora não tão eficiente, este motor também pode ser usado como gerador. Se você colocar um LED \nentre os dois terminais, e der um giro rápido no eixo do motor com os dedos, deverá ver o LED \nacender brevemente. Um gerador mais eficiente (para acender LEDs em uma pipa, por exemplo) pode \nser construído usando um motor com torque maior (e menos RPM), e uma tensão nominal maior (ex: \n12 ou 24V). Para acender Leds e operar circuitos em energia contínua também é necessário construir \num circuito retificador na saída do gerador, com diodos e capacitores, pois a energia gerada é \nalternada. \n7.1.4. Buzzer (cigarra) ativo 5V \n \n• \nTensão nominal: 5V \n• \nFaixa de operação: 3 a 8 V (não aplique mais de 8V) \n• \nSaída de som: 80 dB \n• \nCorrente máxima: 30mA \n• \nFrequência média: 2,7kHz (+/-500Hz) \n7.1.5. Mini microfone de eletreto \n \n \n• \nSensibilidade: -38db +/- 2dB \n• \nImpedância: 2,2k ohms \n• \nFrequência: 20Hz a 16kHz \n• \nOperação: 1 a 10V \n• \nValor típico de operação: 3V \n• \nCorrente máxima: 500mA \n7.1.6. Mini alto-falante \n \n• \nImpedância: 8 ohms \n• \nPotência nominal: 0,5W \nPara usar, é necessário soldar fios ou jumpers nos terminais. Não ligue diretamente em corrente \ncontínua, pois a bobina poderá esquentar e queimar. Não há polaridade. \n",
    "book_id": "introducaoeletronicaartistas",
    "book_title": "Microsoft Word - IntroduçãoEletronicaArtistas.docx",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 122
  },
  {
    "chunk_full": "Introdução a Eletrônica para Artistas \n \n \nc b a 2017 Helder da Rocha \n \n \n122\n7.2. Resistores e capacitores \nTodos os resistores são de ¼ W, exceto o de 10 ohms que suporta até 1W. Todos os capacitores \nsuportam pelo menos 16V. \nResistores \nCapacitores \n \n10 Ω \nmarrom-preto-preto-ouro (100 * 0,1) \n47 Ω \namarelo-violeta-preto-ouro (470 * 0,1) \n100 Ω \nmarrom-preto-marrom (10 * 10) \n220 Ω \nvermelho-vermelho-marrom (22 * 10) \n330 Ω \nlaranja-laranja-marrom (33 * 10) \n470 Ω \namarelo-violeta-marrom (47 * 10) \n680 Ω \nazul-cinza-marrom (68 * 10) \n1k Ω (1000 Ω) \nmarrom-preto-vermelho (10 * 100) \n2,2k Ω \nvermelho-vermelho-vermelho (22 * 100) \n3,3k Ω \nlaranja-laranja-vermelho (33 * 100) \n4,7k Ω \namarelo-violeta-vermelho (47 * 100) \n6,8k Ω \nazul-cinza-vermelho (68 * 100) \n10k Ω (10 000 Ω) \nmarrom-preto-laranja (10 * 1000) \n22k Ω \nvermelho-vermelho-laranja (22 * 1000) \n33k Ω \nlaranja-laranja-laranja (33 * 1000) \n47k Ω \namarelo-violeta-laranja (47 * 1000) \n68k Ω \nazul-cinza-laranja (68 * 1000) \n100k Ω (100 000 Ω) \nmarrom-preto-amarelo (10 * 10000) \n220k Ω \nvermelho-vermelho-amarelo (22 * 10000) \n330k Ω \nlaranja-laranja-amarelo (33 * 10000) \n470k Ω \namarelo-violeta-amarelo (47 * 10000) \n560k Ω \nverde-azul-amarelo (56 * 10000) \n680k Ω \nazul-cinza-amarelo (68 * 10000) \n1M Ω (1 000 000 Ω) \nmarrom-preto-verde (10 * 100000) \n2,2M Ω \nvermelho-vermelho-verde (22 * 100000) \n3,3M Ω \nlaranja-laranja-verde (33 * 100000) \n100pF \n1nF (1000pF, 1kpF) \n \n3,3nF (3,3kpF) \n \n4,7nF (4,7kpF) \n10nF (10kpF, 0,01 µF) \n \n22nF (22kpF, 0,022 µF) \n \n47nF (47kpF, 0,047 µF) \n100nF (0,1 µF) \n \n330nF (0,33 µF) \n \n470nF (0,47 µF) \n1 µF \n \n2,2 µF \n \n3,3 µF \n \n4,7 µF \n \n10 µF \n \n22 µF \n \n33 µF \n47 µF \n \n100 µF \n \n220 µF \n \n470 µF \n \n1000 µF \n \n2200 µF \n",
    "book_id": "introducaoeletronicaartistas",
    "book_title": "Microsoft Word - IntroduçãoEletronicaArtistas.docx",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 123
  },
  {
    "chunk_full": "Introdução a Eletrônica para Artistas \n \n \nc b a 2017 Helder da Rocha \n \n \n123\n7.3. Semicondutores \n7.3.1. Transistores bipolares de junção \nNPN de propósito geral: BC548/548/549 ou 2N3904/2N2222 \n• \nCorrente de coletor típica: 100mA \n• \nGanho típico: 100-300 (quantidade de vezes que a corrente na base é amplificada) \n• \nTensão máxima entre base e emissor: 5V \n• \nTensão máxima entre coletor e emissor: 30V \n• \nDatasheet, série BC: http://cygnus.et.put.poznan.pl/~kklima/aue/BC549_550.pdf \n• \nDatasheet, série 2N: https://www.sparkfun.com/datasheets/Components/2N3904.pdf \n \nPNP de propósito geral: BC557/558/559 ou 2N3906 \nMesmas especificações gerais dos correspondentes NPN. \nDatasheets:  \n• \nBC: https://cdn.instructables.com/ORIG/FYG/F1O8/II0K92HF/FYGF1O8II0K92HF.pdf \n• \n2N: https://www.sparkfun.com/datasheets/Components/2N3906.pdf \n \n7.3.2. Transistores de efeito de campo (MOSFETs) \nMOSFET de propósito geral: 2N7000  \nDatasheet: https://www.onsemi.com/pub/Collateral/2N7000-D.PDF \n \nMOSFET de potência: IRF540 \nDatasheet: http://www.vishay.com/docs/91021/91021.pdf \n \n",
    "book_id": "introducaoeletronicaartistas",
    "book_title": "Microsoft Word - IntroduçãoEletronicaArtistas.docx",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 124
  },
  {
    "chunk_full": "Introdução a Eletrônica para Artistas \n \n \nc b a 2017 Helder da Rocha \n \n \n124\n7.3.3. Diodos \nDiodo de propósito geral: 1N4148 \nEstes diodos são tipicamente usados em circuitos retificadores (para converter sinais de \ncorrente/tensão alternada), para proteção contra pulsos de corrente reversa (em circuitos com \nmotores, relés e transformadores) e em qualquer situação onde for necessário que a corrente flua \napenas em um sentido (nos astáveis 555, para controlar a largura dos pulsos). \nDatasheet: http://www.vishay.com/docs/81857/1n4148.pdf \n                     \n \nDiodo regulador de tensão: 1N4728A – Diodo Zener \nDatasheeet: http://www.vishay.com/docs/85816/1n4728a.pdf \n       \n \nEste diodo mantém uma tensão constante sobre ele mesmo quando a tensão de entrada varia. É usado \nem reguladores de tensão (para garantir que a tensão em uma carga não ultrapasse um determinado \nvalor). A tensão mínima de entrada deve ser de 1 a 2V maior que a tensão de saída. É necessário \ncalcular o valores da resistência e potência do resistor que fará um divisor de tensão com o diodo, e \ntambém a potência suportada pelo diodo zener. \n \nCalculadora online: http://ncalculators.com/electronics/zener-diode-calculator.htm \nÉ muito importante que potências do diodo e resistor usados no circuito sejam iguais ou superiores \naos valores calculados para evitar que esquentem demais.  \nO diodo zener1N4728 incluído no kit é calibrado para regular 3,3V e tem potencia máxima de 1W. A \nmaior parte dos resistores incluídos no kit são de ¼ W (0,25W). \n7.3.4. LEDs  \nLEDs são diodos que emitem luz. Há vários diferentes tipos de LED no kit. \n(Wikimedia) \nTutorial da Sparkfun: LEDs: https://learn.sparkfun.com/tutorials/light-emitting-diodes-leds.  \n",
    "book_id": "introducaoeletronicaartistas",
    "book_title": "Microsoft Word - IntroduçãoEletronicaArtistas.docx",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 125
  },
  {
    "chunk_full": "Introdução a Eletrônica para Artistas \n \n \nc b a 2017 Helder da Rocha \n \n \n125\nDisplay de LED com 7 segmentos HS5161AS \n• \nCatodo comum (o catodo de cada LED é interligado e acessível nos terminais centrais) \n• \nTensão direta típica: 1,8V (esta é a queda de tensão em cada LED) \n• \nTensão reversa máxima: 5V \n• \nCorrente máxima por segmento: 20mA \n• \nCorrente de operação recomendada: 12mA (calcule o resistor usando esta corrente) \n• \nDatasheet: http://www.dipmicro.com/?datasheet=TOS-5161AS.pdf \n \nLED 5mm de alto-brilho \n• \nTensão direta:  \no \nBranco, Azul, Verde, Rosa: 3,2V;  \no \nVermelho, Amarelo: 2V;  \no \nVioleta: 3,4V \n• \nCorrente máxima de operação: 20mA \n• \nTensão/corrente reversa: 5V/10uA \n• \nMáxima corrente pulsada (0,1ms): 100mA \nLED 5mm vermelho difuso \n• \nTensão direta: 2,2V \n• \nCorrente máxima de operação: 20mA \n• \nTensão/corrente reversa: 5V/10uA \n• \nMáxima corrente pulsada (0,1ms): 100mA \nLED 5mm RGB de anodo comum \n \n(fonte: arduino-info.wikispaces.com) \n• \nTensão/corrente reversa: 5V/10uA \n• \nTensão direta (20mA): R 2V, G 3,2V, B 3,2V \n• \nCorrente direta de operação: 20mA/20mA/20mA (60mA) \nLED SMD RGB 5050 \n \n• \nAnodo (+): terminais 1, 2, 3; Catodo (-): terminais 4, 5, 6 \n• \nTensão/corrente reversa: 5V/10uA \n• \nTensão direta (20mA): R 2V, G 3,2V, B 3,2V \n• \nCorrente direta de operação: 20mA/20mA/20mA (60mA) \n",
    "book_id": "introducaoeletronicaartistas",
    "book_title": "Microsoft Word - IntroduçãoEletronicaArtistas.docx",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 126
  },
  {
    "chunk_full": "Introdução a Eletrônica para Artistas \n \n \nc b a 2017 Helder da Rocha \n \n \n126\nLED 5050 WS2812 (NeoPixel)  \n \nEste é um LED RGB 5050 contendo um chip endereçável. Ele não acende com alimentação direta, mas \nrequer um pulso de largura definida que controla qual a cor a ser exibida. Tipicamente é usado em \ncircuitos Arduino, conectando o pino Din a um pino de saída digital do Arduino. O pino Dout é usado \nquando vários LEDs WS2812 são conectados em série. \n• \nDatasheet: https://cdn-shop.adafruit.com/datasheets/WS2812.pdf \n• \nTutoriais: https://learn.sparkfun.com/tutorials/ws2812-breakout-hookup-guide/all.pdf \n      \n   https://www.tweaking4all.com/hardware/arduino/arduino-ws2812-led/  \n   \n   https://learn.adafruit.com/adafruit-neopixel-uberguide/power  \nConecte 5V no positivo, GND no negativo, DI em qualquer pino Arduino (entrada). Se for usar em série \ncom outro, conecte o DO/Dout ao DI/Din do LED seguinte. \n                \n \n• \nTensão nominal: 5V; Tensão máxima absoluta: 5,5V (não ultrapasse esta tensão) \n• \nUsar resistor de 470 ohms entre o Arduíno e primeiro LED. Usar capacitor de 1000uF entre \nterminais de alimentação. Cada LED consome 60mA no brilho máximo. Não alimente (Pino \n5V) mais que 7 LEDs via Arduino (use uma fonte externa). \n7.3.5. Circuitos integrados e outros semicondutores \nL7805CV – Regulador de tensão \nEste componente é usado para limitar a tensão em circuitos que requerem no máximo 5V. Tem \nfuncionamento similar ao diodo zener. Ele permite que circuitos desse tipo sejam alimentados por \nbaterias ou fontes com mais de 5V (o Arduino possui reguladores similares, para 3,3 e 5V). Este \nregulador garante a tensão na saída de 5V desde que a entrada tenha pelo menos 7V. \n• Entrada: 7 a 15V \n• Saída 5V \n• Corrente: 500mA \n• Datasheet: http://www.mouser.com/ds/2/389/l78-974043.pdf  \n \n \nCircuito com exemplo de uso (reduzindo de 9V para 5V): \n \n",
    "book_id": "introducaoeletronicaartistas",
    "book_title": "Microsoft Word - IntroduçãoEletronicaArtistas.docx",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 127
  },
  {
    "chunk_full": "Introdução a Eletrônica para Artistas \n \n \nc b a 2017 Helder da Rocha \n \n \n127\nLM 555 CN – Temporizador multiuso de propósito geral \nO 555 é um circuito integrado simples, barato e versátil que serve para construir diversos circuitos \nfundamentais da eletrônica digital, como alternadores de estado (flip-flop bi-estável), temporizadores \n(cronômetro monoestável) e geradores de pulsos (clock) em onda quadrada (multivibrador astável). \nDatasheet: https://www.diodes.com/assets/Datasheets/NE555_SA555_NA555.pdf \n \nVeja exemplos de uso nos experimentos da apostila. \nCD 4017 BD – Contador de década \nProduz valor lógico alto nos pinos Q0 a Q9 sequencialmente a cada pulso recebido no pino 14. \nReinicia o contador com nível alto no pino 15. Desliga o contador com nível alto no pino 13. \nDatasheet: http://www.ti.com/lit/ds/symlink/cd4017b.pdf \n \nVeja exemplo de uso nos experimentos da apostila. \nCD 4026 BE – Contador de década com decodificador para display de 7 segmentos. \nProduz uma sequência de valores entre 0 e 9 codificados em pinos de saída especificamente \nconfigurados para acender displays de 7 segmentos. \nDatasheet: http://www.ti.com/lit/ds/symlink/cd4026b.pdf \n \nVeja exemplo de uso nos experimentos da apostila. \n",
    "book_id": "introducaoeletronicaartistas",
    "book_title": "Microsoft Word - IntroduçãoEletronicaArtistas.docx",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 128
  },
  {
    "chunk_full": "Introdução a Eletrônica para Artistas \n \n \nc b a 2017 Helder da Rocha \n \n \n128\n7.4. Sensores \n7.4.1. LDRs genéricos de 5 e 7mm (valores típicos) \nSensores resistivos difusos cuja resistência diminui com o aumento da intensidade luminosa. \n \n• \nResistência no escuro: tipicamente 500k a 1M ohms \n• \nResistência no claro: tipicamente 50 a 100 ohms \n7.4.2. Termistor genérico NTC 10k típico \nSensor resistivo cuja resistência diminui com o aumento da temperatura, e aumenta com a redução. \nSua resistência é de 10k ohms em temperatura ambiente de 25 graus Celsius. \n \nTemperatura: Resistência  \n• \n0 °C: 36k Ω \n• \n15 °C: 16k Ω \n• \n20 °C: 13k Ω \n• \n25 °C: 10k Ω \n• \n30 °C: 8k Ω \n• \n35 °C: 6k Ω \n• \n50 °C: 3k Ω \n• \n100 °C: 550 Ω \n7.4.3. LM35DZ – Termômetro de precisão \nO LM35 é um circuito integrado que funciona como sensor de temperatura de precisão, produzindo \numa variação bem definida de tensão entre seus terminais de acordo com a temperatura. \nDatasheet: http://www.ti.com/lit/ds/symlink/lm35.pdf \nCada 10mV (entre a perna OUT e GND) corresponde a 1 grau Celsius. Vcc pode ser 5 a 15V. Este \nmodelo não mede temperaturas negativas. A margem de erro é de 0,5 graus Celsius na faixa entre 2 e \n100 graus Celsius. \nPode-se construir um termômetro de LEDs usando um circuito integrado comparador (como o \nLM3914, que não foi incluído no kit), ou, de forma muito simples ligando o pino central a uma entrada \nanalógica do Arduino, no qual pode-se obter a tensão e calcular a temperatura.  \n \n \n",
    "book_id": "introducaoeletronicaartistas",
    "book_title": "Microsoft Word - IntroduçãoEletronicaArtistas.docx",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 129
  },
  {
    "chunk_full": "Introdução a Eletrônica para Artistas \n \n \nc b a 2017 Helder da Rocha \n \n \n129\n7.4.4. Fototransistor TIL 78 \nO fototransistor incluído no kit tem uma embalagem idêntica a um LED (mas foi distribuído no kit \nentre os transistores.) Se você tentar acender um LED transparente com pernas longas e ele não \nfuncionar, não jogue fora. Pode ser um fototransistor! \n \n(http://eletronicassim.blogspot.com.br/) \n7.4.5. Chave/sensor magnético reed \nO reed é uma chave normalmente aberta que fecha com a aproximação de um imã. \n     \n \n• \nTensão máxima de operação: 200V \n• \nCorrente máxima de operação: 0,5 A \nEvite dobrar os terminais, pois a ampola é muito frágil. Se necessário, dobre os terminais \ncuidadosamente com alicates de ponta fina, e não dobre muito perto do vidro, ou solde fios mais \nlongos diretamente nos terminais. Dependendo de onde for usado, o reed pode ser protegido \nsoldando os terminais em uma placa, ou envolvendo a ampola com resina epóxi. \n7.4.6. Célula piezoelétrica \n \nO sensor piezoelétrico é sensível à deformação, gerando pulsos de corrente alternada quando há \ncompressão ou expansão do material cristalino. Isto pode ser provocado por pressão, aceleração, \nimpactos, tensão, força. O efeito piezoelétrico foi descoberto por Pierre Curie em 1880, mas \naplicações práticas só começaram a surgir na segunda metade do século 20. Sensores piezoelétricos \nsão hoje usados em diversas aplicações, inclusive como forma alternativa de geração de energia. \nO sensor incluído no kit gera pulsos de alguns volts para cada deformação (ex: impacto ou pressão no \ncentro do sensor). Um impacto é suficiente para piscar um LED mas a potência não é suficiente para \nqueimá-lo. \n",
    "book_id": "introducaoeletronicaartistas",
    "book_title": "Microsoft Word - IntroduçãoEletronicaArtistas.docx",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 130
  },
  {
    "chunk_full": "Introdução a Eletrônica para Artistas \n \n \nc b a 2017 Helder da Rocha \n \n \n130\n7.4.7. Célula fotovoltaica de 0,5V (silício policristalino) \nÉ uma célula base para painéis solares. Esta célula é muito frágil. É como vidro superfino. Manuseie \ncom cuidado. \n        \n \n• \nTensão média em luz solar direta: 0,5V \n• \nPotência média: 0,20W \n• \nCorrente máxima fornecida: 400mA \n• \nEficiência: 15-20% \nEsta célula gera apenas 0,5V no máximo. É necessário soldar 10 células em série para gerar 5V e \nalimentar um Arduino. Para acender um LED são necessárias 4 a 6 células em luz solar direta. Duas ou \ntrês células já permitem acionar a cigarra de 5V ou o motor de 3V incluídos no kit. A solda deve ser \nfeita com terminais de estanho. Há um mini-tutorial nesta apostila (e vários tutoriais na Internet \nexplicando como fazer). Depois de soldar, proteja com uma camada de resina epóxi líquido.  \nVocê também pode adquirir painéis prontos com 5V ou mais, além de células flexíveis. Para aplicações \npráticas, normalmente conecta-se o painel a uma placa mini-carregador de baterias, conectado a \nbaterias recarregáveis, para que o circuito alimentado funcione quando não há luz. Esse tipo de \nconfiguração pode ser usada para alimentar um Arduino. \n7.5. Imãs \n7.5.1. Imã de neodímio N24 10x4mm \n \nEste é um imã de terras raras. Os imãs de neodímio Dentre os imãs de neodímio, N24 (24 mega-\ngauss) é um dos mais fracos. Existem imãs N35, N42 e N52 que são bem mais fortes. Este imã é forte o \nsuficiente para acionar sensores com vários centímetros de distância. É ideal para experimentar com \nsolenoides e alto-falantes em wearables (circuitos vestíveis). \nNão aproxime este imã de dispositivos magnéticos, celulares, discos rígidos, etc. Evite impactos, pois a \nproteção metálica externa pode rachar, e o material interno é frágil. \n7.5.2. Imã de ferrite 10x4mm \n \nImã comum “de geladeira”. Pode ser usado para acionar chaves magnéticas, mas é bem mais fraco que \no de neodímio e precisa estar bem próximo para funcionar. Evite impactos, pois o material do imã \nquebra com facilidade. \n7.5.3. Eletroimã \nUm parafuso de 7mm foi incluído no kit para experimentos com eletromagnetismo. Para fazer um \neletroímã enrole umas 200 espiras de fio (de preferencia fio esmaltado fino 28 AWG) no parafuso, e \naplique tensão (pode ser 9V) para magnetizar temporariamente o parafuso, fazendo-o funcionar \ncomo um imã.  \n",
    "book_id": "introducaoeletronicaartistas",
    "book_title": "Microsoft Word - IntroduçãoEletronicaArtistas.docx",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 131
  },
  {
    "chunk_full": "Introdução a Eletrônica para Artistas \n \n \nc b a 2017 Helder da Rocha \n \n \n131\n7.6. Ferramentas e acessórios  \n7.6.1. Multímetro DT830B \nEste modelo de multímetro é distribuído por vários fabricantes diferentes (com qualidade que \ntambém varia bastante). Ele é muito barato e simples, mas é suficiente para usar no curso para medir \ntensão e resistência. Ele não tem boa precisão para valores muito baixos de tensão e resistência, e \nalguns modelos infelizmente não têm fusível no medidor de corrente (apesar do manual e embalagem \ndizer que têm).  \n \nFonte: http://www.explicofacil.com/2014/05/como-utilizar-un-multimetro-o-tester-es.html \nAlguns cuidados básicos são: \n• \nVerifique o que será medido antes, e posicione o seletor na área correspondente. Para \nmedir tensões contínuas, inicie posicionando o seletor em posição correspondente a tensão \nmaior que a alimentação do circuito, e gire o seletor baixando o valor para melhor a \nprecisão, se necessário. \n• \nVocê pode usar o multímetro para medir tensão alternada da rede elétrica, mas é preciso \nposicionar o seletor na seção correspondente a tensão alternada (750 ou 200V) e não tocar \nnas pontas metálicas das pontas de prova, já que há risco de choque. \n• \nA medição de resistência deve ser feita apenas com circuitos desligados. Idealmente, \napenas para componentes individuais. Se o circuito contiver capacitores, descarregue-os \nantes (faça seus terminais tocarem). \n• \nA medição de tensão é feita sem abrir o circuito. Apenas uma pequena parte de corrente \nentra no multímetro. Coloque o multímetro em paralelo com o componente a ser medido. \n• \nA medição de corrente é a que oferece o maior risco, pois requer que toda a corrente do \ntrecho a ser medido passe por dentro do multímetro. Até um pulso breve de alta corrente \npode ser excessivo. Coloque o multímetro em série com o trecho a ser medido. No seletor de \nbaixas correntes, o máximo que o multímetro suporta é 200mA. Mais do que isto pode \n",
    "book_id": "introducaoeletronicaartistas",
    "book_title": "Microsoft Word - IntroduçãoEletronicaArtistas.docx",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 132
  },
  {
    "chunk_full": "Introdução a Eletrônica para Artistas \n \n \nc b a 2017 Helder da Rocha \n \n \n132\nqueimar o fusível do medidor de corrente. Para evitar a queima, primeiro calcule o máximo \nde corrente que pode passar no trecho a ser medido e sempre comece medindo na posição \n10 A. Isto requer o encaixe da ponta de prova vermelha no primeiro soquete, e mover o \nseletor para a posição 10A. Se o valor estiver muito baixo, e menos de 0,2 A \n(preferencialmente menos de 0,1 para maior segurança), mova a ponta de prova para o \nsegundo soquete e gire o seletor para a posição 200m, para ter melhor resolução. Gire o \nseletor para valores menores se necessário.  \n• \nEvite usar as posições de baixa corrente para medir corrente em circuitos com relés, \nmotores e transformadores. Mesmo que consumam baixas correntes, esses componentes \nprovocam breves pulsos muito altos de corrente (acima de 200mA) que podem queimar o \nfusível mesmo antes que qualquer medição seja feita. \nO multímetro DT830B deve ter um fusível que irá se romper se uma corrente maior que 0,2 A for \naplicada no multímetro. Se for um de melhor qualidade, você pode abrir o multímetro e trocar o \nfusível se ele queimar, por outro de 0,2 A. Mas alguns fabricantes ou soldam o fusível no lugar, ou \nmesmo substituem ele por um fio (ou seja, não há fusível). Neste caso, se o amperímetro queimar, \nvocê não conseguirá mais usá-lo (mas é possível que as funções de voltímetro e ohmímetro \ncontinuem a funcionar). \nEste multímetro é vendido por valores que variam de 12 a 50 reais. Se for comprar outro, verifique se \nele permite a troca do fusível e da bateria. Os muito baratos costumam ser quase descartáveis. \nExistem também outros modelos (mais caros) que têm melhor precisão na medição, medem \nresistências maiores, apitam para medir continuidade, além de medirem frequência e capacitância. \n7.6.2. Protoboard de 830 pontos  \nO protoboard (também chamado de breadboard ou base de prototipagem) incluído no kit \ncorresponde a duas vezes o esquema abaixo (60 linhas) e tem as trilhas laterais interrompidas no \nmeio (nem todos têm essa interrupção, e alguns têm mais de uma). \n \nPara interligar um componente nas colunas centrais (a-j), coloque seus terminais em linhas \ndiferentes (interligar um componente na mesma linha numerada é juntar todos os seus terminais no \nmesmo ponto; ligar uma bateria assim causa um curto-circuito). \nUse as laterais para conexões que têm muitas ligações. Normalmente duas laterais são usadas para as \nentradas positivo e negativo da bateria. Alguns protoboards vêm com essas colunas marcadas com “+” \ne “-“. Uma boa prática é usar apenas uma fila de cada lado, e escolher um lado para o positivo e outro \npara o negativo, para evitar o risco de inverter polaridades.  \nVeja mais neste tutorial (Sparkfun): https://learn.sparkfun.com/tutorials/how-to-use-a-breadboard.  \n",
    "book_id": "introducaoeletronicaartistas",
    "book_title": "Microsoft Word - IntroduçãoEletronicaArtistas.docx",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 133
  },
  {
    "chunk_full": "Introdução a Eletrônica para Artistas \n \n \nc b a 2017 Helder da Rocha \n \n \n133\n7.6.3. Placa de circuito impresso universal \nNão existe um padrão para este tipo de placa. O desenho abaixo corresponde à placa incluída no kit. \nUse as fileiras do meio para os terminais positivo e negativo (correspondentes às laterais do \nprotoboard) e as outras para encaixar componentes e circuitos integrados (que cabem perfeitamente \nno centro. \n \nInsira os componentes pelo lado que não tem cobre, e solde os terminais do lado do cobre.  \n \nA solda adere com facilidade ao cobre. Verifique as conexões antes, pois é mais trabalhoso remover o \ncomponente depois de soldado. Após a soldagem, corte os terminais. \nVocê também pode usar uma placa perfurada (sem cobre) ou mesmo perfurar superfícies de plástico, \ncartão, tecido, couro e madeira para montar circuitos, soldando, amarrando ou fixando os terminais \ndo outro lado. É muito importante que as conexões sejam firmes (para evitar mal contato) e bem \nisoladas (para evitar contatos indevidos e curto-circuitos que podem alterar o funcionamento ou até \ndanificar o circuito). \n \n",
    "book_id": "introducaoeletronicaartistas",
    "book_title": "Microsoft Word - IntroduçãoEletronicaArtistas.docx",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 134
  },
  {
    "chunk_full": "Introdução a Eletrônica para Artistas \n \n \nc b a 2017 Helder da Rocha \n \n \n134\n8. Mini-referência de Arduino \nEsta seção contém um resumo das especificações e programação da placa Arduino Nano, que faz \nparte do kit. Veja mais informações e exemplos no capítulo sobre Projetos com Arduino. \n8.1. Programação \nAs seções abaixo contém apenas tópicos de programação que foram abordados na apostila. Para uma \nreferência mais completa e abrangente, consulte o site oficial: http://arduino.cc. \n8.1.1. Sintaxe básica \nUm programa Arduino é escrito na linguagem Processing (derivada de C++) e chamado de sketch. Um \nsketch é armazenado no computador como uma pasta que contém os arquivos que compõem o \nprograma. Um sketch possui no mínimo um arquivo, gravado com a extensão .ino, mas pode conter \noutros, por exemplo, arquivos .h. \nA estrutura básica de um sketch consiste da declaração das funções loop() e setup() que são \nchamadas automaticamente pelo Arduino. A função setup() é chamada uma vez e a função loop() é \nchamada dentro de uma repetição infinita. Para escrever um programa para Arduino, instruções e \nchamadas de outras funções devem ser declaradas dentro das definições de loop() e setup(). \nO sketch mínimo está listado abaixo.  \nvoid setup() { \n \n} \n \nvoid loop() { \n \n} \nÉ importante fazer upload do sketch mínimo antes de construir o circuito de um Arduino, para testar \na comunicação, e para garantir a segurança dos pinos (eles são todos inicializados como entradas de \nalta-impedância). \nInstruções terminam sempre em ponto-e-vírgula. Definição de funções, estruturas de repetição e \nestruturas condicionais têm parênteses para declarar parâmetros, e um bloco delimitado por chaves \npara declarar outras instruções. \nVariáveis e constantes \nVariáveis são declaradas dentro ou fora de funções. Se declaradas fora das funções são variáveis \nglobais e podem ser usadas em qualquer lugar. Se declaradas dentro das funções, como nos \nparâmetros de uma definição de função, só podem ser usadas dentro da função. \nA declaração de variáveis consiste do seu tipo e de um nome, usado como identificador: \nint pino; \nA variável pode ter um valor inicial, que é atribuído a ela pelo operador “=” de atribuição: \nint  pino = 5; \nSe a declaração de uma variável for precedida pela palavra reservada “const”, ela será tratada como \numa constante e não poderá ser mais modificada.  \nconst int pino = 5; \nVariáveis pode ser usadas para gravar vários tipos de dados. Nesta apostila usamos variáveis para \nguardar números de pinos (que são inteiros), valores decimais (ponto-futuante) e outros valores \ninteiros. Para isto declaramos as variáveis como sendo dos tipos int (inteiro) e float (decimais). \n",
    "book_id": "introducaoeletronicaartistas",
    "book_title": "Microsoft Word - IntroduçãoEletronicaArtistas.docx",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 135
  },
  {
    "chunk_full": "Introdução a Eletrônica para Artistas \n \n \nc b a 2017 Helder da Rocha \n \n \n135\nAtribuição \nA operação de atribuição copia o valor do lado direito do sinal de “=” para a variável do lado \nesquerdo.  \nfloat tensao = 12.5; \nEssa operação também acontece quando uma função é chamada com parâmetros. Os valores dos \nparâmetros na chamada da função são copiados para as variáveis declaradas na definição da função. \nPor exemplo, para usar a definição de função abaixo: \nvoid mostrarTensao(float tensao) { \n    Serial.println(tensao); \n} \nFazemos uma chamada da forma: \nimprimirTensao(12.5); \nque é equivalente à atribuição mostrada anteriormente (o valor 12.5 será copiado para a variável \ntensao). \nDurante uma atribuição, o lado direito da expressão é calculado primeiro, e quaisquer variáveis são \nsubstituídas pelos valores que contém. Por isso é possível fazer expressões como: \nint x = 5; \nx = x + 1; \nOnde o x do lado direito é substituído pelo valor anterior de x (5), depois é realizada a soma de 5 + \n1, e por fim o novo valor (6) é atribuído à variável x. \nOperações aritméticas \nOperações aritméticas podem ser feitas envolvendo números e variáveis (que contém números). \nTodas funcionam com tipos float. Divisões entre inteiros requerem que um dos números seja float. \nIsto pode ser feito acrescentando um ponto-decimal e um zero: \nfloat resultado  = 12.0 / 10; \nAs operações básicas são soma (+), subtração (-), multiplicação (*) e divisão (/). Nesta apostila \ntambém usamos a operação de incremento: x++, que é o mesmo que fazer x = x + 1. \nExistem várias outras operações aritméticas que o Arduino suporta, além de funções matemáticas \núteis. Consulte a documentação do Arduino no site arduino.cc para mais detalhes. \nOperações de lógica relacional e booleana \nAs principais operações lógicas relacionais são igual (==), diferente (!=), maior-que (>), menor-que \n(<), maior ou igual (>=) e menor ou igual (<=). Elas geralmente são usadas em parâmetros ou blocos \nque esperam expressões condicionais, como blocos if() e a segunda parte da declaração de uma \nrepetição for(): \nif(digitalRead(6) != HIGH) { ... } \n \nfor(int i = 0; i < analogRead(0); i++) { ... } \nValores lógicos podem ser conectados formando expressões maiores usando proposições OU (OR) e E \n(AND), que são representados pelos símbolos || e &&, respectivamente. Por exemplo: \nif(x >= 5 && y == 9) { ... }   \nExecuta o bloco se x for maior que 5 e se y for igual a 9. \nif(digitalRead(2) == HIGH || analogRead(2) > 512) { ... }  \nExecuta o bloco se o valor lido no pino digital D2 for 5V, ou se o valor lido no pino analógico A2 for \n2,5V (metade de 5V). \n",
    "book_id": "introducaoeletronicaartistas",
    "book_title": "Microsoft Word - IntroduçãoEletronicaArtistas.docx",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 136
  },
  {
    "chunk_full": "Introdução a Eletrônica para Artistas \n \n \nc b a 2017 Helder da Rocha \n \n \n136\nEstrutura condicional: if \nO bloco if() executa seu conteúdo apenas se a expressão entre parênteses for verdadeira. É \nimportante que essa expressão seja uma expressão de lógica relacional (igualdade, diferença, maior \nou menor que) e/ou booleana (expressões OR “||” e AND “&&”). Exemplos de uso: \nif(digitalRead(6) != HIGH) {  \n    analogWrite(9, 128); \n} \nUm bloco if pode ser seguido por um bloco else, que irá conter instruções que serão executadas se a \nexpressão do if não for verdadeira. \nif(digitalRead(6) != HIGH) {  \n    analogWrite(9, 128); \n} else { \n    analogWrite(9, 0); \n} \nEntre os blocos if e else pode também haver zero ou mais expressões else if, que testam situações \nintermediárias. Por exemplo: \nif(digitalRead(6) != HIGH) {  \n    analogWrite(9, 128); \n} else if (analogRead(0) > 512 && analogRead(1) < 255) { \n    digitalWrite(6, LOW); \n} else { \n    analogWrite(9, 0); \n} \nEstrutura de repetição: for \nA estrutura de repetição for tem uma declaração entre parênteses seguido por um bloco contendo \ninstruções: \nfor(int i = 0; i < 6; i++) { \n    analogWrite( 9, i * 51);  // i varia de 0 a 5 (produzirá 0 a 255) \n} \nA declaração contém três partes: \n• \nUma inicialização de variável (a variável que será testada para decidir se a repetição \ncontinua). Esta inicialização acontece apenas uma vez. \n• \nUma expressão condicional para testar a variável. Este teste ocorre antes de cada \nrepetição. \n• \nUma expressão para alterar o valor da variável (incrementar ou decrementar). Esta \noperação acontece no final de cada execução do bloco entre chaves.  \nO código entre as chaves no bloco for será executado zero ou mais vezes, dependendo do resultado da \nexpressão condicional.  \nDefinição de funções \nFunções são definidas declarando seu tipo, nome usado como identificador, sua lista de parâmetros \n(declarando o tipo de cada um) e um bloco entre chaves contendo as instruções que irá executar \nquando for chamada. Por exemplo: \nvoid piscar(int pino, int tempo) { \n    digitalWrite(pino, HIGH); \n    delay(tempo / 2); \n    digitalWrite(pino, LOW); \n    delay(tempo / 2); \n} \n",
    "book_id": "introducaoeletronicaartistas",
    "book_title": "Microsoft Word - IntroduçãoEletronicaArtistas.docx",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 137
  },
  {
    "chunk_full": "Introdução a Eletrônica para Artistas \n \n \nc b a 2017 Helder da Rocha \n \n \n137\nTodas as funções que vimos nesta apostila eram do tipo void – que não retornam valor, mas é possível \ndefinir funções que retornam valor, como por exemplo a função analogRead() que devolve um valor \nint como resposta. A função abaixo soma dois números e retorna o resultado como int: \nint soma(int x, int y) { \n    return x + y; \n} \nChamada de funções \nA sintaxe para chamar uma função consiste em usar seu nome como um comando, e passar valores ou \nvariáveis compatíveis com o tipo e quantidade de seus parâmetros. Para chamar a função piscar() \nacima, podemos usar: \npiscar(5, 1000); \nA chamada irá atribuir o valor 5 à variável pino, e o valor 1000 à variável tempo, e a função executará \ncomandos que farão o pino piscar uma vez, com 500ms entre cada pulso. \nPara chamar uma função que retorna valor, como a função soma() que definimos acima, \nprovavelmente iremos querer guardar o valor retornado em uma variável: \nint resultado = soma(5,9); \nA chamada irá atribuir o valor 5 para x, e o valor 9 para y dentro da definição de função, que vai \nsomar os números e retornar. O valor de retorno será guardado na variável resultado. \nListas indexadas \nListas (ou arrays, ou vetores) guardam uma coleção de itens de um determinado tipo em uma \nvariável, que podem ser referenciados através de seu índice (posição dentro da lista). O índice \ncomeça em zero. Existem várias formas de definir uma lista. As listas que usamos nesta apostila \nforam definidas declarando explicitamente seus componentes entre chaves. \nint pinos = {3, 5, 6, 9, 10, 11}; \nO número de componentes representa o tamanho da lista. O índice varia de 0 a tamanho-da-lista – 1. \nPara recuperar um elemento, use o índice entre colchetes. O comando abaixo grava o valor máximo de \ntensão no pino 9 (quarto pino da lista acima, índice 3): \nanalogWrite( pinos[3], 255); \nOs índices passados podem ser usados também para trocar o valor de um elemento da lista, através \nde uma atribuição. O tipo de dados deve ser compatível: \npinos[0] = 13;  // muda a lista para {13, 5, 6, 9, 10, 11} \nListas são frequentemente usadas em estruturas de repetição, onde o índice é passado como uma \nvariável: \nfor(int i = 0; i < 6; i++) { \n    analogWrite( pinos[i], 255 ); \n} \nA repetição for repete o bloco seis vezes, com um valor diferente de i em cada repetição. O valor de i \nvaria de 0 a 5, assim o bloco produz o valor máximo de tensão PWM em cada um dos seis pinos \nlistados na lista pinos[]. \nDiretiva #include \nÉ usado para incluir arquivos de cabeçalho (.h) e bibliotecas em um sketch.  \nOs arquivos de cabeçalho devem estar na mesma pasta e são incluídos pelo nome, entre aspas (e \nsem ponto e vírgula no final): \n#include \"funcoes.h\" \n",
    "book_id": "introducaoeletronicaartistas",
    "book_title": "Microsoft Word - IntroduçãoEletronicaArtistas.docx",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 138
  },
  {
    "chunk_full": "Introdução a Eletrônica para Artistas \n \n \nc b a 2017 Helder da Rocha \n \n \n138\nBibliotecas precisam estar previamente instaladas no ambiente do Arduino. Use a opção de menu \nSketch / Include Library / Manage Libraries para baixar e instalar novas bibliotecas. Uma vez \ninstaladas, elas poderão ser incluídas chamando o header da biblioteca entre < e >: \n#include <FastLED.h> \nGeralmente a instalação de bibliotecas adiciona vários exemplos de uso que podem ser acessados e \nusados a partir do menu File / Examples da IDE do Arduino. \nDiretiva #define \nNesta apostila usamos #define para definir constantes (como alternativa a usar const int). Por \nexemplo, declarar: \n# define PINO_LED 8 \nTem o mesmo efeito prático que: \nconst int PINO_LED = 8; \nEm programas Arduino, declarar constantes globais usando const int ou #define é uma questão de \nestilo. É importante saber reconhecer os dois casos e entender como funcionam. \n8.1.2. Variáveis e constantes do Arduino \nO Arduino define diversas constantes e variáveis com valores prévios. Eles representam opções e \nníveis lógicos.  \nEstados lógicos \nAs constantes inteiras HIGH e LOW contém respectivamente os valores 1 e 0, e representam níveis \nlógicos dos pinos digitais do Arduino. No Arduino Nano, esses níveis lógicos representam \nrespectivamente os valores de tensão 5 volts, e 0 volts. \nHIGH e LOW podem ser usados em testes: \nif (digitalRead(6) == HIGH) { ... } \nE em funções que alteram o nível lógico dos pinos: \ndigitalWrite(9, LOW); \nFinalidade de um pino (pinMode) \nA função pinMode() permite alterar a finalidade de um pino, configurando-o como entrada, saída ou \nligando-o a um resistor de pull-up ou um LED. Na apostila usamos três opções de pinMode(): \n• \nOUTPUT – configura o pino como saída digital. \n• \nINPUT – configura explicitamente o pino como entrada (opcional – ele já é naturalmente \nentrada) \n• \nINPUT_PULLUP – configura o pino como entrada ligada a um resistor interno de pull-up, \npara que o estado inicial do pino seja HIGH. \n8.1.3. Funções do Arduino \nAs funções abaixo incluem apenas as que foram abordadas na apostila. Para uma referência mais \ncompleta, consulte a documentação no site oficial do Arduino. \nanalogRead(pino-analógico) \nEsta função retorna um valor inteiro de 0 a 1023 correspondente ao nível de tensão em um dos 8 \npinos analógicos. Normalmente o pino deve ser ligado a um divisor de tensão (entre os pinos 5V e \nGND) que pode ser o próprio sensor (se ele tiver 3 terminais) ou no meio da ligação em série de um \nresistor com o sensor (se ele tiver 2 terminais). \nint leitura = analogRead(2); \n",
    "book_id": "introducaoeletronicaartistas",
    "book_title": "Microsoft Word - IntroduçãoEletronicaArtistas.docx",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 139
  },
  {
    "chunk_full": "Introdução a Eletrônica para Artistas \n \n \nc b a 2017 Helder da Rocha \n \n \n139\nanalogWrite(pino-digital-pwm, valor) \nA função recebe dois parâmetros. O primeiro deve ser um pino digital que suporte PWM. O segundo \né um número inteiro de 0 a 255. A função irá gerar no pino correspondente um valor médio \nsimulado de tensão proporcional ao valor passado de 0 a 255. \nanalogWrite(10, 127);  // produz pulso PWM com ciclo de trabalho de 50% \nA função analogWrite não produz saída digital verdadeira. É PWM, ou seja, é uma onda quadrada que \nalterna entre os valores 0 e 5V, em frequência entre 500 e 1KHz (500 a 1000 vezes por segundo), \nvariando o tempo que o pulso está no estado HIGH (ciclo de trabalho). O tempo será proporcional ao \nvalor de 0 a 255 passado pela função. Isto produz um valor médio que simula variação de tensão e \npermite variar brilho de LEDs e velocidades de motor. \nAlgumas aplicações requerem sinais analógicos verdadeiros. Neste caso é preciso construir um filtro \nRC na saída PWM (calcular um capacitor e resistor) para retificar a onda e gerar um valor constante. \ndigitalRead(pino-digital) \nA função retorna o valor lido no pino digital correspondente, que será retornada como HIGH ou LOW. \nNo Arduino Nano pode-se também ler dos pinos analógicos A0 a A7 (que podem ser identificados \ndesta forma, ou através dos números 14 a 19). \nint estado = digitalRead(8); \nSe o valor for um pouco abaixo de 5V ele ainda será interpretado como HIGH até o limite de 3V. \nHavendo menos de 3V no pino, o Arduino irá devolver o valor LOW como resposta. \nO valor lido por digitalRead() sempre será HIGH ou LOW, mesmo quando a entrada tiver valor \nindefinido. Para evitar valores indefinidos, o ideal é configurar o pinMode como INPUT_PULLUP, que \ngarante um valor inicial HIGH. A função digitalRead() também pode ser usada para ler o nível lógico \nde pinos configurados como OUTPUT.  \ndigitalWrite(pino-digital, HIGH ou LOW) \nEsta função requer que o pino esteja habilitado como OUTPUT, através da instrução pinMode(). Ele \nproduzirá na saída uma tensão de 0 ou 5V, conforme o valor passado como parâmetro, \nrespectivamente LOW ou HIGH. \ndigitalWrite(4, LOW); \nProduzir HIGH não significa necessariamente que a saída irá ter corrente. Isto depende da diferença \nde potencial entre a saída e a referência onde ela estiver conectada. Se a referência for 5V, a diferença \nde potencial será zero e HIGH irá desligar o componente na saída (que será ativado com LOW). Veja o \nexemplo usando o LED RGB anodo-comum, que tem esse comportamento. \ndelay(tempo-em-milissegundos) \nA função delay() interrompe o programa por no mínimo o tempo especificado em milissegundos. \ndelay(2000);  // interrompe por 2 segundos  \nUsar delay() com intervalos curtos (tipicamente de 10 segundos) é importante para lidar com sinais \nenviados por chaves, que durante o processo de ligar e desligar podem gerar vários pulsos que serão \nrecebidos pelo Arduino.  \ntone(pino, frequência, duração) \nGera, no pino indicado, uma onda quadrada na frequência especificada, por um tempo determinado. \nIsto pode ser usado para gerar som (ou mesmo fazer um pisca-pisca se forem usadas frequências \nmuito baixas).  \nO comando abaixo gera no pino 9 uma frequência de 523 Hz por 1 segundo: \ntone(9, 523, 1000);  // 523 Hz corresponde ao dó central do piano \n",
    "book_id": "introducaoeletronicaartistas",
    "book_title": "Microsoft Word - IntroduçãoEletronicaArtistas.docx",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 140
  },
  {
    "chunk_full": "Introdução a Eletrônica para Artistas \n \n \nc b a 2017 Helder da Rocha \n \n \n140\nnoTone(pino) \nMuda o estado do pino para LOW: \nnoTone(9); \npinMode(pino, função) \nEsta instrução é obrigatória para configurar pinos como saída digital ou como entrada usando \nresistores pull-up. Nos outros casos é opcional (a saída analógica automaticamente muda a função do \npino quando chamada, e as funções de entrada sem pull-up são o estado natural dos pinos). \nA função pinMode() deve ser chamada dentro de setup(): \nvoid setup() { \n    pinMode(3, OUTPUT);  // configura o pino 3 como saída digital \n} \n8.2. Placa Arduino Nano \nEsta seção contém um resumo e não se refere ao Arduino Nano original, mas ao clone fabricado na \nChina que é usado na oficina (as especificações são um pouco diferentes).  \n8.2.1. Especificações técnicas  \n• \nMicrocontrolador: ATMega328 \n• \nArquitetura: AVR \n• \nTensão de operação: 5V \n• \nConsumo: 19 mA \n• \nClock: 16 MHz \n• \nMemória flash: 32kB (2kB são usados pelo bootloader) \n• \nPinos analógicos: 8 (A0 – A7) \n• \nPinos digitais de entrada e saída: 22 (1-19 + 5V + 3V3 + GND) \n• \nPinos de saída PWM: 6 (3, 5, 6, 9, 10, 11) \n• \nSaídas reguladas: 3,3V (3V3) e 5V (5V) \n• \nTensão de alimentação via pino VIN: 6 a 12V \n• \nTensão de alimentação via USB: 5V \n• \nTensão de alimentação via pino 5V (evite, e não use Vin nem USB): 3,7 a 5,5V \n• \nCorrente máxima por pino de entrada/saída: 40mA (máximo de 200mA no total) \n• \nCorrente máxima nos pinos 5V: 500mA (depende da fonte de alimentação) \n• \nCorrente máxima na saída 3V3: 25mA \n• \nTaxa de comunicação serial (CH340): 2400 a 115200 bps \n• \nDimensões: 18 x 45 mm \n• \nPeso: 7g \nFontes e datasheets:  \n• \nhttps://www.arduino.cc/en/Main/arduinoBoardNano \n• \nhttp://actrl.cz/blog/index.php/2016/arduino-nano-ch340-schematics-and-details/ \n• \nhttps://cdn.sparkfun.com/datasheets/Dev/Arduino/Other/CH340DS1.PDF \nEsquema: \n• \nhttp://actrl.cz/blog/wp-content/uploads/nano_ch340_schematics.pdf \n8.2.2. Pinagem \nVeja um diagrama detalhado com a pinagem do Arduino Nano no capítulo desta apostila sobre \nProjetos com Arduino. \n",
    "book_id": "introducaoeletronicaartistas",
    "book_title": "Microsoft Word - IntroduçãoEletronicaArtistas.docx",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 141
  },
  {
    "chunk_full": "Introdução a Eletrônica para Artistas \n \n \nc b a 2017 Helder da Rocha \n \n \n141\n9. Links e referências \n9.1. Referências bibliográficas \n9.1.1. Livros \n[1]  Charles Platt. Make: Electronics. Second Edition. Maker Media, 2015. Este livro foi usado como \nreferência para vários assuntos abordados na oficina e na apostila, e serviu de inspiração para \nalguns exemplos. É uma introdução à eletrônica para leigos que explora em detalhes os conceitos \nmais importantes, através de vários experimentos. \n[2]  Revista Bê-a-bá da Eletrônica. Números 1 a 20. Editor: Bártolo Fittipaldi. São Paulo, 1982 a \n1984. Pouco menos de 30 edições foram publicadas na década de 80 e influenciaram uma geração \nde entusiastas de eletrônica na época. Ela contém vários circuitos simples e tutoriais com uma \nabordagem didática voltada para iniciantes. Alguns exemplos do curso foram baseados em \ncircuitos publicados nesta revista. \n[3]  Paul Scherz and Simon Monk. Practical Electronics for Inventors. Fourth Edition. McGraw-Hill, \n2016. Um livro enorme (mais de 1000 páginas) com uma abordagem prática e aprofundada de \neletrônica e referências detalhadas de componentes, ferramentas, técnicas, etc. \n[4]  Harry Kybett. Electronics – A Self-teaching guide. 2nd. Edition. John Wiley & Sons, 1986. Um \npequeno guia prático focado nos aspectos teóricos da eletrônica (lei de Ohm, leis de Kirchhoff, \ncapacitores, resistores, divisores de tensão, transistores). \n[5]  Massimo Banzi & Michael Shiloh. Make: Getting Started with Arduino. 3rd Edition. Uma breve \nintrodução à plataforma Arduino pelos criadores da plataforma. \n[6]  Simon Monk. Programming Arduino: getting started with sketches. Second Edition. McGraw-\nHill, 2016. Este livro proporciona uma introdução abrangente a Arduino através de vários exemplos \ne projetos. O autor também disponibiliza um tutorial de Arduino em 18 lições no site da AdaFruit: \nhttps://learn.adafruit.com/series/learn-arduino que inspirou alguns experimentos de Arduino desta apostila. \n[7]  Bill Urmenyi. Electronics for Artists. Self-published. CD-ROM, 2001. Vários capítulos do livro são \nacessíveis via Google Books. O autor vende o livro em formato digital através do seu site \nhttp://www.urmenyi.co.uk/.  \n[8]  Simon Quellen Field. Electronics for Artists: Adding Light, Motion and Sound to your \nArtwork. Chicago Review Press, 2015. O autor disponibilizou o conteúdo do livro em um site \ninterativo em http://artists.scitoys.com/ \n[9]  Ken Rinaldo. Interactive Electronics for Artists and Inventors. 2015. O livro está disponível \npara download ou visualização online no site do artista http://www.kenrinaldo.com/publications/. \n[10]  Emily Lovell. Getting Hands-on with Soft-Circuits. Um guia para construir circuitos de \neletrônica para vestir. Disponível online em http://alumni.media.mit.edu/~emme/guide.pdf  \n9.1.2. Apostilas e material de cursos \n[11]  Mark Allen. Electronics for Artists. Machine Project, Los Angeles, 2007. Apostila da oficina \ndescrita em http://machineproject.com/2007/workshops/electronics-for-artists/. Slides da oficina e outros \nmateriais ainda estão online em http://machineproject.com/files/pdf/. \n[12]  Iain Sharp. Electronics for Artists. 4th Revision. 2010. Apostilas de cursos (e outros materiais) \ndisponível em http://lushprojects.com/electronicsforartists/  e http://lushprojects.com/absolutebeginners/ \n9.1.3. Manuais \n[13]  Guia oficial do Arduino. https://www.arduino.cc/en/Guide/HomePage. Contém referência completa da \nlinguagem, especificação das placas, tutoriais e exemplos. \n",
    "book_id": "introducaoeletronicaartistas",
    "book_title": "Microsoft Word - IntroduçãoEletronicaArtistas.docx",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 142
  },
  {
    "chunk_full": "Introdução a Eletrônica para Artistas \n \n \nc b a 2017 Helder da Rocha \n \n \n142\n9.1.4. Revistas online, tutoriais, vídeos e blogs \n[14]  Instructables. http://www.instructables.com/. Um site que nasceu do laboratório multimídia do MIT e se \ntornou um dos mais populares portais colaborativos da comunidade “maker” DIY (Do-It-Yourself), \ncom mais de 100 mil projetos, muitos usando eletrônica e Arduino.  \n[15]  Makezine. http://makezine.com/projects/. Revista online com vários artigos e tutoriais com projetos \ndetalhados passo-a-passo usando eletrônica e Arduino.  \n[16]  DIY Hacking. https://diyhacking.com/. Outro portal similar aos anteriores, porém mais focado em \nprojetos eletrônicos (que são classificados em niveis de complexidade). \n[17]  Electronics Tutorials. http://www.electronics-tutorials.ws/ Diversos tutoriais detalhados e ilustrados \nsobre conceitos fundamentais da eletrônica, escritos para iniciantes. Veja especialmente os \ntutoriais sobre resistores, capacitores, diodos e transistores, que estão relacionados a temas \nabordados nesta apostila.  \n[18]  Random Nerd Tutorials. http://randomnerdtutorials.com/. Uma coleção de tutoriais de eletrônica \nbásica, Arduino, ESP8266 e Raspberry Pi, com foco em automação residencial. Os tutoriais simples e \ndidáticos sobre o circuito integrado 555, funcionamento de transistores, chaves e potenciômetros \nforam usados como referência para esta apostila. \n[19]  Blog do Arduino. https://blog.arduino.cc/ Blog oficial do Arduino, em inglês. Contém tutoriais de \nprojetos com Arduino.  \n[20]  Tutoriais da AdaFruit. https://learn.adafruit.com. Tem seções sobre wearables, Arduino e outras, com \ntutoriais detalhados. \n[21]  Tutoriais da Sparkfun. https://learn.sparkfun.com. Similares aos da AdaFruit, também com seções \nsobre wearables, Arduino, etc.  \n[22]  Collin’s Lab. https://www.youtube.com/playlist?list=PLjF7R1fz_OOU08_hRcayfVZSmTpBCGJbL Uma série de vídeos \nsobre fundamentos da eletrônica, técnicas e conceitos. É parte do canal da AdaFruit. \n[23]  Instituto Newton C. Braga. http://www.newtoncbraga.com.br/ O autor é um grande divulgador da \neletrônica como hobby, publicou vários livros e escreveu inúmeros artigos de eletrônica popular. No \nsite há vários tutoriais sobre componentes, transistores, circuitos integrados, etc.  \n[24]  Blog Fazedores. http://blog.fazedores.com/. Blog em português com artigos e tutoriais sobre Arduino, \neletrônica e outros assuntos relacionados.  \n[25]  Blog FilipeFlop. Um dos sites mais populares em português. Contém vários tutoriais usando \nArduino e sensores. http://blog.filipeflop.com/  \n[26]  Arduino&Cia. http://www.arduinoecia.com.br/. Outro blog em português com projetos com Arduino.  \n9.2. Software \n9.2.1. Editores e simuladores de circuitos  \nEstes aplicativos permitem desenhar circuitos online, e executar simulações sobre eles, calculando \nautomaticamente correntes e tensões. Eles não são muito simples de usar, e requerem algum \nconhecimento de eletrônica, mas também podem ser usados apenas para desenhar esquemas. Alguns \nsão gratuitos, outros são assinaturas pagas (mas podem ser usados de graça para circuitos simples): \n1. \nPartSim.  \nhttp://www.partsim.com  \n2. \nEasyEDA.  \nhttps://easyeda.com  \n3. \nSchematics Editor.  \nhttp://www.schematics.com/editor  \n",
    "book_id": "introducaoeletronicaartistas",
    "book_title": "Microsoft Word - IntroduçãoEletronicaArtistas.docx",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 143
  },
  {
    "chunk_full": "Introdução a Eletrônica para Artistas \n \n \nc b a 2017 Helder da Rocha \n \n \n143\n4. \nCircuitLab.  \nhttps://www.circuitlab.com/ \n5. \nSimulador animado de circuitos (Falstad)  \nhttp://www.falstad.com/circuit/e-index.html \nO \nautor \ndisponibiliza \nvárias \nanimações \ninterativas. Elas são ótimas para entender \ncomo funcionam diversos tipos de circuitos e \ncomponentes. Você pode interagir clicando na \ntela, mudando os valores dos componentes e \nvariando correntes e tensões mexendo em \ncontroles que alteram a animação.  \nSelecionamos algumas animações abaixo, que \nestão relacionados a temas abordados nesta \napostila: \nFundamentos de eletrônica \n• \nDemonstração da Lei de Ohm: http://www.falstad.com/circuit/e-ohms.html \n• \nResistores em paralelo e divisor de corrente: http://www.falstad.com/circuit/e-resistors.html \n• \nResistores em série e divisor de tensão: http://www.falstad.com/circuit/e-voltdivide.html \n• \nCarga e descarga de um capacitor: http://www.falstad.com/circuit/e-cap.html \nTransistores \n• \nFuncionamento de um transistor como amplificador: http://www.falstad.com/circuit/e-npn.html \n• \nFuncionamento de um transistor como chave: http://www.falstad.com/circuit/e-transswitch.html \n• \nMultivibrador astável com transistores: http://www.falstad.com/circuit/e-multivib-a.html \n• \nCircuito biestável com transistores: http://www.falstad.com/circuit/e-multivib-bi.html \n• \nCircuito monoestável com transistores: http://www.falstad.com/circuit/e-multivib-mono.html \nCircuito integrado 555 \n• \n555 monoestável: http://www.falstad.com/circuit/e-555monostable.html \n• \n555 astável (gerador de ondas quadradas): http://www.falstad.com/circuit/e-555square.html \n• \nPWM com 555: http://www.falstad.com/circuit/e-555pulsemod.html \n• \nFuncionamento interno do 555: http://www.falstad.com/circuit/e-555int.html \n6. \nEveryCircuit  \nhttp://everycircuit.com/  \nÉ uma app para iPad, mas também roda no Chrome, que permite criar simulações de circuitos \nanimados. Há uma galeria com circuitos submetidos pelos membros da comunidade (não tão bem \nfeitos quanto os do Falstad). \n7. \nFritzing \nhttp://fritzing.org/ \nEditor \ngráfico \npara \ndesenhar \ncircuitos, esquemas e placas.  \nEste aplicativo gratuito foi usado \npara desenhar todas as ilustrações \nde protoboard da apostila. Existe \npara Mac, Linux e Windows, e \ntambém \ngera \num \nesquema \nassociado ao protoboard (mas \neditar esquemas no Fritzing não é \nmuito fácil) e placa para imprimir \ncircuito impresso. \n",
    "book_id": "introducaoeletronicaartistas",
    "book_title": "Microsoft Word - IntroduçãoEletronicaArtistas.docx",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 144
  },
  {
    "chunk_full": "Introdução a Eletrônica para Artistas \n \n \nc b a 2017 Helder da Rocha \n \n \n144\n9.2.2. Calculadoras online \nVocê pode sempre calcular os circuitos você mesmo, mas é mais rápido e prático simplesmente \ndigitar as informações que você tem em um formulário e deixar que um computador faça os cálculos \npara você. Os simuladores já fazem isto, mas as páginas a seguir podem ser mais fáceis de usar. As \ninformações estão em inglês, e o uso das calculadoras requer um conhecimento mínimo sobre o que \nse deseja calcular. \n1. Calculadora de Lei de Ohm. \nhttp://www.ohmslawcalculator.com/ohms-law-calculator \nDigite dois valores entre corrente, tensão, resistência e potência, e obtenha outros dois. \n2. Calculadora de divisor de tensão \nhttp://www.ohmslawcalculator.com/voltage-divider-calculator \n3. Calculadora de resistor limitador de corrente para LEDs \nhttp://www.ohmslawcalculator.com/led-resistor-calculator \n4. Conversor de milicandelas para lumens (calculadora de luminosidade de LEDs) \nhttp://www.ohmslawcalculator.com/mcd-to-lumens \n5. Calculadora para circuito de multivibrador astável com transistores \nhttp://www.homemade-circuits.com/p/transistor-astable-multibivrator-amv.html \nEsta calculadora obtém valores de resistores e capacitores, fornecendo-se a duração de cada pulso \n(ligado e desligado): T1 e T2, tensão de alimentação do circuito, tensões das junções do transistor \n(são típicos, você pode deixar os valores que são sugeridos ou usar zero), fator β (ganho) do \ntransistor (use 100 a 250 para BC549) e corrente no coletor (onde ficará o LED). \n6. Calculadora de capacitores e resistores para multivibrador astável 555 \nhttp://houseofjeff.com/555-timer-oscillator-frequency-calculator/ \nRecebe uma frequência ou intervalo de tempo e oferece várias opções de resistores (R1, R2) e \ncapacitores (C) para configurar o 555. Indica também quantos % do pulso corresponde ao ciclo de \ntrabalho (tempo em que o pulso está no estado ligado) para cada sugestão. \n7. Calculadoras de frequência para multivibrador astável 555 \nhttps://www.allaboutcircuits.com/tools/555-timer-astable-circuit/ \nhttp://www.ohmslawcalculator.com/555-astable-calculator \nhttp://www.royalrife.com/555_calculator.html \nEstas calculadoras fazem o oposto da anterior. Elas recebem os valores de R1, R2 e C e calcula \nfrequências e intervalos. É ideal quando você já tem valores fixos para dois dos componentes, e \nprecisa calcular o terceiro. \n8. Caluladora para circuito 555 monoestável \nhttp://www.ohmslawcalculator.com/555-monostable-calculator \nInforme dois valores (ex: capacitor e resistor, ou capacitor e largura do pulso, ou largura do pulso e \nresistor) que o programa calcula o valor restante. \n9. Calculadora de duração de bateria \nhttps://www.digikey.com/en/resources/conversion-calculators/conversion-calculator-battery-life \nNa verdade, a conta é mais complicada. É preciso levar em conta o tipo de bateria e outros aspectos. \nMas este cálculo permite uma boa estimativa. \n10. Calculadora de tempo de carga e descarga de capacitor \nhttps://www.digikey.com/en/resources/conversion-calculators/conversion-calculator-time-constant \n",
    "book_id": "introducaoeletronicaartistas",
    "book_title": "Microsoft Word - IntroduçãoEletronicaArtistas.docx",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 145
  },
  {
    "chunk_full": "Introdução a Eletrônica para Artistas \n \n \nc b a 2017 Helder da Rocha \n \n \n145\n9.3. Fornecedores de material \nEsta é uma lista incompleta de fornecedores de material usado nesta oficina e nos experimentos da \napostila (baseada principalmente nos fornecedores de material para o kit). \nNo exterior (Arduinos, wearables, linha condutiva, acessórios, etc.) \n• \nAdaFruit (New York): https://www.adafruit.com/  \n• \nSparkFun Electronics (Niwot, Colorado): https://www.sparkfun.com/  \n• \nLoja do Arduino (Europa): https://store.arduino.cc/ \nEm São Paulo (Sta. Ifigênia) \n• \nDabi (Componentes Eletrônicos). http://www.dabicomercio.com.br/. R. dos Timbiras, 299. \n• \nELT (Componentes Eletrônicos). R. dos Gusmões, 399. \n• \nMult Comercial (Componentes e Arduino). http://loja.multcomercial.com.br/ R. dos Timbiras, 257. \n• \nMamute Eletrônica (Robótica e Arduino). http://www.mamuteeletronica.com.br. R. Vitória, 125 \nLojas online no Brasil \n• \nRyndack (Componentes Eletrônicos). https://www.ryndackcomponentes.com.br/  \n• \nSoldafria (Componentes Eletrônicos). http://www.soldafria.com.br/  \n• \nFilipeFlop (Robótica e Arduino). http://www.filipeflop.com/  \n• \nRoboCore (Robótica e Arduino). https://www.robocore.net/  \n• \nInstituto Digital (Robótica e Arduino) http://www.institutodigital.com.br/  \n",
    "book_id": "introducaoeletronicaartistas",
    "book_title": "Microsoft Word - IntroduçãoEletronicaArtistas.docx",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 146
  },
  {
    "chunk_full": "Introdução a Eletrônica para Artistas \n \n \nc b a 2017 Helder da Rocha \n \n \n146\n10. Índice de experimentos \n \nINTRODUÇÃO \n1. \nExperimento 1 – Medição de tensão de uma bateria ....................................................................................... 14 \n2. \nExperimento 2 – Ligando LEDs, cigarras e motores com 1,5 e 3V ............................................................. 16 \n3. \nExperimento 3 – Bateria de cobre/zinco com eletrólito de batata ............................................................ 16 \n4. \nExperimento 4 – Um circuito usando chaves ...................................................................................................... 21 \n5. \nExperimento 5 – Teste de condutividade e medição de resistência ......................................................... 24 \n6. \nExperimento 6 – Introdução ao protoboard e divisor de tensão ............................................................... 31 \n7. \nExperimento 7 – Acendendo LEDs com 9 e 12v ................................................................................................ 37 \n8. \nExperimento 8 – Variando as cores de um LED RGB ....................................................................................... 38 \n9. \nExperimento 9 – Carga e descarga de capacitores ............................................................................................ 41 \n10. Alteração 9.1 – Usando a carga do capacitor para acender um LED ......................................................... 42 \n11. Experimento 10 (extra) – Gerador piezoelétrico .............................................................................................. 43 \nTRANSISTORES \n12. Experimento 11 – Transistores: circuito básico ................................................................................................ 46 \n13. Experimento 12 – Luz de emergência com transistor .................................................................................... 47 \n14. Experimento 13 – Fototransistor que desliga a carga ao ser ativado ...................................................... 48 \n15. Experimento 14 – Pisca-pisca alternado com LEDs ......................................................................................... 50 \n16. Alteração 14.1 – Acoplador ótico ............................................................................................................................. 51 \n17. Experimento 15 – Oscilador sonoro ou sirene ................................................................................................... 52 \n18. Alteração 15.1 – Um “theremin” sensível a luz ................................................................................................... 52 \n19. Experimento 16 (extra) – Oscilador sonoro com transistor PNP .............................................................. 53 \nCIRCUITOS INTEGRADOS \n20. Experimento 17 – Disparador acionado por pouca luz .................................................................................. 57 \n21. Experimento 18 – Temporizador ............................................................................................................................. 59 \n22. Alteração 18.1 – Usando um sensor sonoro para disparar o temporizador .......................................... 60 \n23. Alteração 18.2 – Substituindo o LED por um relé ............................................................................................. 62 \n24. Experimento 19 – Pisca-pisca com LED usando 555 ....................................................................................... 64 \n25. Experimento 20 (extra): Mini instrumento musical com 555 ..................................................................... 65 \n26. Experimento 21 (extra) – Dimmer usando PWM .............................................................................................. 69 \n27. Alteração 21.1 – Controle de velocidade de motor com PWM .................................................................... 71 \n28. Experimento 22 (extra): sequenciador de LEDs com o 4017 ...................................................................... 72 \n29. Alteração 22.1 – Sequenciador de LEDs automático com 555 e 4017 ..................................................... 73 \n30. Experimento 23 (extra) – Contador de 0 até 9 com display de 7 segmentos e 4026 ........................ 74 \nINTRODUÇÃO AO ARDUINO \n31. Experimento 24 – Piscando um LED ....................................................................................................................... 87 \n32. Alteração 24.1 – Usando variáveis ........................................................................................................................... 90 \n33. Experimento 25 – Reagindo ao acionamento de chaves liga-desliga ....................................................... 90 \n34. Alteração 25.1 – Invertendo o estado de acionamento .................................................................................. 92 \n35. Experimento 26 – Entrada com resistores pull-up ........................................................................................... 92 \n36. Alteração 26.1 – Substituindo uma chave por um sensor ............................................................................. 94 \n37. Experimento 27 – Piscando suavemente .............................................................................................................. 95 \n38. Experimento 28 – “Theremin” com LDR e potenciômetro ............................................................................ 96 \n39. Experimento 29 – Termômetro ................................................................................................................................ 98 \n40. Experimento 30 (extra) – Acelerando e desacelerando o motor com luz .............................................. 99 \n41. Alteração 30.1 – Usando uma fonte externa para alimentar o motor .................................................... 100 \n42. Experimento 31 (extra) – Definindo funções para controlar um LED RGB ......................................... 102 \n43. Experimento 32 (extra) – Usando bibliotecas para produzir notas musicais .................................... 105 \n44. Experimento 33 (extra) – Usando LEDs RGB endereçáveis ....................................................................... 108 \n \n",
    "book_id": "introducaoeletronicaartistas",
    "book_title": "Microsoft Word - IntroduçãoEletronicaArtistas.docx",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 147
  },
  {
    "chunk_full": "Introdução a Eletrônica para Artistas \n \n \nc b a 2017 Helder da Rocha \n \n \n147\n11. Material usado nos experimentos \nA lista a seguir relaciona todos os componentes que fazem parte do kit distribuído para cada aluno \n(depende da duração e formato da oficina). Há também uma descrição dos principais componentes \nnesta apostila. \n• \n1 caixa organizadora de plastico \n• \n1 bolsa de tecido \n• \n1 apostila impressa (pasta com folhas avulsas) \n \n• \n50g de pó de ferro \n \n• \n1 limão ou batata \n• \n1 prego de zinco \n• \n1 pedaço de cobre \n• \n1 parafuso 7mm com porca \n• \n1 mangueira 5cm (para eletroímã) \n \n• \n0,5 metro de fita dupla-face tecido condutivo de prata 1cm de diâmetro \n• \n0,5 metro de fita adesiva tecido condutivo de prata 1cm de diâmetro \n• \n1 metro de fita adesiva de folha de cobre com 0,5cm de diâmetro \n• \n1 metro de linha de costura condutiva de aço inoxidável \n• \n5 metros de fio esmaltado AWG 26 \n• \n2 imãs de neodímio N24/N35 com 10x4mm \n \n• \nÓculos de proteção (para soldagem) de plástico \n• \n1 alicate de ponta-fina \n• \n1 multímetro digital DT-830B (mede voltagem, corrente e resistência) \n• \n1 base de contatos para montagens (protoboard-breadboard) com 830 furos \n \n• \nJumpers macho (fios) para conexões em protoboard e Arduino \n• \n1,2m de fio rígido AWG 22 para conexões em protoboard (30cm de preto, branco, vermelho, \nazul, verde, amarelo) \n• \n1 placa de fenolite circuito impresso universal \n• \n5 cabos de 20cm com garras jacaré \n• \n1 clip para bateria de 9V \n• \n1 plugue macho P4 com bornes (para usar em bateria 9V) \n• \n1 plugue fêmea P4 com bornes (para circuitos alimentados com fonte de 9V) \n \n• \n1 suporte para uma pilha AAA \n• \n2 pilhas AAA de zinco-carbono com 1,5V \n• \n1 bateria CR2032 de 3V \n• \n1 pegador de roupa de madeira (suporte para pilha CR2032) \n• \n1 fonte de 9V / 1A com saída P4 \n• \n1 fotocélula (fonte de energia solar) de silício poli-cristalino 0,5V, 0,28W \n• \n1 célula piezoelétrica de 2,5cm \n• \n2 chaves tipo alavanca com duas posições \n• \n1 chave de pressão com duas posições \n• \n5 mini chaves tácteis (1 posição) \n• \n1 relé de 5V com 2 pólos e 2 posições \n• \n2 fusíveis pequenos de 0,2A (para multímetro) \n• \n1 mini motor de 3V \n• \n1 mini alto-falante de 8Ω \n• \n1 cigarra (buzzer) de 5V \n• \n1 microfone de eletreto \n",
    "book_id": "introducaoeletronicaartistas",
    "book_title": "Microsoft Word - IntroduçãoEletronicaArtistas.docx",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 148
  },
  {
    "chunk_full": "Introdução a Eletrônica para Artistas \n \n \nc b a 2017 Helder da Rocha \n \n \n148\n• \n1 sensor magnético reed \n• \n1 sensor passivo de temperatura (termistor) NTC 10k \n• \n2 sensores passivos de luminosidade LDR 5mm e 7mm \n \n• \nResistores de ¼ de Watt \no \n1 de cada: 10Ω, 220kΩ, 680kΩ, 2,2MΩ, 3,3MΩ \no \n2 de cada: 47Ω, 330Ω, 680Ω, 2,2kΩ, 3,3kΩ, 4,7kΩ, 6,8kΩ, 22kΩ, 33kΩ, 47kΩ, 68kΩ, \n330kΩ, 470k Ω, 560kΩ, 1MΩ (1 000 000 Ω) \no \n5 de cada: 100kΩ (100 000 Ω) \no \n10 de cada: 100Ω, 1kΩ (1000 Ω), 10kΩ (10 000 Ω),  \no \n14 de cada: 220Ω \no \n16 de cada: 470Ω \n• \nCapacitores de cerâmica e poliéster \no \n1 de cada: 100pF, 3,3nF (3,3kpF) \no \n2 de cada: 1nF (1000pF, 1kpF), 4,7nF (4,7kpF) , 22nF (22kpF, 0,022 µF), 47nF \n(47kpF, 0,047 µF), 100nF (0,1 µF), 330nF (0,33 µF), 470nF (0,47 µF) \no \n8 de cada: 10nF (10kpF, 0,01 µF) \n• \nCapacitores eletrolíticos \no \n1 de cada: 4,7 µF, 33 µF, 470 µF, 1000 µF, 2200 µF \no \n2 de cada: 2,2 µF, 47 µF, 220 µF \no \n3 de cada: 3,3 µF \no \n4 de cada: 1 µF, 10 µF, 22 µF, 100 µF \n \n• \n5 potenciômetros (resistores variáveis) com 5k, 10k, 20k, 50k e 100kΩ \n \n• \n12 LEDs de alto-brilho (transparentes) de 5mm – 2 de cada cor: vermelho, rosa, amarelo, \nverde, azul, violeta \n• \n4 LEDs de alto-brilho (transparentes) de 5mm de luz branca \n• \n10 LEDs difusos vermelhos de 5mm \n• \n1 LED amarelo de 3mm \n• \n1 LED RGB de alto-brilho 5mm e anodo-comum (4 terminais) \n• \n1 LED RGB 5050 (SMD 6 terminais) \n• \n1 LED infravermelho \n• \n1 LED RGB endereçável WS8212 com dissipador \n• \n1 display LED de 7 segmentos catodo comum \n \n• \n4 diodos de silício de propósito geral (1N4148 ou equivalente) \n• \n1 diodo Zener de 3,3V \n• \n1 sensor de temperatura de precisão LM35DZ (embalagem TO-92) \n• \n1 foto-transistor TIL 78 (embalagem LED 5mm) \n• \n4 transistores bijunção NPN de propósito geral (BC548, 2N3904, ou equivalente) \n• \n1 transistor bijunção PNP de propósito geral (BC558, 2N3906, ou equivalente) \n• \n1 transistor MOSFET de baixa potência (2N7000 ou equivalente) \n• \n1 transistor MOSFET de potência nível lógico (IRL540 ou equivalente) \n \n• \n1 circuito integrado 7805 (regulador de tensão 5V) \n• \n3 circuitos integrados 555 (temporizador) \n• \n1 circuito integrado 4017 (contador de década) \n• \n1 circuito integrado 4026 (decodificador para display de 7 segmentos) \n \n• \n1 placa microcontroladora compatível Arduíno Uno, Nano ou similar \n• \n1 cabo USB para Arduino \n",
    "book_id": "introducaoeletronicaartistas",
    "book_title": "Microsoft Word - IntroduçãoEletronicaArtistas.docx",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 149
  },
  {
    "chunk_full": "Introdução a Eletrônica para Artistas \n \n \nc b a 2017 Helder da Rocha \n \n \n149\n12. Sobre o autor \nHelder da Rocha é um peixe abissal. \n \n  \n(Foto: Maurício Franco de Carvalho.) \n \n",
    "book_id": "introducaoeletronicaartistas",
    "book_title": "Microsoft Word - IntroduçãoEletronicaArtistas.docx",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 150
  },
  {
    "chunk_full": "The Nature of Code\nThe Nature of Code\nby Daniel Shiffman\nThe publisher would go here, but there isn’t one; it’s only me.\nversion 1.0, generated December 6, 2012\ni\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 151
  },
  {
    "chunk_full": "Dedication\nDedication\nFor my grandmother, Bella Manel Greenfield (October 13, 1915 - April 3, 2010)\nBella Manel was born in New York City. A pioneering woman in mathematics, she earned her\nPhD in 1939 from New York University under the supervision of Richard Courant. She worked\nfor Ramo-Wooldridge (now TRW) and at the Rand Corporation with Richard Bellman. Later, she\ntaught mathematics at the College of Notre Dame (now Notre Dame de Namur University) in\nBelmont, California, and at UCLA. The Bella Manel Prize for outstanding graduate work by a\nwoman or minority was established at NYU’s Courant Institute in 1995.\nThe Nature of Code (v1.0)\nii\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 152
  },
  {
    "chunk_full": "Editor\nEditor\nShannon Fry\nIllustrations\nIllustrations\nZannah Marsh\nCover Design\nCover Design\nDavid Wilson\nInterior Design\nInterior Design\nDavid Wilson\nWeb Site Design\nWeb Site Design\nSteve Klise\nEditorial and Design Assistant\nEditorial and Design Assistant\nEvan Emolo\nMagic Book Lead Developers\nMagic Book Lead Developers\nRune Madsen, Steve Klise\nMagic Book Researchers\nMagic Book Researchers\nEvan Emolo, Miguel Bermudez, Luisa Peirera Hors\nIndex\nIndex\nWordCo Indexing Services\nCopyright © 2012 by Daniel Shiffman\nISBN-13: 978-0985930806\nISBN-10: 0985930802\nThis work is licensed under the Creative Commons Attribution-NonCommercial 3.0\nUnported License. To view a copy of this license, visit creativecommons.org\n(http://creativecommons.org/licenses/by-nc/3.0/) or send a letter to Creative Commons, 444\nCastro Street, Suite 900, Mountain View, California 94041, USA.\nAll of the book’s source code is licensed under the GNU Lesser General Public License\n(http://creativecommons.org/licenses/LGPL/2.1/) as published by the Free Software\nFoundation; either version 2.1 of the License, or (at your option) any later version.\nThis book was generated by the Magic Book Project (http://magicbookproject.com).\nDedication\niii\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 153
  },
  {
    "chunk_full": "Acknowledgments\nAcknowledgments\n\"The world around us moves in complicated and wonderful ways. We\nspend the earlier parts of our lives learning about our environment\nthrough perception and interaction. We expect the physical world around\nus to behave consistently with our perceptual memory, e.g. if we drop a\nrock it will fall due to gravity, if a gust of wind blows, lighter objects will\nbe tossed by the wind further. This class focuses on understanding,\nsimulating, and incorporating motion-based elements of our physical\nworld into the digital worlds that we create. Our hope is to create\nintuitive, rich, and more satisfying experiences by drawing from the\nperceptual memories of our users.\"\n— James Tu, Dynamic Bodies course description, Spring 2003, ITP\nThe Nature of Code (v1.0)\niv\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 154
  },
  {
    "chunk_full": "A.1 A little bit of history\nA.1 A little bit of history\nIn 2003, as a graduate student at the Interactive Telecommunications Program (ITP) in the\nTisch School of the Arts at New York University, I enrolled in a course called Dynamic\nBodies. The course was taught by interaction designer and ITP adjunct professor James Tu.\nAt the time, my work was focused on a series of software experiments that generated real-\ntime “non-photorealistic” imagery. The applications involved capturing images from a live\nsource and “painting” the colors with elements that moved about the screen according to\nvarious rules. The Dynamic Bodies course—which covered vectors, forces, oscillations,\nparticle systems, recursion, steering, and springs—aligned perfectly with my work.\nI had been using these concepts informally in my own projects, but had never taken the\ntime to closely examine the science behind the algorithms or learn object-oriented\ntechniques to formalize their implementation. That very semester, I also enrolled in\nFoundations of Generative Art Systems, a course taught by Philip Galanter, that focused on\nthe theory and practice of generative art, covering topics such as chaos, cellular automata,\ngenetic algorithms, neural networks, and fractals. Both Tu’s course and Galanter’s course\nopened my eyes to a world of simulation algorithms and techniques that carried me through\nthe next several years of work and teaching, and served as the foundation and inspiration\nfor this book.\nBut there’s another piece of the puzzle missing from this story.\nGalanter’s course was mostly theory-based, while Tu’s was taught using Macromedia\nDirector and the Lingo programming language. That semester, I learned many of the\nalgorithms by translating them into C++ (the language I was using quite awkwardly at the\ntime, well before C++ creative coding environments like openFrameworks and Cinder had\narrived). Towards the end of the semester, I discovered something called Processing\n(http://www.processing.org). Processing was in alpha then (version 0055) and, having had\nsome experience with Java, it intrigued me enough to ask the question: Could this open-\nsource, artist-friendly programming language and environment be the right place to develop\na suite of tutorials and examples about programming and simulation? With the support of\nthe ITP and Processing communities, I embarked on what has now been an almost eight-\nyear journey of teaching a variety of programming concepts and their applications using\nProcessing.\nI’d like to first thank Red Burns, ITP’s founder, who has supported and encouraged me in my\nwork for over ten years. Dan O’Sullivan, the chair of ITP, has been my teaching mentor and\nwas the first to suggest that I try teaching a course on Processing, giving me a reason to\nstart assembling programming tutorials in the first place. Shawn Van Every, developer\nextraordinaire and author of Pro Android Media, has also been a rich source of help and\ninspiration at ITP over the years. ITP faculty members Clay Shirky, Danny Rozin, Katherine\nDillon, Marianne Petit, Marina Zurkow, and Tom Igoe have provided a great deal of support\nand feedback throughout the writing of this book. The rest of the faculty and staff at ITP\nhave also made this possible: Brian Kim, Edward Gordon, George Agudow, John Duane,\nMarlon Evans, Matt Berger, Megan Demarest, Midori Yasuda, and Rob Ryan.\nAcknowledgments\nv\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 155
  },
  {
    "chunk_full": "The students of ITP, too numerous to mention, have been an amazing source of feedback\nthroughout this process. Much of the material in this book comes from my course of the same\ntitle, which I’ve now taught for five years. I have stacks of draft printouts of the book with\nnotes scrawled along the margins as well as a vast archive of student emails with corrections,\ncomments, and generous words of encouragement.\nI am also indebted to the energetic and supportive community of Processing programmers\nand artists. I wouldn’t be writing this book if it weren’t for Casey Reas and Ben Fry, who\ncreated Processing. I’ve learned half of what I know simply from reading through the\nProcessing source code; the elegant simplicity of the Processing language, website, and IDE\nhas made programming accessible and fun for all of my students. I’ve received advice and\ninspiration from many Processing programmers including Andrés Colubri, Jer Thorp, Marius\nWatz, Karsten Schmidt, Robert Hodgin, Seb-Lee Delisle, and Ira Greenberg. Heather Dewey-\nHagborg provided a great deal of excellent feedback on Chapter 10 (Neural Networks) and\nPhilip Galanter helped to clarify the definitions of complexity and complex systems. Scott\nMurray provided some really helpful advice about inline SVGs over e-mail. Many of the titles in\nthe Further Reading section were suggested by Golan Levin.\nI am indebted to Shannon Fry, who edited this book every step of the way. The knowledge\nthat I would always have her careful and thoughtful feedback on my writing allowed me to\nplow ahead, aware that everything would come out sounding better after she got her hands\non my chapters.\nA special mention goes to Zannah Marsh who worked tirelessly to create over a hundred\nillustrations for this book, developing a friendly and informal look. I especially want to thank\nher for her patience and willingness to go with the flow as we changed the illustration\nrequirements several times. I also want to thank David Wilson, who came to my rescue at the\nlast minute and designed the interior layout and cover for the book. I am particularly grateful\nto Steve Klise, who designed and built the book’s website, helping me to develop a \"pay what\nyou want\" model for the digital PDF.\nAs I’ll explain a bit more in the preface, this book was generated with a new open-source\nsystem for publishing called “The Magic Book.” A crack team of ITP programmers, designers,\nand artists worked over the course of more than a year to develop this system, which\ngenerates a book in a variety of formats (PDF, HTML, and more) from one single ASCIIDOC\nfile, all designed with CSS layout. Rune Madsen began the project and developed the original\nRuby / Sinatra framework. I am pretty sure I’d still be struggling with putting the book together\nwell into 2013 if it weren’t for Rune’s dedication to seeing the project through to the end.\nSteve Klise contributed countless bug fixes and engineered the system that allows us to\nrestyle code comments to the side of the code blocks themselves. Miguel Bermudez, Evan\nEmolo, and Luisa Pereira Hors contributed in many ways, learning the ins and outs of\nASCIIDOC as well as CSS Paged Media. ITP researcher Greg Borenstein provided a\ntremendous amount of advice and support along the way regarding the areas of publishing for\nthe Web and print. Prince (http://princexml.com) is the engine the Magic Book uses to\ngenerate a PDF from an HTML document, and I’d like to thank Michael Day, CEO of\nPrinceXML, who answered many of our questions (at lightning speed) along the way.\nThe Nature of Code (v1.0)\nvi\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 156
  },
  {
    "chunk_full": "Finally I’d like to thank my family: my wife, Aliki Caloyeras, who supported this project\nthroughout while having her own giant tome to write, and my children, Elias and Olympia,\nmotivation for finishing this up so that I could spend more time hanging out with them. I’d\nalso like to thank my father, Bernard Shiffman, who generously lent his mathematical\nexpertise and provided feedback along the way, as well as my mother, Doris Yaffe Shiffman,\nand brother, Jonathan Shiffman, who were always tremendously supportive in asking the\nquestion: “How is the book coming along?”\nA.2 Kickstarter\nA.2 Kickstarter\nThere is another organization and community that has made this book possible: Kickstarter.\nIn 2008, I completed work on my first book, Learning Processing, published by Morgan\nKaufmann/Elsevier. Learning Processing took almost three years to finish. I didn’t take a lot\nof care in choosing a publisher or thinking about the terms. I just thought — “Really? You\nwant to publish a book by me? OK, I’ll do it.” Unfortunately, my experience was not entirely\npositive. I had five different editors assigned to me throughout the process, and I received\nlittle to no feedback on the content itself. The publisher outsourced the typesetting, which\nresulted in a great deal of mistakes and inconsistencies in production. In addition, I found\nthe pricing of the book to be off the mark. My goal was to write a friendly, inexpensive\n(black and white), paperback introduction to programming in Processing, and the book\nended up retailing for a \"textbook\" price of $50.\nNow, I want to emphasize that my publisher had good intentions. They honestly wanted to\nproduce the best book possible, one that I would be happy with, that they would be happy\nwith, and that readers would enjoy. And they worked hard to make this happen.\nUnfortunately, they had to work within a very tight budget, and as a result were stretched\nextremely thin. In addition, I don’t think they were terribly familiar with the world of open-\nsource “creative” coding environments like Processing; their world is computer science\ntextbooks.\nAs a result, for this Nature of Code book, I felt it was important to try self-publishing. Since I\ndidn’t get editing support from the publisher, why not hire an editor? I wasn’t happy with the\npricing, so why not set the price myself (or, in the case of the PDF, let the buyer set the\nprice)? Then there’s the question of marketing — does a publisher add value and help you\nreach an audience? In some cases, the answer is yes. The O’Reilly “Make” series, for\nexample, does a wonderful job of creating a community around their books and products.\nStill, in the case of learning to program in Processing, reaching the audience is as simple as\none URL — processing.org.\nUnfortunately, I quickly discovered that there is one thing a publisher offers that I was not\ngetting from my self-publishing path. One very important, highly crucial detail — a deadline.\nOn my own, I floundered for two years, saying I was going to write the Nature of Code book\nbut only drafting a little bit here and there. On my list of things I needed to do, it was always\nat the bottom. Then along came Kickstarter, and with an audience sitting and waiting (and\nAcknowledgments\nvii\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 157
  },
  {
    "chunk_full": "having spent cash money), I lived in fear of not meeting my deadline. And the fact that you are\nreading this now is an indication that it worked.\nMost importantly, self-publishing the book has allowed me a great deal of flexibility in how I\nprice and distribute the content. On Elsevier’s website, you can purchase Learning Processing\nas an e-book for $53.95. That’s right, fifty-three dollars and ninety-five cents. Incidentally, for\neach e-book sold I get a royalty of 5%, which is $2.70. That’s right, two dollars and seventy\ncents. If I self-publish, I can make the book massively cheaper. Selling a digital copy for $10,\nI’m reducing the cost to the reader by over eighty percent and tripling the money paid to me.\nI’m taking this even further with the PDF and allowing buyers to set the price themselves.\nIn addition, by owning all the content, I am able to release the entire book online for free as\nwell as experiment with new digital formats. The raw text of the book, as well as all the code\nand illustrations, is licensed under a Creative Commons Attribution-NonCommercial license\nand is available on GitHub, where readers can submit issues (not to mention pull requests!)\nwith corrections and comments. Finally, by using more flexible print-on-demand services, I can\nmore easily make changes and keep the book current, releasing new editions as often as I\nlike. (A one-time purchase of a digital copy of the book includes lifetime upgrades for free.)\nSo thank you to Kickstarter, both the company (especially Fred Benenson, who convinced me\nto take the plunge in the first place and advised me on how to license the book) as well as all\nthe backers who took a chance on this book. Some of these backers, through generosity\nbeyond the call of duty, earned an extra thank-you as part of their reward:\n•\nAlexandre B.\n•\nRobert Hodgin\n•\nJooYoun Paek\n•\nAngela McNamee (Boyhan)\n•\nBob Ippolito\nAll of the backers directly contributed to the finishing of this book. Just the sheer act of\nsigning up to contribute money for draft and final versions lit a fire in me to finish, not to\nmention provided me with the resources to pay for design and editing work (and some\nbabysitting during Saturday morning writing sessions).\nIn addition to contributing funds, Kickstarter backers read pre-release versions of the chapters\nand provided tons of feedback, catching many errors and pointing out confusing sections of\nthe book. Two such readers that I’d like to thank are Frederik Vanhoutte and Hans de Wolf,\nwhose expert knowledge of Newtonian physics was enormously helpful in the revising of\nChapters 2 and 3.\nThe Nature of Code (v1.0)\nviii\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 158
  },
  {
    "chunk_full": "Preface\nPreface\nP.1 What is this book?\nP.1 What is this book?\nAt ITP (http://itp.nyu.edu), I teach a course entitled Introduction to Computational Media. In\nthis course, the students learn the basics of programming (variables, conditionals, loops,\nobjects, arrays) as well as a survey of applications related to making interactive projects\n(images, pixels, computer vision, networking, data, 3D). The course mostly follows the\nmaterial found in my intro book Learning Processing; in many ways, The Nature of Code\nserves as a follow-up. Once you’ve learned the basics and seen an array of applications,\nyour next step might be to delve deeply into a particular area. For example, you could focus\non computer vision (and read a book like Greg Borenstein’s Making Things See). In the most\nbasic sense, this book is one possible next step in a world of many. It picks up exactly\nwhere Learning Processing leaves off, demonstrating more advanced programming\ntechniques with Processing that focus on algorithms and simulation.\nThe goal of this book is simple. We want to take a look at something that naturally occurs in\nour physical world, then determine how we can write code to simulate that occurrence.\nSo then what is this book exactly? Is it a science book? The answer is a resounding no.\nTrue, we might examine topics that come from physics or biology, but it won’t be our job to\ninvestigate these topics with a particularly high level of academic rigor. Instead, we’re going\nto glance at scientific concepts and grab the parts that we need in the service of building a\nparticular software example.\nIs this an art or design book? I would also say no; after all, we are going to focus on\nalgorithms and their affiliated programming techniques. Sure, the results will all be visual in\nnature (manifested as animated Processing sketches), but they will exist more as\ndemonstrations of the algorithms and programming techniques themselves, drawn only with\nsimple shapes and grayscale. It is my hope, however, that designers and artists can\nincorporate all of the material here into their practice to make new, engaging work.\nPreface\nix\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 159
  },
  {
    "chunk_full": "In the end, if this book is anything, it is really just a good old-fashioned programming book.\nWhile a scientific topic may seed a chapter (Newtonian physics, cellular growth, evolution) or\nthe results might inspire an artistic project, the content itself will always boil down to the code\nimplementation, with a particular focus on object-oriented programming.\nP.2 A word about Processing\nP.2 A word about Processing\nI am using Processing in this book for a number of reasons. For one, it’s the language and\nenvironment with which I am most comfortable, and it’s what I enjoy using for my personal\nwork. Two, it’s free, open-source, and well suited to beginners. There is an active, energetic\ncommunity of people who program with Processing; for many, it’s the first programming\nlanguage they’ve learned. In this sense, I hope that I can reach a wide audience and\ndemonstrate the concepts in a friendly manner by using Processing.\nAll that said, there is nothing that ties what we are doing in this book strictly to Processing.\nThis book could have been written using ActionScript, JavaScript, Java (without Processing),\nor any number of other open-source “creative coding” environments like openFrameworks,\nCinder, or the newly released pocode. It is my hope that after I’ve completed this book, I’ll be\nable to release versions of the examples that run in other environments. If anyone is\ninterested in helping to port the examples, please feel free to contact me\n(daniel@shiffman.net).\nAll of the examples in this book have been tested with Processing 2.0b6, but for the most\npart, they should also work with earlier versions of Processing. I’ll be keeping them up-to-date\nwith whatever the latest version is. The most recent code can always be found on GitHub\n(http://github.com/shiffman/The-Nature-of-Code-Examples).\nP.3 What do you need to know?\nP.3 What do you need to know?\nThe prerequisite for understanding the material in this book could be stated as: “one semester\nof programming instruction with Processing (including familiarity with object-oriented\nprogramming).” That said, there’s no reason why you couldn’t read this book having learned\nprogramming using a different language or development environment. The key here is that\nyou have experience with programming.\nIf you’ve never written any code before, you are going to struggle, because this book\nassumes knowledge of all the basics. I would suggest picking up an introductory book on\nProcessing, a number of which are listed on the Processing website (http://processing.org/\nlearning/books/).\nIf you are an experienced programmer, but haven’t worked with Processing, you can probably\npick it up by downloading Processing (http://processing.org/download/), poking through the\nThe Nature of Code (v1.0)\nx\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 160
  },
  {
    "chunk_full": "examples, and reading through the Getting Started (http://processing.org/learning/\ngettingstarted/) page.\nI should also point out that experience with object-oriented programming is crucial. We’ll\nreview some of the basics in the book’s introduction, but I would suggest reading the\nProcessing tutorial on objects (http://processing.org/learning/objects) first.\nP.4 What are you using to read this book?\nP.4 What are you using to read this book?\nAre you reading this book on a Kindle? Printed paper? On your laptop in PDF form? On a\ntablet showing an animated HTML5 version? Are you strapped to a chair, absorbing the\ncontent directly into your brain via a series of electrodes, tubes, and cartridges?\nThe book you are reading right now was generated with the Magic Book project\n(http://www.magicbookproject.com). The Magic Book is an open-source framework for self-\npublishing developed at ITP (http://itp.nyu.edu). The idea here is that you only need to write\nthe book once as a simple text file. Once you’ve written your content, you press a magic\nbutton, and out comes your book in a variety of formats—PDF, HTML5, printed hardcopy,\nKindle, etc. Everything is designed and styled using CSS. As of the first release, the only\nversions available will be digital PDF, printed hardcopy, and HTML5 (which will include\nanimated versions of the examples using Processing.js). Hopefully over the course of the\nnext year, the book will be available in additional formats. If you’d like to help with this,\nplease contact me (daniel@shiffman.net).\nP.5 The “story” of this book\nP.5 The “story” of this book\nIf you glance over the book’s table of contents, you’ll notice there are ten chapters, each\none covering a different topic. And in one sense, this book is just that—a survey of ten\nconcepts and associated code examples. Nevertheless, in putting together the material, I\nhad always imagined something of a linear narrative. Before you begin reading the\nchapters, I’d like to walk you through this story.\nPart I: Inanimate objects\nPart I: Inanimate objects\nA soccer ball lies in the grass. A kick launches it into the air. Gravity pulls it back down. A\nheavy gust of wind keeps it afloat a moment longer until it falls and bounces off the head of\na jumping player. The soccer ball is not alive; it makes no choices as to how it will move\nthroughout the world. Rather, it is an inanimate object waiting to be pushed and pulled by\nthe forces of its environment.\nPreface\nxi\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 161
  },
  {
    "chunk_full": "How would we model a soccer ball moving in Processing? If you’ve ever programmed a circle\nmoving across a window, then you’ve probably written the following line of code.\nYou draw some shape at location x. With each frame of animation, you increment the value of\nx, redraw the shape and voila—the illusion of motion! Maybe you took it a step or two further,\nand included a y location, as well as variables for speed along the x and y axes.\nPart I of this story will take us one step further. We’re going to take these variables xspeed\nand yspeed and learn how together they form a vector (Chapter 1\nChapter 1), the building block of\nmotion. We won’t get any new functionality out of this, but it will build a solid foundation for\nthe rest of the book.\nOnce we know a little something about vectors, we’re going to quickly realize that a force\n(Chapter 2\nChapter 2) is a vector. Kick a soccer ball and you are applying a force. What does a force\ncause an object to do? According to Isaac Newton, force equals mass times acceleration. That\nforce causes an object to accelerate. Modeling forces will allow us to create systems with\ndynamic motion where objects move according to a variety of rules.\nNow, that soccer ball to which you applied a force might have also been spinning. If an object\nmoves according to its acceleration, it can spin according to its angular acceleration (Chapter\nChapter\n3). Understanding the basics of angles and trigonometry will allow us to model rotating\nobjects as well as grasp the principles behind oscillating motion, like a pendulum swinging or\na spring bouncing.\nOnce we’ve tackled the basics of motion and forces for an individual inanimate object, we’ll\nlearn how to make thousands upon thousands of those objects and manage them in a single\nsystem called a particle system (Chapter 4\nChapter 4). Particle systems will allow us to look at some\nadvanced features of object-oriented programming, namely inheritance and polymorphism.\nIn Chapters 1 through 4, all of the examples will be written from “scratch”—meaning the code\nfor the algorithms driving the motion of the objects will be written directly in Processing. We’re\ncertainly not the first programmers ever to consider the idea of simulating physics in\nanimation, so next we’ll examine how physics libraries (Chapter 5\nChapter 5) can be used to model more\nadvanced and sophisticated behaviors. We’ll look at Box2D (http://www.box2d.org) and\ntoxiclibs' Verlet Physics package (http://toxiclibs.org/).\nx = x + 1;\nx = x + xspeed;\ny = y + yspeed;\nPart II: It’s alive!\nPart II: It’s alive!\nWhat does it mean to model life? Not an easy question to answer, but we can begin by\nbuilding objects that have an ability to perceive their environment. Let’s think about this for a\nThe Nature of Code (v1.0)\nxii\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 162
  },
  {
    "chunk_full": "moment. A block that falls off a table moves according to forces, as does a dolphin\nswimming through the water. But there is a key difference. The block cannot decide to leap\noff that table. The dolphin can decide to leap out of the water. The dolphin can have dreams\nand desires. It can feel hunger or fear, and those feelings can inform its movements. By\nexamining techniques behind modeling autonomous agents (Chapter 6\nChapter 6), we will breathe life\ninto our inanimate objects, allowing them to make decisions about their movements\naccording to their understanding of their environment.\nThrough combining the concept of autonomous agents with what we learned about\nmodeling systems in Chapter 4, we’ll look at models of group behavior that exhibit the\nproperties of complexity. A complex system is typically defined as a system that is “more\nthan the sum of its parts.” While the individual elements of the system may be incredibly\nsimple and easily understood, the behavior of the system as a whole can be highly complex,\nintelligent, and difficult to predict. This will lead us away from thinking purely about\nmodeling motion and into the realm of rule-based systems. What can we model with cellular\nautomata (Chapter 7\nChapter 7), a system of cells living on a grid? What types of patterns can we\ngenerate with fractals (Chapter 8\nChapter 8), the geometry of nature?\nPart III: Intelligence\nPart III: Intelligence\nWe made things move. Then we gave those things hopes and dreams and fears, along with\nrules to live by. The last step in this book will be to make our creations even smarter. Can\nwe apply the biological process of evolution to computational systems (Chapter 9\nChapter 9) in order\nto evolve our objects? Taking inspiration from the human brain, can we program an artificial\nneural network (Chapter 10\nChapter 10) that can learn from its mistakes and allow our objects to adapt\nto their environment?\nP.6 This book as a syllabus\nP.6 This book as a syllabus\nWhile the content in this book certainly makes for an intense and highly compressed\nsemester, I have designed it to fit into a fourteen-week course. Nevertheless, it’s worth\nmentioning that I find that the book chapters sometimes work better expanded across\nmultiple weeks. For example, the syllabus for my course generally works out as follows:\nPreface\nxiii\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 163
  },
  {
    "chunk_full": "Week 1\nWeek 1\nIntroduction and Vectors (Chapter 1)\nWeek 2\nWeek 2\nForces (Chapter 2)\nWeek 3\nWeek 3\nOscillations (Chapter 3)\nWeek 4\nWeek 4\nParticle Systems (Chapter 4)\nWeek 5\nWeek 5\nPhysics Libraries Part I (Chapter 5)\nWeek 6\nWeek 6\nPhysics Libraries Part II & Steering (Chapters 5-6)\nWeek 7\nWeek 7\nPresent midterm projects about motion\nWeek 8\nWeek 8\nComplex Systems: Flocking and 1D Cellular Automata (Chapters 6-7)\nWeek 9\nWeek 9\nComplex Systems: 2D Cellular Automata and Fractals (Chapters 7-8)\nWeek 10\nWeek 10\nGenetic Algorithms (Chapter 9)\nWeek 11\nWeek 11\nNeural Networks (Chapter 10)\nWeeks 12-13\nWeeks 12-13\nFinal project workshop\nWeek 14\nWeek 14\nFinal project presentation\nIf you are considering using this text for a course or workshop, please feel free to contact me.\nI hope to eventually release a companion set of videos and slide presentations as\nsupplementary educational materials.\nP.7 The Ecosystem Project\nP.7 The Ecosystem Project\nAs much as I’d like to pretend you could learn everything by curling up in a comfy chair and\nreading some prose about programming, to learn programming, you’re really going to have to\ndo some programming. You might find it helpful to keep in mind a project idea (or two) to\ndevelop as a set of exercises while going from chapter to chapter. In fact, when teaching the\nNature of Code course at ITP, I have often found that students enjoy building a single project,\nstep by step, week by week, over the course of a semester.\nAt the end of each chapter, you’ll find a series of exercises for one such project—exercises\nthat build on each other, one topic at a time. Consider the following scenario. You’ve been\nasked by a science museum to develop the software for a new exhibit—The Digital Ecosystem,\na world of animated, procedural creatures that live on a projection screen for visitors to enjoy\nas they enter the museum. I don’t mean to suggest that this is a particularly innovative or\ncreative concept. Rather, we’ll use this example project idea as a literal representation of the\ncontent in the book, demonstrating how the elements fit together in a single software project.\nI encourage you to develop your own idea, one that is more abstract and creative in its\nthinking.\nThe Nature of Code (v1.0)\nxiv\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 164
  },
  {
    "chunk_full": "P.8 Where do I find the code online and submit\nP.8 Where do I find the code online and submit\nfeedback?\nfeedback?\nFor all things book-related, please visit the Nature of Code website\n(http://www.natureofcode.com). The raw source text of the book and all of the illustrations\nare on GitHub (http://github.com/shiffman/The-Nature-of-Code). Please leave feedback and\nsubmit corrections using GitHub issues.\nThe source code for all of the examples (and exercises) is also available on GitHub\n(http://github.com/shiffman/The-Nature-of-Code-Examples). The chapters themselves\ninclude code snippets in-line with the text. However, I want to mention that in many cases, I\nhave shortened or simplified the code snippets in order to illustrate a specific point. In all\ncases, the full code with comments can be found via GitHub.\nIf you have questions about the code itself, I would suggest posting them on the Processing\nforum (http://forum.processing.org).\nPreface\nxv\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 165
  },
  {
    "chunk_full": "Table of Contents\nTable of Contents\nAcknowledgments\nA.1 A little bit of history\nA.2 Kickstarter\nPreface\nP.1 What is this book?\nP.2 A word about Processing\nP.3 What do you need to know?\nP.4 What are you using to read this book?\nP.5 The “story” of this book\nP.6 This book as a syllabus\nP.7 The Ecosystem Project\nP.8 Where do I find the code online and submit feedback?\nIntroduction\nI.1 Random Walks\nI.2 The Random Walker Class\nI.3 Probability and Non-Uniform Distributions\niv\nv\nvii\nix\nix\nx\nx\nxi\nxi\nxiii\nxiv\nxv\n1\n1\n2\n7\nThe Nature of Code (v1.0)\nxvi\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 166
  },
  {
    "chunk_full": "I.4 A Normal Distribution of Random Numbers\nI.5 A Custom Distribution of Random Numbers\nI.6 Perlin Noise (A Smoother Approach)\nI.7 Onward\nChapter 1. Vectors\n1.1 Vectors, You Complete Me\n1.2 Vectors for Processing Programmers\n1.3 Vector Addition\n1.4 More Vector Math\n1.5 Vector Magnitude\n1.6 Normalizing Vectors\n1.7 Vector Motion: Velocity\n1.8 Vector Motion: Acceleration\n1.9 Static vs. Non-Static Functions\n1.10 Interactivity with Acceleration\nChapter 2. Forces\n2.1 Forces and Newton’s Laws of Motion\n2.2 Forces and Processing—Newton’s Second Law as a Function\n2.3 Force Accumulation\n2.4 Dealing with Mass\n2.5 Creating Forces\n2.6 Gravity on Earth and Modeling a Force\n2.7 Friction\n2.8 Air and Fluid Resistance\n2.9 Gravitational Attraction\n2.10 Everything Attracts (or Repels) Everything\nChapter 3. Oscillation\n11\n14\n17\n26\n27\n28\n30\n33\n37\n42\n43\n45\n49\n54\n57\n63\n63\n67\n68\n70\n73\n77\n80\n83\n88\n97\n101\nTable of Contents\nxvii\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 167
  },
  {
    "chunk_full": "3.1 Angles\n3.2 Angular Motion\n3.3 Trigonometry\n3.4 Pointing in the Direction of Movement\n3.5 Polar vs. Cartesian Coordinates\n3.6 Oscillation Amplitude and Period\n3.7 Oscillation with Angular Velocity\n3.8 Waves\n3.9 Trigonometry and Forces: The Pendulum\n3.10 Spring Forces\nChapter 4. Particle Systems\n4.1 Why We Need Particle Systems\n4.2 A Single Particle\n4.3 The ArrayList\n4.4 The Particle System Class\n4.5 A System of Systems\n4.6 Inheritance and Polymorphism: An Introduction\n4.7 Inheritance Basics\n4.8 Particles with Inheritance\n4.9 Polymorphism Basics\n4.10 Particle Systems with Polymorphism\n4.11 Particle Systems with Forces\n4.12 Particle Systems with Repellers\n4.13 Image Textures and Additive Blending\nChapter 5. Physics Libraries\n5.1 What Is Box2D and When Is It Useful?\n5.2 Getting Box2D in Processing\n5.3 Box2D Basics\n101\n104\n108\n109\n112\n116\n119\n122\n127\n134\n143\n144\n145\n149\n155\n157\n160\n162\n166\n168\n170\n173\n178\n183\n189\n190\n192\n192\nThe Nature of Code (v1.0)\nxviii\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 168
  },
  {
    "chunk_full": "5.4 Living in a Box2D World\n5.5 Building a Box2D Body\n5.6 Three’s Company: Bodies and Shapes and Fixtures\n5.7 Box2D and Processing: Reunited and It Feels So Good\n5.8 Fixed Box2D Objects\n5.9 A Curvy Boundary\n5.10 Complex Forms\n5.11 Feeling Attached—Box2D Joints\n5.12 Bringing It All Back Home to Forces\n5.13 Collision Events\n5.14 A Brief Interlude—Integration Methods\n5.15 Verlet Physics with toxiclibs\n5.16 Particles and Springs in toxiclibs\n5.17 Putting It All Together: A Simple Interactive Spring\n5.18 Connected Systems, Part I: String\n5.19 Connected Systems, Part II: Force-Directed Graph\n5.20 Attraction and Repulsion Behaviors\nChapter 6. Autonomous Agents\n6.1 Forces from Within\n6.2 Vehicles and Steering\n6.3 The Steering Force\n6.4 Arriving Behavior\n6.5 Your Own Desires: Desired Velocity\n6.6 Flow Fields\n6.7 The Dot Product\n6.8 Path Following\n6.9 Path Following with Multiple Segments\n6.10 Complex Systems\n6.11 Group Behaviors (or: Let’s not run into each other)\n196\n198\n200\n203\n209\n211\n215\n222\n232\n234\n238\n241\n244\n247\n249\n253\n256\n260\n260\n262\n263\n270\n274\n276\n282\n286\n294\n298\n300\nTable of Contents\nxix\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 169
  },
  {
    "chunk_full": "6.12 Combinations\n6.13 Flocking\n6.14 Algorithmic Efficiency (or: Why does my $@(*%! run so slowly?)\n6.15 A Few Last Notes: Optimization Tricks\nChapter 7. Cellular Automata\n7.1 What Is a Cellular Automaton?\n7.2 Elementary Cellular Automata\n7.3 How to Program an Elementary CA\n7.4 Drawing an Elementary CA\n7.5 Wolfram Classification\n7.6 The Game of Life\n7.7 Programming the Game of Life\n7.8 Object-Oriented Cells\n7.9 Variations of Traditional CA\nChapter 8. Fractals\n8.1 What Is a Fractal?\n8.2 Recursion\n8.3 The Cantor Set with a Recursive Function\n8.4 The Koch Curve and the ArrayList Technique\n8.5 Trees\n8.6 L-systems\nChapter 9. The Evolution of Code\n9.1 Genetic Algorithms: Inspired by Actual Events\n9.2 Why Use Genetic Algorithms?\n9.3 Darwinian Natural Selection\n9.4 The Genetic Algorithm, Part I: Creating a Population\n306\n308\n315\n317\n323\n324\n325\n330\n336\n340\n342\n345\n349\n351\n355\n356\n358\n363\n366\n374\n382\n390\n391\n392\n394\n395\nThe Nature of Code (v1.0)\nxx\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 170
  },
  {
    "chunk_full": "9.5 The Genetic Algorithm, Part II: Selection\n9.6 The Genetic Algorithm, Part III: Reproduction\n9.7 Code for Creating the Population\n9.8 Genetic Algorithms: Putting It All Together\n9.9 Genetic Algorithms: Make Them Your Own\n9.10 Evolving Forces: Smart Rockets\n9.11 Smart Rockets: Putting It All Together\n9.12 Interactive Selection\n9.13 Ecosystem Simulation\nChapter 10. Neural Networks\n10.1 Artificial Neural Networks: Introduction and Application\n10.2 The Perceptron\n10.3 Simple Pattern Recognition Using a Perceptron\n10.4 Coding the Perceptron\n10.5 A Steering Perceptron\n10.6 It’s a “Network,” Remember?\n10.7 Neural Network Diagrams\n10.8 Animating Feed Forward\nFurther Reading\nBooks\nPapers and Articles\nIndex\n397\n399\n402\n409\n413\n420\n425\n431\n435\n444\n445\n448\n450\n452\n460\n466\n468\n473\n481\n481\n482\n484\nTable of Contents\nxxi\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 171
  },
  {
    "chunk_full": "Introduction\nIntroduction\n“I am two with nature.”\n— Woody Allen\nHere we are: the beginning. Well, almost the beginning. If it’s been a while since you’ve done\nany programming in Processing (or any math, for that matter), this introduction will get your\nmind back into computational thinking before we approach some of the more difficult and\ncomplex material.\nIn Chapter 1, we’re going to talk about the concept of a vector and how it will serve as the\nbuilding block for simulating motion throughout this book. But before we take that step, let’s\nthink about what it means for something to simply move around the screen. Let’s begin with\none of the best-known and simplest simulations of motion—the random walk.\nI.1 Random Walks\nI.1 Random Walks\nImagine you are standing in the middle of a balance beam. Every ten seconds, you flip a coin.\nHeads, take a step forward. Tails, take a step backward. This is a random walk—a path defined\nas a series of random steps. Stepping off that balance beam and onto the floor, you could\nperform a random walk in two dimensions by flipping that same coin twice with the following\nresults:\nThe Nature of Code (v1.0)\n1\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 172
  },
  {
    "chunk_full": "Flip 1\nFlip 1\nFlip 2\nFlip 2\nResult\nResult\nHeads\nHeads\nStep forward.\nHeads\nTails\nStep right.\nTails\nHeads\nStep left.\nTails\nTails\nStep backward.\nYes, this may seem like a particularly unsophisticated algorithm. Nevertheless, random\nwalks can be used to model phenomena that occur in the real world, from the movements of\nmolecules in a gas to the behavior of a gambler spending a day at the casino. As for us, we\nbegin this book studying a random walk with three goals in mind.\n1.\nWe need to review a programming concept central to this book—object-oriented\nprogramming. The random walker will serve as a template for how we will use\nobject-oriented design to make things that move around a Processing window.\n2.\nThe random walk instigates the two questions that we will ask over and over again\nthroughout this book: “How do we define the rules that govern the behavior of our\nobjects?” and then, “How do we implement these rules in Processing?”\n3.\nThroughout the book, we’ll periodically need a basic understanding of\nrandomness, probability, and Perlin noise. The random walk will allow us to\ndemonstrate a few key points that will come in handy later.\nI.2 The Random Walker Class\nI.2 The Random Walker Class\nLet’s review a bit of object-oriented programming (OOP) first by building a Walker object.\nThis will be only a cursory review. If you have never worked with OOP before, you may want\nsomething more comprehensive; I’d suggest stopping here and reviewing the basics on the\nProcessing website (http://processing.org/learning/objects/) before continuing.\nAn object\nobject in Processing is an entity that has both data and functionality. We are looking to\ndesign a Walker object that both keeps track of its data (where it exists on the screen) and\nhas the capability to perform certain actions (such as draw itself or take a step).\nA class\nclass is the template for building actual instances of objects. Think of a class as the\ncookie cutter; the objects are the cookies themselves.\nLet’s begin by defining the Walker class—what it means to be a Walker object. The Walker\nonly needs two pieces of data—a number for its x-location and one for its y-location.\nclass Walker {\nIntroduction\n2\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 173
  },
  {
    "chunk_full": "Every class must have a constructor, a special function that is called when the object is first\ncreated. You can think of it as the object’s setup(). There, we’ll initialize the Walker’s starting\nlocation (in this case, the center of the window).\nFinally, in addition to data, classes can be defined with functionality. In this example, a Walker\nhas two functions. We first write a function that allows the object to display itself (as a white\ndot).\nThe second function directs the Walker object to take a step. Now, this is where things get a\nbit more interesting. Remember that floor on which we were taking random steps? Well, now\nwe can use a Processing window in that same capacity. There are four possible steps. A step\nto the right can be simulated by incrementing x (x++); to the left by decrementing x (x--);\nforward by going down a pixel (y++); and backward by going up a pixel (y--). How do we pick\nfrom these four choices? Earlier we stated that we could flip two coins. In Processing,\nhowever, when we want to randomly choose from a list of options, we can pick a random\nnumber using random().\nThe above line of code picks a random floating point number between 0 and 4 and converts it\nto an integer, with a result of 0, 1, 2, or 3. Technically speaking, the highest number will never\nbe 4.0, but rather 3.999999999 (with as many 9s as there are decimal places); since the\nprocess of converting to an integer lops off the decimal place, the highest int we can get is 3.\nNext, we take the appropriate step (left, right, up, or down) depending on which random\nnumber was picked.\nObjects have data.\nint x;\nint y;\nObjects have a constructor where they are\ninitialized.\nWalker() {\nx = width/2;\ny = height/2;\n}\nObjects have functions.\nvoid display() {\nstroke(0);\npoint(x,y);\n}\nvoid step() {\n0, 1, 2, or 3\nint choice = int(random(4));\nThe Nature of Code (v1.0)\n3\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 174
  },
  {
    "chunk_full": "Now that we’ve written the class, it’s time to make an actual Walker object in the main part\nof our sketch—setup() and draw(). Assuming we are looking to model a single random\nwalk, we declare one global variable of type Walker.\nThen we create the object in setup() by calling the constructor with the new operator.\nExample I.1: Traditional random walk\nEach time you see the above Example heading in this book, it means there is a\ncorresponding code example available on GitHub (http://github.com/shiffman/The-Nature-\nof-Code-Examples).\nFinally, during each cycle through draw(), we ask the Walker to take a step and draw a dot.\nSince we only draw the background once in setup(), rather than clearing it continually\neach time through draw(), we see the trail of the random walk in our Processing window.\nThe random “choice” determines our step.\nif (choice == 0) {\nx++;\n} else if (choice == 1) {\nx--;\n} else if (choice == 2) {\ny++;\n} else {\ny--;\n}\n}\n}\nA Walker object\nWalker w;\nvoid setup() {\nsize(640,360);\nCreate the Walker.\nw = new Walker();\nbackground(255);\n}\nvoid draw() {\nCall functions on the Walker.\nw.step();\nw.display();\n}\nIntroduction\n4\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 175
  },
  {
    "chunk_full": "There are a couple improvements we could make to the random walker. For one, this\nWalker’s step choices are limited to four options—up, down, left, and right. But any given pixel\nin the window has eight possible neighbors, and a ninth possibility is to stay in the same\nplace.\nTo implement a Walker object that can step to any neighboring pixel (or stay put), we could\npick a number between 0 and 8 (nine possible choices). However, a more efficient way to\nwrite the code would be to simply pick from three possible steps along the x-axis (-1, 0, or 1)\nand three possible steps along the y-axis.\nTaking this further, we could use floating point numbers (i.e. decimal numbers) for x and y\ninstead and move according to an arbitrary random value between -1 and 1.\nFigure I.1\nvoid step() {\nYields -1, 0, or 1\nint stepx = int(random(3))-1;\nint stepy = int(random(3))-1;\nx += stepx;\ny += stepy;\n}\nvoid step() {\nYields any floating point number between\n-1.0 and 1.0\nfloat stepx = random(-1, 1);\nfloat stepy = random(-1, 1);\nThe Nature of Code (v1.0)\n5\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 176
  },
  {
    "chunk_full": "All of these variations on the “traditional” random walk have one thing in common: at any\nmoment in time, the probability that the Walker will take a step in a given direction is equal\nto the probability that the Walker will take a step in any direction. In other words, if there\nare four possible steps, there is a 1 in 4 (or 25%) chance the Walker will take any given step.\nWith nine possible steps, it’s a 1 in 9 (or 11.1%) chance.\nConveniently, this is how the random() function works. Processing’s random number\ngenerator (which operates behind the scenes) produces what is known as a “uniform”\ndistribution of numbers. We can test this distribution with a Processing sketch that counts\neach time a random number is picked and graphs it as the height of a rectangle.\nExample I.2: Random number distribution\nx += stepx;\ny += stepy;\n}\nAn array to keep track of how often random\nnumbers are picked\nint[] randomCounts;\nvoid setup() {\nsize(640,240);\nrandomCounts = new int[20];\n}\nvoid draw() {\nbackground(255);\nPick a random number and increase the\ncount.\nint index = int(random(randomCounts.length));\nrandomCounts[index]++;\nstroke(0);\nfill(175);\nint w = width/randomCounts.length;\nIntroduction\n6\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 177
  },
  {
    "chunk_full": "The above screenshot shows the result of the sketch running for a few minutes. Notice how\neach bar of the graph differs in height. Our sample size (i.e. the number of random numbers\nwe’ve picked) is rather small and there are some occasional discrepancies, where certain\nnumbers are picked more often. Over time, with a good random number generator, this would\neven out.\nPseudo-Random Numbers\nPseudo-Random Numbers\nThe random numbers we get from the random() function are not truly random;\ntherefore they are known as “pseudo-random.” They are the result of a mathematical\nfunction that simulates randomness. This function would yield a pattern over time, but\nthat time period is so long that for us, it’s just as good as pure randomness!\nGraphing the results\nfor (int x = 0; x < randomCounts.length; x++) {\nrect(x*w,height-randomCounts[x],w-1,randomCounts[x]);\n}\n}\nCreate a random walker that has a tendency to move down and to the right. (We’ll see\nthe solution to this in the next section.)\nExercise I.1\nExercise I.1\nI.3 Probability and Non-Uniform Distributions\nI.3 Probability and Non-Uniform Distributions\nRemember when you first started programming in Processing? Perhaps you wanted to draw a\nlot of circles on the screen. So you said to yourself: “Oh, I know. I’ll draw all these circles at\nrandom locations, with random sizes and random colors.” In a computer graphics system, it’s\noften easiest to seed a system with randomness. In this book, however, we’re looking to build\nsystems modeled on what we see in nature. Defaulting to randomness is not a particularly\nthoughtful solution to a design problem—in particular, the kind of problem that involves\ncreating an organic or natural-looking simulation.\nWith a few tricks, we can change the way we use random() to produce “non-uniform”\ndistributions of random numbers. This will come in handy throughout the book as we look at a\nnumber of different scenarios. When we examine genetic algorithms, for example, we’ll need a\nmethodology for performing “selection”—which members of our population should be\nselected to pass their DNA to the next generation? Remember the concept of survival of the\nfittest? Let’s say we have a population of monkeys evolving. Not every monkey will have a\nThe Nature of Code (v1.0)\n7\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 178
  },
  {
    "chunk_full": "equal chance of reproducing. To simulate Darwinian evolution, we can’t simply pick two\nrandom monkeys to be parents. We need the more “fit” ones to be more likely to be chosen.\nWe need to define the “probability of the fittest.” For example, a particularly fast and strong\nmonkey might have a 90% chance of procreating, while a weaker one has only a 10%\nchance.\nLet’s pause here and take a look at probability’s basic principles. First we’ll examine single\nevent probability, i.e. the likelihood that a given event will occur.\nIf you have a system with a certain number of possible outcomes, the probability of the\noccurrence of a given event equals the number of outcomes that qualify as that event\ndivided by the total number of all possible outcomes. A coin toss is a simple example—it has\nonly two possible outcomes, heads or tails. There is only one way to flip heads. The\nprobability that the coin will turn up heads, therefore, is one divided by two: 1/2 or 50%.\nTake a deck of fifty-two cards. The probability of drawing an ace from that deck is:\nnumber of aces / number of cards = 4 / 52 = 0.077 = ~ 8%\nThe probability of drawing a diamond is:\nnumber of diamonds / number of cards = 13 / 52 = 0.25 = 25%\nWe can also calculate the probability of multiple events occurring in sequence. To do this,\nwe simply multiply the individual probabilities of each event.\nThe probability of a coin turning up heads three times in a row is:\n(1/2) * (1/2) * (1/2) = 1/8 (or 0.125)\n…meaning that a coin will turn up heads three times in a row one out of eight times (each\n“time” being three tosses).\nThere are a couple of ways in which we can use the random() function with probability in\ncode. One technique is to fill an array with a selection of numbers—some of which are\nrepeated—then choose random numbers from that array and generate events based on\nthose choices.\nWhat is the probability of drawing two aces in a row from a deck of fifty-two cards?\nExercise I.2\nExercise I.2\nint[] stuff = new int[5]\n1 is stored in the array twice, making it\nmore likely to be picked.\nstuff[0] = 1;\nstuff[1] = 1;\nIntroduction\n8\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 179
  },
  {
    "chunk_full": "Running this code will produce a 40% chance of printing the value 1, a 20% chance of printing\n2, and a 40% chance of printing 3.\nWe can also ask for a random number (let’s make it simple and just consider random floating\npoint values between 0 and 1) and allow an event to occur only if our random number is within\na certain range. For example:\nThis method can also be applied to multiple outcomes. Let’s say that Outcome A has a 60%\nchance of happening, Outcome B, a 10% chance, and Outcome C, a 30% chance. We\nimplement this in code by picking a random float and seeing into what range it falls.\n•\nbetween 0.00 and 0.60 (60%) –> Outcome A\n•\nbetween 0.60 and 0.70 (10%) –> Outcome B\n•\nbetween 0.70 and 1.00 (30%) –> Outcome C\nWe could use the above methodology to create a random walker that tends to move to the\nright. Here is an example of a Walker with the following probabilities:\n•\nchance of moving up: 20%\nstuff[2] = 2;\nstuff[3] = 3;\nstuff[4] = 3;\nPicking a random element from an array\nint index = int(random(stuff.length));\nA probability of 10%\nfloat prob = 0.10;\nA random floating point value between 0\nand 1\nfloat r = random(1);\nIf our random number is less than 0.1, try\nagain!\nif (r < prob) {\n// try again!\n}\nfloat num = random(1);\nIf random number is less than 0.6\nif (num < 0.6) {\nprintln(\"Outcome A\");\nBetween 0.6 and 0.7\n} else if (num < 0.7) {\nprintln(\"Outcome B\");\nGreater than 0.7\n} else {\nprintln(\"Outcome C\");\n}\nThe Nature of Code (v1.0)\n9\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 180
  },
  {
    "chunk_full": "•\nchance of moving down: 20%\n•\nchance of moving left: 20%\n•\nchance of moving right: 40%\nExample I.3: Walker that tends to move to the right\nvoid step() {\nfloat r = random(1);\nA 40% chance of moving to the right!\nif (r < 0.4) {\nx++;\n} else if (r < 0.6) {\nx--;\n} else if (r < 0.8) {\ny++;\n} else {\ny--;\n}\n}\nCreate a random walker with dynamic probabilities. For example, can you give it a\n50% chance of moving in the direction of the mouse?\nExercise I.3\nExercise I.3\nIntroduction\n10\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 181
  },
  {
    "chunk_full": "I.4 A Normal Distribution of Random Numbers\nI.4 A Normal Distribution of Random Numbers\nLet’s go back to that population of simulated Processing monkeys. Your program generates a\nthousand Monkey objects, each with a height value between 200 and 300 (as this is a world\nof monkeys that have heights between 200 and 300 pixels).\nDoes this accurately depict the heights of real-world beings? Think of a crowded sidewalk in\nNew York City. Pick any person off the street and it may appear that their height is random.\nNevertheless, it’s not the kind of random that random() produces. People’s heights are not\nuniformly distributed; there are a great deal more people of average height than there are\nvery tall or very short ones. To simulate nature, we may want it to be more likely that our\nmonkeys are of average height (250 pixels), yet still allow them to be, on occasion, very short\nor very tall.\nA distribution of values that cluster around an average (referred to as the “mean”) is known as\na “normal” distribution. It is also called the Gaussian distribution (named for mathematician\nCarl Friedrich Gauss) or, if you are French, the Laplacian distribution (named for Pierre-Simon\nLaplace). Both mathematicians were working concurrently in the early nineteenth century on\ndefining such a distribution.\nWhen you graph the distribution, you get something that looks like the following, informally\nknown as a bell curve:\nThe curve is generated by a mathematical function that defines the probability of any given\nvalue occurring as a function of the mean (often written as μ, the Greek letter mu) and\nstandard deviation (σ, the Greek letter sigma).\nThe mean is pretty easy to understand. In the case of our height values between 200 and\n300, you probably have an intuitive sense of the mean (i.e. average) as 250. However, what if\nI were to say that the standard deviation is 3 or 15? What does this mean for the numbers? The\nfloat h = random(200,300);\nFigure I.2\nFigure I.3\nThe Nature of Code (v1.0)\n11\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 182
  },
  {
    "chunk_full": "graphs above should give us a hint. The graph on the left shows us the distribution with a\nvery low standard deviation, where the majority of the values cluster closely around the\nmean. The graph on the right shows us a higher standard deviation, where the values are\nmore evenly spread out from the average.\nThe numbers work out as follows: Given a population, 68% of the members of that\npopulation will have values in the range of one standard deviation from the mean, 98%\nwithin two standard deviations, and 99.7% within three standard deviations. Given a\nstandard deviation of 5 pixels, only 0.3% of the monkey heights will be less than 235 pixels\n(three standard deviations below the mean of 250) or greater than 265 pixels (three\nstandard deviations above the mean of 250).\nCalculating Mean and Standard Deviation\nCalculating Mean and Standard Deviation\nConsider a class of ten students who receive the following scores (out of 100) on a\ntest:\n85, 82, 88, 86, 85, 93, 98, 40, 73, 83\nThe mean is the average: 81.3\nThe mean is the average: 81.3\nThe standard deviation is calculated as the square root of the average of the squares\nof deviations around the mean. In other words, take the difference from the mean for\neach person and square it (variance). Calculate the average of all these values and\ntake the square root as the standard deviation.\nScore\nScore\nDifference from Mean\nDifference from Mean\nVariance\nVariance\n85\n85-81.3 = 3.7\n(3.7)2 = 13.69\n40\n40-81.3 = -41.3\n(-41.3)2 = 1705.69\netc.\nAverage Variance:\nAverage Variance:\n254.23\n254.23\nThe standard deviation is the square root of the average variance: 15.13\nThe standard deviation is the square root of the average variance: 15.13\nLuckily for us, to use a normal distribution of random numbers in a Processing sketch, we\ndon’t have to do any of these calculations ourselves. Instead, we can make use of a class\nknown as Random, which we get for free as part of the default Java libraries imported into\nIntroduction\n12\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 183
  },
  {
    "chunk_full": "Processing (see the JavaDocs (http://docs.oracle.com/javase/6/docs/api/java/util/\nRandom.html) for more information).\nTo use the Random class, we must first declare a variable of type Random and create the\nRandom object in setup().\nIf we want to produce a random number with a normal (or Gaussian) distribution each time we\nrun through draw(), it’s as easy as calling the function nextGaussian().\nHere’s the thing. What are we supposed to do with this value? What if we wanted to use it, for\nexample, to assign the x-position of a shape we draw on screen?\nThe nextGaussian() function returns a normal distribution of random numbers with the\nfollowing parameters: a mean of zero and a standard deviation of one. Let’s say we want a\nmean of 320 (the center horizontal pixel in a window of width 640) and a standard deviation of\n60 pixels. We can adjust the value to our parameters by multiplying it by the standard\ndeviation and adding the mean.\nExample I.4: Gaussian distribution\nWe use the variable name “generator”\nbecause what we have here can be thought\nof as a random number generator.\nRandom generator;\nvoid setup() {\nsize(640,360);\ngenerator = new Random();\n}\nvoid draw() {\nAsking for a Gaussian random number.\n(Note nextGaussian() returns a double and\nmust be converted to float.)\nfloat num = (float) generator.nextGaussian();\n}\nvoid draw() {\nThe Nature of Code (v1.0)\n13\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 184
  },
  {
    "chunk_full": "By drawing the ellipses on top of each other with some transparency, we can actually see\nthe distribution. The brightest spot is near the center, where most of the values cluster, but\nevery so often circles are drawn farther to the right or left of the center.\nNote that nextGaussian() returns a double.\nfloat num = (float) generator.nextGaussian();\nfloat sd = 60;\nfloat mean = 320;\nMultiply by the standard deviation and add\nthe mean.\nfloat x = sd * num + mean;\nnoStroke();\nfill(255,10);\nellipse(x,180,16,16);\n}\nConsider a simulation of paint splatter drawn as a collection of colored dots. Most of\nthe paint clusters around a central location, but some dots do splatter out towards the\nedges. Can you use a normal distribution of random numbers to generate the\nlocations of the dots? Can you also use a normal distribution of random numbers to\ngenerate a color palette?\nExercise I.4\nExercise I.4\nA Gaussian random walk is defined as one in which the step size (how far the object\nmoves in a given direction) is generated with a normal distribution. Implement this\nvariation of our random walk.\nExercise I.5\nExercise I.5\nI.5 A Custom Distribution of Random Numbers\nI.5 A Custom Distribution of Random Numbers\nThere will come a time in your life when you do not want a uniform distribution of random\nvalues, or a Gaussian one. Let’s imagine for a moment that you are a random walker in\nsearch of food. Moving randomly around a space seems like a reasonable strategy for\nfinding something to eat. After all, you don’t know where the food is, so you might as well\nsearch randomly until you find it. The problem, as you may have noticed, is that random\nwalkers return to previously visited locations many times (this is known as “oversampling”).\nOne strategy to avoid such a problem is to, every so often, take a very large step. This\nallows the walker to forage randomly around a specific location while periodically jumping\nvery far away to reduce the amount of oversampling. This variation on the random walk\nIntroduction\n14\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 185
  },
  {
    "chunk_full": "(known as a Lévy flight) requires a custom set of probabilities. Though not an exact\nimplementation of a Lévy flight, we could state the probability distribution as follows: the\nlonger the step, the less likely it is to be picked; the shorter the step, the more likely.\nEarlier in this prologue, we saw that we could generate custom probability distributions by\nfilling an array with values (some duplicated so that they would be picked more frequently) or\nby testing the result of random(). We could implement a Lévy flight by saying that there is a\n1% chance of the walker taking a large step.\nHowever, this reduces the probabilities to a fixed number of options. What if we wanted to\nmake a more general rule—the higher a number, the more likely it is to be picked? 3.145\nwould be more likely to be picked than 3.144, even if that likelihood is just a tiny bit greater. In\nother words, if x is the random number, we could map the likelihood on the y-axis with y = x.\nIf we can figure out how to generate a distribution of random numbers according to the above\ngraph, then we will be able to apply the same methodology to any curve for which we have a\nformula.\nOne solution is to pick two random numbers instead of one. The first random number is just\nthat, a random number. The second one, however, is what we’ll call a “qualifying random\nvalue.” It will tell us whether to use the first one or throw it away and pick another one.\nNumbers that have an easier time qualifying will be picked more often, and numbers that\nrarely qualify will be picked infrequently. Here are the steps (for now, let’s consider only\nrandom values between 0 and 1):\nfloat r = random(1);\nA 1% chance of taking a large step\nif (r < 0.01) {\nxstep = random(-100,100);\nystep = random(-100,100);\n} else {\nxstep = random(-1,1);\nystep = random(-1,1);\n}\nFigure I.4\nThe Nature of Code (v1.0)\n15\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 186
  },
  {
    "chunk_full": "1.\nPick a random number: R1\n2.\nCompute a probability P that R1 should qualify. Let’s try: P = R1.\n3.\nPick another random number: R2\n4.\nIf R2 is less than P, then we have found our number—R1!\n5.\nIf R2 is not less than P, go back to step 1 and start over.\nHere we are saying that the likelihood that a random value will qualify is equal to the\nrandom number itself. Let’s say we pick 0.1 for R1. This means that R1 will have a 10% chance\nof qualifying. If we pick 0.83 for R1 then it will have a 83% chance of qualifying. The higher\nthe number, the greater the likelihood that we will actually use it.\nHere is a function (named for the Monte Carlo method, which was named for the Monte\nCarlo casino) that implements the above algorithm, returning a random value between 0 and\n1.\nfloat montecarlo() {\nWe do this “forever” until we find a\nqualifying random value.\nwhile (true) {\nPick a random value.\nfloat r1 = random(1);\nAssign a probability.\nfloat probability = r1;\nPick a second random value.\nfloat r2 = random(1);\nDoes it qualify? If so, we’re done!\nif (r2 < probability) {\nreturn r1;\n}\n}\n}\nIntroduction\n16\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 187
  },
  {
    "chunk_full": "Use a custom probability distribution to vary the size of a step taken by the random\nwalker. The step size can be determined by influencing the range of values picked. Can\nyou map the probability exponentially—i.e. making the likelihood that a value is picked\nequal to the value squared?\n(Later we’ll see how to do this more efficiently using vectors.)\nA uniform distribution of step sizes.\nChange this!\nfloat stepsize = random(0,10);\nfloat stepx = random(-stepsize,stepsize);\nfloat stepy = random(-stepsize,stepsize);\nx += stepx;\ny += stepy;\nExercise I.6\nExercise I.6\nI.6 Perlin Noise (A Smoother Approach)\nI.6 Perlin Noise (A Smoother Approach)\nA good random number generator produces numbers that have no relationship and show no\ndiscernible pattern. As we are beginning to see, a little bit of randomness can be a good thing\nwhen programming organic, lifelike behaviors. However, randomness as the single guiding\nprinciple is not necessarily natural. An algorithm known as “Perlin noise,” named for its\ninventor Ken Perlin, takes this concept into account. Perlin developed the noise function while\nworking on the original Tron movie in the early 1980s; it was designed to create procedural\ntextures for computer-generated effects. In 1997 Perlin won an Academy Award in technical\nachievement for this work. Perlin noise can be used to generate various effects with natural\nqualities, such as clouds, landscapes, and patterned textures like marble.\nPerlin noise has a more organic appearance because it produces a naturally ordered\n(“smooth”) sequence of pseudo-random numbers. The graph on the left below shows Perlin\nnoise over time, with the x-axis representing time; note the smoothness of the curve. The\ngraph on the right shows pure random numbers over time. (The code for generating these\ngraphs is available in the accompanying book downloads.)\nThe Nature of Code (v1.0)\n17\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 188
  },
  {
    "chunk_full": "Processing has a built-in implementation of the Perlin noise algorithm: the function noise().\nThe noise() function takes one, two, or three arguments, as noise is computed in one, two,\nor three dimensions. Let’s start by looking at one-dimensional noise.\nNoise Detail\nNoise Detail\nThe Processing noise reference (http://processing.org/reference/noise_.html) tells us\nthat noise is calculated over several “octaves.” Calling the noiseDetail()\n(http://processing.org/reference/noiseDetail_.html) function will change both the\nnumber of octaves and their importance relative to one another. This in turn changes\nhow the noise function behaves.\nAn online lecture by Ken Perlin lets you learn more about how noise works from\nPerlin himself (http://www.noisemachine.com/talk1/).\nConsider drawing a circle in our Processing window at a random x-location.\nNow, instead of a random x-location, we want a Perlin noise x-location that is “smoother.”\nYou might think that all you need to do is replace random() with noise(), i.e.\nWhile conceptually this is exactly what we want to do—calculate an x-value that ranges\nbetween 0 and the width according to Perlin noise—this is not the correct implementation.\nWhile the arguments to the random() function specify a range of values between a\nminimum and a maximum, noise() does not work this way. Instead, the output range is\nFigure I.5: Noise\nFigure I.6: Random\nA random x-location\nfloat x = random(0,width);\nellipse(x,180,16,16);\nA noise x-location?\nfloat x = noise(0,width);\nIntroduction\n18\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 189
  },
  {
    "chunk_full": "fixed—it always returns a value between 0 and 1. We’ll see in a moment that we can get\naround this easily with Processing’s map() function, but first we must examine what exactly\nnoise() expects us to pass in as an argument.\nWe can think of one-dimensional Perlin noise as a linear sequence of values over time. For\nexample:\nTime\nTime\nNoise Value\nNoise Value\n0\n0.365\n1\n0.363\n2\n0.363\n3\n0.364\n4\n0.366\nNow, in order to access a particular noise value in Processing, we have to pass a specific\n\"moment in time\" to the noise() function. For example:\nAccording to the above table, noise(3) will return 0.364 at time equals 3. We could improve\nthis by using a variable for time and asking for a noise value continuously in draw().\nThe above code results in the same value printed over and over. This happens because we\nare asking for the result of the noise() function at the same point in time—3—over and over.\nIf we increment the time variable t, however, we’ll get a different result.\nfloat n = noise(3);\nfloat t = 3;\nvoid draw() {\nWe need the noise value for a specific\nmoment in time.\nfloat n = noise(t);\nprintln(n);\n}\nTypically we would start at time = 0, though\nthis is arbitrary.\nfloat t = 0;\nvoid draw() {\nfloat n = noise(t);\nprintln(n);\nThe Nature of Code (v1.0)\n19\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 190
  },
  {
    "chunk_full": "How quickly we increment t also affects the smoothness of the noise. If we make large\njumps in time, then we are skipping ahead and the values will be more random.\nTry running the code several times, incrementing t by 0.01, 0.02, 0.05, 0.1, 0.0001, and you\nwill see different results.\nNow, we move forward in time!\nt += 0.01;\n}\nFigure I.7\nMapping Noise\nMapping Noise\nNow we’re ready to answer the question of what to do with the noise value. Once we have\nthe value with a range between 0 and 1, it’s up to us to map that range to what we want. The\neasiest way to do this is with Processing’s map() function. The map() function takes five\narguments. First up is the value we want to map, in this case n. Then we have to give it the\nvalue’s current range (minimum and maximum), followed by our desired range.\nIn this case, we know that noise has a range between 0 and 1, but we’d like to draw our\ncircle with a range between 0 and the window’s width.\nFigure I.8\nIntroduction\n20\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 191
  },
  {
    "chunk_full": "We can apply the exact same logic to our random walker, and assign both its x- and y-values\naccording to Perlin noise.\nExample I.5: Perlin noise walker\nfloat t = 0;\nvoid draw() {\nfloat n = noise(t);\nUsing map() to customize the range of\nPerlin noise\nfloat x = map(n,0,1,0,width);\nellipse(x,180,16,16);\nt += 0.01;\n}\nclass Walker {\nfloat x,y;\nfloat tx,ty;\nWalker() {\ntx = 0;\nty = 10000;\n}\nvoid step() {\nx- and y-location mapped from noise\nx = map(noise(tx), 0, 1, 0, width);\ny = map(noise(ty), 0, 1, 0, height);\nMove forward through “time.”\ntx += 0.01;\nty += 0.01;\n}\n}\nThe Nature of Code (v1.0)\n21\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 192
  },
  {
    "chunk_full": "Notice how the above example requires an additional pair of variables: tx and ty. This is\nbecause we need to keep track of two time variables, one for the x-location of the Walker\nobject and one for the y-location. But there is something a bit odd about these variables.\nWhy does tx start at 0 and ty at 10,000? While these numbers are arbitrary choices, we\nhave very specifically initialized our two time variables with different values. This is because\nthe noise function is deterministic: it gives you the same result for a specific time t each\nand every time. If we asked for the noise value at the same time t for both x and y, then x\nand y would always be equal, meaning that the Walker object would only move along a\ndiagonal. Instead, we simply use two different parts of the noise space, starting at 0 for x\nand 10,000 for y so that x and y can appear to act independently of each other.\nIn truth, there is no actual concept of time at play here. It’s a useful metaphor to help us\nunderstand how the noise function works, but really what we have is space, rather than\ntime. The graph above depicts a linear sequence of noise values in a one-dimensional\nspace, and we can ask for a value at a specific x-location whenever we want. In examples,\nyou will often see a variable named xoff to indicate the x-offset along the noise graph,\nrather than t for time (as noted in the diagram).\nFigure I.9\nIn the above random walker, the result of the noise function is mapped directly to the\nWalker’s location. Create a random walker where you instead map the result of the\nnoise() function to a Walker’s step size.\nExercise I.7\nExercise I.7\nTwo-Dimensional Noise\nTwo-Dimensional Noise\nThis idea of noise values living in a one-dimensional space is important because it leads us\nright into a discussion of two-dimensional space. Let’s think about this for a moment. With\none-dimensional noise, we have a sequence of values in which any given value is similar to\nits neighbor. Because the value is in one dimension, it only has two neighbors: a value that\ncomes before it (to the left on the graph) and one that comes after it (to the right).\nIntroduction\n22\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 193
  },
  {
    "chunk_full": "Two-dimensional noise works exactly the same way conceptually. The difference of course is\nthat we aren’t looking at values along a linear path, but values that are sitting on a grid. Think\nof a piece of graph paper with numbers written into each cell. A given value will be similar to\nall of its neighbors: above, below, to the right, to the left, and along any diagonal.\nIf you were to visualize this graph paper with each value mapped to the brightness of a color,\nyou would get something that looks like clouds. White sits next to light gray, which sits next to\ngray, which sits next to dark gray, which sits next to black, which sits next to dark gray, etc.\nThis is why noise was originally invented. You tweak the parameters a bit or play with color to\nmake the resulting image look more like marble or wood or any other organic texture.\nLet’s take a quick look at how to implement two-dimensional noise in Processing. If you\nwanted to color every pixel of a window randomly, you would need a nested loop, one that\naccessed each pixel and picked a random brightness.\nFigure I.10: 1D Noise\nFigure I.11: 2D Noise\nloadPixels();\nfor (int x = 0; x < width; x++) {\nfor (int y = 0; y < height; y++) {\nA random brightness!\nfloat bright = random(255);\npixels[x+y*width] = color(bright);\n}\n}\nupdatePixels();\nThe Nature of Code (v1.0)\n23\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 194
  },
  {
    "chunk_full": "To color each pixel according to the noise() function, we’ll do exactly the same thing, only\ninstead of calling random() we’ll call noise().\nThis is a nice start conceptually—it gives you a noise value for every (x,y) location in our\ntwo-dimensional space. The problem is that this won’t have the cloudy quality we want.\nJumping from pixel 200 to pixel 201 is too large of a jump through noise. Remember, when\nwe worked with one-dimensional noise, we incremented our time variable by 0.01 each\nframe, not by 1! A pretty good solution to this problem is to just use different variables for\nthe noise arguments. For example, we could increment a variable called xoff each time we\nmove horizontally, and a yoff variable each time we move vertically through the nested\nloops.\nExample I.6: 2D Perlin noise\nA Perlin noise brightness!\nfloat bright = map(noise(x,y),0,1,0,255);\nStart xoff at 0.\nfloat xoff = 0.0;\nfor (int x = 0; x < width; x++) {\nFor every xoff, start yoff at 0.\nfloat yoff = 0.0;\nfor (int y = 0; y < height; y++) {\nUse xoff and yoff for noise().\nfloat bright =\nmap(noise(xoff,yoff),0,1,0,255);\nUse x and y for pixel location.\npixels[x+y*width] = color(bright);\nIncrement yoff.\nyoff += 0.01;\n}\nIncrement xoff.\nxoff += 0.01;\n}\nPlay with color, noiseDetail(), and the rate at which xoff and yoff are\nincremented to achieve different visual effects.\nExercise I.8\nExercise I.8\nIntroduction\n24\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 195
  },
  {
    "chunk_full": "We’ve examined several traditional uses of Perlin noise in this section. With one-dimensional\nnoise, we used smooth values to assign the location of an object to give the appearance of\nwandering. With two-dimensional noise, we created a cloudy pattern with smoothed values on\na plane of pixels. It’s important to remember, however, that Perlin noise values are just\nthat—values. They aren’t inherently tied to pixel locations or color. Any example in this book\nthat has a variable could be controlled via Perlin noise. When we model a wind force, its\nstrength could be controlled by Perlin noise. Same goes for the angles between the branches\nin a fractal tree pattern, or the speed and direction of objects moving along a grid in a flow\nfield simulation.\nAdd a third argument to noise that increments once per cycle through draw() to\nanimate the two-dimensional noise.\nExercise I.9\nExercise I.9\nUse the noise values as the elevations of a landscape. See the screenshot below as a\nreference.\nExercise I.10\nExercise I.10\nThe Nature of Code (v1.0)\n25\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 196
  },
  {
    "chunk_full": "Figure I.12: Tree with Perlin noise\nFigure I.13: Flow field with Perlin noise\nI.7 Onward\nI.7 Onward\nWe began this chapter by talking about how randomness can be a crutch. In many ways, it’s\nthe most obvious answer to the kinds of questions we ask continuously—how should this\nobject move? What color should it be? This obvious answer, however, can also be a lazy\none.\nAs we finish off the introduction, it’s also worth noting that we could just as easily fall into\nthe trap of using Perlin noise as a crutch. How should this object move? Perlin noise! What\ncolor should it be? Perlin noise! How fast should it grow? Perlin noise!\nThe point of all of this is not to say that you should or shouldn’t use randomness. Or that\nyou should or shouldn’t use Perlin noise. The point is that the rules of your system are\ndefined by you, and the larger your toolbox, the more choices you’ll have as you implement\nthose rules. The goal of this book is to fill your toolbox. If all you know is random, then your\ndesign thinking is limited. Sure, Perlin noise helps, but you’ll need more. A lot more.\nI think we’re ready to begin.\nIntroduction\n26\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 197
  },
  {
    "chunk_full": "Chapter 1. Vectors\nChapter 1. Vectors\n“Roger, Roger. What’s our vector, Victor?”\n— Captain Oveur (Airplane)\nThis book is all about looking at the world around us and coming up with clever ways to\nsimulate that world with code. Divided into three parts, the book will start by looking at basic\nphysics—how an apple falls from a tree, a pendulum swings in the air, the earth revolves\naround the sun, etc. Absolutely everything contained within the first five chapters of this book\nrequires the use of the most basic building block for programming motion—the vector\nvector. And so\nthis is where we begin our story.\nNow, the word vector can mean a lot of different things. Vector is the name of a New Wave\nrock band formed in Sacramento, CA in the early 1980s. It’s the name of a breakfast cereal\nmanufactured by Kellogg’s Canada. In the field of epidemiology, a vector is used to describe\nan organism that transmits infection from one host to another. In the C++ programming\nlanguage, a vector (std::vector) is an implementation of a dynamically resizable array data\nstructure. While all these definitions are interesting, they’re not what we’re looking for. What\nwe want is called a Euclidean vector\nEuclidean vector (named for the Greek mathematician Euclid and also\nknown as a geometric vector). When you see the term “vector” in this book, you can assume it\nrefers to a Euclidean vector, defined as an entity that has both magnitude and direction.\nA vector is typically drawn as a arrow; the direction is indicated by where the arrow is\npointing, and the magnitude by the length of the arrow itself.\nThe Nature of Code (v1.0)\n27\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 198
  },
  {
    "chunk_full": "In the above illustration, the vector is drawn as an arrow from point A to point B and serves\nas an instruction for how to travel from A to B.\nFigure 1.1: A vector (drawn as an arrow) has magnitude (length of arrow) and direction (which way\nit is pointing).\n1.1 Vectors, You Complete Me\n1.1 Vectors, You Complete Me\nBefore we dive into more of the details about vectors, let’s look at a basic Processing\nexample that demonstrates why we should care about vectors in the first place. If you’ve\nread any of the introductory Processing textbooks or taken a class on programming with\nProcessing (and hopefully you’ve done one of these things to help prepare you for this\nbook), you probably, at one point or another, learned how to write a simple bouncing ball\nsketch.\nIf you are reading this book as a PDF or in print, then you will only see screenshots of the code.\nMotion, of course, is a key element of our discussion, so to the extent possible, the static screenshots\nwill include trails to give a sense of the behavior. For more about how to draw trails, see the code\nexamples available for download.\nChapter 1. Vectors\n28\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 199
  },
  {
    "chunk_full": "Location\nLocation\nx and y\nSpeed\nSpeed\nxspeed and yspeed\nExample 1.1: Bouncing ball with no vectors\nIn the above example, we have a very simple world—a blank canvas with a circular shape (a\n“ball”) traveling around. This ball has some properties, which are represented in the code as\nvariables.\nIn a more advanced sketch, we could imagine having many more variables:\nVariables for location and speed of ball.\nfloat x = 100;\nfloat y = 100;\nfloat xspeed = 1;\nfloat yspeed = 3.3;\nRemember how Processing works? setup()\nis executed once when the sketch starts\nand draw() loops forever and ever (until you\nquit).\nvoid setup() {\nsize(640,360);\nbackground(255);\n}\nvoid draw() {\nbackground(255);\nMove the ball according to its speed.\nx = x + xspeed;\ny = y + yspeed;\nCheck for bouncing.\nif ((x > width) || (x < 0)) {\nxspeed = xspeed * -1;\n}\nif ((y > height) || (y < 0)) {\nyspeed = yspeed * -1;\n}\nstroke(0);\nfill(175);\nDisplay the ball at the location (x,y).\nellipse(x,y,16,16);\n}\nThe Nature of Code (v1.0)\n29\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 200
  },
  {
    "chunk_full": "Acceleration\nAcceleration\nxacceleration and yacceleration\nTarget location\nTarget location\nxtarget and ytarget\nWind\nWind\nxwind and ywind\nFriction\nFriction\nxfriction and yfriction\nIt’s becoming clearer that for every concept in this world (wind, location, acceleration, etc.),\nwe’ll need two variables. And this is only a two-dimensional world. In a 3D world, we’ll need\nx, y, z, xspeed, yspeed, zspeed, and so on.\nWouldn’t it be nice if we could simplify our code and use fewer variables?\nInstead of:\nWe could simply have…\nTaking this first step in using vectors won’t allow us to do anything new. Just adding vectors\nwon’t magically make your Processing sketches simulate physics. However, they will\nsimplify your code and provide a set of functions for common mathematical operations that\nhappen over and over and over again while programming motion.\nAs an introduction to vectors, we’re going to live in two dimensions for quite some time (at\nleast until we get through the first several chapters). All of these examples can be fairly\neasily extended to three dimensions (and the class we will use—PVector—allows for three\ndimensions.) However, it’s easier to start with just two.\nfloat x;\nfloat y;\nfloat xspeed;\nfloat yspeed;\nVector location;\nVector speed;\n1.2 Vectors for Processing Programmers\n1.2 Vectors for Processing Programmers\nOne way to think of a vector is the difference between two points. Consider how you might\ngo about providing instructions to walk from one point to another.\nHere are some vectors and possible translations:\nChapter 1. Vectors\n30\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 201
  },
  {
    "chunk_full": "(-15, 3)\n(-15, 3)\nWalk fifteen steps west; turn and walk three steps north.\n(3, 4)\n(3, 4)\nWalk three steps east; turn and walk five steps north.\n(2, -1)\n(2, -1)\nWalk two steps east; turn and walk one step south.\nYou’ve probably done this before when programming motion. For every frame of animation\n(i.e. a single cycle through Processing’s draw() loop), you instruct each object on the screen\nto move a certain number of pixels horizontally and a certain number of pixels vertically.\nFor every frame:\nnew location = velocity applied to current location\nnew location = velocity applied to current location\nIf velocity is a vector (the difference between two points), what is location? Is it a vector too?\nTechnically, one might argue that location is not a vector, since it’s not describing how to\nmove from one point to another—it’s simply describing a singular point in space.\nFigure 1.2\nFigure 1.3\nThe Nature of Code (v1.0)\n31\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 202
  },
  {
    "chunk_full": "location\nlocation\nx,y\nvelocity\nvelocity\nxspeed,yspeed\nNevertheless, another way to describe a location is the path taken from the origin to reach\nthat location. Hence, a location can be the vector representing the difference between\nlocation and origin.\nLet’s examine the underlying data for both location and velocity. In the bouncing ball\nexample, we had the following:\nNotice how we are storing the same data for both—two floating point numbers, an x and a y.\nIf we were to write a vector class ourselves, we’d start with something rather basic:\nAt its core, a PVector is just a convenient way to store two values (or three, as we’ll see in\n3D examples).\nAnd so this …\nFigure 1.4\nclass PVector {\nfloat x;\nfloat y;\nPVector(float x_, float y_) {\nx = x_;\ny = y_;\n}\n}\nChapter 1. Vectors\n32\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 203
  },
  {
    "chunk_full": "becomes …\nNow that we have two vector objects (location and velocity), we’re ready to implement the\nalgorithm for motion—location = location + velocity\nlocation = location + velocity. In Example 1.1, without vectors, we had:\nIn an ideal world, we would be able to rewrite the above as:\nHowever, in Processing, the addition operator + is reserved for primitive values (integers,\nfloats, etc.) only. Processing doesn’t know how to add two PVector objects together any more\nthan it knows how to add two PFont objects or PImage objects. Fortunately for us, the\nPVector class includes functions for common mathematical operations.\nfloat x = 100;\nfloat y = 100;\nfloat xspeed = 1;\nfloat yspeed = 3.3;\nPVector location = new PVector(100,100);\nPVector velocity = new PVector(1,3.3);\nAdd each speed to each location.\nx = x + xspeed;\ny = y + yspeed;\nAdd the velocity vector to the location\nvector.\nlocation = location + velocity;\n1.3 Vector Addition\n1.3 Vector Addition\nBefore we continue looking at the PVector class and its add() method (purely for the sake of\nlearning since it’s already implemented for us in Processing itself), let’s examine vector\naddition using the notation found in math and physics textbooks.\nVectors are typically written either in boldface type or with an arrow on top. For the purposes\nof this book, to distinguish a vector\nvector from a scalar\nscalar (scalar refers to a single value, such as an\ninteger or a floating point number), we’ll use the arrow notation:\n•\nVector: u\n→\n•\nScalar: x\nLet’s say I have the following two vectors:\nThe Nature of Code (v1.0)\n33\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 204
  },
  {
    "chunk_full": "Each vector has two components, an x and a y. To add two vectors together, we simply add\nboth x’s and both y’s.\nIn other words:\nw\n→= u\n→+ v\n→\ncan be written as:\nwx = ux + vx\nwy = uy + vy\nThen, replacing u and v with their values from Figure 1.6, we get:\nwx = 5 + 3\nwhich means that:\nFinally, we write that as a vector:\nw\n→= (8, 6)\nFigure 1.5\nFigure 1.6\nChapter 1. Vectors\n34\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 205
  },
  {
    "chunk_full": "Now that we understand how to add two vectors together, we can look at how addition is\nimplemented in the PVector class itself. Let’s write a function called add() that takes another\nPVector object as its argument.\nNow that we see how add() is written inside of PVector, we can return to our bouncing ball\nexample with its location + velocity\nlocation + velocity algorithm and implement vector addition:\nAnd here we are, ready to rewrite the bouncing ball example using PVector.\nExample 1.2: Bouncing ball with PVectors!\nclass PVector {\nfloat x;\nfloat y;\nPVector(float x_, float y_) {\nx = x_;\ny = y_;\n}\nNew! A function to add another PVector to\nthis PVector. Simply add the x components\nand the y components together.\nvoid add(PVector v) {\ny = y + v.y;\nx = x + v.x;\n}\n}\nAdd the current velocity to the location.\nlocation = location + velocity;\nlocation.add(velocity);\nInstead of a bunch of floats, we now just\nhave two PVector variables.\nPVector location;\nPVector velocity;\nvoid setup() {\nsize(640,360);\nlocation = new PVector(100,100);\nvelocity = new PVector(2.5,5);\n}\nvoid draw() {\nbackground(255);\nlocation.add(velocity);\nThe Nature of Code (v1.0)\n35\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 206
  },
  {
    "chunk_full": "Now, you might feel somewhat disappointed. After all, this may initially appear to have\nmade the code more complicated than the original version. While this is a perfectly\nreasonable and valid critique, it’s important to understand that we haven’t fully realized the\npower of programming with vectors just yet. Looking at a simple bouncing ball and only\nimplementing vector addition is just the first step. As we move forward into a more complex\nworld of multiple objects and multiple forces\nforces (which we’ll introduce in Chapter 2), the\nbenefits of PVector will become more apparent.\nWe should, however, note an important aspect of the above transition to programming with\nvectors. Even though we are using PVector objects to describe two values—the x and y of\nlocation and the x and y of velocity—we still often need to refer to the x and y components\nof each PVector individually. When we go to draw an object in Processing, there’s no\nmeans for us to say:\nThe ellipse() function does not allow for a PVector as an argument. An ellipse can only\nbe drawn with two scalar values, an x-coordinate and a y-coordinate. And so we must dig\ninto the PVector object and pull out the x and y components using object-oriented dot\nsyntax.\nThe same issue arises when testing if the circle has reached the edge of the window, and\nwe need to access the individual components of both vectors: location and velocity.\nWe still sometimes need to refer to the\nindividual components of a PVector and\ncan do so using the dot syntax: location.x,\nvelocity.y, etc.\nif ((location.x > width) || (location.x < 0)) {\nvelocity.x = velocity.x * -1;\n}\nif ((location.y > height) || (location.y < 0)) {\nvelocity.y = velocity.y * -1;\n}\nstroke(0);\nfill(175);\nellipse(location.x,location.y,16,16);\n}\nellipse(location,16,16);\nellipse(location.x,location.y,16,16);\nif ((location.x > width) || (location.x < 0)) {\nvelocity.x = velocity.x * -1;\n}\nChapter 1. Vectors\n36\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 207
  },
  {
    "chunk_full": "Find something you’ve previously made in Processing using separate x and y variables\nand use PVectors instead.\nExercise 1.1\nExercise 1.1\nTake one of the walker examples from the introduction and convert it to use PVectors.\nExercise 1.2\nExercise 1.2\nExtend the bouncing ball with vectors example into 3D. Can you get a sphere to bounce\naround a box?\nExercise 1.3\nExercise 1.3\n1.4 More Vector Math\n1.4 More Vector Math\nAddition was really just the first step. There are many mathematical operations that are\ncommonly used with vectors. Below is a comprehensive list of the operations available as\nfunctions in the PVector class. We’ll go through a few of the key ones now. As our examples\nget more sophisticated in later chapters, we’ll continue to reveal the details of more functions.\n•\nadd() — add vectors\n•\nsub() — subtract vectors\n•\nmult() — scale the vector with multiplication\n•\ndiv() — scale the vector with division\n•\nmag() — calculate the magnitude of a vector\n•\nsetMag() - set the magnitude of a vector\n•\nnormalize() — normalize the vector to a unit length of 1\n•\nlimit() — limit the magnitude of a vector\n•\nheading() — the 2D heading of a vector expressed as an angle\n•\nrotate() — rotate a 2D vector by an angle\nThe Nature of Code (v1.0)\n37\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 208
  },
  {
    "chunk_full": "•\nlerp() — linear interpolate to another vector\n•\ndist() — the Euclidean distance between two vectors (considered as points)\n•\nangleBetween() — find the angle between two vectors\n•\ndot() — the dot product of two vectors\n•\ncross() — the cross product of two vectors (only relevant in three dimensions)\n•\nrandom2D() - make a random 2D vector\n•\nrandom3D() - make a random 3D vector\nHaving already covered addition, let’s start with subtraction. This one’s not so bad; just take\nthe plus sign and replace it with a minus!\nVector subtraction\nVector subtraction\nw\n→= u\n→−v\n→\ncan be written as:\nwx = ux −vx\nwy = uy −vy\nand so the function inside PVector looks like:\nThe following example demonstrates vector subtraction by taking the difference between\ntwo points—the mouse location and the center of the window.\nFigure 1.7: Vector Subtraction\nvoid sub(PVector v) {\nx = x - v.x;\ny = y - v.y;\n}\nChapter 1. Vectors\n38\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 209
  },
  {
    "chunk_full": "Example 1.3: Vector subtraction\nBasic Number Properties with Vectors\nBasic Number Properties with Vectors\nAddition with vectors follow the same algebraic rules as with real numbers.\nThe commutative rule:\nThe commutative rule: u\n→+ v\n→= v\n→+ u\n→\nThe associative rule:\nThe associative rule: u\n→+ (v\n→+ w\n→) = (u\n→+ v\n→) + w\n→\nFancy terminology and symbols aside, this is really quite a simple concept. We’re just\nsaying that common sense properties of addition apply to vectors as well.\n3 + 2 = 2 + 3\n(3 + 2) + 1 = 3 + (2 + 1)\nvoid setup() {\nsize(640,360);\n}\nvoid draw() {\nbackground(255);\nTwo PVectors, one for the mouse location\nand one for the center of the window\nPVector mouse\n= new PVector(mouseX,mouseY);\nPVector center = new PVector(width/2,height/2);\nPVector subtraction!\nmouse.sub(center);\nDraw a line to represent the vector.\ntranslate(width/2,height/2);\nline(0,0,mouse.x,mouse.y);\n}\nThe Nature of Code (v1.0)\n39\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 210
  },
  {
    "chunk_full": "Vector multiplication\nVector multiplication\nMoving on to multiplication, we have to think a little bit differently. When we talk about\nmultiplying a vector, what we typically mean is scaling\nscaling a vector. If we wanted to scale a\nvector to twice its size or one-third of its size (leaving its direction the same), we would say:\n“Multiply the vector by 2” or “Multiply the vector by 1/3.” Note that we are multiplying a\nvector by a scalar, a single number, not another vector.\nTo scale a vector, we multiply each component (x and y) by a scalar.\nw\n→= u\n→* n\ncan be written as:\nwx = ux * n\nwy = uy * n\nLet’s look at an example with vector\nnotation.\nu\n→= (−3, 7)\nn = 3\nw\n→= u\n→* n\nwx = −3 * 3\nwy = 7 * 3\nw\n→= (−9, 21)\nTherefore, the function inside the PVector\nclass is written as:\nAnd implementing multiplication in code is as simple as:\nFigure 1.8: Scaling a vector\nvoid mult(float n) {\nWith multiplication, the components of the\nvector are multiplied by a number.\nx = x * n;\ny = y * n;\n}\nPVector u = new PVector(-3,7);\nThis PVector is now three times the size\nand is equal to (-9,21).\nu.mult(3);\nChapter 1. Vectors\n40\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 211
  },
  {
    "chunk_full": "Example 1.4: Multiplying a vector\nDivision works just like multiplication—we\nsimply replace the multiplication sign\n(asterisk) with the division sign (forward\nslash).\nvoid setup() {\nsize(640,360);\n}\nvoid draw() {\nbackground(255);\nPVector mouse = new PVector(mouseX,mouseY);\nPVector center = new PVector(width/2,height/2);\nmouse.sub(center);\nMultiplying a vector! The vector is now half\nits original size (multiplied by 0.5).\nmouse.mult(0.5);\ntranslate(width/2,height/2);\nline(0,0,mouse.x,mouse.y);\n}\nFigure 1.9\nvoid div(float n) {\nx = x / n;\ny = y / n;\n}\nPVector u = new PVector(8,-4);\nDividing a vector! The vector is now half its\noriginal size (divided by 2).\nu.div(2);\nThe Nature of Code (v1.0)\n41\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 212
  },
  {
    "chunk_full": "More Number Properties with Vectors\nMore Number Properties with Vectors\nAs with addition, basic algebraic rules of multiplication apply to vectors.\nThe associative rule: (n * m) * v\n→= n * (m * v\n→)\nThe distributive rule with 2 scalars, 1 vector: (n * m) * v\n→= n * v\n→+ m * v\n→\nThe distributive rule with 2 vectors, 1 scalar: (u\n→+ v\n→) * n = u\n→* n + v\n→* n\n1.5 Vector Magnitude\n1.5 Vector Magnitude\nMultiplication and division, as we just saw, are means by which the length of the vector can\nbe changed without affecting direction. Perhaps you’re wondering: “OK, so how do I know\nwhat the length of a vector is? I know the components (x and y), but how long (in pixels) is\nthe actual arrow?” Understanding how to calculate the length (also known as magnitude\nmagnitude) of\na vector is incredibly useful and important.\nNotice in the above diagram how the\nvector, drawn as an arrow and two\ncomponents (x and y), creates a right\ntriangle. The sides are the components and\nthe hypotenuse is the arrow itself. We’re\nvery lucky to have this right triangle,\nbecause once upon a time, a Greek\nmathematician named Pythagoras\ndeveloped a lovely formula to describe the\nrelationship between the sides and\nhypotenuse of a right triangle.\nThe Pythagorean theorem is a squared\nplus b squared equals c squared.\nArmed with this formula, we can now\ncompute the magnitude of v\n→as follows:\n∥v\n→∥=\nvx * vx + vy * vy\nor in PVector:\nFigure 1.10: The length or “magnitude” of a\nvector v→is often written as: ∥v→∥\nFigure 1.11: The Pythagorean Theorem\nfloat mag() {\nreturn sqrt(x*x + y*y);\n}\nChapter 1. Vectors\n42\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 213
  },
  {
    "chunk_full": "Example 1.5: Vector magnitude\nvoid setup() {\nsize(640,360);\n}\nvoid draw() {\nbackground(255);\nPVector mouse = new PVector(mouseX,mouseY);\nPVector center = new PVector(width/2,height/2);\nmouse.sub(center);\nThe magnitude (i.e. length) of a vector can\nbe accessed via the mag() function. Here it\nis used as the width of a rectangle drawn at\nthe top of the window.\nfloat m = mouse.mag();\nfill(0);\nrect(0,0,m,10);\ntranslate(width/2,height/2);\nline(0,0,mouse.x,mouse.y);\n}\n1.6 Normalizing Vectors\n1.6 Normalizing Vectors\nCalculating the magnitude of a vector is only the beginning. The magnitude function opens\nthe door to many possibilities, the first of which is normalization\nnormalization. Normalizing refers to the\nprocess of making something “standard” or, well, “normal.” In the case of vectors, let’s\nassume for the moment that a standard vector has a length of 1. To normalize a vector,\ntherefore, is to take a vector of any length and, keeping it pointing in the same direction,\nchange its length to 1, turning it into what is called a unit vector\nunit vector.\nThe Nature of Code (v1.0)\n43\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 214
  },
  {
    "chunk_full": "Since it describes a vector’s direction\nwithout regard to its length, it’s useful to\nhave the unit vector readily accessible.\nWe’ll see this come in handy once we start\nto work with forces in Chapter 2.\nFor any given vector u\n→, its unit vector\n(written as u∧) is calculated as follows:\nu∧=\nu\n→\n∥u\n→∥\nIn other words, to normalize a vector, simply divide each component by its magnitude. This\nis pretty intuitive. Say a vector is of length 5. Well, 5 divided by 5 is 1. So, looking at our\nright triangle, we then need to scale the hypotenuse down by dividing by 5. In that process\nthe sides shrink, divided by 5 as well.\nIn the PVector class, we therefore write\nour normalization function as follows:\nOf course, there’s one small issue. What if the magnitude of the vector is 0? We can’t divide\nby 0! Some quick error checking will fix that right up:\nFigure 1.12\nFigure 1.13\nvoid normalize() {\nfloat m = mag();\ndiv(m);\n}\nvoid normalize() {\nfloat m = mag();\nif (m != 0) {\ndiv(m);\n}\n}\nChapter 1. Vectors\n44\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 215
  },
  {
    "chunk_full": "Example 1.6: Normalizing a vector\nvoid draw() {\nbackground(255);\nPVector mouse = new PVector(mouseX,mouseY);\nPVector center = new PVector(width/2,height/2);\nmouse.sub(center);\nIn this example, after the vector is\nnormalized, it is multiplied by 50 so that it is\nviewable onscreen. Note that no matter\nwhere the mouse is, the vector will have the\nsame length (50) due to the normalization\nprocess.\nmouse.normalize();\nmouse.mult(50);\ntranslate(width/2,height/2);\nline(0,0,mouse.x,mouse.y);\n}\n1.7 Vector Motion: Velocity\n1.7 Vector Motion: Velocity\nAll this vector math stuff sounds like something we should know about, but why? How will it\nactually help us write code? The truth of the matter is that we need to have some patience. It\nwill take some time before the awesomeness of using the PVector class fully comes to light.\nThis is actually a common occurrence when first learning a new data structure. For example,\nwhen you first learn about an array, it might seem like much more work to use an array than to\njust have several variables stand for multiple things. But that plan quickly breaks down when\nyou need a hundred, or a thousand, or ten thousand things. The same can be true for\nPVector. What might seem like more work now will pay off later, and pay off quite nicely. And\nyou don’t have to wait too long, as your reward will come in the next chapter.\nFor now, however, we want to focus on simplicity. What does it mean to program motion using\nvectors? We’ve seen the beginning of this in Example 1.2 (see page 35): the bouncing ball. An\nobject on screen has a location (where it is at any given moment) as well as a velocity\n(instructions for how it should move from one moment to the next). Velocity is added to\nlocation:\nThe Nature of Code (v1.0)\n45\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 216
  },
  {
    "chunk_full": "And then we draw the object at that location:\nThis is Motion 101.\n1.\nAdd velocity to location\nAdd velocity to location\n2.\nDraw object at location\nDraw object at location\nIn the bouncing ball example, all of this code happened in Processing’s main tab, within\nsetup() and draw(). What we want to do now is move towards encapsulating all of the\nlogic for motion inside of a class\nclass. This way, we can create a foundation for programming\nmoving objects in Processing. In section I.2 of the introduction (see page 2), “The Random\nWalker Class,” we briefly reviewed the basics of object-oriented-programming (“OOP”).\nBeyond that short introduction, this book assumes experience with objects and classes in\nProcessing. If you need a refresher, I encourage you to check out the Processing objects\ntutorial (http://processing.org/learning/objects/).\nIn this case, we’re going to create a generic Mover class that will describe a thing moving\naround the screen. And so we must consider the following two questions:\n1.\nWhat data does a mover have?\nWhat data does a mover have?\n2.\nWhat functionality does a mover have?\nWhat functionality does a mover have?\nOur Motion 101 algorithm tells us the answers to these questions. A Mover object has two\npieces of data: location and velocity, which are both PVector objects.\nIts functionality is just about as simple. The Mover needs to move and it needs to be seen.\nWe’ll implement these needs as functions named update() and display(). We’ll put all of\nour motion logic code in update() and draw the object in display().\nlocation.add(velocity);\nellipse(location.x,location.y,16,16);\nclass Mover {\nPVector location;\nPVector velocity;\nvoid update() {\nThe Mover moves.\nlocation.add(velocity);\n}\nvoid display() {\nstroke(0);\nfill(175);\nChapter 1. Vectors\n46\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 217
  },
  {
    "chunk_full": "We’ve forgotten one crucial item, however: the object’s constructor\nconstructor. The constructor is a\nspecial function inside of a class that creates the instance of the object itself. It is where you\ngive instructions on how to set up the object. It always has the same name as the class and is\ncalled by invoking the new\nnew operator:\nIn our case, let’s arbitrarily decide to initialize our Mover object by giving it a random location\nand a random velocity.\nIf object-oriented programming is at all new to you, one aspect here may seem a bit\nconfusing. After all, we spent the beginning of this chapter discussing the PVector class. The\nPVector class is the template for making the location object and the velocity object. So\nwhat are they doing inside of yet another object, the Mover object? In fact, this is just about\nthe most normal thing ever. An object is simply something that holds data (and functionality).\nThat data can be numbers (integers, floats, etc.) or other objects! We’ll see this over and over\nagain in this book. For example, in Chapter 4 (see page 144) we’ll write a class to describe a\nsystem of particles. That ParticleSystem object will have as its data a list of Particle\nobjects…and each Particle object will have as its data several PVector objects!\nLet’s finish off the Mover class by incorporating a function to determine what the object should\ndo when it reaches the edge of the window. For now let’s do something simple, and just have\nit wrap around the edges.\nThe Mover is displayed.\nellipse(location.x,location.y,16,16);\n}\n}\nMover m = new Mover();\nMover() {\nlocation = new PVector(random(width),random(height));\nvelocity = new PVector(random(-2,2),random(-2,2));\n}\nvoid checkEdges() {\nWhen it reaches one edge, set location to\nthe other.\nif (location.x > width) {\nlocation.x = 0;\n} else if (location.x < 0) {\nlocation.x = width;\n}\nif (location.y > height) {\nlocation.y = 0;\n} else if (location.y < 0) {\nlocation.y = height;\n}\nThe Nature of Code (v1.0)\n47\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 218
  },
  {
    "chunk_full": "Now that the Mover class is finished, we can look at what we need to do in our main\nprogram. We first declare a Mover object:\nThen initialize the mover in setup():\nand call the appropriate functions in draw():\nHere is the entire example for reference:\nExample 1.7: Motion 101 (velocity)\n}\nMover mover;\nmover = new Mover();\nmover.update();\nmover.checkEdges();\nmover.display();\nDeclare Mover object.\nMover mover;\nvoid setup() {\nsize(640,360);\nCreate Mover object.\nmover = new Mover();\n}\nvoid draw() {\nbackground(255);\nChapter 1. Vectors\n48\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 219
  },
  {
    "chunk_full": "Call functions on Mover object.\nmover.update();\nmover.checkEdges();\nmover.display();\n}\nclass Mover {\nOur object has two PVectors: location and\nvelocity.\nPVector location;\nPVector velocity;\nMover() {\nlocation = new PVector(random(width),random(height));\nvelocity = new PVector(random(-2,2),random(-2,2));\n}\nvoid update() {\nMotion 101: Location changes by velocity.\nlocation.add(velocity);\n}\nvoid display() {\nstroke(0);\nfill(175);\nellipse(location.x,location.y,16,16);\n}\nvoid checkEdges() {\nif (location.x > width) {\nlocation.x = 0;\n} else if (location.x < 0) {\nlocation.x = width;\n}\nif (location.y > height) {\nlocation.y = 0;\n} else if (location.y < 0) {\nlocation.y = height;\n}\n}\n}\n1.8 Vector Motion: Acceleration\n1.8 Vector Motion: Acceleration\nOK. At this point, we should feel comfortable with two things: (1) what a PVector is and (2) how\nwe use PVectors inside of an object to keep track of its location and movement. This is an\nexcellent first step and deserves a mild round of applause. Before standing ovations and\nscreaming fans, however, we need to make one more, somewhat bigger step forward. After\nall, watching the Motion 101 example is fairly boring—the circle never speeds up, never slows\nThe Nature of Code (v1.0)\n49\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 220
  },
  {
    "chunk_full": "down, and never turns. For more interesting motion, for motion that appears in the real\nworld around us, we need to add one more PVector to our class—acceleration.\nThe strict definition of acceleration\nacceleration we’re using here is: the rate of change of velocity. Let’s\nthink about that definition for a moment. Is this a new concept? Not really. Velocity is\ndefined as the rate of change of location. In essence, we are developing a “trickle-down”\neffect. Acceleration affects velocity, which in turn affects location (for some brief\nforeshadowing, this point will become even more crucial in the next chapter, when we see\nhow forces affect acceleration, which affects velocity, which affects location). In code, this\nreads:\nAs an exercise, from this point forward, let’s make a rule for ourselves. Let’s write every\nexample in the rest of this book without ever touching the value of velocity and location\n(except to initialize them). In other words, our goal now for programming motion is: Come up\nwith an algorithm for how we calculate acceleration and let the trickle-down effect work its\nmagic. (In truth, you’ll find reasons to break this rule, but it’s important to illustrate the\nprinciples behind our motion algorithm.) And so we need to come up with some ways to\ncalculate acceleration:\nvelocity.add(acceleration);\nlocation.add(velocity);\nAcceleration Algorithms!\nAcceleration Algorithms!\n1.\nA constant acceleration\n2.\nA totally random acceleration\n3.\nAcceleration towards the mouse\nAlgorithm #1, a constant acceleration, is not particularly interesting, but it is the simplest\nand will help us begin incorporating acceleration into our code. The first thing we need to\ndo is add another PVector to the Mover class:\nAnd incorporate acceleration into the update() function:\nclass Mover {\nPVector location;\nPVector velocity;\nA new PVector for acceleration\nPVector acceleration;\nvoid update() {\nOur motion algorithm is now two lines of\ncode!\nvelocity.add(acceleration);\nlocation.add(velocity);\n}\nChapter 1. Vectors\n50\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 221
  },
  {
    "chunk_full": "We’re almost done. The only missing piece is initialization in the constructor.\nLet’s start the Mover object in the middle of the window…\n…with an initial velocity of zero.\nThis means that when the sketch starts, the object is at rest. We don’t have to worry about\nvelocity anymore, as we are controlling the object’s motion entirely with acceleration.\nSpeaking of which, according to Algorithm #1, our first sketch involves constant acceleration.\nSo let’s pick a value.\nMaybe you’re thinking, “Gosh, those values seem awfully small!” That’s right, they are quite\ntiny. It’s important to realize that our acceleration values (measured in pixels) accumulate over\ntime in the velocity, about thirty times per second depending on our sketch’s frame rate. And\nso to keep the magnitude of the velocity vector within a reasonable range, our acceleration\nvalues should remain quite small. We can also help this cause by incorporating the PVector\nfunction limit().\nThis translates to the following:\nWhat is the magnitude of velocity? If it’s less than 10, no worries; just leave it as is. If it’s more\nthan 10, however, reduce it to 10!\nMover() {\nlocation = new PVector(width/2,height/2);\nvelocity = new PVector(0,0);\nacceleration = new PVector(-0.001,0.01);\n}\nThe limit() function constrains the magnitude\nof a vector.\nvelocity.limit(10);\nThe Nature of Code (v1.0)\n51\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 222
  },
  {
    "chunk_full": "Let’s take a look at the changes to the Mover class, complete with acceleration and\nlimit().\nExample 1.8: Motion 101 (velocity and constant acceleration)\nWrite the limit() function for the PVector class.\nvoid limit(float max) {\nif (_______ > _______) {\n_________();\n____(max);\n}\n}\nExercise 1.4\nExercise 1.4\nclass Mover {\nPVector location;\nPVector velocity;\nAcceleration is the key!\nPVector acceleration;\nThe variable topspeed will limit the\nmagnitude of velocity.\nfloat topspeed;\nMover() {\nlocation = new PVector(width/2,height/2);\nvelocity = new PVector(0,0);\nacceleration = new PVector(-0.001,0.01);\ntopspeed = 10;\n}\nvoid update() {\nVelocity changes by acceleration and is\nlimited by topspeed.\nvelocity.add(acceleration);\nvelocity.limit(topspeed);\nChapter 1. Vectors\n52\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 223
  },
  {
    "chunk_full": "Now on to Algorithm #2, a totally random acceleration. In this case, instead of initializing\nacceleration in the object’s constructor, we want to pick a new acceleration each cycle, i.e.\neach time update() is called.\nExample 1.9: Motion 101 (velocity and random acceleration)\nBecause the random vector is a normalized one, we can try scaling it:\n(a) scaling the acceleration to a constant value\nlocation.add(velocity);\n}\ndisplay() is the same.\nvoid display() {}\ncheckEdges() is the same.\nvoid checkEdges() {}\n}\nCreate a simulation of a car (or runner) that accelerates when you press the up key and\nbrakes when you press the down key.\nExercise 1.5\nExercise 1.5\nvoid update() {\nThe random2D() function will give us a\nPVector of length 1 pointing in a random\ndirection.\nacceleration = PVector.random2D();\nvelocity.add(acceleration);\nvelocity.limit(topspeed);\nlocation.add(velocity);\n}\nacceleration = PVector.random2D();\nThe Nature of Code (v1.0)\n53\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 224
  },
  {
    "chunk_full": "(b) scaling the acceleration to a random value\nWhile this may seem like an obvious point, it’s crucial to understand that acceleration does\nnot merely refer to the speeding up or slowing down of a moving object, but rather any\nchange in velocity in either magnitude or direction. Acceleration is used to steer an object,\nand we’ll see this again and again in future chapters as we begin to program objects that\nmake decisions about how to move about the screen.\nConstant\nacceleration.mult(0.5);\nacceleration = PVector.random2D();\nRandom\nacceleration.mult(random(2));\nReferring back to the Introduction (see page 17), implement acceleration according to\nPerlin noise.\nExercise 1.6\nExercise 1.6\n1.9 Static vs. Non-Static Functions\n1.9 Static vs. Non-Static Functions\nBefore we get to Algorithm #3 (accelerate towards the mouse), we need to cover one more\nrather important aspect of working with vectors and the PVector class: the difference\nbetween using static\nstatic methods and non-static\nnon-static methods.\nForgetting about vectors for a moment, take a look at the following code:\nPretty simple, right? x has the value of 0, we add y to it, and now x is equal to 5. We could\nwrite the corresponding code pretty easily based on what we’ve learned about PVector.\nThe vector v has the value of (0,0), we add u to it, and now v is equal to (4,5). Easy, right?\nLet’s take a look at another example of some simple floating point math:\nfloat x = 0;\nfloat y = 5;\nx = x + y;\nPVector v = new PVector(0,0);\nPVector u = new PVector(4,5);\nv.add(u);\nChapter 1. Vectors\n54\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 225
  },
  {
    "chunk_full": "x has the value of 0, we add y to it, and store the result in a new variable z. The value of x\ndoes not change in this example (neither does y)! This may seem like a trivial point, and one\nthat is quite intuitive when it comes to mathematical operations with floats. However, it’s not\nso obvious with mathematical operations in PVector. Let’s try to write the code based on what\nwe know so far.\nThe above might seem like a good guess, but it’s just not the way the PVector class works. If\nwe look at the definition of add() . . .\nwe see that this code does not accomplish our goal. First, it does not return a new PVector\n(the return type is “void”) and second, it changes the value of the PVector upon which it is\ncalled. In order to add two PVector objects together and return the result as a new PVector,\nwe must use the static add() function.\nFunctions that we call from the class name itself (rather than from a speciﬁc object instance)\nare known as static functions\nstatic functions. Here are two examples of function calls that assume two\nPVector objects, v and u:\nSince you can’t write static functions yourself in Processing, you might not have encountered\nthem before. PVector's static functions allow us to perform generic mathematical operations\non PVector objects without having to adjust the value of one of the input PVectors. Let’s look\nat how we might write the static version of add():\nfloat x = 0;\nfloat y = 5;\nfloat z = x + y;\nPVector v = new PVector(0,0);\nPVector u = new PVector(4,5);\nDon’t be fooled; this is incorrect!!!\nPVector w = v.add(u);\nvoid add(PVector v) {\nx = x + v.x;\ny = y + v.y;\n}\nStatic: called from the class name.\nPVector.add(v,u);\nNot static: called from an object instance.\nv.add(u);\nThe Nature of Code (v1.0)\n55\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 226
  },
  {
    "chunk_full": "There are several differences here:\n•\nThe function is labeled as static\nstatic.\n•\nThe function does not have a void\nvoid return type, but rather returns a PVector.\n•\nThe function creates a new PVector (v3) and returns the sum of the components\nof v1 and v2 in that new PVector.\nWhen you call a static function, instead of referencing an actual object instance, you simply\nreference the name of the class itself.\nThe PVector class has static versions of add(), sub(), mult(), and div().\nThe static version of add allows us to add\ntwo PVectors together and assign the\nresult to a new PVector while leaving the\noriginal PVectors (v and u above) intact.\nstatic PVector add(PVector v1, PVector v2) {\nPVector v3 = new PVector(v1.x + v2.x, v1.y + v2.y);\nreturn v3;\n}\nPVector v = new PVector(0,0);\nPVector u = new PVector(4,5);\nPVector w = v.add(u);\nPVector w = PVector.add(v,u);\nTranslate the following pseudocode to code using static or non-static functions where\nappropriate.\n•\nThe PVector v equals (1,5).\n•\nThe PVector u equals v multiplied by 2.\n•\nThe PVector w equals v minus u.\n•\nDivide the PVector w by 3.\nPVector v = new PVector(1,5);\nPVector u = ________._____(__,__);\nPVector w = ________._____(__,__);\n___________;\nExercise 1.7\nExercise 1.7\nChapter 1. Vectors\n56\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 227
  },
  {
    "chunk_full": "1.10 Interactivity with Acceleration\n1.10 Interactivity with Acceleration\nTo finish out this chapter, let’s try something\na bit more complex and a great deal more\nuseful. We’ll dynamically calculate an\nobject’s acceleration according to a rule\nstated in Algorithm #3 — the object\naccelerates towards the mouse.\nAnytime we want to calculate a vector based\non a rule or a formula, we need to compute\ntwo things: magnitude\nmagnitude and direction\ndirection. Let’s start with direction. We know the acceleration\nvector should point from the object’s location towards the mouse location. Let’s say the object\nis located at the point (x,y) and the mouse at (mouseX,mouseY).\nIn Figure 1.15, we see that we can get a\nvector (dx,dy) by subtracting the object’s\nlocation from the mouse’s location.\n•\ndx = mouseX - x\n•\ndy = mouseY - y\nLet’s rewrite the above using PVector\nsyntax. Assuming we are in the Mover class\nand thus have access to the object’s PVector location, we then have:\nWe now have a PVector that points from the mover’s location all the way to the mouse. If the\nobject were to actually accelerate using that vector, it would appear instantaneously at the\nmouse location. This does not make for good animation, of course, and what we want to do\nnow is decide how quickly that object should accelerate toward the mouse.\nIn order to set the magnitude (whatever it may be) of our acceleration PVector, we must first\n___ that direction vector. That’s right, you said it. Normalize. If we can shrink the vector down\nto its unit vector (of length one) then we have a vector that tells us the direction and can easily\nbe scaled to any value. One multiplied by anything equals anything.\nFigure 1.14\nFigure 1.15\nPVector mouse = new PVector(mouseX,mouseY);\nLook! We’re using the static reference to\nsub() because we want a new PVector\npointing from one point to another.\nPVector dir = PVector.sub(mouse,location);\nfloat anything = ?????\ndir.normalize();\ndir.mult(anything);\nThe Nature of Code (v1.0)\n57\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 228
  },
  {
    "chunk_full": "To summarize, we take the following steps:\n1.\nCalculate a vector that points from the object to the target location (mouse)\n2.\nNormalize that vector (reducing its length to 1)\n3.\nScale that vector to an appropriate value (by multiplying it by some value)\n4.\nAssign that vector to acceleration\nAnd here are those steps in the update() function itself:\nExample 1.10: Accelerating towards the mouse\nYou may be wondering why the circle doesn’t stop when it reaches the target. It’s important\nto note that the object moving has no knowledge about trying to stop at a destination; it\nonly knows where the destination is and tries to go there as quickly as possible. Going as\nvoid update() {\nPVector mouse = new PVector(mouseX,mouseY);\nStep 1: Compute direction\nPVector dir = PVector.sub(mouse,location);\nStep 2: Normalize\ndir.normalize();\nStep 3: Scale\ndir.mult(0.5);\nStep 4: Accelerate\nacceleration = dir;\nvelocity.add(acceleration);\nvelocity.limit(topspeed);\nlocation.add(velocity);\n}\nChapter 1. Vectors\n58\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 229
  },
  {
    "chunk_full": "quickly as possible means it will inevitably overshoot the location and have to turn around,\nagain going as quickly as possible towards the destination, overshooting it again, and so on\nand so forth. Stay tuned; in later chapters we’ll learn how to program an object to arrive\narrive at a\nlocation (slow down on approach).\nThis example is remarkably close to the concept of gravitational attraction (in which the object\nis attracted to the mouse location). Gravitational attraction will be covered in more detail in the\nnext chapter. However, one thing missing here is that the strength of gravity (magnitude of\nacceleration) is inversely proportional to distance. This means that the closer the object is to\nthe mouse, the faster it accelerates.\nLet’s see what this example would look like with an array of movers (rather than just one).\nExample 1.11: Array of movers accelerating towards the mouse\nTry implementing the above example with a variable magnitude of acceleration,\nstronger when it is either closer or farther away.\nExercise 1.8\nExercise 1.8\nAn array of objects\nMover[] movers = new Mover[20];\nvoid setup() {\nsize(640,360);\nbackground(255);\nfor (int i = 0; i < movers.length; i++) {\nThe Nature of Code (v1.0)\n59\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 230
  },
  {
    "chunk_full": "Initialize each object in the array.\nmovers[i] = new Mover();\n}\n}\nvoid draw() {\nbackground(255);\nfor (int i = 0; i < movers.length; i++) {\nCalling functions on all the objects in the\narray\nmovers[i].update();\nmovers[i].checkEdges();\nmovers[i].display();\n}\n}\nclass Mover {\nPVector location;\nPVector velocity;\nPVector acceleration;\nfloat topspeed;\nMover() {\nlocation = new PVector(random(width),random(height));\nvelocity = new PVector(0,0);\ntopspeed = 4;\n}\nvoid update() {\nOur algorithm for calculating\nacceleration:\nFind the vector pointing towards the\nmouse.\nPVector mouse = new PVector(mouseX,mouseY);\nPVector dir = PVector.sub(mouse,location);\nNormalize.\ndir.normalize();\nScale.\ndir.mult(0.5);\nSet to acceleration.\nacceleration = dir;\nMotion 101! Velocity changes by\nacceleration. Location changes by velocity.\nvelocity.add(acceleration);\nvelocity.limit(topspeed);\nlocation.add(velocity);\n}\nChapter 1. Vectors\n60\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 231
  },
  {
    "chunk_full": "Display the Mover\nvoid display() {\nstroke(0);\nfill(175);\nellipse(location.x,location.y,16,16);\n}\nWhat to do at the edges\nvoid checkEdges() {\nif (location.x > width) {\nlocation.x = 0;\n} else if (location.x < 0) {\nlocation.x = width;\n}\nif (location.y > height) {\nlocation.y = 0;\n}\nelse if (location.y < 0) {\nlocation.y = height;\n}\n}\n}\nThe Nature of Code (v1.0)\n61\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 232
  },
  {
    "chunk_full": "The Ecosystem Project\nThe Ecosystem Project\nAs mentioned in the preface, one way to use this book is to build a single project\nover the course of reading it, incorporating elements from each chapter one step\nat a time. We’ll follow the development of an example project throughout this\nbook—a simulation of an ecosystem. Imagine a population of computational\ncreatures swimming around a digital pond, interacting with each other according\nto various rules.\nStep 1 Exercise:\nDevelop a set of rules for simulating the real-world behavior of a creature, such as\na nervous fly, swimming fish, hopping bunny, slithering snake, etc. Can you\ncontrol the object’s motion by only manipulating the acceleration? Try to give the\ncreature a personality through its behavior (rather than through its visual design).\nFigure 1.16: The Ecosystem Project\nChapter 1. Vectors\n62\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 233
  },
  {
    "chunk_full": "Chapter 2. Forces\nChapter 2. Forces\n“Don’t underestimate the Force.”\n— Darth Vader\nIn the final example of Chapter 1, we saw how we could calculate a dynamic acceleration\nbased on a vector pointing from a circle on the screen to the mouse location. The resulting\nmotion resembled a magnetic attraction between circle and mouse, as if some force were\npulling the circle in towards the mouse. In this chapter we will formalize our understanding of\nthe concept of a force and its relationship to acceleration. Our goal, by the end of this chapter,\nis to understand how to make multiple objects move around the screen and respond to a\nvariety of environmental forces.\n2.1 Forces and Newton’s Laws of Motion\n2.1 Forces and Newton’s Laws of Motion\nBefore we begin examining the practical realities of simulating forces in code, let’s take a\nconceptual look at what it means to be a force in the real world. Just like the word “vector,”\n“force” is often used to mean a variety of things. It can indicate a powerful intensity, as in “She\npushed the boulder with great force” or “He spoke forcefully.” The definition of force\nforce that we\ncare about is much more formal and comes from Isaac Newton’s laws of motion:\nA force is a vector that causes an object with mass to accelerate.\nThe Nature of Code (v1.0)\n63\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 234
  },
  {
    "chunk_full": "The good news here is that we recognize the first part of the definition: a force is a vector.\nThank goodness we just spent a whole chapter learning what a vector is and how to\nprogram with PVectors!\nLet’s look at Newton’s three laws of motion in relation to the concept of a force.\nNewton’s First Law\nNewton’s First Law\nNewton’s first law is commonly stated as:\nAn object at rest stays at rest and an object in motion stays in motion.\nHowever, this is missing an important element related to forces. We could expand it by\nstating:\nAn object at rest stays at rest and an object in motion stays in motion at a\nconstant speed and direction unless acted upon by an unbalanced force.\nBy the time Newton came along, the prevailing theory of motion—formulated by\nAristotle—was nearly two thousand years old. It stated that if an object is moving, some sort\nof force is required to keep it moving. Unless that moving thing is being pushed or pulled, it\nwill simply slow down or stop. Right?\nThis, of course, is not true. In the absence of any forces, no force is required to keep an\nobject moving. An object (such as a ball) tossed in the earth’s atmosphere slows down\nbecause of air resistance (a force). An object’s velocity will only remain constant in the\nabsence of any forces or if the forces that act on it cancel each other out, i.e. the net force\nadds up to zero. This is often referred to as equilibrium\nequilibrium. The falling ball will reach a terminal\nvelocity (that stays constant) once the force of air resistance equals the force of gravity.\nFigure 2.1: The pendulum doesn't move because all the forces cancel each other out (add up to a net\nforce of zero).\nChapter 2. Forces\n64\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 235
  },
  {
    "chunk_full": "In our Processing world, we could restate Newton’s first law as follows:\nAn object’s PVector velocity will remain constant if it is in a state of\nequilibrium.\nSkipping Newton’s second law (arguably the most important law for our purposes) for a\nmoment, let’s move on to the third law.\nNewton’s Third Law\nNewton’s Third Law\nThis law is often stated as:\nFor every action there is an equal and opposite reaction.\nThis law frequently causes some confusion in the way that it is stated. For one, it sounds like\none force causes another. Yes, if you push someone, that someone may actively decide to\npush you back. But this is not the action and reaction we are talking about with Newton’s third\nlaw.\nLet’s say you push against a wall. The wall doesn’t actively decide to push back on you. There\nis no “origin” force. Your push simply includes both forces, referred to as an “action/reaction\npair.”\nA better way of stating the law might be:\nForces always occur in pairs. The two forces are of equal strength, but in\nopposite directions.\nNow, this still causes confusion because it sounds like these forces would always cancel each\nother out. This is not the case. Remember, the forces act on different objects. And just\nbecause the two forces are equal, it doesn’t mean that the movements are equal (or that the\nobjects will stop moving).\nTry pushing on a stationary truck. Although the truck is far more powerful than you, unlike a\nmoving one, a stationary truck will never overpower you and send you flying backwards. The\nforce you exert on it is equal and opposite to the force exerted on your hands. The outcome\ndepends on a variety of other factors. If the truck is a small truck on an icy downhill, you’ll\nThe Nature of Code (v1.0)\n65\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 236
  },
  {
    "chunk_full": "probably be able to get it to move. On the other hand, if it’s a very large truck on a dirt road\nand you push hard enough (maybe even take a running start), you could injure your hand.\nAnd if you are wearing roller skates when you push on that truck?\nYou’ll accelerate away from the truck, sliding along the road while the truck stays put. Why\ndo you slide but not the truck? For one, the truck has a much larger mass (which we’ll get\ninto with Newton’s second law). There are other forces at work too, namely the friction of\nthe truck’s tires and your roller skates against the road.\nFigure 2.2\nNewton’s Third Law (as seen through the eyes of Processing)\nNewton’s Third Law (as seen through the eyes of Processing)\nIf we calculate a PVector f that is a force of object A on object B, we must\nalso apply the force—PVector.mult(f,-1);—that B exerts on object A.\nWe’ll see that in the world of Processing programming, we don’t always have to stay true to\nthe above. Sometimes, such as in the case of see gravitational attraction between bodies\n(see page 94), we’ll want to model equal and opposite forces. Other times, such as when\nwe’re simply saying, “Hey, there’s some wind in the environment,” we’re not going to bother\nto model the force that a body exerts back on the air. In fact, we’re not modeling the air at\nall! Remember, we are simply taking inspiration from the physics of the natural world, not\nsimulating everything with perfect precision.\nChapter 2. Forces\n66\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 237
  },
  {
    "chunk_full": "2.2 Forces and Processing—Newton’s Second Law\n2.2 Forces and Processing—Newton’s Second Law\nas a Function\nas a Function\nAnd here we are at the most important law for the Processing programmer.\nNewton’s Second Law\nNewton’s Second Law\nThis law is stated as:\nForce equals mass times acceleration.\nOr:\nF\n→= M × A\n→\nWhy is this the most important law for us? Well, let’s write it a different way.\nA\n→= F\n→/ M\nAcceleration is directly proportional to force and inversely proportional to mass. This means\nthat if you get pushed, the harder you are pushed, the faster you’ll move (accelerate). The\nbigger you are, the slower you’ll move.\nWeight vs. Mass\nWeight vs. Mass\n•\nThe mass\nmass of an object is a measure of the amount of matter in the object\n(measured in kilograms).\n•\nWeight\nWeight, though often mistaken for mass, is technically the force of gravity on\nan object. From Newton’s second law, we can calculate it as mass times the\nacceleration of gravity (w = m * g). Weight is measured in newtons.\n•\nDensity\nDensity is defined as the amount of mass per unit of volume (grams per cubic\ncentimeter, for example).\nNote that an object that has a mass of one kilogram on earth would have a mass of one\nkilogram on the moon. However, it would weigh only one-sixth as much.\nNow, in the world of Processing, what is mass anyway? Aren’t we dealing with pixels? To start\nin a simpler place, let’s say that in our pretend pixel world, all of our objects have a mass\nequal to 1. F/ 1 = F. And so:\nThe Nature of Code (v1.0)\n67\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 238
  },
  {
    "chunk_full": "A\n→= F\n→\nThe acceleration of an object is equal to force. This is great news. After all, we saw in\nChapter 1 that acceleration was the key to controlling the movement of our objects on\nscreen. Location is adjusted by velocity, and velocity by acceleration. Acceleration was\nwhere it all began. Now we learn that force is truly where it all begins.\nLet’s take our Mover class, with location, velocity, and acceleration.\nNow our goal is to be able to add forces to this object, perhaps saying:\nor:\nwhere wind and gravity are PVectors. According to Newton’s second law, we could\nimplement this function as follows.\nclass Mover {\nPVector location;\nPVector velocity;\nPVector acceleration;\n}\nmover.applyForce(wind);\nmover.applyForce(gravity);\nvoid applyForce(PVector force) {\nNewton’s second law at its simplest.\nacceleration = force;\n}\n2.3 Force Accumulation\n2.3 Force Accumulation\nThis looks pretty good. After all, acceleration = force is a literal translation of Newton’s\nsecond law (without mass). Nevertheless, there’s a pretty big problem here. Let’s return to\nwhat we are trying to accomplish: creating a moving object on the screen that responds to\nwind and gravity.\nOk, let’s be the computer for a moment. First, we call applyForce() with wind. And so the\nMover object’s acceleration is now assigned the PVector wind. Second, we call\nmover.applyForce(wind);\nmover.applyForce(gravity);\nmover.update();\nmover.display();\nChapter 2. Forces\n68\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 239
  },
  {
    "chunk_full": "applyForce() with gravity. Now the Mover object’s acceleration is set to the gravity PVector.\nThird, we call update(). What happens in update()? Acceleration is added to velocity.\nWe’re not going to see any error in Processing, but zoinks! We’ve got a major problem. What\nis the value of acceleration when it is added to velocity? It is equal to the gravity force. Wind\nhas been left out! If we call applyForce() more than once, it overrides each previous call.\nHow are we going to handle more than one force?\nThe truth of the matter here is that we started with a simplified statement of Newton’s second\nlaw. Here’s a more accurate way to put it:\nNet Force equals mass times acceleration.\nOr, acceleration is equal to the sum of all forces divided by mass. This makes perfect sense.\nAfter all, as we saw in Newton’s first law, if all the forces add up to zero, an object\nexperiences an equilibrium state (i.e. no acceleration). Our implementation of this is through a\nprocess known as force accumulation\nforce accumulation. It’s actually very simple; all we need to do is add all of\nthe forces together. At any given moment, there might be 1, 2, 6, 12, or 303 forces. As long as\nour object knows how to accumulate them, it doesn’t matter how many forces act on it.\nNow, we’re not finished just yet. Force accumulation has one more piece. Since we’re adding\nall the forces together at any given moment, we have to make sure that we clear acceleration\n(i.e. set it to zero) before each time update() is called. Let’s think about wind for a moment.\nSometimes the wind is very strong, sometimes it’s weak, and sometimes there’s no wind at all.\nAt any given moment, there might be a huge gust of wind, say, when the user holds down the\nmouse.\nWhen the user releases the mouse, the wind will stop, and according to Newton’s first law, the\nobject will continue to move at a constant velocity. However, if we had forgotten to reset\nacceleration to zero, the gust of wind would still be in effect. Even worse, it would add onto\nitself from the previous frame, since we are accumulating forces! Acceleration, in our\nsimulation, has no memory; it is simply calculated based on the environmental forces present\nvelocity.add(acceleration);\nvoid applyForce(PVector force) {\nNewton’s second law, but with force\naccumulation. We now add each force to\nacceleration, one at a time.\nacceleration.add(force);\n}\nif (mousePressed) {\nPVector wind = new PVector(0.5,0);\nmover.applyForce(wind);\n}\nThe Nature of Code (v1.0)\n69\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 240
  },
  {
    "chunk_full": "at a moment in time. This is different than, say, location, which must remember where the\nobject was in the previous frame in order to move properly to the next.\nThe easiest way to implement clearing the acceleration for each frame is to multiply the\nPVector by 0 at the end of update().\nvoid update() {\nvelocity.add(acceleration);\nlocation.add(velocity);\nacceleration.mult(0);\n}\nUsing forces, simulate a helium-filled balloon floating upward and bouncing off the\ntop of a window. Can you add a wind force that changes over time, perhaps\naccording to Perlin noise?\nExercise 2.1\nExercise 2.1\n2.4 Dealing with Mass\n2.4 Dealing with Mass\nOK. We’ve got one tiny little addition to make before we are done with integrating forces\ninto our Mover class and are ready to look at examples. After all, Newton’s second law is\nreally F\n→= M × A\n→, not A\n→= F\n→. Incorporating mass is as easy as adding an instance variable to\nour class, but we need to spend a little more time here because a slight complication will\nemerge.\nFirst we just need to add mass.\nclass Mover {\nPVector location;\nPVector velocity;\nPVector acceleration;\nAdding mass as a float\nfloat mass;\nChapter 2. Forces\n70\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 241
  },
  {
    "chunk_full": "Units of Measurement\nUnits of Measurement\nNow that we are introducing mass, it’s important to make a quick note about units of\nmeasurement. In the real world, things are measured in specific units. We say that two\nobjects are 3 meters apart, the baseball is moving at a rate of 90 miles per hour, or this\nbowling ball has a mass of 6 kilograms. As we’ll see later in this book, sometimes we\nwill want to take real-world units into consideration. However, in this chapter, we’re\ngoing to ignore them for the most part. Our units of measurement are in pixels (“These\ntwo circles are 100 pixels apart”) and frames of animation (“This circle is moving at a\nrate of 2 pixels per frame”). In the case of mass, there isn’t any unit of measurement for\nus to use. We’re just going to make something up. In this example, we’re arbitrarily\npicking the number 10. There is no unit of measurement, though you might enjoy\ninventing a unit of your own, like “1 moog” or “1 yurkle.” It should also be noted that, for\ndemonstration purposes, we’ll tie mass to pixels (drawing, say, a circle with a radius of\n10). This will allow us to visualize the mass of an object. In the real world, however, size\ndoes not definitely indicate mass. A small metal ball could have a much higher mass\nthan a large balloon due to its higher density.\nMass is a scalar (float), not a vector, as it’s just one number describing the amount of matter in\nan object. We could be fancy about things and compute the area of a shape as its mass, but\nit’s simpler to begin by saying, “Hey, the mass of this object is…um, I dunno…how about 10?”\nThis isn’t so great since things only become interesting once we have objects with varying\nmass, but it’ll get us started. Where does mass come in? We use it while applying Newton’s\nsecond law to our object.\nYet again, even though our code looks quite reasonable, we have a fairly major problem here.\nConsider the following scenario with two Mover objects, both being blown away by a wind\nforce.\nMover() {\nlocation = new PVector(random(width),random(height));\nvelocity = new PVector(0,0);\nacceleration = new PVector(0,0);\nmass = 10.0;\n}\nvoid applyForce(PVector force) {\nNewton’s second law (with force\naccumulation and mass)\nforce.div(mass);\nacceleration.add(force);\n}\nThe Nature of Code (v1.0)\n71\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 242
  },
  {
    "chunk_full": "Again, let’s be the computer. Object m1 receives the wind force—(1,0)—divides it by mass\n(10), and adds it to acceleration.\nm1 equals wind force:\n(1,0)\nDivided by mass of 10:\n(0.1,0)\nOK. Moving on to object m2. It also receives the wind force—(1,0). Wait. Hold on a second.\nWhat is the value of the wind force? Taking a closer look, the wind force is actually\nnow—(0.1,0)!! Do you remember this little tidbit about working with objects? When you pass\nan object (in this case a PVector) into a function, you are passing a reference to that object.\nIt’s not a copy! So if a function makes a change to that object (which, in this case, it does by\ndividing by mass) then that object is permanently changed! But we don’t want m2 to receive\na force divided by the mass of object m1. We want it to receive that force in its original\nstate—(1,0). And so we must protect ourselves and make a copy of the PVector f before\ndividing it by mass. Fortunately, the PVector class has a convenient method for making a\ncopy—get(). get() returns a new PVector object with the same data. And so we can\nrevise applyForce() as follows:\nThere’s another way we could write the above function, using the static method div(). For\nhelp with this exercise, review static methods in Chapter 1 (see page 54).\nMover m1 = new Mover();\nMover m2 = new Mover();\nPVector wind = new PVector(1,0);\nm1.applyForce(wind);\nm2.applyForce(wind);\nvoid applyForce(PVector force) {\nMaking a copy of the PVector before using\nit!\nPVector f = force.get();\nf.div(mass);\nacceleration.add(f);\n}\nRewrite the applyForce() method using the static method div() instead of get().\nvoid applyForce(PVector force) {\nPVector f = _______.___(_____,____);\nacceleration.add(f);\n}\nExercise 2.2\nExercise 2.2\nChapter 2. Forces\n72\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 243
  },
  {
    "chunk_full": "2.5 Creating Forces\n2.5 Creating Forces\nLet’s take a moment to remind ourselves where we are. We know what a force is (a vector),\nand we know how to apply a force to an object (divide it by mass and add it to the object’s\nacceleration vector). What are we missing? Well, we have yet to figure out how we get a force\nin the first place. Where do forces come from?\nIn this chapter, we’ll look at two methods for creating forces in our Processing world.\n1.\nMake up a force!\nMake up a force! After all, you are the programmer, the creator of your world.\nThere’s no reason why you can’t just make up a force and apply it.\n2.\nModel a force!\nModel a force! Yes, forces exist in the real world. And physics textbooks often\ncontain formulas for these forces. We can take these formulas, translate them into\nsource code, and model real-world forces in Processing.\nThe easiest way to make up a force is to just pick a number. Let’s start with the idea of\nsimulating wind. How about a wind force that points to the right and is fairly weak? Assuming\na Mover object m, our code would look like:\nThe result isn’t terribly interesting, but it is a good place to start. We create a PVector object,\ninitialize it, and pass it into an object (which in turn will apply it to its own acceleration). If we\nwanted to have two forces, perhaps wind and gravity (a bit stronger, pointing down), we might\nwrite the following:\nExample 2.1: Forces\nPVector wind = new PVector(0.01,0);\nm.applyForce(wind);\nPVector wind = new PVector(0.01,0);\nPVector gravity = new PVector(0,0.1);\nm.applyForce(wind);\nm.applyForce(gravity);\nThe Nature of Code (v1.0)\n73\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 244
  },
  {
    "chunk_full": "Now we have two forces, pointing in different directions with different magnitudes, both\napplied to object m. We’re beginning to get somewhere. We’ve now built a world for our\nobjects in Processing, an environment to which they can actually respond.\nLet’s look at how we could make this example a bit more exciting with many objects of\nvarying mass. To do this, we’ll need a quick review of object-oriented programming. Again,\nwe’re not covering all the basics of programming here (for that you can check out any of the\nintro Processing books listed in the introduction). However, since the idea of creating a\nworld filled with objects is pretty fundamental to all the examples in this book, it’s worth\ntaking a moment to walk through the steps of going from one object to many.\nThis is where we are with the Mover class as a whole. Notice how it is identical to the Mover\nclass created in Chapter 1, with two additions—mass and a new applyForce() function.\nclass Mover {\nPVector location;\nPVector velocity;\nPVector acceleration;\nThe object now has mass!\nfloat mass;\nMover() {\nAnd for now, we’ll just set the mass equal\nto 1 for simplicity.\nmass = 1;\nlocation = new PVector(30,30);\nvelocity = new PVector(0,0);\nacceleration = new PVector(0,0);\n}\nNewton’s second law.\nvoid applyForce(PVector force) {\nReceive a force, divide by mass, and add\nto acceleration.\nPVector f = PVector.div(force,mass);\nacceleration.add(f);\n}\nvoid update() {\nMotion 101 from Chapter 1\nvelocity.add(acceleration);\nlocation.add(velocity);\nNow add clearing the acceleration each\ntime!\nacceleration.mult(0);\n}\nvoid display() {\nstroke(0);\nfill(175);\nellipse(location.x,location.y,mass*16,mass*16);\n}\nScaling the size according to mass.\nChapter 2. Forces\n74\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 245
  },
  {
    "chunk_full": "Now that our class is set, we can choose to create, say, one hundred Mover objects with an\narray.\nAnd then we can initialize all of those Mover objects in setup() with a loop.\nBut now we have a small issue. If we refer back to the Mover object’s constructor…\n…we discover that every Mover object is made exactly the same way. What we want are Mover\nobjects of varying mass that start at varying locations. Here is where we need to increase the\nsophistication of our constructor by adding arguments.\nSomewhat arbitrarily, we are deciding that\nan object bounces when it hits the edges of\na window.\nvoid checkEdges() {\nif (location.x > width) {\nlocation.x = width;\nvelocity.x *= -1;\n} else if (location.x < 0) {\nvelocity.x *= -1;\nlocation.x = 0;\n}\nif (location.y > height) {\nEven though we said we shouldn't touch\nlocation and velocity directly, there are some\nexceptions. Here we are doing so as a quick\nand easy way to reverse the direction of our\nobject when it reaches the edge.\nvelocity.y *= -1;\nlocation.y = height;\n}\n}\n}\nMover[] movers = new Mover[100];\nvoid setup() {\nfor (int i = 0; i < movers.length; i++) {\nmovers[i] = new Mover();\n}\n}\nMover() {\nEvery object has a mass of 1 and a location\nof (30,30).\nmass = 1;\nlocation = new PVector(30,30);\nvelocity = new PVector(0,0);\nacceleration = new PVector(0,0);\n}\nMover(float m, float x , float y) {\nNow setting these variables with arguments\nmass = m;\nlocation = new PVector(x,y);\nThe Nature of Code (v1.0)\n75\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 246
  },
  {
    "chunk_full": "Notice how the mass and location are no longer set to hardcoded numbers, but rather\ninitialized via arguments passed through the constructor. This means we can create a\nvariety of Mover objects: big ones, small ones, ones that start on the left side of the screen,\nones that start on the right, etc.\nWith an array, however, we want to initialize all of the objects with a loop.\nFor each mover created, the mass is set to a random value between 0.1 and 5, the starting\nx-location is set to 0, and the starting y-location is set to 0. Certainly, there are all sorts of\nways we might choose to initialize the objects; this is just a demonstration of one possibility.\nOnce the array of objects is declared, created, and initialized, the rest of the code is simple.\nWe run through every object, hand them each the forces in the environment, and enjoy the\nshow.\nvelocity = new PVector(0,0);\nacceleration = new PVector(0,0);\n}\nA big Mover on the left side of the window\nMover m1 = new Mover(10,0,height/2);\nA small Mover on the right side of the\nwindow\nMover m1 = new Mover(0.1,width,height/2);\nvoid setup() {\nfor (int i = 0; i < movers.length; i++) {\nInitializing many Mover objects, all with\nrandom mass (and all starting at 0,0)\nmovers[i] = new Mover(random(0.1,5),0,0);\n}\n}\nChapter 2. Forces\n76\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 247
  },
  {
    "chunk_full": "Example 2.2: Forces acting on many objects\nNote how in the above image, the smaller circles reach the right of the window faster than the\nlarger ones. This is because of our formula: acceleration = force divided by mass. The larger\nthe mass, the smaller the acceleration.\nvoid draw() {\nbackground(255);\nPVector wind = new PVector(0.01,0);\nMake up two forces.\nPVector gravity = new PVector(0,0.1);\nLoop through all objects and apply both\nforces to each object.\nfor (int i = 0; i < movers.length; i++) {\nmovers[i].applyForce(wind);\nmovers[i].applyForce(gravity);\nmovers[i].update();\nmovers[i].display();\nmovers[i].checkEdges();\n}\n}\nInstead of objects bouncing off the edge of the wall, create an example in which an\ninvisible force pushes back on the objects to keep them in the window. Can you weight\nthe force according to how far the object is from an edge—i.e., the closer it is, the\nstronger the force?\nExercise 2.3\nExercise 2.3\n2.6 Gravity on Earth and Modeling a Force\n2.6 Gravity on Earth and Modeling a Force\nYou may have noticed something woefully inaccurate about this last example. The smaller the\ncircle, the faster it falls. There is a logic to this; after all, we just stated (according to Newton’s\nsecond law) that the smaller the mass, the higher the acceleration. But this is not what\nhappens in the real world. If you were to climb to the top of the Leaning Tower of Pisa and\ndrop two balls of different masses, which one will hit the ground first? According to legend,\nGalileo performed this exact test in 1589, discovering that they fell with the same acceleration,\nhitting the ground at the same time. Why is this? As we will see later in this chapter, the force\nof gravity is calculated relative to an object’s mass. The bigger the object, the stronger the\nforce. So if the force is scaled according to mass, it is canceled out when acceleration is\ndivided by mass. We can implement this in our sketch rather easily by multiplying our made-up\ngravity force by mass.\nThe Nature of Code (v1.0)\n77\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 248
  },
  {
    "chunk_full": "Example 2.3: Gravity scaled by mass\nWhile the objects now fall at the same rate, because the strength of the wind force is\nindependent of mass, the smaller objects still accelerate to the right more quickly.\nMaking up forces will actually get us quite far. The world of Processing is a pretend world of\npixels and you are its master. So whatever you deem appropriate to be a force, well by\ngolly, that’s the force it should be. Nevertheless, there may come a time where you find\nyourself wondering: “But how does it really all work?”\nOpen up any high school physics textbook and you will find some diagrams and formulas\ndescribing many different forces—gravity, electromagnetism, friction, tension, elasticity, and\nmore. In this chapter we’re going to look at two forces—friction and gravity. The point we’re\nmaking here is not that friction and gravity are fundamental forces that you always need to\nhave in your Processing sketches. Rather, we want to evaluate these two forces as case\nstudies for the following process:\n•\nUnderstanding the concept behind a force\n•\nDeconstructing the force’s formula into two parts:\n◦\nHow do we compute the force’s direction?\nfor (int i = 0; i < movers.length; i++) {\nPVector wind = new PVector(0.001,0);\nfloat m = movers[i].mass;\nScaling gravity by mass to be more\naccurate\nPVector gravity = new PVector(0,0.1*m);\nmovers[i].applyForce(wind);\nmovers[i].applyForce(gravity);\nmovers[i].update();\nmovers[i].display();\nmovers[i].checkEdges();\n}\nChapter 2. Forces\n78\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 249
  },
  {
    "chunk_full": "◦\nHow do we compute the force’s magnitude?\n•\nTranslating that formula into Processing code that calculates a PVector to be sent\nthrough our Mover's applyForce() function\nIf we can follow the above steps with two forces, then hopefully if you ever find yourself\nGoogling “atomic nuclei weak nuclear force” at 3 a.m., you will have the skills to take what you\nfind and adapt it for Processing.\nDealing with formulae\nDealing with formulae\nOK, in a moment we’re going to write out the formula for friction. This isn’t the first time\nwe’ve seen a formula in this book; we just finished up our discussion of Newton’s\nsecond law, F\n→= M × A\n→(or force = mass * acceleration). We didn’t spend a lot of time\nworrying about this formula because it’s a nice and simple one. Nevertheless, it’s a\nscary world out there. Just take a look at the equation for a “normal” distribution, which\nwe covered (without looking at the formula) in the Introduction (see page 10).\nf (x; µ, σ2) =\n1\nσ 2π e−\n(x−µ)2\n2σ2\nWhat we’re seeing here is that formulas like to use a lot of symbols (quite often letters\nfrom the Greek alphabet). Let’s take a look at the formula for friction.\nFriction\n→\n= −µN v∧\nIf it’s been a while since you’ve looked at a formula from a math or physics textbook,\nthere are three key points that are important to cover before we move on.\n•\nEvaluate the right side, assign to the left side.\nEvaluate the right side, assign to the left side. This is just like in code! What\nwe’re doing here is evaluating the right side of the equation and assigning it\nto the left. In the case above, we want to calculate the force of friction—the\nleft side tells us what we want to calculate and the right side tells us how to\ndo it.\n•\nAre we talking about a vector or a scalar?\nAre we talking about a vector or a scalar? It’s important for us to realize that\nin some cases, we’ll be looking at a vector; in others, a scalar. For example, in\nthis case the force of friction is a vector. We can see that by the arrow above\nthe word “friction.” It has a magnitude and direction. The right side of the\nequation also has a vector, as indicated by the symbol v∧, which in this case\nstands for the velocity unit vector.\n•\nWhen symbols are placed next to each other, we mean for them to be\nWhen symbols are placed next to each other, we mean for them to be\nmultiplied.\nmultiplied. The formula above actually has four elements: -1, μ, N, and v∧. We\nwant to multiply them together and read the formula as: Friction\n→\n= −1 * µ * N * v∧\nThe Nature of Code (v1.0)\n79\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 250
  },
  {
    "chunk_full": "2.7 Friction\n2.7 Friction\nLet’s begin with friction and follow our steps.\nFriction is a dissipative force\ndissipative force. A dissipative force is one in which the total energy of a\nsystem decreases when an object is in motion. Let’s say you are driving a car. When you\npress your foot down on the brake pedal, the car’s brakes use friction to slow down the\nmotion of the tires. Kinetic energy (motion) is converted into thermal energy (heat).\nWhenever two surfaces come into contact, they experience friction. A complete model of\nfriction would include separate cases for static friction (a body at rest against a surface) and\nkinetic friction (a body in motion against a surface), but for our purposes, we are only going\nto look at the kinetic case.\nHere’s the formula for friction:\nIt’s now up to us to separate this formula into two components that determine the direction\nof friction as well as the magnitude. Based on the diagram above, we can see that friction\npoints in the opposite direction of velocity. In fact, that’s the part of the formula that says -1 *\nv∧, or -1 times the velocity unit vector. In Processing, this would mean taking the velocity\nvector, normalizing it, and multiplying by -1.\nNotice two additional steps here. First, it’s important to make a copy of the velocity vector,\nas we don’t want to reverse the object’s direction by accident. Second, we normalize the\nvector. This is because the magnitude of friction is not associated with how fast it is moving,\nand we want to start with a friction vector of magnitude 1 so that it can easily be scaled.\nFigure 2.3\nPVector friction = velocity.get();\nfriction.normalize();\nLet’s figure out the direction of the friction\nforce (a unit vector in the opposite direction\nof velocity).\nfriction.mult(-1);\nChapter 2. Forces\n80\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 251
  },
  {
    "chunk_full": "According to the formula, the magnitude is μ * N. μ, the Greek letter mu (pronounced “mew”),\nis used here to describe the coefficient of friction\ncoefficient of friction. The coefficient of friction establishes the\nstrength of a friction force for a particular surface. The higher it is, the stronger the friction; the\nlower, the weaker. A block of ice, for example, will have a much lower coefficient of friction\nthan, say, sandpaper. Since we’re in a pretend Processing world, we can arbitrarily set the\ncoefficient based on how much friction we want to simulate.\nNow for the second part: N. N refers to the normal force\nnormal force, the force perpendicular to the\nobject’s motion along a surface. Think of a vehicle driving along a road. The vehicle pushes\ndown against the road with gravity, and Newton’s third law tells us that the road in turn\npushes back against the vehicle. That’s the normal force. The greater the gravitational force,\nthe greater the normal force. As we’ll see in the next section, gravity is associated with mass,\nand so a lightweight sports car would experience less friction than a massive tractor trailer\ntruck. With the diagram above, however, where the object is moving along a surface at an\nangle, computing the normal force is a bit more complicated because it doesn’t point in the\nsame direction as gravity. We’ll need to know something about angles and trigonometry.\nAll of these specifics are important; however, in Processing, a “good enough” simulation can\nbe achieved without them. We can, for example, make friction work with the assumption that\nthe normal force will always have a magnitude of 1. When we get into trigonometry in the next\nchapter, we’ll remember to return to this question and make our friction example a bit more\nsophisticated. Therefore:\nNow that we have both the magnitude and direction for friction, we can put it all together…\n…and add it to our “forces” example, where many objects experience wind, gravity, and now\nfriction:\nfloat c = 0.01;\nfloat normal = 1;\nfloat c = 0.01;\nfloat normal = 1;\nLet’s figure out the magnitude of friction\n(really just an arbitrary constant).\nfloat frictionMag = c*normal;\nPVector friction = velocity.get();\nfriction.mult(-1);\nfriction.normalize();\nTake the unit vector and multiply it by\nmagnitude and we have our force vector!\nfriction.mult(frictionMag);\nThe Nature of Code (v1.0)\n81\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 252
  },
  {
    "chunk_full": "Example 2.4: Including friction\nRunning this example, you’ll notice that the circles don’t even make it to the right side of the\nwindow. Since friction continuously pushes against the object in the opposite direction of its\nmovement, the object continuously slows down. This can be a useful technique or a\nproblem depending on the goals of your visualization.\nNo friction\nWith friction\nvoid draw() {\nbackground(255);\nPVector wind = new PVector(0.001,0);\nWe could scale by mass to be more\naccurate.\nPVector gravity = new PVector(0,0.1);\nfor (int i = 0; i < movers.length; i++) {\nfloat c = 0.01;\nPVector friction = movers[i].velocity.get();\nfriction.mult(-1);\nfriction.normalize();\nfriction.mult(c);\nApply the friction force vector to the object.\nmovers[i].applyForce(friction);\nmovers[i].applyForce(wind);\nmovers[i].applyForce(gravity);\nmovers[i].update();\nmovers[i].display();\nmovers[i].checkEdges();\n}\n}\nChapter 2. Forces\n82\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 253
  },
  {
    "chunk_full": "Create pockets of friction in a Processing sketch so that objects only experience friction\nwhen crossing over those pockets. What if you vary the strength (friction coefficient) of\neach area? What if you make some pockets feature the opposite of friction—i.e., when\nyou enter a given pocket you actually speed up instead of slowing down?\nExercise 2.4\nExercise 2.4\n2.8 Air and Fluid Resistance\n2.8 Air and Fluid Resistance\nFriction also occurs when a body passes through a liquid or gas. This force has many different\nnames, all really meaning the same thing: viscous force, drag force, fluid resistance. While the\nresult is ultimately the same as our previous friction examples (the object slows down), the\nway in which we calculate a drag force will be slightly different. Let’s look at the formula:\nFd = −1\n2 ρv2ACdv∧\nNow let’s break this down and see what we really need for an effective simulation in\nProcessing, making ourselves a much simpler formula in the process.\n•\nFd refers to drag force, the vector we ultimately want to compute and pass into our\napplyForce() function.\n•\n- 1/2 is a constant: -0.5. This is fairly irrelevant in terms of our Processing world, as\nwe will be making up values for other constants anyway. However, the fact that it is\nnegative is important, as it tells us that the force is in the opposite direction of\nvelocity (just as with friction).\nFigure 2.4\nThe Nature of Code (v1.0)\n83\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 254
  },
  {
    "chunk_full": "•\nρ is the Greek letter rho, and refers to the density of the liquid, something we\ndon’t need to worry about. We can simplify the problem and consider this to have\na constant value of 1.\n• v refers to the speed of the object moving. OK, we’ve got this one! The object’s\nspeed is the magnitude of the velocity vector: velocity.magnitude(). And v2 just\nmeans v squared or v * v.\n• A refers to the frontal area of the object that is pushing through the liquid (or gas).\nAn aerodynamic Lamborghini, for example, will experience less air resistance than\na boxy Volvo. Nevertheless, for a basic simulation, we can consider our object to\nbe spherical and ignore this element.\n•\nCd is the coefficient of drag, exactly the same as the coefficient of friction (ρ). This\nis a constant we’ll determine based on whether we want the drag force to be\nstrong or weak.\n•\nv∧Look familiar? It should. This refers to the velocity unit vector, i.e.\nvelocity.normalize(). Just like with friction, drag is a force that points in the\nopposite direction of velocity.\nNow that we’ve analyzed each of these components and determined what we need for a\nsimple simulation, we can reduce our formula to:\nor:\nLet’s implement this force in our Mover class example with one addition. When we wrote our\nfriction example, the force of friction was always present. Whenever an object was moving,\nfriction would slow it down. Here, let’s introduce an element to the environment—a “liquid”\nthat the Mover objects pass through. The Liquid object will be a rectangle and will know\nFigure 2.5: Our simplified drag force formula\nfloat c = 0.1;\nfloat speed = v.mag();\nPart 1 of our formula (magnitude): Cd * v2\nfloat dragMagnitude = c * speed * speed;\nPVector drag = velocity.get();\nPart 2 of our formula (direction): -1 *\nvelocity\ndrag.mult(-1);\ndrag.normalize();\nMagnitude and direction together!\ndrag.mult(dragMagnitude);\nChapter 2. Forces\n84\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 255
  },
  {
    "chunk_full": "about its location, width, height, and “coefficient of drag”—i.e., is it easy for objects to move\nthrough it (like air) or difficult (like molasses)? In addition, it should include a function to draw\nitself on the screen (and two more functions, which we’ll see in a moment).\nThe main program will now include a Liquid object reference as well as a line of code that\ninitializes that object.\nNow comes an interesting question: how do we get the Mover object to talk to the Liquid\nobject? In other words, we want to execute the following:\nWhen a mover passes through a liquid it experiences a drag force.\n…or in object-oriented speak (assuming we are looping through an array of Mover objects with\nindex i):\nclass Liquid {\nThe liquid object includes a variable defining\nits coefficient of drag.\nfloat x,y,w,h;\nfloat c;\nLiquid(float x_, float y_, float w_, float h_, float c_) {\nx = x_;\ny = y_;\nw = w_;\nh = h_;\nc = c_;\n}\nvoid display() {\nnoStroke();\nfill(175);\nrect(x,y,w,h);\n}\n}\nLiquid liquid;\nvoid setup() {\nliquid = new Liquid(0, height/2, width, height/2, 0.1);\n}\nInitialize a Liquid object. Note the coefficient\nis low (0.1), otherwise the object would\ncome to a halt fairly quickly (which may\nsomeday be the effect you want).\nif (movers[i].isInside(liquid)) {\nIf a Mover is inside a Liquid, apply the drag\nforce.\nmovers[i].drag(liquid);\n}\nThe Nature of Code (v1.0)\n85\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 256
  },
  {
    "chunk_full": "The above code tells us that we need to add two functions to the Mover class: (1) a function\nthat determines if a Mover object is inside the Liquid object, and (2) a function that\ncomputes and applies a drag force on the Mover object.\nThe first is easy; we can simply use a conditional statement to determine if the location\nvector rests inside the rectangle defined by the liquid.\nThe drag() function is a bit more complicated; however, we’ve written the code for it\nalready. This is simply an implementation of our formula. The drag force is equal to the\ncoefficient of drag multiplied by the speed of the Mover squared in the opposite direction of\nvelocity!\nAnd with these two functions added to the Mover class, we’re ready to put it all together in\nthe main tab:\nboolean isInside(Liquid l) {\nif (location.x>l.x && location.x<l.x+l.w && location.y>l.y && location.y<l.y+l.h)\n{\nreturn true;\n} else {\nreturn false;\n}\n}\nThis conditional statement determines if\nthe PVector location is inside the rectangle\ndefined by the Liquid class.\nvoid drag(Liquid l) {\nfloat speed = velocity.mag();\nThe force’s magnitude: Cd * v~2~\nfloat dragMagnitude = l.c * speed * speed;\nPVector drag = velocity.get();\ndrag.mult(-1);\nThe force's direction: -1 * velocity\ndrag.normalize();\nFinalize the force: magnitude and direction\ntogether.\ndrag.mult(dragMagnitude);\nApply the force.\napplyForce(drag);\n}\nChapter 2. Forces\n86\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 257
  },
  {
    "chunk_full": "Example 2.5: Fluid Resistance\nRunning the example, you should notice that we are simulating balls falling into water. The\nobjects only slow down when crossing through the gray area at the bottom of the window\n(representing the liquid). You’ll also notice that the smaller objects slow down a great deal\nmore than the larger objects. Remember Newton’s second law? A = F / M. Acceleration equals\nMover[] movers = new Mover[100];\nLiquid liquid;\nvoid setup() {\nsize(360, 640);\nfor (int i = 0; i < movers.length; i++) {\nmovers[i] = new Mover(random(0.1,5),0,0);\n}\nliquid = new Liquid(0, height/2, width, height/2, 0.1);\n}\nvoid draw() {\nbackground(255);\nliquid.display();\nfor (int i = 0; i < movers.length; i++) {\nif (movers[i].isInside(liquid)) {\nmovers[i].drag(liquid);\n}\nfloat m = 0.1*movers[i].mass;\nNote that we are scaling gravity according to\nmass.\nPVector gravity = new PVector(0, m);\nmovers[i].applyForce(gravity);\nmovers[i].update();\nmovers[i].display();\nmovers[i].checkEdges();\n}\n}\nThe Nature of Code (v1.0)\n87\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 258
  },
  {
    "chunk_full": "force divided by mass. A massive object will accelerate less. A smaller object will accelerate\nmore. In this case, the acceleration we’re talking about is the “slowing down” due to drag.\nThe smaller objects will slow down at a greater rate than the larger ones.\nTake a look at our formula for drag again: drag force = coefficient * speed * speed\ndrag force = coefficient * speed * speed.\nThe faster an object moves, the greater the drag force against it. In fact, an object not\nmoving in water experiences no drag at all. Expand the example to drop the balls\nfrom different heights. How does this affect the drag as they hit the water?\nExercise 2.5\nExercise 2.5\nThe formula for drag also included surface area. Can you create a simulation of boxes\nfalling into water with a drag force dependent on the length of the side hitting the\nwater?\nExercise 2.6\nExercise 2.6\nFluid resistance does not only work opposite to the velocity vector, but also\nperpendicular to it. This is known as “lift-induced drag” and will cause an airplane\nwith an angled wing to rise in altitude. Try creating a simulation of lift.\nExercise 2.7\nExercise 2.7\n2.9 Gravitational Attraction\n2.9 Gravitational Attraction\nProbably the most famous force of all is\ngravity. We humans on earth think of\ngravity as an apple hitting Isaac Newton on\nthe head. Gravity means that stuff falls\ndown. But this is only our experience of\ngravity. In truth, just as the earth pulls the\napple towards it due to a gravitational\nforce, the apple pulls the earth as well. The\nthing is, the earth is just so freaking big\nthat it overwhelms all the other gravity\ninteractions. Every object with mass exerts\na gravitational force on every other object.\nAnd there is a formula for calculating the\nstrengths of these forces, as depicted in Figure 2.6.\nFigure 2.6\nChapter 2. Forces\n88\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 259
  },
  {
    "chunk_full": "Let’s examine this formula a bit more closely.\n•\nF refers to the gravitational force, the vector we ultimately want to compute and pass\ninto our applyForce() function.\n•\nG is the universal gravitational constant, which in our world equals 6.67428 x 10-11\nmeters cubed per kilogram per second squared. This is a pretty important number if\nyour name is Isaac Newton or Albert Einstein. It’s not an important number if you are\na Processing programmer. Again, it’s a constant that we can use to make the forces\nin our world weaker or stronger. Just making it equal to one and ignoring it isn’t such\na terrible choice either.\n•\nm1 and m2 are the masses of objects 1 and 2. As we saw with Newton’s second law (\nF\n→= M × A\n→), mass is also something we could choose to ignore. After all, shapes drawn\non the screen don’t actually have a physical mass. However, if we keep these\nvalues, we can create more interesting simulations in which “bigger” objects exert a\nstronger gravitational force than smaller ones.\n•\nr∧refers to the unit vector pointing from object 1 to object 2. As we’ll see in a\nmoment, we can compute this direction vector by subtracting the location of one\nobject from the other.\n•\nr2 refers to the distance between the two objects squared. Let’s take a moment to\nthink about this a bit more. With everything on the top of the formula—G, m1, m2—the\nbigger its value, the stronger the force. Big mass, big force. Big G, big force. Now,\nwhen we divide by something, we have the opposite. The strength of the force is\ninversely proportional to the distance squared. The farther away an object is, the\nweaker the force; the closer, the stronger.\nHopefully by now the formula makes some sense to us. We’ve looked at a diagram and\ndissected the individual components of the formula. Now it’s time to figure out how we\ntranslate the math into Processing code. Let’s make the following assumptions.\nWe have two objects, and:\n1.\nEach object has a location: PVector location1 and PVector location2.\n2.\nEach object has a mass: float mass1 and float mass2.\n3.\nThere is a variable float G for the universal gravitational constant.\nGiven these assumptions, we want to compute PVector force, the force of gravity. We’ll do it\nin two parts. First, we’ll compute the direction of the force r∧in the formula above. Second,\nwe’ll calculate the strength of the force according to the masses and distance.\nThe Nature of Code (v1.0)\n89\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 260
  },
  {
    "chunk_full": "Remember in Chapter 1 (see page 56),\nwhen we figured out how to have an object\naccelerate towards the mouse? (See Figure\n2.7.)\nA vector is the difference between two\npoints. To make a vector that points from\nthe circle to the mouse, we simply subtract\none point from another:\nIn our case, the direction of the attraction force that object 1 exerts on object 2 is equal to:\nDon’t forget that since we want a unit vector, a vector that tells us about direction only, we’ll\nneed to normalize the vector after subtracting the locations.\nOK, we’ve got the direction of the force. Now we just need to compute the magnitude and\nscale the vector accordingly.\nThe only problem is that we don’t know the\ndistance. G, mass1, and mass2 were all\ngivens, but we’ll need to actually compute\ndistance before the above code will work.\nDidn’t we just make a vector that points all\nthe way from one location to another?\nWouldn’t the length of that vector be the\ndistance between two objects?\nWell, if we add just one line of code and\ngrab the magnitude of that vector before\nnormalizing it, then we’ll have the distance.\nFigure 2.7\nPVector dir = PVector.sub(mouse,location);\nPVector dir = PVector.sub(location1,location2);\ndir.normalize();\nfloat m = (G * mass1 * mass2) / (distance * distance);\ndir.mult(m);\nFigure 2.8\nThe vector that points from one object to\nanother\nPVector force = PVector.sub(location1,location2);\nThe length (magnitude) of that vector is the\ndistance between the two objects.\nfloat distance = force.magnitude();\nUse the formula for gravity to compute the\nstrength of the force.\nfloat m = (G * mass1 * mass2) / (distance *\ndistance);\nChapter 2. Forces\n90\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 261
  },
  {
    "chunk_full": "Note that I also renamed the PVector “dir” as “force.” After all, when we’re finished with the\ncalculations, the PVector we started with ends up being the actual force vector we wanted all\nalong.\nNow that we’ve worked out the math and the code for calculating an attractive force\n(emulating gravity), we need to turn our attention to applying this technique in the context of\nan actual Processing sketch. In Example 2.1, you may recall how we created a simple Mover\nobject—a class with PVector’s location, velocity, and acceleration as well as an\napplyForce(). Let’s take this exact class and put it in a sketch with:\n•\nA single Mover object.\n•\nA single Attractor object (a new\nclass that will have a fixed\nlocation).\nThe Mover object will experience a\ngravitational pull towards the Attractor\nobject, as illustrated in Figure 2.9.\nWe can start by making the new Attractor\nclass very simple—giving it a location and a\nmass, along with a function to display itself\n(tying mass to size).\nAnd in our main program, we can add an instance of the Attractor class.\nNormalize and scale the force vector to the\nappropriate magnitude.\nforce.normalize();\nforce.mult(m);\nFigure 2.9\nclass Attractor {\nOur Attractor is a simple object that doesn’t\nmove. We just need a mass and a location.\nfloat mass;\nPVector location;\nAttractor() {\nlocation = new PVector(width/2,height/2);\nmass = 20;\n}\nvoid display() {\nstroke(0);\nfill(175,200);\nellipse(location.x,location.y,mass*2,mass*2);\n}\n}\nThe Nature of Code (v1.0)\n91\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 262
  },
  {
    "chunk_full": "This is a good structure: a main program with a Mover and an Attractor object, and a class\nto handle the variables and behaviors of movers and attractors. The last piece of the puzzle\nis how to get one object to attract the other. How do we get these two objects to talk to\neach other?\nThere are a number of ways we could do this. Here are just a few possibilities.\nTask\nTask\nFunction\nFunction\n1. A function that receives both an Attractor and a Mover:\nattraction(a,m);\n2. A function in the Attractor class that receives a Mover:\na.attract(m);\n3. A function in the Mover class that receives an Attractor:\nm.attractedTo(a);\n4. A function in the Attractor class that receives a Mover\nand returns a PVector, which is the attraction force. That\nattraction force is then passed into the Mover's\napplyForce() function:\nPVector f = a.attract(m);\nm.applyForce(f);\nand so on. . .\nIt’s good to look at a range of options for making objects talk to each other, and you could\nprobably make arguments for each of the above possibilities. I’d like to at least discard the\nfirst one, since an object-oriented approach is really a much better choice over an arbitrary\nfunction not tied to either the Mover or Attractor class. Whether you pick option 2 or\noption 3 is the difference between saying “The attractor attracts the mover” or “The mover\nis attracted to the attractor.” Number 4 is really my favorite, at least in terms of where we\nMover m;\nAttractor a;\nvoid setup() {\nsize(640,360);\nm = new Mover();\nInitialize Attractor object.\na = new Attractor();\n}\nvoid draw() {\nbackground(255);\nDisplay Attractor object.\na.display();\nm.update();\nm.display();\n}\nChapter 2. Forces\n92\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 263
  },
  {
    "chunk_full": "are in this book. After all, we spent a lot of time working out the applyForce() function, and I\nthink our examples will be clearer if we continue with the same methodology.\nIn other words, where we once had:\nWe now have:\nAnd so our draw() function can now be written as:\nWe’re almost there. Since we decided to put the attract() function inside of the Attractor\nclass, we’ll need to actually write that function. The function needs to receive a Mover object\nand return a PVector, i.e.:\nAnd what goes inside that function? All of that nice math we worked out for gravitational\nattraction!\nMade-up force\nPVector f = new PVector(0.1,0);\nm.applyForce(f);\nAttraction force between two objects\nPVector f = a.attract(m);\nm.applyForce(f);\nvoid draw() {\nbackground(255);\nCalculate attraction force and apply it.\nPVector f = a.attract(m);\nm.applyForce(f);\nm.update();\na.display();\nm.display();\n}\nPVector attract(Mover m) {\n}\nPVector attract(Mover m) {\nWhat’s the force’s direction?\nPVector force = PVector.sub(location,m.location);\nfloat distance = force.mag();\nforce.normalize();\nThe Nature of Code (v1.0)\n93\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 264
  },
  {
    "chunk_full": "And we’re done. Sort of. Almost. There’s one small kink we need to work out. Let’s look at\nthe above code again. See that symbol for divide, the slash? Whenever we have one of\nthese, we need to ask ourselves the question: What would happen if the distance happened\nto be a really, really small number or (even worse!) zero??! Well, we know we can’t divide a\nnumber by 0, and if we were to divide a number by something like 0.0001, that is the\nequivalent of multiplying that number by 10,000! Yes, this is the real-world formula for the\nstrength of gravity, but we don’t live in the real world. We live in the Processing world. And\nin the Processing world, the mover could end up being very, very close to the attractor and\nthe force could become so strong the mover would just fly way off the screen. And so with\nthis formula, it’s good for us to be practical and constrain the range of what distance can\nactually be. Maybe, no matter where the Mover actually is, we should never consider it less\nthan 5 pixels or more than 25 pixels away from the attractor.\nFor the same reason that we need to constrain the minimum distance, it’s useful for us to do\nthe same with the maximum. After all, if the mover were to be, say, 500 pixels from the\nattractor (not unreasonable), we’d be dividing the force by 250,000. That force might end\nup being so weak that it’s almost as if we’re not applying it at all.\nNow, it’s really up to you to decide what behaviors you want. But in the case of, “I want\nreasonable-looking attraction that is never absurdly weak or strong,” then constraining the\ndistance is a good technique.\nOur Mover class hasn’t changed at all, so let’s just look at the main program and the\nAttractor class as a whole, adding a variable G for the universal gravitational constant. (On\nthe website, you’ll find that this example also has code that allows you to move the\nAttractor object with the mouse.)\nfloat strength = (G * mass * m.mass) / (distance * distance);\nforce.mult(strength);\nWhat’s the force’s magnitude?\nReturn the force so that it can be applied!\nreturn force;\n}\ndistance = constrain(distance,5,25);\nChapter 2. Forces\n94\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 265
  },
  {
    "chunk_full": "Example 2.6: Attraction\nA Mover and an Attractor\nMover m;\nAttractor a;\nvoid setup() {\nsize(640,360);\nm = new Mover();\na = new Attractor();\n}\nvoid draw() {\nbackground(255);\nApply the attraction force from the Attractor\non the Mover.\nPVector force = a.attract(m);\nm.applyForce(force);\nm.update();\na.display();\nm.display();\n}\nclass Attractor {\nfloat mass;\nPVector location;\nfloat G;\nAttractor() {\nlocation = new PVector(width/2,height/2);\nmass = 20;\nG = 0.4;\n}\nPVector attract(Mover m) {\nPVector force = PVector.sub(location,m.location);\nfloat distance = force.mag();\nRemember, we need to constrain the\ndistance so that our circle doesn’t spin out of\ncontrol.\ndistance = constrain(distance,5.0,25.0);\nforce.normalize();\nfloat strength = (G * mass * m.mass) / (distance * distance);\nforce.mult(strength);\nreturn force;\n}\nvoid display() {\nstroke(0);\nfill(175,200);\nellipse(location.x,location.y,mass*2,mass*2);\n}\n}\nThe Nature of Code (v1.0)\n95\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 266
  },
  {
    "chunk_full": "And we could, of course, expand this example using an array to include many Mover\nobjects, just as we did with friction and drag:\nExample 2.7: Attraction with many Movers\nNow we have 10 Movers!\nMover[] movers = new Mover[10];\nAttractor a;\nvoid setup() {\nsize(400,400);\nfor (int i = 0; i < movers.length; i++) {\nmovers[i] = new Mover(random(0.1,2),random(width),random(height));\n}\na = new Attractor();\n}\nvoid draw() {\nbackground(255);\na.display();\nfor (int i = 0; i < movers.length; i++) {\nEach Mover is initialized randomly.\nWe calculate an attraction force for each\nMover object.\nPVector force = a.attract(movers[i]);\nmovers[i].applyForce(force);\nmovers[i].update();\nmovers[i].display();\n}\n}\nChapter 2. Forces\n96\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 267
  },
  {
    "chunk_full": "In the example above, we have a system (i.e. array) of Mover objects and one\nAttractor object. Build an example that has systems of both movers and attractors.\nWhat if you make the attractors invisible? Can you create a pattern/design from the\ntrails of objects moving around attractors? See the Metropop Denim project by Clayton\nCubitt and Tom Carden (http://processing.org/exhibition/works/metropop/) for an\nexample.\nExercise 2.8\nExercise 2.8\nIt’s worth noting that gravitational attraction is a model we can follow to develop our\nown forces. This chapter isn’t suggesting that you should exclusively create sketches\nthat use gravitational attraction. Rather, you should be thinking creatively about how to\ndesign your own rules to drive the behavior of objects. For example, what happens if\nyou design a force that is weaker the closer it gets and stronger the farther it gets? Or\nwhat if you design your attractor to attract faraway objects, but repel close ones?\nExercise 2.9\nExercise 2.9\n2.10 Everything Attracts (or Repels) Everything\n2.10 Everything Attracts (or Repels) Everything\nHopefully, you found it helpful that we started with a simple scenario—one object attracts\nanother object—and moved on to one object attracts many objects. However, it’s likely that\nyou are going to find yourself in a slightly more complex situation: many objects attract each\nother. In other words, every object in a given system attracts every other object in that system\n(except for itself).\nWe’ve really done almost all of the work for this already. Let’s consider a Processing sketch\nwith an array of Mover objects:\nThe Nature of Code (v1.0)\n97\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 268
  },
  {
    "chunk_full": "The draw() function is where we need to work some magic. Currently, we’re saying: “for\nevery mover i, update and display yourself.” Now what we need to say is: “for every mover\ni, be attracted to every other mover j, and update and display yourself.”\nTo do this, we need to nest a second loop.\nIn the previous example, we had an Attractor object with a function named attract().\nNow, since we have movers attracting movers, all we need to do is copy the attract()\nfunction into the Mover class.\nMover[] movers = new Mover[10];\nvoid setup() {\nsize(400,400);\nfor (int i = 0; i < movers.length; i++) {\nmovers[i] = new Mover(random(0.1,2),random(width),random(height));\n}\n}\nvoid draw() {\nbackground(255);\nfor (int i = 0; i < movers.length; i++) {\nmovers[i].update();\nmovers[i].display();\n}\n}\nfor (int i = 0; i < movers.length; i++) {\nFor every Mover, check every Mover!\nfor (int j = 0; j < movers.length; j++) {\nPVector force = movers[j].attract(movers[i]);\nmovers[i].applyForce(force);\n}\nmovers[i].update();\nmovers[i].display();\n}\nclass Mover {\n// All the other stuff we had before plus. . .\nChapter 2. Forces\n98\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 269
  },
  {
    "chunk_full": "Of course, there’s one small problem. When we are looking at every mover i and every mover\nj, are we OK with the times that i equals j? For example, should mover #3 attract mover #3?\nThe answer, of course, is no. If there are five objects, we only want mover #3 to attract 0, 1, 2,\nand 4, skipping itself. And so, we finish this example by adding a simple conditional statement\nto skip applying the force when i equals j.\nExample 2.8: Mutual attraction\nThe Mover now knows how to attract\nanother Mover.\nPVector attract(Mover m) {\nPVector force = PVector.sub(location,m.location);\nfloat distance = force.mag();\ndistance = constrain(distance,5.0,25.0);\nforce.normalize();\nfloat strength = (G * mass * m.mass) / (distance * distance);\nforce.mult(strength);\nreturn force;\n}\n}\nMover[] movers = new Mover[20];\nfloat g = 0.4;\nvoid setup() {\nsize(400,400);\nfor (int i = 0; i < movers.length; i++) {\nmovers[i] = new Mover(random(0.1,2),random(width),random(height));\n}\n}\nvoid draw() {\nbackground(255);\nfor (int i = 0; i < movers.length; i++) {\nfor (int j = 0; j < movers.length; j++) {\nThe Nature of Code (v1.0)\n99\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 270
  },
  {
    "chunk_full": "The Ecosystem Project\nThe Ecosystem Project\nStep 2 Exercise:\nIncorporate the concept of forces into your ecosystem. Try introducing other\nelements into the environment (food, a predator) for the creature to interact with.\nDoes the creature experience attraction or repulsion to things in its world? Can\nyou think more abstractly and design forces based on the creature’s desires or\ngoals?\nDon’t attract yourself!\nif (i != j) {\nPVector force = movers[j].attract(movers[i]);\nmovers[i].applyForce(force);\n}\n}\nmovers[i].update();\nmovers[i].display();\n}\n}\nChange the attraction force in Example 2.8 to a repulsion force. Can you create an\nexample in which all of the Mover objects are attracted to the mouse, but repel each\nother? Think about how you need to balance the relative strength of the forces and\nhow to most effectively use distance in your force calculations.\nExercise 2.10\nExercise 2.10\nChapter 2. Forces\n100\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 271
  },
  {
    "chunk_full": "Chapter 3. Oscillation\nChapter 3. Oscillation\n“Trigonometry is a sine of the times.”\n— Anonymous\nIn Chapters 1 and 2, we carefully worked out an object-oriented structure to make something\nmove on the screen, using the concept of a vector to represent location, velocity, and\nacceleration driven by forces in the environment. We could move straight from here into\ntopics such as particle systems, steering forces, group behaviors, etc. If we did that, however,\nwe’d skip an important area of mathematics that we’re going to need: trigonometry\ntrigonometry, or the\nmathematics of triangles, specifically right triangles.\nTrigonometry is going to give us a lot of tools. We’ll get to think about angles and angular\nvelocity and acceleration. Trig will teach us about the sine and cosine functions, which when\nused properly can yield an nice ease-in, ease-out wave pattern. It’s going to allow us to\ncalculate more complex forces in an environment that involves angles, such as a pendulum\nswinging or a box sliding down an incline.\nSo this chapter is a bit of a mishmash. We’ll start with the basics of angles in Processing and\ncover many trigonometric topics, tying it all into forces at the end. And by taking this break\nnow, we’ll also pave the way for more advanced examples that require trig later in this book.\n3.1 Angles\n3.1 Angles\nOK. Before we can do any of this stuff, we need to make sure we understand what it means to\nbe an angle in Processing. If you have experience with Processing, you’ve undoubtedly\nencountered this issue while using the rotate() function to rotate and spin objects.\nThe Nature of Code (v1.0)\n101\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 272
  },
  {
    "chunk_full": "The first order of business is to cover radians\nradians and degrees\ndegrees. You’re probably familiar with\nthe concept of an angle in degrees\ndegrees. A full rotation goes from 0 to 360 degrees. 90 degrees\n(a right angle) is 1/4th of 360, shown below as two perpendicular lines.\nIt’s fairly intuitive for us to think of angles in terms of degrees. For example, the square in\nFigure 3.2 is rotated 45 degrees around its center.\nProcessing, however, requires angles to be specified in radians\nradians. A radian is a unit of\nmeasurement for angles defined by the ratio of the length of the arc of a circle to the radius\nof that circle. One radian is the angle at which that ratio equals one (see Figure 3.1). 180\ndegrees = PI radians, 360 degrees = 2*PI radians, 90 degrees = PI/2 radians, etc.\nFigure 3.1\nFigure 3.3\nChapter 3. Oscillation\n102\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 273
  },
  {
    "chunk_full": "The formula to convert from degrees to radians is:\nradians = 2 * PI * (degrees / 360)\nThankfully, if we prefer to think in degrees but code with radians, Processing makes this easy.\nThe radians() function will automatically convert values from degrees to radians, and the\nconstants PI and TWO_PI provide convenient access to these commonly used numbers\n(equivalent to 180 and 360 degrees, respectively). The following code, for example, will rotate\nshapes by 60 degrees.\nIf you are not familiar with how rotation is implemented in Processing, I would suggest this\ntutorial: Processing - Transform 2D (http://www.processing.org/learning/transform2d/).\nWhat is PI?\nWhat is PI?\nThe mathematical constant pi (or π) is a real number defined as the ratio of a circle’s\ncircumference (the distance around the perimeter) to its diameter (a straight line that\npasses through the circle’s center). It is equal to approximately 3.14159 and can be\naccessed in Processing with the built-in variable PI.\nFigure 3.3\nfloat angle = radians(60);\nrotate(angle);\nThe Nature of Code (v1.0)\n103\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 274
  },
  {
    "chunk_full": "Rotate a baton-like object (see below) around its center using translate() and\nrotate().\nExercise 3.1\nExercise 3.1\n3.2 Angular Motion\n3.2 Angular Motion\nRemember all this stuff?\nlocation = location + velocity\nvelocity = velocity + acceleration\nThe stuff we dedicated almost all of Chapters 1 and 2 to? Well, we can apply exactly the\nsame logic to a rotating object.\nangle = angle + angular velocity\nangular velocity = angular velocity + angular acceleration\nIn fact, the above is actually simpler than what we started with because an angle is a scalar\nquantity—a single number, not a vector!\nUsing the answer from Exercise 3.1 above, let’s say we wanted to rotate a baton in\nProcessing by some angle. We would have code like:\nAdding in our principles of motion brings us to the following example.\ntranslate(width/2,height/2);\nrotate(angle);\nline(-50,0,50,0);\nellipse(50,0,8,8);\nellipse(-50,0,8,8);\nChapter 3. Oscillation\n104\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 275
  },
  {
    "chunk_full": "Example 3.1: Angular motion using rotate()\nThe baton starts onscreen with no rotation and then spins faster and faster as the angle of\nrotation accelerates.\nThis idea can be incorporated into our Mover object. For example, we can add the variables\nrelated to angular motion to our Mover.\nLocation\nfloat angle = 0;\nVelocity\nfloat aVelocity = 0;\nAcceleration\nfloat aAcceleration = 0.001;\nvoid setup() {\nsize(640,360);\n}\nvoid draw() {\nbackground(255);\nfill(175);\nstroke(0);\nrectMode(CENTER);\ntranslate(width/2,height/2);\nrotate(angle);\nline(-50,0,50,0);\nellipse(50,0,8,8);\nellipse(-50,0,8,8);\nAngular equivalent of\nvelocity.add(acceleration);\naVelocity += aAcceleration;\nAngular equivalent of location.add(velocity);\nangle += aVelocity;\n}\nThe Nature of Code (v1.0)\n105\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 276
  },
  {
    "chunk_full": "And then in update(), we update both location and angle according to the same algorithm!\nOf course, for any of this to matter, we also would need to rotate the object when displaying\nit.\nNow, if we were to actually go ahead and run the above code, we wouldn’t see anything\nnew. This is because the angular acceleration (float aAcceleration = 0;) is initialized to\nzero. For the object to rotate, we need to give it an acceleration! Certainly, we could hard-\ncode in a different number.\nclass Mover {\nPVector location;\nPVector velocity;\nPVector acceleration;\nfloat mass;\nfloat angle = 0;\nfloat aVelocity = 0;\nfloat aAcceleration = 0;\nvoid update() {\nRegular old-fashioned motion\nvelocity.add(acceleration);\nlocation.add(velocity);\nNewfangled angular motion\naVelocity += aAcceleration;\nangle += aVelocity;\nacceleration.mult(0);\n}\nvoid display() {\nstroke(0);\nfill(175,200);\nrectMode(CENTER);\npushMatrix() and popMatrix() are\nnecessary so that the rotation of this shape\ndoesn’t affect the rest of our world.\npushMatrix();\nSet the origin at the shape’s location.\ntranslate(location.x,location.y);\nRotate by the angle.\nrotate(angle);\nrect(0,0,mass*16,mass*16);\npopMatrix();\n}\nfloat aAcceleration = 0.01;\nChapter 3. Oscillation\n106\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 277
  },
  {
    "chunk_full": "However, we can produce a more interesting result by dynamically assigning an angular\nacceleration according to forces in the environment. Now, we could head far down this road,\ntrying to model the physics of angular acceleration using the concepts of torque\n(http://en.wikipedia.org/wiki/Torque) and moment of inertia (http://en.wikipedia.org/wiki/\nMoment_of_inertia). Nevertheless, this level of simulation is beyond the scope of this book.\n(We will see more about modeling angular acceleration with a pendulum later in this chapter,\nas well as look at how Box2D realistically models rotational motion in Chapter 5.)\nFor now, a quick and dirty solution will do. We can produce reasonable results by simply\ncalculating angular acceleration as a function of the object’s acceleration vector. Here’s one\nsuch example:\nYes, this is completely arbitrary. But it does do something. If the object is accelerating to the\nright, its angular rotation accelerates in a clockwise direction; acceleration to the left results in\na counterclockwise rotation. Of course, it’s important to think about scale in this case. The x\ncomponent of the acceleration vector might be a quantity that’s too large, causing the object\nto spin in a way that looks ridiculous or unrealistic. So dividing the x component by some\nvalue, or perhaps constraining the angular velocity to a reasonable range, could really help.\nHere’s the entire update() function with these tweaks added.\nExample 3.2: Forces with (arbitrary) angular motion\naAcceleration = acceleration.x;\nvoid update() {\nvelocity.add(acceleration);\nlocation.add(velocity);\nCalculate angular acceleration according to\nacceleration’s horizontal direction and\nmagnitude.\naAcceleration = acceleration.x / 10.0;\naVelocity += aAcceleration;\nThe Nature of Code (v1.0)\n107\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 278
  },
  {
    "chunk_full": "Use constrain() to ensure that angular\nvelocity doesn’t spin out of control.\naVelocity = constrain(aVelocity,-0.1,0.1);\nangle += aVelocity;\nacceleration.mult(0);\n}\nStep 1: Create a simulation where objects are shot out of a cannon. Each object\nshould experience a sudden force when shot (just once) as well as gravity (always\npresent).\nStep 2: Add rotation to the object to model its spin as it is shot from the cannon. How\nrealistic can you make it look?\nExercise 3.2\nExercise 3.2\n3.3 Trigonometry\n3.3 Trigonometry\nI think it may be time. We’ve looked at angles, we’ve spun an object. It’s time for:\nsohcahtoa. Yes, sohcahtoa. This seemingly nonsensical word is actually the foundation for a\nlot of computer graphics work. A basic understanding of trigonometry is essential if you\nwant to calculate an angle, figure out the distance between points, work with circles, arcs,\nor lines. And sohcahtoa is a mnemonic device (albeit a somewhat absurd one) for what the\ntrigonometric functions sine, cosine, and tangent mean.\n•\nsoh\nsoh: sine = opposite / hypotenuse\n•\ncah\ncah: cosine = adjacent / hypotenuse\n•\ntoa\ntoa: tangent = opposite / adjacent\nFigure 3.4\nChapter 3. Oscillation\n108\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 279
  },
  {
    "chunk_full": "Take a look at Figure 3.4 again. There’s no\nneed to memorize it, but make sure you feel\ncomfortable with it. Draw it again yourself.\nNow let’s draw it a slightly different way\n(Figure 3.5).\nSee how we create a right triangle out of a\nvector? The vector arrow itself is the\nhypotenuse and the components of the\nvector (x and y) are the sides of the triangle.\nThe angle is an additional means for\nspecifying the vector’s direction (or\n“heading”).\nBecause the trigonometric functions allow us to establish a relationship between the\ncomponents of a vector and its direction + magnitude, they will prove very useful throughout\nthis book. We’ll begin by looking at an example that requires the tangent function.\nFigure 3.5\n3.4 Pointing in the Direction of Movement\n3.4 Pointing in the Direction of Movement\nLet’s go all the way back to Example 1.10, which features a Mover object accelerating towards\nthe mouse.\nYou might notice that almost all of the shapes we’ve been drawing so far are circles. This is\nconvenient for a number of reasons, one of which is that we don’t have to consider the\nquestion of rotation. Rotate a circle and, well, it looks exactly the same. However, there comes\na time in all motion programmers’ lives when they want to draw something on the screen that\npoints in the direction of movement. Perhaps you are drawing an ant, or a car, or a spaceship.\nAnd when we say \"point in the direction of movement,\" what we are really saying is “rotate\naccording to the velocity vector.” Velocity is a vector, with an x and a y component, but to\nrotate in Processing we need an angle, in radians. Let’s draw our trigonometry diagram one\nmore time, with an object’s velocity vector (Figure 3.6).\nThe Nature of Code (v1.0)\n109\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 280
  },
  {
    "chunk_full": "ifif\ntangent(a) = b\nthen\nthen\na = arctangent(b)\nifif\ntangent(angle) = velocityy / velocityx\nthen\nthen\nangle = arctangent(velocityy / velocityx)\nOK. We know that the definition of tangent\nis:\ntangent(angle) = velocityy\nvelocityx\nThe problem with the above is that we\nknow velocity, but we don’t know the\nangle. We have to solve for the angle. This\nis where a special function known as\ninverse tangent comes in, sometimes\nreferred to as arctangent or tan-1. (There is\nalso an inverse sine and an inverse cosine.)\nIf the tangent of some value a equals some value b, then the inverse tangent of b equals a.\nFor example:\nSee how that is the inverse? The above now allows us to solve for the angle:\nNow that we have the formula, let’s see where it should go in our mover’s display()\nfunction. Notice that in Processing, the function for arctangent is called atan().\nNow the above code is pretty darn close, and almost works. We still have a big problem,\nthough. Let’s consider the two velocity vectors depicted below.\nFigure 3.6\nvoid display() {\nSolve for angle by using atan().\nfloat angle = atan(velocity.y/velocity.x);\nstroke(0);\nfill(175);\npushMatrix();\nrectMode(CENTER);\ntranslate(location.x,location.y);\nRotate according to that angle.\nrotate(angle);\nrect(0,0,30,10);\npopMatrix();\n}\nChapter 3. Oscillation\n110\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 281
  },
  {
    "chunk_full": "Though superficially similar, the two vectors point in quite different directions—opposite\ndirections, in fact! However, if we were to apply our formula to solve for the angle to each\nvector…\nV1 ⇒ angle = atan(-4/3) = atan(-1.25) = -0.9272952 radians = -53 degrees\nV2 ⇒ angle = atan(4/-3) = atan(-1.25) = -0.9272952 radians = -53 degrees\n…we get the same angle for each vector. This can’t be right for both; the vectors point in\nopposite directions! The thing is, this is a pretty common problem in computer graphics.\nRather than simply using atan() along with a bunch of conditional statements to account for\npositive/negative scenarios, Processing (along with pretty much all programming\nenvironments) has a nice function called atan2() that does it for you.\nExample 3.3: Pointing in the direction of motion\nFigure 3.7\nvoid display() {\nUsing atan2() to account for all possible\ndirections\nfloat angle = atan2(velocity.y,velocity.x);\nstroke(0);\nfill(175);\npushMatrix();\nrectMode(CENTER);\ntranslate(location.x,location.y);\nThe Nature of Code (v1.0)\n111\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 282
  },
  {
    "chunk_full": "To simplify this even further, the PVector class itself provides a function called heading(),\nwhich takes care of calling atan2() for you so you can get the 2D direction angle, in\nradians, for any Processing PVector.\nRotate according to that angle.\nrotate(angle);\nrect(0,0,30,10);\npopMatrix();\n}\nThe easiest way to do this!\nfloat angle = velocity.heading();\nCreate a simulation of a vehicle that you can drive around the screen using the arrow\nkeys: left arrow accelerates the car to the left, right to the right. The car should point\nin the direction in which it is currently moving.\nExercise 3.3\nExercise 3.3\n3.5 Polar vs. Cartesian Coordinates\n3.5 Polar vs. Cartesian Coordinates\nAny time we display a shape in Processing, we have to specify a pixel location, a set of x\nand y coordinates. These coordinates are known as Cartesian coordinates\nCartesian coordinates, named for\nRené Descartes, the French mathematician who developed the ideas behind Cartesian\nspace.\nAnother useful coordinate system known as polar coordinates\npolar coordinates describes a point in space\nas an angle of rotation around the origin and a radius from the origin. Thinking about this in\nterms of a vector:\nCartesian coordinate—the x,y components of a vector\nPolar coordinate—the magnitude (length) and direction (angle) of a vector\nProcessing’s drawing functions, however, don’t understand polar coordinates. Whenever we\nwant to display something in Processing, we have to specify locations as (x,y) Cartesian\ncoordinates. However, sometimes it is a great deal more convenient for us to think in polar\ncoordinates when designing. Happily for us, with trigonometry we can convert back and\nforth between polar and Cartesian, which allows us to design with whatever coordinate\nsystem we have in mind but always draw with Cartesian coordinates.\nChapter 3. Oscillation\n112\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 283
  },
  {
    "chunk_full": "sine(theta)\n= y/r\n→\ny = r * sine(theta)\ncosine(theta) = x/r\n→\nx = r * cosine(theta)\nFor example, if r is 75 and theta is 45 degrees (or PI/4 radians), we can calculate x and y as\nbelow. The functions for sine and cosine in Processing are sin() and cos(), respectively.\nThey each take one argument, an angle measured in radians.\nThis type of conversion can be useful in certain applications. For example, to move a shape\nalong a circular path using Cartesian coordinates is not so easy. With polar coordinates, on\nthe other hand, it’s simple: increment the angle!\nHere’s how it is done with global variables r and theta.\nFigure 3.8: The Greek letter θ (theta) is often used to denote an angle. Since a polar coordinate is\nconventionally referred to as (r, θ), we’ll use theta as a variable name when referring to an angle.\nfloat r = 75;\nfloat theta = PI / 4;\nConverting from polar (r,theta) to Cartesian\n(x,y)\nfloat x = r * cos(theta);\nfloat y = r * sin(theta);\nThe Nature of Code (v1.0)\n113\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 284
  },
  {
    "chunk_full": "Example 3.4: Polar to Cartesian\nfloat r = 75;\nfloat theta = 0;\nvoid setup() {\nsize(640,360);\nbackground(255);\n}\nvoid draw() {\nPolar coordinates (r,theta) are converted\nto Cartesian (x,y) for use in the ellipse()\nfunction.\nfloat x = r * cos(theta);\nfloat y = r * sin(theta);\nnoStroke();\nfill(0);\nellipse(x+width/2, y+height/2, 16, 16);\ntheta += 0.01;\n}\nChapter 3. Oscillation\n114\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 285
  },
  {
    "chunk_full": "Using Example 3.4 as a basis, draw a spiral path. Start in the center and move outwards.\nNote that this can be done by only changing one line of code and adding one line of\ncode!\nExercise 3.4\nExercise 3.4\nSimulate the spaceship in the game Asteroids. In case you aren’t familiar with Asteroids,\nhere is a brief description: A spaceship (represented as a triangle) floats in two\ndimensional space. The left arrow key turns the spaceship counterclockwise, the right\narrow key, clockwise. The z key applies a “thrust” force in the direction the spaceship is\npointing.\nExercise 3.5\nExercise 3.5\nThe Nature of Code (v1.0)\n115\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 286
  },
  {
    "chunk_full": "3.6 Oscillation Amplitude and Period\n3.6 Oscillation Amplitude and Period\nAre you amazed yet? We’ve seen some pretty great uses of tangent (for finding the angle of\na vector) and sine and cosine (for converting from polar to Cartesian coordinates). We could\nstop right here and be satisfied. But we’re not going to. This is only the beginning. What\nsine and cosine can do for you goes beyond mathematical formulas and right triangles.\nLet’s take a look at a graph of the sine function, where y = sine(x).\nYou’ll notice that the output of the sine function is a smooth curve alternating between –1\nand 1. This type of a behavior is known as oscillation\noscillation, a periodic movement between two\npoints. Plucking a guitar string, swinging a pendulum, bouncing on a pogo stick—these are\nall examples of oscillating motion.\nAnd so we happily discover that we can simulate oscillation in a Processing sketch by\nassigning the output of the sine function to an object’s location. Note that this will follow the\nsame methodology we applied to Perlin noise in the Introduction (see page 17).\nLet’s begin with a really basic scenario. We want a circle to oscillate from the left side to the\nright side of a Processing window.\nFigure 3.9: y = sine(x)\nChapter 3. Oscillation\n116\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 287
  },
  {
    "chunk_full": "This is what is known as simple harmonic motion\nsimple harmonic motion (or, to be fancier, “the periodic sinusoidal\noscillation of an object”). It’s going to be a simple program to write, but before we get into the\ncode, let’s familiarize ourselves with some of the terminology of oscillation (and waves).\nSimple harmonic motion can be expressed as any location (in our case, the x location) as a\nfunction of time, with the following two elements:\n•\nAmplitude\nAmplitude: The distance from the center of motion to either extreme\n•\nPeriod\nPeriod: The amount of time it takes for one complete cycle of motion\nLooking at the graph of sine (Figure 3.9), we can see that the amplitude is 1 and the period is\nTWO_PI; the output of sine never rises above 1 or below -1; and every TWO_PI radians (or 360\ndegrees) the wave pattern repeats.\nNow, in the Processing world we live in, what is amplitude and what is period? Amplitude can\nbe measured rather easily in pixels. In the case of a window 200 pixels wide, we would\noscillate from the center 100 pixels to the right and 100 pixels to the left. Therefore:\nPeriod is the amount of time it takes for one cycle, but what is time in our Processing world? I\nmean, certainly we could say we want the circle to oscillate every three seconds. And we\ncould track the milliseconds—using millis() —in Processing and come up with an elaborate\nalgorithm for oscillating an object according to real-world time. But for us, real-world time\ndoesn’t really matter. The real measure of time in Processing is in frames. The oscillating\nmotion should repeat every 30 frames, or 50 frames, or 1000 frames, etc.\nOnce we have the amplitude and period, it’s time to write a formula to calculate x as a\nfunction of time, which we now know is the current frame count.\nOur amplitude is measured in pixels.\nfloat amplitude = 100;\nOur period is measured in frames (our unit\nof time for animation).\nfloat period = 120;\nfloat x = amplitude * cos(TWO_PI * frameCount / period);\nThe Nature of Code (v1.0)\n117\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 288
  },
  {
    "chunk_full": "Let’s dissect the formula a bit more and try to understand each component. The first is\nprobably the easiest. Whatever comes out of the cosine function we multiply by amplitude.\nWe know that cosine will oscillate between -1 and 1. If we take that value and multiply it by\namplitude then we’ll get the desired result: a value oscillating between -amplitude and\namplitude. (Note: this is also a place where we could use Processing’s map() function to\nmap the output of cosine to a custom range.)\nNow, let’s look at what is inside the cosine function:\nTWO_PI * frameCount / period\nWhat’s going on here? Let’s start with what we know. We know that cosine will repeat every\n2*PI radians—i.e. it will start at 0 and repeat at 2*PI, 4*PI, 6*PI, etc. If the period is 120, then\nwe want the oscillating motion to repeat when the frameCount is at 120 frames, 240 frames,\n360 frames, etc. frameCount is really the only variable; it starts at 0 and counts upward.\nLet’s take a look at what the formula yields with those values.\nframeCount\nframeCount\nframeCount / period\nframeCount / period\nTWO_PI * frameCount /\nTWO_PI * frameCount /\nperiod\nperiod\n0\n0\n0\n60\n0.5\nPI\n120\n1\nTWO_PI\n240\n2\n2 * TWO_PI (or 4* PI)\netc.\nframeCount divided by period tells us how many cycles we’ve completed—are we halfway\nthrough the first cycle? Have we completed two cycles? By multiplying that number by\nTWO_PI, we get the result we want, since TWO_PI is the number of radians required for one\ncosine (or sine) to complete one cycle.\nWrapping this all up, here’s the Processing example that oscillates the x location of a circle\nwith an amplitude of 100 pixels and a period of 120 frames.\nChapter 3. Oscillation\n118\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 289
  },
  {
    "chunk_full": "Example 3.5: Simple Harmonic Motion\nIt’s also worth mentioning the term frequency\nfrequency: the number of cycles per time unit. Frequency\nis equal to 1 divided by period. If the period is 120 frames, then only 1/120th of a cycle is\ncompleted in one frame, and so frequency = 1/120. In the above example, we simply chose to\ndefine the rate of oscillation in terms of period and therefore did not need a variable for\nfrequency.\nvoid setup() {\nsize(640,360);\n}\nvoid draw() {\nbackground(255);\nfloat period = 120;\nfloat amplitude = 100;\nfloat x = amplitude * cos(TWO_PI * frameCount / period);\nstroke(0);\nfill(175);\ntranslate(width/2,height/2);\nline(0,0,x,0);\nellipse(x,0,20,20);\n}\nCalculating horizontal location according to\nthe formula for simple harmonic motion\nUsing the sine function, create a simulation of a weight (sometimes referred to as a\n“bob”) that hangs from a spring from the top of the window. Use the map() function to\ncalculate the vertical location of the bob. Later in this chapter, we’ll see how to recreate\nthis same simulation by modeling the forces of a spring according to Hooke’s law.\nExercise 3.6\nExercise 3.6\n3.7 Oscillation with Angular Velocity\n3.7 Oscillation with Angular Velocity\nAn understanding of the concepts of oscillation, amplitude, and frequency/period is often\nrequired in the course of simulating real-world behaviors. However, there is a slightly easier\nway to rewrite the above example with the same result. Let’s take one more look at our\noscillation formula:\nAnd let’s rewrite it a slightly different way:\nfloat x = amplitude * cos(TWO_PI * frameCount / period);\nThe Nature of Code (v1.0)\n119\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 290
  },
  {
    "chunk_full": "If we care about precisely defining the period of oscillation in terms of frames of animation,\nwe might need the formula the way we first wrote it, but we can just as easily rewrite our\nexample using the concept of angular velocity (and acceleration) from section 3.2 (see page\n104). Assuming:\nin draw(), we can simply say:\nangle is our “some value that increments slowly.”\nExample 3.6: Simple Harmonic Motion II\nJust because we’re not referencing it directly doesn’t mean that we’ve eliminated the\nconcept of period. After all, the greater the angular velocity, the faster the circle will\noscillate (therefore lowering the period). In fact, the number of times it takes to add up the\nangular velocity to get to TWO_PI is the period or:\nperiod = TWO_PI / angular velocity\nfloat x = amplitude * cos ( some value that increments slowly );\nfloat angle = 0;\nfloat aVelocity = 0.05;\nangle += aVelocity;\nfloat x = amplitude * cos(angle);\nfloat angle = 0;\nfloat aVelocity = 0.05;\nvoid setup() {\nsize(640,360);\n}\nvoid draw() {\nbackground(255);\nfloat amplitude = 100;\nfloat x = amplitude * cos(angle);\nUsing the concept of angular velocity to\nincrement an angle variable\nangle += aVelocity;\nellipseMode(CENTER);\nstroke(0);\nfill(175);\ntranslate(width/2,height/2);\nline(0,0,x,0);\nellipse(x,0,20,20);\n}\nChapter 3. Oscillation\n120\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 291
  },
  {
    "chunk_full": "Let’s expand this example a bit more and create an Oscillator class. And let’s assume we\nwant the oscillation to happen along both the x-axis (as above) and the y-axis. To do this, we’ll\nneed two angles, two angular velocities, and two amplitudes (one for each axis). Another\nperfect opportunity for PVector!\nExample 3.7: Oscillator objects\nclass Oscillator\n{\nUsing a PVector to track two angles!\nPVector angle;\nPVector velocity;\nPVector amplitude;\nOscillator()\n{\nangle = new PVector();\nvelocity = new PVector(random(-0.05,0.05),random(-0.05,0.05));\namplitude = new PVector(random(width/2),random(height/2));\n}\nvoid oscillate()\n{\nangle.add(velocity);\n}\nvoid display()\n{\nRandom velocities and amplitudes\nOscillating on the x-axis\nfloat x = sin(angle.x)*amplitude.x;\nOscillating on the y-axis\nfloat y = sin(angle.y)*amplitude.y;\npushMatrix();\ntranslate(width/2,height/2);\nstroke(0);\nfill(175);\nThe Nature of Code (v1.0)\n121\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 292
  },
  {
    "chunk_full": "Drawing the Oscillator as a line connecting\na circle\nline(0,0,x,y);\nellipse(x,y,16,16);\npopMatrix();\n}\n}\nTry initializing each Oscillator object with velocities and amplitudes that are not\nrandom to create some sort of regular pattern. Can you make the oscillators appear\nto be the legs of a insect-like creature?\nExercise 3.7\nExercise 3.7\nIncorporate angular acceleration into the Oscillator object.\nExercise 3.8\nExercise 3.8\n3.8 Waves\n3.8 Waves\nIf you’re saying to yourself, “Um, this is all great and everything, but what I really want is to\ndraw a wave onscreen,” well, then, the time has come. The thing is, we’re about 90% there.\nWhen we oscillate a single circle up and down according to the sine function, what we are\ndoing is looking at a single point along the x-axis of a wave pattern. With a little panache\nand a for loop, we can place a whole bunch of these oscillating circles next to each other.\nThis wavy pattern could be used in the design of the body or appendages of a creature, as\nwell as to simulate a soft surface (such as water).\nHere, we’re going to encounter the same questions of amplitude (height of pattern) and\nperiod. Instead of period referring to time, however, since we’re looking at the full wave, we\nChapter 3. Oscillation\n122\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 293
  },
  {
    "chunk_full": "can talk about period as the width (in pixels) of a full wave cycle. And just as with simple\noscillation, we have the option of computing the wave pattern according to a precise period or\nsimply following the model of angular velocity.\nLet’s go with the simpler case, angular velocity. We know we need to start with an angle, an\nangular velocity, and an amplitude:\nThen we’re going to loop through all of the x values where we want to draw a point of the\nwave. Let’s say every 24 pixels for now. In that loop, we’re going to want to do three things:\n1.\nCalculate the y location according to amplitude and sine of the angle.\n2.\nDraw a circle at the (x,y) location.\n3.\nIncrement the angle according to angular velocity.\nLet’s look at the results with different values for angleVel:\nfloat angle = 0;\nfloat angleVel = 0.2;\nfloat amplitude = 100;\nfor (int x = 0; x <= width; x += 24) {\n1) Calculate the y location according to\namplitude and sine of the angle.\nfloat y = amplitude*sin(angle);\n2) Draw a circle at the (x,y) location.\nellipse(x,y+height/2,48,48);\n3) Increment the angle according to angular\nvelocity.\nangle += angleVel;\n}\nangleVel = 0.05\nangleVel = 0.2\nangleVel = 0.4\nThe Nature of Code (v1.0)\n123\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 294
  },
  {
    "chunk_full": "Notice how, although we’re not precisely computing the period of the wave, the higher the\nangular velocity, the shorter the period. It’s also worth noting that as the period becomes\nshorter, it becomes more and more difficult to make out the wave itself as the distance\nbetween the individual points increases. One option we have is to use beginShape() and\nendShape() to connect the points with a line.\nExample 3.8: Static wave drawn as a continuous line\nYou may have noticed that the above example is static. The wave never changes, never\nundulates. This additional step is a bit tricky. Your first instinct might be to say: “Hey, no\nproblem, we’ll just let theta be a global variable and let it increment from one cycle through\ndraw() to another.”\nWhile it’s a nice thought, it doesn’t work. If you look at the wave, the righthand edge doesn’t\nmatch the lefthand; where it ends in one cycle of draw() can’t be where it starts in the next.\nInstead, what we need to do is have a variable dedicated entirely to tracking what value of\nfloat angle = 0;\nfloat angleVel = 0.2;\nfloat amplitude = 100;\nsize(400,200);\nbackground(255);\nstroke(0);\nstrokeWeight(2);\nnoFill();\nbeginShape();\nfor (int x = 0; x <= width; x += 5) {\nHere’s an example of using the map()\nfunction instead.\nfloat y = map(sin(angle),-1,1,0,height);\nWith beginShape() and endShape(), you\ncall vertex() to set all the vertices of your\nshape.\nvertex(x,y);\nangle +=angleVel;\n}\nendShape();\nChapter 3. Oscillation\n124\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 295
  },
  {
    "chunk_full": "angle the wave should start with. This angle (which we’ll call startAngle) increments with its\nown angular velocity.\nExample 3.9: The Wave\nfloat startAngle = 0;\nfloat angleVel = 0.1;\nvoid setup() {\nsize(400,200);\n}\nvoid draw() {\nbackground(255);\nIn order to move the wave, we start at a\ndifferent theta value each frame. startAngle\n+= 0.02;\nfloat angle = startAngle;\nfor (int x = 0; x <= width; x += 24) {\nfloat y = map(sin(angle),-1,1,0,height);\nstroke(0);\nfill(0,50);\nellipse(x,y,48,48);\nangle += angleVel;\n}\n}\nThe Nature of Code (v1.0)\n125\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 296
  },
  {
    "chunk_full": "Try using the Perlin noise function instead of sine or cosine with the above example.\nExercise 3.9\nExercise 3.9\nEncapsulate the above examples into a Wave class and create a sketch that displays\ntwo waves (with different amplitudes/periods) as in the screenshot below. Move\nbeyond plain circles and lines and try visualizing the wave in a more creative way.\nExercise 3.10\nExercise 3.10\nMore complex waves can be produced by the values of multiple waves together.\nCreate a sketch that implements this, as in the screenshot below.\nExercise 3.11\nExercise 3.11\nChapter 3. Oscillation\n126\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 297
  },
  {
    "chunk_full": "3.9 Trigonometry and Forces: The Pendulum\n3.9 Trigonometry and Forces: The Pendulum\nDo you miss Newton’s laws of motion? I know I sure do. Well, lucky for you, it’s time to bring it\nall back home. After all, it’s been nice learning about triangles and tangents and waves, but\nreally, the core of this book is about simulating the physics of moving bodies. Let’s take a look\nat how trigonometry can help us with this pursuit.\nA pendulum is a bob suspended from a pivot. Obviously a real-world pendulum would live in a\n3D space, but we’re going to look at a simpler scenario, a pendulum in a 2D space—a\nProcessing window (see Figure 3.10).\nIn Chapter 2, we learned how a force (such as the force of gravity in Figure 3.11) causes an\nobject to accelerate. F = M * A or A = F / M. In this case, however, the pendulum bob\ndoesn’t simply fall to the ground because it is attached by an arm to the pivot point. And so, in\norder to determine its angular acceleration, we not only need to look at the force of gravity,\nbut also the force at the angle of the pendulum’s arm (relative to a pendulum at rest with an\nangle of 0).\nIn the above case, since the pendulum’s arm is of fixed length, the only variable in the\nscenario is the angle. We are going to simulate the pendulum’s motion through the use of\nangular velocity and acceleration. The angular acceleration will be calculated using Newton’s\nsecond law with a little trigonometry twist.\nLet’s zoom in on the right triangle from the pendulum diagram.\nFigure 3.10\nFigure 3.11\nThe Nature of Code (v1.0)\n127\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 298
  },
  {
    "chunk_full": "We can see that the force of the pendulum\n(Fp) should point perpendicular to the arm\nof the pendulum in the direction that the\npendulum is swinging. After all, if there\nwere no arm, the bob would just fall\nstraight down. It’s the tension force of the\narm that keeps the bob accelerating\ntowards the pendulum’s rest state. Since\nthe force of gravity (Fp) points downward,\nby making a right triangle out of these two\nvectors, we’ve accomplished something\nquite magnificent. We’ve made the force of\ngravity the hypotenuse of a right triangle\nand separated the vector into two\ncomponents, one of which represents the\nforce of the pendulum. Since sine equals\nopposite over hypotenuse, we have:\nsine(θ) = Fp / Fg\nTherefore:\nFp = Fg * sine(θ)\nLest we forget, we’ve been doing all of this with a single question in mind: What is the\nangular acceleration of the pendulum? Once we have the angular acceleration, we’ll be able\nto apply our rules of motion to find the new angle for the pendulum.\nangular velocity = angular velocity + angular acceleration\nangle = angle + angular velocity\nThe good news is that with Newton’s second law, we know that there is a relationship\nbetween force and acceleration, namely F = M * A, or A = F / M. So if the force of the\npendulum is equal to the force of gravity times sine of the angle, then:\npendulum angular acceleration = acceleration due to gravity * sine (θ)\nThis is a good time to remind ourselves that we’re Processing programmers and not\nphysicists. Yes, we know that the acceleration due to gravity on earth is 9.8 meters per\nsecond squared. But this number isn’t relevant to us. What we have here is just an arbitrary\nconstant (we’ll call it gravity), one that we can use to scale the acceleration to something\nthat feels right.\nangular acceleration = gravity * sine(θ)\nAmazing. After all that, the formula is so simple. You might be wondering, why bother going\nthrough the derivation at all? I mean, learning is great and all, but we could have easily just\nFigure 3.12\nChapter 3. Oscillation\n128\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 299
  },
  {
    "chunk_full": "said, \"Hey, the angular acceleration of a pendulum is some constant times the sine of the\nangle.\" This is just another moment in which we remind ourselves that the purpose of the\nbook is not to learn how pendulums swing or gravity works. The point is to think creatively\nabout how things can move about the screen in a computationally based graphics system. The\npendulum is just a case study. If you can understand the approach to programming a\npendulum, then however you choose to design your onscreen world, you can apply the same\ntechniques.\nOf course, we’re not finished yet. We may be happy with our simple, elegant formula, but we\nstill have to apply it in code. This is most definitely a good time to practice our object-oriented\nprogramming skills and create a Pendulum class. Let’s think about all the properties we’ve\nencountered in our pendulum discussion that the class will need:\n•\narm length\n•\nangle\n•\nangular velocity\n•\nangular acceleration\nWe’ll also need to write a function update() to update the pendulum’s angle according to our\nformula…\nclass Pendulum\n{\nLength of arm\nfloat r;\nPendulum arm angle\nfloat angle;\nAngular velocity\nfloat aVelocity;\nAngular acceleration\nfloat aAcceleration;\nvoid update() {\nArbitrary constant\nfloat gravity = 0.4;\nCalculate acceleration according to our\nformula.\naAcceleration = -1 * gravity * sin(angle);\nIncrement velocity.\naVelocity += aAcceleration;\nIncrement angle.\nangle += aVelocity;\n}\nThe Nature of Code (v1.0)\n129\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 300
  },
  {
    "chunk_full": "…as well as a function display() to draw\nthe pendulum in the window. This begs the\nquestion: “Um, where do we draw the\npendulum?” We know the angle and the\narm length, but how do we know the x,y\n(Cartesian!) coordinates for both the\npendulum’s pivot point (let’s call it origin)\nand bob location (let’s call it location)? This\nmay be getting a little tiring, but the\nanswer, yet again, is trigonometry.\nThe origin is just something we make up,\nas is the arm length. Let’s say:\nWe’ve got the current angle stored in our variable angle. So relative to the origin, the\npendulum’s location is a polar coordinate: (r,angle). And we need it to be Cartesian. Luckily\nfor us, we just spent some time (section 3.5) deriving the formula for converting from polar\nto Cartesian. And so:\nSince the location is relative to wherever the origin happens to be, we can just add origin to\nthe location PVector:\nAnd all that remains is the little matter of drawing a line and ellipse (you should be more\ncreative, of course).\nBefore we put everything together, there’s one last little detail I neglected to mention. Let’s\nthink about the pendulum arm for a moment. Is it a metal rod? A string? A rubber band? How\nis it attached to the pivot point? How long is it? What is its mass? Is it a windy day? There\nare a lot of questions that we could continue to ask that would affect the simulation. We’re\nFigure 3.13\nPVector origin = new PVector(100,10);\nfloat r = 125;\nPVector location = new PVector(r*sin(angle),r*cos(angle));\nlocation.add(origin);\nstroke(0);\nfill(175);\nline(origin.x,origin.y,location.x,location.y);\nellipse(location.x,location.y,16,16);\nChapter 3. Oscillation\n130\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 301
  },
  {
    "chunk_full": "living, of course, in a fantasy world, one where the pendulum’s arm is some idealized rod that\nnever bends and the mass of the bob is concentrated in a single, infinitesimally small point.\nNevertheless, even though we don’t want to worry ourselves with all of the questions, we\nshould add one more variable to our calculation of angular acceleration. To keep things\nsimple, in our derivation of the pendulum’s acceleration, we assumed that the length of the\npendulum’s arm is 1. In fact, the length of the pendulum’s arm affects the acceleration greatly:\nthe longer the arm, the slower the acceleration. To simulate a pendulum more accurately, we\ndivide by that length, in this case r. For a more involved explanation, visit The Simple\nPendulum website (http://calculuslab.deltacollege.edu/ODE/7-A-2/7-A-2-h.html).\nFinally, a real-world pendulum is going to experience some amount of friction (at the pivot\npoint) and air resistance. With our code as is, the pendulum would swing forever, so to make it\nmore realistic we can use a “damping” trick. I say trick because rather than model the\nresistance forces with some degree of accuracy (as we did in Chapter 2), we can achieve a\nsimilar result by simply reducing the angular velocity during each cycle. The following code\nreduces the velocity by 1% (or multiplies it by 99%) during each frame of animation:\nPutting everything together, we have the following example (with the pendulum beginning at a\n45-degree angle).\nExample 3.10: Swinging pendulum\naAcceleration = (-1 * G * sin(angle)) / r;\naVelocity *= 0.99;\nPendulum p;\nvoid setup() {\nsize(640,360);\nThe Nature of Code (v1.0)\n131\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 302
  },
  {
    "chunk_full": "We make a new Pendulum object with an\norigin location and arm length.\np = new Pendulum(new PVector(width/2,10),125);\n}\nvoid draw() {\nbackground(255);\np.go();\n}\nclass Pendulum\n{\nMany, many variables to keep track of the\nPendulum’s various properties\nPVector location;\n// Location of bob\nPVector origin;\n// Location of arm origin\nfloat r;\n// Length of arm\nfloat angle;\n// Pendulum arm angle\nfloat aVelocity;\n// Angle velocity\nfloat aAcceleration; // Angle acceleration\nfloat damping;\n// Arbitrary damping amount\nPendulum(PVector origin_, float r_) {\norigin = origin_.get();\nlocation = new PVector();\nr = r_;\nangle = PI/4;\naVelocity = 0.0;\naAcceleration = 0.0;\nAn arbitrary damping so that the Pendulum\nslows over time\ndamping = 0.995;\n}\nvoid go() {\nupdate();\ndisplay();\n}\nvoid update() {\nfloat gravity = 0.4;\naAcceleration = (-1 * gravity / r) * sin(angle);\nFormula we worked out for angular\nacceleration\nStandard angular motion algorithm\naVelocity += aAcceleration;\nangle += aVelocity;\nApply some damping.\naVelocity *= damping;\n}\nvoid display() {\nWhere is the bob relative to the origin?\nPolar to Cartesian coordinates will tell us!\nlocation.set(r*sin(angle),r*cos(angle),0);\nlocation.add(origin);\nstroke(0);\nChapter 3. Oscillation\n132\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 303
  },
  {
    "chunk_full": "(Note that the version of the example posted on the website has additional code to allow the\nuser to grab the pendulum and swing it with the mouse.)\nThe arm\nline(origin.x,origin.y,location.x,location.y);\nfill(175);\nThe bob\nellipse(location.x,location.y,16,16);\n}\n}\nString together a series of pendulums so that the endpoint of one is the origin point of\nanother. Note that doing this may produce intriguing results but will be wildly inaccurate\nphysically. Simulating an actual double pendulum involves sophisticated equations,\nwhich you can read about here: http://scienceworld.wolfram.com/physics/\nDoublePendulum.html (http://scienceworld.wolfram.com/physics/DoublePendulum.html).\nExercise 3.12\nExercise 3.12\nUsing trigonometry, what is the\nmagnitude of the normal force in the\nillustration on the right (the force\nperpendicular to the incline on which the\nsled rests)? Note that, as indicated, the\n“normal” force is a component of the\nforce of gravity.\nExercise 3.13\nExercise 3.13\nCreate an example that simulates a box sliding down the incline with friction. Note that\nthe magnitude of the friction force is equal to the normal force.\nExercise 3.14\nExercise 3.14\nThe Nature of Code (v1.0)\n133\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 304
  },
  {
    "chunk_full": "3.10 Spring Forces\n3.10 Spring Forces\nIn section 3.6 (see page 115), we looked at modeling simple harmonic motion by mapping\nthe sine wave to a pixel range. Exercise 3.6 (see page 119) asked you to use this technique\nto create a simulation of a bob hanging from a spring. While using the sin() function is a\nquick-and-dirty, one-line-of-code way of getting something up and running, it won’t do if\nwhat we really want is to have a bob hanging from a spring in a two-dimensional space that\nresponds to other forces in the environment (wind, gravity, etc.) To accomplish a simulation\nlike this (one that is identical to the pendulum example, only now the arm is a springy\nconnection), we need to model the forces of a spring using PVector.\nThe force of a spring is calculated according to Hooke’s law, named for Robert Hooke, a\nBritish physicist who developed the formula in 1660. Hooke originally stated the law in Latin:\n\"Ut tensio, sic vis,\" or “As the extension, so the force.” Let’s think of it this way:\nThe force of the spring is directly proportional to the extension of the\nspring.\nFigure 3.14\nChapter 3. Oscillation\n134\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 305
  },
  {
    "chunk_full": "In other words, if you pull on the bob a lot,\nthe force will be strong; if you pull on the\nbob a little, the force will be weak.\nMathematically, the law is stated as follows:\nFspring = - k * x\n•\nk is constant and its value will\nultimately scale the force. Is the\nspring highly elastic or quite rigid?\n•\nx refers to the displacement of the\nspring, i.e. the difference between\nthe current length and the rest\nlength. The rest length is defined\nas the length of the spring in a\nstate of equilibrium.\nNow remember, force is a vector, so we\nneed to calculate both magnitude and\ndirection. Let’s look at one more diagram of\nthe spring and label all the givens we might have in a Processing sketch.\nLet’s establish the following three variables as shown in Figure 3.16.\nFigure 3.15: x = current length - rest length\nFigure 3.16\nPVector anchor;\nPVector location;\nfloat restLength;\nThe Nature of Code (v1.0)\n135\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 306
  },
  {
    "chunk_full": "First, let’s use Hooke’s law to calculate the magnitude of the force. We need to know k and\nx. k is easy; it’s just a constant, so let’s make something up.\nx is perhaps a bit more difficult. We need to know the “difference between the current\nlength and the rest length.” The rest length is defined as the variable restLength. What’s\nthe current length? The distance between the anchor and the bob. And how can we\ncalculate that distance? How about the magnitude of a vector that points from the anchor to\nthe bob? (Note that this is exactly the same process we employed when calculating distance\nin Example 2.9: gravitational attraction.)\nNow that we’ve sorted out the elements necessary for the magnitude of the force (-1 * k * x),\nwe need to figure out the direction, a unit vector pointing in the direction of the force. The\ngood news is that we already have this vector. Right? Just a moment ago we thought to\nourselves: “How we can calculate that distance? How about the magnitude of a vector that\npoints from the anchor to the bob?” Well, that is the direction of the force!\nfloat k = 0.1;\nA vector pointing from anchor to bob gives\nus the current length of the spring.\nPVector dir = PVector.sub(bob,anchor);\nfloat currentLength = dir.mag();\nfloat x = restLength - currentLength;\nFigure 3.17\nChapter 3. Oscillation\n136\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 307
  },
  {
    "chunk_full": "In Figure 3.17, we can see that if we stretch the spring beyond its rest length, there should be\na force pulling it back towards the anchor. And if it shrinks below its rest length, the force\nshould push it away from the anchor. This reversal of direction is accounted for in the formula\nwith the -1. And so all we need to do is normalize the PVector we used for the distance\ncalculation! Let’s take a look at the code and rename that PVector variable as “force.”\nNow that we have the algorithm worked out for computing the spring force vector, the\nquestion remains: what object-oriented programming structure should we use? This, again, is\none of those situations in which there is no “correct” answer. There are several possibilities;\nwhich one we choose depends on the program’s goals and one’s own personal coding style.\nStill, since we’ve been working all along with a Mover class, let’s keep going with this same\nframework. Let’s think of our Mover class as the spring’s “bob.” The bob needs location,\nvelocity, and acceleration vectors to move about the screen. Perfect—we’ve got that\nalready! And perhaps the bob experiences a gravity force via the applyForce() function. Just\none more step—we need to apply the spring force:\nMagnitude of spring force according to\nHooke’s law\nfloat k = 0.1;\nPVector force = PVector.sub(bob,anchor);\nfloat currentLength = dir.mag();\nfloat x = restLength - currentLength;\nDirection of spring force (unit vector)\nforce.normalize();\nPutting it together: direction and magnitude!\nforce.mult(-1 * k * x);\nBob bob;\nvoid setup() {\nbob = new Bob();\n}\nvoid draw()\n{\nOur Chapter 2 “make-up-a-gravity force”\nPVector gravity = new PVector(0,1);\nbob.applyForce(gravity);\nWe need to also calculate and apply a\nspring force!\nPVector springForce = _______________????\nbob.applyForce(spring);\nOur standard update() and display()\nfunctions\nbob.update();\nbob.display();\n}\nThe Nature of Code (v1.0)\n137\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 308
  },
  {
    "chunk_full": "One option would be to write out all of the spring force code in the main draw() loop. But\nthinking ahead to when you might have multiple bobs and multiple spring connections, it\nmakes a good deal of sense to write an additional class, a Spring class. As shown in Figure\n3.18, the Bob class keeps track of the movements of the bob; the Spring class keeps track\nof the spring’s anchor and its rest length and calculates the spring force on the bob.\nThis allows us to write a lovely main program as follows:\nFigure 3.18\nBob bob;\nAdding a Spring object\nSpring spring;\nvoid setup() {\nbob = new Bob();\nspring = new Spring();\n}\nvoid draw()\n{\nPVector gravity = new PVector(0,1);\nbob.applyForce(gravity);\nThis new function in the Spring class will\ntake care of computing the force of the\nspring on the bob.\nspring.connect(bob);\nbob.update();\nbob.display();\nspring.display();\n}\nChapter 3. Oscillation\n138\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 309
  },
  {
    "chunk_full": "You may notice here that this is quite similar to what we did in Example 2.6 (see page 94) with\nan attractor. There, we said something like:\nThe analogous situation here with a spring would be:\nNevertheless, in this example all we said was:\nWhat gives? Why don’t we need to call applyForce() on the bob? The answer is, of course,\nthat we do need to call applyForce() on the bob. Only instead of doing it in draw(), we’re\njust demonstrating that a perfectly reasonable (and sometimes preferable) alternative is to ask\nthe connect() function to internally handle calling applyForce() on the bob.\nWhy do it one way with the Attractor class and another way with the Spring class? When we\nwere first learning about forces, it was a bit clearer to show all the forces being applied in the\nmain draw() loop, and hopefully this helped you learn about force accumulation. Now that\nwe’re more comfortable with that, perhaps it’s simpler to embed some of the details inside the\nobjects themselves.\nLet’s take a look at the rest of the elements in the Spring class.\nPVector force = attractor.attract(mover);\nmover.applyForce(force);\nPVector force = spring.connect(bob);\nbob.applyForce(force);\nspring.connect(bob);\nvoid connect(Bob b) {\nPVector force = some fancy calculations\nThe function connect() takes care of calling\napplyForce() and therefore doesn’t have to\nreturn a vector to the calling area.\nb.applyForce(force);\n}\nThe Nature of Code (v1.0)\n139\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 310
  },
  {
    "chunk_full": "Example 3.11: A Spring connection\nThe full code for this example is included on the book website, and the Web version also\nincorporates two additional features: (1) the Bob class includes functions for mouse\nclass Spring {\nWe need to keep track of the spring’s\nanchor location.\nPVector anchor;\nRest length and spring constant variables\nfloat len;\nfloat k = 0.1;\nThe constructor initializes the anchor point\nand rest length.\nSpring(float x, float y, int l) {\nanchor = new PVector(x,y);\nlen = l;\n}\nCalculate spring force—our implementation\nof Hooke’s Law.\nvoid connect(Bob b) {\nGet a vector pointing from anchor to Bob\nlocation.\nPVector force =\nPVector.sub(b.location,anchor);\nfloat d = force.mag();\nCalculate the displacement between\ndistance and rest length.\nfloat stretch = d - len;\nDirection and magnitude together!\nforce.normalize();\nforce.mult(-1 * k * stretch);\nCall applyForce() right here!\nb.applyForce(force);\n}\nDraw the anchor.\nvoid display() {\nfill(100);\nrectMode(CENTER);\nrect(anchor.x,anchor.y,10,10);\n}\nDraw the spring connection between Bob\nlocation and anchor.\nvoid displayLine(Bob b) {\nstroke(255);\nline(b.location.x,b.location.y,anchor.x,anchor.y);\n}\n}\nChapter 3. Oscillation\n140\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 311
  },
  {
    "chunk_full": "interactivity so that the bob can be dragged around the window, and (2) the Spring object\nincludes a function to constrain the connection’s length between a minimum and a maximum.\nBefore running to see the example online, take a look at this constrain function and see\nif you can fill in the blanks.\nvoid constrainLength(Bob b, float minlen, float maxlen) {\nVector pointing from Bob to Anchor\nPVector dir = PVector.sub(______,______);\nfloat d = dir.mag();\nIs it too short?\nif (d < minlen) {\ndir.normalize();\ndir.mult(________);\nKeep location within constraint.\nb.location = PVector.add(______,______);\nb.velocity.mult(0);\nIs it too long?\n} else if (____________) {\ndir.normalize();\ndir.mult(_________);\nKeep location within constraint.\nb.location = PVector.add(______,______);\nb.velocity.mult(0);\n}\n}\nExercise 3.15\nExercise 3.15\nCreate a system of multiple bobs and spring connections. How would you have a bob\nconnected to a bob with no fixed anchor?\nExercise 3.16\nExercise 3.16\nThe Nature of Code (v1.0)\n141\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 312
  },
  {
    "chunk_full": "The Ecosystem Project\nThe Ecosystem Project\nStep 3 Exercise:\nTake one of your creatures and incorporate oscillation into its motion. You can\nuse the Oscillator class from Example 3.7 as a model. The Oscillator object,\nhowever, oscillates around a single point (the middle of the window). Try\noscillating around a moving point. In other words, design a creature that moves\naround the screen according to location, velocity, and acceleration. But that\ncreature isn’t just a static shape, it’s an oscillating body. Consider tying the speed\nof oscillation to the speed of motion. Think of a butterfly’s flapping wings or the\nlegs of an insect. Can you make it appear that the creature’s internal mechanics\n(oscillation) drive its locomotion? For a sample, check out the\n“AttractionArrayWithOscillation” example with the code download.\nChapter 3. Oscillation\n142\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 313
  },
  {
    "chunk_full": "Chapter 4. Particle\nChapter 4. Particle\nSystems\nSystems\n“That is wise. Were I to invoke logic, however, logic clearly dictates that\nthe needs of the many outweigh the needs of the few.”\n— Spock\nIn 1982, William T. Reeves, a researcher at Lucasfilm Ltd., was working on the film Star Trek II:\nThe Wrath of Khan. Much of the movie revolves around the Genesis Device, a torpedo that\nwhen shot at a barren, lifeless planet has the ability to reorganize matter and create a\nhabitable world for colonization. During the sequence, a wall of fire ripples over the planet\nwhile it is being “terraformed.” The term particle system\nparticle system, an incredibly common and useful\ntechnique in computer graphics, was coined in the creation of this particular effect.\n“A particle system is a collection of many many minute particles that together represent a\nfuzzy object. Over a period of time, particles are generated into a system, move and change\nfrom within the system, and die from the system.”\n—William Reeves, \"Particle Systems—A Technique for Modeling a Class of Fuzzy\nObjects,\" ACM Transactions on Graphics 2:2 (April 1983), 92.\nSince the early 1980s, particle systems have been used in countless video games, animations,\ndigital art pieces, and installations to model various irregular types of natural phenomena,\nsuch as fire, smoke, waterfalls, fog, grass, bubbles, and so on.\nThis chapter will be dedicated to looking at implementation strategies for coding a particle\nsystem. How do we organize our code? Where do we store information related to individual\nparticles versus information related to the system as a whole? The examples we’ll look at will\nThe Nature of Code (v1.0)\n143\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 314
  },
  {
    "chunk_full": "focus on managing the data associated with a particle system. They’ll use simple shapes for\nthe particles and apply only the most basic behaviors (such as gravity). However, by using\nthis framework and building in more interesting ways to render the particles and compute\nbehaviors, you can achieve a variety of effects.\n4.1 Why We Need Particle Systems\n4.1 Why We Need Particle Systems\nWe’ve defined a particle system to be a collection of independent objects, often\nrepresented by a simple shape or dot. Why does this matter? Certainly, the prospect of\nmodeling some of the phenomena we listed (explosions!) is attractive and potentially useful.\nBut really, there’s an even better reason for us to concern ourselves with particle systems. If\nwe want to get anywhere in this nature of code life, we’re going to need to work with\nsystems of many things. We’re going to want to look at balls bouncing, birds flocking,\necosystems evolving, all sorts of things in plural.\nJust about every chapter after this one is going to need to deal with a list of objects. Yes,\nwe’ve done this with an array in some of our first vector and forces examples. But we need\nto go where no array has gone before.\nFirst, we’re going to want to deal with flexible quantities of elements. Sometimes we’ll have\nzero things, sometimes one thing, sometimes ten things, and sometimes ten thousand\nthings. Second, we’re going to want to take a more sophisticated object-oriented approach.\nInstead of simply writing a class to describe a single particle, we’re also going to want to\nwrite a class that describes the collection of particles—the particle system itself. The goal\nhere is to be able to write a main program that looks like the following:\nNo single particle is ever referenced in the above code, yet the result will be full of particles\nflying all over the screen. Getting used to writing Processing sketches with multiple classes,\nand classes that keep lists of instances of other classes, will prove very useful as we get to\nmore advanced chapters in this book.\nFinally, working with particle systems is also a good excuse for us to tackle two other\nadvanced object-oriented programming techniques: inheritance and polymorphism. With the\nAh, isn’t this main program so simple and\nlovely?\nParticleSystem ps;\nvoid setup() {\nsize(640,360);\nps = new ParticleSystem();\n}\nvoid draw() {\nbackground(255);\nps.run();\n}\nChapter 4. Particle Systems\n144\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 315
  },
  {
    "chunk_full": "examples we’ve seen up until now, we’ve always had an array of a single type of object, like\n\"movers\" or “oscillators.” With inheritance (and polymorphism), we’ll learn a convenient way to\nstore a single list that contains objects of different types. This way, a particle system need not\nonly be a system of a single type of particle.\nThough it may seem obvious to you, I’d also like to point out that there are typical\nimplementations of particle systems, and that’s where we will begin in this chapter. However,\nthe fact that the particles in this chapter look or behave a certain way should not limit your\nimagination. Just because particle systems tend to look sparkly, fly forward, and fall with\ngravity doesn’t mean that those are the characteristics yours should have.\nThe focus here is really just how to keep track of a system of many elements. What those\nelements do and how those elements look is up to you.\n4.2 A Single Particle\n4.2 A Single Particle\nBefore we can get rolling on the system itself, we have to write the class that will describe a\nsingle particle. The good news: we’ve done this already. Our Mover class from Chapter 2\nserves as the perfect template. For us, a particle is an independent body that moves about the\nscreen. It has location, velocity, and acceleration, a constructor to initialize those\nvariables, and functions to display() itself and update() its location.\nThis is about as simple as a particle can get. From here, we could take our particle in several\ndirections. We could add an applyForce() function to affect the particle’s behavior (we’ll do\nclass Particle {\nA “Particle” object is just another name for\nour “Mover.” It has location, velocity, and\nacceleration.\nPVector location;\nPVector velocity;\nPVector acceleration;\nParticle(PVector l) {\nlocation = l.get();\nacceleration = new PVector();\nvelocity = new PVector();\n}\nvoid update() {\nvelocity.add(acceleration);\nlocation.add(velocity);\n}\nvoid display() {\nstroke(0);\nfill(175);\nellipse(location.x,location.y,8,8);\n}\n}\nThe Nature of Code (v1.0)\n145\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 316
  },
  {
    "chunk_full": "precisely this in a future example). We could add variables to describe color and shape, or\nreference a PImage to draw the particle. For now, however, let’s focus on adding just one\nadditional detail: lifespan\nlifespan.\nTypical particle systems involve something called an emitter\nemitter. The emitter is the source of\nthe particles and controls the initial settings for the particles, location, velocity, etc. An\nemitter might emit a single burst of particles, or a continuous stream of particles, or both.\nThe point is that for a typical implementation such as this, a particle is born at the emitter\nbut does not live forever. If it were to live forever, our Processing sketch would eventually\ngrind to a halt as the number of particles increases to an unwieldy number over time. As\nnew particles are born, we need old particles to die. This creates the illusion of an infinite\nstream of particles, and the performance of our program does not suffer. There are many\ndifferent ways to decide when a particle dies. For example, it could come into contact with\nanother object, or it could simply leave the screen. For our first Particle class, however,\nwe’re simply going to add a lifespan variable. The timer will start at 255 and count down\nto 0, when the particle will be considered “dead.” And so we expand the Particle class as\nfollows:\nThe reason we chose to start the lifespan at 255 and count down to 0 is for convenience.\nWith those values, we can assign lifespan to act as the alpha transparency for the ellipse\nas well. When the particle is “dead” it will also have faded away onscreen.\nclass Particle {\nPVector location;\nPVector velocity;\nPVector acceleration;\nA new variable to keep track of how long\nthe particle has been “alive”\nfloat lifespan;\nParticle(PVector l) {\nlocation = l.get();\nacceleration = new PVector();\nvelocity = new PVector();\nWe start at 255 and count down for\nconvenience\nlifespan = 255;\n}\nvoid update() {\nvelocity.add(acceleration);\nlocation.add(velocity);\nLifespan decreases\nlifespan -= 2.0;\n}\nvoid display() {\nSince our life ranges from 255 to 0 we can\nuse it for alpha\nstroke(0,lifespan);\nfill(175,lifespan);\nellipse(location.x,location.y,8,8);\n}\n}\nChapter 4. Particle Systems\n146\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 317
  },
  {
    "chunk_full": "With the addition of the lifespan variable, we’ll also need one additional function—a function\nthat can be queried (for a true or false answer) as to whether the particle is alive or dead. This\nwill come in handy when we are writing the ParticleSystem class, whose task will be to\nmanage the list of particles themselves. Writing this function is pretty easy; we just need to\ncheck and see if the value of lifespan is less than 0. If it is we return true, if not we\nreturn false.\nBefore we get to the next step of making many particles, it’s worth taking a moment to make\nsure our particle works correctly and create a sketch with one single Particle object. Here is\nthe full code below, with two small additions. We add a convenience function called run()\nthat simply calls both update() and display() for us. In addition, we give the particle a\nrandom initial velocity as well as a downward acceleration (to simulate gravity).\nExample 4.1: A single particle\nboolean isDead() {\nIs the particle still alive?\nif (lifespan < 0.0) {\nreturn true;\n} else {\nreturn false;\n}\n}\nParticle p;\nvoid setup() {\nsize(640,360);\np = new Particle(new PVector(width/2,10));\n}\nvoid draw() {\nbackground(255);\nThe Nature of Code (v1.0)\n147\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 318
  },
  {
    "chunk_full": "Operating the single Particle\np.run();\nif (p.isDead()) {\nprintln(\"Particle dead!\");\n}\n}\nclass Particle {\nPVector location;\nPVector velocity;\nPVector acceleration;\nfloat lifespan;\nParticle(PVector l) {\nacceleration = new PVector(0,0.05);\nvelocity = new PVector(random(-1,1),random(-2,0));\nlocation = l.get();\nlifespan = 255.0;\n}\nFor demonstration purposes we assign the\nParticle an initial velocity and constant\nacceleration.\nSometimes it’s convenient to have a “run”\nfunction that calls all the other functions we\nneed.\nvoid run() {\nupdate();\ndisplay();\n}\nvoid update() {\nvelocity.add(acceleration);\nlocation.add(velocity);\nlifespan -= 2.0;\n}\nvoid display() {\nstroke(0,lifespan);\nfill(0,lifespan);\nellipse(location.x,location.y,8,8);\n}\nIs the Particle alive or dead?\nboolean isDead() {\nif (lifespan < 0.0) {\nreturn true;\n} else {\nreturn false;\n}\n}\n}\nChapter 4. Particle Systems\n148\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 319
  },
  {
    "chunk_full": "Now that we have a class to describe a single particle, we’re ready for the next big step. How\ndo we keep track of many particles, when we can’t ensure exactly how many particles we\nmight have at any given time?\nRewrite the example so that the particle can respond to force vectors via an\napplyForce() function.\nExercise 4.1\nExercise 4.1\nAdd angular velocity (rotation) to the particle. Create your own non-circle particle\ndesign.\nExercise 4.2\nExercise 4.2\n4.3 The ArrayList\n4.3 The ArrayList\nIn truth, we could use a simple array to manage our Particle objects. Some particle systems\nmight have a fixed number of particles, and arrays are magnificently efficient in those\ninstances. Processing also offers expand(), contract(), subset(), splice(), and other\nmethods for resizing arrays. However, for these examples, we’re going to take a more\nsophisticated approach and use the Java class ArrayList, found in the java.util package\nArrayList Documentation (http://download.oracle.com/javase/6/docs/api/java/util/\nArrayList.html).\nUsing an ArrayList follows the same idea as using a standard array, but with different syntax.\nThe following two code examples (which assume the existence of a generic Particle class)\nproduce the same result: first with an array, and second with an ArrayList.\nThe standard array way:\nint total = 10;\nParticle[] parray = new Particle[total];\nvoid setup() {\nThis is what we’re used to, accessing\nelements on the array via an index and\nbrackets—[ ].\nfor (int i = 0; i < parray.length; i++) {\nparray[i] = new Particle();\n}\nThe Nature of Code (v1.0)\n149\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 320
  },
  {
    "chunk_full": "The new ArrayList way:\nThis last for loop looks pretty similar to our code that looped through a regular array by\naccessing each index. We initialize a variable called i to 0 and count up by 1, accessing\neach element of the ArrayList until we get to the end. However, this is a good time to\nmention the “enhanced for loop” available in Java (and Processing), which is a bit more\nconcise. The enhanced loop works with both ArrayLists and regular arrays and looks like\nthis:\nLet’s translate that. Say “for each” instead of “for” and say “in” instead of “:”. Now you have:\n“For each Particle p in particles, run that Particle p!”\nI know. You cannot contain your excitement. I can’t. I know it’s not necessary, but I just have\nto type that again.\n}\nvoid draw() {\nfor (int i = 0; i < parray.length; i++) {\nParticle p = parray[i];\np.run();\n}\n}\nint total = 10;\nArrayList<Particle> plist = new ArrayList<Particle>();\nvoid setup() {\nfor (int i = 0; i < total; i++) {\nHave you ever seen this syntax before?\nThis is a new feature in Java 1.6 (called\n\"generics\") that Processing now supports.\nIt allows us to specify in advance what type\nof object we intend to put in the ArrayList.\nAn object is added to an ArrayList with\nadd().\nplist.add(new Particle());\n}\n}\nvoid draw() {\nThe size of the ArrayList is returned by\nsize().\nfor (int i = 0; i < plist.size(); i++) {\nAn object is accessed from the ArrayList\nwith get(). Because we are using generics,\nwe do not need to specify a type when we\npull objects out of the ArrayList.\nParticle p = plist.get(i);\np.run();\n}\n}\nArrayList<Particle> plist = new ArrayList<Particle>();\nfor (Particle p: particles) {\np.run();\n}\nChapter 4. Particle Systems\n150\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 321
  },
  {
    "chunk_full": "Simple, elegant, concise, lovely. Take a moment. Breathe. I have some bad news. Yes, we\nlove that enhanced loop and we will get to use it. But not right now. Our particle system\nexamples will require a feature that makes using that loop impossible. Let’s continue.\nThe code we’ve written above doesn’t take advantage of the ArrayList’s resizability, and it\nuses a fixed size of 10. We need to design an example that fits with our particle system\nscenario, where we emit a continuous stream of Particle objects, adding one new particle\nwith each cycle through draw(). We’ll skip rehashing the Particle class code here, as it\ndoesn’t need to change.\nRun the above code for a few minutes and you’ll start to see the frame rate slow down further\nand further until the program grinds to a halt (my tests yielded horrific performance after\nfifteen minutes). The issue of course is that we are creating more and more particles without\nremoving any.\nFortunately, the ArrayList class has a convenient remove() function that allows us to delete\na particle (by referencing its index). This is why we cannot use the new enhanced for loop we\njust learned; the enhanced loop provides no means for deleting elements while iterating.\nHere, we want to call remove() when the particle’s isDead() function returns true.\nThis enhanced loop also works for regular\narrays!\nfor (Particle p : particles) {\np.run();\n}\nArrayList<Particle> particles;\nvoid setup() {\nsize(640,360);\nparticles = new ArrayList<Particle>();\n}\nvoid draw() {\nbackground(255);\nparticles.add(new Particle(new PVector(width/2,50)));\nfor (int i = 0; i < particles.size(); i++) {\nParticle p = particles.get(i);\np.run();\n}\n}\nA new Particle object is added to the\nArrayList every cycle through draw().\nfor (int i = 0; i < particles.size(); i++) {\nParticle p = particles.get(i);\np.run();\nIf the particle is “dead,” we can go ahead\nand delete it from the list.\nif (p.isDead()) {\nparticles.remove(i);\n}\n}\nThe Nature of Code (v1.0)\n151\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 322
  },
  {
    "chunk_full": "Although the above code will run just fine (and the program will never grind to a halt), we\nhave opened up a medium-sized can of worms. Whenever we manipulate the contents of a\nlist while iterating through that very list, we can get ourselves into trouble. Take, for\nexample, the following code.\nThis is a somewhat extreme example (with flawed logic), but it proves the point. In the\nabove case, for each particle in the list, we add a new particle to the list (manipulating the\nsize() of the ArrayList). This will result in an infinite loop, as i can never increment past\nthe size of the ArrayList.\nWhile removing elements from the ArrayList during a loop doesn’t cause the program to\ncrash (as it does with adding), the problem is almost more insidious in that it leaves no\nevidence. To discover the problem we must first establish an important fact. When an object\nis removed from the ArrayList, all elements are shifted one spot to the left. Note the\ndiagram below where particle C (index 2) is removed. Particles A and B keep the same\nindex, while particles D and E shift from 3 and 4 to 2 and 3, respectively.\nLet’s pretend we are i looping through the ArrayList.\nwhen i = 0 → Check particle A → Do not delete\nwhen i = 1 → Check particle B → Do not delete\nwhen i = 2 → Check particle C → Delete!\nSlide particles D and E back from slots 3 and 4 to 2 and 3\nwhen i = 3 → Check particle E → Do not delete\nNotice the problem? We never checked particle D! When C was deleted from slot #2, D\nmoved into slot #2, but i has already moved on to slot # 3. This is not a disaster, since\nfor (int i = 0; i < particles.size(); i++) {\nParticle p = particles.get(i);\np.run();\nparticles.add(new Particle(new PVector(width/2,50)));\n}\nAdding a new Particle to the list while\niterating?\nFigure 4.1\nChapter 4. Particle Systems\n152\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 323
  },
  {
    "chunk_full": "particle D will get checked the next time around. Still, the expectation is that we are writing\ncode to iterate through every single element of the ArrayList. Skipping an element is\nunacceptable.\nThere are two solutions to this problem. The first solution is to simply iterate through the\nArrayList backwards. If you are sliding elements from right to left as elements are removed,\nit’s impossible to skip an element by accident. Here’s how the code would look:\nThis is a perfectly fine solution in ninety-nine cases out of a hundred. But sometimes, the\norder in which the elements are drawn could be important and you may not want to iterate\nbackwards. Java provides a special class—Iterator—that takes care of all of the details of\niteration for you. You get to say:\nHey, I’d like to iterate through this ArrayList. Could you continue to give me the next\nelement in the list one at a time until we get to the end? And if I remove elements or move\nthem around in the list while we’re iterating, will you make sure I don’t look at any elements\ntwice or skip any by accident?\nAn ArrayList can produce an Iterator object for you.\nOnce you’ve got the iterator, the hasNext() function will tell us whether there is a Particle\nfor us to run and the next() function will grab that Particle object itself.\nAnd if you call the remove() function on the Iterator object during the loop, it will delete the\ncurrent Particle object (and not skip ahead past the next one, as we saw with counting\nforward through the ArrayList).\nLooping through the list backwards\nfor (int i = particles.size()-1; i >= 0; i--) {\nParticle p = (Particle) particles.get(i);\np.run();\nif (p.isDead()) {\nparticles.remove(i);\n}\n}\nNote that with the Iterator object, we can\nalso use the new <ClassName> generics\nsyntax and specify the type that the Iterator\nwill reference.\nIterator<Particle> it = particles.iterator();\nAn Iterator object doing the iterating for you\nwhile (it.hasNext()) {\nParticle p = it.next();\np.run();\nif (p.isDead()) {\nThe Nature of Code (v1.0)\n153\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 324
  },
  {
    "chunk_full": "Putting it all together, we have:\nExample 4.2: ArrayList of particles with Iterator\nAn Iterator object doing the deleting for you\nit.remove();\n}\n}\nArrayList<Particle> particles;\nvoid setup() {\nsize(640,360);\nparticles = new ArrayList<Particle>();\n}\nvoid draw() {\nbackground(255);\nparticles.add(new Particle(new PVector(width/2,50)));\nIterator<Particle> it = particles.iterator();\nUsing an Iterator object instead of\ncounting with int i\nwhile (it.hasNext()) {\nParticle p = it.next();\np.run();\nif (p.isDead()) {\nit.remove();\n}\n}\n}\nChapter 4. Particle Systems\n154\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 325
  },
  {
    "chunk_full": "4.4 The Particle System Class\n4.4 The Particle System Class\nOK. Now we’ve done two things. We’ve written a class to describe an individual Particle\nobject. We’ve conquered the ArrayList and used it to manage a list of many Particle\nobjects (with the ability to add and delete at will).\nWe could stop here. However, one additional step we can and should take is to write a class\nto describe the list of Particle objects itself—the ParticleSystem class. This will allow us to\nremove the bulky logic of looping through all particles from the main tab, as well as open up\nthe possibility of having more than one particle system.\nIf you recall the goal we set at the beginning of this chapter, we wanted our main tab to look\nlike this:\nLet’s take the code from Example 4.2 and review a bit of object-oriented programming,\nlooking at how each piece from the main tab can fit into the ParticleSystem class.\nJust one wee ParticleSystem!\nParticleSystem ps;\nvoid setup() {\nsize(640,360);\nps = new ParticleSystem();\n}\nvoid draw() {\nbackground(255);\nps.run();\n}\nThe Nature of Code (v1.0)\n155\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 326
  },
  {
    "chunk_full": "ArrayList in the main tab\nArrayList in the main tab\nArrayList in the ParticleSystem class\nArrayList in the ParticleSystem class\nArrayList<Particle> particles;\nvoid setup() {\nsize(640,360);\nparticles = new ArrayList<Particle>();\n}\nvoid draw() {\nbackground(255);\nparticles.add(new Particle());\nIterator<Particle> it =\nparticles.iterator();\nwhile (it.hasNext()) {\nParticle p = it.next();\np.run();\nif (p.isDead()) {\nit.remove();\n}\n}\n}\nclass ParticleSystem {\nArrayList<Particle> particles;\nParticleSystem() {\nparticles = new ArrayList<Particle>();\n}\nvoid addParticle() {\nparticles.add(new Particle());\n}\nvoid run() {\nIterator<Particle> it =\nparticles.iterator();\nwhile (it.hasNext()) {\nParticle p = it.next();\np.run();\nif (p.isDead()) {\nit.remove();\n}\n}\n}\n}\nWe could also add some new features to the particle system itself. For example, it might be\nuseful for the ParticleSystem class to keep track of an origin point where particles are\nmade. This fits in with the idea of a particle system being an “emitter,” a place where\nparticles are born and sent out into the world. The origin point should be initialized in the\nconstructor.\nExample 4.3: Simple Single Particle System\nclass ParticleSystem {\nArrayList particles;\nThis particular ParticleSystem\nimplementation includes an origin point\nwhere each Particle begins.\nPVector origin;\nParticleSystem(PVector location) {\norigin = location.get();\nparticles = new ArrayList();\n}\nvoid addParticle() {\nThe origin is passed to each Particle when\nit is added.\nparticles.add(new Particle(origin));\n}\nChapter 4. Particle Systems\n156\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 327
  },
  {
    "chunk_full": "Make the origin point move dynamically. Have the particles emit from the mouse\nlocation or use the concepts of velocity and acceleration to make the system move\nautonomously.\nExercise 4.3\nExercise 4.3\nBuilding off Chapter 3’s “Asteroids” example, use a particle system to emit particles\nfrom the ship’s “thrusters” whenever a thrust force is applied. The particles’ initial\nvelocity should be related to the ship’s current direction.\nExercise 4.4\nExercise 4.4\n4.5 A System of Systems\n4.5 A System of Systems\nLet’s review for a moment where we are. We know how to talk about an individual Particle\nobject. We also know how to talk about a system of Particle objects, and this we call a\n“particle system.” And we’ve defined a particle system as a collection of independent objects.\nBut isn’t a particle system itself an object? If that’s the case (which it is), there’s no reason why\nwe couldn’t also have a collection of many particle systems, i.e. a system of systems.\nThis line of thinking could of course take us even further, and you might lock yourself in a\nbasement for days sketching out a diagram of a system of systems of systems of systems of\nsystems of systems. Of systems. After all, this is how the world works. An organ is a system of\ncells, a human body is a system of organs, a neighborhood is a system of human bodies, a city\nis a system of neighborhoods, and so on and so forth. While this is an interesting road to\ntravel down, it’s a bit beyond where we need to be right now. It is, however, quite useful to\nknow how to write a Processing sketch that keeps track of many particle systems, each of\nwhich keep track of many particles. Let’s take the following scenario.\nYou start with a blank screen.\nThe Nature of Code (v1.0)\n157\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 328
  },
  {
    "chunk_full": "You click the mouse and generate a particle system at the mouse’s location.\nEach time you click the mouse, a new particle system is created at the mouse’s location.\nIn Example 4.3 (see page 156), we stored a single reference to a ParticleSystem object in\nthe variable ps.\nFor this new example, what we want to do instead is create an ArrayList to keep track of\nmultiple instances of particle systems. When the program starts, i.e. in setup(), the\nArrayList is empty.\nParticleSystem ps;\nvoid setup() {\nsize(640,360);\nps = new ParticleSystem(1,new PVector(width/2,50));\n}\nvoid draw() {\nbackground(255);\nps.run();\nps.addParticle();\n}\nChapter 4. Particle Systems\n158\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 329
  },
  {
    "chunk_full": "Example 4.4: System of systems\nWhenever the mouse is pressed, a new ParticleSystem object is created and placed into the\nArrayList.\nAnd in draw(), instead of referencing a single ParticleSystem object, we now look through\nall the systems in the ArrayList and call run() on each of them.\nThis time, the type of thing we are putting in\nthe ArrayList is a ParticleSystem itself!\nArrayList<ParticleSystem> systems;\nvoid setup() {\nsize(600,200);\nsystems = new ArrayList<ParticleSystem>();\n}\nvoid mousePressed() {\nsystems.add(new ParticleSystem(new PVector(mouseX,mouseY)));\n}\nvoid draw() {\nbackground(255);\nSince we aren’t deleting elements, we can\nuse our enhanced loop!\nfor (ParticleSystem ps: systems) {\nps.run();\nps.addParticle();\n}\n}\nRewrite Example 4.4 so that each particle system doesn’t live forever. When a particle\nsystem is empty (i.e. has no particles left in its ArrayList), remove it from the\nArrayList systems.\nExercise 4.5\nExercise 4.5\nCreate a simulation of an object shattering into many pieces. How can you turn one\nlarge shape into many small particles? What if there are several large shapes on the\nscreen and they shatter when you click on them?\nExercise 4.6\nExercise 4.6\nThe Nature of Code (v1.0)\n159\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 330
  },
  {
    "chunk_full": "4.6 Inheritance and Polymorphism: An Introduction\n4.6 Inheritance and Polymorphism: An Introduction\nYou may have encountered the terms inheritance and polymorphism in your programming\nlife before this book. After all, they are two of the three fundamental principles behind the\ntheory of object-oriented programming (the other being encapsulation). If you’ve read other\nProcessing or Java programming books, chances are it’s been covered. My beginner text,\nLearning Processing, has close to an entire chapter (#22) dedicated to these two topics.\nStill, perhaps you’ve only learned about it in the abstract sense and never had a reason to\nreally use inheritance and polymorphism. If this is true, you’ve come to the right place.\nWithout these two topics, your ability to program a variety of particles and particle systems\nis extremely limited. (In the next chapter, we’ll also see how understanding these topics will\nhelp us to use physics libraries.)\nImagine the following. It’s a Saturday morning, you’ve just gone out for a lovely jog, had a\ndelicious bowl of cereal, and are sitting quietly at your computer with a cup of warm\nchamomile tea. It’s your old friend So and So’s birthday and you’ve decided you’d like to\nmake a greeting card in Processing. How about some confetti for a birthday? Purple\nconfetti, pink confetti, star-shaped confetti, square confetti, fast confetti, fluttery confetti,\netc. All of these pieces of confetti with different appearances and different behaviors\nexplode onto the screen at once.\nWhat we’ve got here is clearly a particle system—a collection of individual pieces of confetti\n(i.e. particles). We might be able to cleverly design our Particle class to have variables\nthat store its color, shape, behavior, etc. And perhaps we initialize the values of these\nvariables randomly. But what if your particles are drastically different? This could become\nvery messy, having all sorts of code for different ways of being a particle in the same class.\nWell, you might consider doing the following:\nThis is a nice solution: we have three different classes to describe the different kinds of\npieces of confetti that could be part of our particle system. The ParticleSystem\nconstructor could then have some code to pick randomly from the three classes when filling\nthe ArrayList. Note that this probabilistic method is the same one we employed in our\nrandom walk examples in the Introduction (see page 2).\nclass HappyConfetti {\n}\nclass FunConfetti {\n}\nclass WackyConfetti {\n}\nChapter 4. Particle Systems\n160\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 331
  },
  {
    "chunk_full": "OK, we now need to pause for a moment. We’ve done nothing wrong. All we wanted to do\nwas wish our friend a happy birthday and enjoy writing some code. But while the reasoning\nbehind the above approach is quite sound, we’ve opened up two major problems.\nProblem #1: Aren’t we going to be copying/pasting a lot of code between the\ndifferent “confetti” classes?\nYes. Even though our kinds of particles are different enough to merit our breaking them out\ninto separate classes, there is still a ton of code that they will likely share. They’ll all have\nPVectors to keep track of location, velocity, and acceleration; an update() function that\nimplements our motion algorithm; etc.\nThis is where inheritance\ninheritance comes in. Inheritance allows us to write a class that inherits\nvariables and functions from another class, all the while implementing its own custom\nfeatures.\nProblem #2: How will the ArrayList know which objects are which type?\nThis is a pretty serious problem. Remember, we were using generics to tell the ArrayList\nwhat type of objects we’re going to put inside it. Are we suddenly going to need three\ndifferent ArrayLists?\nThis seems awfully inconvenient, given that we really just want one list to keep track of all the\nstuff in the particle system. That can be made possible with polymorphism. Polymorphism will\nallow us to consider objects of different types as the same type and store them in a single\nArrayList.\nclass ParticleSystem {\nParticleSystem(int num) {\nparticles = new ArrayList();\nfor (int i = 0; i < num; i++) {\nfloat r = random(1);\nRandomly picking a \"kind\" of particle\nif\n(r < 0.33) { particles.add(new HappyConfetti()); }\nelse if (r < 0.67) { particles.add(new FunConfetti());\n}\nelse\n{ particles.add(new WackyConfetti()); }\n}\n}\nArrayList<HappyConfetti> a1 = new ArrayList<HappyConfetti>();\nArrayList<FunConfetti>\na2 = new ArrayList<FunConfetti>();\nArrayList<WackyConfetti> a3 = new ArrayList<WackyConfetti>();\nThe Nature of Code (v1.0)\n161\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 332
  },
  {
    "chunk_full": "Now that we understand the problem, let’s look at these two concepts in a bit more detail\nand then create a particle system example that implements both inheritance and\npolymorphism.\n4.7 Inheritance Basics\n4.7 Inheritance Basics\nLet’s take a different example, the world of animals: dogs, cats, monkeys, pandas, wombats,\nand sea nettles. We’ll start by programming a Dog class. A Dog object will have an age\nvariable (an integer), as well as eat(), sleep(), and bark() functions.\nNow, let’s move on to cats.\nclass Dog {\nint age;\nDogs and cats have the same variables\n(age) and functions (eat, sleep).\nDog() {\nage = 0;\n}\nvoid eat() {\nprintln(\"Yum!\");\n}\nvoid sleep() {\nprintln(\"Zzzzzz\");\n}\nA unique function for barking.\nvoid bark() {\nprintln(\"WOOF!\");\n}\n}\nChapter 4. Particle Systems\n162\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 333
  },
  {
    "chunk_full": "As we rewrite the same code for fish, horses, koalas, and lemurs, this process will become\nrather tedious. Instead, let’s develop a generic Animal class that can describe any type of\nanimal. All animals eat and sleep, after all. We could then say:\n•\nA dog is an animal and has all the properties of animals and can do all the things\nanimals do. Also, a dog can bark.\n•\nA cat is an animal and has all the properties of animals and can do all the things\nanimals do. Also, a cat can meow.\nInheritance makes this all possible. With inheritance, classes can inherit properties (variables)\nand functionality (methods) from other classes. A Dog class is a child (subclass\nsubclass) of an Animal\nclass. Children will automatically inherit all variables and functions from the parent\n(superclass\nsuperclass), but can also include functions and variables not found in the parent. Like a\nphylogenetic \"tree of life,\" inheritance follows a tree structure. Dogs inherit from canines,\nwhich inherit from mammals, which inherit from animals, etc.\nclass Cat {\nint age;\nCat() {\nage = 0;\n}\nvoid eat() {\nprintln(\"Yum!\");\n}\nvoid sleep() {\nprintln(\"Zzzzzz\");\n}\nvoid meow() {\nprintln(\"MEOW!\");\n}\n}\nFigure 4.2\nThe Nature of Code (v1.0)\n163\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 334
  },
  {
    "chunk_full": "Here is how the syntax works with inheritance.\nThis brings up two new terms:\n•\nextends\nextends – This keyword is used to indicate a parent for the class being defined.\nNote that classes can only extend one class. However, classes can extend classes\nthat extend other classes, i.e. Dog extends Animal, Terrier extends Dog.\nEverything is inherited all the way down the line.\n•\nsuper()\nsuper() – This calls the constructor in the parent class. In other words, whatever\nyou do in the parent constructor, do so in the child constructor as well. Other code\ncan be written into the constructor in addition to super(). super() can also\nThe Animal class is the parent (or super)\nclass.\nclass Animal {\nDog and Cat inherit the variable age.\nint age;\nAnimal() {\nage = 0;\n}\nDog and Cat inherit the functions eat() and\nsleep().\nvoid eat() {\nprintln(\"Yum!\");\n}\nvoid sleep() {\nprintln(\"Zzzzzz\");\n}\n}\nThe Dog class is the child (or sub) class,\nindicated by the code \"extends Animal\".\nclass Dog extends Animal {\nDog() {\nsuper() executes code found in the parent\nclass.\nsuper();\n}\nWe define bark() in the child class, since it\nisn't part of the parent class.\nvoid bark() {\nprintln(\"WOOF!\");\n}\n}\nclass Cat extends Animal {\nCat() {\nsuper();\n}\nvoid meow() {\nprintln(\"MEOW!\");\n}\n}\nChapter 4. Particle Systems\n164\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 335
  },
  {
    "chunk_full": "receive arguments if there is a parent constructor defined with matching arguments.\nA subclass can be expanded to include additional functions and properties beyond what is\ncontained in the superclass. For example, let’s assume that a Dog object has a haircolor\nvariable in addition to age, which is set randomly in the constructor. The class would now look\nlike this:\nNote how the parent constructor is called via super(), which sets the age to 0, but the\nhaircolor is set inside the Dog constructor itself. If a Dog object eats differently than a generic\nAnimal object, parent functions can be overridden by rewriting the function inside the\nsubclass.\nBut what if a dog eats the same way as a generic animal, just with some extra functionality? A\nsubclass can both run the code from a parent class and incorporate custom code.\nclass Dog extends Animal {\nA child class can introduce new variables\nnot included in the parent.\ncolor haircolor;\nDog() {\nsuper();\nhaircolor = color(random(255));\n}\nvoid bark() {\nprintln(\"WOOF!\");\n}\n}\nclass Dog extends Animal {\ncolor haircolor;\nDog() {\nsuper();\nhaircolor = color(random(255));\n}\nA child can override a parent function if\nnecessary.\nvoid eat() {\nA Dog's specific eating characteristics\nprintln(\"Woof! Woof! Slurp.\")\n}\nvoid bark() {\nprintln(\"WOOF!\");\n}\n}\nThe Nature of Code (v1.0)\n165\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 336
  },
  {
    "chunk_full": "class Dog extends Animal {\ncolor haircolor;\nDog() {\nsuper();\nhaircolor = color(random(255));\n}\nvoid eat() {\nCall eat() from Animal. A child can execute\na function from the parent while adding its\nown code.\nsuper.eat();\nAdd some additional code for a Dog's\nspecific eating characteristics.\nprintln(\"Woof!!!\");\n}\nvoid bark() {\nprintln(\"WOOF!\");\n}\n}\n4.8 Particles with Inheritance\n4.8 Particles with Inheritance\nNow that we’ve had an introduction to the theory of inheritance and its syntax, we can\ndevelop a working example in Processing based on our Particle class.\nLet’s review a simple Particle implementation, further simplified from Example 4.1 (see\npage 147):\nChapter 4. Particle Systems\n166\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 337
  },
  {
    "chunk_full": "Next, we create a subclass from Particle (let’s call it Confetti). It will inherit all the instance\nvariables and methods from Particle. We write a new constructor with the name Confetti\nand execute the code from the parent class by calling super().\nclass Particle {\nPVector location;\nPVector velocity;\nPVector acceleration;\nParticle(PVector l) {\nacceleration = new PVector(0,0.05);\nvelocity = new PVector(random(-1,1),random(-2,0));\nlocation = l.get();\n}\nvoid run() {\nupdate();\ndisplay();\n}\nvoid update() {\nvelocity.add(acceleration);\nlocation.add(velocity);\n}\nvoid display() {\nfill(0);\nellipse(location.x,location.y,8,8);\n}\n}\nclass Confetti extends Particle {\nWe could add variables for only Confetti\nhere.\nConfetti(PVector l) {\nsuper(l);\n}\nThere is no code here because we inherit\nupdate() from parent.\nOverride the display method.\nvoid display() {\nrectMode(CENTER);\nfill(175);\nstroke(0);\nrect(location.x,location.y,8,8);\n}\n}\nThe Nature of Code (v1.0)\n167\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 338
  },
  {
    "chunk_full": "Let’s make this a bit more sophisticated. Let’s say we want to have the Confetti particle\nrotate as it flies through the air. We could, of course, model angular velocity and\nacceleration as we did in Chapter 3. Instead, we’ll try a quick and dirty solution.\nWe know a particle has an x location somewhere between 0 and the width of the window.\nWhat if we said: when the particle’s x location is 0, its rotation should be 0; when its x\nlocation is equal to the width, its rotation should be equal to TWO_PI? Does this ring a bell?\nWhenever we have a value with one range that we want to map to another range, we can\nuse Processing’s map() function, which we learned about in the Introduction (see page 17)!\nAnd just to give it a bit more spin, we can actually map the angle’s range from 0 to\nTWO_PI*2. Let’s look at how this code fits into the display() function.\nNow that we have a Confetti class that extends our base Particle class, we need to\nfigure out how our ParticleSystem class can manage particles of different types within the\nsame system. To accomplish this goal, let’s return to the animal kingdom inheritance\nexample and see how the concept extends into the world of polymorphism.\nfloat angle = map(location.x,0,width,0,TWO_PI);\nvoid display() {\nfloat theta = map(location.x,0,width,0,TWO_PI*2);\nrectMode(CENTER);\nfill(0,lifespan);\nstroke(0,lifespan);\nIf we rotate() a shape in Processing, we\nneed to familiarize ourselves with\ntransformations. For more, visit:\nhttp://processing.org/learning/transform2d/\npushMatrix();\ntranslate(location.x,location.y);\nrotate(theta);\nrect(0,0,8,8);\npopMatrix();\n}\nInstead of using map() to calculate theta, how would you model angular velocity and\nacceleration?\nExercise 4.7\nExercise 4.7\n4.9 Polymorphism Basics\n4.9 Polymorphism Basics\nWith the concept of inheritance under our belts, we can imagine how we would program a\ndiverse animal kingdom using ArrayLists—an array of dogs, an array of cats, of turtles, of\nkiwis, etc. frolicking about.\nChapter 4. Particle Systems\n168\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 339
  },
  {
    "chunk_full": "As the day begins, the animals are all pretty hungry and are looking to eat. So it’s off to\nlooping time (enhanced looping time!)…\nThis works well, but as our world expands to include many more animal species, we’re going\nto get stuck writing a lot of individual loops. Is this really necessary? After all, the creatures\nare all animals, and they all like to eat. Why not just have one ArrayList of Animal objects\nand fill it with all different kinds of animals?\nSeparate ArrayLists for each animal\nArrayList<Dog> dogs = new ArrayList<Dog>();\nArrayList<Cat> cats = new ArrayList<Cat>();\nArrayList<Turtle> turtles = new ArrayList<Turtle>();\nArrayList<Kiwi> kiwis = new ArrayList<Kiwi>();\nfor (int i = 0; i < 10; i++) {\ndogs.add(new Dog());\n}\nfor (int i = 0; i < 15; i++) {\ncats.add(new Cat());\n}\nfor (int i = 0; i < 6; i++) {\nturtles.add(new Turtle());\n}\nfor (int i = 0; i < 98; i++) {\nkiwis.add(new Kiwi());\n}\nSeparate loops for each animal\nfor (Dog d: dogs) {\nd.eat();\n}\nfor (Cat c: cats) {\nc.eat();\n}\nfor (Turtle t: turtles) {\nt.eat();\n}\nfor (Kiwi k: kiwis) {\nk.eat();\n}\nThe Nature of Code (v1.0)\n169\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 340
  },
  {
    "chunk_full": "The ability to treat a Dog object as either a member of the Dog class or the Animal class (its\nparent) is an example of polymorphism. Polymorphism\nPolymorphism (from the Greek polymorphos,\nmeaning many forms) refers to the treatment of a single instance of an object in multiple\nforms. A dog is certainly a dog, but since Dog extends Animal, it can also be considered\nan animal. In code, we can refer to it both ways.\nAlthough the second line of code might initially seem to violate syntax rules, both ways of\ndeclaring a Dog object are legal. Even though we declare spot as an Animal object, we’re\nreally making a Dog object and storing it in the spot variable. And we can safely call all of\nthe Animal class methods on spot because the rules of inheritance dictate that a dog can\ndo anything an animal can.\nWhat if the Dog class, however, overrides the eat() function in the Animal class? Even if\nspot is declared as an Animal, Java will determine that its true identity is that of a Dog and\nrun the appropriate version of the eat() function.\nThis is particularly useful when we have an array or ArrayList.\nArrayList<Animal> kingdom = new ArrayList<Animal>();\nfor (int i = 0; i < 1000; i++) {\nif (i < 100) kingdom.add(new Dog());\nelse if (i < 400) kingdom.add(new Cat());\nelse if (i < 900) kingdom.add(new Turtle());\nelse kingdom.add(new Kiwi());\n}\nfor (Animal a: kingdom) {\na.eat();\n}\nJust one ArrayList for all the animals!\nDog rover = new Dog();\nAnimal spot = new Dog();\n4.10 Particle Systems with Polymorphism\n4.10 Particle Systems with Polymorphism\nLet’s pretend for a moment that polymorphism doesn’t exist and rewrite a ParticleSystem\nclass to include many Particle objects and many Confetti objects.\nclass ParticleSystem {\nWe’re stuck doing everything twice with\ntwo lists!\nArrayList<Particle> particles;\nArrayList<Confetti> confetti;\nChapter 4. Particle Systems\n170\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 341
  },
  {
    "chunk_full": "Notice how we have two separate lists, one for particles and one for confetti. Every action we\nwant to perform we have to do twice! Polymorphism allows us to simplify the above by just\nmaking one ArrayList of particles that contains both standard Particle objects as well as\nConfetti objects. We don’t have to worry about which are which; this will all be taken care of\nfor us! (Also, note that the code for the main program and the classes has not changed, so we\naren’t including it here. See the website for the full example.)\nPVector origin;\nParticleSystem(PVector location) {\norigin = location.get();\nWe’re stuck doing everything twice with two\nlists!\nparticles = new ArrayList<Particle>();\nconfetti = new ArrayList<Confetti>();\n}\nvoid addParticle() {\nWe’re stuck doing everything twice with two\nlists!\nparticles.add(new Particle(origin));\nparticles.add(new Confetti(origin));\n}\nvoid run() {\nWe’re stuck doing everything twice with two\nlists!\nIterator<Particle> it = particles.iterator();\nwhile (it.hasNext()) {\nParticle p = it.next();\np.run();\nif (p.isDead()) {\nit.remove();\n}\n}\nit = confetti.iterator();\nwhile (it.hasNext()) {\nConfetti c = it.next();\nc.run();\nif (c.isDead()) {\nit.remove();\n}\n}\n}\n}\nThe Nature of Code (v1.0)\n171\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 342
  },
  {
    "chunk_full": "Example 4.5: Particle system inheritance and polymorphism\nclass ParticleSystem {\nOne list, for anything that is a Particle or\nextends Particle\nArrayList<Particle> particles;\nPVector origin;\nParticleSystem(PVector location) {\norigin = location.get();\nparticles = new ArrayList<Particle>();\n}\nvoid addParticle() {\nfloat r = random(1);\nWe have a 50% chance of adding each\nkind of Particle.\nif (r < 0.5) {\nparticles.add(new Particle(origin));\n} else {\nparticles.add(new Confetti(origin));\n}\n}\nvoid run() {\nIterator<Particle> it = particles.iterator();\nwhile (it.hasNext()) {\nPolymorphism allows us to treat everything\nas a Particle, whether it is a Particle or a\nConfetti.\nParticle p = it.next();\np.run();\nif (p.isDead()) {\nit.remove();\n}\n}\n}\n}\nChapter 4. Particle Systems\n172\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 343
  },
  {
    "chunk_full": "Create a particle system with different “kinds” of particles in the same system. Try\nvarying more than just the look of the particles. How do you deal with different\nbehaviors using inheritance?\nExercise 4.8\nExercise 4.8\n4.11 Particle Systems with Forces\n4.11 Particle Systems with Forces\nSo far in this chapter, we’ve been focusing on structuring our code in an object-oriented way\nto manage a collection of particles. Maybe you noticed, or maybe you didn’t, but during this\nprocess we unwittingly took a couple steps backward from where we were in previous\nchapters. Let’s examine the constructor of our simple Particle class.\nAnd now let’s look at the update() function.\nOur Particle class is structured to have a constant acceleration, one that never changes. A\nmuch better framework would be to follow Newton’s second law (F = M* A) and incorporate\nthe force accumulation algorithm we worked so hard on in Chapter 2 (see page 68).\nStep 1 would be to add in the applyForce() function. (Remember, we need to make a copy of\nthe PVector before we divide it by mass.)\nParticle(PVector l) {\nWe’re setting acceleration to a constant\nvalue!\nacceleration = new PVector(0,0.05);\nvelocity = new PVector(random(-1,1),random(-2,0));\nlocation = l.get();\nlifespan = 255.0;\n}\nvoid update() {\nvelocity.add(acceleration);\nlocation.add(velocity);\n// Where is the line of code to clear acceleration?\nlifespan -= 2.0;\n}\nvoid applyForce(PVector force) {\nPVector f = force.get();\nf.div(mass);\nacceleration.add(f);\n}\nThe Nature of Code (v1.0)\n173\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 344
  },
  {
    "chunk_full": "Once we have this, we can add in one more line of code to clear the acceleration at the end\nof update().\nAnd our Particle class is complete!\nvoid update() {\nvelocity.add(acceleration);\nlocation.add(velocity);\nThere it is!\nacceleration.mult(0);\nlifespan -= 2.0;\n}\nclass Particle {\nPVector location;\nPVector velocity;\nPVector acceleration;\nfloat lifespan;\nWe could vary mass for more interesting\nresults.\nfloat mass = 1;\nParticle(PVector l) {\nWe now start with acceleration of 0.\nacceleration = new PVector(0,0);\nvelocity = new PVector(random(-1,1),random(-2,0));\nlocation = l.get();\nlifespan = 255.0;\n}\nvoid run() {\nupdate();\ndisplay();\n}\nNewton’s second law & force\naccumulation\nvoid applyForce(PVector force) {\nPVector f = force.get();\nf.div(mass);\nacceleration.add(f);\n}\nStandard update\nvoid update() {\nvelocity.add(acceleration);\nlocation.add(velocity);\nacceleration.mult(0);\nlifespan -= 2.0;\n}\nChapter 4. Particle Systems\n174\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 345
  },
  {
    "chunk_full": "Now that the Particle class is completed, we have a very important question to ask. Where\ndo we call the applyForce() function? Where in the code is it appropriate to apply a force to\na particle? The truth of the matter is that there’s no right or wrong answer; it really depends on\nthe exact functionality and goals of a particular Processing sketch. Still, we can create a\ngeneric situation that would likely apply to most cases and craft a model for applying forces to\nindividual particles in a system.\nLet’s consider the following goal: Apply a force globally every time through draw() to all\nparticles. We’ll pick an easy one for now: a force pointing down, like gravity.\nWe said it should always be applied, i.e. in draw(), so let’s take a look at our draw() function\nas it stands.\nWell, it seems that we have a small problem. applyForce() is a method written inside the\nParticle class, but we don’t have any reference to the individual particles themselves, only\nthe ParticleSystem object: the variable ps.\nSince we want all particles to receive the force, however, we can decide to apply the force to\nthe particle system and let it manage applying the force to all the individual particles:\nOur Particle is a circle.\nvoid display() {\nstroke(255,lifespan);\nfill(255,lifespan);\nellipse(location.x,location.y,8,8);\n}\nShould the Particle be deleted?\nboolean isDead() {\nif (lifespan < 0.0) {\nreturn true;\n} else {\nreturn false;\n}\n}\n}\nPVector gravity = new PVector(0,0.1);\nvoid draw() {\nbackground(100);\nps.addParticle();\nps.run();\n}\nvoid draw() {\nbackground(100);\nPVector gravity = new PVector(0,0.1);\nThe Nature of Code (v1.0)\n175\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 346
  },
  {
    "chunk_full": "Of course, if we call a new function on the ParticleSystem object in draw(), well, we have\nto write that function in the ParticleSystem class. Let’s describe the job that function\nneeds to perform: receive a force as a PVector and apply that force to all the particles.\nNow in code:\nIt almost seems silly to write this function. What we’re saying is “apply a force to a particle\nsystem so that the system can apply that force to all of the individual particles.”\nNevertheless, it’s really quite reasonable. After all, the ParticleSystem object is in charge\nof managing the particles, so if we want to talk to the particles, we’ve got to talk to them\nthrough their manager. (Also, here’s a chance for the enhanced loop since we aren’t\ndeleting particles!)\nHere is the full example (assuming the existence of the Particle class written above; no\nneed to include it again since nothing has changed):\nApplying a force to the system as a whole\nps.applyForce(gravity);\nps.addParticle();\nps.run();\n}\nvoid applyForce(PVector f) {\nfor (Particle p: particles) {\np.applyForce(f);\n}\n}\nChapter 4. Particle Systems\n176\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 347
  },
  {
    "chunk_full": "Example 4.6: Particle system with forces\nParticleSystem ps;\nvoid setup() {\nsize(640,360);\nps = new ParticleSystem(new PVector(width/2,50));\n}\nvoid draw() {\nbackground(100);\nApply a force to all particles.\nPVector gravity = new PVector(0,0.1);\nps.applyForce(gravity);\nps.addParticle();\nps.run();\n}\nclass ParticleSystem {\nArrayList<Particle> particles;\nPVector origin;\nParticleSystem(PVector location) {\norigin = location.get();\nparticles = new ArrayList<Particle>();\n}\nvoid addParticle() {\nparticles.add(new Particle(origin));\n}\nvoid applyForce(PVector f) {\nUsing an enhanced loop to apply the force\nto all particles\nfor (Particle p: particles) {\np.applyForce(f);\n}\n}\nvoid run() {\nCan’t use the enhanced loop because we\nwant to check for particles to delete.\nIterator<Particle> it = particles.iterator();\nwhile (it.hasNext()) {\nParticle p = (Particle) it.next();\np.run();\nif (p.isDead()) {\nit.remove();\n}\n}\n}\n}\nThe Nature of Code (v1.0)\n177\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 348
  },
  {
    "chunk_full": "4.12 Particle Systems with Repellers\n4.12 Particle Systems with Repellers\nWhat if we wanted to take this example one step further and add a Repeller object—the\ninverse of the Attractor object we covered in Chapter 2 (see page 88) that pushes any\nparticles away that get close? This requires a bit more sophistication because, unlike the\ngravity force, each force an attractor or repeller exerts on a particle must be calculated for\neach particle.\nLet’s start solving this problem by examining how we would incorporate a new Repeller\nobject into our simple particle system plus forces example. We’re going to need two major\nadditions to our code:\n1.\nA Repeller object (declared, initialized, and displayed).\n2.\nA function that passes the Repeller object into the ParticleSystem so that it can\napply a force to each particle object.\nFigure 4.3: Gravity force—vectors are all\nidentical\nFigure 4.4: Attractor force—vectors are all\ndifferent\nParticleSystem ps;\nNew thing: we declare a Repeller object.\nRepeller repeller;\nvoid setup() {\nsize(640,360);\nps = new ParticleSystem(new PVector(width/2,50));\nChapter 4. Particle Systems\n178\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 349
  },
  {
    "chunk_full": "Making a Repeller object is quite easy; it’s a duplicate of the Attractor class from Chapter\n2, Example 2.6 .\nThe more difficult question is, how do we write the applyRepeller() function? Instead of\npassing a PVector into a function like we do with applyForce(), we’re going to instead pass\na Repeller object into applyRepeller() and ask that function to do the work of calculating\nthe force between the repeller and all particles. Let’s look at both of these functions side by\nside.\nNew thing: we initialize a Repeller object.\nrepeller = new Repeller(width/2-20,height/2);\n}\nvoid draw() {\nbackground(100);\nps.addParticle();\nPVector gravity = new PVector(0,0.1);\nps.applyForce(gravity);\nNew thing: we need a function to apply a\nforce from a repeller.\nps.applyRepeller(repeller);\nps.run();\nNew thing: we display the Repeller object.\nrepeller.display();\n}\nclass Repeller {\nA Repeller doesn’t move, so you just need\nlocation.\nPVector location;\nfloat r = 10;\nRepeller(float x, float y)\n{\nlocation = new PVector(x,y);\n}\nvoid display() {\nstroke(255);\nfill(255);\nellipse(location.x,location.y,r*2,r*2);\n}\n}\nThe Nature of Code (v1.0)\n179\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 350
  },
  {
    "chunk_full": "applyForce()\napplyForce()\napplyRepeller\napplyRepeller\nvoid applyForce(PVector f) {\nfor (Particle p: particles) {\np.applyForce(f);\n}\n}\nvoid applyRepeller(Repeller r) {\nfor (Particle p: particles) {\nPVector force = r.repel(p);\np.applyForce(force);\n}\n}\nThe functions are almost identical. There are only two differences. One we mentioned\nbefore—a Repeller object is the argument, not a PVector. The second difference is the\nimportant one. We must calculate a custom PVector force for each and every particle and\napply that force. How is that force calculated? In a function called repel(), which is the\ninverse of the attract() function we wrote for the Attractor class.\nNotice how throughout this entire process of adding a repeller to the environment, we’ve\nnever once considered editing the Particle class itself. A particle doesn’t actually have to\nknow anything about the details of its environment; it simply needs to manage its location,\nvelocity, and acceleration, as well as have the ability to receive an external force and act on\nit.\nSo we can now look at this example in its entirety, again leaving out the Particle class,\nwhich hasn’t changed.\nAll the same steps we had to calculate an\nattractive force, only pointing in the\nopposite direction.\nPVector repel(Particle p) {\n1) Get force direction.\nPVector dir =\nPVector.sub(location,p.location);\n2) Get distance (constrain distance).\nfloat d = dir.mag();\nd = constrain(d,5,100);\ndir.normalize();\n3) Calculate magnitude.\nfloat force = -1 * G / (d * d);\n4) Make a vector out of direction and\nmagnitude.\ndir.mult(force);\nreturn dir;\n}\nChapter 4. Particle Systems\n180\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 351
  },
  {
    "chunk_full": "Example 4.7: ParticleSystem with repeller\nOne ParticleSystem\nParticleSystem ps;\nOne repeller\nRepeller repeller;\nvoid setup() {\nsize(640,360);\nps = new ParticleSystem(new PVector(width/2,50));\nrepeller = new Repeller(width/2-20,height/2);\n}\nvoid draw() {\nbackground(100);\nps.addParticle();\nWe’re applying a universal gravity.\nPVector gravity = new PVector(0,0.1);\nps.applyForce(gravity);\nApplying the repeller\nps.applyRepeller(repeller);\nps.run();\nrepeller.display();\n}\nThe ParticleSystem manages all the\nParticles.\nclass ParticleSystem {\nArrayList<Particle> particles;\nPVector origin;\nParticleSystem(PVector location) {\norigin = location.get();\nparticles = new ArrayList<Particle>();\n}\nvoid addParticle() {\nparticles.add(new Particle(origin));\n}\nThe Nature of Code (v1.0)\n181\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 352
  },
  {
    "chunk_full": "Applying a force as a PVector\nvoid applyForce(PVector f) {\nfor (Particle p: particles) {\np.applyForce(f);\n}\n}\nvoid applyRepeller(Repeller r) {\nCalculating a force for each Particle based\non a Repeller\nfor (Particle p: particles) {\nPVector force = r.repel(p);\np.applyForce(force);\n}\n}\nvoid run() {\nIterator<Particle> it = particles.iterator();\nwhile (it.hasNext()) {\nParticle p = (Particle) it.next();\np.run();\nif (p.isDead()) {\nit.remove();\n}\n}\n}\n}\nclass Repeller {\nHow strong is the repeller?\nfloat strength = 100;\nPVector location;\nfloat r = 10;\nRepeller(float x, float y)\n{\nlocation = new PVector(x,y);\n}\nvoid display() {\nstroke(255);\nfill(255);\nellipse(location.x,location.y,r*2,r*2);\n}\nPVector repel(Particle p) {\nThis is the same repel algorithm we used\nin Chapter 2: forces based on gravitational\nattraction.\nPVector dir =\nPVector.sub(location,p.location);\nfloat d = dir.mag();\ndir.normalize();\nd = constrain(d,5,100);\nfloat force = -1 * strength / (d * d);\ndir.mult(force);\nreturn dir;\nChapter 4. Particle Systems\n182\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 353
  },
  {
    "chunk_full": "}\n}\nExpand the above example to include many repellers (using an array or ArrayList).\nExercise 4.9\nExercise 4.9\nCreate a particle system in which each particle responds to every other particle. (Note\nthat we’ll be going through this in detail in Chapter 6.)\nExercise 4.10\nExercise 4.10\n4.13 Image Textures and Additive Blending\n4.13 Image Textures and Additive Blending\nEven though this book is really about behaviors and algorithms rather than computer graphics\nand design, I don’t think we would be able to live with ourselves if we went through a\ndiscussion of particle systems and never once looked at an example that involves texturing\neach particle with an image. The way you choose to draw a particle is a big part of the puzzle\nin terms of designing certain types of visual effects.\nLet’s try to create a smoke simulation in Processing. Take a look at the following two images:\nBoth of these images were generated from identical algorithms. The only difference is that a\nwhite circle is drawn in image A for each particle and a “fuzzy” blob is drawn for each in B.\nWhite circles\nFuzzy images with transparency\nThe Nature of Code (v1.0)\n183\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 354
  },
  {
    "chunk_full": "The good news here is that you get a lot of bang for very little buck. Before you write any\ncode, however, you’ve got to make your image texture! I recommend using PNG format, as\nProcessing will retain the alpha channel (i.e. transparency) when drawing the image, which\nis needed for blending the texture as particles layer on top of each other. Once you’ve\nmade your PNG and deposited it in your sketch’s “data” folder, you are on your way with\njust a few lines of code.\nFirst, we’ll need to declare a PImage object.\nExample 4.8: Image texture particle system\nLoad the image in setup().\nAnd when it comes time to draw the particle, we’ll use the image reference instead of\ndrawing an ellipse or rectangle.\nIncidentally, this smoke example is a nice excuse to revisit the Gaussian number\ndistributions from the Introduction (see page 10). To make the smoke appear a bit more\nrealistic, we don’t want to launch all the particles in a purely random direction. Instead, by\ncreating initial velocity vectors mostly around a mean value (with a lower probability of\noutliers), we’ll get an effect that appears less fountain-like and more like smoke (or fire).\nFigure 4.5\nPImage img;\nvoid setup() {\nLoading the PNG\nimg = loadImage(\"texture.png\");\n}\nvoid render() {\nimageMode(CENTER);\nNote how tint() is the image equivalent of\nshape’s fill().\ntint(255,lifespan);\nimage(img,loc.x,loc.y);\n}\nChapter 4. Particle Systems\n184\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 355
  },
  {
    "chunk_full": "Assuming a Random object called “generator”, we could create initial velocities as follows:\nFinally, in this example, a wind force is applied to the smoke mapped from the mouse’s\nhorizontal location.\nFinally, it’s worth noting that there are many different algorithms for blending colors in\ncomputer graphics. These are often referred to as “blend modes.” By default, when we draw\nsomething on top of something else in Processing, we only see the top layer—this is\ncommonly referred to as a “normal” blend mode. When the pixels have alpha transparency (as\nthey do in the smoke example), Processing uses an alpha compositing algorithm that\ncombines a percentage of the background pixels with the new foreground pixels based on the\nalpha values.\nHowever, it’s possible to draw using other blend modes, and a much loved blend mode for\nparticle systems is “additive.” Additive blending in Processing was pioneered by Robert\nHodgin (http://roberthodgin.com/) in his famous particle system and forces exploration,\nfloat vx = (float) generator.nextGaussian()*0.3;\nfloat vy = (float) generator.nextGaussian()*0.3 - 1.0;\nvel = new PVector(vx,vy);\nvoid draw() {\nbackground(0);\nfloat dx = map(mouseX,0,width,-0.2,0.2);\nWind force points towards mouseX.\nPVector wind = new PVector(dx,0);\nps.applyForce(wind);\nps.run();\nTwo particles are added each cycle through\ndraw().\nfor (int i = 0; i < 2; i++) {\nps.addParticle();\n}\n}\nTry creating your own textures for different types of effects. Can you make it look like\nfire, instead of smoke?\nExercise 4.11\nExercise 4.11\nUse an array of images and assign each Particle object a different image. Even\nthough single images are drawn by multiple particles, make sure you don’t call\nloadImage() any more than you need to, i.e. once for each image file.\nExercise 4.12\nExercise 4.12\nThe Nature of Code (v1.0)\n185\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 356
  },
  {
    "chunk_full": "Magnetosphere, which later became the iTunes visualizer. For more see: Magnetosphere\n(http://roberthodgin.com/magnetosphere-part-2/).\nAdditive blending is in fact one of the simplest blend algorithms and involves adding the\npixel values of one layer to another (capping all values at 255 of course). This results in a\nspace-age glow effect due to the colors getting brighter and brighter with more layers.\nTo achieve additive blending in Processing, you’ll need to use the P2D or P3D renderer.\nExample 4.9: Additive blending\nThen, before you go to draw anything, you set the blend mode using blendMode():\nvoid setup() {\nUsing the P2D renderer\nsize(640,360,P2D);\n}\nvoid draw() {\nAdditive blending\nblendMode(ADD);\nNote that the “glowing” effect of additive\nblending will not work with a white (or very\nbright) background.\nbackground(0);\nAll your other particle stuff would go here.\n}\nUse tint() in combination with additive blending to create a rainbow effect.\nExercise 4.13\nExercise 4.13\nChapter 4. Particle Systems\n186\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 357
  },
  {
    "chunk_full": "Try blending with other modes, such as SUBTRACT, LIGHTEST, DARKEST, DIFFERENCE,\nEXCLUSION,or MULTIPLY.\nExercise 4.14\nExercise 4.14\nThe Nature of Code (v1.0)\n187\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 358
  },
  {
    "chunk_full": "The Ecosystem Project\nThe Ecosystem Project\nStep 4 Exercise:\nTake your creature from Step 3 and build a system of creatures. How can they\ninteract with each other? Can you use inheritance and polymorphism to create a\nvariety of creatures, derived from the same code base? Develop a methodology\nfor how they compete for resources (for example, food). Can you track a\ncreature’s “health” much like we tracked a particle’s lifespan, removing creatures\nwhen appropriate? What rules can you incorporate to control how creatures are\nborn?\n(Also, you might consider using a particle system itself in the design of a creature.\nWhat happens if your emitter is tied to the creature’s location?)\nChapter 4. Particle Systems\n188\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 359
  },
  {
    "chunk_full": "Chapter 5. Physics\nChapter 5. Physics\nLibraries\nLibraries\n“A library implies an act of faith/Which generations still in darkness hid/\nSign in their night in witness of the dawn.”\n— Victor Hugo\nBefore we move on to anything else, let’s revisit some of the things we’ve done in the first\nfour chapters. We have:\n1.\nLearned about concepts from the world of physics — What is a vector? What is a\nforce? What is a wave? etc.\n2.\nUnderstood the math and algorithms behind such concepts.\n3.\nImplemented the algorithms in Processing with an object-oriented approach.\nThese activities have yielded a set of motion simulation examples, allowing us to creatively\ndefine the physics of the worlds we build (whether realistic or fantastical). Of course, we aren’t\nthe first to try this. The world of computer graphics and programming is full of source code\ndedicated to simulation. Just try Googling “open-source physics engine” and you could spend\nthe rest of your day pouring over rich and complex code. And so we must ask the question: If\na code library will take care of physics simulation, why should we bother learning how to write\nany of the algorithms ourselves?\nHere is where the philosophy behind this book comes into play. While many of the libraries\nout there give us physics (and super awesome advanced physics at that) for free, there are\nThe Nature of Code (v1.0)\n189\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 360
  },
  {
    "chunk_full": "significant reasons for learning the fundamentals from scratch before diving into libraries.\nFirst, without an understanding of vectors, forces, and trigonometry, we’d be completely lost\njust reading the documentation of a library. Second, even though a library may take care of\nthe math for us, it won’t necessarily simplify our code. As we’ll see in a moment, there can\nbe a great deal of overhead in simply understanding how a library works and what it\nexpects from you code-wise. Finally, as wonderful as a physics engine might be, if you look\ndeep down into your hearts, it’s likely that you seek to create worlds and visualizations that\nstretch the limits of imagination. A library is great, but it provides a limited set of features.\nIt’s important to know both when to live within limitations in the pursuit of a Processing\nproject and when those limits prove to be confining.\nThis chapter is dedicated to examining two open-source physics libraries—Box2D and\ntoxiclibs’ VerletPhysics engine. With each library, we’ll evaluate its pros and cons and look\nat reasons why you might choose one of these libraries for a given project.\n5.1 What Is Box2D and When Is It Useful?\n5.1 What Is Box2D and When Is It Useful?\nBox2D began as a set of physics tutorials written in C++ by Erin Catto for the Game\nDeveloper’s Conference in 2006. Over the last five years it has evolved into an rich and\nelaborate open-source physics engine. It’s been used for countless projects, most notably\nhighly successful games such as the award-winning puzzle game Crayon Physics and the\nrunaway mobile and tablet hit Angry Birds.\nOne of the key things to realize about Box2D is that it is a true physics engine. Box2D\nknows nothing about computer graphics and the world of pixels; it is simply a library that\ntakes in numbers and spits out more numbers. And what are those numbers? Meters,\nkilograms, seconds, etc. All of Box2D’s measurements and calculations are for real-world\nmeasurements—only its “world” is a two-dimensional plane with top, bottom, left, and right\nedges. You tell it things like: “The gravity of our world is 9.81 newtons per kilogram, and a\ncircle with a radius of four meters and a mass of fifty kilograms is located ten meters above\nthe world’s bottom.” Box2D will then tell you things like: “One second later, the rectangle is\nat five meters from the bottom; two seconds later, it is ten meters below,” etc. While this\nprovides for an amazing and realistic physics engine, it also necessitates lots of complicated\ncode in order to translate back and forth between the physics “world” (a key term in Box2D)\nand the world we want to draw on —the “pixel” world of Processing.\nSo when is it worth it to have this additional overhead? If I just want to simulate a circle\nfalling down a Processing window with gravity, do I really need to write all the extra Box2D\ncode just to get that effect? Certainly, the answer is no. We saw how to do this rather easily\nin the first chapter of this book. Let’s consider another scenario. What if I want to have a\nhundred of those circles falling? And what if those circles aren’t circles at all, but irregularly\nshaped polygons? And what if I want these polygons to bounce off each other in a realistic\nmanner when they collide?\nChapter 5. Physics Libraries\n190\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 361
  },
  {
    "chunk_full": "You may have noticed that the first four chapters of this book, while covering motion and\nforces in detail, has skipped over a rather important aspect of physics simulation—collisions.\nLet’s pretend for a moment that you aren’t reading a chapter about libraries and that we\ndecided right now to cover how to handle collisions in a particle system. We’d have to\nevaluate and learn two distinct algorithms that address these questions:\n1.\nHow do I determine if two shapes are colliding (i.e. intersecting)?\n2.\nHow do I determine the shapes’ velocity after the collision?\nIf we’re thinking about shapes like rectangles or circles, question #1 isn’t too tough. You’ve\nlikely encountered this before. For example, we know two circles are intersecting if the\ndistance between them is less than the sum of their radii.\nOK. Now that we know how to determine if two circles are colliding, how do we calculate their\nvelocities after the collision? This is where we’re going to stop our discussion. Why, you ask?\nIt’s not that understanding the math behind collisions isn’t important or valuable. (In fact, I’m\nincluding additional examples on the website related to collisions without a physics library.)\nThe reason for stopping is that life is short (let this also be a reason for you to consider going\noutside and frolicking instead of programming altogether). We can’t expect to master every\ndetail of physics simulation. And while we could continue this discussion for circles, it’s only\ngoing to lead us to wanting to work with rectangles. And strangely shaped polygons. And\ncurved surfaces. And swinging pendulums colliding with springy springs. And and and and\nand.\nWorking with collisions in our Processing sketch while still having time to spend with our\nfriends and family—that’s the reason for this chapter. Erin Catto spent years developing\nsolutions to these kinds of problems so you don’t need to engineer them yourselves, at least\nfor now.\nFigure 5.1\nThe Nature of Code (v1.0)\n191\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 362
  },
  {
    "chunk_full": "In conclusion, if you find yourself describing an idea for a Processing sketch and the word\n“collisions” comes up, then it’s likely time to learn Box2D. (We’ll also encounter other words\nthat might lead you down this path to Box2D, such as “joint,” “hinge,” “pulley,” “motor,” etc.)\n5.2 Getting Box2D in Processing\n5.2 Getting Box2D in Processing\nSo, if Box2D is a physics engine that knows nothing about pixel-based computer graphics\nand is written in C++, how are we supposed to use it in Processing?\nThe good news is that Box2D is such an amazing and useful library that everyone wants to\nuse it—Flash, Javascript, Python, Ruby programmers. Oh, and Java programmers. There is\nsomething called JBox2D, a Java port of Box2D. And because Processing is built on top of\nJava, JBox2D can be used directly in Processing!\nSo here’s where we are so far.\n•\nBox2D site (http://www.box2d.org/) for reference.\n•\nJBox2D site (http://www.jbox2d.org/) for Processing compatibility.\nThis is all you need to get started writing Box2D code in Processing. However, as we are\ngoing to see in a moment, there are several pieces of functionality we’ll repeatedly need in\nour Processing code, and so it’s worth having one additional layer between our sketches\nand JBox2D. I’m calling this PBox2D—a Processing Box2D “helper” library included as part\nof this book’s code example downloads.\n•\nPBox2D GitHub repository (http://github.com/shiffman/PBox2D)\nIt’s important to realize that PBox2D is not a Processing wrapper for all of Box2D. After all,\nBox2D is a thoughtfully organized and well-structured API and there’s no reason to take it\napart and re-implement it. However, it’s useful to have a small set of functions that help you\nget your Box2D world set up, as well as help you figure out where to draw your Box2D\nshapes. And this is what PBox2D will provide.\nI should also mention before we move forward that there are other Processing libraries that\nwrap Box2D for you. One I would recommend taking a look at is Fisica\n(http://www.ricardmarxer.com/fisica/) by Ricard Marxer.\n5.3 Box2D Basics\n5.3 Box2D Basics\nDo not despair! We really are going to get to the code very soon, and in some ways we’ll\nblow our previous work out of the water. But before we’re ready to do that, it’s important to\nChapter 5. Physics Libraries\n192\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 363
  },
  {
    "chunk_full": "walk through the overall process of using Box2D in Processing. Let’s begin by writing a\npseudocode generalization of all of our examples in Chapters 1 through 4.\nSETUP:\nSETUP:\n1.\nCreate all the objects in our world.\nDRAW:\nDRAW:\n1.\nCalculate all the forces in our world.\n2.\nApply all the forces to our objects (F = M * A).\n3.\nUpdate the locations of all the objects based on their acceleration.\n4.\nDraw all of our objects.\nGreat. Let’s rewrite this pseudocode as it will appear in our Box2D examples.\nSETUP:\nSETUP:\n1.\nCreate all the objects in our world.\nDRAW:\nDRAW:\n1.\nDraw all of our objects.\nThis, of course, is the fantasy of Box2D. We’ve eliminated all of those painful steps of figuring\nout how the objects are moving according to velocity and acceleration. Box2D is going to take\ncare of this for us! The good news is that this does accurately reflect the overall process. Let’s\nimagine Box2D as a magic box.\nIn setup(), we’re going to say to Box2D: “Hello there. Here are all of the things I want in my\nworld.” In draw(), we’re going to politely ask Box2D: “Oh, hello again. If it’s not too much\ntrouble, I’d like to draw all of those things in my world. Could you tell me where they are?”\nThe bad news: it’s not as simple as the above explanation would lead you to believe. For one,\nmaking the stuff that goes in the Box2D world involves wading through the documentation for\nhow different kinds of shapes are built and configured. Second, we have to remember that we\ncan’t tell Box2D anything about pixels, as it will simply get confused and fall apart. Before we\ntell Box2D what we want in our world, we have to convert our pixel units to Box2D “world”\nunits. And the same is true when it comes time to draw our stuff. Box2D is going to tell us the\nlocation of the things in its world, which we then have to translate for the pixel world.\nSETUP\nSETUP\n1.\nCreate everything that lives in our pixel world.\nThe Nature of Code (v1.0)\n193\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 364
  },
  {
    "chunk_full": "2.\nTranslate the pixel world into the Box2D world.\nDRAW\nDRAW\n1.\nAsk Box2D where everything is.\n2.\nTranslate Box2D’s answer into the pixel world.\n3.\nDraw everything.\nNow that we understand that anything we create in our Processing sketch has to be placed\ninto the Box2D world, let’s look at an overview of the elements that make up that world.\nCore elements of a Box2D world:\nCore elements of a Box2D world:\n1.\nWorld\nWorld: Manages the physics simulation. It knows everything about the overall\ncoordinate space and also stores lists of every element in the world (see 2-4\nbelow).\n2.\nBody\nBody: Serves as the primary element in the Box2D world. It has a location. It has a\nvelocity. Sound familiar? The Body is essentially the class we’ve been writing on\nour own in our vectors and forces examples.\n3.\nShape\nShape: Keeps track of all the necessary collision geometry attached to a body.\n4.\nFixture\nFixture: Attaches a shape to a body and sets properties such as density, friction,\nand restitution.\n5.\nJoint\nJoint: Acts as a connection between two bodies (or between one body and the\nworld itself).\nIn the next four sections, we are going to walk through each of the above elements in detail,\nbuilding several examples along the way. But first there is one other important element we\nshould briefly discuss.\n6.\nVec2\nVec2: Describes a vector in the Box2D world.\nAnd so here we are, arriving with trepidation at an unfortunate truth in the world of using\nphysics libraries. Any physics simulation is going to involve the concept of a vector. This is\nthe good part. After all, we just spent several chapters familiarizing ourselves with what it\nmeans to describe motion and forces with vectors. We don’t have to learn anything new\nconceptually.\nNow for the part that makes the single tear fall from my eye: we don’t get to use PVector.\nIt’s nice that Processing has PVector for us, but anytime you use a physics library you will\nprobably discover that the library includes its own vector implementation. This makes sense,\nChapter 5. Physics Libraries\n194\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 365
  },
  {
    "chunk_full": "after all; why should Box2D be expected to know about PVector? And in many cases, the\nphysics engine will want to implement a vector class in a specific way so that it is especially\ncompatible with the rest of the library’s code. So while we don’t have to learn anything new\nconceptually, we do have to get used to some new naming conventions and syntax. Let’s\nquickly demonstrate a few of the basics in Vec2 as compared to those in PVector.\nLet’s say we want to add two vectors together.\nPVector\nPVector\nVec2\nVec2\nPVector a = new PVector(1,-1);\nPVector b = new PVector(3,4);\na.add(b);\nVec2 a = new Vec2(1,-1);\nVec2 b = new Vec2(3,4);\na.addLocal(b);\nPVector a = new PVector(1,-1);\nPVector b = new PVector(3,4);\nPVector c = PVector.add(a,b);\nVec2 a = new Vec2(1,-1);\nVec2 b = new Vec2(3,4);\nVec2 c = a.add(b);\nHow about if we want to multiply and scale them?\nPVector\nPVector\nVec2\nVec2\nPVector a = new PVector(1,-1);\nfloat n = 5;\na.mult(n);\nVec2 a = new Vec2(1,-1);\nfloat n = 5;\na.mulLocal(n);\nPVector a = new PVector(1,-1);\nfloat n = 5;\nPVector c = PVector.mult(a,n);\nVec2 a = new Vec2(1,-1);\nfloat n = 5;\nVec2 c = a.mul(n);\nMagnitude and normalize?\nPVector\nPVector\nVec2\nVec2\nPVector a = new PVector(1,-1);\nfloat m = a.mag();\na.normalize();\nVec2 a = new Vec2(1,-1);\nfloat m = a.length();\na.normalize();\nAs you can see, the concepts are the same, but the function names and the arguments are\nslightly different. For example, instead of static and non-static add() and mult(), if a Vec2 is\naltered, the word “local” is included in the function name—addLocal(), multLocal().\nThe Nature of Code (v1.0)\n195\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 366
  },
  {
    "chunk_full": "We’ll cover the basics of what you need to know here, but if you are looking for more, full\ndocumentation of Vec2 can be found by downloading the JBox2D source code\n(http://code.google.com/p/jbox2d/).\n5.4 Living in a Box2D World\n5.4 Living in a Box2D World\nThe Box2D World object is in charge of everything. It manages the coordinate space of the\nworld, all of the stuff that lives in the world, and decides when time moves forward in the\nworld.\nIn order to have Box2D as part of our Processing sketches, the World is the very first thing\nthat needs to be set up. Here is where PBox2D comes in handy and takes care of making\nthe world for us.\nWhen you call createWorld(), PBox2D will set up a default gravity for you (pointing down);\nhowever, you can always alter the gravity of your world by saying:\nIt’s worth noting that gravity doesn’t have to be fixed, nor does it always have to point\ndownwards; you can adjust the gravity vector while your program is running. Gravity can be\nturned off by setting it to a (0,0) vector.\nSo, what are those numbers 0 and -10? This should remind us of one of the most important\ndetails of using Box2D: the Box2D coordinate system is not your pixel coordinate system!\nLet’s look at how Box2D and a Processing window think differently of their worlds.\nPBox2D box2d;\nvoid setup() {\nbox2d = new PBox2D(this);\nInitializes a Box2D world with default\nsettings\nbox2d.createWorld();\n}\nbox2d.setGravity(0, -10);\nChapter 5. Physics Libraries\n196\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 367
  },
  {
    "chunk_full": "Notice how in Box2D (0,0) is in the center and up is the positive direction along the y-axis!\nBox2D’s coordinate system is just like that lovely old-fashioned Cartesian one with (0,0) in the\ncenter and up pointing in a positive direction. Processing, on the other hand, uses a traditional\ncomputer graphics coordinate system where (0,0) is in the top left corner and down is the\npositive direction along the y-axis. This is why if we want objects to fall down with gravity, we\nneed to give Box2D a gravity force with a negative y-value.\nLuckily for us, if we prefer to think in terms of pixel coordinates (which as Processing\nprogrammers, we are likely to do), PBox2D offers a series of helper functions that convert\nbetween pixel space and Box2D space. Before we move onto the next section and begin\ncreating Box2D bodies, let’s take a look at how these helper functions work.\nLet’s say we want to tell Box2D where the mouse is in its world. We know the mouse is\nlocated at (mouseX,mouseY) in Processing. To convert it, we say we want to convert a\n“coordinate” from “pixels” to “world”—coordPixelsToWorld(). Or:\nWhat if we had a Box2D world coordinate and wanted to translate it to our pixel space?\nFigure 5.2\nVec2 gravity = new Vec2(0, -10);\nConvert mouseX,mouseY to coordinate in\nBox2D world.\nVec2 mouseWorld =\nbox2d.coordPixelsToWorld(mouseX,mouseY);\nTo demonstrate, let’s just make up a world\nposition.\nVec2 worldPos = new Vec2(-10,25);\nConvert to pixel space. This is necessary\nbecause ultimately we are going to want to\ndraw the elements in our window.\nVec2 pixelPos = box2d.coordWorldToPixels(worldPos);\nellipse(pixelPos.x, pixelPos.y,16,16);\nThe Nature of Code (v1.0)\n197\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 368
  },
  {
    "chunk_full": "PBox2D has a set of functions to take care of translating back and forth between the Box2D\nworld and pixels. It’s probably easier to learn about all of these functions during the course\nof actually implementing our examples, but let’s quickly look over the list of the possibilities.\nTask\nTask\nFunction\nFunction\nConvert location from World to Pixels\nVec2 coordWorldToPixels(Vec2 world)\nConvert location from World to Pixels\nVec2 coordWorldToPixels(float worldX,\nfloat worldY)\nConvert location from Pixels to World\nVec2 coordPixelsToWorld(Vec2 screen)\nConvert location from Pixels to World\nVec2 coordPixelsToWorld(float pixelX,\nfloat pixelY)\nScale a dimension (such as height, width, or radius)\nfrom Pixels to World\nfloat scalarPixelsToWorld(float val)\nScale a dimension from World to Pixels\nfloat scalarWorldToPixels(float val)\nThere are also additional functions that allow you to pass or receive a PVector when\ntranslating back and forth, but since we are only working with Box2D in the examples in this\nchapter, it’s easiest to stick with the Vec2 class for all vectors.\nOnce the world is initialized, we are ready to actually put stuff in the world—Box2D bodies.\n5.5 Building a Box2D Body\n5.5 Building a Box2D Body\nA Box2D body is the primary element in the Box2D world. It’s the equivalent to the Mover\nclass we built on our own in previous chapters—the thing that moves around the space and\nexperiences forces. It can also be static (meaning fixed and not moving). It’s important to\nnote, however, that a body has no geometry; it isn’t anything physical. Rather, bodies have\nBox2D shapes attached to them. (This way, a body can be a single rectangle or a rectangle\nattached to a circle, etc.) We’ll look at shapes in a moment; first, let’s build a body.\nStep 1: Define a body.\nStep 1: Define a body.\nThe first thing we have to do is create a “body definition.” This will let us define the\nproperties of the body we intend to make. This may seem a bit awkward at first, but it’s how\nBox2D is structured. Anytime you want to make a “thing,” you have to make a “thing\ndefinition” first. This will hold true for bodies, shapes, and joints.\nChapter 5. Physics Libraries\n198\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 369
  },
  {
    "chunk_full": "Make a body definition before making a\nBody.\nBodyDef bd = new BodyDef();\nStep 2: Configure the body definition.\nStep 2: Configure the body definition.\nThe body definition is where we can set specific properties or attributes of the body we intend\nto make. One attribute of a body, for example, is its starting location. Let’s say we want to\nposition the body in the center of the Processing window.\nDanger, danger! I’m not going to address this with every single example, but it’s important to\nat least point out the perilous path we are taking with the above line of code. Remember, if we\nare going to tell Box2D where we want the body to start, we must give Box2D a world\ncoordinate! Yes, we want to think of its location in terms of pixels, but Box2D doesn’t care.\nAnd so before we pass that position to the body definition, we must make sure to use one of\nour helper conversion functions.\nThe body definition must also specify the “type” of body we want to make. There are three\npossibilities:\n•\nDynamic.\nDynamic. This is what we will use most often—a “fully simulated” body. A dynamic\nbody moves around the world, collides with other bodies, and responds to the forces\nin its environment.\n•\nStatic.\nStatic. A static body is one that cannot move (as if it had an infinite mass). We’ll use\nstatic bodies for fixed platforms and boundaries.\n•\nKinematic.\nKinematic. A kinematic body can be moved manually by setting its velocity directly.\nIf you have a user-controlled object in your world, you can use a kinematic body.\nNote that kinematic bodies collide only with dynamic bodies and not with other static\nor kinematic ones.\nThere are several other properties you can set in the body definition. For example, if you want\nyour body to have a fixed rotation (i.e. never rotate), you can say:\nA Vec2 in the center of the Processing\nwindow\nVec2 center = new Vec2(width/2,height/2);\nA Vec2 in the center of the Processing\nwindow converted to Box2D World\ncoordinates!\nVec2 center =\nbox2d.coordPixelsToWorld(width/2,height/2));\nSetting the position attribute of the Box2D\nbody definition\nbd.position.set(center);\nbd.fixedRotation = true;\nThe Nature of Code (v1.0)\n199\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 370
  },
  {
    "chunk_full": "You can also set a value for linear or angular damping, so that the object continuously slows\nas if there is friction.\nIn addition, fast-moving objects in Box2D should be set as bullets. This tells the Box2D\nengine that the object may move very quickly and to check its collisions more carefully so\nthat it doesn’t accidentally jump over another body.\nbd.linearDamping = 0.8;\nbd.angularDamping = 0.9;\nbd.bullet = true;\nStep 3: Create the body.\nStep 3: Create the body.\nOnce we’re done with the definition (BodyDef), we can create the Body object itself. PBox2D\nprovides a helper function for this—createBody().\nThe Body object is created by passing in\nthe Body Definition. (This allows for making\nmultiple bodies from one definition.)\nBody body = box2d.createBody(bd);\nStep 4: Set any other conditions for the body’s starting state.\nStep 4: Set any other conditions for the body’s starting state.\nFinally, though not required, if you want to set any other initial conditions for the body, such\nas linear or angular velocity, you can do so with the newly created Body object.\nSetting an arbitrary initial velocity\nbody.setLinearVelocity(new Vec2(0,3));\nSetting an arbitrary initial angular velocity\nbody.setAngularVelocity(1.2);\n5.6 Three’s Company: Bodies and Shapes and\n5.6 Three’s Company: Bodies and Shapes and\nFixtures\nFixtures\nA body on its own doesn’t physically exist in the world. It’s like a soul with no human form to\ninhabit. For a body to have mass, we must first define a shape and attach that shape to the\nbody with something known as a fixture.\nThe job of the Box2D Shape class is to keep track of all the necessary collision geometry\nattached to a body. A shape also has several important properties that affect the body’s\nmotion. There is density, which ultimately determines that body’s mass. Shapes also have\nfriction and restitution (“bounciness”) which will be defined through a fixture. One of the\nChapter 5. Physics Libraries\n200\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 371
  },
  {
    "chunk_full": "nice things about Box2D’s methodology, which separates the concepts of bodies and shapes\ninto two separate objects, is that you can attach multiple shapes to a single body in order to\ncreate more complex forms. We’ll see this in a future example.\nTo create a shape, we need to first decide what kind of shape we want to make. For most\nnon-circular shapes, a PolygonShape object will work just fine. For example, let’s look at how\nwe define a rectangle.\nStep 1: Define a shape.\nStep 1: Define a shape.\nNext up, we have to define the width and height of the rectangle. Let’s say we want our\nrectangle to be 150×100 pixels. Remember, pixel units are no good for Box2D shapes! So we\nhave to use our helper functions to convert them first.\nDefine the shape: a polygon.\nPolygonShape ps = new PolygonShape();\nScale dimensions from pixels to Box2D\nworld.\nfloat box2Dw = box2d.scalarPixelsToWorld(150);\nfloat box2Dh = box2d.scalarPixelsToWorld(100);\nUse setAsBox() function to define shape as\na rectangle.\nps.setAsBox(box2Dw, box2Dh);\nStep 2: Create a fixture.\nStep 2: Create a fixture.\nThe shape and body are made as two separate entities. In order to attach a shape to a body,\nwe must make a fixture. A fixture is created, just as with the body, via a fixture definition (i.e.\nFixtureDef class) and assigned a shape.\nOnce we have the fixture definition, we can set parameters that affect the physics for the\nshape being attached.\nFixtureDef fd = new FixtureDef();\nThe fixture is assigned the PolygonShape\nwe just made.\nfd.shape = ps;\nThe coefficient of friction for the shape,\ntypically between 0 and 1\nfd.friction = 0.3;\nThe Shape’s restitution (i.e. elasticity),\ntypically between 0 and 1\nfd.restitution = 0.5;\nThe Shape’s density, measured in kilograms\nper meter squared\nfd.density = 1.0;\nThe Nature of Code (v1.0)\n201\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 372
  },
  {
    "chunk_full": "Step 3: Attach the shape to the body with the fixture.\nStep 3: Attach the shape to the body with the fixture.\nOnce the fixture is defined, all we have left to do is attach the shape to the body with the\nfixture by calling the createFixture() function.\nI should note that Step 2 can be skipped if you do not need to set the physics properties.\n(Box2D will use default values.) You can create a fixture and attach the shape all in one step\nby saying:\nWhile most of our examples will take care of attaching shapes only once when the body is\nfirst built, this is not a limitation of Box2D. Box2D allows for shapes to be created and\ndestroyed on the fly.\nBefore we put any of this code we’ve been writing into a Processing sketch, let’s review all\nthe steps we took to construct a Body.\n1.\nDefine a body using a BodyDef object (set any properties, such as location).\n2.\nCreate the Body object from the body definition.\n3.\nDefine a Shape object using PolygonShape, CircleShape, or any other shape\nclass.\n4.\nDefine a fixture using FixtureDef and assign the fixture a shape (set any\nproperties, such as friction, density, and restitution).\n5.\nAttach the shape to the body.\nCreates the Fixture and attaches the\nShape to the Body object\nbody.createFixture(fd);\nCreates the Fixture and attaches the\nShape with a density of 1\nbody.createFixture(ps,1);\nStep 1. Define the body.\nBodyDef bd = new BodyDef();\nbd.position.set(box2d.coordPixelsToWorld(width/2,height/2));\nStep 2. Create the body.\nBody body = box2d.createBody(bd);\nStep 3. Define the shape.\nPolygonShape ps = new PolygonShape();\nfloat w = box2d.scalarPixelsToWorld(150);\nfloat h = box2d.scalarPixelsToWorld(100);\nps.setAsBox(w, h);\nChapter 5. Physics Libraries\n202\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 373
  },
  {
    "chunk_full": "Step 4. Define the fixture.\nFixtureDef fd = new FixtureDef();\nfd.shape = ps;\nfd.density = 1;\nfd.friction = 0.3;\nfd.restitution = 0.5;\nStep 5. Attach the shape to the body with\nthe Fixture.\nbody.createFixture(fd);\nKnowing what you know about Box2D so far, fill in the blank in the code below that\ndemonstrates how to make a circular shape in Box2D.\nCircleShape cs = new CircleShape();\nfloat radius = 10;\ncs.m_radius = ____________________;\nFixtureDef fd = new FixtureDef();\nfd.shape = cs;\nfd.density = 1;\nfd.friction = 0.1;\nfd.restitution = 0.3;\nbody.createFixture(fd);\nExercise 5.1\nExercise 5.1\n5.7 Box2D and Processing: Reunited and It Feels So\n5.7 Box2D and Processing: Reunited and It Feels So\nGood\nGood\nOnce a body is made, it lives in the Box2D physics world. Box2D will always know it’s there,\ncheck it for collisions, move it appropriately according to the forces, etc. It’ll do all that for you\nwithout you having to lift a finger! What it won’t do, however, is display the body for you. This\nis a good thing. This is your time to shine. When working with Box2D, what we’re essentially\nsaying is, “I want to be the designer of my world, and I want you, Box2D, to compute all the\nphysics.”\nNow, Box2D will keep a list of all the bodies that exist in the world. This can be accessed by\ncalling the World object’s getBodyList() function. Nevertheless, what I’m going to\ndemonstrate here is a technique for keeping your own body lists. Yes, this may be a bit\nredundant and we perhaps sacrifice a bit of efficiency. But we more than make up for that with\nease of use. This methodology will allow us to program like we’re used to in Processing, and\nwe can easily keep track of which bodies are which and render them appropriately. Let’s\nconsider the structure of the following Processing sketch:\nThe Nature of Code (v1.0)\n203\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 374
  },
  {
    "chunk_full": "This looks like any ol’ Processing sketch. We have a main tab called “Boxes” and a\n“Boundary” and a “Box” tab. Let’s think about the Box tab for a moment. The Box tab is\nwhere we will write a simple class to describe a Box object, a rectangular body in our world.\nLet’s write a main tab that creates a new Box whenever the mouse is pressed and stores all\nthe Box objects in an ArrayList. (This is very similar to our approach in the particle system\nexamples from Chapter 4.)\nFigure 5.3\nclass Box\n{\nOur Box object has an x,y location and a\nwidth and a height.\nfloat x,y;\nfloat w,h;\nBox(float x_, float y_) {\nThe location is initalized in the constructor\nvia arguments\nx = x_;\ny = y_;\nw = 16;\nh = 16;\n}\nvoid display() {\nWe draw the Box object using Processing’s\nrect() function.\nfill(175);\nstroke(0);\nrectMode(CENTER);\nrect(x,y,w,h);\n}\n}\nChapter 5. Physics Libraries\n204\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 375
  },
  {
    "chunk_full": "Example 5.1: A comfortable and cozy Processing sketch that needs a little Box2D\nNow, here’s our assignment. Take the above example verbatim, but instead of drawing fixed\nboxes on the screen, draw boxes that experience physics (via Box2D) as soon as they appear.\nWe’ll need two major steps to accomplish our goal.\nA list to store all Box objects\nArrayList<Box> boxes;\nvoid setup() {\nsize(400,300);\nboxes = new ArrayList<Box>();\n}\nvoid draw() {\nbackground(255);\nWhen the mouse is pressed, add a new\nBox object.\nif (mousePressed) {\nBox p = new Box(mouseX,mouseY);\nboxes.add(p);\n}\nDisplay all the Box objects.\nfor (Box b: boxes) {\nb.display();\n}\n}\nStep 1: Add Box2D to our main program (i.e. setup() and draw()).\nStep 1: Add Box2D to our main program (i.e. setup() and draw()).\nThis part is not too tough. We saw this already in our discussion of building a Box2D world.\nThis is taken care of for us by the PBox2D helper class. We can create a PBox2D object and\ninitialize it in setup().\nThen in draw(), we need to make sure we call one very important function: step(). Without\nthis function, nothing would ever happen! step() advances the Box2D world a step further in\ntime. Internally, Box2D sweeps through and looks at all of the Bodies and figures out what to\ndo with them. Just calling step() on its own moves the Box2D world forward with default\nsettings; however, it is customizable (and this is documented in the PBox2D source).\nPBox2D box2d;\nvoid setup() {\nInitialize and create the Box2D world.\nbox2d = new PBox2D(this);\nbox2d.createWorld();\n}\nThe Nature of Code (v1.0)\n205\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 376
  },
  {
    "chunk_full": "void draw() {\nWe must always step through time!\nbox2d.step();\n}\nStep 2: Link every Processing Box object with a Box2D Body object.\nStep 2: Link every Processing Box object with a Box2D Body object.\nAs of this moment, the Box class includes variables for location and width and height. What\nwe now want to say is:\n“I hereby relinquish the command of this object’s position to Box2D. I no longer need to\nkeep track of anything related to location, velocity, and acceleration. Instead, I only need to\nkeep track of a Box2D body and have faith that Box2D will do the rest.”\nWe don’t need (x,y) anymore since, as we’ll see, the body itself will keep track of its\nlocation. The body technically could also keep track of the width and height for us, but since\nBox2D isn’t going to do anything to alter those values over the life of the Box object, we\nmight as well just hold onto them ourselves until it’s time to draw the Box.\nThen, in our constructor, in addition to initializing the width and height, we can go ahead\nand include all of the body and shape code we learned in the previous two sections!\nclass Box\n{\nInstead of any of the usual variables, we\nwill store a reference to a Box2D body.\nBody body;\nfloat w;\nfloat h;\nBox() {\nw = 16;\nh = 16;\nBuild body.\nBodyDef bd = new BodyDef();\nbd.type = BodyType.DYNAMIC;\nbd.position.set(box2d.coordPixelsToWorld(mouseX,mouseY));\nbody = box2d.createBody(bd);\nBuild shape.\nPolygonShape ps = new PolygonShape();\nBox2D considers the width and height of a\nrectangle to be the distance from the\ncenter to the edge (so half of what we\nnormally think of as width or height).\nfloat box2dW = box2d.scalarPixelsToWorld(w/2);\nfloat box2dH = box2d.scalarPixelsToWorld(h/2);\nChapter 5. Physics Libraries\n206\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 377
  },
  {
    "chunk_full": "OK, we’re almost there. Before we introduced Box2D, it was easy to draw the Box. The\nobject’s location was stored in variables x and y.\nBut now Box2D manages the object’s motion, so we can no longer use our own variables to\ndisplay the shape. Not to fear! Our Box object has a reference to the Box2D body associated\nwith it. So all we need to do is politely ask the body, “Pardon me, where are you located?”\nSince this is a task we’ll need to do quite often, PBox2D includes a helper function:\ngetBodyPixelCoord().\nJust knowing the location of a body isn’t enough; we also need to know its angle of rotation.\nOnce we have the location and angle, it’s easy to display the object using translate() and\nrotate(). Note, however, that the Box2D coordinate system considers rotation in the\nopposite direction from Processing, so we need to multiply the angle by -1.\nps.setAsBox(box2dW, box2dH);\nFixtureDef fd = new FixtureDef();\nfd.shape = ps;\nfd.density = 1;\nSet physics parameters.\nfd.friction = 0.3;\nfd.restitution = 0.5;\nAttach the Shape to the Body with the\nFixture.\nbody.createFixture(fd);\n}\nDrawing the object using rect()\nvoid display() {\nfill(175);\nstroke(0);\nrectMode(CENTER);\nrect(x,y,w,h);\n}\nVec2 pos = box2d.getBodyPixelCoord(body);\nfloat a = body.getAngle();\nThe Nature of Code (v1.0)\n207\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 378
  },
  {
    "chunk_full": "In case we want to have objects that can be removed from the Box2D world, it’s also useful\nto include a function to destroy a body, such as:\nFigure 5.4\nvoid display() {\nWe need the Body’s location and angle.\nVec2 pos = box2d.getBodyPixelCoord(body);\nfloat a = body.getAngle();\npushMatrix();\nUsing the Vec2 position and float angle to\ntranslate and rotate the rectangle\ntranslate(pos.x,pos.y);\nrotate(-a);\nfill(175);\nstroke(0);\nrectMode(CENTER);\nrect(0,0,w,h);\npopMatrix();\n}\nThis function removes a body from the\nBox2D world.\nvoid killBody() {\nbox2d.destroyBody(body);\n}\nChapter 5. Physics Libraries\n208\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 379
  },
  {
    "chunk_full": "In this chapter’s code downloads, find the sketch named “box2d_exercise.” Using the\nmethodology outlined in this chapter, add the necessary code to the main and Box tabs\nto implement Box2D physics. The result should appear as in the screenshot above. Be\nmore creative in how you render the boxes.\nExercise 5.2\nExercise 5.2\n5.8 Fixed Box2D Objects\n5.8 Fixed Box2D Objects\nIn the example we just created, the Box objects appear at the mouse location and fall\ndownwards due to Box2D’s default gravity force. What if we wanted to install some immovable\nboundaries in the Box2D world that would block the path of the Box objects (as in the\nillustration below)?\nBox2D makes this easy for us by providing a means to lock a body (and any associated\nshapes) in place. Just set the BodyDef object’s type to STATIC.\nWe can add this feature to our Boxes example by writing a Boundary class and having each\nboundary create a fixed Box2D body.\nBodyDef bd = new BodyDef();\nWhen BodyDef type = STATIC, the Body is\nlocked in place.\nbd.type = BodyType.STATIC;\nThe Nature of Code (v1.0)\n209\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 380
  },
  {
    "chunk_full": "Example 5.2: Falling boxes hitting boundaries\nclass Boundary {\nA boundary is a simple rectangle with x, y,\nwidth, and height.\nfloat x,y;\nfloat w,h;\nBody b;\nBoundary(float x_,float y_, float w_, float h_) {\nx = x_;\ny = y_;\nw = w_;\nh = h_;\nBuild the Box2D Body and Shape.\nBodyDef bd = new BodyDef();\nbd.position.set(box2d.coordPixelsToWorld(x,y));\nMake it fixed by setting type to STATIC!\nbd.type = BodyType.STATIC;\nb = box2d.createBody(bd);\nfloat box2dW = box2d.scalarPixelsToWorld(w/2);\nfloat box2dH = box2d.scalarPixelsToWorld(h/2);\nPolygonShape ps = new PolygonShape();\nThe PolygonShape is just a box.\nps.setAsBox(box2dW, box2dH);\nUsing the createFixture() shortcut\nb.createFixture(ps,1);\n}\nSince we know it can never move, we can\njust draw it the old-fashioned way, using\nour original variables. No need to query\nBox2D.\nvoid display() {\nfill(0);\nstroke(0);\nrectMode(CENTER);\nrect(x,y,w,h);\n}\n}\nChapter 5. Physics Libraries\n210\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 381
  },
  {
    "chunk_full": "5.9 A Curvy Boundary\n5.9 A Curvy Boundary\nIf you want a fixed boundary that is a curved surface (as opposed to a polygon), this can be\nachieved with the shape ChainShape.\nThe ChainShape class is another shape like PolygonShape or CircleShape, so to include one\nin our system, we follow the same steps.\nStep 1: Define a body.\nStep 1: Define a body.\nThe body does not need a position; the\nEdgeShape will take care of that for us. It\nalso does not need a type, as it is STATIC\nby default.\nBodyDef bd = new BodyDef();\nBody body = box2d.world.createBody(bd);\nStep 2: Define the Shape.\nStep 2: Define the Shape.\nChainShape chain = new ChainShape();\nStep 3: Configure the Shape.\nStep 3: Configure the Shape.\nThe ChainShape object is a series of connected vertices. To create the chain, we must first\nspecify an array of vertices (each as a Vec2 object). For example, if we wanted a straight line\nfrom the left-hand side of our window to the right-hand side, we would just need an array of\ntwo vertices: (0,150) and (width,150). (Note that if you want to create a loop where the first\nvertex connects to the last vertex in a loop, you can use the ChainLoop class instead.)\nTo create the chain with the vertices, the array is then passed into a function called\ncreateChain().\nVec2[] vertices = new Vec2[2];\nAdding a vertex on the right side of window\nvertices[0] = box2d.coordPixelsToWorld(0,150);\nAdding a vertex on the left side of window\nvertices[1] = box2d.coordPixelsToWorld(width,150);\nIf you don’t want to use the entire array, you\ncan specify a value less than length.\nchain.createChain(vertices, vertices.length);\nThe Nature of Code (v1.0)\n211\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 382
  },
  {
    "chunk_full": "Step 4: Attach the Shape to the body with a Fixture.\nStep 4: Attach the Shape to the body with a Fixture.\nA Shape is not part of Box2D unless it is attached to a body. Even if it is a fixed boundary\nand never moves, it must still be attached. Just as with other shapes, a ChainShape object\ncan be given properties like restitution and friction with a Fixture.\nNow, if we want to include a ChainShape object in our sketch, we can follow the same\nstrategy as we did with a fixed boundary. Let’s write a class called Surface:\nFixtureDef fd = new FixtureDef();\nA fixture assigned to the ChainShape\nfd.shape = chain;\nfd.density = 1;\nfd.friction = 0.3;\nfd.restitution = 0.5;\nbody.createFixture(fd);\nChapter 5. Physics Libraries\n212\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 383
  },
  {
    "chunk_full": "Example 5.3: ChainShape with three hard-coded vertices\nNotice how the above class includes an ArrayList to store a series of Vec2 objects. Even\nthough we fully intend to store the coordinates of the chain in the chain shape itself, we are\nchoosing the ease of redundancy and keeping our own list of those points as well. Later,\nwhen we go to draw the Surface object, we don’t have to ask Box2D for the locations of the\nchain shape’s vertices.\nclass Surface {\nArrayList<Vec2> surface;\nSurface() {\nsurface = new ArrayList<Vec2>();\n3 vertices in pixel coordinates\nsurface.add(new Vec2(0, height/2+50));\nsurface.add(new Vec2(width/2, height/2+50));\nsurface.add(new Vec2(width, height/2));\nChainShape chain = new ChainShape();\nMake an array of Vec2 for the ChainShape.\nVec2[] vertices = new Vec2[surface.size()];\nfor (int i = 0; i < vertices.length; i++) {\nvertices[i] = box2d.coordPixelsToWorld(surface.get(i));\n}\nConvert each vertex to Box2D World\ncoordinates.\nCreate the ChainShape with array of Vec2.\nchain.createChain(vertices, vertices.length);\nAttach the Shape to the Body.\nBodyDef bd = new BodyDef();\nBody body = box2d.world.createBody(bd);\nbody.createFixture(chain, 1);\n}\nvoid display() {\nstrokeWeight(1);\nstroke(0);\nnoFill();\nDraw the ChainShape as a series of\nvertices.\nbeginShape();\nfor (Vec2 v: surface) {\nvertex(v.x,v.y);\n}\nendShape();\n}\n}\nThe Nature of Code (v1.0)\n213\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 384
  },
  {
    "chunk_full": "What we need in setup() and draw() for the Surface object is quite simple, given that\nBox2D takes care of all of the physics for us.\nPBox2D box2d;\nSurface surface;\nvoid setup() {\nsize(500,300);\nbox2d = new PBox2D(this);\nbox2d.createWorld();\nMake a Surface object.\nsurface = new Surface();\n}\nvoid draw() {\nbox2d.step();\nbackground(255);\nDraw the Surface.\nsurface.display();\n}\nReview how we learned to draw a wave pattern in Chapter 3. Create a ChainShape\nobject out of a sine wave. Try using Perlin noise (see page 17) as well.\nsine wave\nPerlin noise\nExercise 5.3\nExercise 5.3\nChapter 5. Physics Libraries\n214\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 385
  },
  {
    "chunk_full": "5.10 Complex Forms\n5.10 Complex Forms\nNow that we’ve seen how easy it is to make\nsimple geometric forms in Box2D, let’s\nimagine that you want to have a more\ncomplex form, such as a little alien stick\nfigure.\nThere are two strategies in Box2D for\nmaking forms that are more advanced than a\nbasic circle or square. One is to use a\nPolygonShape in a different way. In our\nprevious examples, we used PolygonShape\nto generate a rectangular shape with the\nsetAsBox() function.\nThis was a good way to start because of the inherent simplicity of working with rectangles.\nHowever, a PolygonShape object can also be generated from an array of vectors, which\nallows you to build a completely custom shape as a series of connected vertices. This works\nvery similarly to the ChainShape class.\nExample 5.4: Polygon shapes\nFigure 5.5\nPolygonShape ps = new PolygonShape();\nps.setAsBox(box2dW, box2dH);\nVec2[] vertices = new Vec2[4];\n// An array of 4 vectors\nvertices[0] = box2d.vectorPixelsToWorld(new Vec2(-15, 25));\nvertices[1] = box2d.vectorPixelsToWorld(new Vec2(15, 0));\nvertices[2] = box2d.vectorPixelsToWorld(new Vec2(20, -15));\nvertices[3] = box2d.vectorPixelsToWorld(new Vec2(-10, -10));\nMaking a polygon from that array\nPolygonShape ps = new PolygonShape();\nps.set(vertices, vertices.length);\nThe Nature of Code (v1.0)\n215\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 386
  },
  {
    "chunk_full": "When building your own polygon in Box2D, you must remember two important details.\n1.\nOrder of vertices!\nOrder of vertices! If you are thinking in terms of pixels (as above) the vertices\nshould be defined in counterclockwise order. (When they are translated to Box2D\nWorld vectors, they will actually be in clockwise order since the vertical axis is\nflipped.)\n2.\nConvex shapes only!\nConvex shapes only! A concave shape is one where the surface curves inward.\nConvex is the opposite (see illustration below). Note how in a concave shape\nevery internal angle must be 180 degrees or less. Box2D is not capable of\nhandling collisions for concave shapes. If you need a concave shape, you will have\nto build one out of multiple convex shapes (more about that in a moment).\nFigure 5.6\nChapter 5. Physics Libraries\n216\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 387
  },
  {
    "chunk_full": "Now, when it comes time to display the shape in Processing, we can no longer just use\nrect() or ellipse(). Since the shape is built out of custom vertices, we’ll want to use\nProcessing’s beginShape(), endShape(), and vertex() functions. As we saw with the\nChainShape, we could choose to store the pixel locations of the vertices in our own\nArrayList for drawing. However, it’s also useful to see how we can ask Box2D to report back\nto use the vertex locations.\nFigure 5.7: A concave shape can be drawn with multiple convex shapes.\nvoid display() {\nVec2 pos = box2d.getBodyPixelCoord(body);\nfloat a = body.getAngle();\nFirst we get the Fixture attached to the\nbody...\nFixture f = body.getFixtureList();\n...then the Shape attached to the Fixture.\nPolygonShape ps = (PolygonShape) f.getShape();\nrectMode(CENTER);\npushMatrix();\ntranslate(pos.x,pos.y);\nrotate(-a);\nfill(175);\nstroke(0);\nbeginShape();\nfor (int i = 0; i < ps.getVertexCount(); i++) {\nVec2 v = box2d.vectorWorldToPixels(ps.getVertex(i));\nvertex(v.x,v.y);\n}\nendShape(CLOSE);\npopMatrix();\n}\nWe can loop through that array and convert\neach vertex from Box2D space to pixels.\nThe Nature of Code (v1.0)\n217\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 388
  },
  {
    "chunk_full": "A polygon shape will get us pretty far in Box2D. Nevertheless, the convex shape\nrequirement will severely limit the range of possibilities. The good news is that we can\ncompletely eliminate this restriction by creating a single Box2D body out of multiple shapes!\nLet’s return to our little alien creature and simplify the shape to be a thin rectangle with a\ncircle on top.\nHow can we build a single body with two shapes? Let’s first review how we built a single\nbody with one shape.\nStep 1: Define the body.\nStep 2: Create the body.\nStep 3: Define the shape.\nStep 3: Define the shape.\nStep 4: Attach the shape to the body.\nStep 4: Attach the shape to the body.\nStep 5: Finalize the body’s mass.\nAttaching more than one shape to a body is as simple as repeating steps 3 and 4 over and\nover again.\nStep 3a: Define shape 1.\nStep 3a: Define shape 1.\nStep 4a: Attach shape 1 to the body.\nStep 4a: Attach shape 1 to the body.\nStep 3b: Define shape 2.\nStep 3b: Define shape 2.\nStep 4b: Attach shape 2 to the body.\nStep 4b: Attach shape 2 to the body.\netc. etc. etc.\nLet’s see what this would look like with actual Box2D code.\nUsing the PolygonShape class, create your own polygon design (remember, it must\nbe concave). Some possibilities below.\nExercise 5.4\nExercise 5.4\nMaking the body\nBodyDef bd = new BodyDef();\nbd.type = BodyType.DYNAMIC;\nbd.position.set(box2d.coordPixelsToWorld(center));\nbody = box2d.createBody(bd);\nChapter 5. Physics Libraries\n218\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 389
  },
  {
    "chunk_full": "The above looks pretty good, but sadly, if we run it, we’ll get the following result:\nWhen you attach a shape to a body, by default, the center of the shape will be located at the\ncenter of the body. But in our case, if we take the center of the rectangle to be the center of\nthe body, we want the center of the circle to be offset along the y-axis from the body’s center.\nMaking shape 1 (the rectangle)\nPolygonShape ps = new PolygonShape();\nfloat box2dW = box2d.scalarPixelsToWorld(w/2);\nfloat box2dH = box2d.scalarPixelsToWorld(h/2);\nsd.setAsBox(box2dW, box2dH);\nMaking shape 2 (the circle)\nCircleShape cs = new CircleShape();\ncs.m_radius = box2d.scalarPixelsToWorld(r);\nAttach both shapes with a fixture.\nbody.createFixture(ps,1.0);\nbody.createFixture(cs, 1.0);\nFigure 5.8\nThe Nature of Code (v1.0)\n219\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 390
  },
  {
    "chunk_full": "This is achieved by using the local position of a shape, accessed via a Vec2 variable called\nm_p.\nThen, when we go to draw the body, we use both rect() and ellipse() with the circle\noffset the same way.\nFigure 5.9\nOur offset in pixels\nVec2 offset = new Vec2(0,-h/2);\nConverting the vector to Box2D world\noffset = box2d.vectorPixelsToWorld(offset);\nSetting the local position of the circle\ncircle.m_p.set(offset.x,offset.y);\nChapter 5. Physics Libraries\n220\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 391
  },
  {
    "chunk_full": "Example 5.5: Multiple shapes on one body\nFinishing off this section, I want to stress the following: the stuff you draw in your Processing\nwindow doesn’t magically experience physics simply because we created some Box2D bodies\nand shapes. These examples work because we very carefully matched how we draw our\nelements with how we defined the bodies and shapes we put into the Box2D world. If you\naccidentally draw your shape differently, you won’t get an error, not from Processing or from\nBox2D. However, your sketch will look odd and the physics won’t work correctly. For example,\nwhat if we had written:\nwhen we created the shape, but:\nwhen it came time to display the shape?\nvoid display() {\nVec2 pos = box2d.getBodyPixelCoord(body);\nfloat a = body.getAngle();\nrectMode(CENTER);\npushMatrix();\ntranslate(pos.x,pos.y);\nrotate(-a);\nfill(175);\nstroke(0);\nFirst the rectangle at (0,0)\nrect(0,0,w,h);\nThen the ellipse offset at (0,-h/2)\nellipse(0,-h/2,r*2,r*2);\npopMatrix();\n}\nVec2 offset = new Vec2(0,-h/2);\nellipse(0,h/2,r*2,r*2);\nThe Nature of Code (v1.0)\n221\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 392
  },
  {
    "chunk_full": "The results would look like the image above, where clearly, the collisions are not\nfunctioning as expected. This is not because the physics is broken; it’s because we did not\ncommunicate properly with Box2D, either when we put stuff in the magic world or queried\nthe world for locations.\nMake your own little alien being using multiple shapes attached to a single body. Try\nusing more than one polygon to make a concave shape. Remember, you aren’t\nlimited to using the shape drawing functions in Processing; you can use images,\ncolors, add hair with lines, etc. Think of the Box2D shapes only as skeletons for your\ncreative and fantastical design!\nExercise 5.5\nExercise 5.5\n5.11 Feeling Attached—Box2D Joints\n5.11 Feeling Attached—Box2D Joints\nBox2D joints allow you to connect one\nbody to another, enabling more advanced\nsimulations of swinging pendulums, elastic\nbridges, squishy characters, wheels\nspinning on an axle, etc. There are many\ndifferent kinds of Box2D joints. In this\nchapter we’re going to look at three:\ndistance joints, revolute joints, and\n“mouse” joints.\nLet’s begin with a distance joint, a joint that\nconnects two bodies with a fixed length.\nThe joint is attached to each body at a\nspecified anchor point (a point relative to\nthe body’s center). For any Box2D joint, we\nneed to follow these steps. This, of course,\nis similar to the methodology we used to\nbuild bodies and shapes, with some quirks.\nStep 1. Make sure you have two bodies ready to go.\nStep 1. Make sure you have two bodies ready to go.\nStep 2. Define the joint.\nStep 2. Define the joint.\nStep 3. Configure the joint’s properties (What are the bodies? Where are the anchors?\nStep 3. Configure the joint’s properties (What are the bodies? Where are the anchors?\nWhat is its rest length? Is it elastic or rigid?)\nWhat is its rest length? Is it elastic or rigid?)\nStep 4. Create the joint.\nStep 4. Create the joint.\nLet’s assume we have two Particle objects that each store a reference to a Box2D Body\nobject. We’ll call them particles p1 and p2.\nFigure 5.10\nChapter 5. Physics Libraries\n222\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 393
  },
  {
    "chunk_full": "OK, onto Step 2. Let’s define the joint.\nEasy, right? Now it’s time to configure the joint. First we tell the joint which two bodies it\nconnects:\nThen we set up a rest length. Remember, if our rest length is in pixels, we need to convert it!\nA distance joint also includes two optional settings that can make the joint soft, like a spring\nconnection: frequencyHz and dampingRatio.\nFinally, we create the joint.\nBox2D won’t keep track of what kind of joint we are making, so we have to cast it as a\nDistanceJoint upon creation.\nWe can create Box2D joints anywhere in our Processing sketch. Here’s an example of how we\nmight write a class to describe two Box2D bodies connected with a single joint.\nParticle p1 = new Particle();\nParticle p2 = new Particle();\nDistanceJointDef djd = new DistanceJointDef();\ndjd.bodyA = p1.body;\ndjd.bodyB = p2.body;\ndjd.length = box2d.scalarPixelsToWorld(10);\nMeasured in Hz, like the frequency of\nharmonic oscillation; try values between 1\nand 5.\ndjd.frequencyHz\n= ___;\nDampens the spring; typically a number\nbetween 0 and 1.\ndjd.dampingRatio = ___;\nDistanceJoint dj = (DistanceJoint) box2d.world.createJoint(djd);\nThe Nature of Code (v1.0)\n223\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 394
  },
  {
    "chunk_full": "Example 5.6: DistanceJoint\nclass Pair {\nTwo objects that each have a Box2D body\nParticle p1;\nParticle p2;\nArbitrary rest length\nfloat len = 32;\nPair(float x, float y) {\nProblems can result if the bodies are\ninitialized at the same location.\np1 = new Particle(x,y);\np2 = new Particle(x+random(-1,1),y+random(-1,1));\nMaking the joint!\nDistanceJointDef djd = new DistanceJointDef();\ndjd.bodyA = p1.body;\ndjd.bodyB = p2.body;\ndjd.length = box2d.scalarPixelsToWorld(len);\ndjd.frequencyHz = 0;\n// Try a value less than 5\ndjd.dampingRatio = 0; // Ranges between 0 and 1\nDistanceJoint dj = (DistanceJoint) box2d.world.createJoint(djd);\n}\nvoid display() {\nVec2 pos1 = box2d.getBodyPixelCoord(p1.body);\nVec2 pos2 = box2d.getBodyPixelCoord(p2.body);\nstroke(0);\nline(pos1.x,pos1.y,pos2.x,pos2.y);\np1.display();\np2.display();\n}\n}\nMake the joint. Note that we aren't storing a\nreference to the joint anywhere! We might\nneed to someday, but for now it's OK.\nChapter 5. Physics Libraries\n224\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 395
  },
  {
    "chunk_full": "Another joint you can create in Box2D is a\nrevolute joint. A revolute joint connects two\nBox2D bodies at a common anchor point,\nwhich can also be referred to as a “hinge.”\nThe joint has an “angle” that describes the\nrelative rotation of each body. To use a\nrevolute joint, we follow the same steps we\ndid with the distance joint.\nCreate a simulation of a bridge by using distance joints to connect a sequence of\ncircles (or rectangles) as illustrated to the right. Assign a density of zero to lock the\nendpoints in place. Experiment with different values to make the bridge more or less\n“springy.” It should also be noted that the joints themselves have no physical geometry,\nso in order for your bridge not to have holes, spacing between the nodes will be\nimportant.\nExercise 5.6\nExercise 5.6\nFigure 5.11\nStep 1: Make sure you have two bodies ready to go.\nStep 1: Make sure you have two bodies ready to go.\nLet’s assume we have two Box objects, each of which stores a reference to a Box2D body.\nBox box1 = new Box();\nBox box2 = new Box();\nThe Nature of Code (v1.0)\n225\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 396
  },
  {
    "chunk_full": "Step 2: Define the joint.\nStep 2: Define the joint.\nNow we want a RevoluteJointDef object.\nRevoluteJointDef rjd = new RevoluteJointDef();\nStep 3: Configure the joint’s properties.\nStep 3: Configure the joint’s properties.\nThe most important properties of a revolute joint are the two bodies it connects as well as\ntheir mutual anchor point (i.e. where they are connected). They are set with the function\ninitialize().\nNotice how the first two arguments specify the bodies and the second point specifies the\nanchor, which in this case is located at the center of the first body.\nAn exciting feature of a RevoluteJoint object is that you can motorize it so it spins\nautonomously. For example:\nThe motor can be enabled and disabled while the program is running.\nFinally, the ability for a revolute joint to spin can be constrained between two angles. (By\ndefault, it can rotate a full 360 degrees, or TWO_PI radians.)\nrjd.initialize(box1.body, box2.body, box1.body.getWorldCenter());\nTurn on the motor.\nrjd.enableMotor = true;\nHow fast is the motor?\nrjd.motorSpeed = PI*2;\nHow powerful is the motor?\nrjd.maxMotorTorque = 1000.0;\nrjd.enableLimit = true;\nrjd.lowerAngle = -PI/8;\nrjd.upperAngle = PI/8;\nStep 4: Create the joint.\nStep 4: Create the joint.\nLet’s take a look at all of these steps together in a class called Windmill, which connects\ntwo boxes with a revolute joint. In this case, box1 has a density of zero, so only box2 spins\naround a fixed point.\nRevoluteJoint joint = (RevoluteJoint) box2d.world.createJoint(rjd);\nChapter 5. Physics Libraries\n226\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 397
  },
  {
    "chunk_full": "Example 5.7: Spinning Windmill\nclass Windmill {\nOur “Windmill” is two boxes and one joint.\nRevoluteJoint joint;\nBox box1;\nBox box2;\nWindmill(float x, float y) {\nIn this example, the Box class expects a\nboolean argument that will be used to\ndetermine if the Box is fixed or not. See\nwebsite for the Box class code.\nbox1 = new Box(x,y,120,10,false);\nbox2 = new Box(x,y,10,40,true);\nRevoluteJointDef rjd = new RevoluteJointDef();\nrjd.initialize(box1.body, box2.body, box1.body.getWorldCenter());\nThe joint connects two bodies and is\nanchored at the center of the first body.\nA motor!\nrjd.motorSpeed = PI*2;\nrjd.maxMotorTorque = 1000.0;\nrjd.enableMotor = true;\njoint = (RevoluteJoint) box2d.world.createJoint(rjd);\n}\nCreate the Joint.\nTurning the motor on or off\nvoid toggleMotor() {\nboolean motorstatus = joint.isMotorEnabled();\njoint.enableMotor(!motorstatus);\n}\nThe Nature of Code (v1.0)\n227\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 398
  },
  {
    "chunk_full": "The last joint we’ll look at is a mouse joint. A mouse joint is typically used for moving a body\nwith the mouse. However, it can also be used to drag an object around the screen\naccording to some arbitrary x and y. The joint functions by pulling the body towards a\n“target” position.\nBefore we look at the MouseJoint object itself, let’s ask ourselves why we even need it in\nthe first place. If you look at the Box2D documentation, there is a function called\nsetTransform() that specifically “sets the position of the body’s origin and rotation\n(radians).” If a body has a position, can’t we just assign the body’s position to the mouse?\nWhile this will in fact move the body, it will also have the unfortunate result of breaking the\nphysics. Let’s imagine you built a teleportation machine that allows you to teleport from\nyour bedroom to your kitchen (good for late-night snacking). Now, go ahead and rewrite\nNewton’s laws of motion to account for the possibility of teleportation. Not so easy, right?\nBox2D has the same problem. If you manually assign the location of an body, it’s like saying\n“teleport that body” and Box2D no longer knows how to compute the physics properly.\nvoid display() {\nbox1.display();\nbox2.display();\n}\n}\nUse a revolute joint for the wheels of a\ncar. Use motors so that the car drives\nautonomously. Try using a chain shape\nfor the road’s surface.\nExercise 5.7\nExercise 5.7\nVec2 mouse = box2d.screenToWorld(x,y);\nbody.setTransform(mouse,0);\nChapter 5. Physics Libraries\n228\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 399
  },
  {
    "chunk_full": "However, Box2D does allow you to tie a rope to yourself and get a friend of yours to stand in\nthe kitchen and drag you there. This is what the MouseJoint does. It’s like a string you attach\nto a body and pull towards a target.\nLet’s look at making this joint, assuming we have a Box object called box. This code will look\nidentical to our distance joint with one small difference.\nSo, what’s this line of code all about?\nWell, as we’ve stated, a joint is a connection between two bodies. With a mouse joint, we’re\nsaying that the second body is, well, the ground. Hmm. What the heck is the ground in Box2D?\nOne way to imagine it is to think of the screen as the ground. What we’re doing is making a\njoint that connects a rectangle drawn on the window with the Processing window itself. And\nthe point in the window to which the connection is tied is a moving target.\nOnce we have a mouse joint, we’ll want to update the target location continually while the\nsketch is running.\nTo make this work in an actual Processing sketch, we’ll want to have the following:\n1.\nBox class\nBox class—An object that references a Box2D body.\n2.\nSpring class\nSpring class—An object that manages the mouse joint that drags the Box object\naround.\nJust like before, define the Joint.\nMouseJointDef md = new MouseJointDef();\nWhoa, this is new!\nmd.bodyA = box2d.getGroundBody();\nAttach the Box body.\nmd.bodyB = box.body;\nSet properties.\nmd.maxForce = 5000.0;\nmd.frequencyHz = 5.0;\nmd.dampingRatio = 0.9;\nCreate the joint.\nMouseJoint mouseJoint = (MouseJoint)\nbox2d.world.createJoint(md);.\nmd.bodyA = box2d.getGroundBody();\nVec2 mouseWorld = box2d.coordPixelsToWorld(mouseX,mouseY);\nmouseJoint.setTarget(mouseWorld);\nThe Nature of Code (v1.0)\n229\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 400
  },
  {
    "chunk_full": "3.\nMain tab\nMain tab—Whenever mousePressed() is called, the mouse joint is created;\nwhenever mouseReleased() is called, the mouse joint is destroyed. This allows us\nto interact with a body only when the mouse is pressed.\nLet’s take a look at the main tab. You can find the rest of the code for the Box and Spring\nclasses via the book website.\nExample 5.8: MouseJoint demonstration\nPBox2D box2d;\nOne Box\nBox box;\nObject to manage MouseJoint\nSpring spring;\nvoid setup() {\nsize(400,300);\nbox2d = new PBox2D(this);\nbox2d.createWorld();\nbox = new Box(width/2,height/2);\nThe MouseJoint is really null until we click\nthe mouse.\nspring = new Spring();\n}\nvoid mousePressed() {\nWas the mouse clicked inside the Box?\nif (box.contains(mouseX, mouseY)) {\nIf so, attach the MouseJoint.\nspring.bind(mouseX,mouseY,box);\n}\n}\nvoid mouseReleased() {\nChapter 5. Physics Libraries\n230\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 401
  },
  {
    "chunk_full": "It’s worth noting that while the technique for dragging an object around using a MouseJoint is\nuseful, Box2D also allows a body to have a KINEMATIC type.\nKinematic bodies can be controlled by the\nuser by setting their velocity directly. For\nexample, let’s say you want an object to\nfollow a target (like your mouse). You could\ncreate a vector that points from a body’s\nlocation to a target.\nWhen the mouse is released, we’re done\nwith the MouseJoint.\nspring.destroy();\n}\nvoid draw() {\nbackground(255);\nbox2d.step();\nWe must always update the MouseJoint’s\ntarget.\nspring.update(mouseX,mouseY);\nbox.display();\nspring.display();\n}\nUse a mouse joint to move a Box2D body around the screen according to an algorithm\nor input other than the mouse. For example, assign it a location according to Perlin\nnoise or key presses. Or build your own controller using an Arduino\n(http://www.arduino.cc/).\nExercise 5.8\nExercise 5.8\nBodyDef bd = new BodyDef();\nSetting the body type to Kinematic\nbd.type = BodyType.KINEMATIC;\nFigure 5.12\nVec2 pos = body.getWorldCenter();\nVec2 target = box2d.coordPixelsToWorld(mouseX,mouseY);\nA vector pointing from the body position to\nthe Mouse\nVec2 v = target.sub(pos);\nThe Nature of Code (v1.0)\n231\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 402
  },
  {
    "chunk_full": "Once you have that vector, you could assign it to the body’s velocity so that it moves to the\ntarget.\nYou can also do the same with angular velocity (or leave it alone and allow the physics to\ntake over).\nIt is important to note that kinematic bodies do not collide with other kinematic or static\nbodies. In these cases, the mouse joint strategy is preferable.\nAssigning a body’s velocity directly,\noverriding physics!\nbody.setLinearVelocity(v);\nRedo Exercise 5.8, but use a kinematic body instead.\nExercise 5.9\nExercise 5.9\n5.12 Bringing It All Back Home to Forces\n5.12 Bringing It All Back Home to Forces\nIn Chapter 2, we spent a lot of time thinking about building environments with multiple\nforces. An object might respond to gravitational attraction, wind, air resistance, etc. Clearly\nthere are forces at work in Box2D as we watch rectangles and circles spin and fly around\nthe screen. But so far, we’ve only had the ability to manipulate a single global\nforce—gravity.\nIf we want to use any of our Chapter 2 techniques with Box2D, we need look no further than\nour trusty applyForce() function. In our Mover class we wrote a function called\napplyForce(), which received a vector, divided it by mass, and accumulated it into the\nmover’s acceleration. With Box2D, the same function exists, but we don’t need to write it\nourselves. Instead, we can call the Box2D body’s applyForce() function!\nbox2d = new PBox2D(this);\nbox2d.createWorld();\nSetting the global gravity force\nbox2d.setGravity(0, -20);\nclass Box {\nBody body;\nvoid applyForce(Vec2 force) {\nVec2 pos = body.getWorldCenter();\nCalling the Body's applyForce() function\nbody.applyForce(force, pos);\n}\n}\nChapter 5. Physics Libraries\n232\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 403
  },
  {
    "chunk_full": "Here we are receiving a force vector and passing it along to the Box2D Body object. The key\ndifference is that Box2D is a more sophisticated engine than our examples from Chapter 2.\nOur earlier forces examples assumed that the force was always applied at the mover’s center.\nHere we get to specify exactly where on the body the force is applied. In the above code,\nwe’re just applying it to the center by asking the body for its center, but this could be\nadjusted.\nLet’s say we wanted to use a gravitational attraction force. Remember the code we wrote back\nin Chapter 2 in our Attractor class?\nWe can rewrite the exact same function using Vec2 instead and use it in a Box2D example.\nNote how for our force calculation we can stay completely within the Box2D coordinate\nsystem and never think about pixels.\nPVector attract(Mover m) {\nPVector force = PVector.sub(location,m.location);\nfloat distance = force.mag();\ndistance = constrain(distance,5.0,25.0);\nforce.normalize();\nfloat strength = (g * mass * m.mass) / (distance * distance);\nforce.mult(strength);\nreturn force;\n}\nVec2 attract(Mover m) {\nWe have to ask Box2D for the locations first!\nVec2 pos = body.getWorldCenter();\nVec2 moverPos = m.body.getWorldCenter();\nVec2 force = pos.sub(moverPos);\nfloat distance = force.length();\ndistance = constrain(distance,1,5);\nforce.normalize();\nfloat strength = (G * 1 * m.body.m_mass) / (distance * distance);\nRemember, it’s mulLocal() for Vec2.\nforce.mulLocal(strength);\nreturn force;\n}\nThe Nature of Code (v1.0)\n233\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 404
  },
  {
    "chunk_full": "Take any example you made previously using a force calculation and bring that force\ncalculation into Box2D.\nExercise 5.10\nExercise 5.10\n5.13 Collision Events\n5.13 Collision Events\nNow we’ve seen a survey of what can be done with Box2D. Since this book is not called\n“The Nature of Box2D,” it’s not my intention to cover every single possible feature of the\nBox2D engine. But hopefully by looking at the basics of building bodies, shapes, and joints,\nwhen it comes time to use an aspect of Box2D that we haven’t covered, the skills we’ve\ngained here will make that process considerably less painful. There is one more feature of\nBox2D, however, that I do think is worth covering.\nLet’s ask a question you’ve likely been wondering about:\nWhat if I want something to happen when two Box2D bodies collide? I mean, don’t get me\nwrong—I’m thrilled that Box2D is handling all of the collisions for me. But if it takes care of\neverything for me, how am I supposed to know when things are happening?\nYour first thoughts when considering an event during which two objects collide might be as\nfollows: Well, if I know all the bodies in the system, and I know where they are all located,\nthen I can just start comparing the locations, see which ones are intersecting, and\ndetermine that they’ve collided. That’s a nice thought, but hello??!? The whole point of using\nBox2D is that Box2D will take care of that for us. If we are going to do the geometry to test\nfor intersection ourselves, then all we’re doing is re-implementing Box2D.\nOf course, Box2D has thought of this problem before. It’s a pretty common one. After all, if\nyou intend to make a bajillion dollars selling some game called Angry Birds, you better well\nmake something happen when an ill-tempered pigeon smashes into a cardboard box.\nBox2D alerts you to moments of collision with something called an “interface.” It’s worth\nlearning about interfaces, an advanced feature of object-oriented programming. You can\nChapter 5. Physics Libraries\n234\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 405
  },
  {
    "chunk_full": "take a look at the Java Interface Tutorial (http://download.oracle.com/javase/tutorial/java/\nconcepts/interface.html) as well as the JBox2D ContactListener class. (I have also included\nan example on the website that demonstrates using the interface directly.)\nIf you are using PBox2D, as we are here, you don’t need to implement your own interface.\nDetecting collision events is done through a callback function. Much like mousePressed() is\ntriggered when the mouse is pressed, beginContact() is triggered when two shapes collide.\nBefore the above will work, you must first let PBox2D know you intend to listen for collisions.\n(This allows the library to reduce overhead by default; it won’t bother listening if it doesn’t\nhave to.)\nThere are four collision event callbacks.\n1.\nbeginContact() —Triggered whenever two shapes first come into contact with each\nother.\n2.\nendContact() —Triggered over and over again as long as shapes continue to be in\ncontact.\n3.\npreSolve() —Triggered before Box2D solves the outcome of the collision, i.e.\nbefore beginContact(). It can be used to disable a collision if necessary.\n4.\npostSolve() —Triggered after the outcome of the collision is solved. It allows you\nto gather information about that “solution” (known as an “impulse”).\nThe details behind preSolve() and postSolve() are beyond the scope of this book;\nhowever, we are going to take a close look at beginContact(), which will cover the majority\nof conventional cases in which you want to trigger an action when a collision occurs.\nendContact() works identically to beginContact(), the only difference being that it occurs\nthe moment bodies separate.\nThe mousePressed event with which we are\ncomfortable.\nvoid mousePressed() {\nprintln(\"The mouse was pressed!\");\n}\nWhat our \"beginContact\" event looks like.\nvoid beginContact(Contact cp) {\nprintln(\"Something collided in the Box2D World!\");\n}\nvoid setup() {\nbox2d = new PBox2D(this);\nbox2d.createWorld();\nAdd this line if you want to listen for\ncollisions.\nbox2d.listenForCollisions();\n}\nThe Nature of Code (v1.0)\n235\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 406
  },
  {
    "chunk_full": "beginContact() is written as follows:\nNotice that the function above includes an argument of type Contact. A Contact object\nincludes all the data associated with a collision—the geometry and the forces. Let’s say we\nhave a Processing sketch with Particle objects that store a reference to a Box2D body.\nHere is the process we are going to follow.\nvoid beginContact(Contact cp) {\n}\nStep 1: Contact, could you tell me what two things collided?\nStep 1: Contact, could you tell me what two things collided?\nNow, what has collided here? Is it the bodies? The shapes? The fixtures? Box2D detects\ncollisions between shapes; after all, these are the entities that have geometry. However,\nbecause shapes are attached to bodies with fixtures, what we really want to ask Box2D is:\n“Could you tell me which two fixtures collided?”\nThe contact stores the fixtures as A and B.\nFixture f1 = cp.getFixtureA();\nFixture f2 = cp.getFixtureB();\nStep 2: Fixtures, could you tell me which body you are attached to?\nStep 2: Fixtures, could you tell me which body you are attached to?\ngetBody() gives us the body to which the\nFixture is attached.\nBody b1 = f1.getBody();\nBody b2 = f2.getBody();\nStep 3: Bodies, could you tell me which Particles you are associated\nStep 3: Bodies, could you tell me which Particles you are associated\nwith?\nwith?\nOK, this is the harder part. After all, Box2D doesn’t know anything about our code. Sure, it is\ndoing all sorts of stuff to keep track of the relationships between shapes and bodies and\njoints, but it’s up to us to manage our own objects and their associations with Box2D\nelements. Luckily for us, Box2D provides a function that allows us to attach our Processing\nobject (a Particle) to a Box2D body via the setUserData() and getUserData() methods.\nLet’s take a look at the constructor in our Particle class where the body is made. We are\nexpanding our body-making procedure by one line of code, noted below.\nChapter 5. Physics Libraries\n236\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 407
  },
  {
    "chunk_full": "Later, in our addContact() function, once we know the body, we can access the Particle\nobject with getUserData().\nExample 5.9: CollisionListening\nclass Particle {\nBody body;\nParticle(float x, float y, float r) {\nBodyDef bd = new BodyDef();\nbd.position = box2d.coordPixelsToWorld(x, y);\nbd.type = BodyType.DYNAMIC;\nbody = box2d.createBody(bd);\nCircleShape cs = new CircleShape();\ncs.m_radius = box2d.scalarPixelsToWorld(r);\nbody.createFixture(fd,1);\n\"this\" refers to this Particle object. We are\ntelling the Box2D Body to store a reference\nto this Particle that we can access later.\nbody.setUserData(this);\n}\nvoid beginContact(Contact cp) {\nFixture f1 = cp.getFixtureA();\nFixture f2 = cp.getFixtureB();\nBody b1 = f1.getBody();\nBody b2 = f2.getBody();\nWhen we pull the “user data” object out of\nthe Body object, we have to remind our\nprogram that it is a Particle object. Box2D\ndoesn’t know this.\nParticle p1 = (Particle) b1.getUserData();\nParticle p2 = (Particle) b2.getUserData();\nOnce we have the particles, we can do\nanything to them. Here we just call a\nfunction that changes their color.\np1.change();\np2.change();\n}\nThe Nature of Code (v1.0)\n237\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 408
  },
  {
    "chunk_full": "Now, in many cases, we cannot assume that the objects that collided are all Particle\nobjects. We might have a sketch with Boundary objects, Particle objects, Box objects, etc.\nSo often we will have to query the “user data” and find out what kind of object it is before\nproceeding.\nIt should also be noted that due to how Box2D triggers these callbacks, you cannot create\nor destroy Box2D entities inside of beginContact(), endContact(), preSolve(), or\npostSolve(). If you want to do this, you’ll need to set a variable inside an object\n(something like: markForDeletion = true), which you check during draw() and then\ndelete objects.\nGetting a generic object\nObject o1 = b1.getUserData();\nAsking that object if it’s a Particle\nif (o1.getClass() == Particle.class) {\nParticle p = (Particle) o1;\np.change();\n}\nConsider how polymorphism could help in the above case. Build an example in which\nseveral classes extend one class and therefore eliminate the need for such testing.\nExercise 5.11\nExercise 5.11\nCreate a simulation in which Particle objects disappear when they collide with one\nanother. Use the methodology I just described.\nExercise 5.12\nExercise 5.12\n5.14 A Brief Interlude—Integration Methods\n5.14 A Brief Interlude—Integration Methods\nHas the following ever happened to you? You’re at a fancy cocktail party regaling your\nfriends with tall tales of software physics simulations. Someone pipes up: “Enchanting! But\nwhat integration method are you using?” “What?!” you think to yourself. “Integration?”\nMaybe you’ve heard the term before. Along with “differentiation,” it’s one of the two main\noperations in calculus. Right, calculus. The good news is, we’ve gotten through about 90%\nof the material in this book related to physics simulation and we haven’t really needed to\ndive into calculus. But as we’re coming close to finishing this topic, it’s worth taking a\nmoment to examine the calculus behind what we have been doing and how it relates to the\nmethodology in certain physics libraries (like Box2D and the upcoming toxiclibs).\nChapter 5. Physics Libraries\n238\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 409
  },
  {
    "chunk_full": "Let’s begin by answering the question: “What does integration have to do with location,\nvelocity, and acceleration?” Well, first let’s define differentiation\ndifferentiation, the process of finding a\n“derivative.” The derivative of a function is a measure of how a function changes over time.\nConsider location and its derivative. Location is a point in space, while velocity is change in\nlocation over time. Therefore, velocity can be described as the “derivative” of location. What is\nacceleration? The change in velocity over time—i.e. the “derivative” of velocity.\nNow that we understand the derivative (differentiation), we can define the integral (integration)\nas the inverse of the derivative. In other words, the integral of an object’s velocity over time\ntells us the object’s new location when that time period ends. Location is the integral of\nvelocity, and velocity is the integral of acceleration. Since our physics simulation is founded\nupon the process of calculating acceleration based on forces, we need integration to figure\nout where the object is after a certain period of time (like one frame of animation!)\nSo we’ve been doing integration all along! It looks like this:\nThe above methodology is known as Euler integration (named for the mathematician Leonhard\nEuler, pronounced “Oiler”) or the Euler method. It’s essentially the simplest form of integration\nand very easy to implement in our code (see the two lines above!) However, it is not\nnecessarily the most efficient form, nor is it close to being the most accurate. Why is Euler\ninaccurate? Let’s think about it this way. When you drive a car down the road pressing the gas\npedal with your foot and accelerating, does the car sit in one location at time equals one\nsecond, then disappear and suddenly reappear in a new location at time equals two seconds,\nand do the same thing for three seconds, and four, and five? No, of course not. The car moves\ncontinuously down the road. But what’s happening in our Processing sketch? A circle is at one\nlocation at frame 0, another at frame 1, another at frame 2. Sure, at thirty frames per second,\nwe’re seeing the illusion of motion. But we only calculate a new location every N units of time,\nwhereas the real world is perfectly continuous. This results in some inaccuracies, as shown in\nthe diagram below:\nvelocity.add(acceleration);\nlocation.add(velocity);\nThe Nature of Code (v1.0)\n239\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 410
  },
  {
    "chunk_full": "The “real world” is the curve; Euler simulation is the series of line segments.\nOne option to improve on Euler is to use smaller timesteps—instead of once per frame, we\ncould recalculate an object’s location twenty times per frame. But this isn’t practical; our\nsketch would then run too slowly.\nI still believe that Euler is the best method for learning the basics, and it’s also perfectly\nadequate for most of the projects we might make in Processing. Anything we lose in\nefficiency or inaccuracy we make up in ease of use and understandability. For better\naccuracy, Box2D uses something called symplectic Euler or semi-explicit Euler\n(http://en.wikipedia.org/wiki/Symplectic_Euler_method), a slight modification of Euler.\nThere is also an integration method called Runge-Kutta (named for German mathematicians\nC. Runge and M. W. Kutta), which is used in some physics engines.\nA very popular integration method that our next physics library uses is known as “Verlet\nintegration.” A simple way to describe Verlet integration is to think of our typical motion\nalgorithm without velocity. After all, we don’t really need to store the velocity. If we always\nknow where an object was at one point in time and where it is now, we can extrapolate its\nvelocity. Verlet integration does precisely this, though instead of having a variable for\nvelocity, it calculates velocity while the program is running. Verlet integration is particularly\nwell suited for particle systems, especially particle systems with spring connections\nbetween the particles. We don’t need to worry about the details because toxiclibs, as we’ll\nsee below, takes care of them for us. However, if you are interested, here is the seminal\npaper on Verlet physics, from which just about every Verlet computer graphics simulation is\nderived: \"Advanced Character Physics\" (http://www.gamasutra.com/resource_guide/\n20030121/jacobson_pfv.htm). And of course, you can find out more about Verlet integration\nfrom Wikipedia (http://en.wikipedia.org/wiki/Verlet_integration).\nFigure 5.13\nChapter 5. Physics Libraries\n240\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 411
  },
  {
    "chunk_full": "5.15 Verlet Physics with toxiclibs\n5.15 Verlet Physics with toxiclibs\nFrom toxiclibs.org:\n“toxiclibs is an independent, open source library collection for computational design tasks\nwith Java & Processing developed by Karsten “toxi” Schmidt (thus far). The classes are\npurposefully kept fairly generic in order to maximize re-use in different contexts ranging from\ngenerative design, animation, interaction/interface design, data visualization to architecture\nand digital fabrication, use as teaching tool and more.”\nIn other words, we should thank our lucky stars for toxiclibs. We are only going to focus on a\nfew examples related to Verlet physics, but toxiclibs includes a suite of other wonderful\npackages that help with audio, color, geometry, and more. In particular, if you are looking to\nwork with form and fabrication in Processing, take a look at the geometry package. Demos\ncan be found at Open Processing (http://www.openprocessing.org/portal/?userID=4530).\nWe should note that toxiclibs was designed specifically for use with Processing. This is great\nnews. The trouble we had with making Box2D work in Processing (multiple coordinate\nsystems, Box2D vs. JBox2D vs. PBox2D) is not an issue here. toxiclibs is a library that you just\ndownload, stick in your libraries folder, and use. And the coordinate system that we’ll use for\nthe physics engine is the coordinate system of Processing, so no translating back and forth. In\naddition, toxiclibs is not limited to a 2D world; all of the physics simulations and functions\nwork in both two and three dimensions. So how do you decide which library you should use?\nBox2D or toxiclibs? If you fall into one of the following two categories, your decision is a bit\neasier:\n1. My project involves collisions. I have circles, squares, and other strangely shaped objects\n1. My project involves collisions. I have circles, squares, and other strangely shaped objects\nthat knock each other around and bounce off each other.\nthat knock each other around and bounce off each other.\nIn this case, you are going to need Box2D. toxiclibs does not handle collisions.\n2. My project involves lots of particles flying around the screen. Sometimes they attract\n2. My project involves lots of particles flying around the screen. Sometimes they attract\neach other. Sometimes they repel each other. And sometimes they are connected with\neach other. Sometimes they repel each other. And sometimes they are connected with\nsprings.\nsprings.\nIn this case, toxiclibs is likely your best choice. It is simpler to use than Box2D and particularly\nwell suited to connected systems of particles. toxiclibs is also very high performance, due to\nthe speed of the Verlet integration algorithm (not to mention the fact that the program gets to\nignore all of the collision geometry).\nHere is a little chart that covers some of the features for each physics library.\nThe Nature of Code (v1.0)\n241\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 412
  },
  {
    "chunk_full": "Feature\nFeature\nBox2D\nBox2D\ntoxiclibs VerletPhysics\ntoxiclibs VerletPhysics\nCollision geometry\nYes\nNo\n3D physics\nNo\nYes\nParticle attraction /\nrepulsion forces\nNo\nYes\nSpring connections\nYes\nYes\nOther connections: revolute,\npulley, gear, prismatic\nYes\nNo\nMotors\nYes\nNo\nFriction\nYes\nNo\nGetting toxiclibs\nGetting toxiclibs\nEverything you need to download and install toxiclibs can be found at:\ntoxiclibs (http://toxiclibs.org/)\nWhen you download the library, you’ll notice that it comes with eight modules (i.e. sub-\nfolders), each a library in its own right. For the examples in this chapter, you will only need\n“verletphysics” and “toxiclibscore”; however, I recommend you take a look at and consider\nusing all of the modules!\nOnce you have the library installed to your Processing library folder\n(http://wiki.processing.org/w/How_to_Install_a_Contributed_Library), you are ready to start\nlooking at the following examples.\nCore Elements of VerletPhysics\nCore Elements of VerletPhysics\nWe spent a lot of time working through the core elements of a Box2D world: world, body,\nshape, joint. This gives us a head start on understanding toxiclibs, since it follows a similar\nstructure.\nChapter 5. Physics Libraries\n242\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 413
  },
  {
    "chunk_full": "Box2D\nBox2D\ntoxiclibs VerletPhysics\ntoxiclibs VerletPhysics\nWorld\nVerletPhysics\nBody\nVerletParticle\nShape\nNothing! toxiclibs does not handle shape\ngeometry\nFixture\nNothing! toxiclibs does not handle shape\ngeometry\nJoint\nVerletSpring\nVectors with toxiclibs\nVectors with toxiclibs\nHere we go again. Remember all that time we spent learning the ins and outs of the PVector\nclass? Then remember how when we got to Box2D, we had to translate all those concepts to a\nBox2D vector class: Vec2? Well, it’s time to do it again. toxiclibs also includes its own vector\nclasses, one for two dimensions and one for three: Vec2D and Vec3D.\nAgain, toxiclibs vectors are the same conceptually, but we need to learn a bit of new syntax.\nYou can find all of the documentation for these vector classes here:\nVec2D (http://toxiclibs.org/docs/core/toxi/geom/Vec2D.html)\nVec3D (http://toxiclibs.org/docs/core/toxi/geom/Vec3D.html)\nAnd let’s just review some of the basic vector math operations with PVector translated to\nVec2D (we’re sticking with 2D for simplicity’s sake).\nPVector\nPVector\nVec2D\nVec2D\nPVector a = new PVector(1,-1);\nPVector b = new PVector(3,4);\na.add(b);\nVec2D a = new Vec2D(1,-1);\nVec2D b = new Vec2D(3,4);\na.addSelf(b);\nPVector a = new PVector(1,-1);\nPVector b = new PVector(3,4);\nPVector c = PVector.add(a,b);\nVec2D a = new Vec2D(1,-1);\nVec2D b = new Vec2D(3,4);\nVec2D c = a.add(b);\nPVector a = new PVector(1,-1);\nfloat m = a.mag();\na.normalize();\nVec2D a = new Vec2D(1,-1);\nfloat m = a.magnitude();\na.normalize();\nThe Nature of Code (v1.0)\n243\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 414
  },
  {
    "chunk_full": "Building the toxiclibs physics world\nBuilding the toxiclibs physics world\nThe first thing we need to do to create a toxiclibs physics world in our examples is import\nthe library itself.\nThen we’ll need a reference to our physics world, a VerletPhysics or VerletPhysics2D\nobject (depending on whether we are working in two or three dimensions). The examples in\nthis chapter will operate in 2D only for simplicity, but they could easily be extended into 3D\n(and 3D versions are available with the chapter download).\nOnce you have your VerletPhysics object, you can set some global properties for your\nworld. For example, if you want it to have hard boundaries past which objects cannot travel,\nyou can set its limits:\nIn addition, you can add gravity to the physics world with a GravityBehavior object. A\ngravity behavior requires a vector—how strong and in what direction is the gravity?\nFinally, in order to calculate the physics of the world and move the objects in the world, we\nhave to call update(). Typically this would happen once per frame in draw().\nImporting the libraries\nimport toxi.physics2d.*;\nimport toxi.physics2d.behaviors.*;\nimport toxi.geom.*;\nVerletPhysics2D physics;\nvoid setup() {\nCreating a toxiclibs Verlet physics world\nphysics=new VerletPhysics2D();\nphysics.setWorldBounds(new Rect(0,0,width,height));\nphysics.addBehavior(new GravityBehavior(new Vec2D(0,0.5)));\n}\nvoid draw() {\nThis is the same as Box2D’s “step()”\nfunction\nphysics.update();\n}\n5.16 Particles and Springs in toxiclibs\n5.16 Particles and Springs in toxiclibs\nIn the Box2D examples, we saw how we can create our own class (called, say, Particle)\nand include a reference to a Box2D body.\nChapter 5. Physics Libraries\n244\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 415
  },
  {
    "chunk_full": "This technique is somewhat redundant since Box2D itself keeps track of all of the bodies in its\nworld. However, it allows us to manage which body is which (and therefore how each body is\ndrawn) without having to rely on iterating through Box2D’s internal lists.\nLet’s look at how we might take the same approach with the class VerletParticle2D in\ntoxiclibs. We want to make our own Particle class so that we can draw our particles a certain\nway and include any custom properties. We’d probably write our code as follows:\nLooking at the above, we should first be thrilled to notice that drawing the particle is as simple\nas grabbing the x and y and using them. No awkward conversions between coordinate\nsystems here since toxiclibs is designed to think in pixels. Second, you might notice that this\nParticle class’s sole purpose is to store a reference to a VerletParticle2D object. This\nhints at something. Remember our discussion of inheritance back in Chapter 4: Particle\nSystems? What is a Particle object other than an “augmented” VerletParticle? Why\nbother making a Verlet particle inside a particle when we could simply extend\nVerletParticle?\nclass Particle {\nBody body;\nclass Particle {\nOur Particle has a reference to a\nVerletParticle.\nVerletParticle2D p;\nParticle(Vec2D pos) {\nA VerletParticle needs an initial location (an\nx and y).\np = new VerletParticle2D(pos);\n}\nvoid display() {\nfill(0,150);\nstroke(0);\nWhen it comes time to draw the Particle, we\nask the VerletParticle for its x and y\ncoordinates.\nellipse(p.x,p.y,16,16);\n}\n}\nclass Particle extends VerletParticle2D {\nParticle(Vec2D loc) {\nCalling super() so that the object is\ninitialized properly\nsuper(loc);\n}\nWe want this to be just like a VerletParticle,\nonly with a display() method.\nvoid display() {\nfill(175);\nstroke(0);\nThe Nature of Code (v1.0)\n245\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 416
  },
  {
    "chunk_full": "Remember our multi-step process with the Box2D examples? We had to ask the body for its\nlocation, then convert that location to pixels, then use that location in a drawing function.\nNow, because we have inherited everything from the VerletParticle class, our only step\nis to draw the shape at x and y!\nIncidentally, it’s interesting to note that the VerletParticle2D class is a subclass of Vec2D.\nSo in addition to inheriting everything from VerletParticle2D, our Particle class actually\nhas all of the Vec2D functions available as well.\nWe can now create particles anywhere within our sketch.\nJust making a particle isn’t enough, however. We have to make sure we tell our physics\nworld about them with the addParticle() function.\nIf you look at the toxiclibs documentation, you’ll see that the addParticle() expects a\nVerletParticle2D object.\naddParticle(VerletParticle2D particle)\nAnd how can we then pass into the function our own Particle object? Remember that\nother tenet of object-oriented programming—polymorphism? Here, because our Particle\nclass extends VerletParticle2D, we can choose to treat our particle in two different\nways—as a Particle or as a VerletParticle2D. This is an incredibly powerful feature of\nobject-oriented programming. If we build our custom classes based on classes from\ntoxiclibs, we can use our objects in conjunction with all of the functions toxiclibs has to\noffer.\nIn addition to the VerletParticle class, toxiclibs has a set of classes that allow you to\nconnect particles with spring forces. There are three types of springs in toxiclibs:\n•\nVerletSpring: This class creates a springy connection between two particles in\nspace. A spring’s properties can be configured in such a way as to create a stiff\nstick-like connection or a highly elastic stretchy connection. A particle can also be\nlocked so that only one end of the spring can move.\nWe’ve inherited x and y from VerletParticle!\nellipse(x,y,16,16);\n}\n}\nParticle p1 = new Particle(new Vec2D(100,20));\nParticle p2 = new Particle(new Vec2D(100,180));\nphysics.addParticle(p1);\nphysics.addParticle(p2);\nChapter 5. Physics Libraries\n246\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 417
  },
  {
    "chunk_full": "•\nVerletConstrainedSpring: A VerletConstrainedSpring object is a spring whose\nmaximum distance can be limited. This can help the whole spring system achieve\nbetter stability.\n•\nVerletMinDistanceSpring: A VerletMinDistanceSpring object is a spring that\nonly enforces its rest length if the current distance is less than its rest length. This is\nhandy if you want to ensure objects are at least a certain distance from each other,\nbut don’t care if the distance is bigger than the enforced minimum.\nThe inheritance and polymorphism technique we employed in the previous section also\nproves to be useful when creating springs. A spring expects two particles when it is created.\nAnd again, because our Particle class extends VerletParticle, a VerletSpring object\nwill accept our Particle objects passed into the constructor. Let’s take a look at some\nexample code that assumes the existence of our two previous particles p1 and p2 and creates\na connection between them with a given rest length and strength.\nJust as with particles, in order for the connection to actually be part of the physics world, we\nneed to explicitly add it.\nWhat is the rest length of the spring?\nfloat len = 80;\nHow strong is the spring?\nfloat strength = 0.01;\nVerletSpring2D spring=new VerletSpring2D(p1,p2,len,strength);\nphysics.addSpring(spring);\n5.17 Putting It All Together: A Simple Interactive\n5.17 Putting It All Together: A Simple Interactive\nSpring\nSpring\nOne thing we saw with Box2D is that the physics simulation broke down when we overrode it\nand manually set the location of a body. With toxiclibs, we don’t have this problem. If we want\nto move the location of a particle, we can simply set its x and y location manually. However,\nbefore we do so, it’s generally a good idea to call the lock() function.\nlock() is typically used to lock a particle in place and is identical to setting a Box2D body’s\ndensity to 0. However, here we are going to show how to lock a particle temporarily, move it,\nand then unlock it so that it continues to move according to the physics simulation.  Let’s say\nyou want to move a given particle whenever you click the mouse.\nif (mousePressed) {\nThe Nature of Code (v1.0)\n247\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 418
  },
  {
    "chunk_full": "And now we’re ready to put all of these elements together in a simple example that\nconnects two particles with a spring. One particle is locked in place, and the other can be\nmoved by dragging the mouse. Note that this example is virtually identical to Example 3.11\n(see page 139).\nExample 5.10: Simple Spring with toxiclibs\nFirst lock the particle, then set the x and y,\nthen unlock() it.\np2.lock();\np2.x = mouseX;\np2.y = mouseY;\np2.unlock();\n}\nimport toxi.physics2d.*;\nimport toxi.physics2d.behaviors.*;\nimport toxi.geom.*;\nVerletPhysics2D physics;\nParticle p1;\nParticle p2;\nvoid setup() {\nsize(640,360);\nCreating a physics world\nphysics=new VerletPhysics2D();\nphysics.addBehavior(new GravityBehavior2D(new Vec2D(0,0.5)));\nphysics.setWorldBounds(new Rect(0,0,width,height));\nCreating two Particles\np1 = new Particle(new Vec2D(100,20));\np2 = new Particle(new Vec2D(100,180));\nLocking Particle 1 in place\np1.lock();\nVerletSpring2D spring=new VerletSpring2D(p1,p2,80,0.01);\nCreating one Spring\nChapter 5. Physics Libraries\n248\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 419
  },
  {
    "chunk_full": "Must add everything to the world\nphysics.addParticle(p1);\nphysics.addParticle(p2);\nphysics.addSpring(spring);\n}\nvoid draw() {\nMust update the physics\nphysics.update();\nbackground(255);\nDrawing everything\nline(p1.x,p1.y,p2.x,p2.y);\np1.display();\np2.display();\nif (mousePressed) {\nMoving a Particle according to the mouse\np2.lock();\np2.x = mouseX;\np2.y = mouseY;\np2.unlock();\n}\n}\nHow cute is our simple Particle class?!\nclass Particle extends VerletParticle2D {\nParticle(Vec2D loc) {\nsuper(loc);\n}\nvoid display() {\nfill(175);\nstroke(0);\nellipse(x,y,16,16);\n}\n}\n5.18 Connected Systems, Part I: String\n5.18 Connected Systems, Part I: String\nThe above example, two particles connected with a single spring, is the core building block\nfor what toxiclibs’ physics is particularly well suited for: soft body simulations. For example, a\nstring can be simulated by connecting a line of particles with springs. A blanket can be\nsimulated by connecting a grid of particles with springs. And a cute, cuddly, squishy cartoon\ncharacter can be simulated by a custom layout of particles connected with springs.\nThe Nature of Code (v1.0)\n249\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 420
  },
  {
    "chunk_full": "Let’s begin by simulating a “soft pendulum”—a bob hanging from a string, instead of a rigid\narm like we had in Chapter 3 (see page 131). Let’s use the \"string\" in Figure 5.14 above as\nour model.\nFirst, we’ll need a list of particles (let’s use the same Particle class we built in the previous\nexample).\nNow, let’s say we want to have 20 particles, all spaced 10 pixels apart.\nWe can loop from i equals 0 all the way up to 20, with each particle’s y location set to i *\n10 so that the first particle is at (0,10), the second at (0,20), the third at (0,30), etc.\nFigure 5.14\nArrayList<Particle> particles = new ArrayList<Particle>();\nFigure 5.15\nfloat len = 10;\nfloat numParticles = 20;\nfor(int i=0; i < numPoints; i++) {\nSpacing them out along the x-axis\nParticle particle=new Particle(i*len,10);\nAdd the particle to our list.\nphysics.addParticle(particle);\nChapter 5. Physics Libraries\n250\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 421
  },
  {
    "chunk_full": "Even though it’s a bit redundant, we’re going to add the particle to both the toxiclibs physics\nworld and to our own list. In case we eventually have multiple strings, this will allow us to\nknow which particles are connected to which strings.\nNow for the fun part: It’s time to connect all the particles. Particle 1 will be connected to\nparticle 0, particle 2 to particle 1, 3 to 2, 4 to 3, etc.\nIn other words, particle i needs to be connected to particle i-1 (except for when i equals\nzero).\nNow, what if we want the string to hang from a fixed point? We can lock one of the\nparticles—the first, the last, the middle one, etc. Here’s how we would access the first particle\n(in the ArrayList) and lock it.\nAnd if we want to draw all the particles as being connected with a line, along with a circle for\nthe last particle, we can use beginShape(), endShape(), and vertex(), accessing the\nparticle locations from our ArrayList.\nAdd the particle to the physics world.\nparticles.add(particle);\n}\nFigure 5.16\nif (i != 0) {\nFirst we need a reference to the previous\nparticle.\nParticle previous = particles.get(i-1);\nVerletSpring2D spring = new VerletSpring2D(particle,previous,len,strength);\nThen we make a spring connection between\nthe particle and the previous particle with a\nrest length and strength (both floats).\nWe must not forget to add the spring to the\nphysics world.\nphysics.addSpring(spring);\n}\nParticle head=particles.get(0);\nhead.lock();\nThe Nature of Code (v1.0)\n251\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 422
  },
  {
    "chunk_full": "Example 5.11: Soft swinging pendulum\nThe full code available with the chapter download also demonstrates how to drag the tail\nparticle with the mouse.\nstroke(0);\nnoFill();\nbeginShape();\nfor (Particle p : particles) {\nEach particle is one point in the line.\nvertex(p.x,p.y);\n}\nendShape();\nParticle tail = particles.get(numPoints-1);\nThis draws the last particle as a circle.\ntail.display();\nCreate a hanging cloth simulation using the technique above, but connect all the\nparticles with a grid as demonstrated in the screenshot below.\nExercise 5.13\nExercise 5.13\nChapter 5. Physics Libraries\n252\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 423
  },
  {
    "chunk_full": "5.19 Connected Systems, Part II: Force-Directed\n5.19 Connected Systems, Part II: Force-Directed\nGraph\nGraph\nHave you ever encountered the following scenario?\n“I have a whole bunch of stuff I want to draw on the screen and I want all that stuff to be\nspaced out evenly in a nice, neat, organized manner. Otherwise I have trouble sleeping at\nnight.”\nThis is not an uncommon problem in computational design. One solution is typically referred\nto as a “force-directed graph.” A force-directed graph is a visualization of elements—let’s call\nthem “nodes”—in which the positions of those nodes are not manually assigned. Rather, the\nnodes arrange themselves according to a set of forces. While any forces can be used, a\ntypical example involves spring forces. And so toxiclibs is perfect for this scenario.\nHow do we implement the above?\nFirst, we’ll need a Node class. This is the easy part; it can extend VerletParticle2D. Really,\nthis is just what we did before, only we’re calling it Node now instead of Particle.\nNext we can write a class called Cluster, which will describe a list of nodes.\nclass Node extends VerletParticle2D {\nNode(Vec2D pos) {\nsuper(pos);\n}\nvoid display() {\nfill(0,150);\nstroke(0);\nellipse(x,y,16,16);\n}\n}\nThe Nature of Code (v1.0)\n253\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 424
  },
  {
    "chunk_full": "Let’s assume we added a display() function to draw all the nodes in the cluster and\ncreated a Cluster object in setup() and displayed it in draw(). If we ran the sketch as is,\nnothing would happen. Why? Because we forgot the whole force-directed graph part! We\nneed to connect every single node to every other node with a force. But what exactly do we\nmean by that? Let’s assume we have four Node objects: 0, 1, 2 and 3. Here are our\nconnections:\n0 connected to 1\n0 connected to 2\n0 connected to 3\n1 connected to 2\n1 connected to 3\n2 connected to 3\nNotice two important details about our connection list.\n•\nNo node is connected to itself.\nNo node is connected to itself. We don’t have 0 connected to 0 or 1 connected to\n1.\n•\nWe don’t need to repeat connections in reverse.\nWe don’t need to repeat connections in reverse. In other words, if we’ve already\nsaid 0 is connected to 1, we don’t need to say 1 is connected to 0 because, well, it\nalready is!\nSo how do we write code to make these connections for N number of nodes?\nLook at the left column. It reads: 000 11 22. So we know we need to access each node in\nthe list from 0 to N-1.\nclass Cluster {\nArrayList<Node> nodes;\nWe’ll use this variable for the rest length\nbetween all the nodes.\nfloat diameter;\nCluster(int n, float d, Vec2D center) {\nnodes = new ArrayList<Node>();\ndiameter = d;\nfor (int i = 0; i < n; i++) {\nnodes.add(new Node(center.add(Vec2D.randomVector())));\n}\n}\nHere’s a funny little detail. We’re going to\nhave a problem if all the Node objects start\nin exactly the same location. So we add a\nrandom vector to the center location so that\neach Node is slightly offset.\nfor (int i = 0; i < nodes.size()-1; i++) {\nVerletParticle2D ni = nodes.get(i);\nChapter 5. Physics Libraries\n254\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 425
  },
  {
    "chunk_full": "Now, we know we need to connect node 0 to nodes 1,2,3. For node 1: 2,3. For node 2: 3. So\nfor every node i, we must loop from i+1 until the end of the list.\nWith every two Nodes we find, all we have to do then is make a spring.\nAssuming those connections are made in the Cluster constructor, we can now create a\ncluster in our main tab and see the results!\nExample 5.12: Cluster\nLook how we start j at i + 1.\nfor (int j = i+1; j < nodes.size(); j++) {\nVerletParticle2D nj = nodes.get(j);\nThe Spring connects Nodes “ni” and “nj”.\nphysics.addSpring(new\nVerletSpring2D(ni,nj,diameter,0.01));\n}\n}\nimport toxi.geom.*;\nimport toxi.physics2d.*;\nVerletPhysics2D physics;\nCluster cluster;\nvoid setup() {\nsize(300,300);\nphysics=new VerletPhysics2D();\ncluster = new Cluster(8,100,new Vec2D(width/2,height/2));\n}\nvoid draw() {\nphysics.update();\nbackground(255);\nMake a cluster.\nThe Nature of Code (v1.0)\n255\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 426
  },
  {
    "chunk_full": "Draw the cluster.\ncluster.display();\n}\nUse the Cluster structure as a skeleton for a cute, cuddly, squishy creature (à la\n“Nokia Friends”). Add gravity and also allow the creature to be dragged with the\nmouse.\nExercise 5.14\nExercise 5.14\nExpand the force-directed graph to have more than one Cluster object. Use a\nVerletMinDistanceSpring2D object to connect cluster to cluster.\nExercise 5.15\nExercise 5.15\n5.20 Attraction and Repulsion Behaviors\n5.20 Attraction and Repulsion Behaviors\nWhen we looked at adding an attraction force to Box2D, we found that the Box2D Body\nclass included an applyForce() function. All we needed to do was calculate the attraction\nforce (Force = G * mass1 * mass2 / distance squared) as a vector and apply it to the body.\ntoxiclibs VerletParticle class also includes a function called addForce() that we can use\nto apply any calculated force to a particle.\nHowever, toxiclibs also takes this idea one step further by allowing us to attach some\ncommon forces (let’s call them “behaviors”) to particles, calculating them and applying them\nfor us! For example, if we attach an AttractionBehavior object to a particle, then all other\nparticles in the physics world will be attracted to that particle.\nChapter 5. Physics Libraries\n256\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 427
  },
  {
    "chunk_full": "Let’s say we have a Particle class (that extends VerletParticle).\nOnce we’ve made a Particle object, we can create an AttractionBehavior object\nassociated with that particle.\nNotice how the behavior is created with two parameters—distance and strength. The\ndistance specifies the range within which the behavior will be applied. For example, in the\nabove scenario, only other particles within twenty pixels will feel the attraction force. The\nstrength, of course, specifies how strong the force is.\nFinally, in order for the force to be activated, the behavior needs to be added to the physics\nworld.\nThis means everything that lives in the physics simulation will always be attracted to that\nparticle, as long as it is within the distance threshold.\nEven though toxiclibs does not handle collisions, you can create a collision-like effect by\nadding a repulsive behavior to each and every particle (so that every particle repels every\nother particle). Let’s look at how we might modify our Particle class to do this.\nWe could now recreate our attraction example by having a single Attractor object that\nexerts an attraction behavior over the entire window.\nParticle p = new Particle(new Vec2D(200,200));\nfloat distance = 20;\nfloat strength = 0.1;\nAttractionBehavior behavior = new AttractionBehavior(p, distance, strength);\nphysics.addBehavior(behavior);\nclass Particle extends VerletParticle2D {\nWe’ve added a radius to every Particle.\nfloat r;\nParticle (Vec2D loc) {\nsuper(loc);\nr = 4;\nphysics.addBehavior(new AttractionBehavior(this, r*4, -1));\n}\nvoid display () {\nfill (255);\nstroke (255);\nellipse (x, y, r*2, r*2);\n}\n}\nEvery time a Particle is made, an\nAttractionBehavior is generated and added\nto the physics world. Note that when the\nstrength is negative, it’s a repulsive force!\nThe Nature of Code (v1.0)\n257\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 428
  },
  {
    "chunk_full": "Example 5.13: Attraction/Repulsion\nclass Attractor extends VerletParticle2D {\nfloat r;\nAttractor (Vec2D loc) {\nsuper (loc);\nr = 24;\nphysics.addBehavior(new AttractionBehavior(this, width, 0.1));\n}\nvoid display () {\nfill(0);\nellipse (x, y, r*2, r*2);\n}\n}\nThe AttractionBehavior “distance” equals\nthe width so that it covers the entire\nwindow.\nCreate an object that both attracts and repels. What if it attracts any particle that is far\naway but repels those particles at a short distance?\nExercise 5.16\nExercise 5.16\nUse AttractionBehavior in conjunction with spring forces.\nExercise 5.17\nExercise 5.17\nChapter 5. Physics Libraries\n258\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 429
  },
  {
    "chunk_full": "The Ecosystem Project\nThe Ecosystem Project\nStep 5 Exercise:\nTake your system of creatures from Step 4 and use a physics engine to drive their\nmotion and behaviors. Some possibilities:\n•\nUse Box2D to allow collisions between creatures. Consider triggering\nevents when creatures collide.\n•\nUse Box2D to augment the design of your creatures. Build a skeleton with\ndistance joints or make appendages with revolute joints.\n•\nUse toxiclibs to augment the design of your creature. Use a chain of\ntoxiclibs particles for tentacles or a mesh of springs as a skeleton.\n•\nUse toxiclibs to add attraction and repulsion behaviors to your creatures.\n•\nUse spring (or joint) connections between objects to control their\ninteractions. Create and delete these springs on the fly. Consider making\nthese connections visible or invisible to the viewer.\nThe Nature of Code (v1.0)\n259\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 430
  },
  {
    "chunk_full": "Chapter 6.\nChapter 6.\nAutonomous Agents\nAutonomous Agents\n“This is an exercise in fictional science, or science fiction, if you like that\nbetter.”\n— Valentino Braitenberg\nBelieve it or not, there is a purpose. Well, at least there’s a purpose to the first five chapters\nof this book. We could stop right here; after all, we’ve looked at several different ways of\nmodeling motion and simulating physics. Angry Birds, here we come!\nStill, let’s think for a moment. Why are we here? The nature of code, right? What have we\nbeen designing so far? Inanimate objects. Lifeless shapes sitting on our screens that flop\naround when affected by forces in their environment. What if we could breathe life into\nthose shapes? What if those shapes could live by their own rules? Can shapes have hopes\nand dreams and fears? This is what we are here in this chapter to do—develop autonomous\nagents.\n6.1 Forces from Within\n6.1 Forces from Within\nThe term autonomous agent\nautonomous agent generally refers to an entity that makes its own choices about\nhow to act in its environment without any influence from a leader or global plan. For us,\n“acting” will mean moving. This addition is a significant conceptual leap. Instead of a box\nsitting on a boundary waiting to be pushed by another falling box, we are now going to\nChapter 6. Autonomous Agents\n260\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 431
  },
  {
    "chunk_full": "design a box that has the ability and “desire” to leap out of the way of that other falling box, if\nit so chooses. While the concept of forces that come from within is a major shift in our design\nthinking, our code base will barely change, as these desires and actions are simply\nthat—forces.\nHere are three key components of autonomous agents that we’ll want to keep in mind as we\nbuild our examples.\n•\nAn autonomous agent has a\nAn autonomous agent has a limited\nlimited ability to perceive environment.\nability to perceive environment. It makes\nsense that a living, breathing being should have an awareness of its environment.\nWhat does this mean for us, however? As we look at examples in this chapter, we\nwill point out programming techniques for allowing objects to store references to\nother objects and therefore “perceive” their environment. It’s also crucial that we\nconsider the word limited here. Are we designing an all-knowing rectangle that flies\naround a Processing window, aware of everything else in that window? Or are we\ncreating a shape that can only examine any other object within fifteen pixels of\nitself? Of course, there is no right answer to this question; it all depends. We’ll\nexplore some possibilities as we move forward. For a simulation to feel more\n“natural,” however, limitations are a good thing. An insect, for example, may only be\naware of the sights and smells that immediately surround it. For a real-world\ncreature, we could study the exact science of these limitations. Luckily for us, we\ncan just make stuff up and try it out.\n•\nAn autonomous agent processes the information from its environment and\nAn autonomous agent processes the information from its environment and\ncalculates an action.\ncalculates an action. This will be the easy part for us, as the action is a force. The\nenvironment might tell the agent that there’s a big scary-looking shark swimming\nright at it, and the action will be a powerful force in the opposite direction.\n•\nAn autonomous agent has no leader.\nAn autonomous agent has no leader. This third principle is something we care a\nlittle less about. After all, if you are designing a system where it makes sense to\nhave a leader barking commands at various entities, then that’s what you’ll want to\nimplement. Nevertheless, many of these examples will have no leader for an\nimportant reason. As we get to the end of this chapter and examine group\nbehaviors, we will look at designing collections of autonomous agents that exhibit\nthe properties of complex systems— intelligent and structured group dynamics that\nemerge not from a leader, but from the local interactions of the elements\nthemselves.\nIn the late 1980s, computer scientist Craig Reynolds (http://www.red3d.com/cwr/) developed\nalgorithmic steering behaviors for animated characters. These behaviors allowed individual\nelements to navigate their digital environments in a “lifelike” manner with strategies for\nfleeing, wandering, arriving, pursuing, evading, etc. Used in the case of a single autonomous\nagent, these behaviors are fairly simple to understand and implement. In addition, by building\na system of multiple characters that steer themselves according to simple, locally based rules,\nsurprising levels of complexity emerge. The most famous example is Reynolds’s “boids”\nmodel for “flocking/swarming” behavior.\nThe Nature of Code (v1.0)\n261\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 432
  },
  {
    "chunk_full": "6.2 Vehicles and Steering\n6.2 Vehicles and Steering\nNow that we understand the core concepts behind autonomous agents, we can begin\nwriting the code. There are many places where we could start. Artificial simulations of ant\nand termite colonies are fantastic demonstrations of systems of autonomous agents. (For\nmore on this topic, I encourage you to read Turtles, Termites, and Traffic Jams by Mitchel\nResnick.) However, we want to begin by examining agent behaviors that build on the work\nwe’ve done in the first five chapters of this book: modeling motion with vectors and driving\nmotion with forces. And so it’s time to rename our Mover class that became our Particle\nclass once again. This time we are going to call it Vehicle.\nIn his 1999 paper “Steering Behaviors for Autonomous Characters,” Reynolds uses the word\n“vehicle” to describe his autonomous agents, so we will follow suit.\nWhy Vehicle?\nWhy Vehicle?\nIn 1986, Italian neuroscientist and cyberneticist Valentino Braitenberg described a\nseries of hypothetical vehicles with simple internal structures in his book Vehicles:\nExperiments in Synthetic Psychology. Braitenberg argues that his extraordinarily\nsimple mechanical vehicles manifest behaviors such as fear, aggression, love,\nforesight, and optimism. Reynolds took his inspiration from Braitenberg, and we’ll\ntake ours from Reynolds.\nReynolds describes the motion of idealized vehicles (idealized because we are not\nconcerned with the actual engineering of such vehicles, but simply assume that they exist\nand will respond to our rules) as a series of three layers—Action Selection\nAction Selection, Steering\nSteering, and\nLocomotion\nLocomotion.\n1.\nAction Selection.\nAction Selection. A vehicle has a goal (or goals) and can select an action (or a\ncombination of actions) based on that goal. This is essentially where we left off\nwith autonomous agents. The vehicle takes a look at its environment and\ncalculates an action based on a desire: “I see a zombie marching towards me.\nSince I don’t want my brains to be eaten, I’m going to flee from the zombie.” The\ngoal is to keep one’s brains and the action is to flee. Reynolds’s paper describes\nmany goals and associated actions such as: seek a target, avoid an obstacle, and\nclass Vehicle {\nPVector location;\nPVector velocity;\nPVector acceleration;\n// What else do we need to add?\nChapter 6. Autonomous Agents\n262\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 433
  },
  {
    "chunk_full": "follow a path. In a moment, we’ll start building these examples out with Processing\ncode.\n2.\nSteering.\nSteering. Once an action has been selected, the vehicle has to calculate its next\nmove. For us, the next move will be a force; more specifically, a steering force.\nLuckily, Reynolds has developed a simple steering force formula that we’ll use\nthroughout the examples in this chapter: steering force = desired velocity - current\nsteering force = desired velocity - current\nvelocity\nvelocity. We’ll get into the details of this formula and why it works so effectively in\nthe next section.\n3.\nLocomotion.\nLocomotion. For the most part, we’re going to ignore this third layer. In the case of\nfleeing zombies, the locomotion could be described as “left foot, right foot, left foot,\nright foot, as fast as you can.” In our Processing world, however, a rectangle or circle\nor triangle’s actual movement across a window is irrelevant given that it’s all an\nillusion in the first place. Nevertheless, this isn’t to say that you should ignore\nlocomotion entirely. You will find great value in thinking about the locomotive design\nof your vehicle and how you choose to animate it. The examples in this chapter will\nremain visually bare, and a good exercise would be to elaborate on the animation\nstyle —could you add spinning wheels or oscillating paddles or shuffling legs?\nUltimately, the most important layer for you to consider is #1—Action Selection. What are the\nelements of your system and what are their goals? In this chapter, we are going to look at a\nseries of steering behaviors (i.e. actions): seek, flee, follow a path, follow a flow field, flock\nwith your neighbors, etc. It’s important to realize, however, that the point of understanding\nhow to write the code for these behaviors is not because you should use them in all of your\nprojects. Rather, these are a set of building blocks, a foundation from which you can design\nand develop vehicles with creative goals and new and exciting behaviors. And even though\nwe will think literally in this chapter (follow that pixel!), you should allow yourself to think more\nabstractly (like Braitenberg). What would it mean for your vehicle to have “love” or “fear” as its\ngoal, its driving force? Finally (and we’ll address this later in the chapter), you won’t get very\nfar by developing simulations with only one action. Yes, our first example will be “seek a\ntarget.” But for you to be creative—to make these steering behaviors your own—it will all\ncome down to mixing and matching multiple actions within the same vehicle. So view these\nexamples not as singular behaviors to be emulated, but as pieces of a larger puzzle that you\nwill eventually assemble.\n6.3 The Steering Force\n6.3 The Steering Force\nWe can entertain ourselves by discussing the theoretical principles behind autonomous\nagents and steering as much as we like, but we can’t get anywhere without first\nunderstanding the concept of a steering force. Consider the following scenario. A vehicle\nmoving with velocity desires to seek a target.\nThe Nature of Code (v1.0)\n263\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 434
  },
  {
    "chunk_full": "Its goal and subsequent action is to seek\nthe target in Figure 6.1. If you think back to\nChapter 2, you might begin by making the\ntarget an attractor and apply a gravitational\nforce that pulls the vehicle to the target.\nThis would be a perfectly reasonable\nsolution, but conceptually it’s not what\nwe’re looking for here. We don’t want to\nsimply calculate a force that pushes the\nvehicle towards its target; rather, we are\nasking the vehicle to make an intelligent\ndecision to steer towards the target based\non its perception of its state and\nenvironment (i.e. how fast and in what\ndirection is it currently moving). The vehicle should look at how it desires to move (a vector\npointing to the target), compare that goal with how quickly it is currently moving (its\nvelocity), and apply a force accordingly.\nsteering force = desired velocity - current velocity\nsteering force = desired velocity - current velocity\nOr as we might write in Processing:\nIn the above formula, velocity is no problem. After all, we’ve got a variable for that.\nHowever, we don’t have the desired velocity; this is something we have to calculate. Let’s\ntake a look at Figure 6.2. If we’ve defined the vehicle’s goal as “seeking the target,” then its\ndesired velocity is a vector that points from its current location to the target location.\nAssuming a PVector target, we then have:\nFigure 6.1\nPVector steer = PVector.sub(desired,velocity);\nFigure 6.2\nPVector desired = PVector.sub(target,location);\nChapter 6. Autonomous Agents\n264\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 435
  },
  {
    "chunk_full": "But this isn’t particularly realistic. What if we have a very high-resolution window and the\ntarget is thousands of pixels away? Sure, the vehicle might desire to teleport itself instantly to\nthe target location with a massive velocity, but this won’t make for an effective animation.\nWhat we really want to say is:\nThe vehicle desires to move towards the target at maximum speed.\nIn other words, the vector should point from location to target and with a magnitude equal to\nmaximum speed (i.e. the fastest the vehicle can go). So first, we need to make sure we add a\nvariable to our Vehicle class that stores maximum speed.\nThen, in our desired velocity calculation, we scale according to maximum speed.\nPutting this all together, we can write a function called seek() that receives a PVector target\nand calculates a steering force towards that target.\nclass Vehicle {\nPVector location;\nPVector velocity;\nPVector acceleration;\nMaximum speed\nfloat maxspeed;\nPVector desired = PVector.sub(target,location);\ndesired.normalize();\ndesired.mult(maxspeed);\nFigure 6.3\nvoid seek(PVector target) {\nPVector desired = PVector.sub(target,location);\ndesired.normalize();\nCalculating the desired velocity to target at\nmax speed\ndesired.mult(maxspeed);\nReynolds’s formula for steering force\nPVector steer = PVector.sub(desired,velocity);\nThe Nature of Code (v1.0)\n265\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 436
  },
  {
    "chunk_full": "Note how in the above function we finish by passing the steering force into applyForce().\nThis assumes that we are basing this example on the foundation we built in Chapter 2 (see\npage 66). However, you could just as easily use the steering force with Box2D’s\napplyForce() function or toxiclibs’ addForce() function.\nSo why does this all work so well? Let’s see what the steering force looks like relative to the\nvehicle and target locations.\nAgain, notice how this is not at all the same force as gravitational attraction. Remember one\nof our principles of autonomous agents: An autonomous agent has a limited ability to\nperceive its environment. Here is that ability, subtly embedded into Reynolds’s steering\nformula. If the vehicle weren’t moving at all (zero velocity), desired minus velocity would be\nequal to desired. But this is not the case. The vehicle is aware of its own velocity and its\nsteering force compensates accordingly. This creates a more active simulation, as the way\nin which the vehicle moves towards the targets depends on the way it is moving in the first\nplace.\nIn all of this excitement, however, we’ve missed one last step. What sort of vehicle is this? Is\nit a super sleek race car with amazing handling? Or a giant Mack truck that needs a lot of\nadvance notice to turn? A graceful panda, or a lumbering elephant? Our example code, as it\nstands, has no feature to account for this variability in steering ability. Steering ability can\nbe controlled by limiting the magnitude of the steering force. Let’s call that limit the\n“maximum force” (or maxforce for short). And so finally, we have:\nUsing our physics model and applying the\nforce to the object’s acceleration\napplyForce(steer);\n}\nFigure 6.4\nclass Vehicle {\nPVector location;\nPVector velocity;\nPVector acceleration;\nChapter 6. Autonomous Agents\n266\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 437
  },
  {
    "chunk_full": "followed by:\nLimiting the steering force brings up an important point. We must always remember that it’s\nnot actually our goal to get the vehicle to the target as fast as possible. If that were the case,\nwe would just say “location equals target” and there the vehicle would be. Our goal, as\nReynolds puts it, is to move the vehicle in a “lifelike and improvisational manner.” We’re trying\nto make it appear as if the vehicle is steering its way to the target, and so it’s up to us to play\nwith the forces and variables of the system to simulate a given behavior. For example, a large\nmaximum steering force would result in a very different path than a small one. One is not\ninherently better or worse than the other; it depends on your desired effect. (And of course,\nthese values need not be fixed and could change based on other conditions. Perhaps a\nvehicle has health: the higher the health, the better it can steer.)\nHere is the full Vehicle class, incorporating the rest of the elements from the Chapter 2\nMover object.\nMaximum speed\nfloat maxspeed;\nNow we also have maximum force.\nfloat maxforce;\nvoid seek(PVector target) {\nPVector desired = PVector.sub(target,location);\ndesired.normalize();\ndesired.mult(maxspeed);\nPVector steer = PVector.sub(desired,velocity);\nLimit the magnitude of the steering force.\nsteer.limit(maxforce);\napplyForce(steer);\n}\nFigure 6.5\nThe Nature of Code (v1.0)\n267\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 438
  },
  {
    "chunk_full": "Example 6.1: Seeking a target\nclass Vehicle {\nPVector location;\nPVector velocity;\nPVector acceleration;\nAdditional variable for size\nfloat r;\nfloat maxforce;\nfloat maxspeed;\nVehicle(float x, float y) {\nacceleration = new PVector(0,0);\nvelocity = new PVector(0,0);\nlocation = new PVector(x,y);\nr = 3.0;\nArbitrary values for maxspeed and force;\ntry varying these!\nmaxspeed = 4;\nmaxforce = 0.1;\n}\nOur standard “Euler integration” motion\nmodel\nvoid update() {\nvelocity.add(acceleration);\nvelocity.limit(maxspeed);\nlocation.add(velocity);\nacceleration.mult(0);\n}\nNewton’s second law; we could divide by\nmass if we wanted.\nvoid applyForce(PVector force) {\nacceleration.add(force);\n}\nChapter 6. Autonomous Agents\n268\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 439
  },
  {
    "chunk_full": "Our seek steering force algorithm\nvoid seek(PVector target) {\nPVector desired = PVector.sub(target,location);\ndesired.normalize();\ndesired.mult(maxspeed);\nPVector steer = PVector.sub(desired,velocity);\nsteer.limit(maxforce);\napplyForce(steer);\n}\nvoid display() {\nVehicle is a triangle pointing in the direction\nof velocity; since it is drawn pointing up, we\nrotate it an additional 90 degrees.\nfloat theta = velocity.heading() + PI/2;\nfill(175);\nstroke(0);\npushMatrix();\ntranslate(location.x,location.y);\nrotate(theta);\nbeginShape();\nvertex(0, -r*2);\nvertex(-r, r*2);\nvertex(r, r*2);\nendShape(CLOSE);\npopMatrix();\n}\nImplement a “fleeing” steering behavior (desired vector is inverse of “seek”).\nExercise 6.1\nExercise 6.1\nImplement seeking a moving target, often referred to as “pursuit.” In this case, your\ndesired vector won’t point towards the object’s current location, but rather its “future”\nlocation as extrapolated from its current velocity. We’ll see this ability for a vehicle to\n“predict the future” in later examples.\nExercise 6.2\nExercise 6.2\nCreate a sketch where a vehicle’s maximum force and maximum speed do not remain\nconstant, but rather vary according to environmental factors.\nExercise 6.3\nExercise 6.3\nThe Nature of Code (v1.0)\n269\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 440
  },
  {
    "chunk_full": "6.4 Arriving Behavior\n6.4 Arriving Behavior\nAfter working for a bit with the seeking behavior, you probably are asking yourself, “What if\nI want my vehicle to slow down as it approaches the target?” Before we can even begin to\nanswer this question, we should look at the reasons behind why the seek behavior causes\nthe vehicle to fly past the target so that it has to turn around and go back. Let’s consider the\nbrain of a seeking vehicle. What is it thinking?\nFrame 1: I want to go as fast as possible towards the target!\nFrame 2: I want to go as fast as possible towards the target!\nFrame 3: I want to go as fast as possible towards the target!\nFrame 4: I want to go as fast as possible towards the target!\nFrame 5: I want to go as fast as possible towards the target!\netc.\nThe vehicle is so gosh darn excited about getting to the target that it doesn’t bother to\nmake any intelligent decisions about its speed relative to the target’s proximity. Whether it’s\nfar away or very close, it always wants to go as fast as possible.\nIn some cases, this is the desired behavior (if a missile is flying at a target, it should always\ntravel at maximum speed.) However, in many other cases (a car pulling into a parking spot, a\nbee landing on a flower), the vehicle’s thought process needs to consider its speed relative\nto the distance from its target. For example:\nFrame 1: I’m very far away. I want to go as fast as possible towards the target!\nFrame 2: I’m very far away. I want to go as fast as possible towards the target!\nFrame 3: I’m somewhat far away. I want to go as fast as possible towards the target!\nFrame 4: I’m getting close. I want to go more slowly towards the target!\nFrame 5: I’m almost there. I want to go very slowly towards the target!\nFrame 6: I’m there. I want to stop!\nHow can we implement this “arriving” behavior in code? Let’s return to our seek() function\nand find the line of code where we set the magnitude of the desired velocity.\nFigure 6.6\nFigure 6.7\nChapter 6. Autonomous Agents\n270\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 441
  },
  {
    "chunk_full": "In Example 6.1, the magnitude of the desired vector is always “maximum” speed.\nWhat if we instead said the desired velocity is equal to half the distance?\nWhile this nicely demonstrates our goal of a desired speed tied to our distance from the\ntarget, it’s not particularly reasonable. After all, 10 pixels away is rather close and a desired\nspeed of 5 is rather large. Something like a desired velocity with a magnitude of 5% of the\ndistance would work much better.\nPVector desired = PVector.sub(target,location);\ndesired.normalize();\ndesired.mult(maxspeed);\nFigure 6.8\nFigure 6.9\nPVector desired = PVector.sub(target,location);\ndesired.div(2);\nThe Nature of Code (v1.0)\n271\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 442
  },
  {
    "chunk_full": "Reynolds describes a more sophisticated approach. Let’s imagine a circle around the target\nwith a given radius. If the vehicle is within that circle, it slows down—at the edge of the\ncircle, its desired speed is maximum speed, and at the target itself, its desired speed is 0.\nIn other words, if the distance from the target is less than r, the desired speed is between 0\nand maximum speed mapped according to that distance.\nExample 6.2: Arrive steering behavior\nPVector desired = PVector.sub(target,location);\ndesired.mult(0.05);\nFigure 6.10\nvoid arrive(PVector target) {\nPVector desired = PVector.sub(target,location);\nThe distance is the magnitude of the vector\npointing from location to target.\nfloat d = desired.mag();\ndesired.normalize();\nChapter 6. Autonomous Agents\n272\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 443
  },
  {
    "chunk_full": "The arrive behavior is a great demonstration of the magic of “desired minus velocity.” Let’s\nexamine this model again relative to how we calculated forces in earlier chapters. In the\n“gravitational attraction” examples, the force always pointed directly from the object to the\ntarget (the exact direction of the desired velocity), whether the force was strong or weak.\nThe steering function, however, says: “I have the ability to perceive the environment.” The\nforce isn’t based on just the desired velocity, but on the desired velocity relative to the current\nvelocity. Only things that are alive can know their current velocity. A box falling off a table\ndoesn’t know it’s falling. A cheetah chasing its prey, however, knows it is chasing.\nThe steering force, therefore, is essentially a manifestation of the current velocity’s error\nerror: \"I’m\nsupposed to be going this fast in this direction, but I’m actually going this fast in another\ndirection. My error is the difference between where I want to go and where I am currently\ngoing.\" Taking that error and applying it as a steering force results in more dynamic, lifelike\nsimulations. With gravitational attraction, you would never have a force pointing away from the\ntarget, no matter how close. But with arriving via steering, if you are moving too fast towards\nthe target, the error would actually tell you to slow down!\nIf we are closer than 100 pixels...\nif (d < 100) {\n...set the magnitude according to how close\nwe are.\nfloat m = map(d,0,100,0,maxspeed);\ndesired.mult(m);\n} else {\nOtherwise, proceed at maximum speed.\ndesired.mult(maxspeed);\n}\nThe usual steering = desired - velocity\nPVector steer = PVector.sub(desired,velocity);\nsteer.limit(maxforce);\napplyForce(steer);\n}\nFigure 6.11\nThe Nature of Code (v1.0)\n273\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 444
  },
  {
    "chunk_full": "6.5 Your Own Desires: Desired Velocity\n6.5 Your Own Desires: Desired Velocity\nThe first two examples we’ve covered—seek and arrive—boil down to calculating a single\nvector for each behavior: the desired velocity. And in fact, every single one of Reynolds’s\nsteering behaviors follows this same pattern. In this chapter, we’re going to walk through\nseveral more of Reynolds’s behaviors—flow field, path-following, flocking. First, however, I\nwant to emphasize again that these are examples—demonstrations of common steering\nbehaviors that are useful in procedural animation. They are not the be-all and end-all of\nwhat you can do. As long as you can come up with a vector that describes a vehicle’s\ndesired velocity, then you have created your own steering behavior.\nLet’s see how Reynolds defines the desired velocity for his wandering behavior.\n“Wandering is a type of random steering which has some long term order: the steering\ndirection on one frame is related to the steering direction on the next frame. This produces\nmore interesting motion than, for example, simply generating a random steering direction\neach frame.”\n—Craig Reynolds (http://www.red3d.com/cwr/steer/Wander.html)\nFor Reynolds, the goal of wandering is not\nsimply random motion, but rather a sense\nof moving in one direction for a little while,\nwandering off to the next for a little bit, and\nso on and so forth. So how does Reynolds\ncalculate a desired vector to achieve such\nan effect?\nFigure 6.12 illustrates how the vehicle\npredicts its future location as a fixed\ndistance in front of it (in the direction of its\nvelocity), draws a circle with radius r at that\nlocation, and picks a random point along\nthe circumference of the circle. That\nrandom point moves randomly around the\ncircle in each frame of animation. And that\nrandom point is the vehicle’s target, its desired vector pointing in that direction.\nSounds a bit absurd, right? Or, at the very least, rather arbitrary. In fact, this is a very clever\nand thoughtful solution—it uses randomness to drive a vehicle’s steering, but constrains that\nrandomness along the path of a circle to keep the vehicle’s movement from appearing\njittery, and, well, random.\nBut the seemingly random and arbitrary nature of this solution should drive home the point\nI’m trying to make—these are made-up behaviors inspired by real-life motion. You can just\nas easily concoct some elaborate scenario to compute a desired velocity yourself. And you\nshould.\nFigure 6.12\nChapter 6. Autonomous Agents\n274\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 445
  },
  {
    "chunk_full": "Let’s say we want to create a steering behavior called “stay within walls.” We’ll define the\ndesired velocity as:\nIf a vehicle comes within a distance\nIf a vehicle comes within a distance d of a wall, it desires to move at maximum speed in\nof a wall, it desires to move at maximum speed in\nthe opposite direction of the wall.\nthe opposite direction of the wall.\nIf we define the walls of the space as the edges of a Processing window and the distance d as\n25, the code is rather simple.\nWrite the code for Reynolds’s wandering behavior. Use polar coordinates to calculate\nthe vehicle’s target along a circular path.\nExercise 6.4\nExercise 6.4\nFigure 6.13\nThe Nature of Code (v1.0)\n275\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 446
  },
  {
    "chunk_full": "Example 6.3: “Stay within walls” steering behavior\nif (location. x > 25) {\nPVector desired = new PVector(maxspeed,velocity.y);\nPVector steer = PVector.sub(desired, velocity);\nsteer.limit(maxforce);\napplyForce(steer);\n}\nMake a desired vector that retains the y\ndirection of the vehicle but points the x\ndirection directly away from the window’s\nleft edge.\nCome up with your own arbitrary scheme for calculating a desired velocity.\nExercise 6.5\nExercise 6.5\n6.6 Flow Fields\n6.6 Flow Fields\nNow back to the task at hand. Let’s examine a couple more of Reynolds’s steering\nbehaviors. First, flow field following\nflow field following. What is a flow field? Think of your Processing window\nas a grid. In each cell of the grid lives an arrow pointing in some direction—you know, a\nvector. As a vehicle moves around the screen, it asks, “Hey, what arrow is beneath me?\nThat’s my desired velocity!”\nChapter 6. Autonomous Agents\n276\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 447
  },
  {
    "chunk_full": "Reynolds’s flow field following example has the vehicle predicting its future location and\nfollowing the vector at that spot, but for simplicity’s sake, we’ll have the vehicle simply look to\nthe vector at its current location.\nBefore we can write the additional code for our Vehicle class, we’ll need to build a class that\ndescribes the flow field itself, the grid of vectors. A two-dimensional array is a convenient data\nstructure in which to store a grid of information. If you are not familiar with 2D arrays, I\nsuggest reviewing this online Processing tutorial: 2D array (http://processing.org/learning/\n2darray/). The 2D array is convenient because we reference each element with two indices,\nwhich we can think of as columns and rows.\nNotice how we are defining a third variable called resolution above. What is this variable?\nLet’s say we have a Processing window that is 200 pixels wide by 200 pixels high. We could\nmake a flow field that has a PVector object for every single pixel, or 40,000 PVectors (200 *\n200). This isn’t terribly unreasonable, but in our case, it’s overkill. We don’t need a PVector\nFigure 6.14\nclass FlowField {\nDeclaring a 2D array of PVectors\nPVector[][] field;\nHow many columns and how many rows in\nthe grid?\nint cols, rows;\nResolution of grid relative to window width\nand height in pixels\nint resolution;\nThe Nature of Code (v1.0)\n277\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 448
  },
  {
    "chunk_full": "for every single pixel; we can achieve the same effect by having, say, one every ten pixels\n(20 * 20 = 400). We use this resolution to define the number of columns and rows based on\nthe size of the window divided by resolution:\nNow that we’ve set up the flow field’s data structures, it’s time to compute the vectors in the\nflow field itself. How do we do that? However we feel like it! Perhaps we want to have every\nvector in the flow field pointing to the right.\nOr perhaps we want the vectors to point in random directions.\nFlowField() {\nresolution = 10;\nTotal columns equals width divided by\nresolution.\ncols = width/resolution;\nTotal rows equals height divided by\nresolution.\nrows = height/resolution;\nfield = new PVector[cols][rows];\n}\nFigure 6.15\nUsing a nested loop to hit every column\nand every row of the flow field\nfor (int i = 0; i < cols; i++) {\nfor (int j = 0; j < rows; j++) {\nArbitrary decision to make each vector\npoint to the right\nfield[i][j] = new PVector(1,0);\n}\n}\nChapter 6. Autonomous Agents\n278\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 449
  },
  {
    "chunk_full": "What if we use 2D Perlin noise (mapped to an angle)?\nFigure 6.16\nfor (int i = 0; i < cols; i++) {\nfor (int j = 0; j < rows; j++) {\nA random PVector\nfield[i][j] = PVector.2D();\n}\n}\nFigure 6.17\nfloat xoff = 0;\nfor (int i = 0; i < cols; i++) {\nfloat yoff = 0;\nfor (int j = 0; j < rows; j++) {\nfloat theta = map(noise(xoff,yoff),0,1,0,TWO_PI);\nfield[i][j] = new PVector(cos(theta),sin(theta));\nyoff += 0.1;\n}\nxoff += 0.1;\n}\nNoise\nThe Nature of Code (v1.0)\n279\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 450
  },
  {
    "chunk_full": "Now we’re getting somewhere. Flow fields can be used for simulating various effects, such\nas an irregular gust of wind or the meandering path of a river. Calculating the direction of\nyour vectors using Perlin noise is one way to achieve such an effect. Of course, there’s no\n“correct” way to calculate the vectors of a flow field; it’s really up to you to decide what\nyou’re looking to simulate.\nNow that we have a two-dimensional array storing all of the flow field vectors, we need a\nway for a vehicle to look up its desired vector in the flow field. Let’s say we have a vehicle\nthat lives at a PVector: its location. We first need to divide by the resolution of the grid. For\nexample, if the resolution is 10 and the vehicle is at (100,50), we need to look up column 10\nand row 5.\nBecause a vehicle could theoretically wander off the Processing window, it’s also useful for\nus to employ the constrain() function to make sure we don’t look outside of the flow field\narray. Here is a function we’ll call lookup() that goes in the FlowField class—it receives a\nPVector (presumably the location of our vehicle) and returns the corresponding flow field\nPVector for that location.\nWrite the code to calculate a PVector at every location in the flow field that points\ntowards the center of a window.\nPVector v = new PVector(____________,____________);\nv.______________();\nfield[i][j] = v;\nExercise 6.6\nExercise 6.6\nint column = int(location.x/resolution);\nint row = int(location.y/resolution);\nPVector lookup(PVector lookup) {\nUsing constrain()\nint column = int(constrain(lookup.x/resolution,0,cols-1));\nint row = int(constrain(lookup.y/resolution,0,rows-1));\nChapter 6. Autonomous Agents\n280\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 451
  },
  {
    "chunk_full": "Before we move on to the Vehicle class, let’s take a look at the FlowField class all together.\nSo let’s assume we have a FlowField object called “flow”. Using the lookup() function\nabove, our vehicle can then retrieve a desired vector from the flow field and use Reynolds’s\nrules (steering = desired - velocity) to calculate a steering force.\nNote the use of get() to ensure we return a\ncopy of the PVector.\nreturn field[column][row].get();\n}\nclass FlowField {\nA flow field is a two-dimensional array of\nPVectors.\nPVector[][] field;\nint cols, rows;\nint resolution;\nFlowField(int r) {\nresolution = r;\nDetermine the number of columns and\nrows.\ncols = width/resolution;\nrows = height/resolution;\nfield = new PVector[cols][rows];\ninit();\n}\nvoid init() {\nfloat xoff = 0;\nfor (int i = 0; i < cols; i++) {\nfloat yoff = 0;\nfor (int j = 0; j < rows; j++) {\nfloat theta = map(noise(xoff,yoff),0,1,0,TWO_PI);\nIn this example, we use Perlin noise to seed\nthe vectors.\nfield[i][j] = new PVector(cos(theta),sin(theta));\nyoff += 0.1;\n}\nxoff += 0.1;\n}\n}\nPolar to Cartesian coordinate transformation\nto get x and y components of the vector\nA function to return a PVector based on a\nlocation\nPVector lookup(PVector lookup) {\nint column = int(constrain(lookup.x/resolution,0,cols-1));\nint row = int(constrain(lookup.y/resolution,0,rows-1));\nreturn field[column][row].get();\n}\n}\nThe Nature of Code (v1.0)\n281\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 452
  },
  {
    "chunk_full": "Example 6.4: Flow field following\nclass Vehicle {\nvoid follow(FlowField flow) {\nWhat is the vector at that spot in the flow\nfield?\nPVector desired = flow.lookup(location);\ndesired.mult(maxspeed);\nSteering is desired minus velocity\nPVector steer = PVector.sub(desired,\nvelocity);\nsteer.limit(maxforce);\napplyForce(steer);\n}\nAdapt the flow field example so that the PVectors change over time. (Hint: try using\nthe third dimension of Perlin noise!)\nExercise 6.7\nExercise 6.7\nCan you seed a flow field from a PImage? For example, try having the PVectors point\nfrom dark to light colors (or vice versa).\nExercise 6.8\nExercise 6.8\n6.7 The Dot Product\n6.7 The Dot Product\nIn a moment, we’re going to work through the algorithm (along with accompanying\nmathematics) and code for another of Craig Reynolds’s steering behaviors: Path Following\n(http://www.red3d.com/cwr/steer/PathFollow.html). Before we can do this, however, we have\nChapter 6. Autonomous Agents\n282\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 453
  },
  {
    "chunk_full": "to spend some time learning about another piece of vector math that we skipped in Chapter\n1—the dot product. We haven’t needed it yet, but it’s likely going to prove quite useful for you\n(beyond just this path-following example), so we’ll go over it in detail now.\nRemember all the basic vector math we covered in Chapter 1? Add, subtract, multiply, and\ndivide?\nNotice how in the above diagram, vector multiplication involves multiplying a vector by a\nscalar value. This makes sense; when we want a vector to be twice as large (but facing the\nsame direction), we multiply it by 2. When we want it to be half the size, we multiply it by 0.5.\nHowever, there are two other multiplication-like operations with vectors that are useful in\ncertain scenarios—the dot product and the cross product. For now we’re going to focus on the\ndot product, which is defined as follows. Assume vectors A\n→and B\n→:\nA\n→= (ax, ay)\nB\n→= (bx, by)\nTHE DOT PRODUCT: A\n→· B\n→= ax×bx + ay×by\nFor example, if we have the following two vectors:\nA\n→= (−3, 5)\nB\n→= (10, 1)\nA\n→· B\n→= −3 * 10 + 5 * 1 = −30 + 5 = 35\nNotice that the result of the dot product is a scalar value (a single number) and not a vector.\nIn Processing, this would translate to:\nFigure 6.18\nThe Nature of Code (v1.0)\n283\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 454
  },
  {
    "chunk_full": "And if we were to look in the guts of the PVector source, we’d find a pretty simple\nimplementation of this function:\nThis is simple enough, but why do we need the dot product, and when is it going to be\nuseful for us in code?\nOne of the more common uses of the dot product is to find the angle between two vectors.\nAnother way in which the dot product can be expressed is:\nA\n→· B\n→= ∥A\n→∥× ∥B\n→∥× cos(θ)\nIn other words, A dot B is equal to the magnitude of A times magnitude of B times cosine of\ntheta (with theta defined as the angle between the two vectors A and B).\nThe two formulas for dot product can be derived from one another with trigonometry\n(http://mathworld.wolfram.com/DotProduct.html), but for our purposes we can be happy with\noperating on the assumption that:\nA\n→· B\n→= ∥A\n→∥× ∥B\n→∥× cos(θ)\nA\n→· B\n→= ax×bx + ay×by\nboth hold true and therefore:\nax×bx + ay×by = ∥A\n→∥× ∥B\n→∥× cos(θ)\nNow, let’s start with the following problem.\nWe have the vectors A and B:\nA\n→= (10, 2)\nB\n→= (4, −3)\nWe now have a situation in which we know\neverything except for theta. We know the\ncomponents of the vector and can\ncalculate the magnitude of each vector. We\ncan therefore solve for cosine of theta:\ncos(θ) = ( A\n→· B\n→) / ( ∥A\n→∥× ∥B\n→∥)\nPVector a = new PVector(-3,5);\nPVector b = new PVector(10,1);\nThe PVector class includes a function to\ncalculate the dot product.\nfloat n = a.dot(b);\npublic float dot(PVector v) {\nreturn x*v.x + y*v.y + z*v.z;\n}\nFigure 6.19\nChapter 6. Autonomous Agents\n284\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 455
  },
  {
    "chunk_full": "To solve for theta, we can take the inverse cosine (often expressed as cosine-1 or arccosine).\nθ = cos−1 ( ( A\n→· B\n→) / ( ∥A\n→∥× ∥B\n→∥) )\nLet’s now do the math with actual numbers:\n∥A\n→∥= 10.2\n∥B\n→∥= 5\nTherefore:\nθ = cos−1 ( ( 10 × 4 + 2 × -3 ) / ( 10.2 × 5 ) )\nθ = cos−1 ( 34 / 51 )\nθ = ∼48∘\nThe Processing version of this would be:\nAnd, again, if we were to dig into the guts of the Processing source code, we would see a\nfunction that implements this exact algorithm.\nA couple things to note here:\nPVector a = new PVector(10,2);\nPVector b = new PVector(4,-3);\nfloat theta = acos(a.dot(b) / (a.mag() * b.mag()));\nstatic public float angleBetween(PVector v1, PVector v2) {\nfloat dot = v1.dot(v2);\nfloat theta = (float) Math.acos(dot / (v1.mag() * v2.mag()));\nreturn theta;\n}\nCreate a sketch that displays the angle\nbetween two PVector objects.\nExercise 6.9\nExercise 6.9\nThe Nature of Code (v1.0)\n285\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 456
  },
  {
    "chunk_full": "1.\nIf two vectors (A\n→and B\n→) are orthogonal (i.e. perpendicular), the dot product (A\n→· B\n→)\nis equal to 0.\n2.\nIf two vectors are unit vectors, then the dot product is simply equal to cosine of\nthe angle between them, i.e. A\n→· B\n→= cos(θ) if A\n→and B\n→are of length 1.\n6.8 Path Following\n6.8 Path Following\nNow that we’ve got a basic understanding of the dot product under our belt, we can return\nto a discussion of Craig Reynolds’s path-following algorithm. Let’s quickly clarify something.\nWe are talking about path following, not path finding. Pathfinding refers to a research topic\n(commonly studied in artificial intelligence) that involves solving for the shortest distance\nbetween two points, often in a maze. With path following\npath following, the path already exists and we’re\nasking a vehicle to follow that path.\nBefore we work out the individual pieces, let’s take a look at the overall algorithm for path\nfollowing, as defined by Reynolds.\nWe’ll first define what we mean by a path. There are many ways we could implement a path,\nbut for us, a simple way will be to define a path as a series of connected points:\nFigure 6.20\nChapter 6. Autonomous Agents\n286\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 457
  },
  {
    "chunk_full": "An even simpler path would be a line between two points.\nWe’re also going to consider a path to have a radius. If we think of the path as a road, the\nradius determines the road’s width. With a smaller radius, our vehicles will have to follow the\npath more closely; a wider radius will allow them to stray a bit more.\nPutting this into a class, we have:\nFigure 6.21: Path\nFigure 6.22: Simple path\nclass Path {\nA path is only two points, start and end.\nPVector start;\nPVector end;\nA path has a radius, i.e. how wide it is.\nfloat radius;\nPath() {\nThe Nature of Code (v1.0)\n287\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 458
  },
  {
    "chunk_full": "Now, let’s assume we have a vehicle (as depicted below) outside of the path’s radius,\nmoving with a velocity.\nThe first thing we want to do is predict, assuming a constant velocity, where that vehicle will\nbe in the future.\nOnce we have that location, it’s now our job to find out the vehicle’s current distance from\nthe path of that predicted location. If it’s very far away, well, then, we’ve strayed from the\npath and need to steer back towards it. If it’s close, then we’re doing OK and are following\nthe path nicely.\nPicking some arbitrary values to initialize\nthe path\nradius = 20;\nstart = new PVector(0,height/3);\nend = new PVector(width,2*height/3);\n}\nvoid display() {\n// Display the path.\nstrokeWeight(radius*2);\nstroke(0,100);\nline(start.x,start.y,end.x,end.y);\nstrokeWeight(1);\nstroke(0);\nline(start.x,start.y,end.x,end.y);\n}\n}\nFigure 6.23\nStart by making a copy of the velocity.\nPVector predict = vel.get();\nNormalize it and look 25 pixels ahead by\nscaling the vector up.\npredict.normalize();\npredict.mult(25);\nAdd vector to location to find the predicted\nlocation.\nPVector predictLoc = PVector.add(loc, predict);\nChapter 6. Autonomous Agents\n288\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 459
  },
  {
    "chunk_full": "So, how do we find the distance between a point and a line? This concept is key. The distance\nbetween a point and a line is defined as the length of the normal between that point and line.\nThe normal is a vector that extends from that point and is perpendicular to the line.\nLet’s figure out what we do know. We know we have a vector (call it A\n→) that extends from the\npath’s starting point to the vehicle’s predicted location.\nWe also know that we can define a vector (call it B\n→) that points from the start of the path to the\nend.\nNow, with basic trigonometry, we know that the distance from the path’s start to the normal\npoint is:\n|A| * cos(theta).\nIf we knew theta, we could easily define that normal point as follows:\nFigure 6.24\nPVector a = PVector.sub(predictLoc,path.start);\nPVector b = PVector.sub(path.end,path.start);\nFigure 6.25\nThe Nature of Code (v1.0)\n289\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 460
  },
  {
    "chunk_full": "And if the dot product has taught us anything, it’s that given two vectors, we can get theta,\nthe angle between.\nWhile the above code will work, there’s one more simplification we can make. If you’ll\nnotice, the desired magnitude for vector B\n→is:\na.mag()*cos(theta)\nwhich is the code translation of:\n∥A\n→∥× cos(θ)\nAnd if you recall:\nA\n→· B\n→= ∥A\n→∥× ∥B\n→∥× cos(θ)\nNow, what if vector B\n→is a unit vector, i.e. length 1? Then:\nA\n→· B\n→= ∥A\n→∥× 1 × cos(θ)\nor\nA\n→· B\n→= ∥A\n→∥× cos(θ)\nAnd what are we doing in our code? Normalizing b!\nBecause of this fact, we can simplify our code as:\nThe distance from START to NORMAL\nfloat d = a.mag()*cos(theta);\nb.normalize();\nScale PVector b to that distance.\nb.mult(d);\nThe normal point can be found by adding\nthe scaled version of b to the path’s starting\npoint.\nPVector normalPoint = PVector.add(path.start,b);\nWhat is theta? The angle between A and B\nfloat theta = PVector.angleBetween(a,b);\nb.normalize();\nb.mult(a.mag()*cos(theta));\nPVector normalPoint = PVector.add(path.start,b);\nb.normalize();\nfloat theta = PVector.angleBetween(a,b);\nb.normalize();\nChapter 6. Autonomous Agents\n290\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 461
  },
  {
    "chunk_full": "This process is commonly known as “scalar projection.” |A| cos(θ) is the scalar projection of\n|A| cos(θ) is the scalar projection of\nA onto B.\nA onto B.\nOnce we have the normal point along the path, we have to decide whether the vehicle should\nsteer towards the path and how. Reynolds’s algorithm states that the vehicle should only steer\ntowards the path if it strays beyond the path (i.e., if the distance between the normal point and\nthe predicted future location is greater than the path radius).\nWe can use the dot product to scale b’s\nlength.\nb.mult(a.dot(b));\nPVector normalPoint = PVector.add(path.start,b);\nFigure 6.26\nFigure 6.27\nfloat distance = PVector.dist(predictLoc, normalPoint);\nIf the vehicle is outside the path, seek the\ntarget.\nif (distance > path.radius) {\nWe don’t have to work out the desired\nvelocity and steering force; all that is taken\ncare of by seek(), which we already wrote in\nExample 6.1.\nseek(target);\n}\nThe Nature of Code (v1.0)\n291\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 462
  },
  {
    "chunk_full": "But what is the target?\nReynolds’s algorithm involves picking a point ahead of the normal on the path (see step #3\nabove). But for simplicity, we could just say that the target is the normal itself. This will work\nfairly well:\nSince we know the vector that defines the path (we’re calling it “B”), we can implement\nReynolds’s “point ahead on the path” without too much trouble.\nPutting it all together, we have the following steering function in our Vehicle class.\nfloat distance = PVector.dist(predictLoc, normalPoint);\nif (distance > path.radius) {\nSeek the normal point on the path.\nseek(normalPoint);\n}\nFigure 6.28\nfloat distance = PVector.dist(predictLoc, normalPoint);\nif (distance > path.radius) {\nNormalize and scale b (pick 25 pixels\narbitrarily).\nb.normalize();\nb.mult(25);\nBy adding b to normalPoint, we now move\n25 pixels ahead on the path.\nPVector target = PVector.add(normalPoint,b);\nseek(target);\n}\nChapter 6. Autonomous Agents\n292\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 463
  },
  {
    "chunk_full": "Example 6.5: Simple path following\nNow, you may notice above that instead of using all that dot product/scalar projection code to\nfind the normal point, we instead call a function: getNormalPoint(). In cases like this, it’s\nuseful to break out the code that performs a specific task (finding a normal point) into a\nfunction that it can be used generically in any case where it is required. The function takes\nthree PVectors: the first defines a point in Cartesian space and the second and third\narguments define a line segment.\nvoid follow(Path p) {\nStep 1: Predict the vehicle’s future location.\nPVector predict = vel.get();\npredict.normalize();\npredict.mult(25);\nPVector predictLoc = PVector.add(loc, predict);\nStep 2: Find the normal point along the\npath.\nPVector a = p.start;\nPVector b = p.end;\nPVector normalPoint = getNormalPoint(predictLoc, a, b);\nStep 3: Move a little further along the path\nand set a target.\nPVector dir = PVector.sub(b, a);\ndir.normalize();\ndir.mult(10);\nPVector target = PVector.add(normalPoint, dir);\nStep 4: If we are off the path, seek that\ntarget in order to stay on the path.\nfloat distance =\nPVector.dist(normalPoint, predictLoc);\nif (distance > p.radius) {\nseek(target);\n}\n}\nThe Nature of Code (v1.0)\n293\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 464
  },
  {
    "chunk_full": "What do we have so far? We have a Path class that defines a path as a line between two\npoints. We have a Vehicle class that defines a vehicle that can follow the path (using a\nsteering behavior to seek a target along the path). What is missing?\nTake a deep breath. We’re almost there.\nFigure 6.29\nPVector getNormalPoint(PVector p, PVector a, PVector b) {\nPVector that points from a to p\nPVector ap = PVector.sub(p, a);\nPVector that points from a to b\nPVector ab = PVector.sub(b, a);\nUsing the dot product for scalar projection\nab.normalize();\nab.mult(ap.dot(ab));\nFinding the normal point along the line\nsegment\nPVector normalPoint = PVector.add(a, ab);\nreturn normalPoint;\n}\n6.9 Path Following with Multiple Segments\n6.9 Path Following with Multiple Segments\nWe’ve built a great example so far, yes, but it’s pretty darn limiting. After all, what if we want\nour path to be something that looks more like:\nFigure 6.30\nChapter 6. Autonomous Agents\n294\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 465
  },
  {
    "chunk_full": "While it’s true that we could make this example work for a curved path, we’re much less likely\nto end up needing a cool compress on our forehead if we stick with line segments. In the end,\nwe can always employ the same technique we discovered with Box2D—we can draw whatever\nfancy curved path we want and approximate it behind the scenes with simple geometric\nforms.\nSo, what’s the problem? If we made path following work with one line segment, how do we\nmake it work with a series of connected line segments? Let’s take a look again at our vehicle\ndriving along the screen. Say we arrive at Step 3.\nStep 3: Find a target point on the path.\nStep 3: Find a target point on the path.\nTo find the target, we need to find the normal to the line segment. But now that we have a\nseries of line segments, we have a series of normal points (see above)! Which one do we\nchoose? The solution we’ll employ is to pick the normal point that is (a) closest and (b) on the\npath itself.\nIf we have a point and an infinitely long line, we’ll always have a normal. But, as in the path-\nfollowing example, if we have a point and a line segment, we won’t necessarily find a normal\nthat is on the line segment itself. So if this happens for any of the segments, we can disqualify\nthose normals. Once we are left with normals that are on the path itself (only two in the above\ndiagram), we simply pick the one that is closest to our vehicle’s location.\nFigure 6.31\nFigure 6.32\nThe Nature of Code (v1.0)\n295\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 466
  },
  {
    "chunk_full": "In order to write the code for this, we’ll have to expand our Path class to have an\nArrayList of points (rather than just two, a start and an end).\nNow that we have the Path class defined, it’s the vehicle’s turn to deal with multiple line\nsegments. All we did before was find the normal for one line segment. We can now find the\nnormals for all the line segments in a loop.\nclass Path {\nA Path is now an ArrayList of points\n(PVector objects).\nArrayList<PVector> points;\nfloat radius;\nPath() {\nradius = 20;\npoints = new ArrayList<PVector>();\n}\nThis function allows us to add points to the\npath.\nvoid addPoint(float x, float y) {\n.\nPVector point = new PVector(x,y);\npoints.add(point);\n}\nDisplay the path as a series of points.\nvoid display() {\nstroke(0);\nnoFill();\nbeginShape();\nfor (PVector v : points) {\nvertex(v.x,v.y);\n}\nendShape();\n}\n}\nfor (int i = 0; i < p.points.size()-1; i++) {\nPVector a = p.points.get(i);\nPVector b = p.points.get(i+1);\nChapter 6. Autonomous Agents\n296\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 467
  },
  {
    "chunk_full": "Then we should make sure the normal point is actually between points a and b. Since we\nknow our path goes from left to right in this example, we can test if the x component of\nnormalPoint is outside the x components of a and b.\nAs a little trick, we’ll say that if it’s not within the line segment, let’s just pretend the end point\nof that line segment is the normal. This will ensure that our vehicle always stays on the path,\neven if it strays out of the bounds of our line segments.\nFinally, we’ll need to make sure we find the normal point that is closest to our vehicle. To\naccomplish this, we start with a very high “world record” distance and iterate through each\nnormal point to see if it beats the record (i.e. is less than). Each time a normal point beats the\nrecord, the world record is updated and the winning point is stored in a variable named\ntarget. At the end of the loop, we’ll have the closest normal point in that variable.\nExample 6.6: Path following\nPVector normalPoint = getNormalPoint(predictLoc, a, b);\nFinding the normals for each line segment\nif (normalPoint.x < a.x || normalPoint.x > b.x) {\nUse the end point of the segment as our\nnormal point if we can’t find one.\nnormalPoint = b.get();\n}\nPVector target = null;\nThe Nature of Code (v1.0)\n297\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 468
  },
  {
    "chunk_full": "Start with a very high record that can easily\nbe beaten.\nfloat worldRecord = 1000000;\nfor (int i = 0; i < p.points.size()-1; i++) {\nPVector a = p.points.get(i);\nPVector b = p.points.get(i+1);\nPVector normalPoint = getNormalPoint(predictLoc, a, b);\nif (normalPoint.x < a.x || normalPoint.x > b.x) {\nnormalPoint = b.get();\n}\nfloat distance = PVector.dist(predictLoc, normalPoint);\nIf we beat the record, then this should be\nour target!\nif (distance < worldRecord) {\nworldRecord = distance;\ntarget = normalPoint.get();\n}\n}\nUpdate the path-following example so that the path can go in any direction. (Hint:\nyou’ll need to use the min() and max() function when determining if the normal point\nis inside the line segment.)\nif (normalPoint.x < ____(____,____) || normalPoint.x > ____(____,____)) {\nnormalPoint = b.get();\n}\nExercise 6.10\nExercise 6.10\nCreate a path that changes over time. Can the points that define the path itself have\ntheir own steering behaviors?\nExercise 6.11\nExercise 6.11\n6.10 Complex Systems\n6.10 Complex Systems\nRemember our purpose? To breathe life into the things that move around our Processing\nwindows? By learning to write the code for an autonomous agent and building a series of\nexamples of individual behaviors, hopefully our souls feel a little more full. But this is no\nplace to stop and rest on our laurels. We’re just getting started. After all, there is a deeper\npurpose at work here. Yes, a vehicle is a simulated being that makes decisions about how\nto seek and flow and follow. But what is a life led alone, without the love and support of\nChapter 6. Autonomous Agents\n298\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 469
  },
  {
    "chunk_full": "others? Our purpose here is not only to build individual behaviors for our vehicles, but to put\nour vehicles into systems of many vehicles and allow those vehicles to interact with each\nother.\nLet’s think about a tiny, crawling ant—one single ant. An ant is an autonomous agent; it can\nperceive its environment (using antennae to gather information about the direction and\nstrength of chemical signals) and make decisions about how to move based on those signals.\nBut can a single ant acting alone build a nest, gather food, defend its queen? An ant is a\nsimple unit and can only perceive its immediate environment. A colony of ants, however, is a\nsophisticated complex system, a “superorganism” in which the components work together to\naccomplish difficult and complicated goals.\nWe want to take what we’ve learned during the process of building autonomous agents in\nProcessing into simulations that involve many agents operating in parallel—agents that have\nan ability to perceive not only their physical environment but also the actions of their fellow\nagents, and then act accordingly. We want to create complex systems in Processing.\nWhat is a complex system? A complex system is typically defined as a system that is “more\nthan the sum of its parts.” While the individual elements of the system may be incredibly\nsimple and easily understood, the behavior of the system as a whole can be highly complex,\nintelligent, and difficult to predict. Here are three key principles of complex systems.\n•\nSimple units with short-range relationships.\nSimple units with short-range relationships. This is what we’ve been building all\nalong: vehicles that have a limited perception of their environment.\n•\nSimple units operate in parallel.\nSimple units operate in parallel. This is what we need to simulate in code. For\nevery cycle through Processing’s draw() loop, each unit will decide how to move (to\ncreate the appearance of them all working in parallel).\n•\nSystem as a whole exhibits emergent phenomena.\nSystem as a whole exhibits emergent phenomena. Out of the interactions between\nthese simple units emerges complex behavior, patterns, and intelligence. Here we’re\ntalking about the result we are hoping for in our sketches. Yes, we know this\nhappens in nature (ant colonies, termites, migration patterns, earthquakes,\nsnowflakes, etc.), but can we achieve the same result in our Processing sketches?\nFollowing are three additional features of complex systems that will help frame the discussion,\nas well as provide guidelines for features we will want to include in our software simulations.\nIt’s important to acknowledge that this is a fuzzy set of characteristics and not all complex\nsystems have all of them.\n•\nNon-linearity.\nNon-linearity. This aspect of complex systems is often casually referred to as “the\nbutterfly effect,” coined by mathematician and meteorologist Edward Norton Lorenz,\na pioneer in the study of chaos theory. In 1961, Lorenz was running a computer\nweather simulation for the second time and, perhaps to save a little time, typed in a\nstarting value of 0.506 instead of 0.506127. The end result was completely different\nfrom the first result of the simulation. In other words, the theory is that a single\nbutterfly flapping its wings on the other side of the world could cause a massive\nThe Nature of Code (v1.0)\n299\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 470
  },
  {
    "chunk_full": "weather shift and ruin our weekend at the beach. We call it “non-linear” because\nthere isn’t a linear relationship between a change in initial conditions and a\nchange in outcome. A small change in initial conditions can have a massive effect\non the outcome. Non-linear systems are a superset of chaotic systems. In the next\nchapter, we’ll see how even in a system of many zeros and ones, if we change just\none bit, the result will be completely different.\n•\nCompetition and cooperation.\nCompetition and cooperation. One of the things that often makes a complex\nsystem tick is the presence of both competition and cooperation between the\nelements. In our upcoming flocking system, we will have three rules—alignment,\ncohesion, and separation. Alignment and cohesion will ask the elements to\n“cooperate”—i.e. work together to stay together and move together. Separation,\nhowever, will ask the elements to “compete” for space. As we get to the flocking\nsystem, try taking out the cooperation or the competition and you’ll see how you\nare left without complexity. Competition and cooperation are found in living\ncomplex systems, but not in non-living complex systems like the weather.\n•\nFeedback.\nFeedback. Complex systems often include a feedback loop where the the output\nof the system is fed back into the system to influence its behavior in a positive or\nnegative direction. Let’s say you drive to work each day because the price of gas\nis low. In fact, everyone drives to work. The price of gas goes up as demand\nbegins to exceed supply. You, and everyone else, decide to take the train to work\nbecause driving is too expensive. And the price of gas declines as the demand\ndeclines. The price of gas is both the input of the system (determining whether\nyou choose to drive or ride the train) and the output (the demand that results from\nyour choice). I should note that economic models (like supply/demand, the stock\nmarket) are one example of a human complex system. Others include fads and\ntrends, elections, crowds, and traffic flow.\nComplexity will serve as a theme for the remaining content in this book. In this chapter, we’ll\nbegin by adding one more feature to our Vehicle class: an ability to look at neighboring\nvehicles.\n6.11 Group Behaviors (or: Let’s not run into each\n6.11 Group Behaviors (or: Let’s not run into each\nother)\nother)\nA group is certainly not a new concept. We’ve done this before—in Chapter 4, where we\ndeveloped a framework for managing collections of particles in a ParticleSystem class.\nThere, we stored a list of particles in an ArrayList. We’ll do the same thing here: store a\nbunch of Vehicle objects in an ArrayList.\nChapter 6. Autonomous Agents\n300\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 471
  },
  {
    "chunk_full": "Now when it comes time to deal with all the vehicles in draw(), we simply loop through all of\nthem and call the necessary functions.\nOK, so maybe we want to add a behavior, a force to be applied to all the vehicles. This could\nbe seeking the mouse.\nBut that’s an individual behavior. We’ve already spent thirty-odd pages worrying about\nindividual behaviors. We’re here because we want to apply a group behavior. Let’s begin with\nseparation, a behavior that commands, “Avoid colliding with your neighbors!”\nIs that right? It sounds good, but it’s not. What’s missing? In the case of seek, we said, “Seek\nmouseX and mouseY.” In the case of separate, we’re saying “separate from everyone else.”\nWho is everyone else? It’s the list of all the other vehicles.\nThis is the big leap beyond what we did before with particle systems. Instead of having each\nelement (particle or vehicle) operate on its own, we’re now saying, “Hey you, the vehicle!\nWhen it comes time for you to operate, you need to operate with an awareness of everyone\nelse. So I’m going to go ahead and pass you the ArrayList of everyone else.”\nThis is how we’ve mapped out setup() and draw() to deal with a group behavior.\nDeclare an ArrayList of Vehicle objects.\nArrayList<Vehicle> vehicles;\nvoid setup() {\nInitialize and fill the ArrayList with a bunch of\nVehicles.\nvehicles = new ArrayList<Vehicle>;\nfor (int i = 0; i < 100; i++) {\nvehicles.add(new Vehicle(random(width),random(height)));\n}\n}\nvoid draw(){\nfor (Vehicle v : vehicles) {\nv.update();\nv.display();\n}\n}\nv.seek(mouseX,mouseY);\nv.separate();\nv.separate(vehicles);\nThe Nature of Code (v1.0)\n301\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 472
  },
  {
    "chunk_full": "Of course, this is just the beginning. The\nreal work happens inside the separate()\nfunction itself. Let’s figure out how we want\nto define separation. Reynolds states:\n“Steer to avoid crowding.” In other words, if\na given vehicle is too close to you, steer\naway from that vehicle. Sound familiar? Remember the seek behavior where a vehicle steers\ntowards a target? Reverse that force and we have the flee behavior.\nBut what if more than one vehicle is too\nclose? In this case, we’ll define separation\nas the average of all the vectors pointing\naway from any close vehicles.\nLet’s begin to write the code. As we just\nworked out, we’re writing a function called\nseparate() that receives an ArrayList of\nVehicle objects as an argument.\nInside this function, we’re going to loop through all of the vehicles and see if any are too\nclose.\nArrayList<Vehicle> vehicles;\nvoid setup() {\nsize(320,240);\nvehicles = new ArrayList<Vehicle>();\nfor (int i = 0; i < 100; i++) {\nvehicles.add(new Vehicle(random(width),random(height)));\n}\n}\nvoid draw() {\nbackground(255);\nfor (Vehicle v : vehicles) {\nThis is really the only new thing we’re\ndoing in this section. We’re asking a\nVehicle object to examine all the other\nvehicles in the process of calculating a\nseparation force.\nv.separate(vehicles);\nv.update();\nv.display();\n}\n}\nFigure 6.33\nFigure 6.34\nvoid separate (ArrayList<Vehicle> vehicles) {\n}\nChapter 6. Autonomous Agents\n302\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 473
  },
  {
    "chunk_full": "Notice how in the above code, we are not only checking if the distance is less than a desired\nseparation (i.e. too close!), but also if the distance is greater than zero. This is a little trick that\nmakes sure we don’t ask a vehicle to separate from itself. Remember, all the vehicles are in\nthe ArrayList, so if you aren’t careful you’ll be comparing each vehicle to itself!\nOnce we know that two vehicles are too close, we need to make a vector that points away\nfrom the offending vehicle.\nThis is not enough. We have that vector now, but we need to make sure we calculate the\naverage of all vectors pointing away from close vehicles. How do we compute average? We\nadd up all the vectors and divide by the total.\nThis variable specifies how close is too\nclose.\nfloat desiredseparation = 20;\nfor (Vehicle other : vehicles) {\nfloat d = PVector.dist(location, other.location);\nif ((d > 0) && (d < desiredseparation)) {\nWhat is the distance between me and\nanother Vehicle?\nAny code here will be executed if the\nVehicle is within 20 pixels.\n}\n}\nif ((d > 0) && (d < desiredseparation)) {\nPVector diff = PVector.sub(location, other.location);\ndiff.normalize();\n}\nA PVector pointing away from the other’s\nlocation\nStart with an empty PVector.\nPVector sum = new PVector();\nint count = 0;\nWe have to keep track of how many\nVehicles are too close.\nfor (Vehicle other : vehicles) {\nfloat d = PVector.dist(location, other.location);\nif ((d > 0) && (d < desiredseparation)) {\nPVector diff = PVector.sub(location, other.location);\ndiff.normalize();\nAdd all the vectors together and increment\nthe count.\nsum.add(diff);\ncount++;\n}\n}\nThe Nature of Code (v1.0)\n303\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 474
  },
  {
    "chunk_full": "Once we have the average vector (stored in the PVector object “sum”), that PVector can be\nscaled to maximum speed and become our desired velocity—we desire to move in that\ndirection at maximum speed! And once we have the desired velocity, it’s the same old\nReynolds story: steering equals desired minus velocity.\nLet’s see the function in its entirety. There are two additional improvements, noted in the\ncode comments.\nExample 6.7: Group behavior: Separation\nWe have to make sure we found at least\none close vehicle. We don’t want to bother\ndoing anything if nothing is too close (not to\nmention we can’t divide by zero!)\nif (count > 0) {\nsum.div(count);\n}\nif (count > 0) {\nsum.div(count);\nScale average to maxspeed (this becomes\ndesired).\nsum.setMag(maxspeed);\nReynolds’s steering formula\nPVector steer = PVector.sub(sum,vel);\nsteer.limit(maxforce);\nApply the force to the Vehicle’s\nacceleration.\napplyForce(steer);\n}\nvoid separate (ArrayList<Vehicle> vehicles) {\nChapter 6. Autonomous Agents\n304\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 475
  },
  {
    "chunk_full": "Note how the desired separation is based\non the Vehicle’s size.\nfloat desiredseparation = r*2;\nPVector sum = new PVector();\nint count = 0;\nfor (Vehicle other : vehicles) {\nfloat d = PVector.dist(location, other.location);\nif ((d > 0) && (d < desiredseparation)) {\nPVector diff = PVector.sub(location, other.location);\ndiff.normalize();\nWhat is the magnitude of the PVector\npointing away from the other vehicle? The\ncloser it is, the more we should flee. The\nfarther, the less. So we divide by the\ndistance to weight it appropriately.\ndiff.div(d);\nsum.add(diff);\ncount++;\n}\n}\nif (count > 0) {\nsum.div(count);\nsum.normalize();\nsum.mult(maxspeed);\nPVector steer = PVector.sub(sum, vel);\nsteer.limit(maxforce);\napplyForce(steer);\n}\n}\nRewrite separate() to work in the opposite fashion (“cohesion”). If a vehicle is beyond\na certain distance, steer towards that vehicle. This will keep the group together. (Note\nthat in a moment, we’re going to look at what happens when we have both cohesion\nand separation in the same simulation.)\nExercise 6.12\nExercise 6.12\nThe Nature of Code (v1.0)\n305\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 476
  },
  {
    "chunk_full": "Add the separation force to path following to create a simulation of Reynolds’s\n“Crowd Path Following.”\nExercise 6.13\nExercise 6.13\n6.12 Combinations\n6.12 Combinations\nThe previous two exercises hint at what is perhaps the most important aspect of this\nchapter. After all, what is a Processing sketch with one steering force compared to one with\nmany? How could we even begin to simulate emergence in our sketches with only one rule?\nThe most exciting and intriguing behaviors will come from mixing and matching multiple\nsteering forces, and we’ll need a mechanism for doing so.\nYou may be thinking, “Duh, this is nothing new. We do this all the time.” You would be right.\nIn fact, we did this as early as Chapter 2.\nHere we have a mover that responds to two forces. This all works nicely because of the way\nwe designed the Mover class to accumulate the force vectors into its acceleration vector. In\nthis chapter, however, our forces stem from internal desires of the movers (now called\nvehicles). And those desires can be weighted. Let’s consider a sketch where all vehicles\nhave two desires:\n•\nSeek the mouse location.\nSeek the mouse location.\n•\nSeparate from any vehicles that are too close.\nSeparate from any vehicles that are too close.\nPVector wind = new PVector(0.001,0);\nPVector gravity = new PVector(0,0.1);\nmover.applyForce(wind);\nmover.applyForce(gravity);\nChapter 6. Autonomous Agents\n306\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 477
  },
  {
    "chunk_full": "We might begin by adding a function to the Vehicle class that manages all of the behaviors.\nLet’s call it applyBehaviors().\nHere we see how a single function takes care of calling the other functions that apply the\nforces—separate() and seek(). We could start mucking around with those functions and see\nif we can adjust the strength of the forces they are calculating. But it would be easier for us to\nask those functions to return the forces so that we can adjust their strength before applying\nthem to the vehicle’s acceleration.\nLet’s look at how the seek function changed.\nThis is a subtle change, but incredibly important for us: it allows us to alter the strength of\nthese forces in one place.\nvoid applyBehaviors(ArrayList<Vehicle> vehicles) {\nseparate(vehicles);\nseek(new PVector(mouseX,mouseY));\n}\nvoid applyBehaviors(ArrayList<Vehicle> vehicles) {\nPVector separate = separate(vehicles);\nPVector seek = seek(new PVector(mouseX,mouseY));\nWe have to apply the force here since\nseek() and separate() no longer do so.\napplyForce(separate);\napplyForce(seek);\n}\nPVector seek(PVector target) {\nPVector desired = PVector.sub(target,loc);\ndesired.normalize();\ndesired.mult(maxspeed);\nPVector steer = PVector.sub(desired,vel);\nsteer.limit(maxforce);\nInstead of applying the force we return the\nPVector.\napplyForce(steer);\nreturn steer;\n}\nThe Nature of Code (v1.0)\n307\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 478
  },
  {
    "chunk_full": "Example 6.8: Combining steering behaviors: Seek and separate\nvoid applyBehaviors(ArrayList<Vehicle> vehicles) {\nPVector separate = separate(vehicles);\nPVector seek = seek(new PVector(mouseX,mouseY));\nThese values can be whatever you want\nthem to be! They can be variables that are\ncustomized for each vehicle, or they can\nchange over time.\nseparate.mult(1.5);\nseek.mult(0.5);\napplyForce(separate);\napplyForce(seek);\n}\nRedo Example 6.8 so that the behavior weights are not constants. What happens if\nthey change over time (according to a sine wave or Perlin noise)? Or if some vehicles\nare more concerned with seeking and others more concerned with separating? Can\nyou introduce other steering behaviors as well?\nExercise 6.14\nExercise 6.14\n6.13 Flocking\n6.13 Flocking\nFlocking is an group animal behavior that is characteristic of many living creatures, such as\nbirds, fish, and insects. In 1986, Craig Reynolds created a computer simulation of flocking\nbehavior and documented the algorithm in his paper, “Flocks, Herds, and Schools: A\nDistributed Behavioral Model.” Recreating this simulation in Processing will bring together\nall the concepts in this chapter.\n1.\nWe will use the steering force formula (steer = desired - velocity) to implement the\nrules of flocking.\n2.\nThese steering forces will be group behaviors and require each vehicle to look at\nall the other vehicles.\n3.\nWe will combine and weight multiple forces.\n4.\nThe result will be a complex system—intelligent group behavior will emerge from\nthe simple rules of flocking without the presence of a centralized system or\nleader.\nThe good news is, we’ve already done items 1 through 3 in this chapter, so this section will\nbe about just putting it all together and seeing the result.\nChapter 6. Autonomous Agents\n308\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 479
  },
  {
    "chunk_full": "Before we begin, I should mention that we’re going to change the name of our Vehicle class\n(yet again). Reynolds uses the term “boid” (a made-up word that refers to a bird-like object) to\ndescribe the elements of a flocking system and we will do the same.\nLet’s take an overview of the three rules of flocking.\n1.\nSeparation\nSeparation (also known as “avoidance”): Steer to avoid colliding with your\nneighbors.\n2.\nAlignment\nAlignment (also known as “copy”): Steer in the same direction as your neighbors.\n3.\nCohesion\nCohesion (also known as “center”): Steer towards the center of your neighbors (stay\nwith the group).\nJust as we did with our separate and seek example, we’ll want our Boid objects to have a\nsingle function that manages all the above behaviors. We’ll call this function flock().\nNow, it’s just a matter of implementing the three rules. We did separation before; it’s identical\nto our previous example. Let’s take a look at alignment, or steering in the same direction as\nFigure 6.35\nvoid flock(ArrayList<Boid> boids) {\nThe three flocking rules\nPVector sep = separate(boids);\nPVector ali = align(boids);\nPVector coh = cohesion(boids);\nArbitrary weights for these forces (Try\ndifferent ones!)\nsep.mult(1.5);\nali.mult(1.0);\ncoh.mult(1.0);\nApplying all the forces\napplyForce(sep);\napplyForce(ali);\napplyForce(coh);\n}\nThe Nature of Code (v1.0)\n309\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 480
  },
  {
    "chunk_full": "your neighbors. As with all of our steering behaviors, we’ve got to boil down this concept\ninto a desire: the boid’s desired velocity is the average velocity of its neighbors.\nSo our algorithm is to calculate the average velocity of all the other boids and set that to\ndesired.\nThe above is pretty good, but it’s missing one rather crucial detail. One of the key principles\nbehind complex systems like flocking is that the elements (in this case, boids) have short-\nrange relationships. Thinking about ants again, it’s pretty easy to imagine an ant being able\nto sense its immediate environment, but less so an ant having an awareness of what\nanother ant is doing hundreds of feet away. The fact that the ants can perform such\ncomplex collective behavior from only these neighboring relationships is what makes them\nso exciting in the first place.\nIn our alignment function, we’re taking the average velocity of all the boids, whereas we\nshould really only be looking at the boids within a certain distance. That distance threshold\nis up to you, of course. You could design boids that can see only twenty pixels away or\nboids that can see a hundred pixels away.\nPVector align (ArrayList<Boid> boids) {\nAdd up all the velocities and divide by the\ntotal to calculate the average velocity.\nPVector sum = new PVector(0,0);\nfor (Boid other : boids) {\nsum.add(other.velocity);\n}\nsum.div(boids.size());\nWe desire to go in that direction at\nmaximum speed.\nsum.setMag(maxspeed);\nReynolds’s steering force formula\nPVector steer = PVector.sub(sum,velocity);\nsteer.limit(maxforce);\nreturn steer;\n}\nChapter 6. Autonomous Agents\n310\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 481
  },
  {
    "chunk_full": "Much like we did with separation (only calculating a force for others within a certain distance),\nwe’ll want to do the same with alignment (and cohesion).\nFigure 6.36\nPVector align (ArrayList<Boid> boids) {\nThis is an arbitrary value and could vary\nfrom boid to boid.\nfloat neighbordist = 50;\nPVector sum = new PVector(0,0);\nint count = 0;\nfor (Boid other : boids) {\nfloat d = PVector.dist(location,other.location);\nif ((d > 0) && (d < neighbordist)) {\nsum.add(other.velocity);\nFor an average, we need to keep track of\nhow many boids are within the distance.\ncount++;\n}\n}\nif (count > 0) {\nsum.div(count);\nsum.normalize();\nsum.mult(maxspeed);\nPVector steer = PVector.sub(sum,velocity);\nsteer.limit(maxforce);\nreturn steer;\nIf we don’t find any close boids, the steering\nforce is zero.\n} else {\nreturn new PVector(0,0);\n}\n}\nThe Nature of Code (v1.0)\n311\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 482
  },
  {
    "chunk_full": "Finally, we are ready for cohesion. Here our code is virtually identical to that for\nalignment—only instead of calculating the average velocity of the boid’s neighbors, we want\nto calculate the average location of the boid’s neighbors (and use that as a target to seek).\nIt’s also worth taking the time to write a class called Flock, which will be virtually identical\nto the ParticleSystem class we wrote in Chapter 4 with only one tiny change: When we\ncall run() on each Boid object (as we did to each Particle object), we’ll pass in a\nreference to the entire ArrayList of boids.\nCan you write the above code so that\nboids can only see other boids that are\nactually within their “peripheral” vision\n(as if they had eyes)?\nExercise 6.15\nExercise 6.15\nPVector cohesion (ArrayList<Boid> boids) {\nfloat neighbordist = 50;\nPVector sum = new PVector(0,0);\nint count = 0;\nfor (Boid other : boids) {\nfloat d = PVector.dist(location,other.location);\nif ((d > 0) && (d < neighbordist)) {\nAdding up all the others’ locations\nsum.add(other.location);\ncount++;\n}\n}\nif (count > 0) {\nsum.div(count);\nHere we make use of the seek() function\nwe wrote in Example 6.8. The target we\nseek is the average location of our\nneighbors.\nreturn seek(sum);\n} else {\nreturn new PVector(0,0);\n}\n}\nChapter 6. Autonomous Agents\n312\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 483
  },
  {
    "chunk_full": "And our main program will look like:\nExample 6.9: Flocking\nclass Flock {\nArrayList<Boid> boids;\nFlock() {\nboids = new ArrayList<Boid>();\n}\nvoid run() {\nfor (Boid b : boids) {\nEach Boid object must know about all the\nother Boids.\nb.run(boids);\n}\n}\nvoid addBoid(Boid b) {\nboids.add(b);\n}\n}\nA Flock object manages the entire group.\nFlock flock;\nvoid setup() {\nsize(300,200);\nflock = new Flock();\nfor (int i = 0; i < 100; i++) {\nBoid b = new Boid(width/2,height/2);\nThe Flock starts out with 100 Boids.\nflock.addBoid(b);\n}\n}\nvoid draw() {\nbackground(255);\nflock.run();\n}\nThe Nature of Code (v1.0)\n313\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 484
  },
  {
    "chunk_full": "Combine flocking with some other steering behaviors.\nExercise 6.16\nExercise 6.16\nIn his book The Computational Beauty\nof Nature (MIT Press, 2000), Gary Flake\ndescribes a fourth rule for flocking:\n“View: move laterally away from any\nboid that blocks the view.” Have your\nboids follow this rule.\nExercise 6.17\nExercise 6.17\nCreate a flocking simulation where all of the parameters (separation weight, cohesion\nweight, alignment weight, maximum force, maximum speed) change over time. They\ncould be controlled by Perlin noise or by user interaction. (For example, you could\nuse a library such as controlp5 (http://www.sojamo.de/libraries/controlP5/) to tie the\nvalues to slider positions.)\nExercise 6.18\nExercise 6.18\nVisualize the flock in an entirely different way.\nExercise 6.19\nExercise 6.19\nChapter 6. Autonomous Agents\n314\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 485
  },
  {
    "chunk_full": "6.14 Algorithmic Efficiency (or: Why does my $@(*%!\n6.14 Algorithmic Efficiency (or: Why does my $@(*%!\nrun so slowly?)\nrun so slowly?)\nI would like to hide the dark truth behind we’ve just done, because I would like you to be\nhappy and live a fulfilling and meaningful life. But I also would like to be able to sleep at night\nwithout worrying about you so much. So it is with a heavy heart that I must bring up this topic.\nGroup behaviors are wonderful. But they can be slow, and the more elements in the group,\nthe slower they can be. Usually, when we talk about Processing sketches running slowly, it’s\nbecause drawing to the screen can be slow—the more you draw, the slower your sketch runs.\nThis is actually a case, however, where the slowness derives from the algorithm itself. Let’s\ndiscuss.\nComputer scientists classify algorithms with something called “Big O notation,” which\ndescribes the efficiency of an algorithm: how many computational cycles does it require to\ncomplete? Let’s consider a simple analog search problem. You have a basket containing one\nhundred chocolate treats, only one of which is pure dark chocolate. That’s the one you want\nto eat. To find it, you pick the chocolates out of the basket one by one. Sure, you might be\nlucky and find it on the first try, but in the worst-case scenario you have to check all one\nhundred before you find the dark chocolate. To find one thing in one hundred, you have to\ncheck one hundred things (or to find one thing in N things, you have to check N times.) Your\nBig O Notation is N. This, incidentally, is the Big O Notation that describes our simple particle\nsystem. If we have N particles, we have to run and display those particles N times.\nNow, let’s think about a group behavior (such as flocking). For every Boid object, we have to\ncheck every other Boid object (for its velocity and location). Let’s say we have one hundred\nboids. For boid #1, we need to check one hundred boids; for boid #2, we need to check one\nhundred boids, and so on and so forth. For one hundred boids, we need to perform one\nhundred times one hundred checks, or ten thousand. No problem: computers are fast and can\ndo things ten thousand times pretty easily. Let’s try one thousand.\n1,000 x 1,000 = 1,000,000 cycles.\nOK, this is rather slow, but still somewhat manageable. Let’s try 10,000 elements:\n10,000 x 10,000 elements = 100,000,000 cycles.\nNow, we’re really getting slow. Really, really, really slow.\nNotice something odd? As the number of elements increases by a factor of 10, the number of\nrequired cycles increases by a factor of 100. Or as the number of elements increases by a\nfactor of N, the cycles increase by a factor of N times N. This is known as Big O Notation N-\nSquared.\nI know what you are thinking. You are thinking: “No problem; with flocking, we only need to\nconsider the boids that are close to other boids. So even if we have 1,000 boids, we can just\nlook at, say, the 5 closest boids and then we only have 5,000 cycles.” You pause for a\nThe Nature of Code (v1.0)\n315\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 486
  },
  {
    "chunk_full": "moment, and then start thinking: “So for each boid I just need to check all the boids and find\nthe five closest ones and I’m good!” See the catch-22? Even if we only want to look at the\nclose ones, the only way to know what the close ones are would be to check all of them.\nOr is there another way?\nLet’s take a number that we might actually want to use, but would still run too slowly: 2,000\n(4,000,000 cycles required).\nWhat if we could divide the screen into a grid? We would take all 2,000 boids and assign\neach boid to a cell within that grid. We would then be able to look at each boid and\ncompare it to its neighbors within that cell at any given moment. Imagine a 10 x 10 grid. In a\nsystem of 2,000 elements, on average, approximately 20 elements would be found in each\ncell (20 x 10 x 10 = 2,000). Each cell would then require 20 x 20 = 400 cycles. With 100 cells,\nwe’d have 100 x 400 = 40,000 cycles, a massive savings over 4,000,000.\nThis technique is known as “bin-lattice spatial subdivision” and is outlined in more detail in\n(surprise, surprise) Reynolds’s 2000 paper, “Interaction with Groups of Autonomous\nCharacters” (http://www.red3d.com/cwr/papers/2000/pip.pdf). How do we implement such\nan algorithm in Processing? One way is to keep multiple ArrayLists. One ArrayList would\nkeep track of all the boids, just like in our flocking example.\nIn addition to that ArrayList, we store an additional reference to each Boid object in a\ntwo-dimensional ArrayList. For each cell in the grid, there is an ArrayList that tracks the\nobjects in that cell.\nFigure 6.37\nArrayList<Boid> boids;\nChapter 6. Autonomous Agents\n316\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 487
  },
  {
    "chunk_full": "In the main draw() loop, each Boid object then registers itself in the appropriate cell\naccording to its location.\nThen when it comes time to have the boids check for neighbors, they can look at only those in\ntheir particular cell (in truth, we also need to check neighboring cells to deal with border\ncases).\nExample 6.10: Bin-lattice spatial subdivision\nWe’re only covering the basics here; for the full code, check the book’s website.\nNow, there are certainly flaws with this system. What if all the boids congregate in the corner\nand live in the same cell? Then don’t we have to check all 2,000 against all 2,000?\nThe good news is that this need for optimization is a common one and there are a wide\nvariety of similar techniques out there. For us, it’s likely that a basic approach will be good\nenough (in most cases, you won’t need one at all.) For another, more sophisticated approach,\ncheck out toxiclibs' Octree examples (http://toxiclibs.org/2010/02/new-package-simutils/).\nArrayList<Boid>[][] grid;\nint column = int(boid.x) / resolution;\nint row\n= int(boid.y) /resolution;\ngrid[column][row].add(boid);\nint column = int(boid.x) / resolution;\nint row\n= int(boid.y) /resolution;\nboid.flock(boids);\nInstead of looking at all the boids, just this\ncell\nboid.flock(grid[column][row]);\n6.15 A Few Last Notes: Optimization Tricks\n6.15 A Few Last Notes: Optimization Tricks\nThis is something of a momentous occasion. The end of Chapter 6 marks the end of our story\nof motion (in the context of this book, that is). We started with the concept of a vector, moved\non to forces, designed systems of many elements, examined physics libraries, built entities\nwith hopes and dreams and fears, and simulated emergence. The story doesn’t end here, but\nit does take a bit of a turn. The next two chapters won’t focus on moving bodies, but rather on\nsystems of rules. Before we get there, I have a few quick items I’d like to mention that are\nimportant when working with the examples in Chapters 1 through 6. They also relate to\noptimizing your code, which fits in with the previous section.\nThe Nature of Code (v1.0)\n317\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 488
  },
  {
    "chunk_full": "1) Magnitude squared (or sometimes distance squared)\n1) Magnitude squared (or sometimes distance squared)\nWhat is magnitude squared and when should you use it? Let’s revisit how the magnitude of\na vector is calculated.\nMagnitude requires the square root operation. And it should. After all, if you want the\nmagnitude of a vector, then you’ve got to look up the Pythagorean theorem and compute it\n(we did this in Chapter 1). However, if you could somehow skip using the square root, your\ncode would run faster. Let’s consider a situation where you just want to know the relative\nmagnitude of a vector. For example, is the magnitude greater than ten? (Assume a PVector\nv.)\nWell, this is equivalent to saying:\nAnd how is magnitude squared calculated?\nSame as magnitude, but without the square root. In the case of a single PVector object, this\nwill never make a significant difference on a Processing sketch. However, if you are\ncomputing the magnitude of thousands of PVector objects each time through draw(), using\nmagSq() instead of mag() could help your code run a wee bit faster. (Note: magSq() is only\navailable in Processing 2.0a1 or later.)\nfloat mag() {\nreturn sqrt(x*x + y*y);\n}\nif (v.mag() > 10) {\n// Do Something!\n}\nif (v.magSq() > 100) {\n// Do Something!\n}\nfloat magSq() {\nreturn x*x + y*y;\n}\n2) Sine and cosine lookup tables\n2) Sine and cosine lookup tables\nThere’s a pattern here. What kinds of functions are slow to compute? Square root. Sine.\nCosine. Tangent. Again, if you just need a sine or cosine value here or there in your code,\nyou are never going to run into a problem. But what if you had something like this?\nChapter 6. Autonomous Agents\n318\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 489
  },
  {
    "chunk_full": "Sure, this is a totally ridiculous code snippet that you would never write. But it illustrates a\ncertain point. If you are calculating the sine of pi ten thousand times, why not just calculate it\nonce, save that value, and refer to it whenever necessary? This is the principle behind sine\nand cosine lookup tables. Instead of calling the sine and cosine functions in your code\nwhenever you need them, you can build an array that stores the results of sine and cosine at\nangles between 0 and TWO_PI and just look up the values when you need them. For example,\nhere are two arrays that store the sine and cosine values for every angle, 0 to 359 degrees.\nNow, what if you need the value of sine of pi?\nA more sophisticated example of this technique is available on the Processing wiki\n(http://wiki.processing.org/w/Sin/Cos_look-up_table).\nvoid draw() {\nfor (int i = 0; i < 10000; i++) {\nprintln(sin(PI));\n}\n}\nfloat sinvalues[] = new float[360];\nfloat cosvalues[] = new float[360];\nfor (int i = 0; i < 360; i++) {\nsinvalues[i] = sin(radians(i));\ncosvalues[i] = cos(radians(i));\n}\nint angle = int(degrees(PI));\nfloat answer = sinvalues[angle];\n3) Making gajillions of unnecessary PVector objects\n3) Making gajillions of unnecessary PVector objects\nI have to admit, I am perhaps the biggest culprit of this last note. In fact, in the interest of\nwriting clear and understandable examples, I often choose to make extra PVector objects\nwhen I absolutely do not need to. For the most part, this is not a problem at all. But\nsometimes, it can be. Let’s take a look at an example.\nLet’s say our ArrayList of vehicles has one thousand vehicles in it. We just made one\nthousand new PVector objects every single time through draw(). Now, on any ol’ laptop or\ndesktop computer you’ve purchased in recent times, your sketch will likely not register a\ncomplaint, run slowly, or have any problems. After all, you’ve got tons of RAM, and Java will\nvoid draw() {\nfor (Vehicle v : vehicles) {\nPVector mouse = new PVector(mouseX,mouseY);\nv.seek(mouse);\n}\n}\nThe Nature of Code (v1.0)\n319\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 490
  },
  {
    "chunk_full": "be able to handle making a thousand or so temporary objects and dispose of them without\nmuch of a problem.\nIf your numbers grow larger (and they easily could) or perhaps more likely, if you are\nworking with Processing on Android, you will almost certainly run into a problem. In cases\nlike this you want to look for ways to reduce the number of PVector objects you make. An\nobvious fix for the above code is:\nNow you’ve made just one PVector instead of one thousand. Even better, you could turn\nthe PVector into a global variable and just assign the x and y value:\nNow you never make a new PVector; you use just one over the length of your sketch!\nThroughout the book’s examples, you can find lots of opportunities to reduce the number of\ntemporary objects. Let’s look at one more. Here is a snippet from our seek() function.\nSee how we’ve made two PVector objects? First, we figure out the desired vector, then we\ncalculate the steering force. Notice how we could rewrite this to create only one PVector.\nvoid draw() {\nPVector mouse = new PVector(mouseX,mouseY);\nfor (Vehicle v : vehicles) {\nv.seek(mouse);\n}\n}\nPVector mouse = new PVector();\nvoid draw() {\nmouse.x = mouseX;\nmouse.y = mouseY;\nfor (Vehicle v : vehicles) {\nv.seek(mouse);\n}\n}\nPVector desired = PVector.sub(target,location);\ndesired.normalize();\ndesired.mult(maxspeed);\nCreate a new PVector to store the steering\nforce.\nPVector steer = PVector.sub(desired,velocity);\nsteer.limit(maxforce);\nreturn steer;\nPVector desired = PVector.sub(target, location);\ndesired.normalize();\ndesired.mult(maxspeed);\nChapter 6. Autonomous Agents\n320\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 491
  },
  {
    "chunk_full": "We don’t actually need a second PVector called steer. We could just use the desired\nPVector object and turn it into the steering force by subtracting velocity. I didn’t do this in my\nexample because it is more confusing to read. But in some cases, it may be greatly more\nefficient.\nCalculate the steering force in the desired\nPVector.\ndesired.sub(velocity);\ndesired.limit(maxforce);\nreturn desired;\nEliminate as many temporary PVector objects from the flocking example as possible.\nAlso use magSq() where possible.\nExercise 6.20\nExercise 6.20\nUse steering behaviors with Box2D or toxiclibs.\nExercise 6.21\nExercise 6.21\nThe Nature of Code (v1.0)\n321\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 492
  },
  {
    "chunk_full": "The Ecosystem Project\nThe Ecosystem Project\nStep 6 Exercise:\nUse the concept of steering forces to drive the behavior of the creatures in your\necosystem. Some possibilities:\n•\nCreate “schools” or “flocks” of creatures.\n•\nUse a seeking behavior for creatures to search for food (for chasing\nmoving prey, consider “pursuit”).\n•\nUse a flow field for the ecosystem environment. For example, how does\nyour system behave if the creatures live in a flowing river?\n•\nBuild a creature with countless steering behaviors (as many as you can\nreasonably add). Think about ways to vary the weights of these\nbehaviors so that you can dial those behaviors up and down, mixing and\nmatching on the fly. How are creatures’ initial weights set? What rules\ndrive how the weights change over time?\n•\nComplex systems can be nested. Can you design a single creature out of\na flock of boids? And can you then make a flock of those creatures?\n•\nComplex systems can have memory (and be adaptive). Can the history of\nyour ecosystem affect the behavior in its current state? (This could be\nthe driving force behind how the creatures adjust their steering force\nweights.)\nChapter 6. Autonomous Agents\n322\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 493
  },
  {
    "chunk_full": "Chapter 7. Cellular\nChapter 7. Cellular\nAutomata\nAutomata\n“To play life you must have a fairly large checkerboard and a plentiful\nsupply of flat counters of two colors. It is possible to work with pencil and\ngraph paper but it is much easier, particularly for beginners, to use\ncounters and a board.”\n— Martin Gardner, Scientific American (October 1970)\nIn this chapter, we’re going to take a break from talking about vectors and motion. In fact, the\nrest of the book will mostly focus on systems and algorithms (albeit ones that we can, should,\nand will apply to moving bodies). In the previous chapter, we encountered our first Processing\nexample of a complex system: flocking. We briefly stated the core principles behind complex\nsystems: more than the sum of its parts, a complex system is a system of elements, operating\nin parallel, with short-range relationships that as a whole exhibit emergent behavior. This\nentire chapter is going to be dedicated to building another complex system simulation in\nProcessing. Oddly, we are going to take some steps backward and simplify the elements of\nour system. No longer are the individual elements going to be members of a physics world;\ninstead we will build a system out of the simplest digital element possible, a single bit. This bit\nis going to be called a cell and its value (0 or 1) will be called its state. Working with such\nsimple elements will help us understand more of the details behind how complex systems\nwork, and we’ll also be able to elaborate on some programming techniques that we can apply\nto code-based projects.\nThe Nature of Code (v1.0)\n323\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 494
  },
  {
    "chunk_full": "7.1 What Is a Cellular Automaton?\n7.1 What Is a Cellular Automaton?\nFirst, let’s get one thing straight. The term cellular automata\ncellular automata is plural. Our code examples\nwill simulate just one—a cellular automaton\ncellular automaton, singular. To simplify our lives, we’ll also refer\nto cellular automata as “CA.”\nIn Chapters 1 through 6, our objects (mover, particle, vehicle, boid) generally existed in only\none “state.” They might have moved around with advanced behaviors and physics, but\nultimately they remained the same type of object over the course of their digital lifetime.\nWe’ve alluded to the possibility that these entities can change over time (for example, the\nweights of steering “desires” can vary), but we haven’t fully put this into practice. In this\ncontext, cellular automata make a great first step in building a system of many objects that\nhave varying states over time.\nA cellular automaton is a model of a system of “cell” objects with the following\ncharacteristics.\n•\nThe cells live on a grid\ngrid. (We’ll see examples in both one and two dimensions in\nthis chapter, though a cellular automaton can exist in any finite number of\ndimensions.)\n•\nEach cell has a state\nstate. The number of state possibilities is typically finite. The\nsimplest example has the two possibilities of 1 and 0 (otherwise referred to as “on”\nand “off” or “alive” and “dead”).\n•\nEach cell has a neighborhood\nneighborhood. This can be defined in any number of ways, but it is\ntypically a list of adjacent cells.\nThe development of cellular automata systems is typically attributed to Stanisław Ulam and\nJohn von Neumann, who were both researchers at the Los Alamos National Laboratory in\nFigure 7.1\nChapter 7. Cellular Automata\n324\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 495
  },
  {
    "chunk_full": "New Mexico in the 1940s. Ulam was studying the growth of crystals and von Neumann was\nimagining a world of self-replicating robots. That’s right, robots that build copies of\nthemselves. Once we see some examples of CA visualized, it’ll be clear how one might\nimagine modeling crystal growth; the robots idea is perhaps less obvious. Consider the design\nof a robot as a pattern on a grid of cells (think of filling in some squares on a piece of graph\npaper). Now consider a set of simple rules that would allow that pattern to create copies of\nitself on that grid. This is essentially the process of a CA that exhibits behavior similar to\nbiological reproduction and evolution. (Incidentally, von Neumann’s cells had twenty-nine\npossible states.) Von Neumann’s work in self-replication and CA is conceptually similar to\nwhat is probably the most famous cellular automaton: the “Game of Life,” which we will\ndiscuss in detail in section 7.3.\nPerhaps the most significant scientific (and lengthy) work studying cellular automata arrived in\n2002: Stephen Wolfram’s 1,280-page A New Kind of Science (http://www.wolframscience.com/\nnksonline/toc.html). Available in its entirety for free online, Wolfram’s book discusses how CA\nare not simply neat tricks, but are relevant to the study of biology, chemistry, physics, and all\nbranches of science. This chapter will barely scratch the surface of the theories Wolfram\noutlines (we will focus on the code implementation) so if the examples provided spark your\ncuriosity, you’ll find plenty more to read about in his book.\n7.2 Elementary Cellular Automata\n7.2 Elementary Cellular Automata\nThe examples in this chapter will begin with a simulation of Wolfram’s work. To understand\nWolfram’s elementary CA, we should ask ourselves the question: “What is the simplest cellular\nautomaton we can imagine?” What’s exciting about this question and its answer is that even\nwith the simplest CA imaginable, we will see the properties of complex systems at work.\nLet’s build Wolfram’s elementary CA from scratch. Concepts first, then code. What are the\nthree key elements of a CA?\n1) Grid\nGrid. The simplest grid would be one-dimensional: a line of cells.\n2) States\nStates. The simplest set of states (beyond having only one state) would be two states: 0 or\n1.\nFigure 7.2\nFigure 7.3\nThe Nature of Code (v1.0)\n325\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 496
  },
  {
    "chunk_full": "3) Neighborhood\nNeighborhood. The simplest neighborhood in one dimension for any given cell would be\nthe cell itself and its two adjacent neighbors: one to the left and one to the right.\nSo we begin with a line of cells, each with an initial state (let’s say it is random), and each\nwith two neighbors. We’ll have to figure out what we want to do with the cells on the edges\n(since those have only one neighbor each), but this is something we can sort out later.\nWe haven’t yet discussed, however, what is perhaps the most important detail of how\ncellular automata work—time. We’re not really talking about real-world time here, but about\nthe CA living over a period of time, which could also be called a generation\ngeneration and, in our\ncase, will likely refer to the frame count\nframe count of an animation. The figures above show us the CA\nat time equals 0 or generation 0. The questions we have to ask ourselves are: How do we\ncompute the states for all cells at generation 1? And generation 2? And so on and so forth.\nLet’s say we have an individual cell in the CA, and let’s call it CELL. The formula for\ncalculating CELL’s state at any given time t is as follows:\nCELL state at time t = f(CELL neighborhood at time t - 1)\nIn other words, a cell’s new state is a function of all the states in the cell’s neighborhood at\nthe previous moment in time (or during the previous generation). We calculate a new state\nvalue by looking at all the previous neighbor states.\nFigure 7.4: A neighborhood is three cells.\nFigure 7.5: The edge cell only has a neighborhood of two.\nFigure 7.6\nChapter 7. Cellular Automata\n326\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 497
  },
  {
    "chunk_full": "Now, in the world of cellular automata, there are many ways we could compute a cell’s state\nfrom a group of cells. Consider blurring an image. (Guess what? Image processing works with\nCA-like rules.) A pixel’s new state (i.e. its color) is the average of all of its neighbors’ colors.\nWe could also say that a cell’s new state is the sum of all of its neighbors’ states. With\nWolfram’s elementary CA, however, we can actually do something a bit simpler and seemingly\nabsurd: We can look at all the possible configurations of a cell and its neighbor and define the\nstate outcome for every possible configuration. It seems ridiculous—wouldn’t there be way too\nmany possibilities for this to be practical? Let’s give it a try.\nWe have three cells, each with a state of 0 or 1. How many possible ways can we configure\nthe states? If you love binary, you’ll notice that three cells define a 3-bit number, and how high\ncan you count with 3 bits? Up to 8. Let’s have a look.\nOnce we have defined all the possible neighborhoods, we need to define an outcome (new\nstate value: 0 or 1) for each neighborhood configuration.\nThe standard Wolfram model is to start generation 0 with all cells having a state of 0 except\nfor the middle cell, which should have a state of 1.\nFigure 7.7\nFigure 7.8\nFigure 7.9\nFigure 7.10\nThe Nature of Code (v1.0)\n327\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 498
  },
  {
    "chunk_full": "Referring to the ruleset above, let’s see how a given cell (we’ll pick the center one) would\nchange from generation 0 to generation 1.\nTry applying the same logic to all of the cells above and fill in the empty cells.\nNow, let’s go past just one generation and color the cells —0 means white, 1 means\nblack—and stack the generations, with each new generation appearing below the previous\none.\nThe low-resolution shape we’re seeing above is the “Sierpiński triangle.” Named after the\nPolish mathematician Wacław Sierpiński, it’s a fractal pattern that we’ll examine in the next\nchapter. That’s right: this incredibly simple system of 0s and 1s, with little neighborhoods of\nthree cells, can generate a shape as sophisticated and detailed as the Sierpiński triangle.\nLet’s look at it again, only with each cell a single pixel wide so that the resolution is much\nhigher.\nFigure 7.11\nFigure 7.12: Rule 90\nChapter 7. Cellular Automata\n328\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 499
  },
  {
    "chunk_full": "This particular result didn’t happen by accident. I picked this set of rules because of the\npattern it generates. Take a look at Figure 7.8 one more time. Notice how there are eight\npossible neighborhood configurations; we therefore define a “ruleset” as a list of 8 bits.\nSo this particular rule can be illustrated as follows:\nEight 0s and 1s means an 8-bit number. How many combinations of eight 0s and 1s are there?\n256. This is just like how we define the components of an RGB color. We get 8 bits for red,\ngreen, and blue, meaning we make colors with values from 0 to 255 (256 possibilities).\nIn terms of a Wolfram elementary CA, we have now discovered that there are 256 possible\nrulesets. The above ruleset is commonly referred to as “Rule 90” because if you convert the\nbinary sequence—01011010—to a decimal number, you’ll get the integer 90. Let’s try looking at\nthe results of another ruleset.\nFigure 7.13: Rule 90\nFigure 7.14: Rule 90\nThe Nature of Code (v1.0)\n329\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 500
  },
  {
    "chunk_full": "As we can now see, the simple act of\ncreating a CA and defining a ruleset does\nnot guarantee visually interesting results.\nOut of all 256 rulesets, only a handful\nproduce compelling outcomes. However,\nit’s quite incredible that even one of these\nrulesets for a one-dimensional CA with only\ntwo possible states can produce the\npatterns we see every day in nature (see\nFigure 7.16), and it demonstrates how\nvaluable these systems can be in\nsimulation and pattern generation.\nBefore we go too far down the road of how\nWolfram classifies the results of varying\nrulesets, let’s look at how we actually build\na Processing sketch that generates the\nWolfram CA and visualizes it onscreen.\nFigure 7.15: Rule 222\nFigure 7.16: A Textile Cone Snail (Conus textile),\nCod Hole, Great Barrier Reef, Australia, 7\nAugust 2005. Photographer: Richard Ling\nrichard@research.canon.com.au\n7.3 How to Program an Elementary CA\n7.3 How to Program an Elementary CA\nYou may be thinking: “OK, I’ve got this cell thing. And the cell thing has some properties,\nlike a state, what generation it’s on, who its neighbors are, where it lives pixel-wise on the\nscreen. And maybe it has some functions: it can display itself, it can generate its new state,\netc.” This line of thinking is an excellent one and would likely lead you to write some code\nlike this:\nclass Cell {\n}\nChapter 7. Cellular Automata\n330\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 501
  },
  {
    "chunk_full": "This line of thinking, however, is not the road we will first travel. Later in this chapter, we will\ndiscuss why an object-oriented approach could prove valuable in developing a CA simulation,\nbut to begin, we can work with a more elementary data structure. After all, what is an\nelementary CA but a list of 0s and 1s? Certainly, we could describe the following CA\ngeneration using an array:\nTo draw that array, we simply check if we’ve got a 0 or a 1 and create a fill accordingly.\nNow that we have the array to describe the cell states of a given generation (which we’ll\nultimately consider the “current” generation), we need a mechanism by which to compute the\nnext generation. Let’s think about the pseudocode of what we are doing at the moment.\nFor every cell in the array:\nFor every cell in the array:\n•\nTake a look at the neighborhood states: left, middle, right.\nTake a look at the neighborhood states: left, middle, right.\n•\nLook up the new value for the cell state according to some ruleset.\nLook up the new value for the cell state according to some ruleset.\n•\nSet the cell’s state to that new value.\nSet the cell’s state to that new value.\nThis may lead you to write some code like this:\nFigure 7.17\nint[] cells = {1,0,1,0,0,0,0,1,0,1,1,1,0,0,0,1,1,1,0,0};\nLoop through every cell.\nfor (int i = 0; i < cells.length; i++) {\nif (cells[i] == 0) fill(255);\nCreate a fill based on its state (0 or 1).\nelse fill(0);\nstroke(0);\nrect(i*50,0,50,50);\n}\nFor every cell in the array...\nfor (int i = 0; i < cells.length; i++) {\n...take a look at the neighborhood.\nint left\n= cell[i-1];\nint middle = cell[i];\nint right\n= cell[i+1];\nLook up the new value according to the\nrules.\nint newstate = rules(left,middle,right);\nThe Nature of Code (v1.0)\n331\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 502
  },
  {
    "chunk_full": "We’re fairly close to getting this right, but we’ve made one minor blunder and one major\nblunder in the above code. Let’s talk about what we’ve done well so far.\nNotice how easy it is to look at a cell’s neighbors. Because an array is an ordered list of\ndata, we can use the fact that the indices are numbered to know which cells are next to\nwhich cells. We know that cell number 15, for example, has cell 14 to its left and 16 to its\nright. More generally, we can say that for any cell i, its neighbors are i-1 and i+1.\nWe’re also farming out the calculation of a new state value to some function called rules().\nObviously, we’re going to have to write this function ourselves, but the point we’re making\nhere is modularity. We have a basic framework for the CA in this function, and if we later\nwant to change how the rules operate, we don’t have to touch that framework; we can\nsimply rewrite the rules() function to compute the new states differently.\nSo what have we done wrong? Let’s talk through how the code will execute. First, we look\nat cell index i equals 0. Now let’s look at 0’s neighbors. Left is index -1. Middle is index 0.\nAnd right is index 1. However, our array by definition does not have an element with the\nindex -1. It starts with 0. This is a problem we’ve alluded to before: the edge cases.\nHow do we deal with the cells on the edge who don’t have a neighbor to both their left and\ntheir right? Here are three possible solutions to this problem:\n1.\nEdges remain constant.\nEdges remain constant. This is perhaps the simplest solution. We never bother to\nevaluate the edges and always leave their state value constant (0 or 1).\n2.\nEdges wrap around.\nEdges wrap around. Think of the CA as a strip of paper and turn that strip of\npaper into a ring. The cell on the left edge is a neighbor of the cell on the right\nand vice versa. This can create the appearance of an infinite grid and is probably\nthe most used solution.\n3.\nEdges have different neighborhoods and rules.\nEdges have different neighborhoods and rules. If we wanted to, we could treat\nthe edge cells differently and create rules for cells that have a neighborhood of\ntwo instead of three. You may want to do this in some circumstances, but in our\ncase, it’s going to be a lot of extra lines of code for little benefit.\nTo make the code easiest to read and understand right now, we’ll go with option #1 and just\nskip the edge cases, leaving their values constant. This can be accomplished by starting the\nloop one cell later and ending one cell earlier:\nSet the cell’s state to the new value.\ncell[i] = newstate;\n}\nChapter 7. Cellular Automata\n332\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 503
  },
  {
    "chunk_full": "There’s one more problem we have to fix before we’re done. It’s subtle and you won’t get a\ncompilation error; the CA just won’t perform correctly. However, identifying this problem is\nabsolutely fundamental to the techniques behind programming CA simulations. It all lies in this\nline of code:\nThis seems like a perfectly innocent line. After all, we’ve computed the new state value and\nwe’re simply giving the cell its new state. But in the next iteration, you’ll discover a massive\nbug. Let’s say we’ve just computed the new state for cell #5. What do we do next? We\ncalculate the new state value for cell #6.\nCell #6, generation 0 = some state, 0 or 1\nCell #6, generation 1 = a function of states for cell #5\ncell #5, cell #6, and cell #7 at *generation 0*\nNotice how we need the value of cell #5 at generation 0 in order to calculate cell #6’s new\nstate at generation 1? A cell’s new state is a function of the previous neighbor states. Do we\nknow cell #5’s value at generation 0? Remember, Processing just executes this line of code\nfor i = 5.\nOnce this happens, we no longer have access to cell #5’s state at generation 0, and cell index\n5 is storing the value for generation 1. We cannot overwrite the values in the array while we\nare processing the array, because we need those values to calculate the new values. A\nsolution to this problem is to have two arrays, one to store the current generation states and\none for the next generation states.\nA loop that ignores the first and last cell\nfor (int i = 1; i < cells.length-1; i++) {\nint left\n= cell[i-1];\nint middle = cell[i];\nint right\n= cell[i+1];\nint newstate = rules(left,middle,right);\ncell[i] = newstate;\n}\ncell[i] = newstate;\ncell[i] = newstate;\nAnother array to store the states for the next\ngeneration.\nint[] newcells = new int[cells.length];\nfor (int i = 1; i < cells.length-1; i++) {\nLook at the states from the current array.\nint left\n= cell[i-1];\nint middle = cell[i];\nint right\n= cell[i+1];\nint newstate = rules(left,middle,right);\nSaving the new state in the new array\nnewcells[i] = newstate;\n}\nThe Nature of Code (v1.0)\n333\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 504
  },
  {
    "chunk_full": "Once the entire array of values is processed, we can then discard the old array and set it\nequal to the new array of states.\nWe’re almost done. The above code is complete except for the fact that we haven’t yet\nwritten the rules() function that computes the new state value based on the neighborhood\n(left, middle, and right cells). We know that function needs to return an integer (0 or 1) as\nwell as receive three arguments (for the three neighbors).\nNow, there are many ways we could write this function, but I’d like to start with a long-\nwinded one that will hopefully provide a clear illustration of what we are doing.\nLet’s first establish how we are storing the ruleset. The ruleset, if you remember from the\nprevious section, is a series of 8 bits (0 or 1) that defines that outcome for every possible\nneighborhood configuration.\nWe can store this ruleset in Processing as an array.\nAnd then say:\nIf left, middle, and right all have the state 1, then that matches the configuration 111 and the\nnew state should be equal to the first value in the ruleset array. We can now duplicate this\nstrategy for all eight possibilities.\nThe new generation becomes the current\ngeneration.\ncells = newcells;\nFunction receives 3 ints and returns 1.\nint rules (int a, int b, int c) {\nFigure 7.14 (repeated)\nint[] ruleset = {0,1,0,1,1,0,1,0};\nif (a == 1 && b == 1 && c == 1) return ruleset[0];\nChapter 7. Cellular Automata\n334\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 505
  },
  {
    "chunk_full": "I like having the example written as above because it describes line by line exactly what is\nhappening for each neighborhood configuration. However, it’s not a great solution. After all,\nwhat if we design a CA that has 4 possible states (0-3) and suddenly we have 64 possible\nneighborhood configurations? With 10 possible states, we have 1,000 configurations. Certainly\nwe don’t want to type in 1,000 lines of code!\nAnother solution, though perhaps a bit more difficult to follow, is to convert the neighborhood\nconfiguration (a 3-bit number) into a regular integer and use that value as the index into the\nruleset array. This can be done in Java like so.\nThere’s one tiny problem with this solution, however. Let’s say we are implementing rule 222:\nAnd we have the neighborhood “111”. The resulting state is equal to ruleset index 0, as we see\nin the first way we wrote the function.\nIf we convert “111” to a decimal number, we get 7. But we don’t want ruleset[7]; we want\nruleset[0]. For this to work, we need to write the ruleset with the bits in reverse order, i.e.\nint rules (int a, int b, int c) {\nif\n(a == 1 && b == 1 && c == 1) return ruleset[0];\nelse if (a == 1 && b == 1 && c == 0) return ruleset[1];\nelse if (a == 1 && b == 0 && c == 1) return ruleset[2];\nelse if (a == 1 && b == 0 && c == 0) return ruleset[3];\nelse if (a == 0 && b == 1 && c == 1) return ruleset[4];\nelse if (a == 0 && b == 1 && c == 0) return ruleset[5];\nelse if (a == 0 && b == 0 && c == 1) return ruleset[6];\nelse if (a == 0 && b == 0 && c == 0) return ruleset[7];\nFor this function to be valid, we have to\nmake sure something is returned in cases\nwhere the states do not match one of the\neight possibilities. We know this is\nimpossible given the rest of our code, but\nProcessing does not.\nreturn 0;\n}\nint rules (int a, int b, int c) {\nA quick way to join three bits into a String\nString s = \"\" + a + b + c;\nThe second argument ‘2’ indicates that we\nintend to parse a binary number (base 2).\nint index = Integer.parseInt(s,2);\nreturn ruleset[index];\n}\nRule 222\nint[] ruleset = {1,1,0,1,1,1,1,0};\nif (a == 1 && b == 1 && c == 1) return ruleset[0];\nThe Nature of Code (v1.0)\n335\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 506
  },
  {
    "chunk_full": "So far in this section, we’ve written everything we need to compute the generations for a\nWolfram elementary CA. Let’s take a moment to organize the above code into a class, which\nwill ultimately help in the design of our overall sketch.\nRule 222 in “reverse” order\nint[] ruleset = {0,1,1,1,1,0,1,1};\nclass CA {\nWe need an array for the cells and one for\nthe rules.\nint[] cells;\nint[] ruleset;\nCA() {\ncells = new int[width];\nArbitrarily starting with rule 90\nruleset = {0,1,0,1,1,0,1,0};\nfor (int i = 0; i < cells.length; i++) {\ncells[i] = 0;\n}\nAll cells start with state 0, except the center\ncell has state 1.\ncells[cells.length/2] = 1;\n}\nvoid generate() {\nCompute the next generation.\nint[] nextgen = new int[cells.length];\nfor (int i = 1; i < cells.length-1; i++) {\nint left\n= cells[i-1];\nint me\n= cells[i];\nint right\n= cells[i+1];\nnextgen[i] = rules(left, me, right);\n}\ncells = nextgen;\n}\nLook up a new state from the ruleset.\nint rules (int a, int b, int c) {\nString s = \"\" + a + b + c;\nint index = Integer.parseInt(s,2);\nreturn ruleset[index];\n}\n}\n7.4 Drawing an Elementary CA\n7.4 Drawing an Elementary CA\nWhat’s missing? Presumably, it’s our intention to display cells and their states in visual form.\nAs we saw earlier, the standard technique for doing this is to stack the generations one on\ntop of each other and draw a rectangle that is black (for state 1) or white (for state 0).\nChapter 7. Cellular Automata\n336\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 507
  },
  {
    "chunk_full": "Before we implement this particular visualization, I’d like to point out two things.\nOne, this visual interpretation of the data is completely literal. It’s useful for demonstrating the\nalgorithms and results of Wolfram’s elementary CA, but it shouldn’t necessarily drive your own\npersonal work. It’s rather unlikely that you are building a project that needs precisely this\nalgorithm with this visual style. So while learning to draw the CA in this way will help you\nunderstand and implement CA systems, this skill should exist only as a foundation.\nSecond, the fact that we are visualizing a one-dimensional CA with a two-dimensional image\ncan be confusing. It’s very important to remember that this is not a 2D CA. We are simply\nchoosing to show a history of all the generations stacked vertically. This technique creates a\ntwo-dimensional image out of many instances of one-dimensional data. But the system itself is\none-dimensional. Later, we are going to look at an actual 2D CA (the Game of Life) and\ndiscuss how we might choose to display such a system.\nThe good news is that drawing the CA is not particularly difficult. Let’s begin by looking at\nhow we would render a single generation. Assume we have a Processing window 600 pixels\nwide and we want each cell to be a 10x10 square. We therefore have a CA with 60 cells. Of\ncourse, we can calculate this value dynamically.\nAssuming we’ve gone through the process of generating the cell states (which we did in the\nprevious section), we can now loop through the entire array of cells, drawing a black cell when\nthe state is 1 and a white one when the state is 0.\nFigure 7.12 (repeated)\nint w = 10;\nHow many cells fit across given a certain\nwidth\nint[] cells = new int[width/w];\nfor (int i = 0; i < cells.length; i++) {\nBlack or white fill?\nif (cells[i] == 1) fill(0);\nelse\nfill(255);\nThe Nature of Code (v1.0)\n337\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 508
  },
  {
    "chunk_full": "In truth, we could optimize the above by having a white background and only drawing when\nthere is a black cell (saving us the work of drawing many white squares), but in most cases\nthis solution is good enough (and necessary for other more sophisticated designs with\nvarying colors, etc.) Also, if we wanted each cell to be represented as a single pixel, we\nwould not want to use Processing’s rect() function, but rather access the pixel array\ndirectly.\nIn the above code, you’ll notice the y-location for each rectangle is 0. If we want the\ngenerations to be drawn next to each other, with each row of cells marking a new\ngeneration, we’ll also need to compute a y-location based on how many iterations of the CA\nwe’ve executed. We could accomplish this by adding a “generation” variable (an integer) to\nour CA class and incrementing it each time through generate(). With these additions, we\ncan now look at the CA class with all the features for both computing and drawing the CA.\nExample 7.1: Wolfram elementary cellular automata\nNotice how the x-location is the cell index\ntimes the cell width. In the above scenario,\nthis would give us cells located at x equals\n0, 10, 20, 30, all the way up to 600.\nrect(i*w, 0, w, w);\n}\nclass CA {\nint[] cells;\nint[] ruleset;\nint w = 10;\nThe CA should keep track of how many\ngenerations.\nint generation = 0;\nCA() {\ncells = new int[width/w];\nruleset = {0,1,0,1,1,0,1,0};\ncells[cells.length/2] = 1;\n}\nChapter 7. Cellular Automata\n338\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 509
  },
  {
    "chunk_full": "Function to compute the next generation\nvoid generate() {\nint[] nextgen = new int[cells.length];\nfor (int i = 1; i < cells.length-1; i++) {\nint left\n= cells[i-1];\nint me\n= cells[i];\nint right\n= cells[i+1];\nnextgen[i] = rules(left, me, right);\n}\ncells = nextgen;\nIncrement the generation counter.\ngeneration++;\n}\nint rules(int a, int b, int c) {\nString s = \"\" + a + b + c;\nint index = Integer.parseInt(s,2);\nreturn ruleset[index];\n}\nfor (int i = 0; i < cells.length; i++) {\nif (cells[i] == 1) fill(0);\nelse\nfill(255);\nSet the y-location according to the\ngeneration.\nrect(i*w, generation*w, w, w);\n}\n}\nExpand Example 7.1 to have the following feature: when the CA reaches the bottom of\nthe Processing window, the CA starts over with a new, random ruleset.\nExercise 7.1\nExercise 7.1\nExamine what patterns occur if you initialize the first generation with each cell having a\nrandom state.\nExercise 7.2\nExercise 7.2\nVisualize the CA in a non-traditional way. Break all the rules you can; don’t feel tied to\nusing squares on a perfect grid with black and white.\nExercise 7.3\nExercise 7.3\nThe Nature of Code (v1.0)\n339\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 510
  },
  {
    "chunk_full": "Create a visualization of the CA that scrolls upwards as the generations increase so\nthat you can view the generations to “infinity.” Hint: instead of keeping track of only\none generation at a time, you’ll need to store a history of generations, always adding\na new one and deleting the oldest one in each frame.\nExercise 7.4\nExercise 7.4\n7.5 Wolfram Classification\n7.5 Wolfram Classification\nBefore we move on to looking at CA in two dimensions, it’s worth taking a brief look at\nWolfram’s classification for cellular automata. As we noted earlier, the vast majority of\nelementary CA rulesets produce uninspiring results, while some result in wondrously\ncomplex patterns like those found in nature. Wolfram has divided up the range of outcomes\ninto four classes:\nClass 1: Uniformity.\nClass 1: Uniformity. Class 1 CAs end up, after some number of generations, with every cell\nconstant. This is not terribly exciting to watch. Rule 222 (above) is a class 1 CA; if you run it\nfor enough generations, every cell will eventually become and remain black.\nFigure 7.18: Rule 222\nFigure 7.19: Rule 190\nChapter 7. Cellular Automata\n340\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 511
  },
  {
    "chunk_full": "Class 2: Repetition.\nClass 2: Repetition. Like class 1 CAs, class 2 CAs remain stable, but the cell states are not\nconstant. Rather, they oscillate in some regular pattern back and forth from 0 to 1 to 0 to 1 and\nso on. In rule 190 (above), each cell follows the sequence 11101110111011101110.\nClass 3: Random.\nClass 3: Random. Class 3 CAs appear random and have no easily discernible pattern. In fact,\nrule 30 (above) is used as a random number generator in Wolfram’s Mathematica software.\nAgain, this is a moment where we can feel amazed that such a simple system with simple rules\ncan descend into a chaotic and random pattern.\nClass 4: Complexity.\nClass 4: Complexity. Class 4 CAs can be thought of as a mix between class 2 and class 3.\nOne can find repetitive, oscillating patterns inside the CA, but where and when these patterns\nappear is unpredictable and seemingly random. Class 4 CAs exhibit the properties of complex\nsystems that we described earlier in this chapter and in Chapter 6. If a class 3 CA wowed you,\nthen a class 4 like Rule 110 above should really blow your mind.\nFigure 7.20: Rule 30\nFigure 7.21: Rule 110\nCreate a Processing sketch that saves an image for every possible ruleset. Can you\nclassify them?\nExercise 7.5\nExercise 7.5\nThe Nature of Code (v1.0)\n341\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 512
  },
  {
    "chunk_full": "7.6 The Game of Life\n7.6 The Game of Life\nThe next step we are going to take is to move from a one-dimensional CA to a two-\ndimensional one. This will introduce some additional complexity; each cell will have a bigger\nneighborhood, but that will open up the door to a range of possible applications. After all,\nmost of what we do in computer graphics lives in two dimensions, and this chapter will\ndemonstrate how to apply CA thinking to what we draw in our Processing sketches.\nIn 1970, Martin Gardner wrote an article in Scientific American that documented\nmathematician John Conway’s new “Game of Life,” describing it as “recreational”\nmathematics and suggesting that the reader get out a chessboard and some checkers and\n“play.” While the Game of Life has become something of a computational cliché (make note\nof the myriad projects that display the Game of Life on LEDs, screens, projection surfaces,\netc.), it is still important for us to build it from scratch. For one, it provides a good\nopportunity to practice our skills with two-dimensional arrays, object orientation, etc. But\nperhaps more importantly, its core principles are tied directly to our core goals—simulating\nthe natural world with code. Though we may want to avoid simply duplicating it without a\ngreat deal of thought or care, the algorithm and its technical implementation will provide us\nwith the inspiration and foundation to build simulations that exhibit the characteristics and\nbehaviors of biological systems of reproduction.\nUnlike von Neumann, who created an extraordinarily complex system of states and rules,\nConway wanted to achieve a similar “lifelike” result with the simplest set of rules possible.\nMartin Gardner outlined Conway’s goals as follows:\n“1. There should be no initial pattern for which there is a simple proof that the population\ncan grow without limit. 2. There should be initial patterns that apparently do grow\nwithout limit. 3. There should be simple initial patterns that grow and change for a\nconsiderable period of time before coming to an end in three possible ways: fading away\ncompletely (from overcrowding or becoming too sparse), settling into a stable\nconfiguration that remains unchanged thereafter, or entering an oscillating phase in\nwhich they repeat an endless cycle of two or more periods.”\n—Martin Gardner, Scientific American (http://www.ibiblio.org/lifepatterns/\noctober1970.html) 223 (October 1970): 120-123.\nThe above might sound a bit cryptic, but it essentially describes a Wolfram class 4 CA. The\nCA should be patterned but unpredictable over time, eventually settling into a uniform or\noscillating state. In other words, though Conway didn’t use this terminology, it should have\nall those properties of a complex system that we keep mentioning.\nLet’s look at how the Game of Life works. It won’t take up too much time or space, since\nwe’ve covered the basics of CA already.\nChapter 7. Cellular Automata\n342\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 513
  },
  {
    "chunk_full": "First, instead of a line of cells, we now have\na two-dimensional matrix of cells. As with\nthe elementary CA, the possible states are 0\nor 1. Only in this case, since we’re talking\nabout “life,\" 0 means dead and 1 means\nalive.\nThe cell’s neighborhood has also expanded.\nIf a neighbor is an adjacent cell, a\nneighborhood is now nine cells instead of\nthree.\nWith three cells, we had a 3-bit number or\neight possible configurations. With nine\ncells, we have 9 bits, or 512 possible\nneighborhoods. In most cases, it would be\nimpractical to define an outcome for every\nsingle possibility. The Game of Life gets\naround this problem by defining a set of rules according to general characteristics of the\nneighborhood. In other words, is the neighborhood overpopulated with life? Surrounded by\ndeath? Or just right? Here are the rules of life.\n1.\nDeath.\nDeath. If a cell is alive (state = 1) it will die (state becomes 0) under the following\ncircumstances.\n◦\nOverpopulation:\nOverpopulation: If the cell has four or more alive neighbors, it dies.\n◦\nLoneliness:\nLoneliness: If the cell has one or fewer alive neighbors, it dies.\n2.\nBirth.\nBirth. If a cell is dead (state = 0) it will come to life (state becomes 1) if it has exactly\nthree alive neighbors (no more, no less).\n3.\nStasis.\nStasis. In all other cases, the cell state does not change. To be thorough, let’s\ndescribe those scenarios.\n◦\nStaying Alive:\nStaying Alive: If a cell is alive and has exactly two or three live neighbors,\nit stays alive.\n◦\nStaying Dead:\nStaying Dead: If a cell is dead and has anything other than three live\nneighbors, it stays dead.\nLet’s look at a few examples.\nFigure 7.22\nThe Nature of Code (v1.0)\n343\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 514
  },
  {
    "chunk_full": "With the elementary CA, we were able to look at all the generations next to each other,\nstacked as rows in a 2D grid. With the Game of Life, however, the CA itself is in two\ndimensions. We could try creating an elaborate 3D visualization of the results and stack all\nthe generations in a cube structure (and in fact, you might want to try this as an exercise).\nNevertheless, the typical way the Game of Life is displayed is to treat each generation as a\nsingle frame in an animation. So instead of viewing all the generations at once, we see them\none at a time, and the result resembles rapidly growing bacteria in a petri dish.\nOne of the exciting aspects of the Game of Life is that there are initial patterns that yield\nintriguing results. For example, some remain static and never change.\nThere are patterns that oscillate back and forth between two states.\nFigure 7.23\nFigure 7.24\nFigure 7.25\nChapter 7. Cellular Automata\n344\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 515
  },
  {
    "chunk_full": "And there are also patterns that from generation to generation move about the grid. (It’s\nimportant to note that the cells themselves aren’t actually moving, although we see the\nappearance of motion in the result as the cells turn on and off.)\nIf you are interested in these patterns, there are several good “out of the box” Game of Life\ndemonstrations online that allow you to configure the CA’s initial state and watch it run at\nvarying speeds. Two examples you might want to examine are:\n•\nExploring Emergence (http://llk.media.mit.edu/projects/emergence/) by Mitchel\nResnick and Brian Silverman, Lifelong Kindergarten Group, MIT Media Laboratory\n•\nConway’s Game of Life (http://stevenklise.github.com/ConwaysGameOfLife) by\nSteven Klise (uses Processing.js!)\nFor the example we’ll build from scratch in the next section, it will be easier to simply\nrandomly set the states for each cell.\nFigure 7.26\n7.7 Programming the Game of Life\n7.7 Programming the Game of Life\nNow we just need to extend our code from the Wolfram CA to two dimensions. We used a\none-dimensional array to store the list of cell states before, and for the Game of Life, we can\nuse a two-dimensional array (http://www.processing.org/learning/2darray/).\nWe’ll begin by initializing each cell of the board with a random state: 0 or 1.\nint[][] board = new int[columns][rows];\nfor (int x = 0; x < columns; x++) {\nfor (int y = 0; y < rows; y++) {\nInitialize each cell with a 0 or 1.\ncurrent[x][y] = int(random(2));\n}\n}\nThe Nature of Code (v1.0)\n345\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 516
  },
  {
    "chunk_full": "And to compute the next generation, just as before, we need a fresh 2D array to write to as\nwe analyze each cell’s neighborhood and calculate a new state.\nOK. Before we can sort out how to actually\ncalculate the new state, we need to know\nhow we can reference each cell’s neighbor.\nIn the case of the 1D CA, this was simple: if\na cell index was i, its neighbors were i-1\nand i+1. Here each cell doesn’t have a\nsingle index, but rather a column and row\nindex: x,y. As shown in Figure 7.27, we can\nsee that its neighbors are: (x-1,y-1) (x,y-1),\n(x+1,y-2), (x-1,y), (x+1,y), (x-1,y+1), (x,y+1), and\n(x+1,y+1).\nAll of the Game of Life rules operate by\nknowing how many neighbors are alive. So\nif we create a neighbor counter variable\nand increment it each time we find a\nneighbor with a state of 1, we’ll have the\ntotal of live neighbors.\nint[][] next = new int[columns][rows];\nfor (int x = 0; x < columns; x++) {\nfor (int y = 0; y < rows; y++) {\nWe need a new state for each cell.\nnext[x][y] = _______________?;\n}\n}\nFigure 7.27\nint neighbors = 0;\nTop row of neighbors\nif (board[x-1][y-1] == 1) neighbors++;\nif (board[x\n][y-1] == 1) neighbors++;\nif (board[x+1][y-1] == 1) neighbors++;\nMiddle row of neighbors (note we don’t\ncount self)\nif (board[x-1][y]\n== 1) neighbors++;\nif (board[x+1][y]\n== 1) neighbors++;\nBottom row of neighbors\nif (board[x-1][y+1] == 1) neighbors++;\nif (board[x\n][y+1] == 1) neighbors++;\nif (board[x+1][y+1] == 1) neighbors++;\nChapter 7. Cellular Automata\n346\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 517
  },
  {
    "chunk_full": "And again, just as with the Wolfram CA, we find ourselves in a situation where the above is a\nuseful and clear way to write the code for teaching purposes, allowing us to see every step\n(each time we find a neighbor with a state of one, we increase a counter). Nevertheless, it’s a\nbit silly to say, “If the cell state equals one, add one to a counter” when we could just say,\n“Add the cell state to a counter.” After all, if the state is only a 0 or 1, the sum of all the\nneighbors’ states will yield the total number of live cells. Since the neighbors are arranged in a\nmini 3x3 grid, we can add them all up with another loop.\nOf course, we’ve made a mistake in the code above. In the Game of Life, the cell itself does\nnot count as one of the neighbors. We could use a conditional to skip adding the state when\nboth i and j equal 0, but another option would be to just subtract the cell state once we’ve\nfinished the loop.\nFinally, once we know the total number of live neighbors, we can decide what the cell’s new\nstate should be according to the rules: birth, death, or stasis.\nPutting this all together, we have:\nfor (int i = -1; i <= 1; i++) {\nfor (int j = -1; j <= 1; j++) {\nAdd up all the neighbors’ states.\nneighbors += board[x+i][y+j];\n}\n}\nWhoops! Subtract the cell’s state, which we\ndon’t want in the total.\nneighbors -= board[x][y];\nIf it is alive and has less than 2 live\nneighbors, it dies from loneliness.\nif\n((board[x][y] == 1) && (neighbors <\n2)) {\nnext[x][y] = 0;\n}\nIf it is alive and has more than 3 live\nneighbors, it dies from overpopulation.\nelse if ((board[x][y] == 1) && (neighbors >\n3)) {\nnext[x][y] = 0;\n}\nIf it is dead and has exactly 3 live\nneighbors, it is born!\nelse if ((board[x][y] == 0) && (neighbors == 3)) {\nnext[x][y] = 1;\n}\nIn all other cases, its state remains the\nsame.\nelse {\nnext[x][y] = board[x][y];\n}\nThe next board\nint[][] next = new int[columns][rows];\nThe Nature of Code (v1.0)\n347\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 518
  },
  {
    "chunk_full": "Finally, once the next generation is calculated, we can employ the same method we used to\ndraw the Wolfram CA—a square for each spot, white for off, black for on.\nExample 7.2: Game of Life\nLooping but skipping the edge cells\nfor (int x = 1; x < columns-1; x++) {\nfor (int y = 1; y < rows-1; y++) {\nAdd up all the neighbor states to calculate\nthe number of live neighbors.\nint neighbors = 0;\nfor (int i = -1; i <= 1; i++) {\nfor (int j = -1; j <= 1; j++) {\nneighbors += board[x+i][y+j];\n}\n}\nCorrect by subtracting the cell state itself.\nneighbors -= board[x][y];\nThe rules of life!\nif\n((board[x][y] == 1) && (neighbors <\n2)) next[x][y] = 0;\nelse if ((board[x][y] == 1) && (neighbors >\n3)) next[x][y] = 0;\nelse if ((board[x][y] == 0) && (neighbors == 3)) next[x][y] = 1;\nelse next[x][y] = board[x][y];\n}\n}\nThe 2D array “next” is now the current\nboard.\nboard = next;\nfor ( int i = 0; i < columns;i++) {\nfor ( int j = 0; j < rows;j++) {\nBlack when state = 1\nif ((board[i][j] == 1)) fill(0);\nChapter 7. Cellular Automata\n348\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 519
  },
  {
    "chunk_full": "White when state = 0\nelse fill(255);\nstroke(0);\nrect(i*w, j*w, w, w);\n}\n}\nCreate a Game of Life simulation that allows you to manually configure the grid by\ndrawing or with specific known patterns.\nExercise 7.6\nExercise 7.6\nImplement “wrap-around” for the Game of Life so that cells on the edges have\nneighbors on the opposite side of the grid.\nExercise 7.7\nExercise 7.7\nWhile the above solution (Example 7.2) is convenient, it is not particularly memory-\nefficient. It creates a new 2D array for every frame of animation! This matters very little\nfor a Processing desktop application, but if you were implementing the Game of Life on\na microcontroller or mobile device, you’d want to be more careful. One solution is to\nhave only two arrays and constantly swap them, writing the next set of states into\nwhichever one isn’t the current array. Implement this particular solution.\nExercise 7.8\nExercise 7.8\n7.8 Object-Oriented Cells\n7.8 Object-Oriented Cells\nOver the course of the previous six chapters, we’ve slowly built examples of systems of\nobjects with properties that move about the screen. And in this chapter, although we’ve been\ntalking about a “cell” as if it were an object, we actually haven’t been using any object\norientation in our code (other than a class to describe the CA system as a whole). This has\nworked because a cell is such an enormously simple object (a single bit). However, in a\nmoment, we are going to discuss some ideas for further developing CA systems, many of\nwhich involve keeping track of multiple properties for each cell. For example, what if a cell\nneeded to remember its last ten states? Or what if we wanted to apply some of our motion\nand physics thinking to a CA and have the cells move about the window, dynamically\nchanging their neighbors from frame to frame?\nThe Nature of Code (v1.0)\n349\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 520
  },
  {
    "chunk_full": "To accomplish any of these ideas (and more), it would be helpful to see how we might treat\na cell as an object with multiple properties, rather than as a single 0 or 1. To show this, let’s\njust recreate the Game of Life simulation. Only instead of:\nLet’s have:\nwhere Cell is a class we will write. What are the properties of a Cell object? In our Game\nof Life example, each cell has a location and size, as well as a state.\nIn the non-OOP version, we used a separate 2D array to keep track of the states for the\ncurrent and next generation. By making a cell an object, however, each cell could keep\ntrack of both states. In this case, we’ll think of the cell as remembering its previous state (for\nwhen new states need to be computed).\nThis allows us to visualize more information about what the state is doing. For example, we\ncould choose to color a cell differently if its state has changed. For example:\nExample 7.3: Game of Life OOP\nint[][] board;\nCell[][] board;\nclass Cell {\nLocation and size\nfloat x, y;\nfloat w;\nWhat is the cell’s state?\nint state;\nWhat was its previous state?\nint previous;\nvoid display() {\nChapter 7. Cellular Automata\n350\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 521
  },
  {
    "chunk_full": "Not much else about the code (at least for our purposes here) has to change. The neighbors\ncan still be counted the same way; the difference is that we now need to refer to the object’s\nstate variables as we loop through the 2D array.\nIf the cell is born, color it blue!\nif (previous == 0 && state == 1) fill(0,0,255);\nelse if (state == 1) fill(0);\nelse if (previous == 1 && state == 0) fill(255,0,0);\nelse fill(255);\nrect(x, y, w, w);\n}\nIf the cell dies, color it red!\nfor (int x = 1; x < columns-1; x++) {\nfor (int y = 1; y < rows-1; y++) {\nint neighbors = 0;\nfor (int i = -1; i <= 1; i++) {\nfor (int j = -1; j <= 1; j++) {\nUse the previous state when tracking\nneighbors.\nneighbors += board[x+i][y+j].previous;\n}\n}\nneighbors -= board[x][y].previous;\nWe are calling a function newState() to\nassign a new state to each cell.\nif\n((board[x][y].state == 1) && (neighbors <\n2)) board[x][y].newState(0);\nelse if ((board[x][y].state == 1) && (neighbors >\n3)) board[x][y].newState(0);\nelse if ((board[x][y].state == 0) && (neighbors == 3)) board[x][y].newState(1);\nelse do nothing!\n}\n}\n7.9 Variations of Traditional CA\n7.9 Variations of Traditional CA\nNow that we have covered the basic concepts, algorithms, and programming strategies\nbehind the most famous 1D and 2D cellular automata, it’s time to think about how you might\ntake this foundation of code and build on it, developing creative applications of CAs in your\nown work. In this section, we’ll talk through some ideas for expanding the features of the CA\nexamples. Example answers to each of these exercises can be found on the book website.\n1) Non-rectangular Grids\n1) Non-rectangular Grids. There’s no particular reason why you should limit yourself to having\nyour cells on a rectangular grid. What happens if you design a CA with another type of shape?\nThe Nature of Code (v1.0)\n351\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 522
  },
  {
    "chunk_full": "2) Probabilistic\n2) Probabilistic. The rules of a CA don’t necessarily have to define an exact outcome.\n3) Continuous\n3) Continuous. We’ve looked at examples where the cell’s state can only be a 1 or a 0. But\nwhat if the cell’s state was a floating point number between 0 and 1?\n4) Image Processing\n4) Image Processing. We briefly touched on this earlier, but many image-processing\nalgorithms operate on CA-like rules. Blurring an image is creating a new pixel out of the\naverage of a neighborhood of pixels. Simulations of ink dispersing on paper or water\nrippling over an image can be achieved with CA rules.\nCreate a CA using a grid of hexagons (as below), each with six neighbors.\nExercise 7.9\nExercise 7.9\nRewrite the Game of Life rules as follows:\nOverpopulation: If the cell has four or more alive neighbors, it has a 80% chance of\ndying.\nLoneliness: If the cell has one or fewer alive neighbors, it has a 60% chance of dying.\nEtc.\nExercise 7.10\nExercise 7.10\nAdapt Wolfram elementary CA to have the state be a float. You could define rules\nsuch as, “If the state is greater than 0.5” or “…less than 0.2.”\nExercise 7.11\nExercise 7.11\nChapter 7. Cellular Automata\n352\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 523
  },
  {
    "chunk_full": "5) Historical\n5) Historical. In the Game of Life object-oriented example, we used two variables to keep\ntrack of its state: current and previous. What if you use an array to keep track of a cell’s state\nhistory? This relates to the idea of a “complex adaptive system,” one that has the ability to\nadapt and change its rules over time by learning from its history. We’ll see an example of this\nin Chapter 10: Neural Networks.\n6) Moving cells\n6) Moving cells. In these basic examples, cells have a fixed position on a grid, but you could\nbuild a CA with cells that have no fixed position and instead move about the screen.\n7) Nesting\n7) Nesting. Another feature of complex systems is that they can be nested. Our world tends to\nwork this way: a city is a complex system of people, a person is a complex system of organs,\nan organ is a complex system of cells, and so on and so forth.\nCreate a CA in which a pixel is a cell and a color is its state.\nExercise 7.12\nExercise 7.12\nVisualize the Game of Life by coloring each cell according to how long it’s been alive or\ndead. Can you also use the cell’s history to inform the rules?\nExercise 7.13\nExercise 7.13\nUse CA rules in a flocking system. What if each boid had a state (that perhaps informs\nits steering behaviors) and its neighborhood changed from frame to frame as it moved\ncloser to or further from other boids?\nExercise 7.14\nExercise 7.14\nDesign a CA in which each cell itself is a smaller CA or a system of boids.\nExercise 7.15\nExercise 7.15\nThe Nature of Code (v1.0)\n353\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 524
  },
  {
    "chunk_full": "The Ecosystem Project\nThe Ecosystem Project\nStep 7 Exercise:\nIncorporate cellular automata into your ecosystem. Some possibilities:\n•\nGive each creature a state. How can that state drive their behavior?\nTaking inspiration from CA, how can that state change over time\naccording to its neighbors’ states?\n•\nConsider the ecosystem’s world to be a CA. The creatures move from\ntile to tile. Each tile has a state—is it land? water? food?\n•\nUse a CA to generate a pattern for the design of a creature in your\necosystem.\nChapter 7. Cellular Automata\n354\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 525
  },
  {
    "chunk_full": "Chapter 8. Fractals\nChapter 8. Fractals\n“Pathological monsters! cried the terrified mathematician\nEvery one of them a splinter in my eye\nI hate the Peano Space and the Koch Curve\nI fear the Cantor Ternary Set\nThe Sierpinski Gasket makes me wanna cry\nAnd a million miles away a butterfly flapped its wings\nOn a cold November day a man named Benoit Mandelbrot was born”\n— Jonathan Coulton, lyrics from “Mandelbrot Set”\nOnce upon a time, I took a course in high school called “Geometry.” Perhaps you did too. You\nlearned about shapes in one dimension, two dimensions, and maybe even three. What is the\ncircumference of a circle? The area of a rectangle? The distance between a point and a line?\nCome to think of it, we’ve been studying geometry all along in this book, using vectors to\ndescribe the motion of bodies in Cartesian space. This sort of geometry is generally referred\nto as Euclidean geometry, after the Greek mathematician Euclid.\nFigure 8.1\nThe Nature of Code (v1.0)\n355\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 526
  },
  {
    "chunk_full": "For us nature coders, we have to ask the question: Can we describe our world with\nEuclidean geometry? The LCD screen I’m staring at right now sure looks like a rectangle.\nAnd the plum I ate this morning is circular. But what if I were to look further, and consider\nthe trees that line the street, the leaves that hang off those trees, the lightning from last\nnight’s thunderstorm, the cauliflower I ate for dinner, the blood vessels in my body, and the\nmountains and coastlines that cover land beyond New York City? Most of the stuff you find\nin nature cannot be described by the idealized geometrical forms of Euclidean geometry. So\nif we want to start building computational designs with patterns beyond the simple shapes\nellipse(), rect(), and line(), it’s time for us to learn about the concepts behind and\ntechniques for simulating the geometry of nature: fractals.\n8.1 What Is a Fractal?\n8.1 What Is a Fractal?\nThe term fractal\nfractal (from the Latin fractus, meaning “broken”) was coined by the\nmathematician Benoit Mandelbrot in 1975. In his seminal work “The Fractal Geometry of\nNature,” he defines a fractal as “a rough or fragmented geometric shape that can be split\ninto parts, each of which is (at least approximately) a reduced-size copy of the whole.”\nLet’s illustrate this definition with two simple examples. First, let’s think about a tree\nbranching structure (for which we’ll write the code later):\nFigure 8.2: One of the most well-known and recognizable fractal patterns is named for Benoit\nMandelbrot himself. Generating the Mandelbrot set involves testing the properties of complex\nnumbers after they are passed through an iterative function. Do they tend to infinity? Do they stay\nbounded? While a fascinating mathematical discussion, this “escape-time” algorithm is a less\npractical method for generating fractals than the recursive techniques we’ll examine in this chapter.\nHowever, an example for generating the Mandelbrot set is included in the code examples.\nChapter 8. Fractals\n356\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 527
  },
  {
    "chunk_full": "Notice how the tree in Figure 8.3 has a single root with two branches connected at its end.\nEach one of those branches has two branches at its end and those branches have two\nbranches and so on and so forth. What if we were to pluck one branch from the tree and\nexamine it on its own?\nLooking closely at a given section of the tree, we find that the shape of this branch resembles\nthe tree itself. This is known as self-similarity\nself-similarity; as Mandelbrot stated, each part is a “reduced-\nsize copy of the whole.”\nThe above tree is perfectly symmetrical and the parts are, in fact, exact replicas of the whole.\nHowever, fractals do not have to be perfectly self-similar. Let’s take a look at a graph of the\nstock market (adapted from actual Apple stock data).\nAnd one more.\nFigure 8.3\nFigure 8.4\nFigure 8.5: Graph A\nThe Nature of Code (v1.0)\n357\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 528
  },
  {
    "chunk_full": "In these graphs, the x-axis is time and the y-axis is the stock’s value. It’s not an accident that\nI omitted the labels, however. Graphs of stock market data are examples of fractals because\nthey look the same at any scale. Are these graphs of the stock over one year? One day?\nOne hour? There’s no way for you to know without a label. (Incidentally, graph A shows six\nmonths’ worth of data and graph B zooms into a tiny part of graph A, showing six hours.)\nThis is an example of a stochastic\nstochastic fractal, meaning that it is built out of probabilities and\nrandomness. Unlike the deterministic tree-branching structure, it is statistically self-similar.\nAs we go through the examples in this chapter, we will look at both deterministic and\nstochastic techniques for generating fractal patterns.\nWhile self-similarity is a key trait of fractals, it’s important to realize that self-similarity alone\ndoes not make a fractal. After all, a line is self-similar. A line looks the same at any scale,\nand can be thought of as comprising lots of little lines. But it’s not a fractal. Fractals are\ncharacterized by having a fine structure at small scales (keep zooming into the stock market\ngraph and you’ll continue to find fluctuations) and cannot be described with Euclidean\ngeometry. If you can say “It’s a line!” then it’s not a fractal.\nAnother fundamental component of fractal geometry is recursion. Fractals all have a\nrecursive definition. We’ll start with recursion before developing techniques and code\nexamples for building fractal patterns in Processing.\nFigure 8.6: Graph B\nFigure 8.7\n8.2 Recursion\n8.2 Recursion\nLet’s begin our discussion of recursion by examining the first appearance of fractals in\nmodern mathematics. In 1883, German mathematician George Cantor developed simple\nrules to generate an infinite set:\nChapter 8. Fractals\n358\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 529
  },
  {
    "chunk_full": "There is a feedback loop at work here. Take a single line and break it into two. Then return to\nthose two lines and apply the same rule, breaking each line into two, and now we’re left with\nfour. Then return to those four lines and apply the rule. Now you’ve got eight. This process is\nknown as recursion\nrecursion: the repeated application of a rule to successive results. Cantor was\ninterested in what happens when you apply these rules an infinite number of times. We,\nhowever, are working in a finite pixel space and can mostly ignore the questions and\nparadoxes that arise from infinite recursion. We will instead construct our code in such a way\nthat we do not apply the rules forever (which would cause our program to freeze).\nBefore we implement the Cantor set, let’s take a look at what it means to have recursion in\ncode. Here’s something we’re used to doing all the time—calling a function inside another\nfunction.\nWhat would happen if we called the function we are defining within the function itself? Can\nsomeFunction() call someFunction()?\nIn fact, this is not only allowed, but it’s quite common (and essential to how we will implement\nthe Cantor set). Functions that call themselves are recursive and good for solving certain\nproblems. For example, certain mathematical calculations are implemented recursively; the\nmost common example is factorial.\nThe factorial of any number n, usually written as n!, is defined as:\nn! = n * n – 1 * . . . . * 3 * 2 * 1\n0! = 1\nHere we’ll write a function in Processing that uses a for loop to calculate factorial:\nFigure 8.8: The Cantor set\nvoid someFunction() {\nCalling the function background() in the\ndefinition of someFunction()\nbackground(0);\n}\nvoid someFunction() {\nsomeFunction();\n}\nint factorial(int n) {\nint f = 1;\nThe Nature of Code (v1.0)\n359\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 530
  },
  {
    "chunk_full": "Upon close examination, you’ll notice something interesting about how factorial works. Let’s\nlook at 4! and 3!\n4! = 4 * 3 * 2 * 1\n3! = 3 * 2 * 1\ntherefore. . .\ntherefore. . .\n4! = 4 * 3!\nIn more general terms, for any positive integer n:\nn! = n * (n-1)!\n1! = 1\nWritten out:\nThe factorial of n is defined as n times the factorial of n-1.\nThe definition of factorial\nfactorial includes factorial\nfactorial?! It’s kind of like defining “tired\" as “the feeling\nyou get when you are tired.” This concept of self-reference in functions is an example of\nrecursion. And we can use it to write a factorial function that calls itself.\nIt may look crazy, but it works. Here are the steps that happen when factorial(4) is\ncalled.\nUsing a regular loop to compute factorial\nfor (int i = 0; i < n; i++) {\nf = f * (i+1);\n}\nreturn f;\n}\nint factorial(int n) {\nif (n == 1) {\nreturn 1;\n} else {\nreturn n * factorial(n-1);\n}\n}\nChapter 8. Fractals\n360\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 531
  },
  {
    "chunk_full": "We can apply the same principle to graphics with interesting results, as we will see in many\nexamples throughout this chapter. Take a look at this recursive function.\nExample 8.1: Recursive Circles I\ndrawCircle() draws an ellipse based on a set of parameters that it receives as arguments. It\nthen calls itself with those same parameters, adjusting them slightly. The result is a series of\ncircles, each of which is drawn inside the previous circle.\nFigure 8.9\nvoid drawCircle(int x, int y, float radius) {\nellipse(x, y, radius, radius);\nif(radius > 2) {\nradius *= 0.75f;\nThe drawCircle() function is calling itself\nrecursively.\ndrawCircle(x, y, radius);\n}\n}\nThe Nature of Code (v1.0)\n361\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 532
  },
  {
    "chunk_full": "Notice that the above function only recursively calls itself if the radius is greater than 2. This\nis a crucial point. As with iteration, all recursive functions must have an exit condition! You\nlikely are already aware that all for and while loops must include a boolean expression\nthat eventually evaluates to false, thus exiting the loop. Without one, the program would\ncrash, caught inside of an infinite loop. The same can be said about recursion. If a recursive\nfunction calls itself forever and ever, you’ll be most likely be treated to a nice frozen screen.\nThis circles example is rather trivial; it could easily be achieved through simple iteration.\nHowever, for scenarios in which a function calls itself more than once, recursion becomes\nwonderfully elegant.\nLet’s make drawCircle() a bit more complex. For every circle displayed, draw a circle half\nits size to the left and right of that circle.\nExample 8.2: Recursion twice\nvoid setup() {\nsize(640,360);\n}\nvoid draw() {\nbackground(255);\ndrawCircle(width/2,height/2,200);\n}\nvoid drawCircle(float x, float y, float radius) {\nstroke(0);\nnoFill();\nellipse(x, y, radius, radius);\nif(radius > 2) {\ndrawCircle() calls itself twice, creating a\nbranching effect. For every circle, a\nsmaller circle is drawn to the left and the\nright.\ndrawCircle(x + radius/2, y, radius/2);\ndrawCircle(x - radius/2, y, radius/2);\n}\n}\nChapter 8. Fractals\n362\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 533
  },
  {
    "chunk_full": "With just a little more code, we could also add a circle above and below each circle.\nExample 8.3: Recursion four times\nTry reproducing this sketch with iteration instead of recursion—I dare you!\nvoid drawCircle(float x, float y, float radius) {\nellipse(x, y, radius, radius);\nif(radius > 8) {\ndrawCircle(x + radius/2, y, radius/2);\ndrawCircle(x - radius/2, y, radius/2);\ndrawCircle(x, y + radius/2, radius/2);\ndrawCircle(x, y - radius/2, radius/2);\n}\n}\n8.3 The Cantor Set with a Recursive Function\n8.3 The Cantor Set with a Recursive Function\nNow we’re ready to visualize the Cantor set in Processing using a recursive function. Where\ndo we begin? Well, we know that the Cantor set begins with a line. So let’s start there and\nwrite a function that draws a line.\nThe above cantor() function draws a line that starts at pixel coordinate (x,y) with a length of\nlen. (The line is drawn horizontally here, but this is an arbitrary decision.) So if we called that\nfunction, saying:\nwe’d get the following:\nvoid cantor(float x, float y, float len) {\nline(x,y,x+len,y);\n}\ncantor(10, 20, width-20);\nThe Nature of Code (v1.0)\n363\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 534
  },
  {
    "chunk_full": "Now, the Cantor rule tells us to erase the\nmiddle third of that line, which leaves us\nwith two lines, one from the beginning of\nthe line to the one-third mark, and one from\nthe two-thirds mark to the end of the line.\nWe can now add two more lines of code to\ndraw the second pair of lines, moving the\ny-location down a bunch of pixels so that\nwe can see the result below the original\nline.\nWhile this is a fine start, such a manual approach of calling line() for each line is not what\nwe want. It will get unwieldy very quickly, as we’d need four, then eight, then sixteen calls to\nline(). Yes, a for loop is our usual way around such a problem, but give that a try and\nyou’ll see that working out the math for each iteration quickly proves inordinately\ncomplicated. Here is where recursion comes and rescues us.\nTake a look at where we draw that first line from the start to the one-third mark.\nInstead of calling the line() function directly, we can simply call the cantor() function\nitself. After all, what does the cantor() function do? It draws a line at an (x,y) location with a\ngiven length! And so:\nFigure 8.10\nFigure 8.11\nvoid cantor(float x, float y, float len) {\nline(x,y,x+len,y);\ny += 20;\nFrom start to 1/3rd\nline(x,y,x+len/3,y);\nFrom 2/3rd to end\nline(x+len*2/3,y,x+len,y);\n}\nFigure 8.12\nline(x,y,x+len/3,y);\nline(x,y,x+len/3,y);\nbecomes ------->\ncantor(x,y,len/3);\nChapter 8. Fractals\n364\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 535
  },
  {
    "chunk_full": "And for the second line:\nLeaving us with:\nAnd since the cantor() function is called recursively, the same rule will be applied to the next\nlines and to the next and to the next as cantor() calls itself again and again! Now, don’t go\nand run this code yet. We’re missing that crucial element: an exit condition. We’ll want to\nmake sure we stop at some point—for example, if the length of the line ever is less than 1\npixel.\nExample 8.4: Cantor set\nline(x+len*2/3,y,x+len,y);\nbecomes ------->\ncantor(x+len*2/3,y,len/3);\nvoid cantor(float x, float y, float len) {\nline(x,y,x+len,y);\ny += 20;\ncantor(x,y,len/3);\ncantor(x+len*2/3,y,len/3);\n}\nvoid cantor(float x, float y, float len) {\nStop at 1 pixel!\nif (len >= 1) {\nline(x,y,x+len,y);\ny += 20;\ncantor(x,y,len/3);\ncantor(x+len*2/3,y,len/3);\n}\n}\nThe Nature of Code (v1.0)\n365\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 536
  },
  {
    "chunk_full": "Using drawCircle() and the Cantor set as models, generate your own pattern with\nrecursion. Here is a screenshot of one that uses lines.\nExercise 8.1\nExercise 8.1\n8.4 The Koch Curve and the ArrayList Technique\n8.4 The Koch Curve and the ArrayList Technique\nWriting a function that recursively calls itself is one technique for generating a fractal\npattern on screen. However, what if you wanted the lines in the above Cantor set to exist as\nindividual objects that could be moved independently? The recursive function is simple and\nelegant, but it does not allow you to do much besides simply generating the pattern itself.\nHowever, there is another way we can apply recursion in combination with an ArrayList\nthat will allow us to not only generate a fractal pattern, but keep track of all its individual\nparts as objects.\nTo demonstrate this technique, let’s look at another famous fractal pattern, discovered in\n1904 by Swedish mathematician Helge von Koch. Here are the rules. (Note that it starts the\nsame way as the Cantor set, with a single line.)\nChapter 8. Fractals\n366\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 537
  },
  {
    "chunk_full": "The result looks like:\nThe “Monster” Curve\nThe “Monster” Curve\nThe Koch curve and other fractal patterns are often called “mathematical monsters.”\nThis is due to an odd paradox that emerges when you apply the recursive definition an\ninfinite number of times. If the length of the original starting line is one, the first\niteration of the Koch curve will yield a line of length four-thirds (each segment is one-\nthird the length of the starting line). Do it again and you get a length of sixteen-ninths.\nAs you iterate towards infinity, the length of the Koch curve approaches infinity. Yet it\nfits in the tiny finite space provided right here on this paper (or screen)!\nSince we are working in the Processing land of finite pixels, this theoretical paradox\nwon’t be a factor for us. We’ll have to limit the number of times we recursively apply\nthe Koch rules so that our program won’t run out of memory or crash.\nWe could proceed in the same manner as we did with the Cantor set, and write a recursive\nfunction that iteratively applies the Koch rules over and over. Nevertheless, we are going to\ntackle this problem in a different manner by treating each segment of the Koch curve as an\nindividual object. This will open up some design possibilities. For example, if each segment is\nFigure 8.13\nFigure 8.14\nThe Nature of Code (v1.0)\n367\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 538
  },
  {
    "chunk_full": "an object, we could allow each segment to move independently from its original location\nand participate in a physics simulation. In addition, we could use a random color, line\nthickness, etc. to display each segment differently.\nIn order to accomplish our goal of treating each segment as an individual object, we must\nfirst decide what this object should be in the first place. What data should it store? What\nfunctions should it have?\nThe Koch curve is a series of connected lines, and so we will think of each segment as a\n“KochLine.” Each KochLine object has a start point (“a”) and an end point (“b”). These points\nare PVector objects, and the line is drawn with Processing’s line() function.\nNow that we have our KochLine class, we can get started on the main program. We’ll need\na data structure to keep track of what will eventually become many KochLine objects, and\nan ArrayList (see Chapter 4 for a review of ArrayLists) will do just fine.\nIn setup(), we’ll want to create the ArrayList and add the first line segment to it, a line\nthat stretches from 0 to the width of the sketch.\nclass KochLine {\nA line between two points: start and end\nPVector start;\nPVector end;\nKochLine(PVector a, PVector b) {\nstart = a.get();\nend = b.get();\n}\nvoid display() {\nstroke(0);\nDraw the line from PVector start to end.\nline(start.x, start.y, end.x, end.y);\n}\n}\nArrayList<KochLine> lines;\nvoid setup() {\nsize(600, 300);\nCreate the ArrayList.\nlines = new ArrayList<KochLine>();\nLeft side of window\nPVector start = new PVector(0, 200);\nRight side of window\nPVector end\n= new PVector(width, 200);\nChapter 8. Fractals\n368\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 539
  },
  {
    "chunk_full": "Then in draw(), all KochLine objects (just one right now) can be displayed in a loop.\nThis is our foundation. Let’s review what we have so far:\n•\nKochLine class:\nKochLine class: A class to keep track of a line from point A to B.\n•\nArrayList:\nArrayList: A list of all KochLine objects.\nWith the above elements, how and where do we apply Koch rules and principles of recursion?\nRemember the Game of Life cellular automata? In that simulation, we always kept track of two\ngenerations: current and next. When we were finished computing the next generation, next\nbecame current and we moved on to computing the new next generation.  We are going to\napply a similar technique here. We have an ArrayList that keeps track of the current set of\nKochLine objects (at the start of the program, there is only one). We will need a second\nArrayList (let’s call it “next”) where we will place all the new KochLine objects that are\ngenerated from applying the Koch rules. For every KochLine object in the current ArrayList,\nfour new KochLine objects are added to the next ArrayList. When we’re done, the next\nArrayList becomes the current one.\nThe first KochLine object\nlines.add(new KochLine(start, end));\n}\nvoid draw() {\nbackground(255);\nfor (KochLine l : lines) {\nl.display();\n}\n}\nFigure 8.15\nThe Nature of Code (v1.0)\n369\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 540
  },
  {
    "chunk_full": "Here’s how the code will look:\nBy calling generate() over and over again (for example, each time the mouse is pressed),\nwe recursively apply the Koch curve rules to the existing set of KochLine objects.  Of\ncourse, the above omits the real “work” here, which is figuring out those rules. How do we\nbreak one line segment into four as described by the rules? While this can be accomplished\nwith some simple arithmetic and trigonometry, since our KochLine object uses PVector, this\nis a nice opportunity for us to practice our vector math. Let’s establish how many points we\nneed to compute for each KochLine object.\nAs you can see from the above figure, we need five points (a, b, c, d, and e) to generate the\nnew KochLine objects and make the new line segments (ab, cb, cd, and de).\nWhere do we get these points? Since we have a KochLine object, why not ask the\nKochLine object to compute all these points for us?\nvoid generate() {\nCreate the next ArrayList...\nArrayList next = new ArrayList<KochLine>();\n...for every current line.\nfor (KochLine l : lines) {\nAdd four new lines. (We need to figure out\nhow to compute the locations of these\nlines!)\nnext.add(new KochLine(???,???));\nnext.add(new KochLine(???,???));\nnext.add(new KochLine(???,???));\nnext.add(new KochLine(???,???));\n}\nThe new ArrayList is now the one we care\nabout!\nlines = next;\n}\nFigure 8.16\nnext.add(new KochLine(a,b));\nnext.add(new KochLine(b,c));\nnext.add(new KochLine(c,d));\nnext.add(new KochLine(d,e));\nvoid generate() {\nArrayList next = new ArrayList<KochLine>();\nfor (KochLine l : lines) {\nChapter 8. Fractals\n370\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 541
  },
  {
    "chunk_full": "Now we just need to write five new functions in the KochLine class, each one returning a\nPVector according to Figure 8.16 (see page 370) above. Let’s knock off kochA() and kochE()\nfirst, which are simply the start and end points of the original line.\nNow let’s move on to points B and D. B is one-third of the way along the line segment and D is\ntwo-thirds. Here we can make a PVector that points from start to end and shrink it to one-third\nthe length for B and two-thirds the length for D to find these points.\nThe KochLine object has five functions,\neach of which return a PVector according to\nthe Koch rules.\nPVector a = l.kochA();\nPVector b = l.kochB();\nPVector c = l.kochC();\nPVector d = l.kochD();\nPVector e = l.kochE();\nnext.add(new KochLine(a, b));\nnext.add(new KochLine(b, c));\nnext.add(new KochLine(c, d));\nnext.add(new KochLine(d, e));\n}\nlines = next;\n}\nPVector kochA() {\nNote the use of get(), which returns a copy\nof the PVector. As was noted in Chapter 6,\nsection 14, we want to avoid making copies\nwhenever possible, but here we will need a\nnew PVector in case we want the segments\nto move independently of each other.\nreturn start.get();\n}\nPVector kochE() {\nreturn end.get();\n}\nFigure 8.17\nPVector kochB() {\nPVector from start to end\nPVector v = PVector.sub(end, start);\nOne-third the length\nv.div(3);\nThe Nature of Code (v1.0)\n371\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 542
  },
  {
    "chunk_full": "The last point, C, is the most difficult one to find. However, if you recall that the angles of an\nequilateral triangle are all sixty degrees, this makes it a little bit easier. If we know how to\nfind point B with a PVector one-third the length of the line, what if we were to rotate that\nsame PVector sixty degrees and move along that vector from point B? We’d be at point C!\nPutting it all together, if we call generate() five times in setup(), we’ll see the following\nresult.\nAdd that PVector to the beginning of the\nline to find the new point.\nv.add(start);\nreturn v;\n}\nPVector kochD() {\nPVector v = PVector.sub(end, start);\nSame thing here, only we need to move\ntwo-thirds along the line instead of one-\nthird.\nv.mult(2/3.0);\nv.add(start);\nreturn v;\n}\nFigure 8.18\nPVector kochC() {\nStart at the beginning.\nPVector a = start.get();\nPVector v = PVector.sub(end, start);\nMove 1/3rd of the way to point B.\nv.div(3);\na.add(v);\nRotate “above” the line 60 degrees.\nv.rotate(-radians(60));\nMove along that vector to point C.\na.add(v);\nreturn a;\n}\nChapter 8. Fractals\n372\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 543
  },
  {
    "chunk_full": "Example 8.5: Koch curve\nArrayList<KochLine> lines;\nvoid setup() {\nsize(600, 300);\nbackground(255);\nlines = new ArrayList<KochLine>();\nPVector start = new PVector(0, 200);\nPVector end\n= new PVector(width, 200);\nlines.add(new KochLine(start, end));\nArbitrarily apply the Koch rules five times.\nfor (int i = 0; i < 5; i++) {\ngenerate();\n}\n}\nDraw the Koch snowflake (or some other\nvariation of the Koch curve).\nExercise 8.2\nExercise 8.2\nThe Nature of Code (v1.0)\n373\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 544
  },
  {
    "chunk_full": "Try animating the Koch curve. For example, can you draw it from left to right? Can you\nvary the visual design of the line segments? Can you move the line segments using\ntechniques from earlier chapters? What if each line segment were made into a spring\n(toxiclibs) or joint (Box2D)?\nExercise 8.3\nExercise 8.3\nRewrite the Cantor set example using objects and an ArrayList.\nExercise 8.4\nExercise 8.4\nDraw the Sierpiński triangle (as seen in Wolfram elementary CA) using recursion.\nExercise 8.5\nExercise 8.5\n8.5 Trees\n8.5 Trees\nThe fractals we have examined in this chapter so far are deterministic, meaning they have\nno randomness and will always produce the identical outcome each time they are run. They\nare excellent demonstrations of classic fractals and the programming techniques behind\ndrawing them, but are too precise to feel natural. In this next part of the chapter, I want to\nexamine some techniques behind generating a stochastic (or non-deterministic) fractal. The\nexample we’ll use is a branching tree. Let’s first walk through the steps to create a\ndeterministic version. Here are our production rules:\nChapter 8. Fractals\n374\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 545
  },
  {
    "chunk_full": "Again, we have a nice fractal with a recursive definition: A branch is a line with two branches\nconnected to it.\nThe part that is a bit more difficult than our previous fractals lies in the use of the word rotate\nin the fractal’s rules. Each new branch must rotate relative to the previous branch, which is\nrotated relative to all its previous branches. Luckily for us, Processing has a mechanism to\nkeep track of rotations for us—the transformation matrix\ntransformation matrix. If you aren’t familiar with the\nfunctions pushMatrix() and popMatrix(), I suggest you read the online Processing tutorial\n2D Transformations (http://processing.org/learning/transform2d/), which will cover the\nconcepts you’ll need for this particular example.\nLet’s begin by drawing a single branch, the trunk of the tree. Since we are going to involve the\nrotate() function, we’ll need to make sure we are continuously translating along the\nbranches while we draw the tree. And since the root starts at the bottom of the window (see\nabove), the first step requires translating to that spot:\nFigure 8.19\ntranslate(width/2,height);\nThe Nature of Code (v1.0)\n375\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 546
  },
  {
    "chunk_full": "…followed by drawing a line upwards\n(Figure 8.20):\nOnce we’ve finished the root, we just need to translate to the end and rotate in order to\ndraw the next branch. (Eventually, we’re going to need to package up what we’re doing\nright now into a recursive function, but let’s sort out the steps first.)\nRemember, when we rotate in Processing, we are always rotating around the point of origin,\nso here the point of origin must always be translated to the end of our current branch.\nNow that we have a branch going to the right, we need one going to the left. We can use\npushMatrix() to save the transformation state before we rotate, letting us call\npopMatrix() to restore that state and draw the branch to the left. Let’s look at all the code\ntogether.\nFigure 8.20\nline(0,0,0,-100);\nFigure 8.21\ntranslate(0,-100);\nrotate(PI/6);\nline(0,0,0,-100);\nChapter 8. Fractals\n376\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 547
  },
  {
    "chunk_full": "If you think of each call to the function line() as a “branch,” you can see from the code\nabove that we have implemented our definition of branching as a line that has two lines\nconnected to its end. We could keep adding more and more calls to line() for more and\nmore branches, but just as with the Cantor set and Koch curve, our code would become\nincredibly complicated and unwieldy. Instead, we can use the above logic as our foundation\nfor writing a recursive function, replacing the direct calls to line() with our own function\ncalled branch(). Let’s take a look.\nExample 8.6: Recursive tree\nFigure 8.22\nFigure 8.23\ntranslate(width/2,height);\nThe root\nline(0,0,0,-100);\ntranslate(0,-100);\npushMatrix();\nrotate(PI/6);\nBranch to the right\nline(0,0,0,-100);\npopMatrix();\nrotate(-PI/6);\nBranch to the left\nline(0,0,0,-100);\nvoid branch() {\nDraw the branch itself.\nline(0, 0, 0, -100);\nTranslate to the end.\ntranslate(0, -100);\npushMatrix();\nThe Nature of Code (v1.0)\n377\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 548
  },
  {
    "chunk_full": "Notice how in the above code we use pushMatrix() and popMatrix() around each\nsubsequent call to branch(). This is one of those elegant code solutions that feels almost\nlike magic. Each call to branch() takes a moment to remember the location of that\nparticular branch. If you turn yourself into Processing for a moment and try to follow the\nrecursive function with pencil and paper, you’ll notice that it draws all of the branches to the\nright first. When it gets to the end, popMatrix() will pop us back along all of the branches\nwe’ve drawn and start sending branches out to the left.\nYou may have noticed that the recursive function we just wrote would not actually draw the\nabove tree. After all, it has no exit condition and would get stuck in infinite recursive calls to\nitself. You’ll also probably notice that the branches of the tree get shorter at each level.\nLet’s look at how we can shrink the length of the lines as the tree is drawn, and stop\nbranching once the lines have become too short.\nRotate to the right and branch again.\nrotate(PI/6);\nbranch();\npopMatrix();\npushMatrix();\nRotate to the left and branch again.\nrotate(-PI/6);\nbranch();\npopMatrix();\n}\nEmulate the Processing code in Example 8.6 (see page 377) and number the\nbranches in the above diagram in the order that Processing would actually draw each\none.\nExercise 8.6\nExercise 8.6\nChapter 8. Fractals\n378\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 549
  },
  {
    "chunk_full": "We’ve also included a variable for theta that allows us, when writing the rest of the code in\nsetup() and draw(), to vary the branching angle according to, say, the mouseX location.\nExample 8.7: Recursive tree\nEach branch now receives its length as an\nargument.\nvoid branch(float len) {\nline(0, 0, 0, -len);\ntranslate(0, -len);\nEach branch’s length shrinks by two-thirds.\nlen *= 0.66;\nif (len > 2) {\npushMatrix();\nrotate(theta);\nSubsequent calls to branch() include the\nlength argument.\nbranch(len);\npopMatrix();\npushMatrix();\nrotate(-theta);\nbranch(len);\npopMatrix();\n}\n}\nfloat theta;\nvoid setup() {\nsize(300, 200);\n}\nvoid draw() {\nbackground(255);\nThe Nature of Code (v1.0)\n379\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 550
  },
  {
    "chunk_full": "The recursive tree fractal is a nice example of a scenario in which adding a little bit of\nrandomness can make the tree look more natural. Take a look outside and you’ll notice that\nbranch lengths and angles vary from branch to branch, not to mention the fact that branches\ndon’t all have exactly the same number of smaller branches. First, let’s see what happens\nPick an angle according to the mouse\nlocation.\ntheta = map(mouseX,0,width,0,PI/2);\nThe first branch starts at the bottom of the\nwindow.\ntranslate(width/2, height);\nstroke(0);\nbranch(60);\n}\nVary the strokeWeight() for each branch. Make the root thick and each subsequent\nbranch thinner.\nExercise 8.7\nExercise 8.7\nThe tree structure can also be generated using the ArrayList technique\ndemonstrated with the Koch curve. Recreate the tree using a Branch object and an\nArrayList to keep track of the branches. Hint: you’ll want to keep track of the branch\ndirections and lengths using vector math instead of Processing transformations.\nExercise 8.8\nExercise 8.8\nOnce you have the tree built with an ArrayList of Branch objects, animate the tree’s\ngrowth. Can you draw leaves at the end of the branches?\nExercise 8.9\nExercise 8.9\nChapter 8. Fractals\n380\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 551
  },
  {
    "chunk_full": "when we simply vary the angle and length. This is a pretty easy one, given that we can just\nask Processing for a random number each time we draw the tree.\nIn the above function, we always call branch() twice. But why not pick a random number of\nbranches and call branch() that number of times?\nvoid branch(float len) {\nStart by picking a random angle for each\nbranch.\nfloat theta = random(0,PI/3);\nline(0, 0, 0, -len);\ntranslate(0, -len);\nlen *= 0.66;\nif (len > 2) {\npushMatrix();\nrotate(theta);\nbranch(len);\npopMatrix();\npushMatrix();\nrotate(-theta);\nbranch(len);\npopMatrix();\n}\n}\nThe Nature of Code (v1.0)\n381\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 552
  },
  {
    "chunk_full": "Example 8.8: Stochastic tree\nvoid branch(float len) {\nline(0, 0, 0, -len);\ntranslate(0, -len);\nif (len > 2) {\nCall branch() a random number of times.\nint n = int(random(1,4));\nfor (int i = 0; i < n; i++) {\nEach branch gets its own random angle.\nfloat theta = random(-PI/2, PI/2);\npushMatrix();\nrotate(theta);\nbranch(h);\npopMatrix();\n}\n}\nSet the angles of the branches of the tree according to Perlin noise values. Adjust the\nnoise values over time to animate the tree. See if you can get it to appear as if it is\nblowing in the wind.\nExercise 8.10\nExercise 8.10\nUse toxiclibs to simulate tree physics. Each branch of the tree should be two particles\nconnected with a spring. How can you get the tree to stand up and not fall down?\nExercise 8.11\nExercise 8.11\n8.6 L-systems\n8.6 L-systems\nIn 1968, Hungarian botanist Aristid Lindenmayer developed a grammar-based system to\nmodel the growth patterns of plants. L-systems (short for Lindenmayer systems) can be used\nto generate all of the recursive fractal patterns we’ve seen so far in this chapter. We don’t\nneed L-systems to do the kind of work we’re doing here; however, they are incredibly useful\nbecause they provide a mechanism for keeping track of fractal structures that require\ncomplex and multi-faceted production rules.\nChapter 8. Fractals\n382\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 553
  },
  {
    "chunk_full": "In order to create an example that implements L-systems in Processing, we are going to have\nto be comfortable with working with (a) recursion, (b) transformation matrices, and (c) strings.\nSo far we’ve worked with recursion and transformations, but strings are new here. We will\nassume the basics, but if that is not comfortable for you, I would suggest taking a look at the\nProcessing tutorial Strings and Drawing Text (http://www.processing.org/learning/text/).\nAn L-system involves three main components:\n•\nAlphabet.\nAlphabet. An L-system’s alphabet is comprised of the valid characters that can be\nincluded. For example, we could say the alphabet is “ABC,” meaning that any valid\n“sentence” (a string of characters) in an L-system can only include these three\ncharacters.\n•\nAxiom.\nAxiom. The axiom is a sentence (made up with characters from the alphabet) that\ndescribes the initial state of the system. For example, with the alphabet “ABC,” some\nexample axioms are “AAA” or “B” or “ACBAB.”\n•\nRules.\nRules. The rules of an L-system are applied to the axiom and then applied\nrecursively, generating new sentences over and over again. An L-system rule\nincludes two sentences, a “predecessor” and a “successor.” For example, with the\nRule “A —> AB”, whenever an “A” is found in a string, it is replaced with “AB.”\nLet’s begin with a very simple L-system. (This is, in fact, Lindenmayer’s original L-system for\nmodeling the growth of algae.)\nAlphabet: A B\nAxiom: A\nRules: (A → AB) (B → A)\nAs with our recursive fractal shapes, we can\nconsider each successive application of the\nL-system rules to be a generation.\nGeneration 0 is, by definition, the axiom.\nLet’s look at how we might create these\ngenerations with code. We’ll start by using a\nString object to store the axiom.\nAnd once again, just as we did with the Game of Life and the Koch curve ArrayList\nexamples, we will need an entirely separate string to keep track of the “next” generation.\nFigure 8.24: And so on and so forth...\nString current = \"A\";\nThe Nature of Code (v1.0)\n383\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 554
  },
  {
    "chunk_full": "Now it’s time to apply the rules to the current generation and place the results in the next.\nAnd when we’re done, current can become next.\nTo be sure this is working, let’s package it into a function and and call it every time the\nmouse is pressed.\nExample 8.9: Simple L-system sentence generation\nString next = \"\";\nfor (int i = 0; i < current.length(); i++) {\nchar c = current.charAt(i);\nProduction rule A --> AB\nif (c == 'A') {\nnext += \"AB\";\nProduction rule B --> A\n} else if (c == 'B') {\nnext += \"A\";\n}\n}\ncurrent = next;\nStart with an axiom.\nString current = \"A\";\nLet’s keep track of how many generations.\nint count = 0;\nvoid setup() {\nprintln(\"Generation \" + count + \": \" + current);\n}\nvoid draw() {\n}\nvoid mousePressed() {\nString next = \"\";\nChapter 8. Fractals\n384\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 555
  },
  {
    "chunk_full": "Since the rules are applied recursively to each generation, the length of the string grows\nexponentially. By generation #11, the sentence is 233 characters long; by generation #22, it is\nover 46,000 characters long. The Java String class, while convenient to use, is a grossly\ninefficient data structure for concatenating large strings. A String object is “immutable,”\nwhich means once the object is created it can never be changed. Whenever you add on to the\nend of a String object, Java has to make a brand new String object (even if you are using\nthe same variable name).\nIn most cases, this is fine, but why duplicate a 46,000-character string if you don’t have to?\nFor better efficiency in our L-system examples, we’ll use the StringBuffer class, which is\noptimized for this type of task and can easily be converted into a string after concatenation is\ncomplete.\nYou may find yourself wondering right about now: what exactly is the point of all this? After all,\nisn’t this a chapter about drawing fractal patterns? Yes, the recursive nature of the L-system\nsentence structure seems relevant to the discussion, but how exactly does this model plant\ngrowth in a visual way?\nTraverse the current String and make the\nnew one.\nfor (int i = 0; i < current.length(); i++) {\nchar c = current.charAt(i);\nif (c == 'A') {\nnext += \"AB\";\n}\nelse if (c == 'B') {\nnext += \"A\";\n}\n}\ncurrent = next;\ncount++;\nprintln(\"Generation \" + count + \": \" + current);\n}\nString s = \"blah\";\ns += \"add some more stuff\";\nA StringBuffer for the “next” sentence\nStringBuffer next = new StringBuffer();\nfor (int i = 0; i < current.length(); i++) {\nchar c = current.charAt(i);\nif (c == 'A') {\nappend() instead of +=\nnext.append(\"AB\");\n} else if (c == 'B') {\nnext.append(\"A\");\n}\n}\nStringBuffer can easily be converted back to\na String.\ncurrent = next.toString();\nThe Nature of Code (v1.0)\n385\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 556
  },
  {
    "chunk_full": "What we’ve left unsaid until now is that embedded into these L-system sentences are\ninstructions for drawing. Let’s see how this works with another example.\nAlphabet: A B\nAxiom: A\nRules: (A → ABA) (B → BBB)\nTo read a sentence, we’ll translate it in the following way:\nA: Draw a line forward.\nB: Move forward without drawing.\nLet’s look at the sentence of each generation and its visual output.\nGeneration 0: A\nGeneration 1: ABA\nGeneration 2: ABABBBABA\nGeneration 3: ABABBBABABBBBBBBBBABABBBABA\nLook familiar? This is the Cantor set generated with an L-system.\nThe following alphabet is often used with L-systems: “FG+-[]”, meaning:\nF: Draw a line and move forward\nG: Move forward (without drawing a line)\n+: Turn right\n-: Turn left\n[: Save current location\n]: Restore previous location\nThis type of drawing framework is often referred to as “Turtle graphics” (from the old days\nof LOGO programming). Imagine a turtle sitting on your computer screen to which you could\nissue a small set of commands: turn left, turn right, draw a line, etc. Processing isn’t set up\nto operate this way by default, but by using translate(), rotate(), and line(), we can\nemulate a Turtle graphics engine fairly easily.\nFigure 8.25\nChapter 8. Fractals\n386\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 557
  },
  {
    "chunk_full": "Here’s how we would translate the above L-system alphabet into Processing code.\nF: line(0,0,0,len); translate(0,len);\nG: translate(0,len);\n+: rotate(angle);\n-: rotate(-angle);\n[: pushMatrix();\n]: popMatrix();\nAssuming we have a sentence generated from the L-system, we can walk through the\nsentence character by character and call the appropriate function as outlined above.\nThe next example will draw a more elaborate structure with the following L-system.\nAlphabet: FG+-[]\nAxiom: F\nRules: F -→ FF+[+F-F-F]-[-F+F+F]\nThe example available for download on the book’s website takes all of the L-system code\nprovided in this section and organizes it into three classes:\n•\nRule: A class that stores the predecessor and successor strings for an L-system rule.\n•\nLSystem: A class to iterate a new L-system generation (as demonstrated with the\nStringBuffer technique).\n•\nTurtle: A class to manage reading the L-system sentence and following its\ninstructions to draw on the screen.\nfor (int i = 0; i < sentence.length(); i++) {\nLooking at each character one at a time\nchar c = sentence.charAt(i);\nPerforming the correct task for each\ncharacter. This could also be written with a\n“case” statement, which might be nicer to\nlook at, but leaving it as an if/else if\nstructure helps readers not familiar with\ncase statements.\nif (c == 'F') {\nline(0,0,len,0);\ntranslate(len,0);\n} else if (c == 'F') {\ntranslate(len,0);\n} else if (c == '+') {\nrotate(theta);\n} else if (c == '-') {\nrotate(-theta);\n} else if (c == '[') {\npushMatrix();\n} else if (c == ']') {\npopMatrix();\n}\n}\nThe Nature of Code (v1.0)\n387\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 558
  },
  {
    "chunk_full": "We won’t write out these classes here since they simply duplicate the code we’ve already\nworked out in this chapter. However, let’s see how they are put together in the main tab.\nExample 8.10: LSystem\nLSystem lsys;\nTurtle turtle;\nvoid setup() {\nsize(600,600);\nA ruleset is an array of Rule objects.\nRule[] ruleset = new Rule[1];\nruleset[0] = new Rule('F',\"FF+[+F-F-F]-[-F+F+F]\");\nThe L-system is created with an axiom and\na ruleset.\nlsys = new LSystem(\"F\",ruleset);\nturtle = new Turtle(lsys.getSentence(),width/4,radians(25));\n}\nvoid draw() {\nbackground(255);\nThe Turtle graphics renderer is given a\nsentence, a starting length, and an angle\nfor rotations.\nStart at the bottom of the window and draw.\ntranslate(width/2,height);\nturtle.render();\n}\nvoid mousePressed() {\nGenerate a new sentence when the mouse\nis pressed.\nlsys.generate();\nturtle.setToDo(lsys.getSentence());\nThe length shrinks each generation.\nturtle.changeLen(0.5);\n}\nChapter 8. Fractals\n388\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 559
  },
  {
    "chunk_full": "The Ecosystem Project\nThe Ecosystem Project\nStep 8 Exercise:\nIncorporate fractals into your ecosystem. Some possibilities:\n•\nAdd plant-like creatures to the ecosystem environment.\n•\nLet’s say one of your plants is similar to a tree. Can you add leaves or\nflowers to the end of the branches? What if the leaves can fall off the tree\n(depending on a wind force)? What if you add fruit that can be picked and\neaten by the creatures?\n•\nDesign a creature with a fractal pattern.\n•\nUse an L-system to generate instructions for how a creature should move\nor behave.\nUse an L-system as a set of instructions for creating objects stored in an ArrayList.\nUse trigonometry and vector math to perform the rotations instead of matrix\ntransformations (much like we did in the Koch curve example).\nExercise 8.12\nExercise 8.12\nThe seminal work in L-systems and plant structures, The Algorithmic Beauty of Plants\nby Przemysław Prusinkiewicz and Aristid Lindenmayer, was published in 1990. It is\navailable for free in its entirety online (http://algorithmicbotany.org/papers/#abop).\nChapter 1 describes many sophisticated L-systems with additional drawing rules and\navailable alphabet characters. In addition, it describes several methods for generating\nstochastic L-systems. Expand the L-system example to include one or more additional\nfeatures described by Prusinkiewicz and Lindenmayer.\nExercise 8.13\nExercise 8.13\nIn this chapter, we emphasized using fractal algorithms for generating visual patterns.\nHowever, fractals can be found in other creative mediums. For example, fractal patterns\nare evident in Johann Sebastian Bach’s Cello Suite no. 3. The structure of David Foster\nWallace’s novel Infinite Jest was inspired by fractals. Consider using the examples in\nthis chapter to generate audio or text.\nExercise 8.14\nExercise 8.14\nThe Nature of Code (v1.0)\n389\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 560
  },
  {
    "chunk_full": "Chapter 9. The\nChapter 9. The\nEvolution of Code\nEvolution of Code\n“The fact that life evolved out of nearly nothing, some 10 billion years\nafter the universe evolved out of literally nothing, is a fact so staggering\nthat I would be mad to attempt words to do it justice.”\n— Richard Dawkins\nLet’s take a moment to think back to a simpler time, when you wrote your first Processing\nsketches and life was free and easy. What is one of programming’s fundamental concepts\nthat you likely used in those first sketches and continue to use over and over again?\nVariables. Variables allow you to save data and reuse that data while a program runs. This,\nof course, is nothing new to us. In fact, we have moved far beyond a sketch with just one or\ntwo variables and on to more complex data structures—variables made from custom types\n(objects) that include both data and functionality. We’ve made our own little worlds of\nmovers and particles and vehicles and cells and trees.\nIn each and every example in this book, the variables of these objects have to be initialized.\nPerhaps you made a whole bunch of particles with random colors and sizes or a list of\nvehicles all starting at the same x,y location on screen. But instead of acting as “intelligent\ndesigners” and assigning the properties of our objects through randomness or thoughtful\nconsideration, we can let a process found in nature—evolution—decide for us.\nCan we think of the variables of an object as its DNA? Can objects make other objects and\npass down their DNA to a new generation? Can our simulation evolve?\nChapter 9. The Evolution of Code\n390\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 561
  },
  {
    "chunk_full": " The answer to all these questions is yes. After all, we wouldn’t be able to face ourselves in the\nmirror as nature-of-coders without tackling a simulation of one of the most powerful\nalgorithmic processes found in nature itself. This chapter is dedicated to examining the\nprinciples behind biological evolution and finding ways to apply those principles in code.\n9.1 Genetic Algorithms: Inspired by Actual Events\n9.1 Genetic Algorithms: Inspired by Actual Events\nIt’s important for us to clarify the goals of this chapter. We will not go into depth about the\nscience of genetics and evolution as it happens in the real world. We won’t be making Punnett\nsquares (sorry to disappoint) and there will be no discussion of nucleotides, protein synthesis,\nRNA, and other topics related to the actual biological processes of evolution. Instead, we are\ngoing to look at the core principles behind Darwinian evolutionary theory and develop a set of\nalgorithms inspired by these principles. We don’t care so much about an accurate simulation\nof evolution; rather, we care about methods for applying evolutionary strategies in software.\nThis is not to say that a project with more scientific depth wouldn’t have value, and I\nencourage readers with a particular interest in this topic to explore possibilities for expanding\nthe examples provided with additional evolutionary features. Nevertheless, for the sake of\nkeeping things manageable, we’re going to stick to the basics, which will be plenty complex\nand exciting.\nThe term “genetic algorithm” refers to a specific algorithm implemented in a specific way to\nsolve specific sorts of problems. While the formal genetic algorithm itself will serve as the\nfoundation for the examples we create in this chapter, we needn’t worry about implementing\nthe algorithm with perfect accuracy, given that we are looking for creative uses of\nevolutionary theories in our code. This chapter will be broken down into the following three\nparts (with the majority of the time spent on the first).\n1.\nTraditional Genetic Algorithm.\nTraditional Genetic Algorithm. We’ll begin with the traditional computer science\ngenetic algorithm. This algorithm was developed to solve problems in which the\nsolution space is so vast that a “brute force” algorithm would simply take too long.\nHere’s an example: I’m thinking of a number. A number between one and one billion.\nHow long will it take for you to guess it? Solving a problem with “brute force” refers\nto the process of checking every possible solution. Is it one? Is it two? Is it three? Is\nit four? And so and and so forth. Though luck does play a factor here, with brute\nforce we would often find ourselves patiently waiting for years while you count to\none billion. However, what if I could tell you if an answer you gave was good or bad?\nWarm or cold? Very warm? Hot? Super, super cold? If you could evaluate how “fit” a\nguess is, you could pick other numbers closer to that guess and arrive at the answer\nmore quickly. Your answer could evolve.\n2.\nInteractive Selection.\nInteractive Selection. Once we establish the traditional computer science algorithm,\nwe’ll look at other applications of genetic algorithms in the visual arts. Interactive\nselection refers to the process of evolving something (often an computer-generated\nimage) through user interaction. Let’s say you walk into a museum gallery and see\nThe Nature of Code (v1.0)\n391\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 562
  },
  {
    "chunk_full": "ten paintings. With interactive selection, you would pick your favorites and allow\nan algorithmic process to generate (or “evolve”) new paintings based on your\npreferences.\n3.\nEcosystem Simulation.\nEcosystem Simulation. The traditional computer science genetic algorithm and\ninteractive selection technique are what you will likely find if you search online or\nread a textbook about artificial intelligence. But as we’ll soon see, they don’t really\nsimulate the process of evolution as it happens in the real world. In this chapter, I\nwant to also explore techniques for simulating the process of evolution in an\necosystem of pseudo-living beings. How can our objects that move about the\nscreen meet each other, mate, and pass their genes on to a new generation? This\nwould apply directly to the Ecosystem Project outlined at the end of each chapter.\n9.2 Why Use Genetic Algorithms?\n9.2 Why Use Genetic Algorithms?\nWhile computer simulations of evolutionary processes date back to the 1950s, much of what\nwe think of as genetic algorithms (also known as “GAs”) today was developed by John\nHolland, a professor at the University of Michigan, whose book Adaptation in Natural and\nArtificial Systems pioneered GA research. Today, more genetic algorithms are part of a\nwider field of research, often referred to as \"Evolutionary Computing.\"\nTo help illustrate the traditional genetic algorithm, we are going to start with monkeys. No,\nnot our evolutionary ancestors. We’re going to start with some fictional monkeys that bang\naway on keyboards with the goal of typing out the complete works of Shakespeare.\nFigure 9.1\nChapter 9. The Evolution of Code\n392\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 563
  },
  {
    "chunk_full": "The “infinite monkey theorem” is stated as follows: A monkey hitting keys randomly on a\ntypewriter will eventually type the complete works of Shakespeare (given an infinite amount of\ntime). The problem with this theory is that the probability of said monkey actually typing\nShakespeare is so low that even if that monkey started at the Big Bang, it’s unbelievably\nunlikely we’d even have Hamlet at this point.\nLet’s consider a monkey named George. George types on a reduced typewriter containing\nonly twenty-seven characters: twenty-six letters and one space bar. So the probability of\nGeorge hitting any given key is one in twenty-seven.\nLet’s consider the phrase “to be or not to be that is the question” (we’re simplifying it from the\noriginal “To be, or not to be: that is the question”). The phrase is 39 characters long. If George\nstarts typing, the chance he’ll get the first character right is 1 in 27. Since the probability he’ll\nget the second character right is also 1 in 27, he has a 1 in 27*27 chance of landing the first\ntwo characters in correct order—which follows directly from our discussion of \"event\nprobability\" in the Introduction (see page 7). Therefore, the probability that George will type\nthe full phrase is:\n(1/27) multiplied by itself 39 times, i.e. (1/27)39\nwhich equals a 1 in\n66,555,937,033,867,822,607,895,549,241,096,482,953,017,615,834,735,226,163 chance of\ngetting it right!\nNeedless to say, even hitting just this one phrase, not to mention an entire play, is highly\nunlikely. Even if George is a computer simulation and can type one million random phrases\nper second, for George to have a 99% probability of eventually getting it right, he would have\nto type for 9,719,096,182,010,563,073,125,591,133,903,305,625,605,017 years. (Note that the\nage of the universe is estimated to be a mere 13,750,000,000 years.)\nThe point of all these unfathomably large numbers is not to give you a headache, but to\ndemonstrate that a brute force algorithm (typing every possible random phrase) is not a\nreasonable strategy for arriving randomly at “to be or not to be that is the question”. Enter\ngenetic algorithms, which will show that we can still start with random phrases and find the\nsolution through simulated evolution.\nNow, it’s worth noting that this problem (arrive at the phrase “to be or not to be that is the\nquestion”) is a ridiculous one. Since we know the answer, all we need to do is type it. Here’s a\nProcessing sketch that solves the problem.\nNevertheless, the point here is that solving a problem with a known answer allows us to easily\ntest our code. Once we’ve successfully solved the problem, we can feel more confident in\nusing genetic algorithms to do some actual useful work: solving problems with unknown\nanswers. So this first example serves no real purpose other than to demonstrate how genetic\nstring s = \"To be or not to be that is the question\";\nprintln(s);\nThe Nature of Code (v1.0)\n393\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 564
  },
  {
    "chunk_full": "algorithms work. If we test the GA results against the known answer and get “to be or not to\nbe”, then we’ve succeeded in writing our genetic algorithm.\nCreate a sketch that generates random strings. We’ll need to know how to do this in\norder to implement the genetic algorithm example that will shortly follow. How long\ndoes it take for Processing to randomly generate the string “cat”? How could you\nadapt this to generate a random design using Processing’s shape-drawing functions?\nExercise 9.1\nExercise 9.1\n9.3 Darwinian Natural Selection\n9.3 Darwinian Natural Selection\nBefore we begin walking through the genetic algorithm, let’s take a moment to describe\nthree core principles of Darwinian evolution that will be required as we implement our\nsimulation. In order for natural selection to occur as it does in nature, all three of these\nelements must be present.\n1.\nHeredity.\nHeredity. There must be a process in place by which children receive the\nproperties of their parents. If creatures live long enough to reproduce, then their\ntraits are passed down to their children in the next generation of creatures.\n2.\nVariation.\nVariation. There must be a variety of traits present in the population or a means\nwith which to introduce variation. For example, let’s say there is a population of\nbeetles in which all the beetles are exactly the same: same color, same size, same\nwingspan, same everything. Without any variety in the population, the children will\nalways be identical to the parents and to each other. New combinations of traits\ncan never occur and nothing can evolve.\n3.\nSelection.\nSelection. There must be a mechanism by which some members of a population\nhave the opportunity to be parents and pass down their genetic information and\nsome do not. This is typically referred to as “survival of the fittest.” For example,\nlet’s say a population of gazelles is chased by lions every day. The faster gazelles\nare more likely to escape the lions and are therefore more likely to live longer and\nhave a chance to reproduce and pass their genes down to their children. The term\nfittest, however, can be a bit misleading. Generally, we think of it as meaning\nbigger, faster, or stronger. While this may be the case in some instances, natural\nselection operates on the principle that some traits are better adapted for the\ncreature’s environment and therefore produce a greater likelihood of surviving\nand reproducing. It has nothing to do with a given creature being “better” (after all,\nthis is a subjective term) or more “physically fit.” In the case of our typing\nmonkeys, for example, a more “fit” monkey is one that has typed a phrase closer\nto “to be or not to be”.\nChapter 9. The Evolution of Code\n394\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 565
  },
  {
    "chunk_full": "Next I’d like to walk through the narrative of the genetic algorithm. We’ll do this in the context\nof the typing monkey. The algorithm itself will be divided into two parts: a set of conditions for\ninitialization (i.e. Processing’s setup()) and the steps that are repeated over and over again\n(i.e. Processing’s draw()) until we arrive at the correct answer.\n9.4 The Genetic Algorithm, Part I: Creating a\n9.4 The Genetic Algorithm, Part I: Creating a\nPopulation\nPopulation\nIn the context of the typing monkey example, we will create a population of phrases. (Note\nthat we are using the term “phrase” rather loosely, meaning a string of characters.) This begs\nthe question: How do we create this population? Here is where the Darwinian principle of\nvariation\nvariation applies. Let’s say, for simplicity, that we are trying to evolve the phrase “cat” and\nthat we have a population of three phrases.\nhug\nrid\nwon\nSure, there is variety in the three phrases above, but try to mix and match the characters\nevery which way and you will never get cat. There is not enough variety here to evolve the\noptimal solution. However, if we had a population of thousands of phrases, all generated\nrandomly, chances are that at least one member of the population will have a c as the first\ncharacter, one will have an a as the second, and one a t as the third. A large population will\nmost likely give us enough variety to generate the desired phrase (and in Part 2 of the\nalgorithm, we’ll have another opportunity to introduce even more variation in case there isn’t\nenough in the first place). So we can be more specific in describing Step 1 and say:\nCreate a population of randomly generated elements.\nThis brings up another important question. What is the element itself? As we move through\nthe examples in this chapter, we’ll see several different scenarios; we might have a population\nof images or a population of vehicles à la Chapter 6 (see page 308). The key, and the part that\nis new for us in this chapter, is that each member of the population has a virtual “DNA,” a set\nof properties (we can call them “genes”) that describe how a given element looks or behaves.\nIn the case of the typing monkey, for example, the DNA is simply a string of characters.\nIn the field of genetics, there is an important distinction between the concepts genotype and\nphenotype. The actual genetic code—in our case, the digital information itself—is an element’s\ngenotype\ngenotype. This is what gets passed down from generation to generation. The phenotype\nphenotype,\nhowever, is the expression of that data. This distinction is key to how you will use genetic\nalgorithms in your own work. What are the objects in your world? How will you design the\nThe Nature of Code (v1.0)\n395\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 566
  },
  {
    "chunk_full": "genotype for your objects (the data structure to store each object’s properties) as well as\nthe phenotype (what are you using these variables to express?) We do this all the time in\ngraphics programming. The simplest example is probably color.\nGenotype\nGenotype\nPhenotype\nPhenotype\nint c = 255;\nint c = 127;\nint c = 0;\nAs we can see, the genotype is the digital information. Each color is a variable that stores an\ninteger and we choose to express that integer as a color. But how we choose to express the\ndata is arbitrary. In a different approach, we could have used the integer to describe the\nlength of a line, the weight of a force, etc.\nSame Genotype\nSame Genotype\nDifferent Phenotype (line length)\nDifferent Phenotype (line length)\nint c = 255;\nint c = 127;\nint c = 0;\nThe nice thing about our monkey-typing example is that there is no difference between\ngenotype and phenotype. The DNA data itself is a string of characters and the expression of\nthat data is that very string.\nSo, we can finally end the discussion of this first step and be more specific with its\ndescription, saying:\nCreate a population of N elements, each with randomly generated DNA.\nChapter 9. The Evolution of Code\n396\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 567
  },
  {
    "chunk_full": "9.5 The Genetic Algorithm, Part II: Selection\n9.5 The Genetic Algorithm, Part II: Selection\nHere is where we apply the Darwinian principle of selection. We need to evaluate the\npopulation and determine which members are fit to be selected as parents for the next\ngeneration. The process of selection can be divided into two steps.\n1) Evaluate fitness.\n1) Evaluate fitness.\nFor our genetic algorithm to function properly, we will need to design what is referred to as a\nfitness function\nfitness function. The function will produce a numeric score to describe the fitness of a given\nmember of the population. This, of course, is not how the real world works at all. Creatures are\nnot given a score; they simply survive or not. But in the case of the traditional genetic\nalgorithm, where we are trying to evolve an optimal solution to a problem, we need to be able\nto numerically evaluate any given possible solution.\nLet’s examine our current example, the typing monkey. Again, let’s simplify the scenario and\nsay we are attempting to evolve the word “cat”. We have three members of the population:\nhut, car, and box. Car is obviously the most fit, given that it has two correct characters, hut has\nonly one, and box has zero. And there it is, our fitness function:\nfitness = the number of correct characters\nDNA\nDNA\nFitness\nFitness\nhut\n1\ncar\n2\nbox\n0\nWe will eventually want to look at examples with more sophisticated fitness functions, but this\nis a good place to start.\n2) Create a mating pool.\n2) Create a mating pool.\nOnce the fitness has been calculated for all members of the population, we can then select\nwhich members are fit to become parents and place them in a mating pool. There are several\ndifferent approaches we could take here. For example, we could employ what is known as the\nelitist\nelitist method and say, “Which two members of the population scored the highest? You two\nwill make all the children for the next generation.” This is probably one of the easier methods\nto program; however, it flies in the face of the principle of variation. If two members of the\npopulation (out of perhaps thousands) are the only ones available to reproduce, the next\ngeneration will have little variety and this may stunt the evolutionary process. We could\ninstead make a mating pool out of a larger number—for example, the top 50% of the\nThe Nature of Code (v1.0)\n397\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 568
  },
  {
    "chunk_full": "population, 500 out of 1,000. This is also just as easy to program, but it will not produce\noptimal results. In this case, the high-scoring top elements would have the same chance of\nbeing selected as a parent as the ones toward the middle. And why should element number\n500 have a solid shot of reproducing, while element number 501 has no shot?\nA better solution for the mating pool is to use a probabilistic\nprobabilistic method, which we’ll call the\n“wheel of fortune” (also known as the “roulette wheel”). To illustrate this method, let’s\nconsider a simple example where we have a population of five elements, each with a fitness\nscore.\nElement\nElement\nFitness\nFitness\nA\n3\nB\n4\nC\n0.5\nD\n1.5\nE\n1\nThe first thing we’ll want to do is normalize\nnormalize all the scores. Remember normalizing a vector?\nThat involved taking an vector and standardizing its length, setting it to 1. When we\nnormalize a set of fitness scores, we are standardizing their range to between 0 and 1, as a\npercentage of total fitness. Let’s add up all the fitness scores.\ntotal fitness = 3 + 4 + 0.5 + 1.5 + 1 = 10\nThen let’s divide each score by the total fitness, giving us the normalized fitness.\nElement\nElement\nFitness\nFitness\nNormalized Fitness\nNormalized Fitness\nExpressed as a\nExpressed as a\nPercentage\nPercentage\nA\n3\n0.3\n30%\nB\n4\n0.4\n40%\nC\n0.5\n0.05\n5%\nD\n1.5\n0.15\n15%\nE\n1\n0.1\n10%\nChapter 9. The Evolution of Code\n398\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 569
  },
  {
    "chunk_full": "Now it’s time for the wheel of fortune.\nSpin the wheel and you’ll notice that Element B has the highest chance of being selected,\nfollowed by A, then D, then E, and finally C. This probability-based selection according to\nfitness is an excellent approach. One, it guarantees that the highest-scoring elements will be\nmost likely to reproduce. Two, it does not entirely eliminate any variation from the population.\nUnlike with the elitist method, even the lowest-scoring element (in this case C) has a chance\nto pass its information down to the next generation. It’s quite possible (and often the case)\nthat even low-scoring elements have a tiny nugget of genetic code that is truly useful and\nshould not entirely be eliminated from the population. For example, in the case of evolving “to\nbe or not to be”, we might have the following elements.\nA: to be or not to go\nB: to be or not to pi\nC: xxxxxxxxxxxxxxxxbe\nAs you can see, elements A and B are clearly the most fit and would have the highest score.\nBut neither contains the correct characters for the end of the phrase. Element C, even though\nit would receive a very low score, happens to have the genetic data for the end of the phrase.\nAnd so while we would want A and B to be picked to generate the majority of the next\ngeneration, we would still want C to have a small chance to participate in the reproductive\nprocess.\nFigure 9.2\n9.6 The Genetic Algorithm, Part III: Reproduction\n9.6 The Genetic Algorithm, Part III: Reproduction\nNow that we have a strategy for picking parents, we need to figure out how to use\nreproduction to make the population’s next generation, keeping in mind the Darwinian\nprinciple of heredity—that children inherit properties from their parents. Again, there are a\nnumber of different techniques we could employ here. For example, one reasonable (and easy\nto program) strategy is asexual reproduction, meaning we pick just one parent and create a\nThe Nature of Code (v1.0)\n399\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 570
  },
  {
    "chunk_full": "child that is an exact copy of that parent. The standard approach with genetic algorithms,\nhowever, is to pick two parents and create a child according to the following steps.\n1) Crossover.\n1) Crossover.\nCrossover involves creating a child out of the genetic code of two parents. In the case of\nthe monkey-typing example, let’s assume we’ve picked two phrases from the mating pool\n(as outlined in our selection step).\nParent A: FORK\nParent B: PLAY\nIt’s now up to us to make a child phrase from these two. Perhaps the most obvious way\n(let’s call this the 50/50 method) would be to take the first two characters from A and the\nsecond two from B, leaving us with:\nA variation of this technique is to pick a random midpoint. In other words, we don’t have to\npick exactly half of the code from each parent. We could sometimes end up with FLAY, and\nsometimes with FORY. This is preferable to the 50/50 approach, since we increase the\nvariety of possibilities for the next generation.\nFigure 9.3\nFigure 9.4: Picking a random midpoint\nChapter 9. The Evolution of Code\n400\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 571
  },
  {
    "chunk_full": "Another possibility is to randomly select a parent for each character in the child string. You\ncan think of this as flipping a coin four times: heads take from parent A, tails from parent B.\nHere we could end up with many different results such as: PLRY, FLRK, FLRY, FORY, etc.\nThis strategy will produce essentially the same results as the random midpoint method;\nhowever, if the order of the genetic information plays some role in expressing the phenotype,\nyou may prefer one solution over the other.\n2) Mutation.\n2) Mutation.\nOnce the child DNA has been created via crossover, we apply one final process before adding\nthe child to the next generation—mutation\nmutation. Mutation is an optional step, as there are some\ncases in which it is unnecessary. However, it exists because of the Darwinian principle of\nvariation. We created an initial population randomly, making sure that we start with a variety of\nelements. However, there can only be so much variety when seeding the first generation, and\nmutation allows us to introduce additional variety throughout the evolutionary process itself.\nMutation is described in terms of a rate. A\ngiven genetic algorithm might have a\nmutation rate of 5% or 1% or 0.1%, etc. Let’s\nassume we just finished with crossover and\nended up with the child FORY. If we have a\nmutation rate of 1%, this means that for each\ncharacter in the phrase generated from\ncrossover, there is a 1% chance that it will\nmutate. What does it mean for a character to\nmutate? In this case, we define mutation as\npicking a new random character. A 1%\nprobability is fairly low, and most of the time mutation will not occur at all in a four-character\nstring (96% of the time to be more precise). However, when it does, the mutated character is\nreplaced with a randomly generated one (see Figure 9.6).\nFigure 9.5: Coin-flipping approach\nFigure 9.6\nThe Nature of Code (v1.0)\n401\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 572
  },
  {
    "chunk_full": "As we’ll see in some of the examples, the mutation rate can greatly affect the behavior of\nthe system. Certainly, a very high mutation rate (such as, say, 80%) would negate the\nevolutionary process itself. If the majority of a child’s genes are generated randomly, then\nwe cannot guarantee that the more “fit” genes occur with greater frequency with each\nsuccessive generation.\nThe process of selection (picking two parents) and reproduction (crossover and mutation) is\napplied over and over again N times until we have a new population of N elements. At this\npoint, the new population of children becomes the current population and we loop back to\nevaluate fitness and perform selection and reproduction again.\nNow that we have described all the steps of the genetic algorithm in detail, it’s time to\ntranslate these steps into Processing code. Because the previous description was a bit\nlongwinded, let’s look at an overview of the algorithm first. We’ll then cover each of the\nthree steps in its own section, working out the code.\nSETUP:\nSETUP:\nStep 1: Initialize\nInitialize. Create a population of N elements, each with randomly generated DNA.\nLOOP:\nLOOP:\nStep 2: Selection\nSelection. Evaluate the fitness of each element of the population and build a mating\npool.\nStep 3: Reproduction\nReproduction. Repeat N times:\na) Pick two parents with probability according to relative fitness.\nb) Crossover—create a “child” by combining the DNA of these two parents.\nc) Mutation—mutate the child’s DNA based on a given probability.\nd) Add the new child to a new population.\nStep 4. Replace the old population with the new population and return to Step 2.\n9.7 Code for Creating the Population\n9.7 Code for Creating the Population\nStep 1: Initialize Population\nStep 1: Initialize Population\nIf we’re going to create a population, we need a data structure to store a list of members of\nthe population. In most cases (such as our typing-monkey example), the number of elements\nin the population can be fixed, and so we use an array. (Later we’ll see examples that\ninvolve a growing/shrinking population and we’ll use an ArrayList.) But an array of what?\nWe need an object that stores the genetic information for a member of the population. Let’s\ncall it DNA\nDNA.\nChapter 9. The Evolution of Code\n402\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 573
  },
  {
    "chunk_full": "The population will then be an array of DNA objects.\nBut what stuff goes in the DNA class? For a typing monkey, its DNA is the random phrase it\ntypes, a string of characters.\nWhile this is perfectly reasonable for this particular example, we’re not going to use an actual\nString object as the genetic code. Instead, we’ll use an array of characters.\nBy using an array, we’ll be able to extend all the code we write into other examples. For\nexample, the DNA of a creature in a physics system might be an array of PVectors—or for an\nimage, an array of integers (RGB colors). We can describe any set of properties in an array,\nand even though a string is convenient for this particular sketch, an array will serve as a better\nfoundation for future evolutionary examples.\nOur genetic algorithm dictates that we create a population of N elements, each with randomly\ngenerated DNA. Therefore, in the object’s constructor, we randomly create each character of\nthe array.\nNow that we have the constructor, we can return to setup() and initialize each DNA object in\nthe population array.\nclass DNA {\n}\nA population of 100 DNA objects\nDNA[] population = new DNA[100];\nclass DNA {\nString phrase;\n}\nclass DNA {\nEach \"gene\" is one element of the array. We\nneed 18 genes because “to be or not to be”\nis 18 characters long.\nchar[] genes = new char[18];\n}\nclass DNA {\nchar[] genes = new char[18];\nDNA() {\nfor (int i = 0; i < genes.length; i++) {\nPicking randomly from a range of characters\nwith ASCII values between 32 and 128. For\nmore about ASCII: http://en.wikipedia.org/\nwiki/ASCII\ngenes[i] = (char) random(32,128);\n}\n}\n}\nThe Nature of Code (v1.0)\n403\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 574
  },
  {
    "chunk_full": "Our DNA class is not at all complete. We’ll need to add functions to it to perform all the other\ntasks in our genetic algorithm, which we’ll do as we walk through steps 2 and 3.\nDNA[] population = new DNA[100];\nvoid setup() {\nfor (int i = 0; i < population.length; i++) {\nInitializing each member of the population\npopulation[i] = new DNA();\n}\n}\nStep 2: Selection\nStep 2: Selection\nStep 2 reads, “Evaluate the fitness of each element of the population and build a mating\npool.” Let’s first evaluate each object’s fitness. Earlier we stated that one possible fitness\nfunction for our typed phrases is the total number of correct characters. Let’s revise this\nfitness function a little bit and state it as the percentage of correct characters—i.e., the total\nnumber of correct characters divided by the total characters.\nFitness = Total # Characters Correct/Total # Characters\nWhere should we calculate the fitness? Since the DNA class contains the genetic information\n(the phrase we will test against the target phrase), we can write a function inside the DNA\nclass itself to score its own fitness. Let’s assume we have a target phrase:\nWe can now compare each “gene” against the corresponding character in the target phrase,\nincrementing a counter each time we get a correct character.\nString target = \"to be or not to be\";\nclass DNA {\nWe are adding another variable to the DNA\nclass to track fitness.\nfloat fitness;\nFunction to score fitness\nvoid fitness () {\nint score = 0;\nfor (int i = 0; i < genes.length; i++) {\nIs the character correct?\nif (genes[i] == target.charAt(i)) {\nIf so, increment the score.\nscore++;\n}\n}\nFitness is the percentage correct.\nfitness = float(score)/target.length();\n}\nChapter 9. The Evolution of Code\n404\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 575
  },
  {
    "chunk_full": "In the main tab’s draw(), the very first step we’ll take is to call the fitness function for each\nmember of the population.\nAfter we have all the fitness scores, we can build the “mating pool” that we’ll need for the\nreproduction step. The mating pool is a data structure from which we’ll continuously pick two\nparents. Recalling our description of the selection process, we want to pick parents with\nprobabilities calculated according to fitness. In other words, the members of the population\nthat have the highest fitness scores should be most likely to be picked; those with the lowest\nscores, the least likely.\nIn the Introduction (see page 7), we covered the basics of probability and generating a custom\ndistribution of random numbers. We’re going to use those techniques to assign a probability\nto each member of the population, picking parents by spinning the “wheel of fortune.” Let’s\nlook at Figure 9.2 again.\nIt might be fun to do something ridiculous and actually program a simulation of a spinning\nwheel as depicted above. But this is quite unnecessary.\nvoid draw() {\nfor (int i = 0; i < population.length; i++) {\npopulation[i].fitness();\n}\nFigure 9.2 (again)\nThe Nature of Code (v1.0)\n405\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 576
  },
  {
    "chunk_full": "Instead we can pick from the five options\n(ABCDE) according to their probabilities by\nfilling an ArrayList with multiple instances\nof each parent. In other words, let’s say you\nhad a bucket of wooden letters—30 As, 40\nBs, 5 Cs, 15 Ds, and 10 Es.\nIf you pick a random letter out of that\nbucket, there’s a 30% chance you’ll get an\nA, a 5% chance you’ll get a C, and so on.\nFor us, that bucket is an ArrayList, and\neach wooden letter is a potential parent.\nWe add each parent to the ArrayList N\nnumber of times where N is equal to its\npercentage score.\nFigure 9.7\nStart with an empty mating pool.\nArrayList<DNA> matingPool = new\nArrayList<DNA>();\nfor (int i = 0; i < population.length; i++) {\nn is equal to fitness times 100, which\nleaves us with an integer between 0 and\n100.\nint n = int(population[i].fitness * 100);\nfor (int j = 0; j < n; j++) {\nAdd each member of the population to the\nmating pool N times.\nmatingPool.add(population[i]);\n}\n}\nOne of the other methods we used to generate a custom distribution of random\nnumbers is called the Monte Carlo method. This technique involved picking two\nrandom numbers, with the second number acting as a qualifying number and\ndetermining if the first random number should be kept or thrown away. Rewrite the\nabove mating pool algorithm to use the Monte Carlo method instead.\nExercise 9.2\nExercise 9.2\nChapter 9. The Evolution of Code\n406\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 577
  },
  {
    "chunk_full": "In some cases, the wheel of fortune algorithm will have an extraordinarily high\npreference for some elements over others. Take the following probabilities:\nA: 98%\nB: 1%\nC: 1%\nThis is sometimes undesirable given how it will decrease the amount of variety in this\nsystem. A solution to this problem is to replace the calculated fitness scores with the\nordinals of scoring (meaning their rank).\nA: 50% (3/6)\nB: 33% (2/6)\nC: 17% (1/6)\nRewrite the mating pool algorithm to use this method instead.\nExercise 9.3\nExercise 9.3\nStep 3: Reproduction\nStep 3: Reproduction\nWith the mating pool ready to go, it’s time to make some babies. The first step is to pick two\nparents. Again, it’s somewhat of an arbitrary decision to pick two parents. It certainly mirrors\nhuman reproduction and is the standard means in the traditional GA, but in terms of your\nwork, there really aren’t any restrictions here. You could choose to perform “asexual”\nreproduction with one parent, or come up with a scheme for picking three or four parents from\nwhich to generate child DNA. For this code demonstration, we’ll stick to two parents and call\nthem parentA and parentB.\nFirst thing we need are two random indices into the mating pool—random numbers between 0\nand the size of the ArrayList.\nWe can use these indices to retrieve an actual DNA instance from the mating pool.\nBecause we have multiple instances of the same DNA objects in the mating pool (not to\nmention that we could pick the same random number twice), it’s possible that parentA and\nparentB could be the same DNA object. If we wanted to be strict, we could write some code to\nint a = int(random(matingPool.size()));\nint b = int(random(matingPool.size()));\nDNA parentA = matingPool.get(a);\nDNA parentB = matingPool.get(b);\nThe Nature of Code (v1.0)\n407\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 578
  },
  {
    "chunk_full": "ensure that we haven’t picked the same parent twice, but we would gain very little\nefficiency for all that extra code. Still, it‘s worth trying this as an exercise.\nOnce we have the two parents, we can perform crossover\ncrossover to generate the child DNA,\nfollowed by mutation\nmutation.\nOf course, the functions crossover() and mutate() don’t magically exist in our DNA class;\nwe have to write them. The way we called crossover() above indicates that the function\nreceives an instance of DNA as an argument and returns a new instance of DNA, the child.\nThe above crossover function uses the “random midpoint” method of crossover, in which\nthe first section of genes is taken from parent A and the second section from parent B.\nAdd code to the above to guarantee that you have picked two unique “parents.”\nExercise 9.4\nExercise 9.4\nA function for crossover\nDNA child = parentA.crossover(parentB);\nA function for mutation\nchild.mutate();\nThe function receives one argument (DNA)\nand returns DNA.\nDNA crossover(DNA partner) {\nThe child is a new instance of DNA. Note\nthat the DNA is generated randomly in the\nconstructor, but we will overwrite it below\nwith DNA from parents.\nDNA child = new DNA();\nPicking a random “midpoint” in the genes\narray\nint midpoint = int(random(genes.length));\nfor (int i = 0; i < genes.length; i++) {\nBefore midpoint copy genes from one\nparent, after midpoint copy genes from the\nother parent\nif (i > midpoint) child.genes[i] = genes[i];\nelse child.genes[i] = partner.genes[i];\n}\nReturn the new child DNA\nreturn child;\n}\nChapter 9. The Evolution of Code\n408\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 579
  },
  {
    "chunk_full": "The mutate() function is even simpler to write than crossover(). All we need to do is loop\nthrough the array of genes and for each randomly pick a new character according to the\nmutation rate. With a mutation rate of 1%, for example, we would pick a new character one\ntime out of a hundred.\nThe entire function therefore reads:\nRewrite the crossover function to use the “coin flipping” method instead, in which each\ngene has a 50% chance of coming from parent A and a 50% chance of coming from\nparent B.\nExercise 9.5\nExercise 9.5\nfloat mutationRate = 0.01;\nif (random(1) < mutationRate) {\nAny code here would be executed 1% of the\ntime.\n}\nvoid mutate() {\nLooking at each gene in the array\nfor (int i = 0; i < genes.length; i++) {\nif (random(1) < mutationRate) {\nMutation, a new random character\ngenes[i] = (char) random(32,128);\n}\n}\n}\n9.8 Genetic Algorithms: Putting It All Together\n9.8 Genetic Algorithms: Putting It All Together\nYou may have noticed that we’ve essentially walked through the steps of the genetic\nalgorithm twice, once describing it in narrative form and another time with code snippets\nimplementing each of the steps. What I’d like to do in this section is condense the previous\ntwo sections into one page, with the algorithm described in just three steps and the\ncorresponding code alongside.\nThe Nature of Code (v1.0)\n409\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 580
  },
  {
    "chunk_full": "Example 9.1: Genetic algorithm: Evolving Shakespeare\nVariables we need for our GA\nMutation rate\nfloat mutationRate;\nPopulation total\nint totalPopulation = 150;\nPopulation array\nDNA[] population;\nMating pool ArrayList\nArrayList<DNA> matingPool;\nTarget phrase\nString target;\nvoid setup() {\nsize(640, 360);\nInitializing target phrase and mutation rate\ntarget = \"to be or not to be\";\nmutationRate = 0.01;\nStep 1: Initialize Population\npopulation = new DNA[totalPopulation];\nfor (int i = 0; i < population.length; i++) {\npopulation[i] = new DNA();\n}\n}\nvoid draw() {\nStep 2: Selection\nStep 2a: Calculate fitness.\nfor (int i = 0; i < population.length; i++) {\npopulation[i].fitness();\n}\nChapter 9. The Evolution of Code\n410\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 581
  },
  {
    "chunk_full": "The main tab precisely mirrors the steps of the genetic algorithm. However, most of the\nfunctionality called upon is actually present in the DNA class itself.\nStep 2b: Build mating pool.\nArrayList<DNA> matingPool = new ArrayList<DNA>();\nfor (int i = 0; i < population.length; i++) {\nAdd each member n times according to its\nfitness score.\nint n = int(population[i].fitness * 100);\nfor (int j = 0; j < n; j++) {\nmatingPool.add(population[i]);\n}\n}\nStep 3: Reproduction\nfor (int i = 0; i < population.length; i++) {\nint a = int(random(matingPool.size()));\nint b = int(random(matingPool.size()));\nDNA partnerA = matingPool.get(a);\nDNA partnerB = matingPool.get(b);\nStep 3a: Crossover\nDNA child = partnerA.crossover(partnerB);\nStep 3b: Mutation\nchild.mutate(mutationRate);\nNote that we are overwriting the population\nwith the new children. When draw() loops,\nwe will perform all the same steps with the\nnew population of children.\npopulation[i] = child;\n}\n}\nclass DNA {\nchar[] genes;\nfloat fitness;\nCreate DNA randomly.\nDNA() {\ngenes = new char[target.length()];\nfor (int i = 0; i < genes.length; i++) {\ngenes[i] = (char) random(32,128);\n}\n}\nCalculate fitness.\nvoid fitness() {\nint score = 0;\nfor (int i = 0; i < genes.length; i++) {\nif (genes[i] == target.charAt(i)) {\nscore++;\n}\n}\nfitness = float(score)/target.length();\n}\nThe Nature of Code (v1.0)\n411\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 582
  },
  {
    "chunk_full": "Crossover\nDNA crossover(DNA partner) {\nDNA child = new DNA(genes.length);\nint midpoint = int(random(genes.length));\nfor (int i = 0; i < genes.length; i++) {\nif (i > midpoint) child.genes[i] = genes[i];\nelse\nchild.genes[i] = partner.genes[i];\n}\nreturn child;\n}\nMutation\nvoid mutate(float mutationRate) {\nfor (int i = 0; i < genes.length; i++) {\nif (random(1) < mutationRate) {\ngenes[i] = (char) random(32,128);\n}\n}\n}\nConvert to String—PHENOTYPE.\nString getPhrase() {\nreturn new String(genes);\n}\n}\nAdd features to the above example to report more information about the progress of\nthe genetic algorithm itself. For example, show the phrase closest to the target each\ngeneration, as well as report on the number of generations, average fitness, etc. Stop\nthe genetic algorithm once it has solved the phrase. Consider writing a Population\nclass to manage the GA, instead of including all the code in draw().\nExercise 9.6\nExercise 9.6\nChapter 9. The Evolution of Code\n412\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 583
  },
  {
    "chunk_full": "9.9 Genetic Algorithms: Make Them Your Own\n9.9 Genetic Algorithms: Make Them Your Own\nThe nice thing about using genetic algorithms in a project is that example code can easily be\nported from application to application. The core mechanics of selection and reproduction\ndon’t need to change. There are, however, three key components to genetic algorithms that\nyou, the developer, will have to customize for each use. This is crucial to moving beyond\ntrivial demonstrations of evolutionary simulations (as in the Shakespeare example) to creative\nuses in projects that you make in Processing and other creative programming environments.\nKey #1: Varying the variables\nKey #1: Varying the variables\nThere aren’t a lot of variables to the genetic algorithm itself. In fact, if you look at the previous\nexample’s code, you’ll see only two global variables (not including the arrays and ArrayLists\nto store the population and mating pool).\nThese two variables can greatly affect the behavior of the system, and it’s not such a good\nidea to arbitrarily assign them values (though tweaking them through trial and error is a\nperfectly reasonable way to arrive at optimal values).\nThe values I chose for the Shakespeare demonstration were picked to virtually guarantee that\nthe genetic algorithm would solve for the phrase, but not too quickly (approximately 1,000\ngenerations on average) so as to demonstrate the process over a reasonable period of time. A\nmuch larger population, however, would yield faster results (if the goal were algorithmic\nefficiency rather than demonstration). Here is a table of some results.\nTotal Population\nTotal Population\nMutation Rate\nMutation Rate\nNumber of\nNumber of\nGenerations until\nGenerations until\nPhrase Solved\nPhrase Solved\nTotal Time (in\nTotal Time (in\nseconds) until Phrase\nseconds) until Phrase\nSolved\nSolved\n150\n1%\n1089\n18.8\n300\n1%\n448\n8.2\n1,000\n1%\n71\n1.8\n50,000\n1%\n27\n4.3\nNotice how increasing the population size drastically reduces the number of generations\nneeded to solve for the phrase. However, it doesn’t necessarily reduce the amount of time.\nOnce our population balloons to fifty thousand elements, the sketch runs slowly, given the\nfloat mutationRate = 0.01;\nint totalPopulation = 150;\nThe Nature of Code (v1.0)\n413\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 584
  },
  {
    "chunk_full": "amount of time required to process fitness and build a mating pool out of so many elements.\n(There are, of course, optimizations that could be made should you require such a large\npopulation.)\nIn addition to the population size, the mutation rate can greatly affect performance.\nTotal Population\nTotal Population\nMutation Rate\nMutation Rate\nNumber of\nNumber of\nGenerations until\nGenerations until\nPhrase Solved\nPhrase Solved\nTotal Time (in\nTotal Time (in\nseconds) until\nseconds) until\nPhrase Solved\nPhrase Solved\n1,000\n0%\n37 or never?\n1.2 or never?\n1,000\n1%\n71\n1.8\n1,000\n2%\n60\n1.6\n1,000\n10%\nnever?\nnever?\nWithout any mutation at all (0%), you just have to get lucky. If all the correct characters are\npresent somewhere in some member of the initial population, you’ll evolve the phrase very\nquickly. If not, there is no way for the sketch to ever reach the exact phrase. Run it a few\ntimes and you’ll see both instances. In addition, once the mutation rate gets high enough\n(10%, for example), there is so much randomness involved (1 out of every 10 letters is\nrandom in each new child) that the simulation is pretty much back to a random typing\nmonkey. In theory, it will eventually solve the phrase, but you may be waiting much, much\nlonger than is reasonable.\nKey #2: The fitness function\nKey #2: The fitness function\nPlaying around with the mutation rate or population total is pretty easy and involves little\nmore than typing numbers in your sketch. The real hard work of a developing a genetic\nalgorithm is in writing a fitness function. If you cannot define your problem’s goals and\nevaluate numerically how well those goals have been achieved, then you will not have\nsuccessful evolution in your simulation.\nBefore we think about other scenarios with other fitness functions, let’s look at flaws in our\nShakespearean fitness function. Consider solving for a phrase that is not nineteen\ncharacters long, but one thousand. Now, let’s say there are two members of the population,\none with 800 characters correct and one with 801. Here are their fitness scores:\nPhrase A:\n800 characters correct\nfitness = 80%\nPhrase B:\n801 characters correct\nfitness = 80.1%\nChapter 9. The Evolution of Code\n414\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 585
  },
  {
    "chunk_full": "There are a couple of problems here. First, we are adding elements to the mating pool N\nnumbers of times, where N equals fitness multiplied by 100. Objects can only be added to an\nArrayList a whole number of times, and so A and B will both be added 80 times, giving them\nan equal probability of being selected. Even with an improved solution that takes floating\npoint probabilities into account, 80.1% is only a teeny tiny bit higher than 80%. But getting 801\ncharacters right is a whole lot better than 800 in the evolutionary scenario. We really want to\nmake that additional character count. We want the fitness score for 801 characters to be\nexponentially better than the score for 800.\nTo put it another way, let’s graph the fitness function.\nThis is a linear graph; as the number of characters goes up, so does the fitness score.\nHowever, what if the fitness increased exponentially as the number of correct characters\nincreased? Our graph could then look something like:\nThe more correct characters, the even greater the fitness. We can achieve this type of result\nin a number of different ways. For example, we could say:\nfitness = (number of correct characters) * (number of correct characters)\nLet’s say we have two members of the population, one with five correct characters and one\nwith six. The number 6 is a 20% increase over the number 5. Let’s look at the fitness scores\nsquared.\nFigure 9.8\nFigure 9.9\nThe Nature of Code (v1.0)\n415\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 586
  },
  {
    "chunk_full": "Characters correct\nCharacters correct\nFitness\nFitness\n5\n25\n6\n36\nThe fitness scores increase exponentially relative to the number of correct characters. 36 is\na 44% increase over 25.\nHere’s another formula.\nfitness = 2(number of correct characters)\nCharacters correct\nCharacters correct\nFitness\nFitness\n1\n2\n2\n4\n3\n8\n4\n16\nHere, the fitness scores increase at a faster rate, doubling with each additional correct\ncharacter.\nWhile this rather specific discussion of exponential vs. linear fitness functions is an\nimportant detail in the design of a good fitness function, I don’t want us to miss the more\nimportant point here: Design your own fitness function! I seriously doubt that any project\nyou undertake in Processing with genetic algorithms will actually involve counting the\ncorrect number of characters in a string. In the context of this book, it’s more likely you will\nbe looking to evolve a creature that is part of a physics system. Perhaps you are looking to\noptimize the weights of steering behaviors so a creature can best escape a predator or\navoid an obstacle or make it through a maze. You have to ask yourself what you’re hoping\nto evaluate.\nRewrite the fitness function to increase exponentially according to the number of\ncorrect characters. Note that you will also have to normalize the fitness values to a\nrange between 0 and 1 so they can be added to the mating pool a reasonable\nnumber of times.\nExercise 9.7\nExercise 9.7\nChapter 9. The Evolution of Code\n416\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 587
  },
  {
    "chunk_full": "Let’s consider a racing simulation in which a vehicle is evolving a design optimized for speed.\nfitness = total number of frames required for vehicle to reach target\nHow about a cannon that is evolving the optimal way to shoot a target?\nfitness = cannonball distance to target\nThe design of computer-controlled players in a game is also a common scenario. Let’s say you\nare programming a soccer game in which the user is the goalie. The rest of the players are\ncontrolled by your program and have a set of parameters that determine how they kick a ball\ntowards the goal. What would the fitness score for any given player be?\nfitness = total goals scored\nThis, obviously, is a simplistic take on the game of soccer, but it illustrates the point. The more\ngoals a player scores, the higher its fitness, and the more likely its genetic information will\nappear in the next game. Even with a fitness function as simple as the one described here,\nthis scenario is demonstrating something very powerful—the adaptability of a system. If the\nplayers continue to evolve from game to game to game, when a new human user enters the\ngame with a completely different strategy, the system will quickly discover that the fitness\nscores are going down and evolve a new optimal strategy. It will adapt. (Don’t worry, there is\nvery little danger in this resulting in sentient robots that will enslave all humans.)\nIn the end, if you do not have a fitness function that effectively evaluates the performance of\nthe individual elements of your population, you will not have any evolution. And the fitness\nfunction from one example will likely not apply to a totally different project. So this is the part\nwhere you get to shine. You have to design a function, sometimes from scratch, that works for\nyour particular project. And where do you do this? All you have to edit are those few lines of\ncode inside the function that computes the fitness variable.\nvoid fitness() {\n????????????\n????????????\nfitness = ??????????\n}\nKey #3: Genotype and Phenotype\nKey #3: Genotype and Phenotype\nThe final key to designing your own genetic algorithm relates to how you choose to encode\nthe properties of your system. What are you trying to express, and how can you translate that\nexpression into a bunch of numbers? What is the genotype and phenotype?\nWhen talking about the fitness function, we happily assumed we could create computer-\ncontrolled kickers that each had a “set of parameters that determine how they kick a ball\ntowards the goal.” However, what those parameters are and how you choose to encode them\nis up to you.\nThe Nature of Code (v1.0)\n417\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 588
  },
  {
    "chunk_full": "We started with the Shakespeare example because of how easy it was to design both the\ngenotype (an array of characters) and its expression, the phenotype (the string drawn in the\nwindow).\nThe good news is—and we hinted at this at the start of this chapter—you’ve really been\ndoing this all along. Anytime you write a class in Processing, you make a whole bunch of\nvariables.\nAll we need to do to evolve those parameters is to turn them into an array, so that the array\ncan be used with all of the functions—crossover(), mutate(), etc.—found in the DNA class.\nOne common solution is to use an array of floating point numbers between 0 and 1.\nNotice how we’ve now put the genetic data (genotype) and its expression (phenotype) into\ntwo separate classes. The DNA class is the genotype and the Vehicle class uses a DNA\nobject to drive its behaviors and express that data visually—it is the phenotype. The two can\nbe linked by creating a DNA instance inside the Vehicle class itself.\nclass Vehicle {\nfloat maxspeed;\nfloat maxforce;\nfloat size;\nfloat separationWeight;\n// etc.\nclass DNA {\nAn array of floats\nfloat[] genes;\nDNA(int num) {\ngenes = new float[num];\nfor (int i = 0; i < genes.length; i++) {\nAlways pick a number between 0 and 1.\ngenes[i] = float(1);\n}\n}\nclass Vehicle {\nA DNA object embedded into the Vehicle\nclass\nDNA dna;\nfloat maxspeed;\nfloat maxforce;\nfloat size;\nfloat separationWeight;\nEtc.\nVehicle() {\nDNA = new DNA(4);\nChapter 9. The Evolution of Code\n418\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 589
  },
  {
    "chunk_full": "Of course, you most likely don’t want all your variables to have a range between 0 and 1. But\nrather than try to remember how to adjust those ranges in the DNA class itself, it’s easier to\npull the genetic information from the DNA object and use Processing’s map() function to\nchange the range. For example, if you want a size variable between 10 and 72, you would say:\nIn other cases, you will want to design a genotype that is an array of objects. Consider the\ndesign of a rocket with a series of “thruster” engines. You could describe each thruster with a\nPVector that outlines its direction and relative strength.\nThe phenotype would be a Rocket class that participates in a physics system.\nWhat’s great about this technique of dividing the genotype and phenotype into separate\nclasses (DNA and Rocket for example) is that when it comes time to build all of the code, you’ll\nnotice that the DNA class we developed earlier remains intact. The only thing that changes is\nthe array’s data type (float, PVector, etc.) and the expression of that data in the phenotype\nclass.\nIn the next section, we’ll follow this idea a bit further and walk through the necessary steps for\nan example that involves moving bodies and an array of PVectors as DNA.\nUsing the genes to set variables\nmaxspeed = dna.genes[0];\nmaxforce = dna.genes[1];\nsize = dna.genes[2];\nseparationWeight = dna.genes[3];\nEtc.\n}\nsize = map(dna.genes[2],0,1,10,72);\nclass DNA {\nThe genotype is an array of PVectors.\nPVector[] genes;\nDNA(int num) {\ngenes = new float[num];\nfor (int i = 0; i < genes.length; i++) {\nA PVector pointing in a random direction\ngenes[i] = PVector.random2D();\nAnd scaled randomly\ngenes[i].mult(random(10));\n}\n}\nclass Rocket {\nDNA dna;\n// etc.\nThe Nature of Code (v1.0)\n419\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 590
  },
  {
    "chunk_full": "9.10 Evolving Forces: Smart Rockets\n9.10 Evolving Forces: Smart Rockets\nWe picked the rocket idea for a specific reason. In 2009, Jer Thorp (http://blprnt.com)\nreleased a genetic algorithms example on his blog entitled “Smart Rockets.” Jer points out\nthat NASA uses evolutionary computing techniques to solve all sorts of problems, from\nsatellite antenna design to rocket firing patterns. This inspired him to create a Flash\ndemonstration of evolving rockets. Here is a description of the scenario:\nA population of rockets launches from the bottom of the screen with the goal of hitting a\ntarget at the top of the screen (with obstacles blocking a straight line path).\nEach rocket is equipped with five thrusters\nof variable strength and direction. The\nthrusters don’t fire all at once and\ncontinuously; rather, they fire one at a time\nin a custom sequence.\nIn this section, we’re going to evolve our\nown simplified Smart Rockets, inspired by\nJer Thorp’s. When we get to the end of the\nsection, we’ll leave implementing some of\nJer’s additional advanced features as an\nexercise.\nOur rockets will have only one thruster, and this thruster will be able to fire in any direction\nwith any strength for every frame of animation. This isn’t particularly realistic, but it will\nmake building out the framework a little easier. (We can always make the rocket and its\nthrusters more advanced and realistic later.)\nLet’s start by taking our basic Mover class from Chapter 2 examples and renaming it Rocket.\nFigure 9.10\nFigure 9.11\nclass Rocket {\nChapter 9. The Evolution of Code\n420\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 591
  },
  {
    "chunk_full": "Using the above framework, we can implement our smart rocket by saying that for every frame\nof animation, we call applyForce() with a new force. The “thruster” applies a single force to\nthe rocket each time through draw().\nConsidering this example, let’s go through the three keys to programming our own custom\ngenetic algorithm example as outlined in the previous section.\nKey #1: Population size and mutation rate\nKey #1: Population size and mutation rate\nWe can actually hold off on this first key for the moment. Our strategy will be to pick some\nreasonable numbers (a population of 100 rockets, mutation rate of 1%) and build out the\nsystem, playing with these numbers once we have our sketch up and running.\nKey #2: The fitness function\nKey #2: The fitness function\nWe stated the goal of a rocket reaching a target. In other words, the closer a rocket gets to\nthe target, the higher the fitness. Fitness is inversely proportional to distance: the smaller the\ndistance, the greater the fitness; the greater the distance, the smaller the fitness.\nLet’s assume we have a PVector target.\nThis is perhaps the simplest fitness function we could write. By using one divided by distance,\nlarge distances become small numbers and small distances become large.\nA rocket has three vectors: location,\nvelocity, acceleration.\nPVector location;\nPVector velocity;\nPVector acceleration;\nAccumulating forces into acceleration\n(Newton’s 2nd law)\nvoid applyForce(PVector f) {\nacceleration.add(f);\n}\nOur simple physics model (Euler integration)\nvoid update() {\nVelocity changes according to acceleration.\nvelocity.add(acceleration);\nLocation changes according to velocity.\nlocation.add(velocity);\nacceleration.mult(0);\n}\n}\nvoid fitness() {\nHow close did we get?\nfloat d = PVector.dist(location,target);\nFitness is inversely proportional to distance.\nfitness = 1/d;\n}\nThe Nature of Code (v1.0)\n421\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 592
  },
  {
    "chunk_full": "distance\ndistance\n1 / distance\n1 / distance\n300\n1 / 300 = 0.0033\n100\n1 / 100 = 0.01\n5\n1 / 5 = 0.2\n1\n1 / 1 = 1.0\n0.1\n1 / 0.1 = 10\nAnd if we wanted to use our exponential trick from the previous section, we could use one\ndivided by distance squared.\ndistance\ndistance\n1 / distance\n1 / distance\n(1 / distance)\n(1 / distance)2\n300\n1 / 400 = 0.0025\n0.00000625\n100\n1 / 100 = 0.01\n0.0001\n5\n1 / 5 = 0.2\n0.04\n1\n1 / 1 = 1.0\n1.0\n0.1\n1 / 0.1 = 10\n100\nThere are several additional improvements we’ll want to make to the fitness function, but\nthis simple one is a good start.\nKey #3: Genotype and Phenotype\nWe stated that each rocket has a thruster that fires in a variable direction with a variable\nmagnitude in each frame. And so we need a PVector for each frame of animation. Our\ngenotype, the data required to encode the rocket’s behavior, is therefore an array of\nPVectors.\nvoid fitness() {\nfloat d = PVector.dist(location,target);\nSquaring 1 divided by distance\nfitness = pow(1/d,2);\n}\nChapter 9. The Evolution of Code\n422\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 593
  },
  {
    "chunk_full": "The happy news here is that we don’t really have to do anything else to the DNA class. All of\nthe functionality we developed for the typing monkey (crossover and mutation) applies here.\nThe one difference we do have to consider is how we initialize the array of genes. With the\ntyping monkey, we had an array of characters and picked a random character for each\nelement of the array. Here we’ll do exactly the same thing and initialize a DNA sequence as an\narray of random PVectors. Now, your instinct in creating a random PVector might be as\nfollows:\nThis is perfectly fine and will likely do the\ntrick. However, if we were to draw every\nsingle possible vector we might pick, the\nresult would fill a square (see Figure 9.12). In\nthis case, it probably doesn’t matter, but\nthere is a slight bias to diagonals here given\nthat a PVector from the center of a square\nto a corner is longer than a purely vertical or\nhorizontal one.\nWhat would be better here is to pick a\nrandom angle and make a PVector of length\none from that angle, giving us a circle (see\nFigure 9.13). This could be easily done with\na quick polar to Cartesian conversion (see\npage 112), but a quicker path to the result is\njust to use PVector's random2D().\nA PVector of length one is actually going to be quite a large force. Remember, forces are\napplied to acceleration, which accumulates into velocity thirty times per second. So, for this\nexample, we can also add one more variable to the DNA class: a maximum force that scales all\nthe PVectors. This will control the thruster power.\nclass DNA {\nPVector[] genes;\nPVector v = new PVector(random(-1,1),random(-1,1));\nFigure 9.12\nFigure 9.13\nfor (int i = 0; i < genes.length; i++) {\nMaking a PVector from a random angle\ngenes[i] = PVector.random2D();\n}\nThe Nature of Code (v1.0)\n423\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 594
  },
  {
    "chunk_full": "Notice also that we created an array of PVectors with length lifetime. We need a PVector\nfor each frame of the rocket’s life, and the above assumes the existence of a global variable\nlifetime that stores the total number of frames in each generation’s life cycle.\nThe expression of this array of PVectors, the phenotype, is a Rocket class modeled on our\nbasic PVector and forces examples from Chapter 2. All we need to do is add an instance of\na DNA object to the class. The fitness variable will also live here. Only the Rocket object\nknows how to compute its distance to the target, and therefore the fitness function will live\nhere in the phenotype as well.\nWhat are we using the DNA for? We are marching through the array of PVectors and\napplying them one at a time as a force to the rocket. To do this, we’ll also have to add an\ninteger that acts as a counter to walk through the array.\nclass DNA {\nThe genetic sequence is an array of\nPVectors.\nPVector[] genes;\nHow strong can the thrusters be?\nfloat maxforce = 0.1;\nDNA() {\nWe need a PVector for every frame of the\nrocket’s life.\ngenes = new PVector[lifetime];\nfor (int i = 0; i < genes.length; i++) {\ngenes[i] = PVector.random2D();\nScaling the PVectors randomly, but no\nstronger than maximum force\ngenes[i].mult(random(0, maxforce));\n}\n}\nclass Rocket {\nA Rocket has DNA.\nDNA dna;\nA Rocket has fitness.\nfloat fitness;\nPVector location;\nPVector velocity;\nPVector acceleration;\nint geneCounter = 0;\nvoid run() {\nApply a force from the genes array.\napplyForce(dna.genes[geneCounter]);\nGo to the next force in the genes array.\ngeneCounter++;\nChapter 9. The Evolution of Code\n424\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 595
  },
  {
    "chunk_full": "Update the Rocket’s physics.\nupdate();\n}\n9.11 Smart Rockets: Putting It All Together\n9.11 Smart Rockets: Putting It All Together\nWe now have our DNA class (genotype) and our Rocket class (phenotype). The last piece of\nthe puzzle is a Population class, which manages an array of rockets and has the functionality\nfor selection and reproduction. Again, the happy news here is that we barely have to change\nanything from the Shakespeare monkey example. The process for building a mating pool and\ngenerating a new array of child rockets is exactly the same as what we did with our population\nof strings.\nThere is one fairly significant change, however. With typing monkeys, a random phrase was\nevaluated as soon as it was created. The string of characters had no lifespan; it existed purely\nfor the purpose of calculating its fitness and then we moved on. The rockets, however, need\nto live for a period of time before they can be evaluated; they need to be given a chance to\nmake their attempt at reaching the target. Therefore, we need to add one more function to the\nPopulation class that runs the physics simulation itself. This is identical to what we did in the\nrun() function of a particle system—update all the particle locations and draw them.\nFinally, we’re ready for setup() and draw(). Here in the main tab, our primary responsibility is\nto implement the steps of the genetic algorithm in the appropriate order by calling the\nfunctions in the Population class.\nclass Population {\nPopulation has variables to keep track of\nmutation rate, current population array,\nmating pool, and number of generations.\nfloat mutationRate;\nRocket[] population;\nArrayList<Rocket> matingPool;\nint generations;\nThese functions haven’t changed, so no\nneed to go through the code again.\nvoid fitness() {}\nvoid selection() {}\nvoid reproduction() {}\nvoid live () {\nfor (int i = 0; i < population.length; i++) {\nThe run function takes care of the forces,\nupdating the rocket’s location, and\ndisplaying it.\npopulation[i].run();\n}\n}\nThe Nature of Code (v1.0)\n425\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 596
  },
  {
    "chunk_full": "However, unlike the Shakespeare example, we don’t want to do this every frame. Rather,\nour steps work as follows:\n1.\nCreate a population of rockets\n2.\nLet the rockets live for N frames\n3.\nEvolve the next generation\n◦\nSelection\n◦\nReproduction\n4.\nReturn to Step #2\nExample 9.2: Simple Smart Rockets\npopulation.fitness();\npopulation.selection();\npopulation.reproduction();\nHow many frames does a generation live\nfor?\nint lifetime;\nWhat frame are we on?\nint lifeCounter;\nThe population\nPopulation population;\nvoid setup() {\nsize(640, 480);\nlifetime = 500;\nlifeCounter = 0;\nfloat mutationRate = 0.01;\nChapter 9. The Evolution of Code\n426\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 597
  },
  {
    "chunk_full": "The above example works, but it isn’t particularly interesting. After all, the rockets simply\nevolve to having DNA with a bunch of vectors that point straight upwards. In the next\nexample, we’re going to talk through two suggested improvements for the example and\nprovide code snippets that implement these improvements.\nImprovement #1: Obstacles\nAdding obstacles that the rockets must avoid will make the system more complex and\ndemonstrate the power of the evolutionary algorithm more effectively. We can make\nrectangular, stationary obstacles fairly easily by creating a class that stores a location and\ndimensions.\nStep 1: Create the population. Here is where\nwe could play with the mutation rate and\npopulation size.\npopulation = new Population(mutationRate, 50);\n}\nvoid draw() {\nbackground(255);\nThe revised genetic algorithm\nif (lifeCounter < lifetime) {\nStep 2: The rockets live their life until\nlifeCounter reaches lifetime.\npopulation.live();\nlifeCounter++;\n} else {\nWhen lifetime is reached, reset lifeCounter\nand evolve the next generation (Steps 3 and\n4, selection and reproduction).\nlifeCounter = 0;\npopulation.fitness();\npopulation.selection();\npopulation.reproduction();\n}\n}\nThe Nature of Code (v1.0)\n427\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 598
  },
  {
    "chunk_full": "Example 9.3: Smart Rockets\nWe can also write a contains() function that will return true or return false to\ndetermine if a rocket has hit the obstacle.\nAssuming we make an ArrayList of obstacles, we can then have each rocket check to see\nif it has collided with an obstacle and set a boolean flag to be true if it does, adding a\nfunction to the rocket class.\nIf the rocket hits an obstacle, we choose to stop it from updating its location.\nAnd we also have an opportunity to adjust the rocket’s fitness. We consider it to be pretty\nterrible if the rocket hits an obstacle, and so its fitness should be greatly reduced.\nclass Obstacle {\nAn obstacle is a location (top left corner of\nrectangle) with a width and height.\nPVector location;\nfloat w,h;\nboolean contains(PVector v) {\nif (v.x > location.x && v.x < location.x + w && v.y > location.y && v.y <\nlocation.y + h) {\nreturn true;\n} else {\nreturn false;\n}\n}\nThis new function lives in the rocket class\nand checks if a rocket has hit an obstacle.\nvoid obstacles() {\nfor (Obstacle obs : obstacles) {\nif (obs.contains(location)) {\nstopped = true;\n}\n}\n}\nvoid run() {\nOnly run the rocket if it doesn’t hit an\nobstacle.\nif (!stopped) {\napplyForce(dna.genes[geneCounter]);\ngeneCounter = (geneCounter + 1) % dna.genes.length;\nupdate();\nobstacles();\n}\n}\nChapter 9. The Evolution of Code\n428\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 599
  },
  {
    "chunk_full": "Improvement #2: Evolve reaching the target faster\nIf you look closely at our first Smart Rockets example, you’ll notice that the rockets are not\nrewarded for getting to the target faster. The only variable in their fitness calculation is the\ndistance to the target at the end of the generation’s life. In fact, in the event that the rockets\nget very close to the target but overshoot it and fly past, they may actually be penalized for\ngetting to the target faster. Slow and steady wins the race in this case.\nWe could improve the algorithm to optimize for speed a number of ways. First, instead of\nusing the distance to the target at the end of the generation, we could use the distance that is\nthe closest to the target at any point during the rocket’s life. We would call this the rocket’s\n“record” distance. (All of the code snippets in this section live inside the Rocket class.)\nIn addition, a rocket should be rewarded according to how quickly it reaches the target. The\nfaster it reaches the target, the higher the fitness. The slower, the lower. To accomplish this,\nwe can increment a counter every cycle of the rocket’s life until it reaches the target. At the\nend of its life, the counter will equal the amount of time the rocket took to reach that target.\nFitness is also inversely proportional to finishTime, and so we can improve our fitness\nfunction as follows:\nvoid fitness() {\nfloat d = dist(location.x, location.y, target.location.x, target.location.y);\nfitness = pow(1/d, 2);\nif (stopped) fitness *= 0.1;\n}\nvoid checkTarget() {\nfloat d = dist(location.x, location.y, target.location.x, target.location.y);\nEvery frame, we check its distance and see\nif it’s closer than the “record” distance. If it is,\nwe have a new record.\nif (d < recordDist) recordDist = d;\nIf the object reaches the target, set a\nboolean flag to true.\nif (target.contains(location)) {\nhitTarget = true;\n} else if (!hitTarget) {\nAs long as we haven’t yet reached the\ntarget, keep incrementing the counter.\nfinishTime++;\n}\n}\nvoid fitness() {\nFinish time and record distance!\nfitness = (1/(finishTime*recordDist));\nThe Nature of Code (v1.0)\n429\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 600
  },
  {
    "chunk_full": "These improvements are both incorporated into the code for Example 9.3: Smart Rockets.\nOne of the more famous implementations of genetic algorithms in computer graphics is Karl\nSims’s “Evolved Virtual Creatures.” In Sims’s work, a population of digital creatures (in a\nsimulated physics environment) is evaluated for the creatures' ability to perform tasks, such\nas swimming, running, jumping, following, and competing for a green cube.\nMake it exponential.\nfitness = pow(fitness, 2);\nFitness goes way down if you hit an\nobstacle.\nif (stopped) fitness *= 0.1;\nYou are rewarded for reaching the target.\nif (hitTarget) fitness *= 2;\n}\nCreate a more complex obstacle course. As you make it more difficult for the rockets\nto reach the target, do you need to improve other aspects of the GA—for example,\nthe fitness function?\nExercise 9.8\nExercise 9.8\nImplement the rocket firing pattern of Jer Thorp’s Smart Rockets. Each rocket only\ngets five thrusters (of any direction and strength) that follow a firing sequence (of\narbitrary length). Jer’s simulation (http://www.blprnt.com/smartrockets/) also gives the\nrockets a finite amount of fuel.\nExercise 9.9\nExercise 9.9\nVisualize the rockets differently. Can you draw a line for the shortest path to the\ntarget? Can you add particle systems that act as smoke in the direction of the rocket\nthrusters?\nExercise 9.10\nExercise 9.10\nAnother way to achieve a similar result is to evolve a flow field. Can you make the\ngenotype of a rocket a flow field of PVectors?\nExercise 9.11\nExercise 9.11\nChapter 9. The Evolution of Code\n430\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 601
  },
  {
    "chunk_full": "One of the innovations in Sims’s work is a node-based genotype. In other words, the\ncreature’s DNA is not a linear list of PVectors or numbers, but a map of nodes. (For an\nexample of this, take a look at Exercise 5.15 (see page 256), toxiclibs' Force Directed Graph.)\nThe phenotype is the creature’s design itself, a network of limbs connected with muscles.\nUsing toxiclibs or Box2D as the physics\nmodel, can you create a simplified 2D\nversion of Sims’s creatures? For a\nlengthier description of Sims’s\ntechniques, I suggest you watch the\nvideo and read Sims’s paper Virtual\nCreatures (http://www.karlsims.com/\nevolved-virtual-creatures.html). In\naddition, you can find a similar example\nthat uses Box2D to evolve a “car”:\nBoxCar2D (http://boxcar2d.com/).\nExercise 9.12\nExercise 9.12\n9.12 Interactive Selection\n9.12 Interactive Selection\nIn addition to Evolved Virtual Creatures, Sims is also well known for his museum installation\nGalapagos. Originally installed in the Intercommunication Center in Tokyo in 1997, the\ninstallation consists of twelve monitors displaying computer-generated images. These images\nevolve over time, following the genetic algorithm steps of selection and reproduction. The\ninnovation here is not the use of the genetic algorithm itself, but rather the strategy behind\nthe fitness function. In front of each monitor is a sensor on the floor that can detect the\npresence of a user viewing the screen. The fitness of an image is tied to the length of time\nthat viewers look at the image. This is known as interactive selection, a genetic algorithm with\nfitness values assigned by users.\nThink of all the rating systems you’ve ever used. Could you evolve the perfect movie by\nscoring all films according to your Netflix ratings? The perfect singer according to American\nIdol voting?\nThe Nature of Code (v1.0)\n431\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 602
  },
  {
    "chunk_full": "To illustrate this technique, we’re going to\nbuild a population of simple faces. Each\nface will have a set of properties: head\nsize, head color, eye location, eye size,\nmouth color, mouth location, mouth width,\nand mouth height.\nThe face’s DNA (genotype) is an array of\nfloating point numbers between 0 and 1,\nwith a single value for each property.\nThe phenotype is a Face class that includes an instance of a DNA object.\nWhen it comes time to draw the face on screen, we can use Processing’s map() function to\nconvert any gene value to the appropriate range for pixel dimensions or color values. (In\nthis case, we are also using colorMode() to set the RGB ranges between 0 and 1.)\nFigure 9.14\nclass DNA {\nfloat[] genes;\nWe need 20 numbers to draw the face.\nint len = 20;\nDNA() {\ngenes = new float[len];\nfor (int i = 0; i < genes.length; i++) {\nEach gene is a random float between 0\nand 1.\ngenes[i] = random(0,1);\n}\n}\nclass Face {\nDNA dna;\nfloat fitness;\nvoid display() {\nUsing map() to convert the genes to a\nrange for drawing the face.\nfloat r\n= map(dna.genes[0],0,1,0,70);\ncolor c\n= color(dna.genes[1],dna.genes[2],dna.genes[3]);\nfloat eye_y\n= map(dna.genes[4],0,1,0,5);\nfloat eye_x\n= map(dna.genes[5],0,1,0,10);\nfloat eye_size\n= map(dna.genes[5],0,1,0,10);\ncolor eyecolor\n= color(dna.genes[4],dna.genes[5],dna.genes[6]);\ncolor mouthColor = color(dna.genes[7],dna.genes[8],dna.genes[9]);\nfloat mouth_y\n= map(dna.genes[5],0,1,0,25);\nfloat mouth_x\n= map(dna.genes[5],0,1,-25,25);\nfloat mouthw\n= map(dna.genes[5],0,1,0,50);\nfloat mouthh\n= map(dna.genes[5],0,1,0,10);\nChapter 9. The Evolution of Code\n432\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 603
  },
  {
    "chunk_full": "So far, we’re not really doing anything new. This is what we’ve done in every GA example so\nfar. What’s new is that we are not going to write a fitness() function in which the score is\ncomputed based on a math formula. Instead, we are going to ask the user to assign the\nfitness.\nNow, how best to ask a user to assign fitness is really more of an interaction design problem,\nand it isn’t really within the scope of this book. So we’re not going to launch into an elaborate\ndiscussion of how to program sliders or build your own hardware dials or build a Web app for\nusers to submit online scores. How you choose to acquire fitness scores is really up to you\nand the particular application you are developing.\nFor this simple demonstration, we’ll increase fitness whenever a user rolls the mouse over a\nface. The next generation is created when the user presses a button with an “evolve next\ngeneration” label.\nLet’s look at how the steps of the genetic algorithm are applied in the main tab, noting how\nfitness is assigned according to mouse interaction and the next generation is created on a\nbutton press. The rest of the code for checking mouse locations, button interactions, etc. can\nbe found in the accompanying example code.\nExample 9.4: Interactive selection\nPopulation population;\nButton button;\nvoid setup() {\nsize(780,200);\nfloat mutationRate = 0.05;\npopulation = new Population(mutationRate,10);\nbutton = new Button(15,150,160,20, \"evolve new generation\");\n}\nvoid draw() {\npopulation.display();\nThe Nature of Code (v1.0)\n433\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 604
  },
  {
    "chunk_full": "This example, it should be noted, is really just a demonstration of the idea of interactive\nselection and does not achieve a particularly meaningful result. For one, we didn’t take\nmuch care in the visual design of the faces; they are just a few simple shapes with sizes and\ncolors. Sims, for example, used more elaborate mathematical functions as his images’\ngenotype. You might also consider a vector-based approach, in which a design’s genotype\nis a set of points and/or paths.\nThe more significant problem here, however, is one of time. In the natural world, evolution\noccurs over millions of years. In the computer simulation world of our previous examples,\nwe were able to evolve behaviors relatively quickly because we were producing new\ngenerations algorithmically. In the Shakespeare monkey example, a new generation was\nborn in each frame of animation (approximately sixty per second). Since the fitness values\nwere computed according to a math formula, we could also have had arbitrarily large\npopulations that increased the speed of evolution. In the case of interactive selection,\nhowever, we have to sit and wait for a user to rate each and every member of the\npopulation before we can get to the next generation. A large population would be\nunreasonably tedious to deal with—not to mention, how many generations could you stand\nto sit through?\nThere are certainly clever solutions around this. Sims’s Galapagos exhibit concealed the\nrating process from the users, as it occurred through the normal behavior of looking at\nartwork in a museum setting. Building a Web application that would allow many users to rate\na population in a distributed fashion is also a good strategy for achieving many ratings for\nlarge populations quickly.\nIn the end, the key to a successful interactive selection system boils down to the same keys\nwe previously established. What is the genotype and phenotype? And how do you calculate\nfitness, which in this case we can revise to say: “What is your strategy for assigning fitness\naccording to user interaction?”\nThe mouse location is passed to the\npopulation, which will score each face\naccording to rollover time.\npopulation.rollover(mouseX,mouseY);\nbutton.display();\n}\nvoid mousePressed() {\nWhen a button is pressed, the new\ngeneration is created via selection and\nreproduction.\nif (button.clicked(mouseX,mouseY)) {\npopulation.selection();\npopulation.reproduction();\n}\n}\nChapter 9. The Evolution of Code\n434\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 605
  },
  {
    "chunk_full": "Build your own interactive selection project. In addition to a visual design, consider\nevolving sounds—for example, a short sequence of tones. Can you devise a strategy,\nsuch as a Web application or physical sensor system, to acquire ratings from many\nusers over time?\nExercise 9.14\nExercise 9.14\n9.13 Ecosystem Simulation\n9.13 Ecosystem Simulation\nYou may have noticed something a bit odd about every single evolutionary system we’ve built\nso far in this chapter. After all, in the real world, a population of babies isn’t born all at the\nsame time. Those babies don’t then grow up and all reproduce at exactly the same time, then\ninstantly die to leave the population size perfectly stable. That would be ridiculous. Not to\nmention the fact that there is certainly no one running around the forest with a calculator\ncrunching numbers and assigning fitness values to all the creatures.\nIn the real world, we don’t really have “survival of the fittest”; we have “survival of the\nsurvivors.” Things that happen to live longer, for whatever reason, have a greater chance of\nreproducing. Babies are born, they live for a while, maybe they themselves have babies,\nmaybe they don’t, and then they die.\nYou won’t necessarily find simulations of “real-world” evolution in artificial intelligence\ntextbooks. Genetic algorithms are generally used in the more formal manner we outlined in\nthis chapter. However, since we are reading this book to develop simulations of natural\nsystems, it’s worth looking at some ways in which we might use a genetic algorithm to build\nsomething that resembles a living “ecosystem,” much like the one we’ve described in the\nexercises at the end of each chapter.\nLet’s begin by developing a very simple scenario. We’ll create a creature called a \"bloop,\" a\ncircle that moves about the screen according to Perlin noise. The creature will have a radius\nand a maximum speed. The bigger it is, the slower it moves; the smaller, the faster.\nclass Bloop {\nA location\nPVector location;\nVariables for size and speed\nfloat r;\nfloat maxspeed;\nThe Nature of Code (v1.0)\n435\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 606
  },
  {
    "chunk_full": "The above is missing a few details (such as initializing the variables in the constructor), but\nyou get the idea.\nFor this example, we’ll want to store the population of bloops in an ArrayList, rather than\nan array, as we expect the population to grow and shrink according to how often bloops die\nor are born. We can store this ArrayList in a class called World, which will manage all the\nelements of the bloops’ world.\nSo far, what we have is just a rehashing of our particle system example from Chapter 5. We\nhave an entity (Bloop) that moves around the window and a class (World) that manages a\nvariable quantity of these entities. To turn this into a system that evolves, we need to add\ntwo additional features to our world:\n•\nBloops die.\nBloops die.\n•\nBloops are born.\nBloops are born.\nSome variables for Perlin noise\ncalculations\nfloat xoff, yoff;\nvoid update() {\nfloat vx = map(noise(xoff),0,1,-maxspeed,maxspeed);\nfloat vy = map(noise(yoff),0,1,-maxspeed,maxspeed);\nA little Perlin noise algorithm to calculate a\nvelocity\nPVector velocity = new PVector(vx,vy);\nxoff += 0.01;\nyoff += 0.01;\nThe bloop moves.\nlocation.add(velocity);\n}\nA bloop is a circle.\nvoid display() {\nellipse(location.x, location.y, r, r);\n}\n}\nclass World {\nA list of bloops\nArrayList<Bloop> bloops;\nWorld(int num) {\nbloops = new ArrayList<Bloop>();\nfor (int i = 0; i < num; i++) {\nMaking an initial population of bloops\nbloops.add(new Bloop());\n}\n}\nChapter 9. The Evolution of Code\n436\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 607
  },
  {
    "chunk_full": "Bloops dying is our replacement for a fitness function, the process of “selection.” If a bloop\ndies, it cannot be selected to be a parent, because it simply no longer exists! One way we can\nbuild a mechanism to ensure bloop deaths in our world is by adding a health variable to the\nBloop class.\nIn each frame of animation, a bloop loses some health.\nIf health drops below 0, the bloop dies.\nThis is a good first step, but we haven’t really achieved anything. After all, if all bloops start\nwith 100 health points and lose 1 point per frame, then all bloops will live for the exact same\namount of time and die together. If every single bloop lives the same amount of time, they all\nhave equal chances of reproducing and therefore nothing will evolve.\nThere are many ways we could achieve variable lifespans with a more sophisticated world. For\nexample, we could introduce predators that eat bloops. Perhaps the faster bloops would be\nable to escape being eaten more easily, and therefore our world would evolve to have faster\nand faster bloops. Another option would be to introduce food. When a bloop eats food, it\nincreases its health points, and therefore extends its life.\nLet’s assume we have an ArrayList of PVector locations for food, named “food.” We could\ntest each bloop’s proximity to each food location. If the bloop is close enough, it eats the food\n(which is then removed from the world) and increases its health.\nclass Bloop {\nA bloop is born with 100 health points.\nfloat health = 100;\nvoid update() {\nAll that other stuff for movement\nDeath is always looming!\nhealth -= 1;\n}\nWe add a function to the Bloop class to test\nif the bloop is alive or dead.\nboolean dead() {\nif (health < 0.0) {\nreturn true;\n} else {\nreturn false;\n}\n}\nvoid eat() {\nfor (int i = food.size()-1; i >= 0; i--) {\nPVector foodLocation = food.get(i);\nfloat d = PVector.dist(location, foodLocation);\nThe Nature of Code (v1.0)\n437\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 608
  },
  {
    "chunk_full": "Now we have a scenario in which bloops that eat more food live longer and have a greater\nlikelihood of reproducing. Therefore, we expect that our system would evolve bloops with\nan optimal ability to find and eat food.\nNow that we have built our world, it’s time to add the components required for evolution.\nFirst we should establish our genotype and phenotype.\nIs the Bloop close to the food?\nif (d < r/2) {\nIf so, it gets 100 more health points.\nhealth += 100;\nThe food is no longer available for other\nBloops.\nfood.remove(i);\n}\n}\n}\nGenotype and Phenotype\nGenotype and Phenotype\nThe ability for a bloop to find food is tied to two variables—size and speed. Bigger bloops\nwill find food more easily simply because their size will allow them to intersect with food\nlocations more often. And faster bloops will find more food because they can cover more\nground in a shorter period of time.\nSince size and speed are inversely related\n(large bloops are slow, small bloops are\nfast), we only need a genotype with a\nsingle number.\nFigure 9.15\nclass DNA {\nfloat[] genes;\nDNA() {\nChapter 9. The Evolution of Code\n438\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 609
  },
  {
    "chunk_full": "The phenotype then is the bloop itself, whose size and speed is assigned by adding an\ninstance of a DNA object to the Bloop class.\nNotice that with maxspeed, the range is mapped to between 15 and 0, meaning a bloop with a\ngene value of 0 moves at a speed of 15 and a bloop with a gene value of 1 doesn’t move at all\n(speed of 0).\nIt may seem absurd to use an array when all\nwe have is a single value, but we stick with\nan array in case we want to make more\nsophisticated bloops later.\ngenes = new float[1];\nfor (int i = 0; i < genes.length; i++) {\ngenes[i] = random(0,1);\n}\n}\nclass Bloop {\nPVector location;\nfloat health;\nA bloop now has DNA.\nDNA dna;\nfloat r;\nfloat maxspeed;\nBloop(DNA dna_) {\nlocation = new PVector(width/2,height/2);\nhealth = 200;\ndna = dna_;\nmaxspeed and r (radius) are mapped to\nvalues according to the DNA.\nmaxspeed = map(dna.genes[0], 0, 1, 15, 0);\nr\n= map(dna.genes[0], 0, 1, 0, 50);\n}\nSelection and Reproduction\nSelection and Reproduction\nNow that we have the genotype and phenotype, we need to move on to devising a means for\nbloops to be selected as parents. We stated before that the longer a bloop lives, the more\nchances it has to reproduce. The length of life is the bloop’s fitness.\nOne option would be to say that whenever two bloops come into contact with each other, they\nmake a new bloop. The longer a bloop lives, the more likely it is to come into contact with\nanother bloop. (This would also affect the evolutionary outcome given that, in addition to\neating food, their ability to find other bloops is a factor in the likelihood of having a baby.)\nA simpler option would be to have “asexual” reproduction, meaning a bloop does not require\na partner. It can, at any moment, make a clone of itself, another bloop with the same genetic\nmakeup. If we state this selection algorithm as follows:\nAt any given moment, a bloop has a 1% chance of reproducing.\nAt any given moment, a bloop has a 1% chance of reproducing.\nThe Nature of Code (v1.0)\n439\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 610
  },
  {
    "chunk_full": "…then the longer a bloop lives, the more likely it will make at least one child. This is\nequivalent to saying the more times you play the lottery, the greater the likelihood you’ll win\n(though I’m sorry to say your chances of that are still essentially zero).\nTo implement this selection algorithm, we can write a function in the Bloop class that picks\na random number every frame. If the number is less than 0.01 (1%), a new bloop is born.\nHow does a bloop reproduce? In our previous examples, the reproduction process involved\ncalling the crossover() function in the DNA class and making a new object from the newly\nmade DNA. Here, since we are making a child from a single parent, we’ll call a function\ncalled copy() instead.\nNote also that we’ve reduced the probability of reproducing from 1% to 0.05%. This value\nmakes quite a difference; with a high probability of reproducing, the system will quickly tend\ntowards overpopulation. Too low a probability, and everything will likely quickly die out.\nWriting the copy() function into the DNA class is easy since Processing includes a function\narraycopy() that copies the contents of one array into another.\nThis function will return a new bloop, the\nchild.\nBloop reproduce() {\nA 1% chance of executing the code in this\nconditional, i.e. a 1% chance of\nreproducing\nif (random(1) < 0.01) {\n// Make the Bloop baby\n}\n}\nBloop reproduce() {\nif (random(1) < 0.0005) {\nMake a copy of the DNA.\nDNA childDNA = dna.copy();\n1% mutation rate\nchildDNA.mutate(0.01);\nMake a new bloop at the same location\nwith the new DNA.\nreturn new Bloop(location, childDNA);\n} else {\nIf the bloop does not reproduce, return null.\nreturn null;\n}\n}\nclass DNA {\nThis copy() function replaces crossover() in\nthis example.\nDNA copy() {\nChapter 9. The Evolution of Code\n440\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 611
  },
  {
    "chunk_full": "Now that we have all the pieces in place for selection and reproduction, we can finalize the\nWorld class that manages the list of all Bloop objects (as well as a Food object, which itself is\na list of PVector locations for food).\nBefore you run the example, take a moment to guess what size and speed of bloops the\nsystem will evolve towards. We’ll discuss following the code.\nExample 9.5: Evolution ecosystem\nMake a new array the same length and copy\nits contents.\nfloat[] newgenes = new float[genes.length];\narraycopy(genes,newgenes);\nreturn new DNA(newgenes);\n}\n}\nWorld world;\nsetup() and draw() do nothing more than\ncreate and run a World object.\nvoid setup() {\nsize(600,400);\nworld = new World(20);\n}\nvoid draw() {\nbackground(255);\nworld.run();\n}\nclass World {\nThe World object keeps track of the\npopulation bloops as well as the food.\nArrayList<Bloop> bloops;\nFood food;\nWorld(int num) {\nfood = new Food(num);\nbloops = new ArrayList<Bloop>();\nThe Nature of Code (v1.0)\n441\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 612
  },
  {
    "chunk_full": "If you guessed medium-sized bloops with medium speed, you were right. With the design of\nthis system, bloops that are large are simply too slow to find food. And bloops that are fast\nare too small to find food. The ones that are able to live the longest tend to be in the\nmiddle, large enough and fast enough to find food (but not too large or too fast). There are\nalso some anomalies. For example, if it so happens that a bunch of large bloops end up in\nthe same location (and barely move because they are so large), they may all die out\nsuddenly, leaving a lot of food for one large bloop who happens to be there to eat and\nallowing a mini-population of large bloops to sustain themselves for a period of time in one\nlocation.\nThis example is rather simplistic given its single gene and asexual reproduction. Here are\nsome suggestions for how you might apply the bloop example in a more elaborate\necosystem simulation.\nCreating the population\nfor (int i = 0; i < num; i++) {\nPVector location = new PVector(random(width),random(height));\nDNA dna = new DNA();\nbloops.add(new Bloop(l,dna));\n}\n}\nvoid run() {\nfood.run();\nfor (int i = bloops.size()-1; i >= 0; i--) {\nThe bloops live their life.\nBloop b = bloops.get(i);\nb.run();\nb.eat(food);\nIf one dies, it is removed from the\npopulation and food is added at its\nlocation.\nif (b.dead()) {\nbloops.remove(i);\nfood.add(b.location);\n}\nHere is where each living bloop has a\nchance to reproduce. As long as a child is\nmade (i.e. not null) it is added to the\npopulation.\nBloop child = b.reproduce();\nif (child != null) bloops.add(child);\n}\n}\n}\nChapter 9. The Evolution of Code\n442\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 613
  },
  {
    "chunk_full": "The Ecosystem Project\nThe Ecosystem Project\nStep 9 Exercise:\nAdd evolution to your ecosystem, building from the examples in this chapter.\n•\nAdd a population of predators to your ecosystem. Biological evolution\nbetween predators and prey (or parasites and hosts) is often referred to as\nan “arms race,” in which the creatures continuously adapt and counter-\nadapt to each other. Can you achieve this behavior in a system of multiple\ncreatures?\n•\nHow would you implement crossover and mutation between two parents in\nan ecosystem modeled after the bloops? Try implementing an algorithm so\nthat two creatures meet and mate when within a certain proximity. Can\nyou make creatures with gender?\n•\nTry using the weights of multiple steering forces as a creature’s DNA. Can\nyou create a scenario in which creatures evolve to cooperate with each\nother?\n•\nOne of the greatest challenges in ecosystem simulations is achieving a\nnice balance. You will likely find that most of your attempts result in either\nmass overpopulation (followed by mass extinction) or simply mass\nextinction straight away. What techniques can you employ to achieve\nbalance? Consider using the genetic algorithm itself to evolve optimal\nparameters for an ecosystem.\nThe Nature of Code (v1.0)\n443\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 614
  },
  {
    "chunk_full": "Chapter 10. Neural\nChapter 10. Neural\nNetworks\nNetworks\n“You can’t process me with a normal brain.”\n— Charlie Sheen\nWe’re at the end of our story. This is the last official chapter of this book (though I envision\nadditional supplemental material for the website and perhaps new chapters in the future).\nWe began with inanimate objects living in a world of forces and gave those objects desires,\nautonomy, and the ability to take action according to a system of rules. Next, we allowed\nthose objects to live in a population and evolve over time. Now we ask: What is each\nobject’s decision-making process? How can it adjust its choices by learning over time? Can\na computational entity process its environment and generate a decision?\nThe human brain can be described as a biological neural network—an interconnected web\nof neurons transmitting elaborate patterns of electrical signals. Dendrites receive input\nsignals and, based on those inputs, fire an output signal via an axon. Or something like that.\nHow the human brain actually works is an elaborate and complex mystery, one that we\ncertainly are not going to attempt to tackle in rigorous detail in this chapter.\nChapter 10. Neural Networks\n444\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 615
  },
  {
    "chunk_full": "The good news is that developing engaging animated systems with code does not require\nscientific rigor or accuracy, as we’ve learned throughout this book. We can simply be inspired\nby the idea of brain function.\nIn this chapter, we’ll begin with a conceptual overview of the properties and features of neural\nnetworks and build the simplest possible example of one (a network that consists of a single\nneuron). Afterwards, we’ll examine strategies for creating a “Brain” object that can be inserted\ninto our Vehicle class and used to determine steering. Finally, we’ll also look at techniques\nfor visualizing and animating a network of neurons.\nFigure 10.1\n10.1 Artificial Neural Networks: Introduction and\n10.1 Artificial Neural Networks: Introduction and\nApplication\nApplication\nComputer scientists have long been inspired by the human brain. In 1943, Warren S.\nMcCulloch, a neuroscientist, and Walter Pitts, a logician, developed the first conceptual model\nof an artificial neural network. In their paper, \"A logical calculus of the ideas imminent in\nnervous activity,” they describe the concept of a neuron, a single cell living in a network of\ncells that receives inputs, processes those inputs, and generates an output.\nTheir work, and the work of many scientists and researchers that followed, was not meant to\naccurately describe how the biological brain works. Rather, an artificial neural network (which\nwe will now simply refer to as a “neural network”) was designed as a computational model\nbased on the brain to solve certain kinds of problems.\nIt’s probably pretty obvious to you that there are problems that are incredibly simple for a\ncomputer to solve, but difficult for you. Take the square root of 964,324, for example. A quick\nline of code produces the value 982, a number Processing computed in less than a\nmillisecond. There are, on the other hand, problems that are incredibly simple for you or me to\nsolve, but not so easy for a computer. Show any toddler a picture of a kitten or puppy and\nthey’ll be able to tell you very quickly which one is which. Say hello and shake my hand one\nmorning and you should be able to pick me out of a crowd of people the next day. But need a\nmachine to perform one of these tasks? Scientists have already spent entire careers\nresearching and implementing complex solutions.\nThe Nature of Code (v1.0)\n445\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 616
  },
  {
    "chunk_full": "The most common application of neural networks in computing today is to perform one of\nthese “easy-for-a-human, difficult-for-a-machine” tasks, often referred to as pattern\nrecognition. Applications range from optical character recognition (turning printed or\nhandwritten scans into digital text) to facial recognition. We don’t have the time or need to\nuse some of these more elaborate artificial intelligence algorithms here, but if you are\ninterested in researching neural networks, I’d recommend the books Artificial Intelligence: A\nModern Approach by Stuart J. Russell and Peter Norvig and AI for Game Developers by\nDavid M. Bourg and Glenn Seemann.\nA neural network is a “connectionist”\ncomputational system. The computational\nsystems we write are procedural; a\nprogram starts at the first line of code,\nexecutes it, and goes on to the next,\nfollowing instructions in a linear fashion. A\ntrue neural network does not follow a linear\npath. Rather, information is processed\ncollectively, in parallel throughout a\nnetwork of nodes (the nodes, in this case,\nbeing neurons).\nHere we have yet another example of a\ncomplex system, much like the ones we\nexamined in Chapters 6, 7, and 8. The\nindividual elements of the network, the neurons, are simple. They read an input, process it,\nand generate an output. A network of many neurons, however, can exhibit incredibly rich\nand intelligent behaviors.\nOne of the key elements of a neural network is its ability to learn. A neural network is not\njust a complex system, but a complex adaptive\nadaptive system, meaning it can change its internal\nstructure based on the information flowing through it. Typically, this is achieved through the\nadjusting of weights. In the diagram above, each line represents a connection between two\nneurons and indicates the pathway for the flow of information. Each connection has a\nweight\nweight, a number that controls the signal between the two neurons. If the network\ngenerates a “good” output (which we’ll define later), there is no need to adjust the weights.\nHowever, if the network generates a “poor” output—an error, so to speak—then the system\nadapts, altering the weights in order to improve subsequent results.\nThere are several strategies for learning, and we’ll examine two of them in this chapter.\n•\nSupervised Learning\nSupervised Learning —Essentially, a strategy that involves a teacher that is\nsmarter than the network itself. For example, let’s take the facial recognition\nexample. The teacher shows the network a bunch of faces, and the teacher\nalready knows the name associated with each face. The network makes its\nguesses, then the teacher provides the network with the answers. The network\ncan then compare its answers to the known “correct” ones and make adjustments\nFigure 10.2\nChapter 10. Neural Networks\n446\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 617
  },
  {
    "chunk_full": "according to its errors. Our first neural network in the next section will follow this\nmodel.\n•\nUnsupervised Learning\nUnsupervised Learning —Required when there isn’t an example data set with known\nanswers. Imagine searching for a hidden pattern in a data set. An application of this\nis clustering, i.e. dividing a set of elements into groups according to some unknown\npattern. We won’t be looking at any examples of unsupervised learning in this\nchapter, as this strategy is less relevant for our examples.\n•\nReinforcement Learning\nReinforcement Learning —A strategy built on observation. Think of a little mouse\nrunning through a maze. If it turns left, it gets a piece of cheese; if it turns right, it\nreceives a little shock. (Don’t worry, this is just a pretend mouse.) Presumably, the\nmouse will learn over time to turn left. Its neural network makes a decision with an\noutcome (turn left or right) and observes its environment (yum or ouch). If the\nobservation is negative, the network can adjust its weights in order to make a\ndifferent decision the next time. Reinforcement learning is common in robotics. At\ntime t, the robot performs a task and observes the results. Did it crash into a wall or\nfall off a table? Or is it unharmed? We’ll look at reinforcement learning in the context\nof our simulated steering vehicles.\nThis ability of a neural network to learn, to make adjustments to its structure over time, is what\nmakes it so useful in the field of artificial intelligence. Here are some standard uses of neural\nnetworks in software today.\n•\nPattern Recognition\nPattern Recognition —We’ve mentioned this several times already and it’s probably\nthe most common application. Examples are facial recognition, optical character\nrecognition, etc.\n•\nTime Series Prediction\nTime Series Prediction —Neural networks can be used to make predictions. Will the\nstock rise or fall tomorrow? Will it rain or be sunny?\n•\nSignal Processing\nSignal Processing —Cochlear implants and hearing aids need to filter out\nunnecessary noise and amplify the important sounds. Neural networks can be\ntrained to process an audio signal and filter it appropriately.\n•\nControl\nControl —You may have read about recent research advances in self-driving cars.\nNeural networks are often used to manage steering decisions of physical vehicles\n(or simulated ones).\n•\nSoft Sensors\nSoft Sensors —A soft sensor refers to the process of analyzing a collection of many\nmeasurements. A thermometer can tell you the temperature of the air, but what if\nyou also knew the humidity, barometric pressure, dewpoint, air quality, air density,\netc.? Neural networks can be employed to process the input data from many\nindividual sensors and evaluate them as a whole.\n•\nAnomaly Detection\nAnomaly Detection —Because neural networks are so good at recognizing patterns,\nthey can also be trained to generate an output when something occurs that doesn’t\nThe Nature of Code (v1.0)\n447\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 618
  },
  {
    "chunk_full": "fit the pattern. Think of a neural network monitoring your daily routine over a long\nperiod of time. After learning the patterns of your behavior, it could alert you when\nsomething is amiss.\nThis is by no means a comprehensive list of applications of neural networks. But hopefully it\ngives you an overall sense of the features and possibilities. The thing is, neural networks\nare complicated and difficult. They involve all sorts of fancy mathematics. While this is all\nfascinating (and incredibly important to scientific research), a lot of the techniques are not\nvery practical in the world of building interactive, animated Processing sketches. Not to\nmention that in order to cover all this material, we would need another book—or more likely,\na series of books.\nSo instead, we’ll begin our last hurrah in the nature of code with the simplest of all neural\nnetworks, in an effort to understand how the overall concepts are applied in code. Then\nwe’ll look at some Processing sketches that generate visual results inspired by these\nconcepts.\n10.2 The Perceptron\n10.2 The Perceptron\nInvented in 1957 by Frank Rosenblatt at the Cornell Aeronautical Laboratory, a perceptron is\nthe simplest neural network possible: a computational model of a single neuron. A\nperceptron consists of one or more inputs, a processor, and a single output.\nA perceptron follows the “feed-forward” model, meaning inputs are sent into the neuron,\nare processed, and result in an output. In the diagram above, this means the network (one\nneuron) reads from left to right: inputs come in, output goes out.\nLet’s follow each of these steps in more detail.\nStep 1: Receive inputs.\nSay we have a perceptron with two inputs—let’s call them x1 and x2.\nFigure 10.3: The perceptron\nChapter 10. Neural Networks\n448\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 619
  },
  {
    "chunk_full": "Input 0: x1 = 12\nInput 1: x2 = 4\nStep 2: Weight inputs.\nEach input that is sent into the neuron must first be weighted, i.e. multiplied by some value\n(often a number between -1 and 1). When creating a perceptron, we’ll typically begin by\nassigning random weights. Here, let’s give the inputs the following weights:\nWeight 0: 0.5\nWeight 1: -1\nWe take each input and multiply it by its weight.\nInput 0 * Weight 0 ⇒ 12 * 0.5 = 6\nInput 1 * Weight 1 ⇒ 4 * -1 = -4\nStep 3: Sum inputs.\nThe weighted inputs are then summed.\nSum = 6 + -4 = 2\nStep 4: Generate output.\nThe output of a perceptron is generated by passing that sum through an activation function. In\nthe case of a simple binary output, the activation function is what tells the perceptron whether\nto “fire” or not. You can envision an LED connected to the output signal: if it fires, the light\ngoes on; if not, it stays off.\nActivation functions can get a little bit hairy. If you start reading one of those artificial\nintelligence textbooks looking for more info about activation functions, you may soon find\nyourself reaching for a calculus textbook. However, with our friend the simple perceptron,\nwe’re going to do something really easy. Let’s make the activation function the sign of the\nsum. In other words, if the sum is a positive number, the output is 1; if it is negative, the output\nis -1.\nOutput = sign(sum) ⇒ sign(2) ⇒ +1\nThe Nature of Code (v1.0)\n449\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 620
  },
  {
    "chunk_full": "Let’s review and condense these steps so we can implement them with a code snippet.\nThe Perceptron Algorithm:\nThe Perceptron Algorithm:\n1.\nFor every input, multiply that input by its weight.\n2.\nSum all of the weighted inputs.\n3.\nCompute the output of the perceptron based on that sum passed through an\nactivation function (the sign of the sum).\nLet’s assume we have two arrays of numbers, the inputs and the weights. For example:\n“For every input” implies a loop that multiplies each input by its corresponding weight.\nSince we need the sum, we can add up the results in that very loop.\nOnce we have the sum we can compute the output.\nfloat[] inputs\n= {12 , 4};\nfloat[] weights = {0.5,-1};\nSteps 1 and 2: Add up all the weighted\ninputs.\nfloat sum = 0;\nfor (int i = 0; i < inputs.length; i++) {\nsum += inputs[i]*weights[i];\n}\nStep 3: Passing the sum through an\nactivation function\nfloat output = activate(sum);\nThe activation function\nint activate(float sum) {\nReturn a 1 if positive, -1 if negative.\nif (sum > 0) return 1;\nelse return -1;\n}\n10.3 Simple Pattern Recognition Using a\n10.3 Simple Pattern Recognition Using a\nPerceptron\nPerceptron\nNow that we understand the computational process of a perceptron, we can look at an\nexample of one in action. We stated that neural networks are often used for pattern\nrecognition applications, such as facial recognition. Even simple perceptrons can\ndemonstrate the basics of classification, as in the following example.\nChapter 10. Neural Networks\n450\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 621
  },
  {
    "chunk_full": "Consider a line in two-dimensional space.\nPoints in that space can be classified as\nliving on either one side of the line or the\nother. While this is a somewhat silly example\n(since there is clearly no need for a neural\nnetwork; we can determine on which side a\npoint lies with some simple algebra), it\nshows how a perceptron can be trained to\nrecognize points on one side versus\nanother.\nLet’s say a perceptron has 2 inputs (the x-\nand y-coordinates of a point). Using a sign activation function, the output will either be -1 or\n1—i.e., the input data is classified according to the sign of the output. In the above diagram,\nwe can see how each point is either below the line (-1) or above (+1).\nThe perceptron itself can be diagrammed as follows:\nWe can see how there are two inputs (x and y), a weight for each input (weightx and weighty),\nas well as a processing neuron that generates the output.\nThere is a pretty significant problem here, however. Let’s consider the point (0,0). What if we\nsend this point into the perceptron as its input: x = 0 and y = 0? What will the sum of its\nweighted inputs be? No matter what the weights are, the sum will always be 0! But this can’t\nbe right—after all, the point (0,0) could certainly be above or below various lines in our two-\ndimensional world.\nTo avoid this dilemma, our perceptron will require a third input, typically referred to as a bias\nbias\ninput. A bias input always has the value of 1 and is also weighted. Here is our perceptron with\nthe addition of the bias:\nFigure 10.4\nFigure 10.5\nThe Nature of Code (v1.0)\n451\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 622
  },
  {
    "chunk_full": "Let’s go back to the point (0,0). Here are our inputs:\n0 * weight for x = 0\n0 * weight for y = 0\n1 * weight for bias = weight for bias\nThe output is the sum of the above three values, 0 plus 0 plus the bias’s weight. Therefore,\nthe bias, on its own, answers the question as to where (0,0) is in relation to the line. If the\nbias’s weight is positive, (0,0) is above the line; negative, it is below. It “biases” the\nperceptron’s understanding of the line’s position relative to (0,0).\nFigure 10.6\n10.4 Coding the Perceptron\n10.4 Coding the Perceptron\nWe’re now ready to assemble the code for a Perceptron class. The only data the\nperceptron needs to track are the input weights, and we could use an array of floats to store\nthese.\nThe constructor could receive an argument indicating the number of inputs (in this case\nthree: x, y, and a bias) and size the array accordingly.\nA perceptron needs to be able to receive inputs and generate an output. We can package\nthese requirements into a function called feedforward(). In this example, we’ll have the\nclass Perceptron {\nfloat[] weights;\nPerceptron(int n) {\nweights = new float[n];\nfor (int i = 0; i < weights.length; i++) {\nThe weights are picked randomly to start.\nweights[i] = random(-1,1);\n}\n}\nChapter 10. Neural Networks\n452\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 623
  },
  {
    "chunk_full": "perceptron receive its inputs as an array (which should be the same length as the array of\nweights) and return the output as an integer.\nPresumably, we could now create a Perceptron object and ask it to make a guess for any\ngiven point.\nDid the perceptron get it right? At this point, the perceptron has no better than a 50/50\nchance of arriving at the right answer. Remember, when we created it, we gave each weight a\nrandom value. A neural network isn’t magic. It’s not going to be able to guess anything\ncorrectly unless we teach it how to!\nTo train a neural network to answer correctly, we’re going to employ the method of\nsupervised learning that we described in section 10.1 (see page 445).\nWith this method, the network is provided with inputs for which there is a known answer. This\nway the network can find out if it has made a correct guess. If it’s incorrect, the network can\nlearn from its mistake and adjust its weights. The process is as follows:\n1.\nProvide the perceptron with inputs for which there is a known answer.\nint feedforward(float[] inputs) {\nfloat sum = 0;\nfor (int i = 0; i < weights.length; i++) {\nsum += inputs[i]*weights[i];\n}\nResult is the sign of the sum, -1 or +1. Here\nthe perceptron is making a guess. Is it on\none side of the line or the other?\nreturn activate(sum);\n}\nFigure 10.7\nCreate the Perceptron.\nPerceptron p = new Perceptron(3);\nThe input is 3 values: x,y and bias.\nfloat[] point = {50,-12,1};\nThe answer!\nint result = p.feedforward(point);\nThe Nature of Code (v1.0)\n453\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 624
  },
  {
    "chunk_full": "2.\nAsk the perceptron to guess an answer.\n3.\nCompute the error. (Did it get the answer right or wrong?)\n4.\nAdjust all the weights according to the error.\n5.\nReturn to Step 1 and repeat!\nSteps 1 through 4 can be packaged into a function. Before we can write the entire function,\nhowever, we need to examine Steps 3 and 4 in more detail. How do we define the\nperceptron’s error? And how should we adjust the weights according to this error?\nThe perceptron’s error can be defined as the difference between the desired answer and its\nguess.\nERROR = DESIRED OUTPUT - GUESS OUTPUT\nThe above formula may look familiar to you. In Chapter 6 (see page 263), we computed a\nsteering force as the difference between our desired velocity and our current velocity.\nSTEERING = DESIRED VELOCITY - CURRENT VELOCITY\nThis was also an error calculation. The current velocity acts as a guess and the error (the\nsteering force) tells us how to adjust the velocity in the right direction. In a moment, we’ll\nsee how adjusting the vehicle’s velocity to follow a target is just like adjusting the weights\nof a neural network to arrive at the right answer.\nIn the case of the perceptron, the output has only two possible values: +1\n+1 or -1-1. This means\nthere are only three possible errors.\nIf the perceptron guesses the correct answer, then the guess equals the desired output and\nthe error is 0. If the correct answer is -1 and we’ve guessed +1, then the error is -2. If the\ncorrect answer is +1 and we’ve guessed -1, then the error is +2.\nDesired\nDesired\nGuess\nGuess\nError\nError\n-1\n-1\n0\n-1\n+1\n-2\n+1\n-1\n+2\n+1\n+1\n0\nChapter 10. Neural Networks\n454\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 625
  },
  {
    "chunk_full": "The error is the determining factor in how the perceptron’s weights should be adjusted. For\nany given weight, what we are looking to calculate is the change in weight, often called\nΔweight (or “delta” weight, delta being the Greek letter Δ).\nNEW WEIGHT = WEIGHT + ΔWEIGHT\nΔweight is calculated as the error multiplied by the input.\nΔWEIGHT = ERROR * INPUT\nTherefore:\nNEW WEIGHT = WEIGHT + ERROR * INPUT\nTo understand why this works, we can again return to steering (see page 263). A steering\nforce is essentially an error in velocity. If we apply that force as our acceleration (Δvelocity),\nthen we adjust our velocity to move in the correct direction. This is what we want to do with\nour neural network’s weights. We want to adjust them in the right direction, as defined by the\nerror.\nWith steering, however, we had an additional variable that controlled the vehicle’s ability to\nsteer: the maximum force. With a high maximum force, the vehicle was able to accelerate and\nturn very quickly; with a lower force, the vehicle would take longer to adjust its velocity. The\nneural network will employ a similar strategy with a variable called the “learning constant.”\nWe’ll add in the learning constant as follows:\nNEW WEIGHT = WEIGHT + ERROR * INPUT * LEARNING CONSTANT\nNotice that a high learning constant means the weight will change more drastically. This may\nhelp us arrive at a solution more quickly, but with such large changes in weight it’s possible\nwe will overshoot the optimal weights. With a small learning constant, the weights will be\nadjusted slowly, requiring more training time but allowing the network to make very small\nadjustments that could improve the network’s overall accuracy.\nAssuming the addition of a variable c for the learning constant, we can now write a training\nfunction for the perceptron following the above steps.\nA new variable is introduced to control the\nlearning rate.\nfloat c = 0.01;\nStep 1: Provide the inputs and known\nanswer. These are passed in as arguments\nto train().\nvoid train(float[] inputs, int desired) {\nStep 2: Guess according to those inputs.\nint guess = feedforward(inputs);\nStep 3: Compute the error (difference\nbetween answer and guess).\nfloat error = desired - guess;\nThe Nature of Code (v1.0)\n455\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 626
  },
  {
    "chunk_full": "We can now see the Perceptron class as a whole.\nTo train the perceptron, we need a set of inputs with a known answer. We could package\nthis up in a class like so:\nStep 4: Adjust all the weights according to\nthe error and learning constant.\nfor (int i = 0; i < weights.length; i++) {\nweights[i] += c * error * inputs[i];\n}\n}\nclass Perceptron {\nThe Perceptron stores its weights and\nlearning constants.\nfloat[] weights;\nfloat c = 0.01;\nPerceptron(int n) {\nweights = new float[n];\nWeights start off random.\nfor (int i = 0; i < weights.length; i++) {\nweights[i] = random(-1,1);\n}\n}\nReturn an output based on inputs.\nint feedforward(float[] inputs) {\nfloat sum = 0;\nfor (int i = 0; i < weights.length; i++) {\nsum += inputs[i]*weights[i];\n}\nreturn activate(sum);\n}\nOutput is a +1 or -1.\nint activate(float sum) {\nif (sum > 0) return 1;\nelse return -1;\n}\nTrain the network against known data.\nvoid train(float[] inputs, int desired) {\nint guess = feedforward(inputs);\nfloat error = desired - guess;\nfor (int i = 0; i < weights.length; i++) {\nweights[i] += c * error * inputs[i];\n}\n}\n}\nclass Trainer {\nChapter 10. Neural Networks\n456\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 627
  },
  {
    "chunk_full": "Now the question becomes, how do we pick a point and know whether it is above or below a\nline? Let’s start with the formula for a line, where y is calculated as a function of x:\ny = f(x)\nIn generic terms, a line can be described as:\ny = ax + b\nHere’s a specific example:\ny = 2*x + 1\nWe can then write a Processing function with this in mind.\nSo, if we make up a point:\nHow do we know if this point is above or below the line? The line function f(x) gives us the y\nvalue on the line for that x position. Let’s call that yline.\nIf the y value we are examining is above the line, it will be less than yline.\nA \"Trainer\" object stores the inputs and the\ncorrect answer.\nfloat[] inputs;\nint answer;\nTrainer(float x, float y, int a) {\ninputs = new float[3];\ninputs[0] = x;\ninputs[1] = y;\nNote that the Trainer has the bias input built\ninto its array.\ninputs[2] = 1;\nanswer = a;\n}\n}\nA function to calculate y based on x along a\nline\nfloat f(float x) {\nreturn 2*x+1;\n}\nfloat x = random(width);\nfloat y = random(height);\nThe y position on the line\nfloat yline = f(x);\nThe Nature of Code (v1.0)\n457\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 628
  },
  {
    "chunk_full": "We can then make a Trainer object with the inputs and the correct answer.\nAssuming we had a Perceptron object ptron, we could then train it by sending the inputs\nalong with the known answer.\nNow, it’s important to remember that this is just a demonstration. Remember our\nShakespeare-typing monkeys (see page 392)? We asked our genetic algorithm to solve for\n“to be or not to be”—an answer we already knew. We did this to make sure our genetic\nalgorithm worked properly. The same reasoning applies to this example. We don’t need a\nperceptron to tell us whether a point is above or below a line; we can do that with simple\nmath. We are using this scenario, one that we can easily solve without a perceptron, to\ndemonstrate the perceptron’s algorithm as well as easily confirm that it is working properly.\nLet’s look at how the perceptron works with an array of many training points.\nFigure 10.8\nif (y < yline) {\nThe answer is -1 if y is above the line.\nanswer = -1;\n} else {\nanswer = 1;\n}\nTrainer t = new Trainer(x, y, answer);\nptron.train(t.inputs,t.answer);\nChapter 10. Neural Networks\n458\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 629
  },
  {
    "chunk_full": "Example 10.1: The Perceptron\nThe Perceptron\nPerceptron ptron;\n2,000 training points\nTrainer[] training = new Trainer[2000];\nint count = 0;\nThe formula for a line\nfloat f(float x) {\nreturn 2*x+1;\n}\nvoid setup() {\nsize(640, 360);\nptron = new Perceptron(3);\nMake 2,000 training points.\nfor (int i = 0; i < training.length; i++) {\nfloat x = random(-width/2,width/2);\nfloat y = random(-height/2,height/2);\nIs the correct answer 1 or -1?\nint answer = 1;\nif (y < f(x)) answer = -1;\ntraining[i] = new Trainer(x, y, answer);\n}\n}\nvoid draw() {\nbackground(255);\ntranslate(width/2,height/2);\nptron.train(training[count].inputs, training[count].answer);\nFor animation, we are training one point at a\ntime.\ncount = (count + 1) % training.length;\nfor (int i = 0; i < count; i++) {\nstroke(0);\nint guess = ptron.feedforward(training[i].inputs);\nThe Nature of Code (v1.0)\n459\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 630
  },
  {
    "chunk_full": "Show the classification—no fill for -1,\nblack for +1.\nif (guess > 0) noFill();\nelse\nfill(0);\nellipse(training[i].inputs[0], training[i].inputs[1], 8, 8);\n}\n}\nInstead of using the supervised learning model above, can you train the neural\nnetwork to find the right weights by using a genetic algorithm?\nExercise 10.1\nExercise 10.1\nVisualize the perceptron itself. Draw the inputs, the processing node, and the output.\nExercise 10.2\nExercise 10.2\n10.5 A Steering Perceptron\n10.5 A Steering Perceptron\nWhile classifying points according to their position above or below a line was a useful\ndemonstration of the perceptron in action, it doesn’t have much practical relevance to the\nother examples throughout this book. In this section, we’ll take the concepts of a perceptron\n(array of inputs, single output), apply it to steering behaviors, and demonstrate\nreinforcement learning along the way.\nWe are now going to take significant creative license with the concept of a neural network.\nThis will allow us to stick with the basics and avoid some of the highly complex algorithms\nassociated with more sophisticated neural networks. Here we’re not so concerned with\nfollowing rules outlined in artificial intelligence textbooks—we’re just hoping to make\nsomething interesting and brain-like.\nRemember our good friend the Vehicle class? You know, that one for making objects with\na location, velocity, and acceleration? That could obey Newton’s laws with an\napplyForce() function and move around the window according to a variety of steering\nrules?\nWhat if we added one more variable to our Vehicle class?\nclass Vehicle {\nChapter 10. Neural Networks\n460\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 631
  },
  {
    "chunk_full": "Here’s our scenario. Let’s say we have a Processing sketch with an ArrayList of targets and\na single vehicle.\nLet’s say that the vehicle seeks all of the targets. According to the principles of Chapter 6, we\nwould next write a function that calculates a steering force towards each target, applying each\nforce one at a time to the object’s acceleration. Assuming the targets are an ArrayList of\nPVector objects, it would look something like:\nIn Chapter 6, we also examined how we could create more dynamic simulations by weighting\neach steering force according to some rule. For example, we could say that the farther you\nare from a target, the stronger the force.\nGiving the vehicle a brain!\nPerceptron brain;\nPVector location;\nPVector velocity;\nPVector acceleration;\n//etc...\nFigure 10.9\nvoid seek(ArrayList<PVector> targets) {\nfor (PVector target : targets) {\nFor every target, apply a steering force\ntowards the target.\nPVector force = seek(targets.get(i));\napplyForce(force);\n}\n}\nvoid seek(ArrayList<PVector> targets) {\nfor (PVector target : targets) {\nPVector force = seek(targets.get(i));\nfloat d = PVector.dist(target,location);\nfloat weight = map(d,0,width,0,5);\nThe Nature of Code (v1.0)\n461\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 632
  },
  {
    "chunk_full": "But what if instead we could ask our brain (i.e. perceptron) to take in all the forces as an\ninput, process them according to weights of the perceptron inputs, and generate an output\nsteering force? What if we could instead say:\nIn other words, instead of weighting and accumulating the forces inside our vehicle, we\nsimply pass an array of forces to the vehicle’s “brain” object and allow the brain to weight\nand sum the forces for us. The output is then applied as a steering force. This opens up a\nrange of possibilities. A vehicle could make decisions as to how to steer on its own, learning\nfrom its mistakes and responding to stimuli in its environment. Let’s see how this works.\nWe can use the line classification perceptron as a model, with one important difference—the\ninputs are not single numbers, but vectors! Let’s look at how the feedforward() function\nworks in our vehicle’s perceptron, alongside the one from our previous example.\nVehicle PVector inputs\nVehicle PVector inputs\nLine float inputs\nLine float inputs\nPVector feedforward(PVector[] forces) {\n// Sum is a PVector.\nPVector sum = new PVector();\nfor (int i = 0; i < weights.length; i++) {\n// Vector addition and multiplication\nforces[i].mult(weights[i]);\nsum.add(forces[i]);\n}\n// No activation function\nreturn sum;\n}\nint feedforward(float[] inputs) {\n// Sum is a float.\nfloat sum = 0;\nfor (int i = 0; i < weights.length; i++) {\n// Scalar addition and multiplication\nsum += inputs[i]*weights[i];\n}\n// Activation function\nreturn activate(sum);\n}\nWeighting each steering force individually\nforce.mult(weight);\napplyForce(force);\n}\n}\nvoid seek(ArrayList<PVector> targets) {\nMake an array of inputs for our brain.\nPVector[] forces = new\nPVector[targets.size()];\nfor (int i = 0; i < forces.length; i++) {\nFill the array with a steering force for each\ntarget.\nforces[i] = seek(targets.get(i));\n}\nAsk our brain for a result and apply that as\nthe force!\nPVector output = brain.process(forces);\napplyForce(output);\n}\nChapter 10. Neural Networks\n462\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 633
  },
  {
    "chunk_full": "Note how these two functions implement nearly identical algorithms, with two differences:\n1.\nSumming PVectors.\nSumming PVectors. Instead of a series of numbers added together, each input is a\nPVector and must be multiplied by the weight and added to a sum according to the\nmathematical PVector functions.\n2.\nNo activation function.\nNo activation function. In this case, we’re taking the result and applying it directly\nas a steering force for the vehicle, so we’re not asking for a simple boolean value\nthat classifies it in one of two categories. Rather, we’re asking for raw output itself,\nthe resulting overall force.\nOnce the resulting steering force has been applied, it’s time to give feedback to the brain, i.e.\nreinforcement learning. Was the decision to steer in that particular direction a good one or a\nbad one? Presumably if some of the targets were predators (resulting in being eaten) and\nsome of the targets were food (resulting in greater health), the network would adjust its\nweights in order to steer away from the predators and towards the food.\nLet’s take a simpler example, where the vehicle simply wants to stay close to the center of the\nwindow. We’ll train the brain as follows:\nHere we are passing the brain a copy of all\nthe inputs (which it will need for error\ncorrection) as well as an observation about\nits environment: a PVector that points from\nits current location to where it desires to be.\nThis PVector essentially serves as the\nerror—the longer the PVector, the worse the vehicle is performing; the shorter, the better.\nThe brain can then apply this “error” vector (which has two error values, one for x and one for\ny) as a means for adjusting the weights, just as we did in the line classification example.\nTraining the Vehicle\nTraining the Vehicle\nTraining the Line Classifier\nTraining the Line Classifier\nvoid train(PVector[] forces, PVector error) {\nfor (int i = 0; i < weights.length; i++) {\nweights[i] += c*error.x*forces[i].x;\nweights[i] += c*error.y*forces[i].y;\n}\n}\nvoid train(float[] inputs, int desired) {\nint guess = feedforward(inputs);\nfloat error = desired - guess;\nfor (int i = 0; i < weights.length; i++) {\nweights[i] += c * error * inputs[i];\n}\n}\nPVector desired = new PVector(width/2,height/2);\nPVector error = PVector.sub(desired, location);\nbrain.train(forces,error);\nFigure 10.10\nThe Nature of Code (v1.0)\n463\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 634
  },
  {
    "chunk_full": "Because the vehicle observes its own error, there is no need to calculate one; we can\nsimply receive the error as an argument. Notice how the change in weight is processed\ntwice, once for the error along the x-axis and once for the y-axis.\nWe can now look at the Vehicle class and see how the steer function uses a perceptron to\ncontrol the overall steering force. The new content from this chapter is highlighted.\nExample 10.2: Perceptron steering\nweights[i] += c*error.x*forces[i].x;\nweights[i] += c*error.y*forces[i].y;\nclass Vehicle {\nThe Vehicle now has a brain.\nPerceptron brain;\nSame old variables for physics\nPVector location;\nPVector velocity;\nPVector acceleration;\nfloat maxforce;\nfloat maxspeed;\nThe Vehicle creates a perceptron with n\ninputs and a learning constant.\nVehicle(int n, float x, float y) {\nbrain = new Perceptron(n,0.001);\nacceleration = new PVector(0,0);\nvelocity = new PVector(0,0);\nlocation = new PVector(x,y);\nmaxspeed = 4;\nmaxforce = 0.1;\n}\nChapter 10. Neural Networks\n464\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 635
  },
  {
    "chunk_full": "Same old update() function\nvoid update() {\nvelocity.add(acceleration);\nvelocity.limit(maxspeed);\nlocation.add(velocity);\nacceleration.mult(0);\n}\nSame old applyForce() function\nvoid applyForce(PVector force) {\nacceleration.add(force);\n}\nvoid steer(ArrayList<PVector> targets) {\nPVector[] forces = new PVector[targets.size()];\nfor (int i = 0; i < forces.length; i++) {\nforces[i] = seek(targets.get(i));\n}\nAll the steering forces are inputs.\nPVector result = brain.feedforward(forces);\nThe result is applied.\napplyForce(result);\nPVector desired = new PVector(width/2,height/2);\nPVector error = PVector.sub(desired, location);\nbrain.train(forces,error);\n}\nThe brain is trained according to the\ndistance to the center.\nSame old seek() function\nPVector seek(PVector target) {\nPVector desired = PVector.sub(target,location);\ndesired.normalize();\ndesired.mult(maxspeed);\nPVector steer = PVector.sub(desired,velocity);\nsteer.limit(maxforce);\nreturn steer;\n}\n}\nThe Nature of Code (v1.0)\n465\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 636
  },
  {
    "chunk_full": "Visualize the weights of the network. Try mapping each target’s corresponding\nweight to its brightness.\nExercise 10.3\nExercise 10.3\nTry different rules for reinforcement learning. What if some targets are desirable and\nsome are undesirable?\nExercise 10.4\nExercise 10.4\n10.6 It’s a “Network,” Remember?\n10.6 It’s a “Network,” Remember?\nYes, a perceptron can have multiple inputs, but it is still a lonely neuron. The power of\nneural networks comes in the networking itself. Perceptrons are, sadly, incredibly limited in\ntheir abilities. If you read an AI textbook, it will say that a perceptron can only solve linearly\nlinearly\nseparable\nseparable problems. What’s a linearly separable problem? Let’s take a look at our first\nexample, which determined whether points were on one side of a line or the other.\nOn the left of Figure 10.11, we have classic linearly separable data. Graph all of the\npossibilities; if you can classify the data with a straight line, then it is linearly separable. On\nthe right, however, is non-linearly separable data. You can’t draw a straight line to separate\nthe black dots from the gray ones.\nOne of the simplest examples of a non-linearly separable problem is XOR, or “exclusive or.”\nWe’re all familiar with AND. For A AND B to be true, both A and B must be true. With OR,\neither A or B can be true for A OR B to evaluate as true. These are both linearly separable\nproblems. Let’s look at the solution space, a “truth table.”\nFigure 10.11\nChapter 10. Neural Networks\n466\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 637
  },
  {
    "chunk_full": "See how you can draw a line to separate the true outputs from the false ones?\nXOR is the equivalent of OR and NOT AND. In other words, A XOR B only evaluates to true if\none of them is true. If both are false or both are true, then we get false. Take a look at the\nfollowing truth table.\nThis is not linearly separable. Try to draw a straight line to separate the true outputs from the\nfalse ones—you can’t!\nSo perceptrons can’t even solve something as simple as XOR. But what if we made a network\nout of two perceptrons? If one perceptron can solve OR and one perceptron can solve NOT\nAND, then two perceptrons combined can solve XOR.\nFigure 10.12\nFigure 10.13\nThe Nature of Code (v1.0)\n467\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 638
  },
  {
    "chunk_full": "The above diagram is known as a multi-layered perceptron, a network of many neurons.\nSome are input neurons and receive the inputs, some are part of what’s called a “hidden”\nlayer (as they are connected to neither the inputs nor the outputs of the network directly),\nand then there are the output neurons, from which we read the results.\nTraining these networks is much more complicated. With the simple perceptron, we could\neasily evaluate how to change the weights according to the error. But here there are so\nmany different connections, each in a different layer of the network. How does one know\nhow much each neuron or connection contributed to the overall error of the network?\nThe solution to optimizing weights of a multi-layered network is known as backpropagation\nbackpropagation.\nThe output of the network is generated in the same manner as a perceptron. The inputs\nmultiplied by the weights are summed and fed forward through the network. The difference\nhere is that they pass through additional layers of neurons before reaching the output.\nTraining the network (i.e. adjusting the weights) also involves taking the error (desired result\n- guess). The error, however, must be fed backwards through the network. The final error\nultimately adjusts the weights of all the connections.\nBackpropagation is a bit beyond the scope of this book and involves a fancier activation\nfunction (called the sigmoid function) as well as some basic calculus. If you are interested in\nhow backpropagation works, check the book website (and GitHub repository) for an\nexample that solves XOR using a multi-layered feed forward network with backpropagation.\nInstead, here we’ll focus on a code framework for building the visual architecture of a\nnetwork. We’ll make Neuron objects and Connection objects from which a Network object\ncan be created and animated to show the feed forward process. This will closely resemble\nsome of the force-directed graph examples we examined in Chapter 5 (toxiclibs).\nFigure 10.14\n10.7 Neural Network Diagrams\n10.7 Neural Network Diagrams\nOur goal will be to create the following simple network diagram:\nChapter 10. Neural Networks\n468\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 639
  },
  {
    "chunk_full": "The primary building block for this diagram is a neuron. For the purpose of this example, the\nNeuron class describes an entity with an (x,y) location.\nThe Network class can then manage an ArrayList of neurons, as well as have its own\nlocation (so that each neuron is drawn relative to the network’s center). This is particle\nsystems 101. We have a single element (a neuron) and a network (a “system” of many\nneurons).\nFigure 10.15\nAn incredibly simple Neuron class stores\nand displays the location of a single neuron.\nclass Neuron {\nPVector location;\nNeuron(float x, float y) {\nlocation = new PVector(x, y);\n}\nvoid display() {\nstroke(0);\nfill(0);\nellipse(location.x, location.y, 16, 16);\n}\n}\nA Network is a list of neurons.\nclass Network {\nArrayList<Neuron> neurons;\nPVector location;\nNetwork(float x, float y) {\nlocation = new PVector(x,y);\nneurons = new ArrayList<Neuron>();\n}\nWe can add an neuron to the network.\nvoid addNeuron(Neuron n) {\nneurons.add(n);\n}\nThe Nature of Code (v1.0)\n469\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 640
  },
  {
    "chunk_full": "Now we can pretty easily make the diagram above.\nThe above yields:\nWe can draw the entire network.\nvoid display() {\npushMatrix();\ntranslate(location.x, location.y);\nfor (Neuron n : neurons) {\nn.display();\n}\npopMatrix();\n}\n}\nNetwork network;\nvoid setup() {\nsize(640, 360);\nMake a Network.\nnetwork = new Network(width/2,height/2);\nMake the Neurons.\nNeuron a = new Neuron(-200,0);\nNeuron b = new Neuron(0,100);\nNeuron c = new Neuron(0,-100);\nNeuron d = new Neuron(200,0);\nAdd the Neurons to the network.\nnetwork.addNeuron(a);\nnetwork.addNeuron(b);\nnetwork.addNeuron(c);\nnetwork.addNeuron(d);\n}\nvoid draw() {\nbackground(255);\nShow the network.\nnetwork.display();\n}\nChapter 10. Neural Networks\n470\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 641
  },
  {
    "chunk_full": "What’s missing, of course, is the connection. We can consider a Connection object to be\nmade up of three elements, two neurons (from Neuron a to Neuron b) and a weight.\nOnce we have the idea of a Connection object, we can write a function (let’s put it inside the\nNetwork class) that connects two neurons together—the goal being that in addition to making\nthe neurons in setup(), we can also connect them.\nThe Network class therefore needs a new function called connect(), which makes a\nConnection object between the two specified neurons.\nclass Connection {\nA connection is between two neurons.\nNeuron a;\nNeuron b;\nA connection has a weight.\nfloat weight;\nConnection(Neuron from, Neuron to,float w) {\nweight = w;\na = from;\nb = to;\n}\nA connection is drawn as a line.\nvoid display() {\nstroke(0);\nstrokeWeight(weight*4);\nline(a.location.x, a.location.y, b.location.x, b.location.y);\n}\n}\nvoid setup() {\nsize(640, 360);\nnetwork = new Network(width/2,height/2);\nNeuron a = new Neuron(-200,0);\nNeuron b = new Neuron(0,100);\nNeuron c = new Neuron(0,-100);\nNeuron d = new Neuron(200,0);\nMaking connections between the neurons\nnetwork.connect(a,b);\nnetwork.connect(a,c);\nnetwork.connect(b,d);\nnetwork.connect(c,d);\nnetwork.addNeuron(a);\nnetwork.addNeuron(b);\nnetwork.addNeuron(c);\nnetwork.addNeuron(d);\n}\nThe Nature of Code (v1.0)\n471\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 642
  },
  {
    "chunk_full": "Presumably, we might think that the Network should store an ArrayList of connections,\njust like it stores an ArrayList of neurons. While useful, in this case such an ArrayList is\nnot necessary and is missing an important feature that we need. Ultimately we plan to “feed\nforward\" the neurons through the network, so the Neuron objects themselves must know to\nwhich neurons they are connected in the “forward” direction. In other words, each neuron\nshould have its own list of Connection objects. When a connects to b, we want a to store a\nreference of that connection so that it can pass its output to b when the time comes.\nIn some cases, we also might want Neuron b to know about this connection, but in this\nparticular example we are only going to pass information in one direction.\nFor this to work, we have to add an ArrayList of connections to the Neuron class. Then we\nimplement the addConnection() function that stores the connection in that ArrayList.\nThe neuron’s display() function can draw the connections as well. And finally, we have\nour network diagram.\nvoid connect(Neuron a, Neuron b) {\nConnection has a random weight.\nConnection c = new Connection(a, b,\nrandom(1));\n// But what do we do with the Connection object?\n}\nvoid connect(Neuron a, Neuron b) {\nConnection c = new Connection(a, b, random(1));\na.addConnection(c);\n}\nclass Neuron {\nPVector location;\nThe neuron stores its connections.\nArrayList<Connection> connections;\nNeuron(float x, float y) {\nlocation = new PVector(x, y);\nconnections = new ArrayList<Connection>();\n}\nAdding a connection to this neuron\nvoid addConnection(Connection c) {\nconnections.add(c);\n}\nChapter 10. Neural Networks\n472\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 643
  },
  {
    "chunk_full": "Example 10.3: Neural network diagram\nvoid display() {\nstroke(0);\nstrokeWeight(1);\nfill(0);\nellipse(location.x, location.y, 16, 16);\nDrawing all the connections\nfor (Connection c : connections) {\nc.display();\n}\n}\n}\n10.8 Animating Feed Forward\n10.8 Animating Feed Forward\nAn interesting problem to consider is how to visualize the flow of information as it travels\nthroughout a neural network. Our network is built on the feed forward model, meaning that an\ninput arrives at the first neuron (drawn on the lefthand side of the window) and the output of\nthat neuron flows across the connections to the right until it exits as output from the network\nitself.\nOur first step is to add a function to the network to receive this input, which we’ll make a\nrandom number between 0 and 1.\nvoid setup() {\nAll our old network set up code\nA new function to send in an input\nnetwork.feedforward(random(1));\n}\nThe Nature of Code (v1.0)\n473\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 644
  },
  {
    "chunk_full": "The network, which manages all the neurons, can choose to which neurons it should apply\nthat input. In this case, we’ll do something simple and just feed a single input into the first\nneuron in the ArrayList, which happens to be the left-most one.\nWhat did we do? Well, we made it necessary to add a function called feedforward() in the\nNeuron class that will receive the input and process it.\nIf you recall from working with our perceptron, the standard task that the processing unit\nperforms is to sum up all of its inputs. So if our Neuron class adds a variable called sum, it\ncan simply accumulate the inputs as they are received.\nThe neuron can then decide whether it should “fire,” or pass an output through any of its\nconnections to the next layer in the network. Here we can create a really simple activation\nfunction: if the sum is greater than 1, fire!\nclass Network {\nA new function to feed an input into the\nneuron\nvoid feedforward(float input) {\nNeuron start = neurons.get(0);\nstart.feedforward(input);\n}\nclass Neuron\nvoid feedforward(float input) {\nWhat do we do with the input?\n}\nclass Neuron\nint sum = 0;\nvoid feedforward(float input) {\nAccumulate the sums.\nsum += input;\n}\nvoid feedforward(float input) {\nsum += input;\nActivate the neuron and fire the outputs?\nif (sum > 1) {\nfire();\nIf we’ve fired off our output, we can reset\nour sum to 0.\nsum = 0;\n}\n}\nChapter 10. Neural Networks\n474\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 645
  },
  {
    "chunk_full": "Now, what do we do in the fire() function? If you recall, each neuron keeps track of its\nconnections to other neurons. So all we need to do is loop through those connections and\nfeedforward() the neuron’s output. For this simple example, we’ll just take the neuron’s sum\nvariable and make it the output.\nHere’s where things get a little tricky. After all, our job here is not to actually make a\nfunctioning neural network, but to animate a simulation of one. If the neural network were just\ncontinuing its work, it would instantly pass those inputs (multiplied by the connection’s weight)\nalong to the connected neurons. We’d say something like:\nBut this is not what we want. What we want to do is draw something that we can see traveling\nalong the connection from Neuron a to Neuron b.\nLet’s first think about how we might do that. We know the location of Neuron a; it’s the\nPVector a.location. Neuron b is located at b.location. We need to start something moving\nfrom Neuron a by creating another PVector that will store the path of our traveling data.\nOnce we have a copy of that location, we can use any of the motion algorithms that we’ve\nstudied throughout this book to move along this path. Here—let’s pick something very simple\nand just interpolate from a to b.\nAlong with the connection’s line, we can then draw a circle at that location:\nThis resembles the following:\nvoid fire() {\nfor (Connection c : connections) {\nThe Neuron sends the sum out through all\nof its connections\nc.feedforward(sum);\n}\n}\nclass Connection {\nvoid feedforward(float val) {\nb.feedforward(val*weight);\n}\nPVector sender = a.location.get();\nsender.x = lerp(sender.x, b.location.x, 0.1);\nsender.y = lerp(sender.y, b.location.y, 0.1);\nstroke(0);\nline(a.location.x, a.location.y, b.location.x, b.location.y);\nfill(0);\nellipse(sender.x, sender.y, 8, 8);\nThe Nature of Code (v1.0)\n475\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 646
  },
  {
    "chunk_full": "OK, so that’s how we might move something along the connection. But how do we know\nwhen to do so? We start this process the moment the Connection object receives the\n“feedforward” signal. We can keep track of this process by employing a simple boolean to\nknow whether the connection is sending or not. Before, we had:\nNow, instead of sending the value on straight away, we’ll trigger an animation:\nNotice how our Connection class now needs three new variables. We need a boolean\n“sending” that starts as false and that will track whether or not the connection is actively\nsending (i.e. animating). We need a PVector “sender” for the location where we’ll draw the\ntraveling dot. And since we aren’t passing the output along this instant, we’ll need to store it\nin a variable that will do the job later.\nThe feedforward() function is called the moment the connection becomes active. Once it’s\nactive, we’ll need to call another function continuously (each time through draw()), one that\nwill update the location of the traveling data.\nFigure 10.16\nvoid feedforward(float val) {\nb.feedforward(val*weight);\n}\nclass Connection {\nboolean sending = false;\nPVector sender;\nfloat output;\nvoid feedforward(float val) {\nSending is now true.\nsending = true;\nStart the animation at the location of\nNeuron A.\nsender = a.location.get();\nStore the output for when it is actually time\nto feed it forward.\noutput = val*weight;\n}\nvoid update() {\nif (sending) {\nAs long as we’re sending, interpolate our\npoints.\nsender.x = lerp(sender.x, b.location.x,\n0.1);\nsender.y = lerp(sender.y, b.location.y, 0.1);\n}\n}\nChapter 10. Neural Networks\n476\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 647
  },
  {
    "chunk_full": "We’re missing a key element, however. We need to check if the sender has arrived at location\nb, and if it has, feed forward that output to the next neuron.\nLet’s look at the Connection class all together, as well as our new draw() function.\nExample 10.4: Animating a neural network diagram\nvoid update() {\nif (sending) {\nsender.x = lerp(sender.x, b.location.x, 0.1);\nsender.y = lerp(sender.y, b.location.y, 0.1);\nHow far are we from neuron b?\nfloat d = PVector.dist(sender, b.location);\nIf we’re close enough (within one pixel) pass\non the output. Turn off sending.\nif (d < 1) {\nb.feedforward(output);\nsending = false;\n}\n}\n}\nvoid draw() {\nbackground(255);\nThe Network now has a new update()\nmethod that updates all of the Connection\nobjects.\nnetwork.update();\nnetwork.display();\nif (frameCount % 30 == 0) {\nWe are choosing to send in an input every\n30 frames.\nnetwork.feedforward(random(1));\n}\n}\nclass Connection {\nThe Nature of Code (v1.0)\n477\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 648
  },
  {
    "chunk_full": "The Connection’s data\nfloat weight;\nNeuron a;\nNeuron b;\nVariables to track the animation\nboolean sending = false;\nPVector sender;\nfloat output = 0;\nConnection(Neuron from, Neuron to, float w) {\nweight = w;\na = from;\nb = to;\n}\nThe Connection is active with data\ntraveling from a to b.\nvoid feedforward(float val) {\noutput = val*weight;\nsender = a.location.get();\nsending = true;\n}\nUpdate the animation if it is sending.\nvoid update() {\nif (sending) {\nsender.x = lerp(sender.x, b.location.x, 0.1);\nsender.y = lerp(sender.y, b.location.y, 0.1);\nfloat d = PVector.dist(sender, b.location);\nif (d < 1) {\nb.feedforward(output);\nsending = false;\n}\n}\n}\nDraw the connection as a line and traveling\ncircle.\nvoid display() {\nstroke(0);\nstrokeWeight(1+weight*4);\nline(a.location.x, a.location.y, b.location.x, b.location.y);\nif (sending) {\nfill(0);\nstrokeWeight(1);\nellipse(sender.x, sender.y, 16, 16);\n}\n}\n}\nChapter 10. Neural Networks\n478\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 649
  },
  {
    "chunk_full": "The network in the above example was manually configured by setting the location of\neach neuron and its connections with hard-coded values. Rewrite this example to\ngenerate the network’s layout via an algorithm. Can you make a circular network\ndiagram? A random one? An example of a multi-layered network is below.\nExercise 10.5\nExercise 10.5\nRewrite the example so that each neuron keeps track of its forward and backward\nconnections. Can you feed inputs through the network in any direction?\nExercise 10.6\nExercise 10.6\nInstead of lerp(), use moving bodies with steering forces to visualize the flow of\ninformation in the network.\nExercise 10.7\nExercise 10.7\nThe Nature of Code (v1.0)\n479\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 650
  },
  {
    "chunk_full": "The Ecosystem Project\nThe Ecosystem Project\nStep 10 Exercise:\nTry incorporating the concept of a “brain” into your creatures.\n•\nUse reinforcement learning in the creatures’ decision-making process.\n•\nCreate a creature that features a visualization of its brain as part of its\ndesign (even if the brain itself is not functional).\n•\nCan the ecosystem as a whole emulate the brain? Can elements of the\nenvironment be neurons and the creatures act as inputs and outputs?\nThe end\nThe end\nIf you’re still reading, thank you! You’ve reached the end of the book. But for as much\nmaterial as this book contains, we’ve barely scratched the surface of the world we inhabit\nand of techniques for simulating it. It’s my intention for this book to live as an ongoing\nproject, and I hope to continue adding new tutorials and examples to the book’s website\n(http://natureofcode.com) as well as expand and update the printed material. Your feedback\nis truly appreciated, so please get in touch via email at (daniel@shiffman.net) or by\ncontributing to the GitHub repository (http://github.com/shiffman/The-Nature-of-Code/), in\nkeeping with the open-source spirit of the project. Share your work. Keep in touch. Let’s be\ntwo with nature.\nChapter 10. Neural Networks\n480\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 651
  },
  {
    "chunk_full": "Further Reading\nFurther Reading\nBooks\nBooks\n•\nAlexander, R. McNeill. Principles of Animal Locomotion (http://t.co/IQ0iranE).\nPrinceton, NJ: Princeton University Press, 2002.\n•\nBentley, Peter. Evolutionary Design by Computers (http://t.co/XIp7b1zw). San\nFrancisco: Morgan Kaufmann Publishers, 1999.\n•\nBohnacker, Hartmut, Benedikt Gross, Julia Laub, and Claudius Lazzeroni. Generative\nDesign: Visualize, Program, and Create with Processing (http://t.co/8yekmakL). New\nYork: Princeton Architectural Press, 2012.\n•\nFlake, Gary William. The Computational Beauty of Nature: Computer Explorations of\nFractals, Chaos, Complex Systems, and Adaptation (http://t.co/KdbTo1ZX).\nCambridge, MA: MIT Press, 1998.\n•\nHale, Nathan Cabot. Abstraction in Art and Nature (http://t.co/ztbQ1zCL). New York:\nDover, 1993.\n•\nHildebrandt, Stefan, and Anthony J. Tromba. Mathematics and Optimal Form\n(http://t.co/IQ0iranE). New York: Scientific American Library, 1985. Distributed by\nW. H. Freeman.\n•\nKline, Morris. Mathematics and the Physical World (http://t.co/v84SZnGx). New York:\nCrowell, [1959].\n•\nKodicek, Danny. Mathematics and Physics for Programmers (http://t.co/ygDdHMak).\nHingham, MA: Charles River Media, 2005.\nThe Nature of Code (v1.0)\n481\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 652
  },
  {
    "chunk_full": "•\nMcMahon, Thomas A., and John Tyler Bonner. On Size and Life (http://t.co/\nEhX3KwZB). New York: Scientific American Library, 1983. Distributed by W. H.\nFreeman.\n•\nMandelbrot, Benoit B. The Fractal Geometry of Nature (http://t.co/jHRQ5sQC). San\nFrancisco: W. H. Freeman, 1982.\n•\nPearce, Peter. Structure in Nature Is a Strategy for Design (http://t.co/zaGQMOMc).\nCambridge, MA: MIT Press, 1980.\n•\nPearson, Matt. Generative Art (http://t.co/bXCWfgOC). Greenwich, CT: Manning\nPublications, 2011. Distributed by Pearson Education.\n•\nPrusinkiewicz, Przemysław, and Aristid Lindenmayer. The Algorithmic Beauty of\nPlants (http://t.co/koD7FhJQ). New York: Springer-Verlag, 1990.\n•\nReas, Casey, and Chandler McWilliams. Form+Code in Design, Art, and\nArchitecture (http://t.co/1jGgwhvU). Design Briefs. New York: Princeton\nArchitectural Press, 2010.\n•\nReas, Casey, and Ben Fry. Processing: A Programming Handbook for Visual\nDesigners and Artists (http://t.co/dtODdOQp). Cambridge, MA: MIT Press, 2007.\n•\nThompson, D’Arcy Wentworth. On Growth and Form: The Complete Revised\nEdition (http://t.co/vncWa1uW). New York: Dover, 1992.\n•\nVogel., Steven. Life in Moving Fluids (http://t.co/fyTbVta1). Princeton, NJ: Princeton\nUniversity Press, 1994.\n•\nWade, David. Li: Dynamic Form in Nature (http://t.co/1QYDlsDH). Wooden Books.\nNew York: Walker & Co., 2003.\n•\nWaterman, Talbot H. Animal Navigation (http://t.co/c2otv8LZ). New York: Scientific\nAmerican Library, 1989. Distributed by W. H. Freeman.\n•\nWhyte, Lancelot Law. Aspects of Form: A Symposium on Form in Nature and Art\n(http://t.co/f7UkVLQM). Midland Books, MB 31. Bloomington: Indiana University\nPress, 1966.\nFor other books that use Processing, see Processing Books (http://www.processing.org/\nlearning/books).\nPapers and Articles\nPapers and Articles\n•\nGalanter, Philip. \"The Problem with Evolutionary Art Is…\" (http://bit.ly/S7dhnq)\nPaper presented at EvoCOMNET’10: The 7th European Event on the Application of\nFurther Reading\n482\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 653
  },
  {
    "chunk_full": "Nature-inspired Techniques for Telecommunication Networks and other Parallel and\nDistributed Systems, April 7-9, 2010.\n•\nGardner, Martin. \"Mathematical Games: The Fantastic Combinations of John\nConway’s New Solitaire Game Life.\" (http://www.ibiblio.org/lifepatterns/\noctober1970.html) Scientific American 229 (October 1970): 120-23.\n•\nReeves, William T. \"Particle Systems—A Technique for Modeling a Class of Fuzzy\nObjects.\" (http://dl.acm.org/citation.cfm?id=357320) ACM Transactions on Graphics\n2:2 (April 1983): 91-108.\n•\nSims, Karl. \"Artificial Evolution for Computer Graphics.\" (http://www.karlsims.com/\npapers/siggraph91.html) Paper presented at SIGGRAPH '91: The 18th Annual\nConference on Computer Graphics and Interactive Techniques, Las Vegas, NV, July\n28-August 2, 1991.\n•\n---. \"Evolving Virtual Creatures.\" (http://www.karlsims.com/papers/siggraph94.pdf)\nPaper presented at SIGGRAPH '94: The 21st Annual Conference on Computer\nGraphics and Interactive Techniques, Orlando, FL, July 24-29, 1994.\n•\n---. \"Particle Animation and Rendering Using Data Parallel Computation.\"\n(http://www.karlsims.com/papers/ParticlesSiggraph90.pdf) Paper presented at\nSIGGRAPH '90: The 17th Annual Conference on Computer Graphics and Interactive\nTechniques, Dallas, TX, August 6-10, 1990.\nThe Nature of Code (v1.0)\n483\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 654
  },
  {
    "chunk_full": "Index\nIndex\nA\nacceleration\n49, 50, 67, 69, 104, 127, 131\nNewton's second law\n67\nalgorithms for\n50\nangular, determining\n127\ndamping\n131\nforce accumulation and\n69\nrotation\n104\nacceleration algorithms\n50, 53, 57\nconstant\n50\ninteractive\n57\nrandom\n53\naction selection\n262\nactivation functions of neural networks\n449\nAdaptation in Natural and Artificial Systems\n(Holland)\n392\nadd() function (PVector class)\n33, 34\nimplementation of\n34\nadd() function (Vec2 class)\n195\naddForce() function (toxiclibs)\n256\naddition operator\n33\nadditive blend mode\n185\naddLocal() function (Vec2)\n195\naddParticle() function (toxiclibs)\n246\nAI for Game Developers (Bourg/Seemann)\n445\nAlgorithmic Beauty of Plants, The (Prusinkiewicz/\nLindenmayer)\n389\nalignment (flocking)\n309, 311\nimplementing\n311\nalphabet (L-system component)\n383\namplitude\n117\nangleBetween() function (PVector class)\n37\nangles\n101, 102, 104, 112\nmeasuring in degrees\n101\nmeasuring in radians\n102\nmotion\n104\ntheta (θ)\n112\nAngry Birds\n190\nangular acceleration\n127\nangular velocity, oscillation with\n119\nanomaly detection\n447\nants, modeling for\n299\napplyForce() function (Box2D)\n232\nAristotle\n64\nArrayList class (Java)\n149, 150, 151, 153, 366\nIterator class and\n153\nfractals and\n366\ngenerics, using\n150\nresizability of\n151\narrays\n8, 15\nLévy flights, implementing with\n15\ncustom distributions, creating with\n8\nIndex\n484\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 655
  },
  {
    "chunk_full": "arrays (2D)\n277\narriving behavior\n270, 273\nsteering force and\n273\nartificial intelligence\n444, 445\npattern recognition\n445\nArtificial Intelligence: A Modern Approach (Russell/\nNorvig)\n445\natan() function (Processing)\n110\natan2() function (Processing)\n111\nAttractionBehavior class (toxiclibs)\n256\nautonomous agents\n260, 261, 262, 263, 270, 274,\n276, 282, 286, 298, 306, 308, 317\naction selection\n262\narriving behavior\n270\ncombinations\n306\ncomplex systems and\n298\ndesired velocity\n274\ndot product\n282\nefficiency\n317\nflocking\n308\nflow field following\n276\nkey components of\n261\nlocomotion\n263\npath following\n286\nsteering\n262\naxiom (L-system component)\n383\nB\nbackpropagation\n468\nbeginContact() function (PBox2D)\n235\nbeginShape() function (Processing)\n217\nbell curve\n11\nmean and\n11\nbias input, perceptron\n450\nBig O Notation\n315\nBig O Notation N-Squared\n315\nbin-lattice spatial subdivision\n316\nblend modes\n185, 186\nadditive\n185\nlist of\n186\nbody (Box2D element)\n194, 198, 199, 200, 202,\n218\nBodyDef type\n198\nattaching fixture element to\n202\nbuilding\n198\nbullet setting for\n200\ninitial settings for\n200\nmultiple shapes and\n218\nobject, creating\n200\ntypes of\n199\nbody lists, maintaining in Processing\n203\nBodyDef type (body element)\n198, 199, 209\nSTATIC type\n209\nconfiguring\n199\nboids model\n261\nbouncing ball sketch\n28, 35\nimplementing with vectors\n35\nboundaries\n209, 211\ncurvy\n211\nfixed\n209\nBourg, David M.\n445\nBox2D\n190, 192, 194, 196, 209, 215, 216, 241\nFisica\n192\nJBox2D and\n192\nPBox2D helper class\n192\nPVector vs.\n194\nProcessing and\n192\ncomplex forms in\n215\nconcave shapes and\n216\ncoordinate system vs. Processing\n196\ncore elements\n194\nfixed objects in\n209\norder of vertices\n216\noverhead with\n190\ntoxiclibs vs.\n241\nusage\n192\nBraitenberg, Valentino\n262\nbrute force method\n392\nC\nCantor set\n358, 363, 386\nThe Nature of Code (v1.0)\n485\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 656
  },
  {
    "chunk_full": "L-systems and\n386\nrecursion and\n363\nCantor, George\n358\nCartesian coordinates\n112, 113\npolar coordinates, converting from\n113\nCatto, Erin\n190\ncellular automaton (automata)\n324, 325, 326, 328,\n330, 334, 340, 342, 351, 352, 353\nSierpiński triangle\n328\nWolfram algorithm for\n325\nWolfram classification\n340\ncharacteristics of\n324\ncontinuous\n352\ndefined\n324\nelementary\n325\nelementary, implementing\n330\nhistorical\n353\nimage processing\n352\nmoving cells\n353\nnesting\n353\nnon-rectangular grids and\n351\nprobabilistic\n352\nrulesets, defining\n334\nself-replicating cells\n324\ntime and\n326\ntwo-dimensional\n342\nvariations of\n351\nChainShape class\n211\nconfiguring\n211\nclass (Processing)\n2, 3, 47\nconstructor\n3 , 47\ndefined\n2\nfunctionality\n3\ncode duplication, inheritance and\n161\ncoefficient of friction\n80\nmu (μ)\n80\ncohesion (flocking)\n309, 312\nimplementing\n312\ncollisions\n190, 234, 235, 241, 301\nBox2D and\n234\navoiding in group behavior\n301\nbeginContact() function (PBox2D)\n235\ntoxiclibs and\n241\ncombinations\n306\ncomplex systems\n298, 299, 300, 323, 342, 446\nGame of Life as\n342\ncellular automata\n323\ncompetition/cooperation component\n300\nconnectionist computational system\n446\nfeedback component\n300\ngroup behavior\n300\nkey principles of\n299\nnon-linearity component\n299\nsuperorganisms\n299\nComplexity class (Wolfram classification)\n341\nComputational Beauty of Nature (Flake)\n314\nconnected systems\n249, 253\nforce-directed graphs\n253\nstrings\n249\nconnectionist computational system\n446\nconstrain() function (Processing)\n280\nconstructor\n3, 47, 75\narguments, adding to\n75\nContact objects (PBox2D)\n236\nContactListener class (JBox2D)\n234\ncontinuous (cellular automata)\n352\ncontract() function (Processing)\n149\ncontrol (of physical objects)\n447\nConway's Game of Life (Klise)\n345\nConway, John\n342\ncoordinate systems\n112, 196\nBox2D vs. Processing\n196\nCartesian\n112\nProcessing and\n112\npolar\n112\ncoordPixelsToWorld() function (PBox2D)\n197\ncoordWorldToPixels() function (PBox2D)\n198\nCornell Aeronautical Laboratory\n448\ncos() function (Processing)\n113\ncosine lookup tables\n318\nCrayon Physics\n190\ncreateBody() function (PBox2D)\n200\ncreateFixture() function (PBox2D)\n202\ncross() function (PVector class)\n37\nIndex\n486\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 657
  },
  {
    "chunk_full": "crossover (natural selection algorithms)\n400, 408\nimplementing\n408\nD\ndamping\n131\ndampingRatio setting (Box2D joint element)\n223\nDarwinian natural selection\n394\ndegrees\n101, 103\nradians, converting to\n103\ndelta weight\n454\ndensity\n67\nderivatives\n238\nDescartes, René\n112\ndesired velocity\n264, 274\ndestroyBody() function (PBox2D)\n208\ndifferentiation\n238\ndissipative force\n80\ndist() function (PVector class)\n37\ndistance joints\n222\nDistanceJointDef (Box2D joint type)\n223\ndistributions, custom\n14\ndistributions, non-uniform\n7, 8, 14, 15, 16, 17\nMonte Carlo method\n16\nPerlin noise\n17\ncreating with arrays\n8\ncustom\n14\nprobability and\n7\nqualifying random values\n15\ndistributions, normal\n11\ndiv() function (PVector class)\n41\ndot product (PVector)\n282, 283, 284\ndefined\n283\ntheta\n284\ndot syntax\n35\ndot() function (PVector class)\n37\ndrag force\n83\ndynamic (body type)\n199\nE\necosystem simulation genetic algorithms\n392, 435,\n437, 438, 439\ngenotype\n438\nlifespans, varying\n437\nphenotype\n438\nreproduction\n439\nselection\n439\nefficiency\n315, 316, 317, 318, 319\nBig O Notation\n315\nBig O Notation N-Squared\n315\nbin-lattice spatial subdivision\n316\nmagSq() function (PVector class).\n318\nsine/cosine lookup tables\n318\ntemporary objects and\n319\nelementary cellular automata\n325, 330, 332, 333,\n336\ndrawing\n336\nedge cases and\n332\ngenerations, maintaining integrity of\n333\nimplementing\n330\nemitter\n146\nendContact() function (PBox2D)\n235\nendShape() function (Processing)\n217\nequilibrium\n64\nEuclid\n27, 355\nEuclidean geometry\n355, 358\nfractals and\n358\nEuclidean vector\n27\nEuler integration\n239, 240\nsymplectic Euler (Box2D)\n240\nEuler, Leonhard\n239\nevolution\n390, 391, 394\nDarwinian natural selection\n394\ngenetic algorithms\n391\nmodeling\n390\nevolutionary computing\n392\nEvolved Virtual Creatures (Sims)\n430\nexclusive or (XOR)\n466\nexit conditions for recursion\n361\nexpand() function (Processing)\n149\nExploring Emergence (Resnick/Silverman)\n345\nextends keyword (Processing)\n164\nThe Nature of Code (v1.0)\n487\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 658
  },
  {
    "chunk_full": "F\nfactorial\n359\nfeed-forward model (neural networks)\n448, 473\nanimating\n473\nFisica\n192\nfitness functions (natural selection algorithms)\n397,\n414, 416, 417, 427, 429, 436\navoidance of obstacles and\n427\ndesign your own\n416\necosystem simulations and\n436\nevolving for specific attributes\n429\nexponential vs. linear\n414\nrobotic enslavement of humanity and\n417\nfixture (Box2D element)\n194, 201, 202\nattaching to body element\n202\ncreating\n201\nFlake, Gary\n314\nflocking\n308, 309, 315, 316\nbin-lattice spatial subdivision\n316\nperformance and\n315\nrules of\n309\nFlocks, Herds, and Schools: A Distributed Behavioral\nModel (Reynolds)\n308\nflow field following\n276, 277\nresolution and\n277\nfluid resistance, modeling\n83\nfor loops\n150\nArrayList objects and\n150\nenhanced\n150\nforce accumulation\n69\nforce-directed graphs\n253\nforces\n63, 64, 68, 71, 73, 77, 78, 80, 83, 89,\n127, 131, 134, 173, 178, 232, 260, 263\nHooke's law\n134\nNewton's laws of motion\n63\naccumulation of\n68\napplyForce() function\n232\napplying to objects\n71\napplying to single objects in a system\n178\nautonomous agents and\n260\ncreating\n73\ndamping\n131\ndefined\n63\nequilibrium\n64\nfluid resistance\n83\nfriction, modeling\n80\ngravity, modeling\n77\nmodels of, building\n78\nparticle systems with\n173\nsprings\n134\nsteering\n263\nterminal velocity\n64\ntrigonometry and\n127\nuniversal gravitational constant\n89\nforces, modeling\n77\nreal forces\n77\nformulae, evaluating in code\n79\nFractal Geometry of Nature, The (Mandelbrot)\n356\nfractals\n355, 356, 357, 358, 366, 374, 375, 382\nKoch curve\n366\nL-systems and\n382\ndefined\n356\nfine structure of\n358\nrecursion\n358\nself-replicating behavior of\n357\nstochastic\n358\ntransformation matrix (Processing)\n375\ntrees and\n374\nfrequency (of oscillations)\n119\nfrequencyHz setting (Box2D joint element)\n223\nfriction\n79, 80, 81, 83, 84, 131\napplying to an object\n84\ncoefficient of friction\n80\ndamping\n131\ndetermining direction/magnitude of\n80\nformula for\n79\nmodeling with formulae\n80\nmu (μ)\n80\nnormal force\n81\nrho (ρ)\n83\nfunctionality\n3\nfunctions\n54\nIndex\n488\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 659
  },
  {
    "chunk_full": "static vs. non-static\n54\nG\nGalileo\n77\nGame of Life\n324, 342, 343, 344\ndrawing\n344\nrules of\n343\nGardner, Martin\n342\nGauss, Carl Friedrich\n11\nGaussian distribution\n11\ngenetic algorithms\n391, 392, 394, 395, 397, 409,\n413, 414, 420, 435\nDarwinian natural selection\n394\nSmart Rockets (Thorp)\n420\nbuilding\n409\ndefined\n391\necosystem simulation\n392 , 435\nfitness algorithms, modifying\n414\ninteractive selection\n391\nmodifying\n413\nmutation rate, varying\n413\npopulation maximum, varying\n413\npopulations, creating\n395\npurpose of\n392\nselection, implementing\n397\ntraditional\n391\ngenotype (natural selection algorithms)\n395, 417,\n438\necosystem simulation\n438\nmodifying\n417\ngeometric vector\n27\ngetAngle() function (PBox2D)\n207\ngetBodyList() function (World class)\n203\ngetBodyPixelCoord() function (PBox2D)\n207\ngetGroundBody() function (Box2D joint element)\n229\ngravity\n88, 89, 94, 128, 244\nGravityBehavior (toxiclibs)\n244\nimplementing model of\n89\nmodeling\n88\nmodeling reality vs. arbitrary values\n128\nmodeling with trigonometry\n128\nplacing limits on model of\n94\nuniversal gravitational constant\n89\nGravityBehavior class (toxiclibs)\n244\ngrid (cellular automata)\n324\ngroup behavior\n300, 301, 306, 308\ncollisions, avoiding\n301\ncombinations\n306\nflocking\n308\nH\nheading() function (PVector class)\n37, 112\nheredity (natural selection)\n394, 399, 400, 401\ncrossover\n400\nimplementing\n399\nmutation\n401\nhistorical (cellular automata)\n353\nHodgin, Robert\n185\nHolland, John\n392\nHooke's law\n134, 135\nformula for expressing\n135\nHooke, Robert\n134\nI\nimage processing (cellular automata)\n352\nimage textures\n183, 184, 185\nPImage objects (Processing)\n184\nPNG format and\n184\nblend modes\n185\ninfinite monkey theorem\n392\ninheritance\n144, 160, 163, 164, 165, 166\nadding functionality to superclass objects\n165\nextends keyword (Processing)\n164\nimplementing\n166\noverriding superclass functions\n165\nsubclass\n163\nsuper() function (Processing)\n164\nsuperclasses\n163\nsyntax for\n163\nintegration\n238, 239, 240\nThe Nature of Code (v1.0)\n489\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 660
  },
  {
    "chunk_full": "Euler integration\n239\nRunge-Kutta method\n240\nInteraction with Groups of Autonomous Characters\n(Reynolds)\n316\ninteractive selection genetic algorithms\n391, 431,\n433, 434\ntime lag and\n434\nuser interaction and\n433\ninterfaces\n234\niterating\n152, 153\nIterator class (Java)\n153\nremoving elements in for loops\n152\nIterator class (Java)\n153\niTunes visualizer\n185\nJ\nJava\n192\nJBox2D\n192, 195, 234\nContactListener class\n234\nfull documentation for\n195\njoint (Box2D element)\n194, 222, 225, 228\ndistance\n222\nmouse type\n228\nrevolute type\n225\nK\nkinematic (body type)\n199, 231\nMouseJoints and\n231\nKlise, Steven\n345\nKoch curve\n366, 369\nimplementing\n369\nKutta, M. W.\n240\nL\nL-systems\n382, 383, 386\ncomponents of\n383\ntranslating into code\n386\nLaplace, Pierre-Simon\n11\nLaplacian distribution\n11\nlearning constant\n455\nLearning Processing (Shiffman)\n160\nlerp() function (PVector class)\n37\nlimit() function (PVector class)\n37, 51\nLindenmayer systems\n382\nLindenmayer, Aristid\n382, 389\nlinearly separable problems\n466\nlocations\n31\nas vectors\n31\nlock() function (toxiclibs)\n247\nlocomotion\n263\nLogical calculus of the ideas imminent in nervous\nactivity, A (McCulloch/Pitts)\n445\nLos Alamos National Laboratory\n324\nLucasfilm Ltd.\n143\nLévy flight\n14, 15\nimplementing with arrays\n15\nimplementing with qualifying random values\n15\nM\nm_p variable (Vec2 class)\n220\nmag() function (PVector class)\n43, 318\nmagSq() function vs.\n318\nMagnetosphere\n185\nmagnitude (of vectors)\n42, 51\nlimiting\n51\nmagSq() function (PVector class).\n318\nMandelbrot, Benoit\n356\nmap() function (Processing)\n20, 117\noscillation and\n117\nMarxer, Ricard\n192\nmass\n67, 70\nmodeling\n70\nunits of measurement, defining\n70\nweight vs.\n67\nmating pools (natural selection)\n397, 405\ncreating\n397\nIndex\n490\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 661
  },
  {
    "chunk_full": "implementing\n405\nMcCulloch, Warren S.\n445\nmean\n11\nmethods, static vs. non-static\n54\nmillis() function (Processing)\n117\nMonster curve\n367\nMonte Carlo method\n16\nmotion\n45, 104, 112\nangular\n104\nheading() function (PVector class)\n112\nmouse joint (Box2D joint type)\n228\nsetTransform() function\n228\nmouse joint (Box2D Joint type)\n228\nmoving cells (cellular automata)\n353\nmu (μ)\n11, 80\nmult() function (PVector class)\n40\nimplementation\n40\nmutation (natural selection algorithms)\n401, 409\nimplementing\n409\nrate of\n401\nN\nnatural fractals\n374\nnatural phenomena\n2, 7, 17, 67, 70, 73, 77, 78,\n80, 83, 88, 89, 127, 128, 184, 260, 299, 300,\n308, 324, 355, 374, 382, 383, 390, 391, 394,\n435\nDarwinian natural selection\n394\nL-systems and\n382\nNewton's second law, modeling\n67\nPerlin noise and\n17\nants, modeling\n299\nautonomous agents\n260\ncellular automata\n324\necosystems, modeling\n435\nevolution\n390\nflocking\n308\nfluid resistance, modeling\n83\nforces, modeling\n73 , 77\nfractals\n355\nfriction\n80\ngenetic algorithms\n391\ngravity\n77 , 88 , 89\ngroup behavior\n300\nmass, modeling\n70\nmodeling reality vs. arbitrary values\n128\nmodeling with random walks\n2\nmodeling with the random() function\n7\nphysics (real world), modeling\n78\npivots, modeling\n127\nplant growth, modeling\n383\nsmoke, modeling with particle systems\n184\ntrees and\n374\nnatural selection algorithms\n394, 395, 397, 398,\n399\nfitness functions\n397\nmating pools, creating\n397\npopulations, creating\n395\nprobability\n398\nreproduction\n399\nnaturally ordered sequence of numbers\n17\nneighborhood (cellular automata)\n325\nnesting (cellular automata)\n353\nneural networks\n444, 445, 446, 447, 448, 449,\n467, 468, 473, 475\nactivation functions of\n449\nanimating\n473\nbackpropagation\n468\nconnectionist computational system\n446\ndiagramming\n468\nlearning and\n446\nnetworks of perceptrons\n467\npattern recognition\n445\nperceptron\n448\nreal vs. simulated\n475\nreinforcement learning\n447\nsupervised learning\n446\nunsupervised learning\n447\nuses of\n447\nNew Kind of Science, A (Wolfram)\n325\nnew operator (objects)\n4\nNewton's first law\n64, 65\nPVector class and\n65\nNewton's second law\n67\nThe Nature of Code (v1.0)\n491\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 662
  },
  {
    "chunk_full": "Newton's third law\n65, 66\nPVector class and\n66\nNewton, Isaac\n63\nnextGaussian() function (Random class)\n13\ndefault mean/standard deviation settings of\n13\nnoise() function (Processing)\n18\narguments for\n18\nnoiseDetail() function (Processing)\n18\nnon-linearly separable problems\n466\nnon-rectangular grids (cellular automata)\n351\nnon-uniform distributions\n7, 8, 14, 15, 16, 17\nMonte Carlo method\n16\nPerlin noise\n17\ncreating with arrays\n8\ncustom\n14\nprobability and\n7\nqualifying random values\n15\nnormal distribution\n11\nnormal force\n81\nnormal points\n291, 295\nseries of, for path following\n295\nnormalization\n43, 398\nmating pools, creating with\n398\nnormalize() function (PVector class)\n44\nNorvig, Peter\n445\nO\nobject\n2, 4, 92, 349\ncells in cellular automata as\n349\ndefined\n2\ninteraction between\n92\nnew operator\n4\nobject-oriented programming\n2, 35, 72, 137, 144,\n155, 160, 168, 176, 349, 419\ncellular automata and\n349\nclass\n2\nclasses of user-defined objects, creating\n155\ndot syntax\n35\ngenotype/phenotype objects and\n419\ninheritance\n144 , 160\ninstances of subclasses, creating\n168\nmaintaining encapsulation\n176\nobject\n2\npolymorphism\n144 , 160 , 168\nreferences to vs. copies of objects\n72\nreview of\n2\nstructures, choosing between\n137\noptimization\n318, 319\nmagSq() function (PVector class).\n318\nsine/cosine lookup tables\n318\ntemporary objects and\n319\noscillation\n116, 117, 119, 120, 122, 124\namplitude\n117\nangular velocity and\n119\nfrequency of\n119\non two axes\n120\nperiod\n117\nsimple harmonic motion\n117\nsimulating with sine curves\n116\nvarying\n124\nwaves\n122\noversampling\n14\nP\nparticle systems\n143, 144, 145, 146, 149, 155,\n156, 157, 170, 173, 178, 184, 240, 246\nArrayList, using\n149\nVerlet integration and\n240\naddParticle() function (toxiclibs)\n246\napplying force to single particles in\n178\nclass for, creating\n155\ndead particles, checking for\n146\nemitter\n146\nforces and\n173\nlifespan of particles\n146\nmultiple systems, organizing\n157\norigin point (of particles)\n156\nparticles in\n145\npolymorphism, using\n170\npurpose of\n144\nsmoke, modeling\n184\nIndex\n492\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 663
  },
  {
    "chunk_full": "particles\n145, 146, 147, 178, 244, 245\nVerletParticle2D object (toxiclibs)\n245\napplying force to single particles in\n178\ndeath, checking for\n146\nlifespan of\n146\ntesting\n147\ntoxiclibs implementation of\n244\npath\n286\npath following\n286, 288, 291, 292, 294\ncurrent distance from path, finding\n288\nmultiple segments\n294\nnormal points\n291\npathfinding vs.\n286\ntarget, determining\n292\npathfinding\n286\npattern recognition\n445, 450\nperceptron and\n450\nPBox2D helper class\n192, 196, 197, 207\ncoordinate systems, converting between\n197\ncreateWorld() function\n196\ngetBodyPixelCoord() function (PBox2D)\n207\nperceptron\n448, 450, 455, 456, 460, 466, 467\nbias input\n450\nerror calculations and\n450\nimplementing\n448\nlearning constant\n455\nlinearly separable problems and\n466\nnetworks of\n467\npattern recognition with\n450\nsteering and\n460\ntraining\n456\nperformance\n315, 316, 317, 318, 319\nBig O Notation\n315\nBig O Notation N-Squared\n315\nbin-lattice spatial subdivision\n316\nmagSq() function (PVector class).\n318\nsine/cosine lookup tables\n318\ntemporary objects and\n319\nperiod\n117, 122\ndefined in pixels rather than frames\n122\nPerlin noise\n17, 18, 20, 22, 279\nflow field following and\n279\nmap() function\n20\nnatural phenomena, modeling with\n17\nnoise() function (Processing)\n18\ntwo-dimensional\n22\nphenotype (natural selection algorithms)\n395, 417,\n438\necosystem simulation\n438\nphysics\n78, 189, 190\ncollisions\n190\nmodeling\n78\nopen-source libraries for\n189\nphysics libraries\n189, 190\nBox2D\n190\npi (π)\n103\nPI variable (Processing)\n103\nPImage objects (Processing)\n184\nPitts, Walter\n445\npivots, modeling\n127\nplant growth, modeling\n383\nPNG graphic file format\n184\npolar coordinates\n112, 113\nCartesian coordinates, converting to\n113\nPolygonShape class\n215\nas list of vectors\n215\npolymorphism\n144, 160, 168, 170\ncreating object instances with\n170\npopMatrix() function (Processing)\n375\npopulations (genetic algorithms)\n395, 402, 435\ncreating\n395\necosystem simulations and\n435\nelements of\n395\nimplementing\n402\npostSolve() function (PBox2D)\n235\npreSolve() function (PBox2D)\n235\nprobabilistic (cellular automata)\n352\nprobability\n7, 8, 11, 352, 392, 398\ncellular automata based on\n352\ninfinite monkey theorem\n392\nmean\n11\nnatural selection algorithms and\n398\nnon-uniform distributions and\n7\nThe Nature of Code (v1.0)\n493\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 664
  },
  {
    "chunk_full": "normal distributions\n11\nstandard deviation\n11\nprobability of the fittest\n7\nProcessing\n2, 12, 18, 30, 46, 54, 102, 103, 110,\n111, 112, 117, 163, 183, 184, 192, 196, 203, 205,\n241\nBox2D and\n192\nBox2D objects, adding to projects\n205\nJBox2D\n192\nOOP online tutorial\n46\nPImage objects\n184\nRandom class\n12\nangles, measuring in\n102\natan() function\n110\natan2() function\n111\nbody lists, maintaining\n203\nclass inheritance, syntax for\n163\ncoordinate systems and\n112\ncoordinate systems vs. Box2D\n196\nincorporating images into projects\n183\nmeasuring time in\n117\nnoise() function\n18\nnoiseDetail() function\n18\nradians() function\n103\nreview of object-oriented programming with\n2\nrotation tutorial\n103\nstatic vs. non-static methods\n54\ntoxiclibs and\n241\nvectors and\n30\nPrusinkiewicz, Przemysław\n389\npseudo-random numbers\n7, 17\nPerlin noise and\n17\npushMatrix() function (Processing)\n375\nPVector class (Processing)\n30, 37, 38, 40, 41, 43,\n44, 51, 65, 66, 112, 194\nBox2D vs.\n194\nNewton's first law and\n65\nNewton's third law and\n66\ndiv() function\n41\nfunction list for\n37\nheading() function\n112\nlimit() function\n51\nmag() function\n43\nmathematical functions for\n37\nmult() function\n40\nnormalize() function\n44\nsub() function\n38\nPythagoras\n42\nPythagorean theorem\n42\nQ\nqualifying random values\n15, 16\nMonte Carlo method\n16\nR\nradians\n102, 103\nconverting from degrees\n103\nradians() function (Processing)\n103\nRandom class (Processing)\n12, 13\nnextGaussian() function\n13\nRandom class (Wolfram classification)\n341\nrandom number generators\n3, 6, 7, 14\ncustom distributions, creating\n14\nnon-uniform distributions, creating\n7\npseudo-random numbers\n7\nrandom() function\n3\nuniform number distributions and\n6\nrandom walks\n1, 14\nGaussian\n14\nLévy flight\n14\noversampling\n14\nrandom() function\n3, 7, 8\nnatural phenomena, modeling with\n7\nnon-uniform distributions, creating with\n8\nrandom2D() function (PVector class)\n37\nrandom3D() function (PVector class)\n37\nreal forces\n77\nrecursion\n358, 359, 361, 366\nArrayList objects and\n366\nexit conditions\n361\nfactorial\n359\nIndex\n494\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 665
  },
  {
    "chunk_full": "implementing\n359\nReeves, William T.\n143\nreinforcement learning (neural networks)\n447\nreinforcement learning(neural networks)\n463\nremove() function (ArrayList class)\n151\nRepetition class (Wolfram classification)\n340\nreproduction (natural selection algorithms)\n399, 407,\n439\necosystem simulation\n439\nimplementing\n407\nrepulsion\n302\ngroup behavior and\n302\nResnick, Mitchel\n262, 345\nresolution, flow field following and\n277\nrest length (Box2D joint element)\n223\nrevolute joint type (Box2D)\n225, 226\nproperties, configuring\n226\nRevoluteJointDef object (Box2D joint element)\n226\nReynolds, Craig\n261, 286\npath following algorithm\n286\nrho (ρ)\n83\nRosenblatt, Frank\n448\nrotate() function (PBox2D)\n207\nrotate() function (PVector class)\n37\nrotation\n104, 109\npointing towards movement\n109\nroulette wheel probability method\n398\nrules (L-system component)\n383\nrulesets for cellular automata\n334\nRunge, C.\n240\nRunge-Kutta method\n240\nRussell, Stuart J.\n445\nS\nscalar notation, vs. vector notation\n33\nscalar projection\n291\nscalarPixelsToWorld() function (PBox2D)\n198\nscalarWorldToPixels() function (PBox2D)\n198\nSchmidt, Karsten\n241\nSeemann, Glenn\n445\nselection (natural selection algorithms)\n394, 397,\n404, 439\necosystem simulation\n439\nimplementing\n397 , 404\nself-replicating cells\n324\nself-similarity of fractals\n357\nseparation (flocking)\n309, 310\nimplementing\n310\nsetGravity() function (World class)\n196\nsetTransform() function (Box2D)\n228\nShape (Box2D element)\n200, 201, 220\ndefining\n201\nfriction attribute\n200\nlocal position for\n220\nrestitution attribute\n200\nshape (Box2D element)\n194\nshapes\n104, 112, 113\ndisplaying\n112\nmoving with polar coordinates\n113\nrotating\n104\nshort range relationships\n299, 310\ncomplex systems\n299\nflocking behavior and\n310\nSierpiński triangle\n328\nSierpiński, Wacław\n328\nsigma (σ)\n11\nsignal processing\n447\nSilverman, Brian\n345\nsimple harmonic motion\n117\nSims, Karl\n430\nsin() function (Processing)\n113\nsine lookup tables\n318\nsize() function (ArrayList class)\n152\nSmart Rockets (Thorp)\n420\nsoft sensors\n447\nsohcahtoa\n108\nsplice() function (Processing)\n149\nsprings\n134, 135, 136, 246, 247\nHooke's law\n134\nVerletConstrainedSpring class (toxiclibs)\n246\nVerletMinDistanceSpring class (toxiclibs)\n246\nVerletSpring class (toxiclibs)\n246\nThe Nature of Code (v1.0)\n495\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 666
  },
  {
    "chunk_full": "direction of force, determining\n136\nlock() function (toxiclibs)\n247\nmagnitude of force, determining\n135\nrest length\n136\ntoxiclibs and\n246\nstandard deviation\n11, 12\ncalculating\n12\nvariance\n12\nStar Trek II: The Wrath of Khan (1982)\n143\nstate (cellular automata)\n324\nstatic (body type)\n199, 209\nstatic functions\n55\nsteering behaviors\n274, 276, 302, 460\nflow field following\n276\ngroup behavior and\n302\nperceptron for\n460\nwandering\n274\nSteering Behaviors for Autonomous Characters\n(Reynolds)\n262\nsteering force\n262, 264, 266, 273\narriving behavior and\n273\ndesired velocity\n264\nmagnitude of\n266\nsteering perceptron\n460, 463\nreinforcement learning(neural networks)\n463\nstep() function (Box2D)\n205\nstochastic fractals\n358, 374\ntrees as\n374\nStringBuffer class\n385\nstrings\n251, 385\nStringBuffer class vs.\n385\nhanging from fixed points\n251\nsub() function (PVector class)\n38\nsubclass\n163, 165\nadding functionality to superclass objects\n165\nsubset() function (Processing)\n149\nsuper() function(Processing)\n164\nsuperclasses\n163, 165, 169, 170\nadding functionality within subclasses\n165\noverriding functions from\n165 , 170\npolymorphism and\n169\nsuperorganisms\n299\nsupervised learning (neural networks)\n446\nsymplectic Euler (Box2D)\n240\nT\ntangent\n110, 111, 112\natan() function (arctangent)\n110\natan2() function\n111\nheading() function (PVector class)\n112\nterminal velocity\n64\ntheta (θ)\n112, 284\ndot product and\n284\nThorp, Jer\n420\ntime\n117, 326\ncellular automata and\n326\nmillis() function, measuring with\n117\ntime series prediction\n447\ntoxiclibs\n241, 242, 244, 246, 249, 253, 256\nAttractionBehavior class\n256\nBox2D vs.\n241\nVerletPhysics class\n244\nVerletPhysics2D class\n244\nattraction/repulsion behaviors and\n256\nconnected systems\n249\ndownloading\n242\nforce-directed graphs\n253\nparticles, implementing in\n244\nsprings\n246\nworld, building\n244\ntraditional genetic algorithms\n391\ntransformation matrix (Processing)\n375\nTransformations tutorial (Processing)\n375\ntranslate() function (PBox2D)\n207\ntrees\n374\ntrigonometry\n108, 110, 113, 127\natan() function\n110\ncos() function (Processing)\n113\nforces and\n127\nsin() function (Processing)\n113\nsohcahtoa\n108\ntangent\n110\nIndex\n496\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 667
  },
  {
    "chunk_full": "Tron (1982)\n17\nTurtle graphics\n386\nTurtles, Termites, and Traffic Jams (Resnick)\n262\ntwo-dimensional cellular automata\n342, 345\nimplementing\n345\nU\nUlam, Stanisław\n324\nuniform number distributions\n6\nUniformity class (Wolfram classification)\n340\nunit vectors\n43\nuniversal gravitational constant\n89\nunsupervised learning (neural networks)\n447\nupdate() function (toxiclibs)\n244\nV\nvariance\n12\nvariation (natural selection)\n394\nVec2 (Box2D element)\n194, 195\nadding vectors with\n195\nmanitude, finding\n195\nmultiplying vectors with\n195\nnormalizing vectors\n195\nscaling vectors with\n195\nVec2D (toxiclibs type)\n243, 246\nVerletParticle2D class and\n246\nmath functions for\n243\nVec3D (toxiclibs type)\n243\nvector notation, vs. scalar notation\n33\nvectors\n27, 28, 30, 31, 33, 39, 40, 41, 42, 43,\n45, 49, 109, 110, 194, 278, 282\nProcessing and\n30\nVec2 (Box2D element)\n194\nacceleration\n49\nadding\n33\nas right triangles\n109\nassociative/distributive rules for multiplication/\ndivision of\n41\nbouncing ball sketch\n28\ncommutative/associative rules of addition/\nsubtraction with\n39\ndefined\n27\ndot product\n282\nflow fields, computing for\n278\nlocations and\n31\nmagnitude\n42\nmotion, implementing with\n33\nmultiplying\n40\nnormalization\n43\nnotation\n27\nscaling\n40\ntangent\n110\nunit vectors\n43\nvelocity and\n31 , 45\nVehicles: Experiments in Synthetic Psychology\n(Braitenberg)\n262\nvelocity\n31, 45, 49, 51, 274\nacceleration\n49\nas vector\n31\ndesired, for autonomous agents\n274\nlimiting\n51\nVerlet integration\n240, 241\ntoxiclibs\n241\nVerletConstrainedSpring class (toxiclibs)\n246\nVerletMinDistanceSpring class (toxiclibs)\n246\nVerletParticle2D object (toxiclibs)\n245\nVerletPhysics class (toxiclibs)\n242, 244\ncore elements of\n242\nVerletPhysics2D class (toxiclibs)\n244\nVerletSpring class (toxiclibs)\n246\nviscous force\n83\nvon Neumann, John\n324\nW\nwandering behavior (Reynolds)\n274\nwaves\n122, 124\nangular velocity, defining with\n122\nvarying\n124\nweight\n67, 446\nThe Nature of Code (v1.0)\n497\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 668
  },
  {
    "chunk_full": "mass vs.\n67\nneural networks and\n446\nwheel of fortune probability method\n398\nWolfram classification\n340, 341\nComplexity class\n341\nRandom class\n341\nRepetition class\n340\nUniformity class\n340\nWolfram, Stephen\n325, 340\nWolfram classification\n340\nelementary cellular automata algorithm\n325\nWorld class (Box2D)\n194, 196, 203\ncreateWorld() function (PBox2D)\n196\ngetBodyList() function\n203\nX\nXOR (exclusive or)\n466\nIndex\n498\n",
    "book_id": "the_nature_of_code",
    "book_title": "The Nature of Code",
    "book_author": "Unknown",
    "topic_id": "art_direction",
    "topic_label": "art direction",
    "chunk_index": 669
  }
]