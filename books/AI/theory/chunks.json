[
  {
    "chunk_full": "# Contents\n\n  1. [Cover](html/Cover.xhtml)\n  2. [Front Matter](html/470825_1_En_BookFrontmatter_OnlinePDF.xhtml)\n  3. [1\\. Introduction to AI and UX](html/470825_1_En_1_Chapter.xhtml)\n  4. [2\\. AI and UX: Parallel Journeys](html/470825_1_En_2_Chapter.xhtml)\n  5. [3\\. AI-Enabled Products Are Emerging All Around Us](html/470825_1_En_3_Chapter.xhtml)\n  6. [4\\. Garbage In, Garbage Out](html/470825_1_En_4_Chapter.xhtml)\n  7. [5\\. Applying a UX Framework](html/470825_1_En_5_Chapter.xhtml)\n  8. [Back Matter](html/470825_1_En_BookBackmatter_OnlinePDF.xhtml)\n\n# Landmarks\n\n  1. [Cover](html/Cover.xhtml)\n  2. [Table of Contents](html/470825_1_En_BookFrontmatter_OnlinePDF.xhtml#Toc)\n  3. [Body Matter](html/470825_1_En_1_Chapter.xhtml)\n\n\n![Cover image](../images/978-1-4842-5775-3_CoverFigure.jpg)\n\n\nGavin Lew and  Robert M. Schumacher Jr.\n\n# AI and UX\n\n## Why Artificial Intelligence Needs User Experience\n\n1st ed.\n\n![../images/470825_1_En_BookFrontmatter_Figa_HTML.png](../images/470825_1_En_BookFrontmatter_Figa_HTML.png)\n\nGavin Lew\n\nS Barrington, IL, USA\n\nRobert M. Schumacher Jr.\n\nWheaton, IL, USA\n\nAny source code or other supplementary material referenced by the author in\nthis book is available to readers on GitHub via the book’s product page,\nlocated at\n[www.​apress.​com/​9781484257746](http://www.apress.com/9781484257746). For\nmore detailed information, please visit [http://​www.​apress.​com/​source-\ncode](http://www.apress.com/source-code).\n\nISBN 978-1-4842-5774-6 e-ISBN 978-1-4842-5775-3\n\n<https://doi.org/10.1007/978-1-4842-5775-3>\n\n© Gavin Lew, Robert M. Schumacher Jr. 2020\n\nThis work is subject to copyright. All rights are reserved by the Publisher,\nwhether the whole or part of the material is concerned, specifically the\nrights of translation, reprinting, reuse of illustrations, recitation,\nbroadcasting, reproduction on microfilms or in any other physical way, and\ntransmission or information storage and retrieval, electronic adaptation,\ncomputer software, or by similar or dissimilar methodology now known or\nhereafter developed.\n\nThe use of general descriptive names, registered names, trademarks, service\nmarks, etc. in this publication does not imply, even in the absence of a\nspecific statement, that such names are exempt from the relevant protective\nlaws and regulations and therefore free for general use.\n\nThe publisher, the authors and the editors are safe to assume that the advice\nand information in this book are believed to be true and accurate at the date\nof publication. Neither the publisher nor the authors or the editors give a\nwarranty, expressed or implied, with respect to the material contained herein\nor for any errors or omissions that may have been made. The publisher remains\nneutral with regard to jurisdictional claims in published maps and\ninstitutional affiliations.\n\nDistributed to the book trade worldwide by Springer Science+Business Media New\nYork, 1 New York Plaza, New York, NY 100043. Phone 1-800-SPRINGER, fax (201)\n348-4505, e-mail orders-ny@springer-sbm.com, or visit www.springeronline.com.\nApress Media, LLC is a California LLC and the sole member (owner) is Springer\nScience + Business Media Finance Inc (SSBM Finance Inc). SSBM Finance Inc is a\nDelaware corporation.\n\nPreface\n\n## Our perspectives and biases\n\nWe have both been around long enough to see technology grow from mail order\nHeathkit computers sold from the ads in _Popular Mechanics_ to the exponential\nand ubiquitous presence that technology has in our lives. And we’re not that\nold.\n\nBecause computing advances came at us so fast, the user was often seen simply\nas an input/output device. The user had to adapt to the system rather than\nbuilding the system around the user’s skills, knowledge, and capabilities.\nWhat has driven us professionally and personally is that what we do as user\nexperience (UX) professionals matters in the lives of people. While working\ntogether at Ameritech (a “Baby Bell” regional phone company) in the 1990s, we\nwere involved with making products more successful by focusing on the UX. We\nevaluated products and often paused to shake our heads and think, “Why would\nanyone design the product _this way_?”\n\nTo put it simply, _we believe experiences matter_. We want to make the world a\nlittle easier for people.\n\nOur perspective on how AI can be more successful is admittedly and unashamedly\nfrom a UX point of view. AI needs a focus on UX to be successful.\n\nUX has a history with DNA strands from several places, most notably\npsychology. We were both trained as experimental psychologists, but along the\nway we had some significant exposure to computer science and AI in particular.\nIt was easy to be seduced by programs like Eliza that seemingly converses with\nyou or believe in proclamations of the glorious future that AI would bring\nbecause the culture had such limited experience in computing technology. It\nwas mysterious and magical. But as the scales fell away, we saw it for what it\nwas: _code_. What we thought was smarter computing was simply clever code that\noften fooled the user. That’s not to say that computer scientists weren’t\nsincere—they understood that these were not true thinking machines. But those\nwho did not understand (journalists and the rest of us) may have gotten over\nour skis as to what AI could do. The payment for overhyping in the eyes of the\npublic was to lose faith and trust in AI.\n\nPart of that loss of trust, as shown in several examples, was due to the fact\nthat AI was often unpolished. The thinking was often this: the AI engine\nworks, yay! But there was not a lot of attention to how the end users\nbenefitted from the AI tool. Humans are impatient and fickle creatures; unless\nthey are going to see the benefit very early on, they often will not invest\nthe time or attention needed to appreciate the AI brilliance. And this is what\nhappened. A bad experience with AI poisons the well. People won’t go back.\nWhat’s worse is that those users will often paint a whole class of AI-enabled\nproducts with the same brush.\n\nThese failed experiences in AI bore a remarkable similarity to failures we saw\ndue to poor UX in product design. Bad experiences meant poor perceptions, lack\nof usage, and ultimately declining success.\n\nBut with AI, we often witnessed a gap where those who normally have strong\nopinions would give AI technology the benefit of the doubt, as it was beyond\ntheir area of expertise. If AI is to be successful, the design matters. The UX\nmatters. How people would interact with AI matters. We believe UX _can_ help;\nthat’s the main point of the book!\n\n## About this book\n\nAI and UX are expansive, and we are unable to plumb the depths of either of\nthem. We try to stay close to what we know and what we thought was relevant to\nmake our points.\n\nWe don’t wish to lump all AI applications together. In this book, we mainly\ncenter on AI that directly touches people doing tasks—whether it’s at home, in\nthe office, or on the go. The focus is not on financial trading algorithms or\nepidemiological modeling or the AI that runs in the background of industrial\nautomation that does not rely on or present information to people. Our focus\nin this book is on the AI that most of us will experience—specifically the AI\nthat is experienced by us all in commonly used applications.\n\nWe employ dialogs in the book that allows us to be more casual and communicate\nmore as you would talking to a friend or colleague. Sometimes we use the\ndialogs to make the point, other times to reinforce it. It is our hope that\nthis technique is successful in highlighting our key points.\n\nThe book is laid out so that in the early chapters, we describe the relevant\nhistory of both AI and UX—and how that history intertwined in the lives of\nsome very influential researchers. We then lay out the specific problem in\nChapter [4](470825_1_En_4_Chapter.xhtml). Chapter\n[5](470825_1_En_5_Chapter.xhtml) is where we are prescriptive about how UX can\nbenefit AI through the user-centered design model.\n\nAcknowledgments\n\nAny endeavor of this sort does not happen without support from an awesome\ncollection of people. What started as a number of discussions with a\ncolleague, Dan Delaney, over lunch perhaps 5 years ago formed the thesis that\ndrove the book, which was to not focus on the algorithms but the impact AI\ncould have on everyday people. But we knew that this could only occur if AI\nwas successful. We spent our careers shaping the design of products to better\nfit those who would use and benefit. _What was the applicability to AI?_ Those\nlunchtime discussions became a concept used in the book where Bob and Gavin\nhave a dialog. We wanted to use these discussions to bridge the gap between\nhistory and opinion to give the reader something tangible and, hopefully, an\ninsightful perspective. Dan is acknowledged as a major influence, and we wish\nhe was able to have played a larger role.\n\nWe also would like to thank Ethan Lew, Gavin Lew’s eldest son who is studying\ncomputer science. When Gavin gave a presentation on some of the key messages\nin the book, Ethan called the main points _rudimentary_. Somehow Gavin was\nable to recover when one of the founding fathers of cognitive science and\nsubsequently UX, Don Norman, told Gavin after listening to the presentation,\n“Rudimentary means it has appeal to a wider audience. Write the book.” Both\nEthan and Don helped energize our belief that the time was right for the\ndiscipline of UX and the development of AI to come together and design better\noutcomes. We can all play a role in shaping AI to be more successful.\n\nWe also want to express our sincere gratitude to Claudette Lew, Gavin’s\nspouse, for countless hours reading revisions for clarity. We are also\nindebted to our team at Bold Insight. Their perspectives were invaluable in\nshaping our ideas and crafting our book. Their insight during brainstorming\nmeetings to review for flow and comprehension helped us bring this whole thing\ntogether.\n\nWe also want to acknowledge the early support from JD Lavaccare who helped\ncraft our early drafts. Our editorial team at Apress was ever-present and\nalways helpful—their ideas sharpened our language and clarified our thinking.\nLastly, many thanks to our families who put up with us while we were catatonic\nfacing a blank page. We did our best to stay with this moving target. Any\nerrors or omissions are entirely our own.\n\nContents\n\n[Chapter 1:​ Introduction to AI and UX](470825_1_En_1_Chapter.xhtml) 1\n\n[Chapter 2:​ AI and UX:​ Parallel Journeys](470825_1_En_2_Chapter.xhtml) 25\n\n[Chapter 3:​ AI-Enabled Products Are Emerging All Around\nUs](470825_1_En_3_Chapter.xhtml) 55\n\n[Chapter 4:​ Garbage In, Garbage Out](470825_1_En_4_Chapter.xhtml) 87\n\n[Chapter 5:​ Applying a UX Framework](470825_1_En_5_Chapter.xhtml) 109\n\n[Index](470825_1_En_BookBackmatter_OnlinePDF.xhtml#Ind1) 139\n\nAbout the Authors\n\nGavin Lew\n\nhas over 25 years of experience in the corporate and academic environment. He\nfounded User Centric and grew the company to be the largest private UX\nconsultancy in the United States. After selling the company, he continued to\nlead a North American UX team to become one of the most profitable business\nunits of the parent organization. He is a frequent presenter at national and\ninternational conferences and the inventor of several patents. He is an\nadjunct professor at DePaul and Northwestern universities. Gavin has a Masters\nin Experimental Psychology from Loyola University and is currently the\nManaging Partner of Bold Insight, part of ReSight Global, a globally funded UX\nconsulting practice across North America, Europe, and Asia.\n\nRobert M. Schumacher Jr.\n\nhas more than 30 years of experience in academic, agency, and corporate\nworlds. He co-owned User Centric with Gavin from its early stages until it was\nsold to GfK in 2012. While at User Centric, Bob helped found the User\nExperience Alliance, a global alliance of UX agencies. Also, he founded User\nExperience Ltd, a UX agency in Beijing. He is co-founder, co-owner, and\nManaging Partner of Bold Insight, part of ReSight Global, a global UX company.\nBob was the editor of and contributor to _The Handbook of Global User\nResearch_ (2009). He has several patents and dozens of technical publications,\nincluding user interface standards for health records for the US government.\nHe also is an Adjunct Professor at Northwestern University. Bob has a Ph.D. in\nCognitive and Experimental Psychology from the University of Illinois at\nUrbana-Champaign.\n\n\n© Gavin Lew, Robert M. Schumacher Jr. 2020\n\nG. Lew, R. M. Schumacher Jr. AI and UX\n<https://doi.org/10.1007/978-1-4842-5775-3_1>\n\n# 1\\. Introduction to AI and UX\n\nThere and back again\n\nGavin Lew1 and  Robert M. Schumacher Jr. 2\n\n(1)\n\nS Barrington, IL, USA\n\n(2)\n\nWheaton, IL, USA\n\nName any field that’s full of complex, intractable problems and that has gobs\nof data, and you’ll find a field that is actively looking to incorporate\nartificial intelligence (AI). There are direct consumer applications of AI,\nfrom virtual assistants like Alexa and Siri to the algorithms powering\nFacebook and Twitter’s timelines, to the recommendations that shape our media\nconsumption habits on Netflix and Spotify. MIT is investing over a billion\ndollars to reshape its academic program to “create a new college that combines\nAI, machine learning, and data science with other academic disciplines.” The\ncollege started September 2019 and will expand into an entirely new space in\n2022.1 Even in areas where you’d not expect to find a whiff of AI, it emerges:\nin the advertising campaign to its new fragrance called Y , Yves Saint Laurent\nshowcased a model who is a Stanford University graduate and a researcher in\nmachine vision.2 The commercial showcases AI as hip and cool—even displaying\nlines of Python code, as well as striking good looks to sell a fragrance line.\nAI has truly achieved mainstream appeal in a manner not seen before. AI is no\nlonger associated with geeks and nerds. AI now sells product.\n\nThe Incredible Journey Of AI\n\nGAVIN: _The Hobbit, or There and Back Again_ by J. R. R. Tolkien tells of\nBilbo Baggins’ incredible journey and how he brought his experience back home\nto tell his tale. That novel opened the door to science fiction and fantasy\nfor me.\n\nBOB: Same for me as well. As I got older, science fiction became more real and\napproachable. What was once fantasy is now tangible. Consider artificial\nintelligence. It has gone farther and faster than I would have believed even a\ndecade ago. And while AI did not encounter dragons, wizards, and elves as in\n_The Hobbit_ , AI did have perils and pitfalls on the journey.\n\nGAVIN: Like Bilbo, the story of AI is a journey that carries lessons to be\nlearned. I think Tolkien’s story was not about where the future can take you,\nbut to not forget what the past can teach, inform, and make better. _The\nHobbit_ was the prelude to an even bigger story that became _The Lord of the\nRings_. AI may indeed have a great future, but getting it right will require\nsome new thinking; this book is a UX researcher’s tale on AI.\n\n**The point** AI has a long history. Learning from its mistakes made in the\npast can set the AI of today for success in the future.\n\nThe world, both inside and outside the tech industry, is abuzz with AI.\n\nThere must be more to AI than being a company’s newest cool thing and giving\nfodder to marketers. The foundation that unlocks the massive opportunity to\nanswer questions and make human lives easier is the power and intrigue of AI.\nBut its potential is dependent upon having technology work. Because when\ntechnology does not work, there are consequences.\n\nOverhyped Failures have Consequences\n\nGAVIN: The excitement around AI is white-hot. As an example, in health care,\nVirginia Rometty, former CEO of IBM, said that AI could usher in a medical\n“Golden Age.”3 AI is in the news everywhere.\n\nBOB: When one thinks of overhyped environments, I think of another Golden Age:\nthe tulip craze in 17th-century Holland. Investing in tulip bulbs became\nhighly fashionable, sending the market straight up. As the hype grew, a\nspeculative bubble emerged where a single bulb hit 10 times an average\nworker’s annual salary.4 Inevitably the market failed to sustain the crazy\nprices and the bubble burst.\n\nGAVIN: Like “tulip mania,” the hype around AI is high, if not “irrationally\nexuberant.” But what may be surprising to many is that this is not the first\ntime AI has been hyped up. The boom years of AI in the late1950s came to a\ncrash in the decade that followed. Virtually all funding for AI was cut and it\ntook another couple of decades to see investment resume.\n\nBOB: This slash of funding all things AI spanned a decade. As research began\nto move into robotics, the accompanying hype around robots led to another\ncrash in the 1980s. AI’s history is long and has seen peaks and valleys. My\nhope is that today’s exuberance we will remember the lessons from the past, so\nthis new era of AI will see a more successful future.\n\n**The point** Failures have implications and have occurred more than once with\nAI. Learning from mistakes made in the past can set the AI of today for\nsuccess in the future.\n\n## Artificial intelligence\n\nThe term “artificial intelligence” was originated by computer scientists John\nMcCarthy, Marvin Minsky, Nathaniel Rochester, and Claude Shannon in 1955. They\ndefined AI as “…making a machine behave in ways that would be called\nintelligent if a human were so behaving.”5 Of course, this still leaves the\ndefinition of AI widely open to interpretation based on the subjective\ndefinition of what is “intelligent” behavior. (Needless to say, we know a lot\nof humans who we don’t think behave intelligently). AI’s definition remains\nelusive and changeable.\n\nThe question “What is intelligence?” is outside the scope of this book,\nfraught as it is with philosophical complications. But, in general, we would\nsupport a version of the original definition of artificial intelligence. The\ndomains contained within artificial intelligence all share a common thread of\nautomating tasks that might otherwise require humans to exercise their\nintelligence.\n\nThere are alternative definitions, such as the one offered by computer\nscientist Roger Schank. In 1991, Schank laid out four possible definitions for\nAI as:\n\n  1. 1.\n\nTechnology that can divine insights with no direction from humans;\n\n  2. 2.\n\n“Inference engines” that can be fed information about any particular field and\ncalculate proper courses of action;\n\n  3. 3.\n\nAny technology that does something that has never been done by technology\nbefore; and\n\n  4. 4.\n\nAny machine capable of learning.\n\nWe see these as four different ways of defining “intelligence.” Schank\nendorses the fourth definition, thereby endorsing the idea that learning is a\nnecessary part of intelligence.\n\nFor the purposes of this book, we will not be using Schank’s definition of\nAI—or anyone else’s. Doing so would require us to redefine past AI systems,\nand even some present AI systems, as outside the realm of AI, which we do not\nintend to do. Machine learning is often a central part of AI, but it’s fairly\nrare. Plenty of AI systems aren’t great at learning on their own, but they can\nstill accomplish tasks that many would consider intelligent. In this book, we\nwant to discuss as many applications of AI as possible, whether they are\ncapable of learning or not. So, we will define **artificial intelligence** in\nthe most broad way possible.\n\nDefinition\n\n**Artificial intelligence** , or AI, has a meaning that is much contested. For\nour purposes, artificial intelligence is any technology that appears to adapt\nits knowledge or learns from experiences in a way that would be considered\nintelligent.\n\nA wide variety of technologies can be considered part of AI. So, we’ve adopted\na broad definition of the term. Today’s AI applications can do many tasks that\nwere previously thinkable or would require exceptional effort. Machine\ntranslators, like Google Translate, can translate between hundreds of\nlanguages in a split second at adequate quality for many applications. Medical\nand business AI can analyze large swaths of data and output insights that can\nhelp professionals do their jobs more efficiently. And, of course, virtual\nassistants allow users to complete tasks such as sending messages and ordering\nproducts with that most natural of interfaces—voice.\n\nThe emergence of artificial intelligence is closely timed with that of user\nexperience, both coming at the advent of the computer age. We’ll go over it in\nmore detail in Chapter [2](470825_1_En_2_Chapter.xhtml) but suffice it to say\nthat some of the most important innovations in AI and computing in\ngeneral—neural networks, Internet gateways, graphical user interface (GUI),\nand more—were made possible by the work of psychologists-turned-computer\nscientists. UX is heavily influenced by psychology, and many of the\npsychologists’ questions were focused on something akin to human-computer\ninteraction (even if some of their work predates the advent of that particular\nfield).\n\nThe point\n\nAI’s definition has centered on using computational methods to accomplish\nintelligent tasks. We won’t concern ourselves with which complex tasks are\n“intelligent” and which aren’t. We’re more concerned with trying to help make\nAI more successful by applying a UX-centered approach.\n\n## User experience\n\nDon Norman coined the term “user experience” in 1993, while working for Apple.\nIn a video for his research organization, the Nielsen Norman Group, Norman\ndescribes user experience as a holistic concept, incorporating the entirety of\nan experience of buying and using a product.6 He presents the example of\npurchasing a computer in the 1990s, imagining the difficulty of lugging the\ncomputer box into one’s car and the intractability of the computer’s setup\nprocess. He implies that these experiences—even as they are seemingly divorced\nfrom the actual functionality of the device—can affect the user’s overall\nperception of the device’s functionality. This reveals the all-encompassing\nnature of **user experience**.\n\nDefinition\n\n**User experience** , or UX, asks designers to look at new technologies as\nexperiences, not products. UX designers use models based on social sciences,\nespecially psychology, to design experiences so that users can effectively and\nefficiently interact with things in their world.\n\n## UX vis-à-vis AI\n\n> _If technology doesn’t work for people…it doesn’t work._7\n\nThis was an old marketing slogan used by Ameritech, which was a Regional Bell\nOperating Company (RBOC). Ameritech formed after the breakup of the Bell\nSystem monopoly in 1984 where AT&T provided long-distance telephone service\nwhile the other RBOCs provided local telephone service. Many know RBOCs as\nPacific Bell (Pac Bell), SBC (Southwestern Bell), NYNEX, BellSouth, US West,\nand so on. The Ameritech slogan represented the work of a small team of 20\nhuman factors engineers and researchers managed by Arnie Lund. Arnie was a\nmentor to us (the authors) and to dozens who learned under his leadership; we\nsaw the evolution of human factors to user experience while working for Arnie.\n\nThe role of this team was to make products “work” for the user. It seems\nrather simple that products, of course, need to work as they were intended.\nBut the key is not whether they work for an engineer, but for the user—someone\nwho bought the product or possibly received it as a gift. Think about some\nproducts you purchased. Think about the ones with batteries or those that plug\ninto the wall or even connect to the Internet. Would you say the setup\nexperience was easy? Unfortunately, there are a lot of products that just make\nus shake our heads and ask who made it so hard to use? The Ameritech slogan\nencapsulated the research and design that was needed not simply to integrate\ntechnology into new products but to transform experiences for people as a\ncritical criterion of success. Incorporating a user-centered approach was not\nthe norm in 1995. UX was not “table stakes” as it is now, but it was such a\nunique selling point for Ameritech that they featured it in broadcast TV ads.\n\nIf AI Doesn’t Work for People, IT Doesn’t Work\n\nGAVIN: The human factors team at Ameritech, under Arnie Lund, was an amazing\ngroup at a special time. For me, it was my first experience applying\npsychology, research, and user-centered design to make positive changes in a\nproduct.\n\nBOB: The team included some spectacular minds who were allowed the freedom to\nmake products useful, usable, and engaging at a time before Apple grabbed hold\nof the _Think different_ mantra.\n\nGAVIN: I remember Ameritech as an organization that had tens of thousands of\nemployees, but yet an entire TV advertising campaign was based on the work of\na very small team.\n\nBOB: The commercials were referred to as the Ameritech Test Town ads.8. They\nillustrated how Ameritech’s human factors team would test new technology with\npeople in everyday places like diners, cars, and barber shops to ensure that\nproducts did not just function, but people could actually use them.\n\nGAVIN: I remember the ads were memorable and scored high on traditional\naudience measures. Everyone in the Midwest in the mid-1990s knew the ads. As I\nremember, the campaign had an ad recall measure that was off the charts good.\nBut, when only half of the respondents thought the ad was about Ameritech;\nhalf mistakenly said it was AT&T.\n\nBOB: That’s true. I’m sure the marketing people were chagrined, but to us, it\ndidn’t matter. The point was not to show off some amazing new technology; it\nwas not about slick cutting-edge features. The real message was about the\n_experience_. If the person who bought the device could not use it, then\nnothing mattered. My 88-year-old father still contacts me every other day\nabout how frustrated he is with his computer.\n\nGAVIN: This was the essence of what made a good UX. And honestly, we have\nstill not come all that far today. Sure, the technology has accelerated, but\nif the person can’t book the trip on a website or program their digital watch\neasily, then the device is just a digital pet rock because it will get little\nto no use. Around 20 years have gone by and while there is certainly an\nincreased awareness of UX, our lives are still as—or perhaps even\nmore—frustrated by products and services in our world.\n\nBOB: As devices get “smarter,” some people might think they will reduce our\nfrustrations. In fact, there is a school of thought that user interfaces will\njust fade into the background. They will be the underlying technology,\nincreasingly invisible because they are so intuitive. I am not entirely sold\non this yet, but we’ll come back to that. For now, as we read on about AI and\nall that AI promises, _the emphasis on how people experience AI seems to be\nmissing._\n\n**The point** Product developers increasingly recognize that good design\nmatters. Understanding how people interact is critical to product success.\n\nThe Ameritech “Test Town” commercials showed short vignettes of futuristic\ntechnology in everyday life. In one, there was a coffee shop where patrons\nwore devices; one of those devices frustrated the user because it kept\nflashing 12:00. (Something many of us can relate to!) The premise was that\nthere were people who worked at Ameritech who were making products easier to\nuse; products that did not just work, but “worked for people.”\n\nThese ads brought attention to the work being done behind the scenes to\nimprove the utility and usability of the technology in Ameritech’s new\nproducts and services. The field of UX is focused on understanding and\nimproving the connection between humans and technology so that the experience\ncould be more than satisfying.\n\nWhen you think of a user experience, consider adjectives and adverbs that\ndescribe the interaction, such as in Figure 1-1. When you design a product or\nservice, you don’t want it to simply be satisfying; sometimes satisfying just\nmeans ‘good enough.’ But isn’t that what you hear about? Customer\n_satisfaction_? But we argue that a product’s success tends to need more than\na satisfying experience. When one focuses on the user experience of a product,\nthe interaction design needs to do much more. When you think of something that\nyou really enjoy using, the words you might use are that is _addictive, fun,\nengaging, intuitive,_ and so on. These are the descriptors that make a user\nexperience great. For success, we must strive for more than satisfaction. We\nneed product to be associated with UX adjectives and adverbs.\n\n![../images/470825_1_En_1_Chapter/470825_1_En_1_Fig1_HTML.jpg](../images/470825_1_En_1_Chapter/470825_1_En_1_Fig1_HTML.jpg)\n\nFigure 1-1\n\nAdjectives and adverbs that are used to describe interactions. These UX\nadjectives and adverbs raise the bar for an experience of a product beyond\n“satisfying”\n\nWe, as UX professionals, can’t help but look at AI as simply an application.\nIt is not just about the promise of what AI can do for us. We believe that\npeering at AI through a UX prism will be instructive; a good UX experience is\nvital to ensure success and future propagation of AI. UX is our area of\nexpertise, and the thesis of this book is to propose applying UX principles to\nthe design and development of user interfaces to artificial intelligence\napplications.\n\nIn the past, AI has been designed by thinking about the functions and the\ncode: _What if we could get AI to do X?_ Designers and developers have big\ndreams for a mind-bending set of AI applications and set out to achieve them.\nWhat we think they ignore all too often is this: _Once AI can do X, what will\nit be like to use AI to do X?_ In other words, developers need to think about\nwhat the _experience_ of using their AI product will be like—even in the early\nstages, when that product is just a big idea. It’s great to dream that the bot\nwill convert speech to text to action, but if the speech is in a crowded bar\nand the speaker just had dental work, how useful is the bot? At this point, we\nposit that an essential element in AI’s success hinges on understanding and\nimproving the _user experience_ —not on giving AI all manner of new functions.\nMost AI applications already have plenty of useful functions, but what good\nare functions if the user can’t use them or doesn’t know how to access them?\n\nHaving a Good Initial Experience Goes a Long Way\n\nBOB: So much time and effort goes into product design. When a bad experience\nhappens, sometimes I think how close the designers came to getting it right.\nWhat could the team that made it have done to get it right? There is often a\nfine line between success and failure.\n\nGAVIN: Yeah, think of voice-enabled calling in a car. The auto manufacturers\nhave had this around for a decade. But now, almost everyone has a phone; how\nmany use the voice feature in their vehicles? The old adage of “fool me once,\nshame on you. Fool me twice, shame on me” might apply to human to human\ninteractions, but human to AI interactions is more like, “Fail me once and you\ntend to not try again.”\n\nBOB: Exactly, imagine a mom driving a car full of kids to a soccer game. She\ntries to use voice calling in the car. If the mom hears, “I don’t understand\nthat command,” do you think she will ever try again? Will she realize the\nambient background noise (i.e., children playing) might have interfered with\nAI’s ability to understand? Most won’t.\n\nGAVIN: Applying this logic to all the technology around us, this need for a\ngood user experience goes beyond voice calling—think about the effort that\ngoes into the design of the 500+ features in a BMW 540, for example. So much\ntime and cost goes into building these features. But, how many do people\nactually use? Just because a feature’s there doesn’t mean it’s useful or\nusable.\n\nBOB: UX focuses on more than how the feature works. Half the battle is helping\npeople get to the feature. Once accessed, does the feature map to how people\nexpect it to work? These are core principles of good design. AI is not a\npanacea. Understanding how users will interact with this output is the\nexperience. And that where focus on the UX is key.\n\n**The point** Products embedded with new technology do not automatically\nensure success—a positive interaction is essential.\n\n## UX framework\n\nWith UX described as an important driver for success, how UX integrates into\nAI-enabled products starts with the introduction of a **UX framework** which\nwill lay the foundation for topics in this book.\n\nDefinition\n\nThe **UX framework** is our method of considering user experience while\ndesigning an AI application. This framework is rooted in classic user-centered\ndesign where the user is at the center, not technology.\n\n## AI-UX principles\n\nIn order to understand how a UX framework can be applied to AI, we’ll consider\nthese three AI-UX principles: context, interaction, and trust. These are\nindependent dimensions that make up our UX framework for AI. We will cover\nthis model in depth in later chapters, but it is instructive to give a small\ntasting of these now. See Figure 1-2.\n\n![../images/470825_1_En_1_Chapter/470825_1_En_1_Fig2_HTML.jpg](../images/470825_1_En_1_Chapter/470825_1_En_1_Fig2_HTML.jpg)\n\nFigure 1-2\n\nAI-UX principles that need to be addressed when designing AI products and\nservices\n\nThe point\n\nThe next wave of AI needs to be designed with a UX framework in mind or risk\nthe further limiting of acceptance. Good UX for AI applications will propel\ngrowth.\n\n### Context\n\nIn recent years, IBM spent $4 billion on acquisitions and expenses in the\nhealthcare sector, mostly to boost the capabilities of its new medical AI,\nWatson Health.9 The results have been mixed. Watson Health has shown some\nincredible promise, but it’s also stagnated. _The Wall Street Journa_ _l_\npublished a scathing article about Watson Health’s failures in 2018.10 The\narticle alleged that “more than a dozen” clients have cut back or altogether\ndropped their usage of Watson Health’s oncology (cancer treatment) programs\nand that there is little to no recorded evidence of Watson Health’s\neffectiveness as a tool for helping patients.\n\nIn 2017, Watson Health’s ability to create cancer treatment plans was tested\nfor agreement with doctors’ recommended treatment plans in both India and\nSouth Korea. When Watson was tested on lung, colon, and rectal cancer patients\nin India, it achieved agreement rates ranging from 81% to 96%. But when it was\ntested on gastric cancer patients in South Korea, it achieved just 49%\nagreement. Researchers blamed the discrepancy on the diagnostic guidelines\nused by South Korean doctors, which differed from those Watson was trained on\nin the United States.11\n\nDon’t Limit AI to Imitating Human Behaviors\n\nBOB: So Watson learned how to diagnose and recommend cancer treatments using a\nUS dataset. Everyone cheered when Watson recommended treatment plans that US\ndoctors recommended. But, when applied to South Korean cases, it missed the\nmark. But, is this the criterion for success? Replicating what US doctors do?\n\nGAVIN: This is the point! AI should not merely replicate. The fact that AI\nfound a difference is noteworthy. Perhaps we need to change our thinking. AI\nfound a difference. And _this_ is the insight. AI is raising its hand and\neffectively asking, “What are South Korean oncologists doing that US\noncologists are not? And why are they making those decisions?” Instead, many\ninterpreted this as an inability to imitate human decision making and\ntherefore Watson failed.\n\nBOB: Yes! Improving health outcomes is the goal—not whether a computer’s\nrecommendations correlate to a human. AI’s contribution asks us to investigate\nthe difference in treatments. This might advance care by looking at the\ndifferences to find factors that improve outcomes. It is the difference that\nmight help.\n\n**The point** We limit AI by determining success on whether its outcomes\ncorrelate with human outcomes. This is merely the first step where AI can\nidentify differences. Knowledge is furthered when this insight spurs more\nquestions. This leads to the ultimate goal: better health outcomes. When we\nmake the goal about replicating human outcomes, we do a disservice to AI.\n\nIdeally, Watson should be applauded for identifying that there is a difference\nin treatment plans between US and South Korean cases. AI does not have to\nsolve the entire problem. Finding a difference that was previously unknown is\na big step. AI alerted us to a difference. Now, we can investigate and\npossibly save lives.\n\nThis is a good example of engaging more than just programmers with the\nchallenge of making AI solve problems. Let’s take control and design a product\nwith AI as a team. Bring in product teams, programmers, oncologists, and even\nmarketing to approach the problem and not assume AI will figure it out by\nitself.\n\nWe’ll talk more about AI examples, like Watson Health in Chapter\n[3](470825_1_En_3_Chapter.xhtml), but for now, suffice it to say that it is\none example of how AI can stagnate without an awareness of **context**.\n\nDefinition\n\n**Context** includes the outside information that AI can use to perform a\ntask. It includes information about the user and why they are making the\nrequest, as well as information about the external world.\n\nThe point\n\nThose who work on AI-enabled products need to understand the context of its\noutput—what its output means, how it compares to objectives and expectations,\nand more.\n\n### Interaction\n\nGavin’s college roommate at the University of California-San Diego was Craig\nNies, who studied computer science. He went on to help develop Falcon, a\npioneering AI algorithm that detected credit card fraud in the early 1990s.\nGavin talked to Craig while working on this book, and Craig explained how he\nused a type of AI known as a neural network to detect suspicious purchases.\nFalcon incorporated variables like geographic location and types of stores to\ngive each transaction a score. High-scored transactions meant a phone call to\nthe cardholder and a possible canceled transaction.\n\nThis worked well in many situations, including situations of actual fraud. But\nit had trouble with false positives, in which it marked non-fraudulent\npurposes as fraudulent. One particularly problematic case was international\ntrips. Back in the 1990s, not everyone had a cell phone, and even those cell\nphones that were around had difficulties with international calling. If you\ndidn’t have your working phone on you, or the credit card company neglected to\ncall, you were liable to lose your credit card for your entire international\ntrip. Of course, this could spell disaster.\n\nToday, there is only one additional step involved. The credit card companies\nstill use similar fraud detection mechanisms, identifying likely fraudulent\npurchases by geographic location and store type, among other factors. But now,\nthanks to the ubiquity of smartphones, not to mention the increasing coverage\nof mobile data and Wi-Fi networks, your credit card company can send an alert\nto your phone that asks whether or not you made the suspicious purchase in\nquestion. If you did, you can tap _Yes_ and your purchase will go through just\nfine.\n\nThat additional **interaction** with the user makes a world of difference.\n\nDefinition\n\n**Interaction** refers to AI engaging the user in a way in which they can\nrespond. That engagement could come in many forms: a message in the AI’s\ninterface, a text message, a push notification to their smartphone, etc.\n\nWhen the AI system makes its conclusion that the purchase is likely fraud, it\ndoesn’t act immediately to cancel the transaction and lock down the card.\nInstead, the AI algorithm has a convenient way to reach out to the user and\nmake sure that the users don’t object to it taking this action. While this\nrequest for user consent is not foolproof (e.g., perhaps the user’s phone has\nbeen stolen alongside the card, or it is out of battery), it works better than\nthe old method of giving the user a phone call. It’s a more effective\ninteraction—a necessity when the possible impacts are so great.\n\nThe point\n\nBefore AI takes a potentially impactful action on a user’s behalf, it should\nattempt to interact with the user. Communication is key. This is the\nexperience that AI needs. The interaction needs to be designed.\n\n### Trust\n\nWhen you think of **trust** relative to a device interaction, the initial\nexpectation is that the device does what it is supposed to do. But from a UX\nperspective, **trust** can go further. Here is an example. If you are familiar\nwith the iPhone, when you hear the Siri startup beep, what is your reaction?\nYou might recoil a little: _Ugh, I accidentally pressed it again_. But why\ndoes Siri deliver such a negative visceral reaction in many of us? It’s\nsupposed to be a helpful tool, after all. Why don’t we _trust_ it?\n\nDefinition\n\n**Trust** is when users feel that an AI system will successfully perform the\ntask that a user wants it to perform, without any unexpected outcomes.\nUnexpected outcomes can include performing additional (unnecessary or\nunhelpful) tasks that the user did not ask for or breaching the user’s privacy\nin a way that the user could not have anticipated. Trust is sticky—that is, if\na user trusts a service, they’re likely to keep trusting it, and if they don’t\ntrust it, they’re likely to continue to mistrust it.\n\nSiri is one of many voice assistants that _listen_ for spoken commands. The\nvoice assistant recognizes phrases and processes the information. Consider the\nexample described in the car full of kids with the mom engaging a voice\nfeature. In the early days of voice assistants, the AI system listened for\nsimple grammar that took a verb + subject and turned it into a command, like,\n“Call [Dad].” As the technology continued to improve, dictation of speech to\ntext became more and more accurate.\n\nDefinition\n\nA **voice assistant (or virtual assistant)** is an AI-based program that\nallows users to interact with an application through a natural language\ninterface. Virtual assistants have tens of thousands of applications available\nto users that can perform all manner of tasks for the user: get weather, make\ncat sounds, tell jokes, sing songs, etc.\n\nWhen Siri was first released in 2011 on iPhones (in what Apple termed a “beta”\nversion),12 early reviews were met with cheer and accolades (finally!) there\nwas a voice-based interface. Unfortunately, the honeymoon did not last long.\nUsers began to express frustration and strong negative associations formed. As\nexamples, Siri had issues with both usability and functionality; Siri could be\ntriggered accidentally and its speech recognition was not nearly as robust as\nusers expected. All too often Siri would apologize and say, “Sorry, I don’t\nunderstand…” And even if it did properly recognize the speech, it frequently\nmisconstrued the request. Very quickly, users also wanted to do more with it\nthan it was designed for. In short, users (err, customers) originally had in\ntheir minds an application with such great promise, but the reality was\nexceedingly modest. Apple had a failure on their hands.13\n\nTrust is Formed by More Positive Experiences\n\nBOB: For any product, whether it has AI or not, the bare minimum should be\nthat it be usable and useful. It needs to be easy to operate, perform the\ntasks that users ask of it accurately, and not perform tasks it isn’t asked to\ndo. That is setting the bar really low, but there are many products in the\nmarketplace that are so poorly designed where this minimum bar is not met.\n\nGAVIN: Just think about how many TV remotes sit on the coffee table of your\nliving room and the sheer number of buttons! It makes you question how much\neffort went into the design of the remote. Consider the experience of using\nthe remote. It is often at night.\n\nBOB: The worst is the remote that controls the TV settings. Sometimes we\nfumble in the dimly lit room and accidentally press the wrong button on the\nremote. When you fumble in the dark, how often has a MENU or SETUP popup\nwindow appeared when you thought you pressed the BACK button?\n\nGAVIN: The design challenge becomes even more difficult when we move to AI.\nThink about the Siri experience which is entirely independent of a screen\ninterface like one on a TV. Because everything happens by voice, the dialog\nneeds to be well developed. It must work or people will abandon it.\n\nBOB: When things work well or the experience is good one, one forms a feeling\nof trust. And when the voice dialog on Siri does not work, what happens next?\nRepeat the phrase? But, how many times would someone try? This delivers a\nfeeling of mistrust.\n\nGAVIN: This is important because it can explain why Siri has fallen into\ndisuse. After multiple failures, trust evaporates. The result is that people\njust stop using the product.\n\nBOB: And with a voice assistant, the feeling can be persistent. Let’s say\nApple made Siri better and solved some of the “I am sorry, I don’t know that\nyet” interactions. How would you know?\n\nGAVIN: All that effort put in to make Siri smarter would be for naught. This\nputs the product on a downward path where it is difficult to recover.\n\n**The point** Our perception of a product is the sum total of the experiences\nthat we have with that product. Does the product deliver the value we had\nhoped? Our willingness to “trust” the product hangs in that balance.\n\n### The role of trust in UX\n\nThe field of behavioral economics, which combines psychology and economics,\nhas provided insights that are critical to the pillar of “trust” in UX. Daniel\nKahneman, a Nobel Prize winner and critical figure in behavioral economics,\ndivides the brain into two systems.14 “System 1” is the system of passions\nthat guides the brain: it offers the intuitive judgments that we make\nconstantly and governs our emotional responses to situations. “System 2” is\nthe system of reason: it comes to considered judgments after long periods of\nanalysis. Kahneman wishes to subvert the conventional wisdom that rational\nthinking is always superior to emotion and that poor decisions usually come\nfrom following instincts rather than reason. Kahneman points out that\nintuition is often effective. System 1 is what allows us to drive a car,\nmaintain most of our social relationships, and often even to answer\nintellectual questions.\n\nBehavioral economists like Kahneman have proposed three important heuristics,\nor mental shortcuts, that are used often by System 1: affect, availability,\nand representativeness. For our purposes, we are going to focus on the affect\nheuristic. The affect heuristic dictates that our initial emotional judgments\nof someone or something will dictate whether or not we trust that person or\nthing.15 This is what sank Siri. Initially, the virtual assistant was\ncumbersome to use and that affective association of negative feelings with\nSiri lingered even after the service itself was improved.\n\nIn 2014, Amazon came out with its Echo, featuring the Alexa virtual assistant.\nThe device presented the virtual assistant with a particular use scenario in\nmind—one that was particularly accommodating to virtual assistant use. In the\nhome, where the Amazon Echo is meant to sit, you’re less likely to feel\nashamed about talking to your tech. That’s not the only difference between the\nEcho and Siri—the Echo also looks completely different, sitting as it does in\na cylindrical device that is dedicated entirely to its use.\n\nAmazon worked hard to make sure that its virtual assistant, Alexa, would\nprovide a good experience from its inception. They may have been inspired by\nthe failure of Amazon’s Fire Phone just a couple years earlier. The Fire Phone\nseemed like a minimum viable product,16 and it flopped out of the gate. But\nthe Echo was different. While building the Echo, Amazon ran tests including\nthe “Wizard of Oz test.” In that test, users asked questions, which were fed\nto a programmer in the next room, who typed in responses that were then played\nin Alexa’s voice.17 Amazon used the test to analyze the vocal attributes that\ngot the best response from users. Amazon took the time and effort to build a\nproduct that would engender trust, and it showed. We don’t exactly know what\nuser research Apple did with Siri, but whatever they did, the outcome was not\nas successful as it was for Amazon.\n\nThe point\n\nTrust is vitally important to user adoption, and it’s easily lost. Developers\nneed to be careful to design an experience that engenders trust.\n\n## The need for UX design\n\nAt the core of the UX outlook on design is the concept of “affordances,”\ndeveloped by psychologist James Gibson. **Affordances** are points of\ninteraction between object and perceiver that allow the perceiver to\nunderstand the object’s features (the things that the object can do for the\nperceiver and for other agents).18 Gibson sees these properties as extant in\nthe universe.\n\nCertain affordances are easy for us to discover, perhaps directed by cultural\nnorms for using an object or by the object’s design. Doors with flat plates\nafford pushing, while doors with loop handles afford pulling. Google’s\nhomepage is dominated by a single text box with a search button and lots of\nwhite space, indicating that it allows you to search for anything you’d like.\nRecognition of the power affordances have to provide information to the user\nis woven into the design of the product to improve usability, function, and\nuse.\n\nHowever, some of an object’s features may be less clear—meaning that the\ncorresponding affordances are only present for users who are in the know. For\ntech products, these are the kind of features that end up being revealed to\nusers by accident or through viral online articles. (They have titles like “10\nThings You Didn’t Know Your Phone Could Do.”) If the user does not know that\nthe object has a certain function made clear by an affordance, that function\nbecomes much less useful. Whenever the number of functions exceeds the number\nof affordances, there is trouble. Therefore, the designer must be skilled at\ncommunicating to the user what the object is capable of—in other words, the\ndesigner must create “signifiers” (another term from Norman) that communicate\nthe object’s affordances (e.g., Google’s search box).19\n\nAffordance generation is two-sided. An object must have certain properties,\nand a user must recognize possible functions for those properties. For this\nreason, users of products may discover affordances that the designer never\nintended. For example, Facebook likely intended its groups feature to link\nreal-life groups of friends, coworkers, and classmates, but many users have\ninstead used them to share memes and inside jokes with like-minded strangers.\nFacebook seems to have welcomed the opportunity to retain younger users, even\nrolling out a new screening feature that particularly helps meme-based\ngroups.20 After users discovered a new affordance for Facebook’s groups\nfeature, Facebook updated its product to reflect a use case that they were\nnever planning for. This is illustrative of the active role that users can\nplay in further shaping the design. Design needs to recognize the many ways\nthat a user might use a feature.\n\nIT’s not Always About Aesthetics\n\nGAVIN: Sometimes the user’s needs can come in conflict with the aspirations of\nthe designer. Consider Apple’s $5 billion dollar state-of-the-art headquarters\nbuilt in 2018 by architect Norman Foster. The building used rounded glass that\nwas designed to “achieve an exact level of transparency and whiteness.” 21\n\nBOB: The problem was that people could not tell where the door ended and the\nwall began. And even the building inspector cautioned on this risk. But to the\narchitect, it was all about the design and not about those affordances.\n\nGAVIN: What happened? Workers walked right into the glass so hard that 911 was\ncalled three times in the first month! Employees were so fearful that they\nplaced their own affordances on the walls–sticky notes–to prevent more\ninjuries.\n\nBOB: But the building designers removed the sticky notes because it was said\nto have detracted from the building’s design aesthetics.\n\nGAVIN: Not only is this ironic that it happened at Apple, but talk about\narchitects not living in the places they design. We’ve heard that Apple-\napproved stickers were made after that to provide better affordances to\ndistracted walkers in an attempt to reduce 911 calls due to injuries.\n\n**The point** Sometimes the design for design sake can get in the way. How\nusers actually engage (or in this case, walk) can often be at odds with the\naesthetic. Design needs to work for the whole user, not simply what the eye\nsees.\n\nThe user-centered design ethos that is at the core of UX differs from the\nstereotypical association of the term “design” with form and aesthetics. While\nform and aesthetics are certainly important components of an experience, they\nneed to be combined with functionality in order to deliver the best possible\nexperience to the user. UX design focuses on the ways in which form and\nfunction can complement one another, without compromising either one _for_ the\nother.\n\nThis vision of design was well articulated in a lengthy opinion piece co-\nauthored by Don Norman and Bruce Tognazzini in 2015, criticizing the design of\nApple’s operating system for its smartphones and tablets.22 Norman and\nTognazzini, who had both worked for Apple during the pre-iDevice days, felt\nthat Apple had once been a leader in user-centered design, but that it had\nsince lost its compass. They centered their criticisms around iOS’s lack of\ncertain useful affordances, such as a universal back button to undo actions,\nas well as its lack of signifiers for many of the affordances it does have.\n\nApple’s gestural interfaces rely on the concept of second nature. Human beings\nare built to learn and adopt new systems of interacting with the world and\nquickly become so familiar with them that they become instinctive.23 This is\nwhat we have all done with swiping around our phones, pinching to zoom in and\nout, and the rest of the gestures that allow us to use smartphones and\ntablets. Apple, however, has gone the extra mile with its gestural interfaces,\nworking in all manner of different gestures. To see this in action, all you\nhave to do is go to an Apple store and swipe all of the various devices there\nwith three fingers or your palm in different directions and patterns. Odds are\nthe device will start to do lots of unexpected functions.\n\nThe problem that Norman and Tognazzini identify is that most users have no\nnatural way of discovering these gestural features. There are no on-screen\nindicators that these features are present, and very few users are going to\nexperiment with the OS or read the manual in order to find out about them. So,\nfor all intents and purposes, these features don’t exist for most users—except\nto confuse them when they accidentally trigger them while trying to do\nsomething else. That leads to negative interactions.\n\nUsers’ perception of their experiences is vital to their continuing to return\nto those same experiences time and time again. People will generally tend to\nbuild patterns of engagement with objects in their world that they consider to\nbe profitable and enjoyable.\n\nUX Describes the Holistic Experience of Interacting With a Product\n\nBOB: If I need to get something done, I’m not going to use an iPad. I’m not\ngoing to use a smartphone. I’m certainly not going to tell Alexa. I’m going to\nturn on my computer and point and click my way through a complicated task. And\nthat’s not just because it has a faster processor; it’s because a computer is\neasier to use with respect to complicated tasks.\n\nGAVIN: Norman and Tognazzini pointed it out. When Xerox and Apple were working\non the first point-and-click graphical user interfaces, they had UX principles\nin mind—even if the term wasn’t invented yet.\n\nBOB: Today, with touch interfaces, it seems like that’s backward. The\ntechnology that enabled multi-touch interactions seemed to be “natural\ngestures” as Steve Jobs called them. In many ways, the gesture itself took\nprecedence over the function. There was almost a look of disgust at those who\ndid not know how to pinch, swipe, or flick with their fingers to interact with\nthe iPhone. It was as if the gesture was more important than the function\nitself.\n\nGAVIN: Apple’s first commercials on the iPhone—which were incidentally paid by\nAT&T in the US launch in 2007—were all about how to use the iPhone. It was as\nif Apple made the purpose of their commercials to show the user manual. This\nwas a phenomenal sleight of hand. Who spends hundreds of millions in marketing\nto show people how to use a product? And Apple argued that touch was so\nsimple—but what came first, the user-manual commercials or the gesture?\n\nBOB: This is a critical consideration as we move into a new era of AI. It\nseems like we didn’t really master how to maximize functionality in touch\ninterfaces to the extent that we did with point-and-click interfaces. But\nwhile we try to fix that, we’re going to have to turn some of our attention\nelsewhere. AI brings up all types of new interface possibilities. Voice\ninterfaces are the obvious one, thanks to the virtual assistant. But there’s\nmore. There are gestural interfaces where there is a camera watching for\nmovement, like raising your hand in front of smart TV or like the Microsoft\nKinect. Computers are starting to read facial expressions and detect\naffect—even being present in a room is data to AI. Down the road, there may\neven be neural interfaces—brain waves going directly from your brain to the\ncomputer.\n\nBOB: Neural interfaces are a ways away. But I get what you mean, especially\nwith voice. Voice interfaces are a difficult case for usability, even more so\nthan touch, because there are fewer opportunities to communicate affordances\nto the user. Without any visual signifiers, UX gets a lot harder. With screen-\nbased interfaces, as a designer you can provide visual affordances and rely on\npeople’s ability to recognize information. With voice, the interface is\nnatural language—appearing to be wide open to the infinite number of sentences\nand questions that I could ask.\n\nGAVIN: These are important questions that we’re facing in tech right now. But\nUX people aren’t always in the room where the decision makers are, and, more\noften than not, they should be.\n\n**The point** UX describes the holistic experience of interacting with a\nproduct.\n\n## Conclusion: Where we’re going\n\nIn the next chapter, we will look at the simultaneous, but independent\ndevelopment of the fields of AI and UX (then called human-computer interaction\nor human factors) and consider the relevant historical and intersectional\npoints that help us to glean lessons from the past and move toward better\ndesign. We’ll also discuss the legacy of a few psychologists and how their\nwork shaped both UX and AI. In Chapter [3](470825_1_En_3_Chapter.xhtml), we\nwill examine the state of AI today and where UX is and isn’t coming into play.\nWe will also take a look at some of the psychological principles underlying\nhuman-computer interaction, a key component of interaction. In the later\nchapters, we will propose, and justify, our UX framework for AI success and\ndiscuss its implications for the future.\n\nFootnotes\n\n1\n\nKnight, Will (2018). “MIT has just announced a $1 billion plan to build a new\ncollege for AI.” MIT Technology Review. Last updated October 15, 2018. Last\naccessed June 2, 2020. [www.technologyreview.com/f/612293/mit-has-just-\nannounced-a-1-billion-plan-to-create-a-new-college-for-\nai/](http://www.technologyreview.com/f/612293/mit-has-just-\nannounced-a-1-billion-plan-to-create-a-new-college-for-ai/).\n\n2\n\nJames, Vincent (2017). “AI is so hot right now researchers are posing for Yves\nSaint Laurent.” The VERGE. Last updated August 31, 2017. Last accessed August\n12, 2019. [www.theverge.com/tldr/2017/8/31/16234342/ai-so-hot-right-now-ysl-\nalexandre-robicquet](http://www.theverge.com/tldr/2017/8/31/16234342/ai-so-\nhot-right-now-ysl-alexandre-robicquet).\n\n3\n\nStrickland, Eliza (2019). “How IBM Watson Overpromised and Underdelivered on\nAI Health Care.” IEEE Spectrum. Last updated April 2, 2019. Last accessed June\n2, 2020. [https://spectrum.ieee.org/biomedical/diagnostics/how-ibm-watson-\noverpromised-and-underdelivered-on-ai-health-\ncare](https://spectrum.ieee.org/biomedical/diagnostics/how-ibm-watson-\noverpromised-and-underdelivered-on-ai-health-care).\n\n4\n\nGoldgar, Anne (2008). Tulipmania: money, honor, and knowledge in the Dutch\nGolden Age. University of Chicago Press.\n\n5\n\nPress, Gil. “Artificial Intelligence (AI) Defined.” Forbes. Published August\n27, 2017. Accessed June 2, 2020.\n[www.forbes.com/sites/gilpress/2017/08/27/artificial-intelligence-ai-\ndefined/#44ac4d9f7661](http://www.forbes.com/sites/gilpress/2017/08/27/artificial-\nintelligence-ai-defined/%252344ac4d9f7661).\n\n6\n\nNNgroup. “Don Norman: The term ‘UX’.” YouTube video, 01:49. Posted July 2,\n2016. Accessed June 2, 2020.\n[www.youtube.com/watch?v=9BdtGjoIN4E](http://www.youtube.com/watch%253Fv%253D9BdtGjoIN4E).\n\n7\n\nOddly, Ameritech never filed for a trademark for this slogan. Ultimately, a\nregistered trademark was given to User Centric, Inc., a company that was owned\nby both authors, in 2005.\n\n8\n\nSee\n[www.youtube.com/watch?v=lKUZPR52uCU](http://www.youtube.com/watch%253Fv%253DlKUZPR52uCU)\nfor one of the commercials. Last accessed June 2, 2020.\n\n9\n\nAgence France-Presse. “IBM Buys Truven Health Analytics for $2.6 Billion.”\nIndustryWeek. Last updated February 16, 2016. Accessed June 2, 2020.\n[www.industryweek.com/finance/ibm-buys-truven-health-\nanalytics-26-billion](http://www.industryweek.com/finance/ibm-buys-truven-\nhealth-analytics-26-billion).\n\n10\n\nHernandez, Daniela and Ted Greenwald. “IBM has a Watson dilemma.” _The Wall\nStreet Journal_. Last updated August 11, 2018. Accessed June 2, 2020.\n[www.wsj.com/articles/ibm-bet-billions-that-watson-could-improve-cancer-\ntreatment-it-hasnt-worked-1533961147](http://www.wsj.com/articles/ibm-bet-\nbillions-that-watson-could-improve-cancer-treatment-it-hasnt-\nworked-1533961147).\n\n11\n\nRamsey, Lydia. “Here’s how often IBM’s Watson agrees with doctors on the best\nway to treat cancer.” Business Insider India. Last updated June 2, 2017.\nAccessed June 2, 2020. [www.businessinsider.in/Heres-how-often-IBMs-Watson-\nagrees-with-doctors-on-the-best-way-to-treat-\ncancer/articleshow/58965531.cms](http://www.businessinsider.in/Heres-how-\noften-IBMs-Watson-agrees-with-doctors-on-the-best-way-to-treat-\ncancer/articleshow/58965531.cms).\n\n12\n\nTsukayama, Hayley. “Apple’s Siri shows she’s only a beta.” The Washington\nPost. Last updated November 4, 2011. Accessed June 2, 2020.\n[www.washingtonpost.com/business/technology/apples-siri-shows-shes-only-a-\nbeta/2011/11/04/gIQA6wdzlM_story.html?](http://www.washingtonpost.com/business/technology/apples-\nsiri-shows-shes-only-a-beta/2011/11/04/gIQA6wdzlM_story.html?)\n\n13\n\nKinsella, Brett. “How Siri Got Off Track – The Information.” Last updated\nMarch 14, 2018. Accessed August 14, 2019.\n[https://voicebot.ai/2018/03/14/siri-got-off-track-\ninformation/](https://voicebot.ai/2018/03/14/siri-got-off-track-information/).\n\n14\n\nBhalla, Jag. “Kahneman’s Mind-Clarifying Strangers: System 1 & System 2.” Big\nThink. Last updated March 7, 2014. Accessed June 2, 2020.\n[https://bigthink.com/errors-we-live-by/kahnemans-mind-clarifying-\nbiases](https://bigthink.com/errors-we-live-by/kahnemans-mind-clarifying-\nbiases).\n\n15\n\n“Affect heuristic.” Behavioral Economics. Accessed June 2, 2020.\n[www.behavioraleconomics.com/resources/mini-encyclopedia-of-be/affect-\nheuristic/](http://www.behavioraleconomics.com/resources/mini-encyclopedia-of-\nbe/affect-heuristic/).\n\n16\n\nA “minimum viable product” (MVP) is one that has only the necessary features\nto deliver value to users and capture market share; frequently, it is also\nused to learn from the market how to grow and improve the product.\n\n17\n\nKim, Eugene. “The inside story of how Amazon created Echo, the next billion-\ndollar business no one saw coming.” Business Insider Australia. Last updated\nApril 2, 2016. Accessed June 2, 2020. [www.businessinsider.com.au/the-inside-\nstory-of-how-amazon-created-\necho-2016-4](http://www.businessinsider.com.au/the-inside-story-of-how-amazon-\ncreated-echo-2016-4).\n\n18\n\nGibson, James J. “The Theory of Affordances.” Semantic Scholar. Accessed June\n2, 2020. From _The Ecological Approach to Visual Perception_. Houghton Mifflin\n(Boston): 1979.\n[https://pdfs.semanticscholar.org/eab2/b1523b942ca7ae44e7495c496bc87628f9e1.pdf](https://pdfs.semanticscholar.org/eab2/b1523b942ca7ae44e7495c496bc87628f9e1.pdf).\n\n19\n\nNorman, Don. “Signifiers, not affordances.” [jnd.org](http://jnd.org). Last\nupdated November 17, 2008. Accessed June 2, 2020.\n[https://jnd.org/signifiers_not_affordances/](https://jnd.org/signifiers_not_affordances/).\n\n20\n\nSung, Morgan. “The only good thing left on Facebook is private meme groups.”\nMashable. Last updated August 9, 2018. Accessed June 2, 2020.\n[https://mashable.com/article/weird-facebook-specific-meme-\ngroups/](https://mashable.com/article/weird-facebook-specific-meme-groups/).\n\n21\n\nGibbs, Samuel (2018). “Is Alexa always listening? New study examines\naccidental triggers of digital assistants.” The Guardian. Last updated March\n5, 2018. Accessed June 2, 2020.\n[www.theguardian.com/technology/2018/mar/05/apple-park-workers-hurt-glass-\nwalls-norman-foster-steve-\njobs](https://www.theguardian.com/technology/2018/mar/05/apple-park-workers-\nhurt-glass-walls-norman-foster-steve-jobs).\n\n22\n\nNorman, Don and Tognazzini, Bruce. “How Apple Is Giving Design A Bad Name.”\nFast Company. Last updated November 10, 2015. Accessed June 2, 2020.\n[www.fastcompany.com/3053406/how-apple-is-giving-design-a-bad-\nname](http://www.fastcompany.com/3053406/how-apple-is-giving-design-a-bad-\nname).\n\n23\n\nBhalla, Jag. “Inheriting Second Natures.” Scientific American. Last updated\nApril 25, 2013. Accessed June 2, 2020.\n[https://blogs.scientificamerican.com/guest-blog/inheriting-second-\nnatures/](https://blogs.scientificamerican.com/guest-blog/inheriting-second-\nnatures/).\n\n\n© Gavin Lew, Robert M. Schumacher Jr. 2020\n\nG. Lew, R. M. Schumacher Jr. AI and UX\n<https://doi.org/10.1007/978-1-4842-5775-3_2>\n\n# 2\\. AI and UX: Parallel Journeys\n\nGavin Lew1 and  Robert M. Schumacher Jr. 2\n\n(1)\n\nS Barrington, IL, USA\n\n(2)\n\nWheaton, IL, USA\n\nIn this chapter, we’re going to take you through some key milestones of both\nAI and UX, pointing out lessons we take from the formation of the two fields.\nWhile the histories of AI and UX can fill entire volumes of their own, we will\nfocus on specific portions of each.\n\nIf we step back and look at how AI and UX started as separate disciplines,\nfollowing those journeys provides an interesting perspective of lessons\nlearned and insight. We believe that at the confluence of these two\ndisciplines is where AI will have much more success.\n\nUX is a relatively modern discipline with its roots in the field of\npsychology; as technology emerged, it became known as the field of human-\ncomputer interaction (HCI). HCI is about optimizing the experience people have\nwith technology. It is about design and it recognizes that a designer’s\ninitial attempt on the design of the product might require modification to\nmake the experience a positive one. (We’ll discuss this in more detail later\nin this chapter.) As such, HCI emphasizes an interactive process. At every\nstep of the interaction, there can and should be opportunities for the\ncomputer and the human to step back and provide each other feedback, to make\nsure that each party contributes positively and works comfortably with the\nother. AI opens up many possibilities in this type of interaction, as AI-\nenabled computers are becoming capable of learning about humans as a user in\nthe same way that a real flesh-and-blood personal assistant might. This would\nmake a better-calibrated AI assistant, one far more valuable than simply a\ntool to be operated: a partner rather than a servant.\n\n## The Turing Test and its impact on AI\n\nThe exact start of AI is a subject of discussion, but for practical purposes,\nwe choose to start with the work of computer scientist, Alan Turing. In 1950,\nTuring proposed a test to determine whether a computer can be said to be\nacting intelligently. He felt that an intelligent computer was one that could\nbe mistaken for a human being by another human being. His experiment was\nmanifest in several forms that would test for computer intelligence. The\nclearest form involved a user sending a question to an unknown respondent,\neither a human or a computer, which would provide an answer anonymously. The\nuser would then be tasked with determining whether this answer came from a\nhuman or from a computer. If the user could not identify the category of the\nrespondent with at least 50% accuracy, the computer would be said to have\nachieved intelligence, thereby passing the “Turing Test.”\n\nDefinition\n\nThe **Turing Test** is a procedure intended to determine the intelligence of a\ncomputer by asking a series of questions to assess whether a human is unable\nto distinguish if a computer or human is giving responses.1\n\nThe Turing Test has become a defining measure of AI, particularly for AI\nopponents, who believe that today’s AI cannot be said to truly be\n“intelligent.” However, some AI opponents, such as philosopher John Searle,\nhave proposed that Turing’s classification of machines that seem human as\nintelligent may have even gone too far, since Turing’s definition of\nintelligent computers would be limited to machines that imitate humans.2\nSearle argues that intention is missing from the Turing Test and the\ndefinition of AI goes beyond syntax.3 Elon Musk takes a similar view of\nintelligence4 to Searle, proposing that AI simply delegates tasks to\nconstituent algorithms which consider individual factors and does not have the\nability to consider complex variables on its own, so this would argue AI is\nnot really intelligent at all.\n\nAs far as we know, no computer has ever passed the Turing Test—though a recent\ndemonstration by Google Duplex (making a haircut appointment) was eerily\nclose.5 The Google Duplex demonstrations are fascinating as they represent\nexamples of natural language dialog. The recordings start with Google’s Voice\nAI placing a telephone call to a human receptionist to schedule a hair\nappointment and a call to make a reservation with a human hostess at a\nrestaurant.6 What is fascinating is the verbal and nonverbal cues that were\ndesigned into the computer voice, such as pauses and inflection by Duplex,\nwere interpreted by the human successfully. On face, Duplex engaged into a\nconversation with a human where the human does not appear to realize that a\nmachine is operating at the other end of the call. It’s unclear how many\niterations Google actually had to go through to get this example. But, in this\ndemonstration, the machine—through both verbal and nonverbal cues—seemed to\nsuccessfully navigate a human conversation without the human showing any\nknowledge of or negative reaction to the fact that it was a machine that was\ncalling.\n\nAI has a Distinctly Human Element\n\nBOB: Regardless of whether or not the Turing Test is a sufficient litmus test\nfor the presence of AI, it’s had a profound influence on how we define AI.\n\nGAVIN: The Turing Test has captured the attention of the masses interested in\nthe future of AI. The Turing Test has an inherent simplicity as a game of\nimitation and asked the question: can a computer fool a human?\n\nBOB: Popular science fiction elevated the Turing Test in movies, such as _Ex\nMachina_ , which was explicit in its use of the Turing Test, and _Blade\nRunner_ , where the protagonist played by Harrison Ford tests whether a woman\nis a human or “replicant.”\n\nGAVIN: Turing’s influence on computing and our perception of intelligence is\nundeniable, that’s for sure.\n\n■ **The point** Throughout the short history of AI, there has been a desire to\nmake the AI have human qualities.\n\nAs AI continues to evolve, one trap that needs to be avoided is to advance a\ndefinition of AI that juxtaposes AI _against_ a human. This might be too\nlimiting to where AI that complements human decision making would be\nundervalued. Put simply, there is more to AI than conversation. Moreover, by\nsimply defining intelligence based on Turing’s test, the development of AI\ncould lead to a belief that a program seems more intelligent than it actually\nis.\n\n### Attribution of intelligence to AI\n\nOne of the most notorious programs in computer science was written in 1966 by\nJoseph Weizenbaum, a professor at MIT. The program was called ELIZA. The name\ncomes from George Bernard Shaw’s play, _Pygmalion_ , in which Eliza Doolittle\nstudies elocution and etiquette. In this program, Weizenbaum modeled the\ninteraction similar to how a Rogerian psychotherapist might respond to a\npatient.\n\nThe person simply types responses to the questions Eliza asked.\n\nExample of Eliza Conversing With a Person\n\nELIZA: Tell me your problem.\n\nPERSON: I am depressed.\n\nELIZA: How long have you been feeling depressed?\n\nPERSON: Since my daughter left home.\n\nELIZA: Please go on.\n\nPERSON: I don’t want her to get into trouble.\n\nELIZA: What would it mean to you if she got into trouble?\n\nAnd so on…\n\n**The point** This conversation between the user and the program left an\nimpression that Eliza embodied human-like intelligence. Because the dialog\nfelt natural, the computer must be intelligent. But, did this constitute\nintelligence? Is all that is necessary for intelligence is to fool a human?\n\nELIZA was successful and people may have poured their lives into it. But, in\nall of these conversations, there was no learning algorithm to carefully\nanalyze the data. In fact, not much of what was typed was saved as this was\n1966. Some proclaimed that Weizenbaum had solved natural language through his\nprogram.\n\nWeizenbaum ended up on a crusade against his own program.7 ELIZA was more of a\nCarl Rogers parody. The program did not know psychology, just semantic logic\nto reflect questions back. But because the program _felt_ human, intelligence\nwas bestowed to it. This is an example of AI that can capture the imagination\nof the masses. And this makes AI susceptible to being overhyped.\n\n### The influence of hype\n\nWith ELIZA, the hype was brought about from the users. In other examples, hype\ncan come from its creators, investors, government, media, or market forces.\n\n### Historically, AI overpromised and under-delivered\n\nIn the past, artificial intelligence has had something of an image problem. In\n2006, _The New York Times_ ’ John Markoff called AI “a technology field that\nfor decades has overpromised and under-delivered”8 in the lead paragraph of a\nstory about an AI success.\n\nOne of the earliest attempts to develop AI was machine translation, which has\nits genesis in the post-World War II information theories of Claude Shannon\nand Norbert Weaver; there was substantial progress in code breaking as well as\ntheories about universal principles underlying language.9\n\nDefinition\n\n**Machine translation** is the translation of one language into another via a\ncomputer program.\n\nMachine translation’s paramount example is the now (in)famous Georgetown-IBM\nexperiment10 where in a public demonstration in 1954, a program developed by\nGeorgetown University and IBM researchers successfully translated many\nRussian-language sentences to English. This demonstration earned the\nexperiment major media coverage. In the heat of the Cold War, a machine that\ncould translate Russian documents into English would have been very compelling\nto American national defense interests. A number of headlines—“The bilingual\nmachine,” for example—greatly exaggerated the machine’s capabilities.11 This\ncoverage was accompanied by massive investment leading to wild predictions\nabout the future capabilities of machine translation. One professor who worked\non the experiment was quoted in the _Christian Science Monitor_ saying that\nmachine translation in “important functional areas of several areas” might be\nready in 3–5 years.12 Hype was running extremely high. The reality was far\ndifferent: the machine could only translate 250 words and 49 sentences.\n\nIndeed, the program’s focus was on translating a set of narrow scientific\nsentences in the domain of chemistry, but the press coverage focused more on a\nselect group of “less specific” examples which were included with the\nexperiment. According to linguist W. John Hutchins,13 even these few less\nspecific examples shared features in common with the scientific sentences that\nmade them easier for the system to analyze. Perhaps because of these few\ncontrary examples, the people covering the Georgetown-IBM experiment did not\ngrasp the leap in difficulty between translating a defined set of static\nsentences to translating something as complex and dynamic as policy documents\nor newspapers.\n\nThe Georgetown-IBM translator may have seemed intelligent in its initial\ntesting, but further analysis proved its limitations. For one thing, it was\nbased on a rigid rules-based system. Just six rules were used to encode the\nentire conversion from English to Russian.14 This, obviously, inadequately\ncaptures the complexity of the task of translation. Plus, language only\nloosely follows rules—for proof, look no further than the plethora of\nirregular verbs in any language.15 Not to belabor the point but the program\nwas trained on a narrow corpus and its main function was to translate\nscientific sentences, which is only an initial step towards translating\nRussian documents and communications.\n\nThis very early public test of **machine translation** featured AI that seemed\nto pass the Turing Test—but that accomplishment was deceptive.\n\nDon’t Believe that the Power of Computing can Overcome all\n\nGAVIN: The Georgetown-IBM experiment was a machine language initiative that\nstarted as a demonstration to translate certain chemistry documents. And this\nresulted in investment that spurred a decade of research in machine language.\n\nBOB: Looking back now, you could argue the logic of applying something that\nmarginally worked in the domain chemistry to be generalized to the entire\nRussian language. It seems overly simplistic, but, at the time, this chemistry\ncorpus of terms might have been the best available dataset. Over the past 70\nyears, the field of linguistics has evolved significantly, and the nuisances\nof language are now recognized to be far more complex.\n\nGAVIN: Nevertheless, the fascination that the power of computing would find\npatterns and solve mutual translation between English and Russian is an all\ntoo common theme. I suspect that the researchers were clear in the limitations\nof the work, but as with Greenspan’s now famous “irrational exuberance” quote\nthat described the hype associated with the stock market, expectations can\noften take on a life of their own.\n\n**The point** We must not believe that the power of computing can overcome\nall. What shows promise in one domain (chemistry) might not be widely\ngeneralizable to others.\n\nThe Georgetown and IBM researchers who presented the program in public may\nhave chosen to hide the flaws of their machine translator. They did so by\nlimiting the translation to the scientific sentences that the machine could\nhandle. The few selected sentences that the machine translated during the\ndemonstration were likely chosen to fit into the tightly constrained rules and\nvocabulary of the system.16\n\nThe weakness of the Turing Test as a measure of intelligence can be seen in\njournalists’ and funders’ initial, hype-building reactions to the Georgetown-\nIBM experiment’s deceptively human-like results.17 Upon witnessing a machine\nthat could seemingly translate Russian sentences with the near accuracy of a\nhuman translator, journalists18 must have thought they had seen something\nwhose capabilities significantly outstripped the reality of the program.\n\nYet these journalists were ignorant or unaware of the limited nature of the\nGeorgetown-IBM technology (and the organizers of the experiment may have\nnudged them in that direction with their choices for public display). If the\nmachine had been tested on sentences outside the few that were preselected by\nthe researchers, it wouldn’t have appeared to be so impressive. Journalists\nwrote articles that hyped the technology’s capability. But the technology\nwasn’t ready to match the hype. Nearly 60 years later, machine translation is\nstill considered imperfect at best.19\n\nThe point\n\nHype can play a large influence on whether the product is judged a success or\nfailure.\n\n### AI failures resulted in AI winters\n\nA most devastating consequence of this irrational hype was the suspension of\nfunding for AI research. As described, the hype and perceived success from the\nGeorgetown-IBM experiment resulted in massive interest and substantially\nincreased investment in machine translation research; however, that research\nsoon stagnated as the difficulty of the real challenge associated with\n**machine translation** began to sink in.20 By the late 1960s, the bloom had\ncome off the rose. Hutchins specifically tied funding cuts to the Automatic\nLanguage Processing Advisory Committee (ALPAC) report, released in 1966.21\n\nThe ALPAC report, sponsored by several US government agencies in science and\nnational security, was highly critical of machine translation, implying that\nit was less efficient and more costly than human-based translation for the\ntask of translating Russian documents.22 At best, the report said that\ncomputing could be a tool for use in human translation and linguistics\nstudies, but not as a translator itself.23 The report went on to say that\nmachine-translated text needed further editing from human translators, which\nseemed to defeat the purpose of using it in place of human translators.24 The\nconclusions of the report led to a drastic reduction in machine translation\nfunding for many years afterward.\n\nIn a key portion, the report used the Georgetown-IBM experiment as evidence\nthat **machine translation** had not improved in a decade’s worth of effort.\nThe report compared the Georgetown-IBM results directly with results from\nsubsequent Georgetown machine translators, finding that original Georgetown-\nIBM’s results had been more accurate than advanced versions. That said,\nHutchins defined the original Georgetown-IBM experiment as not an authentic\ntest of the latest machine translation technology but as a spectacle “intended\nto generate attention and funds.”25 Despite this, ALPAC judged later results\nagainst Georgetown-IBM as if it had been a true showing of AI’s capabilities.\nEven though **machine translation** may have actually improved in the late\n1950s and early 1960s, it was judged against its hype, not against its\ncapabilities.\n\nAs machine translation was one of the most important early manifestations of\nAI, this report had an impact on the field of AI in general. The ALPAC report\nand the corresponding domain-specific machine translation winter were part of\na chain reaction that eventually led to what is considered the first **AI\nwinte** **r**.26\n\nDefinition\n\nAn **AI winter** is a period when research and investment into AI stagnates\nsignificantly. During these periods, AI development gains a negative\nreputation as an intractable problem. This leads to decreased investment in AI\nresearch, which further exacerbates the problem. We identify two types of AI\nwinters: some are domain specific, where only a certain subfield of AI is\naffected, and some are general, in which the entire field of AI research is\naffected.\n\nToday, there are lots of different terms for technology that encapsulate\nAI—expert systems, machine learning, neural networks, deep learning, chatbots,\nand many more. Much of that renaming started in the 1970s, when AI became a\nbad word. After early developments in AI in the 1950s, the field was hot—not\ntoo different from right now, though on a smaller scale. But in the decade or\ntwo that followed, funding agencies (specifically US and UK governments)\nlabeled the work a failure and halted funding—the first-ever general AI\nwinter.27\n\nAI suffered greatly from this long-term lapse in funding. In order to get\naround AI’s newfound negative reputation, AI researchers had to come up with\nnew terms that specifically did not mention AI to get funding. So, following\nthe AI winter, new labels like _expert systems_ emerged.\n\nGiven the seeming promise of AI today, it may be difficult to contemplate that\nanother AI winter may be just over the horizon. While there is much effort and\ninvestment directed toward AI, progress in AI has been prone to stagnation and\npessimism in the past.\n\nIf AI does enter another winter, we believe a significant contributing factor\nwill be that AI designers and developers neglected the role UX plays in\nsuccessful design. There is another contributing factor and that is the\nvelocity of technology infusion into everyday life. In the 1950s, many homes\nhad no television or telephone. Now the demands are higher for applications;\nusers will reject applications with poor UX. As AI gets embedded into more\nconsumer applications where user expectations are higher, it is only\ninevitable that AI will need better UX.\n\nThe first AI winter followed the ALPAC report and was associated with a\ngovernmental stop to funding related to machine language efforts. This\ninvestment freeze lasted into the 1970s in the United States. Negative funding\nattention and news continued with the 1973 Lighthill Report, where Sir James\nLighthill reported to English Parliament results similar to those of ALPAC. AI\nwas directly criticized as being overhyped and not delivering on its\npromise.28\n\nAI by Any Other Name\n\nBOB: So, was it that the underlying theory and technology in the Georgetown-\nIBM experiment were flawed or was it just hype that created the failure?\n\nGAVIN: I think it was both. The ALPAC report pulled no punches and led to a\ncollapse in any research in machine translation—a domain-specific AI winter.\nHuge hype for machine translation turned out to be misplaced and the result\nwas a significant cut in funding.\n\nBOB: Yes, funding requests with the terms “machine translation” or “artificial\nintelligence” disappeared. Not unlike the old adage of “throwing the baby out\nwith the bath water,” a major failure in one domain makes the whole field look\nsuspect. That’s the danger of hype. If it doesn’t match the actual\ncapabilities of the product, it can be hard to regain the trust.\n\nGAVIN: The first general AI winter formed a pattern of initial signs of\npromise, to hype, to failure, and subsequently a freeze in future funding.\nThis cycle led to significant consequences for the field. But scientists are\nsmart; out from the ashes, AI bloomed again, but this time using new\nterminology such as _expert systems,_ which led to advancements in robotics.\n\nBOB: So, under its new names, AI garnered over $1 billion in new investment in\nthe 1980s, ushered in by private sector companies in the United States,\nBritain, and Japan.\n\nGAVIN: Actually, Japan’s advances in AI spawned US and British international\ncompetition to keep up with the Japanese. Notable examples are the European\nStrategic Program on Research and Information Technology, the Strategic\nComputing Initiative, and Microelectronics and Computer Technology Corporation\nin the United States. Unfortunately, hype emerged again, and when these\ncompanies failed to deliver on the lofty promises, the second AI winter29 was\nsaid to occur in 1993.\n\n**The point** AI has seen boom and bust cycles multiple times in its history.\n\n## Can another AI winter happen? It already did\n\nOften lessons from the past are ignored with the _hope that this time will be\ndifferent_. Will another **AI winter** happen in our lifetime is not the\nquestion because one happened before our very eyes.\n\nConsider Apple’s **voice assistant,** Siri. Siri was not launched fully\nfunctional right out of the gates. The “beta” version was introduced with a\nlot of fanfare. Soon Apple pulled it out of “beta” and released more fully\nfledged versions in subsequent updates to iOS—versions that were far more\nfunctional and usable than the original—the potential for many users to adapt\nto it was greatly reduced. However, Siri users had already formed their\nimpressions, and considering the **AI-UX principle** of **trust** , those\nimpressions were long-lasting. Not to be too cheeky, but one bad Apple\n(released too early) spoiled the barrel.\n\nHow Siri Impacted Cortana\n\nBOB: Look, to the Siri fans out there, Apple did an amazing job relative to\nprevious **voice assistants**. When working for Baby Bell companies many years\nback, we often tested **voice assistants**. Siri was a generation ahead of\nanything we had in our labs.\n\nGAVIN: And Siri was the first-ever **virtual assistant** to achieve major\nmarket penetration. In 2016, industry researcher Carolina Milanesi found that\n98% of iPhone users had given Siri at least one chance.30 This is a phenomenal\nachievement in mass use of a product.\n\nBOB: The problem though was _continued use_. When 98% were asked how much they\nused it, most replied “rarely” or “sometimes” (70%). In short, almost all\n_tried it_ , but most _stopped using it_.\n\nGAVIN: Apple hyped Siri for its ability to understand the spoken word, and\nSiri captured the attention of the masses. But after time, most users were\nsorely disappointed with hearing the response, “I’m sorry. I don’t understand\nthat,” and abandoned it after a few initial failures.\n\nBOB: To have so many try a product that is designed to be used daily (i.e.,\n“Siri, what is the weather like today?”) and practically abandon its use is\nnot simply a shame; it is a commercial loss. The effort to get a customer to\ntry something and lose them, well, you poison the well.\n\nGAVIN: Even now, if you were to play the Siri prompt (“bee boom”), a chill\ngoes up my spine because I must have accidentally pressed it. But this feeling\nof a chill negatively impacted other **voice assistants**. Ask yourself: Have\nyou ever tried Cortana (Microsoft’s voice feature on Windows OS)? Did you try\nit? Even once? And why did you not try it?\n\nBOB: No. Never gave it a try. Because to me, Cortana was just another Siri. In\nfact, I moved to Android partly because Siri was so lame.\n\nGAVIN: In speaking to Microsoft Cortana design and development teams, they\nwould vociferously argue how much different (or better) their **voice\nassistant** Cortana was from Siri. But because of the failure of **trust** ,\npeople who used Siri tended to associate the technology with Cortana.\n\nBOB: Ask if anyone has tried Bixby, Samsung’s mobile phone **voice assistant**\n, and you get blank stares.\n\n**The point** Violating an **AI-UX principle** like **trust** can be powerful\nenough to prevent users from trying similar, but competitive products. This is\narguably a domain-specific AI winter.\n\nThese negative feelings toward Siri extended to other virtual assistants that\nwere perceived to be similar to Siri. As other virtual assistants came along,\nsome users had already generalized their experiences with virtual assistants\nas a category and reached their own conclusions. The immediate impact of this\nwas to reduce the likelihood of adoption. For instance, only 22% of Windows PC\nusers ended up using Cortana.31\n\nUltimately, Cortana was likely hit even harder than Siri itself by this **AI\nwinter** because Siri was able to overcome and still exists. Cortana was\neventually repurposed as a lesser service. In 2019, Microsoft announced that,\ngoing forward, they intended to make Cortana a “skill” or “app” for users of\nvarious virtual assistants and operating systems that would allow them to\naccess information for subscribers to the Microsoft 365 productivity suite.32\nThis meant that Cortana would no longer be equivalent to Siri.\n\nUnlike Siri, Cortana was a vastly capable **virtual assistant** at its launch,\nespecially for productivity functions. Its “notebook” feature, modeled after\nthe notebooks that human personal assistants keep on their clients’\nidiosyncrasies, offered an unmatched level of personalization.33 Cortana’s\nnotebook also offered users the ability to delete some of the data that it had\ncollected on them. This privacy feature exceeded any offered by other\nassistants.34\n\nDespite these very different capabilities, users simply did not engage. Many\ncould not get past what they thought Siri represented.\n\nMoreover, **interaction** also became a problem for Siri. Speaking to your\nphone was accompanied by social stigma. In 2016, industry research by Creative\nStrategies indicated that “shame” about talking to a smartphone in public was\na prominent reason why many users did not use Siri regularly.35 The most\nstigmatized places for **voice assistant** use—public spaces—also happen to be\ncommon use cases for smartphones. Ditto for many of the common use cases for\nlaptop computers: workplace, library, and classroom. Though in our very\nunscientific observations, an increasing number of people are using the voice\nrecognition services on their phones these days.\n\nThe Emergence of Alexa\n\nBOB: Perhaps the reason we do not readily think of the impact that stemmed\nfrom the poor initial **MVP** experience with Siri is because this **AI\nwinter** lasted only a couple years not decades. This rebirth of the **virtual\nassistant** emerged as Amazon’s Alexa.\n\nGAVIN: But look what it took for the masses to try another **voice\nassistant**. Alexa embodied an entirely new form factor, something that sat\nlike a black obelisk on the kitchen counter. This changed the environment of\nuse. Where the device was placed afforded a visual cue to engage Alexa.\n\nBOB: It also allowed Amazon to bring forth Alexa with more features than Siri.\nAmazon was determined to learn from the failed experience of its Amazon Fire\nPhone. The Fire had a **voice assistant** feature, and Amazon’s Jeff Bezos did\nnot want to make Alexa’s **voice assistant** an **MVP** version. He wanted to\nthink big.\n\nGAVIN: Almost overnight, Jeff Bezos dropped $50 million and authorized\nheadcount of 200 to “build a cloud-based computer that would respond to voice\ncommands, ‘like the one in Star Trek’.”36\n\n**The point** Alexa emerged as a **voice assistant** and broke out of the **AI\nwinter** that similar products could not, but it needed an entirely different\nform factor to get users to try it. And when users tried, Jeff Bezos was\ndetermined to not have users experience an MVP version, but much bigger.\n\n## “Lick” and the origins of UX\n\nIn the early days of computing, computers were seen as a means of extending\nhuman capability by making computation faster. In fact, through the 1930s,\n“computer” was a name used for humans whose job it was to make calculations.37\nBut there were a few who saw computers and computing quite differently. The\none person who foresaw what computing was to become was J. C. R. Licklider,\nalso known as “Lick.” Lick did not start out as a computer scientist; he was\nan experimental psychologist, to be more precise, a highly regarded\npsychoacoustician, a psychologist who studies the perception of sound. Lick\nworked at MIT’s Lincoln Labs and started a program in the 1950s to introduce\nengineering students to psychology—a precursor to future **human-computer\ninteraction** (HCI) university programs.\n\nDefinition\n\n**Human-computer interaction** is an area of research dedicated to\nunderstanding of how people interact with computers and the application of\ncertain psychological principles to the design of computer systems. 38\n\nLick became head of MIT’s human factors group where he transitioned from work\nin psychoacoustics to computer science because of his strong belief that\ndigital computers would be best used in tandem with human beings to augment\nand extend each other’s capabilities.39 In his most well-known paper, Man-\nComputer Symbiosis40, Lick described a computer assistant that would answer\nquestions when asked, do simulations, display results in graphical form, and\nextrapolate solutions for new situations from past experience.41 (Sounds a\nlittle like AI, doesn’t it?) He also conceived the “Intergalactic Computer\nNetwork” in 1963—an idea that heralded the modern-day Internet.42\n\nEventually, Lick was recognized for his expertise and became the head of the\nInformation Processing Techniques Office (IPTO) of the US Department of\nDefense Advanced Research Projects Agency (ARPA). Once there, Lick fully\nembraced his new career in computer engineering. He was given a budget of over\n$10 million dollars to launch the vision he cited in _Man-Computer Symbiosis_.\nIn the intertwining of HCI and AI, Lick was the one who initially funded the\nwork of the AI and Internet pioneers Marvin Minsky, Douglas Engelbart, Allen\nNewell, Herb Simon, and John McCarthy.43 Through this funding, he spawned many\nof the computing “things” we know today (e.g., the mouse, hypertext, time-\nshared computing, windows, tablet, etc.). Who could have predicted that a\nhumble experimental psychologist, turned computer scientist, would be known as\nthe “Johnny Appleseed of the Internet”?44\n\nAI and Humans are Complementary\n\nGAVIN: Bob, you’re a huge fan of Lick.\n\nBOB: With good reason. Lick was the first person to merge principles of\npsychology into computer science. His work was foundational for computer\nscience, AI and UX. Lick pioneered an idea essential to UX that computers can\nand should be leveraged for efficient collaboration among people.\n\nGAVIN: You can certainly see that in technology. Computers have become the\nprimary place where communication and collaboration happen. I can have a\ndigital meeting with someone who’s halfway across the world and collaborate\nwith them on a project. It seems obvious to us, but it’s really a monumental\ndifference from the way the world was even just 20 years ago, let alone in\nLick’s day.\n\nBOB: We now exist in a world where computers are not just calculators but the\nprimary medium of communication among humans. That vision came from Lick and\nothers like him who saw the potential of digital technology to facilitate\ncommunication.\n\n**The point** Lick formed the basis of where AI is headed today—where AI and\nhumans are complementary.\n\nLicklider’s legacy lived on in others, particularly of note is Robert (Bob)\nTaylor. Taylor had been greatly influenced by Lick’s ideas in _Man-Computer\nSymbiosis_ and had a similar DNA to Lick’s from psychologist to\npsychoacoustician to computer scientist. Lick and Taylor met in 1962 when Lick\nwas running the IPTO at ARPA. They co-authored a paper in 1968 called “The\nComputer as a Communication Device,”45 illustrating their shared vision of\nusing computers to enhance human communication.46 They begin the paper with\nthe following two sentences:\n\n> _In a few years, men will be able to communicate more effectively through a\n> machine than face to face. That is a rather startling thing to say, but it\n> is our conclusion._\n\nLick and Taylor describe, in 1968, a future world that must have seemed very\nodd at the time. Fast forward to today where our lives are filled with video\ncalls, email, text messaging, and social media. This goes to show how\ndifferently people thought of computing back in the 1960s and how forward-\nlooking Lick and Taylor were at the time. This paper was a clear-eyed vision\nof the Internet and how we communicate today.\n\nTaylor eventually succeeded Lick as the director of IPTO. While there, he\nstarted development on a networking service that allowed users to access the\ninformation stored on remote computers.47 One of the problems he saw though\nwas that each of the groups he funded were isolated communities and were\nunable to communicate with one another. His vision to interconnect these\ncommunities gave rise to the ARPANET and eventually the Internet.\n\nAfter finishing his time at IPTO, Taylor eventually found his way to Xerox\nPARC (Palo Alto Research Center) and managed its Computer Science Lab, a\npioneering laboratory for new and developing computing technologies that would\ngo on to change the world as we know it. We’ll discuss Xerox PARC later in\nthis chapter. But, first, let’s return to the world of AI and see what was\ngoing on during this time period.\n\n## Expert systems and the second AI winter\n\nFollowing the first AI winter that was initiated by the ALPAC findings that\nconcluded unfavorable progress in machine translation, scientists eventually\nadapted and proposed research into new AI concepts. This was the rise of\n**expert systems** in the late 1970s and into the 1980s. Instead of focusing\non translation, an expert system was a type of AI that used rule-based systems\nto systematically solve problems.48\n\nDefinition\n\n**Expert systems** operate based on a set of if-then rules and draw upon a\n“knowledge base” that mimics, in some way, how experts might perform a task.\n\nAccording to Edward Feigenbaum, one of the early AI pioneers following the\nfirst AI winter, expert systems brought positive impacts of computer science\nin mathematics and statistics to other, more qualitative fields.49, 50 In the\n1980s, expert systems had a massive spike in popularity, as they entered\npopular usage in corporate settings. Though expert systems are still used for\nbusiness applications and emerge as concepts like _clinical decision making_\nfor electronic health record systems (EHR) ,51 their popularity fell\ndramatically in the late 1980s and early 1990s, as an AI winter hit.52\n\nFeigenbaum outlined two components of an expert system: the “knowledge base,”\na set of if-then rules which includes expert-level formal and informal\nknowledge in a particular field, and the “inference engine,” a system for\nweighting the information from the knowledge base in order to apply it to\nparticular situations.53 While many expert systems benefit from machine\nlearning, meaning they can adjust their rules without programmer input, even\nthese adaptable expert systems are generally reliant on the knowledge entered\ninto them, at least as a starting point.\n\nThis dependence on programmed rules poses problems when expert systems are\napplied to highly specific fields of inquiry. Feigenbaum identified such a\nproblem54 in 1980, citing a “bottleneck” in “knowledge acquisition” that\nresulted from the difficulty in programming expert knowledge into a computer.\nSince machine learning was unable to directly translate expert knowledge texts\ninto its knowledge base and since experts in many fields did not have the\ncomputer science knowledge necessary to program the expert system themselves,\nprogrammers acted as an intermediary between experts and AI. If programmers\nmisinterpreted or misrepresented expert knowledge, the resulting\nmisinformation would become part of the expert system. This was particularly\nproblematic in cases where the experts’ knowledge was of the unstated sort\nthat comes with extensive experience within a field. If the expert could not\nproperly express this unstated knowledge, it would be difficult to program it\ninto the expert system. In fact, psychologists tried to get at this problem of\n“knowledge elicitation” from experts in order to support the development of\nexpert systems.55 Getting people (particularly experts) to talk about what\nthey know and express that knowledge in rules-based format suitable for\nmachines turns out to be a gnarly problem.\n\nThese limitations of the expert system architecture were part of the problem\nthat eventually put them into decline. The failure of expert systems led to a\nyears-long period in which the development of AI in general was stagnant. We\ncannot say exactly why expert systems stalled in the 1980s, although\nirrationally high expectations for a limited form of AI certainly played a\nrole. But it is likely that the perceived failures of expert systems\nnegatively impacted other areas of AI.\n\nAI Began to Embrace Complexity\n\nGAVIN: Just think about what it took to build “rule-based” systems. You needed\ncomputer scientists who programmed the “brain,” but you also needed to enter\n“information” to essentially embed domain knowledge into the system as data.\n\nBOB: When the objective was **machine translation** , the elements were words\nand sentences. But when you are building **expert systems** like autonomous\nrobotics, this effort adds a physical dimension, like one that would perform\non an automated assembly line.\n\nGAVIN: The sheer amount of knowledge at play makes for a complicated world,\none that had some programmers coding. Others worked to take knowledge and\ncreate training datasets. Others worked on computer vision. Still others\nworked on robotic functions to enable the mechanical degrees of freedom to\ncomplete physical actions. The need to have machines learn on their own has\nnecessitated our current definitions of artificial intelligence. There was\nsimply too much work to be done.\n\nBOB: AI winters have come and gone—but they were hardly the “Dark Ages.” The\nscience advanced. As technology advanced, challenges only became greater.\nWhether changing its name or its focus, many pushed through AI failures to get\nus to where we are today.\n\n**The point**\n\nAI winters stifled funding, but the challenge of AI captured the attention of\ngreat minds who wanted to advance technology and science.\n\nOf course, failure provides the lessons that we carry forward when we pick\nourselves up, dust ourselves off, and move on. Failure helps us be better\nprepared for the next time we are at a crossroads. We think that AI is at such\na crossroads now and that lessons learned from the failure that led to the\nexpert system AI winter can help us get through it. AI scholar Roger Schank,56\na contemporary of Feigenbaum and other expert systems proponents, outlined his\nopinion on the shortcomings of expert systems in 1991. Schank believes that\nexpert systems, especially after encouragement from venture capitalists, were\ndone in by an overemphasized focus on their inference engines.57\n\nSchank describes venture capitalists seeing dollar signs and encouraging the\ndevelopment of an inference machine “shell”—a sort of build-your-own-expert-\nsystem machine. They could sell this general engine to various types of\ncompanies who would program it with their specific expertise. The problem with\nthis approach, for Schank, is that the inference engine is not really doing\nmuch of the work in an expert system.58 All it does, he says, is choose an\noutput based on values already represented in the knowledge base. Just like\nmachine translation, the inference engine developed hype that was\nincommensurate with its actual capabilities.\n\nThese inference machine “shells” lost the intelligence found in the\nprogrammers’ learning process. Programmers were constantly learning about the\nexpert knowledge in a particular field and then adding that knowledge into the\nknowledge base.59 Since there is no such thing as expertise without a specific\ndomain on which to work, Schank argues that the shells that venture\ncapitalists attempted to create were not AI at all—that is, the AI is in the\nknowledge base, not the rules engine.\n\nThe point\n\nFailure can be devastating, but can teach us valuable lessons.\n\n## Xerox PARC and trusting human-centered insights\n\nThe history of Xerox’s Palo Alto Research Center (PARC) is remarkable that a\ncompany known for its copiers gave us some of the greatest innovations of all\ntime. In the last decades of the 20th century, Xerox PARC was the premier tech\nresearch facility in the world. Here are just some of the important\ninnovations in which Xerox PARC played a major role: personal computer,\ngraphical user interface (GUI), laser printer, computer mouse, object-oriented\nprogramming (Smalltalk), and Ethernet.60 The GUI and the mouse made computing\nmuch easier for most people to understand, allowing them to use the computer’s\ncapabilities without having to learn complex commands. The design of the early\nsystems was made easier by applying psychological principles to how\ncomputers—both software and hardware—were designed.\n\nTech’s Greatest Accomplishments had Psychological Roots\n\nBOB: Bob Taylor, who was head of the Computer Sciences Division at Xerox PARC,\nrecruited the brightest minds from his ARPA network and other Bay Area\ninstitutions, such as Douglas Engelbart’s Augmentation Research Center. These\nscientists introduced the concepts of the computer mouse, windows-based\ninterfaces, and networking.\n\nGAVIN: Xerox PARC was one of those places that had a center of excellence\n(_COE_) that attracted the world’s brightest. This wasn’t like the COEs we see\ntoday that are used as a business strategy. Instead, PARC was recognized like\nNiels Bohr’s institute at Copenhagen when it was the world center for quantum\nphysics in the 1920s or the way postwar Greenwich Village drew artists\ninspired by Abstract Expressionism or how Motown Records attracted the most\ncreative writers and musicians in soul music.61 It was vibrant!\n\nBOB: PARC was indeed an institute of knowledge. Despite building such a\nremarkable combination of talent and innovative new ideas, the sustainability\nof such an institution can still be transitory. By the 1980s, the diffusion of\nXerox PARC scientists began. But much of where technology stands today is\nbecause of Xerox PARC’s gathering and the eventual dispersion that allowed\nadvancement to move from invention and research to commercialization.\n\n**The point** Xerox PARC is where human-computer interaction made progress in\ntechnology with roots in psychology.\n\nEric Schmidt, former chairman of Google and later Alphabet, said—perhaps with\na bit of exaggeration—that “Bob Taylor invented almost everything in one form\nor another that we use today in the office and at home.” Taylor led Xerox PARC\nduring its formative period. For Taylor, collaboration was critical to the\nsuccess of his products. Taylor and the rest of his team at PARC garnered\ninsights through group creativity, and Taylor often emphasized the group\ncomponent of his teams’ work at Xerox PARC.62\n\nWhile Lick and Taylor poured the foundation, a group of scientists built on\nthat, recognizing that there was an applied psychology to humans interacting\nwith computers. A “user-centered” framework began to emerge at Stanford and\nPARC; this framework was eventually articulated in the 1983 book _The\nPsychology of Human-Computer Interaction_63 by Stuart Card, Thomas Moran, and\nAllen Newell. Though the book predates the widespread presence of personal\ncomputing—let alone the Internet—it tightly described human behavior in the\ncontext of interacting with a computer system.\n\nThe expression of core concepts such as showed that the computer as an\ninterlocutor was now in play and restates Lick’s vision of 1960.\n\n> _The user is not an operator. He does not operate the computer; he\n> communicates with it to accomplish a task. Thus, we are creating a new arena\n> of human action: communication with machines rather than operation of\n> machines._(Emphasis theirs)64\n\n_The Psychology of Human-Computer Interaction_ argued that psychological\nprinciples should be used in the design phase of computer software and\nhardware in order to make them more compatible with the skills, knowledge,\ncapabilities, and biases of their users.65 While computers are, ultimately,\ntools for humans to use, they also need to be designed in a way that would\nenable users to effectively work with them. In short, the fundamental idea\nthat we have to understand how people are wired and then adapt the machine\n(i.e., the computer) to better fit the user arose from Card, Moran, and\nNewell.\n\nAlan Newell also had a hand in some of the earliest AI systems in existence;\nhe saw the computer as a digital representation of human problem-solving\nprocesses.66 Newell’s principal interest was in determining the structure of\nthe human mind, and he felt that structure was best modeled by computer\nsystems. By building computers with complex hardware and software\narchitectures, Newell intended to create an overarching theory of the function\nof the human brain.\n\nNewell’s contributions to computer science were a byproduct of his goal of\nmodeling human cognition. Nevertheless, he is one of the most important\nprogenitors of AI, and he based his developments on psychological principles.\n\nPsychology and Computing Should go Hand in Hand\n\nBOB: There’s an important dialog between psychologists who were trying to\nmodel the mind and brain and computer scientists who were trying to get\ncomputers to think.\n\nGAVIN: Sometimes, it seems like the same people were doing both.\n\nBOB: Right—the line between computer scientists and cognitive psychologists\nwas blurred. But you had people like Newell and others who saw the complex\narchitecture of computers as a way to understand the cognitive architecture of\nthe brain.\n\nGAVIN: This is a dance. On one hand, you have computer scientists building\ncomplex programs and hardware systems to mimic the brain, and on the other\nhand, you have psychologists who are trying to argue how to integrate a human\ninto the system.\n\nA simple example is the old “green screen” cathode-ray tube (_CRT_ ) monitor,\nwhere characters lit the screen up in green. One anecdotal story had the\nhardware technologists pulling their hair out because the psychology\nresearchers argued that the move from all caps font to mixed-case font would\nbe better from a human performance perspective if the screen had black\ncharacters on a white background. This was a debate because the hardware\ntechnology required to make it easier for the human is vastly different from a\nCRT. Even with this story, you can imagine how having computer scientists and\npsychologists in the same room advanced the field.\n\nBOB: It’s really the basis of where we’re at today. Even though computers and\nbrains don’t work the same way, the work of people like Allan Newell created\ninsights on both sides. Especially on the computing side, conceptualizing a\ncomputer as being fundamentally like a brain helped make a lot of gains in\ncomputing.\n\nGAVIN: Psychology and computer science can work hand in hand.\n\nBOB: Ideally, they would. But it doesn’t always happen that way. For instance,\nin today’s world, most companies are hiring computer scientists to do natural\nlanguage processing and eschewing linguists or psycholinguists. Language is\nmore than a math problem.\n\n**The point** Psychology and computing should go hand in hand. In the past,\ncomputer scientists with a psychology background generated new, creative\ninsights.\n\n## Bouncing back from failure\n\nThe AI winter(s) can be understood through a “hype curve” for AI laid out by\nAI researcher Tim Menzies.67 Menzies says that AI, like other technologies,\nhas reached a “peak of inflated expectations” early in its career (in the\nmid-1980s). This was the result of a quick rise to prominence and\noveroptimism. Once those who had believed AI’s hype discovered that it still\nhad a long way to go, this was followed by a “trough of disillusionment” (the\nAI winter); see Figure 2-1. However, this trough did not last all that long.\nBy 2003, when Menzies was writing, he felt AI had made a slow rise toward a\nlevel of success above the trough but below the peak, or a “plateau of\nprofitability.”\n\n![../images/470825_1_En_2_Chapter/470825_1_En_2_Fig1_HTML.jpg](../images/470825_1_En_2_Chapter/470825_1_En_2_Fig1_HTML.jpg)\n\nFigure 2-1\n\nThe hype cycle for new technology\n\nAI’s gradual climb back to solvency after the AI winter of the late 1980s is a\nstory of rebirth that can also provide lessons for us. One of the key\ncomponents of this rebirth was a type of AI called neural networks that we\nalluded to earlier. Neural networks actually date back to at least the\n1950s,68 but they became popular in the 1990s, in the wake of the AI winter as\na means to continue AI research under a different name.69 They included an\nemphasis on and newfound focus on a property of intelligence that Schank\nemphasized in 1991. Schank argued that “intelligence entails learning,”70\nimplying that true AI needs to be able to learn in order to be intelligent.\nWhile expert systems had many valuable capabilities, they were rarely capable\nof machine learning. **Artificial neural networks** offered more room for\nmachine learning capabilities.\n\nDefinition\n\n**Artificial neural networks** , sometimes simply called neural networks, is a\ntype of AI system that is loosely based on the architecture of the brain, with\nsignals sent between artificial neurons in the system. The system features\nlayers of nodes that receive information, and based on a calculated weighted\nthreshold, information is passed on to the next layer of nodes and so forth.71\n\n**Neural networks** come broadly in two types: supervised and unsupervised.\nSupervised neural networks are trained on a relevant dataset for which the\nresearchers have already identified correct conclusions. If they are asked to\ngroup data, they will do so based on the criteria they have learned from the\ndata on which they were trained. Unsupervised neural networks are given no\nguidance about how to group data or what correct groupings might look like. If\nasked to group data, they must generate groupings on their own.\n\nNeural networks also had significant grounding in psychological principles.\nThe work of David Rumelhart exemplifies this relationship. Rumelhart, who\nworked closely with UX pioneer Don Norman (among many others), was a\nmathematical psychologist whose work as a professor at the University of\nCalifornia-San Diego was similar to Newell’s. Rumelhart focused on modeling\nhuman cognition in a computer architecture, and his work was important to the\nadvancement of neural networks—specifically back propagation which enabled\nmachines to “learn” if exposed to many (i.e., thousands) of instances and non-\ninstances of a stimulus and response.72\n\nFeigenbaum said that “the AI field…tends to reward individuals for reinventing\nand renaming concepts and methods which are well explored.”73 Neural networks\nare certainly intended to solve the same sorts of problems as expert systems:\nthey are targeted at applying the capabilities of computer technologies to\nqualitative problems, rather than the usual quantitative problems. (Humans are\nvery good at qualitative reasoning; computers are not good at all in this\nspace.) Supervised neural networks in particular could be accused of being a\nrenamed version of expert systems, since the training data they rely on could\nbe conceived of as a knowledge base and the neuron-inspired architecture as an\ninference engine.\n\nThere is some truth to the concept that AI’s comeback is due to its new name;\nthis helped its re-adoption in the marketplace. After all, once a user\n(individual, corporate, or government) decides that “expert systems,” for\nexample, do not work for them, it is unlikely that they’ll want to try\nanything called “expert systems” for a long time afterward. If we want to\nreintroduce AI back into their vocabularies, we must have some way of\nindicating to these users that a technology is different enough from its\npredecessor to be worth giving another chance. The rise of a new subcategory\nof AI with a new name seems to have been enough to do that.\n\nHowever, slapping a new name on a similar technology is not enough to regain\nusers’ trust on its own. The technology must be different enough from its\npredecessor for the renaming to seem apt. Neural networks were not simply a\nrenamed, slightly altered version of expert systems. They have radical\ndifferences in both their architecture and their capabilities, especially for\nthose neural networks which allow a neural network to adjust the weights of\nits artificial neurons according to the effectiveness of those neurons in\nproducing an accurate result (i.e., “back propagation”). This is just the sort\nof learning Schank believes is essential to AI.\n\n## Norman and the rise of UX\n\nAs AI morphed, so did user experience. The timing of the rise of neural\nnetworks (in the early 1990s) roughly coincides with Don Norman’s coining of\nthe term “user experience” in 1993. Where HCI was originally focused heavily\non the psychology of cognitive, motor, and perceptual functions, UX is defined\nat a higher level—the experiences that people have with things in their world,\nnot just computers. HCI seemed too confining for a domain that now included\ntoasters and door handles. Moreover, Norman, among others, championed the role\nof beauty and emotion and their impact on the user experience. Socio-technical\nfactors also play a big part. So UX casts a broader net over people’s\ninteractions with stuff. That’s not to say that HCI is/was irrelevant; it was\njust too limiting for the ways in which we experience our world.\n\nBut where is this leading? As of today, UX continues to grow because\ntechnologies build on each other and the world is getting increasingly\ncomplex.74 We come into contact daily with things we have no mental model for,\ninterfaces that present unique features, and experiences that are richer and\ndeeper than they’ve ever been. These new products and services take advantage\nof new technology, but how do people learn to interact with things that are\nnew to the world? These new interactions with new interfaces can be\nchallenging for adoption.\n\nMore and more those interfaces contain AI algorithms. A child growing up a\ndecade from now may find it archaic to have to type into a computer when they\nlearn from the very beginning that an AI-natural-language-understanding Alexa\ncan handle so many (albeit mundane) requests. We may not recognize when our\nuser experience is managed by an AI algorithm. Whenever a designer/developer\nsurfaces the user interface to an AI system, there is an experience to\nevaluate. The perceived goodness of that interface (the UX) may determine the\nsuccess of that application.\n\nThe point\n\nAs UX evolves from HCI, it becomes more relevant to AI.\n\n## Ensuring success for AI-embedded products\n\nFor those who code, design, manage, market, maintain, fund, or simply have\ninterest in AI, having an understanding about the evolution of the field and\nthe somewhat parallel path of UX is necessary to explore. Before moving on to\nChapter [3](470825_1_En_3_Chapter.xhtml), which will address some of today’s\nverticals of AI investment, a pause to register an observation is in order:\nthere is a distinct possibility that another AI winter is on the horizon.\n\nThe hype is certainly here. There is a tremendous amount of money going into\nAI. Commercials touting the impressive feats of some new application come at\nus daily. Whole colleges are devoting resources, faculty, students, and even\nbuildings toward AI. But as described earlier, hype can often be followed by a\ntrough.\n\nIn many ways, we need to return to Licklider’s perspective that there exists a\nsymbiosis between the human and the machine. Each has something to contribute.\nEach will be more successful if there is an understanding about who does what,\nwhen, and how. For AI to succeed, to avoid another winter, it needs good UX.\n\nAI is at an important crossroads. In order to understand how AI can achieve\nsuccess, we must set one key assumption: Let’s assume the underlying code—the\nalgorithms—work. That it is functional and purposeful. Let’s assume also that\nAI can deliver on the promise and opportunity. The challenge is whether\nsuccess will follow. Know that success does not simply happen; it needs to be\ncarefully developed and properly placed. Google was not the first search\nengine. Facebook was not the first social media network. There are many\nfactors that make one product a success and another a historical footnote.\n\nThe missing element is not the speed of AI or even the possible uncovering of\npatterns previously unknown. It will be on whether the product you build can\ntake advantage of the insight or idea and be successful. The key position put\nforth is that AI is here, but much around it needs to be shaped, developed,\nand made more usable. The moment is here where the two fields developing in\nparallel should converge.\n\nThe Confluence of AI and UX\n\nBOB: At what point does our story of the computer scientist and psychologist\nmerge? The term HCI itself embodies not just computers but the interaction\nwith humans.\n\nGAVIN: Again, the dance follows the AI timeline. From the beginning, there was\nconvergence on the future of computing and how it may integrate with humanity.\nNeural networks brought ideas that computer networks may mimic the brain.\nProgrammers build AI systems and cognitive psychologists focused on making\ntechnology work for people.\n\nBOB: The time is now. We started our careers over 25 years ago pleading for a\nbudget to make the technology “user friendly” or “usable.” Today, the value of\na good product experience is not just nice to have, but clearly linked to the\nbrand experience and tied to company value.\n\nGAVIN: People speak about the Apple brand with high reverence. Much time and\neffort went into the design of not only the product (e.g., iMac, iPod or\niPhone), but the design of the brand experience. The Apple brand almost\ntranscends a singular product. Perhaps out of necessity, businesses recognize\nthe value of the brand experience—as AI embeds itself into new products, good\ndesign matters. Focus on the experience is where the emphasis needs to be. No\none cares about the AI algorithm or if an unsupervised neural net was used.\nPeople care about good experiences. Or more aptly, people pay for good\nexperiences.\n\n**The point** HCI’s time is now. AI technology is at a point where the\ndifferentiator for success is the user experience.\n\n## Conclusion: Where we’re going\n\nIn the next chapter, we will explore AI at the 30,000 foot level. This will\ndescribe core elements of how AI works so we can explore areas where we can\napply UX to improve AI outcomes.\n\nIt is important to note that the emphasis will not be detailed technical\ninformation; in fact, let’s assume the AI algorithms at the heart of it all\nwork just fine. What can we do as product managers, marketers, researchers, UX\npractitioners, or even technophiles to understand the potential gaps and\nopportunities for nonprogrammers and non-data scientists to impact product\nsuccess? The next chapter is about all of the areas where AI is emerging and\nto identify where a better experience would make a difference.\n\nFootnotes\n\n1\n\nMifsud, Courtney (2017). “A brief history of artificial intelligence.”\nArtificial Intelligence: The Future of Humankind. Time Inc. Books. Pp 20–21.\n\n2\n\nSearle, John (1980). “Minds, Brains and Programs,” The Behavioral and Brain\nSciences. 3, pp. 417–424.\n\n3\n\nGünther, Marios (2012). “Could a machine think? Alan M. Turing vs. John R.\nSearle.” Universite Paris IV Unite de Formation et de Philosphie et\nSociologie. January 2012. Accessed June 16, 2020.\n[https://philarchive.org/archive/MARCAM-4](https://philarchive.org/archive/MARCAM-4).\n\n4\n\nGergshorn, Dave. “Elon Musk and Mark Zuckerberg can’t agree on what AI is,\nbecause no one knows what the term really means.” Quartz. Last updated March\n30, 2017. Accessed June, 16, 2020. [https://qz.com/945102/elon-musk-and-mark-\nzuckerberg-cant-agree-on-what-ai-is-because-nobody-knows-what-the-term-really-\nmeans/](https://qz.com/945102/elon-musk-and-mark-zuckerberg-cant-agree-on-\nwhat-ai-is-because-nobody-knows-what-the-term-really-means/).\n\n5\n\nOppermann, Artem. “Did Google Duplex beat the Turing Test? Yes and No.”\nTowardsDataScience.com. May 20, 2018. Accessed June 16, 2020.\n[https://towardsdatascience.com/did-google-duplex-beat-the-turing-test-yes-\nand-no-a2b87d1c9f58](https://towardsdatascience.com/did-google-duplex-beat-\nthe-turing-test-yes-and-no-a2b87d1c9f58). Accessed June 16, 2020.\n\n6\n\nLeviathan, Yaniv & Matias, Yossi (2018). “Google Duplex: An AI system for\naccomplishing real-world tasks over the phone.” May 8, 2018. Accessed June 16,\n2020. [https://ai.googleblog.com/2018/05/duplex-ai-system-for-natural-\nconversation.html](https://ai.googleblog.com/2018/05/duplex-ai-system-for-\nnatural-conversation.html). May 8, 2018. Accessed June 16, 2020.\n\n7\n\nCampbell-Kelly, Martin (2008). “Professor Joseph Weizenbaum: Creator of the\n‘Eliza’ program.” The Independent. Independent News and Media. March 18, 2008.\nAccessed June 16, 2020. [www.independent.co.uk/news/obituaries/professor-\njoseph-weizenbaum-creator-of-the-eliza-\nprogram-797162.html.](http://www.independent.co.uk/news/obituaries/professor-\njoseph-weizenbaum-creator-of-the-eliza-program-797162.html)\n\n8\n\nMarkoff, John. “Behind Artificial Intelligence, a Squadron of Bright Real\nPeople.” New York Times. October 14, 2005. Accessed June 16, 2020.\n[www.nytimes.com/2005/10/14/technology/behind-artificial-intelligence-a-\nsquadron-of-bright-real-\npeople.html.](http://www.nytimes.com/2005/10/14/technology/behind-artificial-\nintelligence-a-squadron-of-bright-real-people.html)\n\n9\n\nHutchins, W. John. “The history of machine translation in a nutshell.” Last\nupdated November 2005. Accessed June 16, 2020.\n[www.hutchinsweb.me.uk/Nutshell-2005.pdf](http://www.hutchinsweb.me.uk/Nutshell-2005.pdf).\n\n10\n\nHutchins, W. John. “The Georgetown-IBM Experiment Demonstrated in January\n1954.” In _Conference of the Association for Machine Translation in the\nAmericas_ , pp. 102–114. Springer, Berlin, Heidelberg, 2004.\n\n11\n\nHutchins, “Georgetown,” 103.\n\n12\n\nHutchins, “Georgetown,” 104.\n\n13\n\nHutchins, “Georgetown,” 112.\n\n14\n\nHutchins, “Georgetown,” 106–107.\n\n15\n\nFor instance, the verb “to walk” follows a regular structure: I walk, you\nwalk, she walks, we walk, and they walk. The verb “to be” however is highly\nirregular: I am, you are, he is, we are, and they are. In many languages,\nhigh-frequency verbs are irregular in their construction and usage.\n\n16\n\nHutchins, “Georgetown,” 110.\n\n17\n\nSome tens of millions of dollars were provided by funding agencies.\n\n18\n\nHutchins, “Georgetown,” 103.\n\n19\n\n“Will Machines Ever Master Translation?” IEEE Spectrum. Last updated January\n15, 2013. Accessed June 16, 2020.\n[https://spectrum.ieee.org/podcast/robotics/artificial-intelligence/will-\nmachines-ever-master-\ntranslation](https://spectrum.ieee.org/podcast/robotics/artificial-\nintelligence/will-machines-ever-master-translation).\n\n20\n\nHutchins, “Georgetown,” 113.\n\n21\n\nHutchins, John. “ALPAC: the (in)famous report.” Originally published in _MT\nNews International_ 14 (1996). Accessed June 16, 2020.\n[www.hutchinsweb.me.uk/ALPAC-1996.pdf](http://www.hutchinsweb.me.uk/ALPAC-1996.pdf).\n\n22\n\nHutchins, “ALPAC,” 2, 6.\n\n23\n\nHutchins, “ALPAC,” 6.\n\n24\n\nHutchins, “ALPAC,” 3.\n\n25\n\nHutchins, 113.\n\n26\n\nBostrom, Nick. _Superintelligence: Paths, Dangers, Strategies_. New York:\nOxford University Press, 2014. 8.\n\n27\n\nSchmelzer, Ron (2018). “Are we heading into another AI winter?” Medium. Posted\nJune 26, 2018. Accessed September 4, 2019.\n[https://medium.com/cognilytica/are-we-heading-to-another-ai-\nwinter-e4e30acb60b2](https://medium.com/cognilytica/are-we-heading-to-another-\nai-winter-e4e30acb60b2).\n\n28\n\nHendler, James. “Avoiding Another AI Winter.” IEEE Intelligent Systems, 2008.\nAccessed May 15, 2019.\n[www.researchgate.net/publication/3454567_Avoiding_Another_AI_Winter](http://www.researchgate.net/publication/3454567_Avoiding_Another_AI_Winter).\n\n29\n\nHubbs, Christian (2019). “The dangers of government funded artificial\nintelligence.” Mises Institute. [https://mises.org/wire/dangers-government-\nfunded-artificial-intelligence](https://mises.org/wire/dangers-government-\nfunded-artificial-intelligence). Posted March 30, 2019. Accessed August 26,\n2019.\n\n30\n\nMilanesi, Carolina. “Voice Assistant Anyone? Yes please, but not in public!”\nCreative Strategies. Last updated June 16, 2020. Accessed June 26, 2019.\n[https://creativestrategies.com/voice-assistant-anyone-yes-please-but-not-in-\npublic/](https://creativestrategies.com/voice-assistant-anyone-yes-please-but-\nnot-in-public/).\n\n31\n\nBacchus, Arif. “In the age of Alexa and Siri, Cortana’s halo has gone dim.”\nDigital Trends. Last updated February 16, 2019. Accessed June 16, 2020.\n[www.digitaltrends.com/computing/cortana-is-\ndead/](http://www.digitaltrends.com/computing/cortana-is-dead/).\n\n32\n\nWarren, Tom. “Microsoft No Longer Sees Cortana as an Alexa or Google Assistant\nCompetitor” The Verge. January 18, 2019. Accessed June 16, 2020.\n[www.theverge.com/2019/1/18/18187992/microsoft-cortana-satya-nadella-alexa-\ngoogle-assistant-\ncompetitor](http://www.theverge.com/2019/1/18/18187992/microsoft-cortana-\nsatya-nadella-alexa-google-assistant-competitor).\n\n33\n\nBeres, Damon. “Microsoft’s Cortana Is Like Siri With A Human Personality.”\nHuffPost. June 29, 2015. Accessed June 16, 2020.\n[www.huffpost.com/entry/microsofts-cortana-is-like-siri-with-a-human-\npersonality_n_55b7be94e4b0a13f9d1a685a](http://www.huffpost.com/entry/microsofts-\ncortana-is-like-siri-with-a-human-personality_n_55b7be94e4b0a13f9d1a685a).\n\n34\n\nHachman, Mark. “Microsoft’s Cortana guards user privacy with ‘Notebook’.” PC\nWorld. Last updated February 21, 2014. Accessed June 16, 2020.\n[www.pcworld.com/article/2099943/microsofts-cortana-digital-assistant-guards-\nuser-privacy-with-\nnotebook.html](http://www.pcworld.com/article/2099943/microsofts-cortana-\ndigital-assistant-guards-user-privacy-with-notebook.html).\n\n35\n\nReisinger, Don. “You’re embarrassed to use Siri in public, aren’t you?”\nFortune. Last updated June 6, 2016. Accessed June 16, 2020.\n[http://fortune.com/2016/06/06/siri-use-public-\napple/](http://fortune.com/2016/06/06/siri-use-public-apple/).\n\n36\n\nBariso, Justin (2019). “Jeff Bezos Gave an Amazon Employee Extraordinary\nAdvice After His Epic Fail. It’s a Lesson in Emotional Intelligence. The story\nof how Amazon turned a spectacular failure into something brilliant.” Inc.\nDecember 9, 2019. Accessed June 16, 2020. [www.inc.com/justin-bariso/jeff-\nbezos-gave-an-amazon-employee-extraordinary-advice-after-his-epic-fail-its-a-\nlesson-in-emotional-intelligence.html](http://www.inc.com/justin-bariso/jeff-\nbezos-gave-an-amazon-employee-extraordinary-advice-after-his-epic-fail-its-a-\nlesson-in-emotional-intelligence.html).\n\n37\n\nMontecino, Virginia. “History of Computing.” George Mason University. Last\nupdated November 2010. Accessed June 16, 2020.\n[https://mason.gmu.edu/~montecin/computer-hist-\nweb.htm](https://mason.gmu.edu/%257Emontecin/computer-hist-web.htm).\n\n38\n\nCarroll, John M. & Kjeldskov, J. (2013). “The encyclopedia of human-computer\ninteraction. 2nd Edition.” Interaction Design Foundation. [www.interaction-\ndesign.org/literature/book/the-encyclopedia-of-human-computer-interaction-2nd-\ned/human-computer-interaction-brief-intro](http://www.interaction-\ndesign.org/literature/book/the-encyclopedia-of-human-computer-interaction-2nd-\ned/human-computer-interaction-brief-intro). Accessed August 26, 2019.\n\n39\n\nHafner, Katie and Lyon, Matthew. Where Wizards Stay Up Late: The Origins of\nthe Internet, 10–13, 28–47. New York: Simon & Schuster (1996).\n\n40\n\nLicklider, J. C. R., [“Man-Computer\nSymbiosis,”](http://medg.lcs.mit.edu/people/psz/Licklider.html) _IRE\nTransactions on Human Factors in Electronics_ , vol. HFE-1, 4–11, March 1960.\n\n41\n\n“Joseph Licklider.” [https://history-\ncomputer.com/Internet/Birth/Licklider.html](https://history-\ncomputer.com/Internet/Birth/Licklider.html). Retrieved July 30, 2019.\n\n42\n\nLicklider, J. C. R. (23 April 1963). “[Topics for Discussion at the\nForthcoming Meeting, Memorandum For:​ Members and Affiliates of the\nIntergalactic Computer Network.​”](http://www.kurzweilai.net/memorandum-for-\nmembers-and-affiliates-of-the-intergalactic-computer-network) Washington,\nD.C.: Advanced Research Projects Agency, via KurzweilAI.net. Retrieved August\n18, 2019.\n\n43\n\n“Joseph Licklider,” History-Computer, [https://history-\ncomputer.com/Internet/Birth/Licklider.html](https://history-\ncomputer.com/Internet/Birth/Licklider.html). Accessed July 30, 2019.\n\n44\n\nWaldrop, M. Mitchell (2001). _The Dream Machine: J. C. R. Licklider and the\nRevolution That Made Computing Personal_. New York: Viking Penguin. p. 470.\n\n45\n\n[J.​ C.​ R.​ Licklider](https://en.wikipedia.org/wiki/J._C._R._Licklider);\nRobert Taylor (April 1968). [“The Computer as a Communication\nDevice.​”](http://www.kurzweilai.net/the-computer-as-a-communication-device)\n_Science and Technology_.\n\n46\n\n“Robert Taylor.” Internet Hall of Fame. Accessed July 9, 2019.\n[www.internethalloffame.org/inductees/robert-\ntaylor](http://www.internethalloffame.org/inductees/robert-taylor).\n\n47\n\nHafner, Lyon, Where Wizards Stay Up Late.\n\n48\n\nBostrom, _Superintelligence_ , 9.\n\n49\n\nFeigenbaum, Edward A. “Knowledge Engineering: The Applied Side of Artificial\nIntelligence.” No. STAN-CS-80-812. Stanford Heuristics Programming Project,\n1980. Accessed May 20, 2019.\n\n50\n\nFeigenbaum. “Knowledge Engineering.” 9.0.\n\n51\n\nHow Can Artificial Intelligence (AI) Improve Clinician EHR Use? Jason,\nChristopher. December 2, 2019. [https://ehrintelligence.com/news/how-can-\nartificial-intelligence-ai-improve-clinician-ehr-\nuse](https://ehrintelligence.com/news/how-can-artificial-intelligence-ai-\nimprove-clinician-ehr-use). Accessed May 22, 2020).\n\n52\n\nBostrom, _Superintelligence_ , 9.\n\n53\n\nFeigenbaum. “Knowledge Engineering,” 1.2.\n\n54\n\nFeigenbaum. “Knowledge Engineering,” 10.4.\n\n55\n\nHoffman, RR (Ed.). (1992). The psychology of expertise: Cognitive research and\nempirical AI. New York, NY, US: Springer-Verlag Publishing.\n\n56\n\nSchank, Roger C. “Where’s the AI?” AI Magazine 12/4 (1991): 38–49. Accessed\nMay 21, 2019.\n[www.aaai.org/ojs/index.php/aimagazine/article/view/917/835](http://www.aaai.org/ojs/index.php/aimagazine/article/view/917/835).\n\n57\n\nSchank, 40.\n\n58\n\nSchank, 45.\n\n59\n\nSchank, 45.\n\n60\n\nDennis, “Xerox PARC.”\n\n61\n\nKim-Pang, Alex Soojung (2000). “The Xerox PARC visit.” Making the Macintosh:\nTechnology and Culture in Silicon Valley. Created: July 14, 2000. Accessed\nAugust 28, 2019.\n[https://web.stanford.edu/dept/SUL/sites/mac/parc.html](https://web.stanford.edu/dept/SUL/sites/mac/parc.html).\n\n62\n\nBerlin, Leslie. “You’ve Never Heard of Tech Legend Bob Taylor, but he Invented\nAlmost Everything.” Wired. Last updated April 21, 2017. Accessed June 7, 2019.\n[www.wired.com/2017/04/youve-never-heard-tech-legend-bob-taylor-invented-\nalmost-everything/](http://www.wired.com/2017/04/youve-never-heard-tech-\nlegend-bob-taylor-invented-almost-everything/).\n\n63\n\nCard, Stuart K., Moran, Thomas P., and Newell, Allen. _The Psychology of\nHuman-Computer Interaction_ vii–xi, 1–19. Hillsdale, NJ: Lawrence Erlbaum\nAssociates (1983).\n\n64\n\nCard et al. _Psychology_ , 7.\n\n65\n\nCard et al. _Psychology_ , 11–12.\n\n66\n\nPiccinini, Gualtiero. “Allan Newell.” University of Missouri-St. Louis.\n[www.umsl.edu/~piccininig/Newell%205.htm](http://www.umsl.edu/%257Epiccininig/Newell%2525205.htm).\nAccessed June 25, 2019.\n\n67\n\nMenzies, Tim. “21st-Century AI: Proud, Not Smug.” IEEE 3 (2003): 18–24.\n\n68\n\n[https://towardsdatascience.com/a-concise-history-of-neural-\nnetworks-2070655d3fec](https://towardsdatascience.com/a-concise-history-of-\nneural-networks-2070655d3fec) (retrieved July 30, 2019).\n\n69\n\nBostrom, _Superintelligence_ , 9.\n\n70\n\nSchank, 40.\n\n71\n\nHardesty, Larry (2017). “Explained: Neural networks. Ballyhooed artificial-\nintelligence technique known as ‘deep learning’ revives 70-year-old idea.” MIT\nNews. [http://news.mit.edu/2017/explained-neural-networks-deep-\nlearning-0414](http://news.mit.edu/2017/explained-neural-networks-deep-\nlearning-0414). Posted April 14, 2017. Accessed. August 28, 2019.\n\n72\n\nRemembering David E. Rumelhart (1942–2011). Association for Psychological\nScience. Accessed July 8, 2019. [www.psychologicalscience.org/observer/david-\nrumelhart](http://www.psychologicalscience.org/observer/david-rumelhart).\n\n73\n\nFeigenbaum, “Knowledge Engineering,” 10.2.\n\n74\n\nNielsen, Jakob (2017). “A 100-Year View of User Experience.” Last updated:\nDecember 24, 2017. Last accessed July 30, 2019.\n[www.nngroup.com/articles/100-years-\nux/](http://www.nngroup.com/articles/100-years-ux/).\n\n\n© Gavin Lew, Robert M. Schumacher Jr. 2020\n\nG. Lew, R. M. Schumacher Jr. AI and UX\n<https://doi.org/10.1007/978-1-4842-5775-3_3>\n\n# 3\\. AI-Enabled Products Are Emerging All Around Us\n\nTechnology is everywhere\n\nGavin Lew1 and  Robert M. Schumacher Jr. 2\n\n(1)\n\nS Barrington, IL, USA\n\n(2)\n\nWheaton, IL, USA\n\nAccess to computing power is at our fingertips. In the palm of our hands, the\nmobile phones we hold are smarter than desktop machines that sat on our office\nspaces a decade ago. Still in our lifetime, the small screen that we stare at\nto read news, check email, or play a game has the computing power that used to\ntake up an entire room as a mainframe computer. With this power comes\nconnectivity that gives us access to information. The convergence of computing\npower, connectivity, and data opens the doors to so much more. Simplistically,\nthese connected devices form what is called the Internet of Things (IoT). And\nnow, companies are embedding many of these devices with AI. What started with\nvoice-enabled platforms connecting to IoT devices now has broader implications\nof bringing intelligence to what would be **ubiquitous computing** .\n\nDefinition\n\n**Ubiquitous computing** describes a state in which we interact with computers\nthroughout our daily lives without even considering them as computers.1\n\nUbiquitious Computing\n\nGAVIN: As we enter into the 2020s, we are creeping ever closer to a state of\n**ubiquitous computing**. The rapid growth of the Internet of Things (IoT) and\nincreased connectivity could mean AI could learn from your behaviors and\npredict your habits. For example, your thermostat in your house might know you\nlike to keep the temperature at 73 degrees Fahrenheit when you are home. When\nyou leave for work, the system reduces the temperature, but as you drive home,\nthe system turns it back up. _Y_ _ou don’t even think twice about the devices\nor connectivity to make this happen._ It just works. You walk into the house,\nand _ahhh_.\n\nBOB: You said a crucial part at the end there: “it just works.” For it to\n“just work,” the UX has to build trust. The virtual assistant has to\nunderstand exactly what your query means, communicate it to your thermostat,\nand do it in a timely and consistent fashion so that you’ll learn to trust it.\nOnly if you trust it will you feel like it “just works.”\n\n**The point** We are at a unique moment where ubiquitous computing is here,\nbut what will make the difference is the _experience._\n\nMoving toward ubiquitous computing requires us to develop products with the\nkind of UX that reduces barriers between the user and the device. In the\nmedical technology space, there are already ubiquitous computing tools—medical\ninstruments that are connected to the Internet of Things. These are not\nnecessarily AI in and of themselves, but their data could be fed into AI\nsystems. There are already Internet-enabled medical instruments numbering in\nthe millions,2 and there will likely only be more in the years to come. The\nnumber of things that are “smart” or “Internet enabled” is in the billions.\nJust walk through the Consumer Electronics Show in Las Vegas every year and\nyou’ll see some really far out stuff that’s smart: fishing rods, fabrics,\nforks, and soccer balls to name a few.\n\nAI systems will find their role in the interplay across non-intelligent\ndevices, AI systems (themselves), and human beings. For one thing, AI systems\nmay be powerful enough to collect and synthesize the massive amounts of data\nthrown off by a large collection of non-intelligent IoT devices. For another,\ndesigners can take a lesson from these non-intelligent devices by giving their\nAI systems a distinct role and use cases for the user. If the user begins to\ntake these use cases for granted, the barriers between the users and the AI\nmay start to fall, which is a good thing.\n\nAI has an important role to play in the matrix of humans and devices that will\nrise in the next decade. It goes without saying that things are moving\nquickly—very quickly. While expert systems and neural networks are still\nunderlying some of our most important technology systems, they have been\nsucceeded as buzzwords by other terms: virtual assistants, deep learning,\nnatural language processing/understanding, and more. IBM cites3 “dynamic\nintelligence ” as a strength of its Watson Health program, a leader in the\nmedical AI space. As Roger Schank predicted, learning and adaptability have\nbecome important parts of AI development.\n\nNeural networks continue to provide a viable architecture for AI development,\nthough they have taken on yet another name. “Deep learning” is one of the most\ncommon terms we hear coming from the AI side of things today. Deep learning\nsystems are essentially machine learning-capable, multilayered, unsupervised\nneural networks, according to the hardware company NVIDIA.4 The major\ndifference between them and the neural networks of the 1990s is that today’s\ndeep learning systems are far more capable of “learning” without human\nintervention than any past systems.\n\nDeep learning systems have been applied across a wide variety of fields, and\nwe don’t have time to address them all here. We will address some of the more\nglamorous subfields of AI today, including virtual assistants and self-driving\ncars. However, to begin, we want to look at one of the most important areas\nwhere AI is making an impact: medicine.\n\n## Medical AI as a team player\n\nHealthcare is a complex industry. Because it involves human lives, the stakes\nare naturally high, but so is the potential for high reward and opportunity.\nCompanies have targeted healthcare as an area to integrate AI products and\nservices.\n\n> _AI is real, it’s mainstream, it’s here and it can change almost everything\n> about healthcare._\n>\n> —Virginia Rometty, CEO, IBM5\n\nHealthcare is Complex and AI Might not have all the Building Blocks\n\nBOB: IBM Watson, an AI system intended for healthcare applications, kicked off\na marketing launch that in some ways did too well.\n\nGAVIN: It started when IBM Watson defeated two human contestants in a game of\nJeopardy in 2011. Following that victory, IBM announced that AI had mastered\nnatural language and would now take on healthcare, a leap that at face value\nleft a lot more questions than answers.\n\nBOB: But from that moment on, IBM Watson was marketed as an AI doctor. In\n2014, IBM did demonstrations where Watson was fed a bag full of symptoms and\nout came a list of possible diagnoses with confidence levels and links to\nsupporting medical literature. The CEO said this was the start of a new\n“Golden Age.”6\n\nGAVIN: Generalizing natural language to play Jeopardy is one thing.\nUnfortunately, the billions spent since 2011 were heavily focused on being a\ndoctor—to take in symptoms and output diagnosis and treatment plans. Watson\nset the target to be as good a doctor.\n\nBOB: This might be a case of believing your own press. When you evoke messages\nof ushering in a revolution in healthcare, the course you set might be a bit\ntoo ambitious.\n\n**The point** Healthcare is complex and AI may still not be up to the standard\nrequired.\n\nIBM Watson was discussed in Chapter [1](470825_1_En_1_Chapter.xhtml) in the\noncology example where Watson predicted cancer treatment recommendations well\nfor US doctors, but not for South Korean doctors. This was an example of how\ntwo separate AI outcomes were compared instead of merging the data to simply\nset geographic **context** (i.e., flagging data that were US cases and those\nthat were South Korean). Watson was admonished for not correlating to\nrecommended South Korean treatment plans. We argued that AI found insight—that\nsomething different is happening between the US and South Korean oncologists.\nPerhaps identifying this finding could lead to better health outcomes.\nHealthcare is complex and there is an opportunity for AI to uncover insights\nusing data already collected.\n\nNow, let’s look at medical AI from a different angle. We think medical AI\nprovides an effective illustration of how AI can overcome fear and\ndisillusionment. Specifically, medical AI deals with one of the most important\nfears surrounding AI’s proliferation in general: the idea that people will\nlose their jobs to automated systems.\n\nAt its inception in 2011, a spokesperson for the company said IBM Watson would\nmake diagnoses based on some 200 million pages of data, including academic\nresearch, insurance claims, and medical records.7 If this had come to pass,\nmedical AI would have been capable of doing many of the things that doctors do\non a day-to-day basis (though perhaps not with the same level of accuracy, as\nwe’ll discuss).\n\nNaturally, this allowed onlookers to envision a day where some doctors—not to\nmention other medical professionals—are put out of a job. The doctors at\nhighest risk seemed to be those whose work is more focused on the sorts of\nanalytical tasks that AI does well (such as radiologists, who are experts in\nanalyzing medical scans). In 2016, the University of Pennsylvania physician\nSaurabh Jha predicted that radiologists would lose their jobs to AI within “10\nto 40 years, but closer to 10 years.”8\n\nAs it turns out, IBM Watson in particular was not as capable as\nadvertised—staving off any fears of AI-induced consequences for employment in\nmedicine. Daniela Hernandez and Ted Greenwald authored a concerning article in\n_The Wall Street Journal_ that we described in Chapter\n[1](470825_1_En_1_Chapter.xhtml).9 Hernandez and Greenwald cited difficulties\nwith inconsistent data formatting and the still-developing state of cancer\nresearch as possible reasons why IBM Watson has stagnated. IBM Watson, in its\ncurrent state, only accesses research, not medical records or insurance\nclaims. The dream of unsupervised or lightly supervised neural networks\ndiagnosing cancer on their own still awaits access to patient information and\npatient outcomes.\n\nIn 2018, it became clear that IBM had overhyped its own product in the initial\nstages.10 Its developers may have thought it would develop more quickly than\nit actually did. Specifically, IBM Watson’s oncology program is reportedly too\noften inaccurate to be reliable. One healthcare industry expert interviewed by\nMearian said that IBM Watson was released to the world too early. It still\nneeded more time to develop its knowledge base.11 IBM disputes the claims that\nWatson is inaccurate, saying that it has helped a significant portion (2–10%)\nof cancer patients to change to a different treatment and that it often agrees\nwith physician recommendations.12\n\nNew York Medical College’s Douglas Miller and IBM’s Eric Brown co-authored a\nstudy in _The American Journal of Medicine_ that assuaged fears about AI\ntaking medical jobs.13 Though Miller and Brown did not rule out the\npossibility of unforeseen consequences arising in the future of AI, they cited\nissues with accuracy and intuition as reasons that AI is not yet set to\novertake any physicians. Rather, Miller and Brown recommended that physicians\nuse medical AI as a powerful “tool” which can assist them in diagnosis and\ntreatment.14\n\nThe point\n\nAt this juncture, AI augments human cognition in medicine (and most other\nareas) but does not replace it.\n\nIf medical AI can follow Miller and Brown’s advice, it can avoid falling into\na hype-driven freefall like the one that machine translation experienced in\nthe late 1960s. In the past decade, medical AI certainly overpromised and\nunder-delivered. But it is still delivering something that can be useful to\ndoctors. The question is whether developers and doctors can adapt to the new\nworld and build a product that is more appropriate considering AI’s current\ncapabilities.\n\nAs it turns out, doctors are already doing their part. A Department of\nVeterans Affairs doctor cited by Hernandez and Greenwald said the service can\nbe helpful in the search for relevant academic research: “Dr. Kelley said\nWatson’s recommendations can be wrong, even for tried-and-true treatments. On\nthe other hand, he said, it is fast and useful at finding relevant medical\narticles, saving time and sometimes surfacing information doctors aren’t aware\nof.”15\n\nMedical AI can focus on what it does well—analyzing a large corpus of research\nfor relevant articles, for example—allowing doctors to spend more time working\non the diagnostic tasks that AI isn’t capable of yet. It can also serve as a\nsecond-opinion generator. An AI diagnosis shouldn’t be the only diagnosis a\npatient receives, but it can supplement a doctor and perhaps point out if\nthere is something else to consider. This could be a vitally important tool in\nthe United States, where a high percentage of patients with serious conditions\nare often misdiagnosed.16 Medical AI will present an external voice that can\nhelp reduce error frequency.\n\nTraining AI to Learn is Just the First Step\n\nGAVIN: Interestingly, IBM Watson and its goal of consuming mass volumes of\nmedical literature to look for patterns may have been its first misstep.\n\nBOB: Scanning articles for patterns is by definition a form of AI, but how\ndoes it map to how a doctor reads articles? Does AI value the same parts of a\npaper that a human doctor values? Do the data scientist and AI developer\nbelieve that the truth will wash out and true knowledge can be uncovered, or\nis the practice of medicine more nuanced?\n\nGAVIN: AI looks for correlations and statistical patterns. So, decades of\nmedical literature would set precedent. But, what about the latest study using\na new therapy, like gene therapy? What if this new direction changes\neverything? Eventually, scores of peer-reviewed publications will be written,\nbut today, for patients suffering, how much weight would AI place on the\nlatest groundbreaking study? How does AI differentiate groundbreaking from an\nisolated study amongst volumes of peer-reviewed and non-peer reviewed\narticles? AI in healthcare must be in the business of improving health\noutcomes for patients today.\n\n**The point** Just training AI to learn using both historical research data\nand cutting-edge novel treatments recently published. This is just the first\nstep in the complex walk that is healthcare. Doctors need to **trust** AI and\nthen incorporate its insights into practice.\n\nIn medicine, AI would be better served if it assisted human beings, rather\nthan project the goal of replacing them. Lick, discussed in Chapter\n[2](470825_1_En_2_Chapter.xhtml), would have loved this application of his AI\nprinciple. Medical AI has the potential to increase doctors’ accuracy, while\nnot threatening their job security. Many of the products in the medical AI\nspace are designed specifically with this purpose in mind. Eric Topol, the\nauthor of a book on medical AI, envisions a future in which AI helps doctors\nto spend more time working with individual patients, rather than spending\ntheir days reading dozens of scans, without the time to focus on any\nparticular patient.17 Topol sees doctors offloading their routine tasks and\nhaving more time for the sort of “intuitive” work that Miller and Brown argue\nhumans are best suited to do. He cites AI programs that are already working to\ndiagnose specific diseases, such as an algorithm that detects diabetic\nretinopathy, a vision-impairing condition caused by diabetes. Topol’s vision\nof AI would lead to a collaborative relationship in which medical\nprofessionals and medical AI each take on the portion of their work that they\nare best suited to do.\n\nAI as Just Part of a Team\n\nGAVIN: If AI works together with human beings, they can do more as a team than\neither one could do individually. That should be the model for AI everywhere.\n\nBOB: If AI is seen as collaborating with a human user, maybe it won’t seem so\nscary or dangerous. It’ll seem more helpful.\n\nGAVIN: In the medical AI space, it seems like we backed into that\nrelationship. Initially, there was a lot of hype surrounding medical AI, and\nit was supposed to do all these amazing things. But that turned out to be\neasier said than done.\n\nBOB: It reminds me of machine translation in the 1950s and 1960s. The computer\nscientists of the 1960s thought that once they had taught a machine to\ntranslate a few Russian phrases into English, teaching it to become a fluent\ntranslator would come along quickly. The designers of medical AI might have\nmade the same mistake. It’s possible to get AI to do certain tasks in the\nmedical space, but making it a generalized medical intelligence is way more\ndifficult.\n\nGAVIN: But it seems like there’s a path for medical AI to avoid the domain-\nspecific AI winter that plagued machine translation after the ALPAC report. If\nit can concentrate on what it does well—for now, combing through its archives\nfor medical research or offering a second opinion—it can be very useful.\n\nBOB: In some sense, the “irrational exuberance” clouded reality and led many\nto underestimate the difficulty of the problems. Because the systems are still\ndeveloping, there has to be an allocation of responsibility. There should be\nsome ability for doctors to say, for example, “For this type of cancer, I\ndon’t want a treatment plan right now, can you just show me the relevant\nresearch?” But maybe for this other type of cancer, the treatment plans are\nuseful. And then maybe the AI can learn what kind of feedback to give its\nusers.\n\nGAVIN: This collaboration between AI and doctor could result in better health\noutcomes. Imagine, a doctor asking for only the newest treatments in a rare\ncondition that is spreading quickly. But, this is where the design matters.\nProgrammers can develop the next wave of medical AI with interactions in mind.\n\n**The point** AI can be dangerously overhyped as being more capable than a\nhuman in a complex job. But AI’s real value is as a helpful member of the team\nif it can adapt to its strengths and limitations.\n\nHealthcare is a good example of where buying into the technology can be too\nappealing. Sometimes we build applications based on poor assumptions, even a\nhunch, and not based on true need. Similar to how holding a hammer makes every\nproblem looks like a nail. But in AI’s case, it is, “Here is technology and\nlet’s apply it everywhere,” or worse, the dreaded, “if you build it, they will\ncome” fallacy. Martin Kohn, who was the chief medical scientist at IBM during\nthe Jeopardy period, was once so enamored by the technology’s potential that\nhe called it a trap. “Merely proving that you have powerful technology is not\nsufficient,” he says. “Prove to me that it will actually do something\nuseful—that it will make my life better, and my patients’ lives better.”18\nSince Kohn has left IBM, he still is in search of peer-reviewed papers in the\nmedical journals to demonstrate that AI can improve patient outcomes and save\nhealth systems money. “To date there’s very little in the way of such\npublications,” he says, “and none of consequence for Watson.”19\n\n## The rise of virtual assistants\n\nIn Chapter [2](470825_1_En_2_Chapter.xhtml), we discussed the initial\nstagnation of **voice and virtual assistants**. After Siri was released in\nbeta form, the assistant’s limited capabilities led to public frustration with\nthe service and with virtual assistants in general. This led to what we’d\nconsider a domain-specific **AI Winter** —one that had a dramatic effect on\nMicrosoft’s Cortana in particular. Yet the emergence of Amazon’s Alexa opened\nthe door to try voice again.\n\nLet’s explore what it took to restore the impression of what a **voice\nassistant** could and could not do. What considerations in research and design\nshould we attribute to Alexa’s success?\n\nBeyond Siri\n\nGAVIN: Let’s talk about Siri, our favorite virtual assistant that most know\nabout but rarely use.\n\nBOB: I would argue that more people accidentally wake Siri than those who\nproactively engage it to actually solve tasks.\n\nGAVIN: That’s the problem. And I have spoken to designers of other voice-based\nvirtual assistants, and they go on about how different their system is from\nSiri. They talk about how Siri treats errors like a joke. But, instead, their\nsystem is unique because it took a different approach, one that presented a\nmore _mature_ interaction. Some spoke of _triggered interactions_ where after\nso many successful instances of the same voice command, hints would be added\nto responses to reveal new features or shortcuts. After a failed voice command\nwhere some information was recognized, the voice response would change to\ninclude help. While this may or may not improve **interactions** , the problem\nwas the lack of adoption that really hurt potentially novel features from\nevolving further.\n\nBOB: Imagine the millions of dollars that went into developing Microsoft’s\nCortana or Samsung’s Bixby. How many users never even tried to use them? And\nit probably wasn’t solely because they didn’t like Microsoft or Samsung, but\nmaybe they assumed the experience would be just like using Siri?\n\nGAVIN: This is what people do when they experience frustration or cannot get a\nproduct to work. People often generalize the experience to other similar\nproducts. This is a domain-specific **AI winter**. Novel approaches to voice\nassistants do not advance simply because they never had the chance.\n\nBOB: At least in the case of Amazon Fire’s failure, sometimes smart ideas can\nfind new life in a different form factor--one that sat like an obelisk on the\nkitchen table.\n\nGAVIN: Amazon Echo breathed new life into voice assistants because a C-level\nexecutive like Jeff Bezos took the risk when he saw the opportunity.20\n\n**The point** Designing a winning product depends on many factors, the least\nof which involves seizing opportunities to advance novel features. Look at\nolder product designs to find features that deserve a second chance.\n\nThe revolution in development and deployment of virtual assistants came from\nthe confluence of (1) vastly improved natural language processing and natural\nlanguage understanding, (2) high speed Internet, (3) cloud computing, and (4)\nubiquitous deployment of tiny microphones and speakers. But even these things\ndid not help Siri succeed. What made a material difference for Alexa was that\nit was embedded in a standalone device: the Echo. Mobile virtual assistants\ndiffer, because they have other primary uses, and the virtual assistant\nfunction is just a new feature within a larger ecosystem.\n\nThis means that the Echo, unlike the iPhone or PC, was designed for the\nspecific purpose of serving as an effective virtual assistant. The Echo was\ndesigned with context of use in mind. Its cylindrical form, combined with\nmarketing, meant that the Echo had clearly defined use cases in the home.\n\nThere’s a clear lesson to be found in the Echo’s success: a product with\nclearly defined use cases was effective in helping consumers understand the\ncontext of use for a new product category. Let’s explore three contexts a\nlittle deeper.\n\n### Context of use\n\nUsing a virtual assistant in the home is no big deal. Using it at a ballpark\nis a no-go. For Alexa on the Amazon Echo platform, the task of incorporating\ncontext of use is easy: the technology simply has to be designed to\nincorporate a single-use category. In fact, the Echo will likely stay in the\nsame room for the duration of its use. (Not to mention that Amazon can sell a\nlot more if you need one in each room of the house!) Perhaps it could adapt\nbased on whether it is in the bedroom or the kitchen. But for an assistant\nthat primarily lives on the phone, like Siri, the task is more complicated.21\nSiri could be used in the kitchen or the bedroom, but also in the car, in a\nprivate office space, or even in some more public setting (a lobby, a public\nspace, etc.) With location services, perhaps it could tailor its responses for\nthe exact location/room where it is being used, but the use case is\ncomplicated, given the wide variety of possibilities.\n\nAnother part of context of use incorporates not just where the user is, but\nwhat they are doing. After all, an Amazon Echo user might be in the kitchen\ncooking—in which case, a timer or a recipe might be useful for Alexa to have\non hand. (Alexa has 60,000 recipes on hand for that situation.)22 But they\nalso might be in the kitchen chatting with their spouse—in which case, access\nto their shared calendar might be most important. For assistants on mobile\nplatforms, of course, the contextual possibilities are exponentially higher.\nEven if a virtual assistant is designed to be used in the kitchen, there is\nstill a wide variety of things that a user might do within that context that\nthe assistant needs to be prepared for.\n\nThe point\n\nWhere you are matters. Allowing AI to know where the user is can improve the\nexperience by making it more accurate and relevant. Designing AI with\n**context of use** can support AI to be more insightful.\n\n### Conversational context\n\nWhen you’re talking to a virtual assistant, it should remember what you were\ntalking about. This is easier said than done. When Business Insider was\ntesting the top four virtual assistants (Siri, Alexa, Google Assistant, and\nCortana) back in 2016, they tried asking each virtual assistant about the next\nBoston Celtics basketball game. All four did fine with that query, but when\nthey asked the follow-up, “Who is their top scorer?”, everyone got lost. The\nassistants didn’t understand that “their” was a reference to the Celtics from\nthe previous question. That’s an example of assistants missing out on\nconversational context.23 And it’s not an anomaly in using virtual assistants.\nAssistants often break the natural flow of conversation by losing track of\nconversational context—that is, what was just discussed and what pronouns in\nthe current query might refer to that prior information.\n\nThe point\n\nWe already described **machine translation** and its challenges with language,\nbut designing with an understanding of a temporal context can improve\nconversations. AI needs to move beyond simple query to task **interactions**\nand anticipate most likely follow-up queries instead of assuming the\nconversation only had one question with no follow-up.\n\nConversational context errors are some of the most frustrating ones that still\nplague virtual assistants today. It’s a difficult problem for humans in normal\nconversation, let alone for voice dialog designers and programmers. Sometimes,\na subsequent query refers to the previous query, and other times, it’s an\nentirely new question. But it’s also immensely frustrating to users to ask\nwhat seems to be a natural follow-up question and get an unnatural answer.\n\nWhen dialog with an assistant goes bad, it’s because the normal conventions of\nhuman conversation are not followed. Tracking pronouns is just one instance.\nThere are many others. In fact, most human conversation holds true to certain\n“maxims”24 laid out by linguist and philosopher Paul Grice: quantity, quality,\nrelation, and manner. See Table 3-1.\n\nTable 3-1\n\nGrice’s four maxims of communication\n\n• Quantity: Information o Make your contribution as informative as is required\nfor the current purposes of the exchange. o Do not make your contribution more\ninformative than is required.| • **Quality** : Truth o Do not say what you\nbelieve to be false. o Do not say that for which you lack adequate evidence.  \n---|---  \n• Relation: Relevance o Be relevant.| • Manner: Clarify o Avoid obscurity of\nexpression. o Avoid ambiguity. o Be brief (avoid unnecessary prolixity). o Be\norderly.  \n  \nVoice assistants need to conform to human conversational norms in order to be\naccepted by human beings. They can do this by adapting to Grice’s\nconversational maxims. To be successful, AI researchers need to embrace what\nlinguists and psycholinguists already know about how people communicate\neffectively.\n\nApplying Grice’s Maxims to an Alexa Skill\n\nGAVIN: When we say that “AI should be designed to ________,” how does this\noccur? Are we describing a future state or can this be done today?\n\nBOB: Anybody with basic technical skills can try to build a conversation that\nAlexa can respond to. In fact, I built one in 45 min with no knowledge of how\nto program an Alexa or experience with the Alexa Skills Kit. Amazon\nessentially hid the code behind a web interface that allows for skill\ncreation.\n\nGAVIN: So, the barrier of being a developer was removed? You literally dragged\nand dropped what you wanted Alexa to listen for?\n\nBOB: Yes. I did not have technical coding skill, but Amazon built an interface\nto design what Alexa should _say_ and what to _listen for_ and then what Alexa\nshould _say in response_. So, what we are recommending is to incorporate UX\nprinciples in the design.\n\nGAVIN: So, you are not talking about algorithms. Your suggestion is to design\na **voice assistant** that follows conversational norms, the kind of things in\nGricean maxims.\n\nBOB: This is where UX and AI can merge and make the product much _smarter_\nbecause it pulls in UX principles into the design. In this example,\n**conversational context** would give AI a distinctive edge.\n\n**The point** We can learn a lot from what linguists already know about\ncommunication patterns. Using Grice’s maxims can inform the dialog and lend to\nanticipated follow-on questions. Analyzing errors can suggest remedy or\nconversational corrections. These are examples where **conversational\ncontext** can be integrated into AI-enabled products.\n\n### Informational/user context\n\nThis wide-ranging third category of context includes the resources to which a\nservice has access: both resources about the user’s attributes and ones that\nhelp the user when looking things up. Google, unsurprisingly, is exemplary in\nboth of these categories. They are best at gathering data about you—it’s\npractically their _modus operandi_ —and they leverage that data to personalize\nyour results. Similarly, Google has access to the wealth of information that\nis the first place that most of us go when we want to learn about something:\nGoogle. That gives it a distinct advantage over its competitors in terms of\naccess to external information. Alexa and Siri have to rely on outside sources\nfor information about, say, that actor whose most recent movie role you can’t\nquite remember.\n\nAlso consider the benefit AI would have knowing the speaker. Alexa and Google\nHome are moving toward identifying the speaker in the home because this\n**informational context** of the user is the mom or the dad would presumably\nplay a role in determining the AI-enabled output. For example, the response to\nthe utterance “play some music” would be more relevant if the **voice\nassistant** knew who was asking.\n\nThe point\n\nIn simplest terms, **virtual assistants** perform tasks which at the core\nrequires an input (an utterance) and a resulting output (a response). If AI\nwas designed to apply **AI-UX principles** such as **context,** AI could\nproduce much more useful experiences.\n\n### Examples: AI-enabled vehicles\n\nAll of these types of context are relevant as **virtual assistants** expand to\nnew domains. One space where **virtual assistants** will grow is in mobility\n(e.g., cars). Currently, cars are one of the most popular use cases for voice-\nbased assistants. According to industry research by Voicebot.ai,25 **virtual\nassistants** have about 90 million monthly users on smartphones, about 77\nmillion in cars, and about 46 million on smart speakers.\n\nThe car is an interesting use case for the **virtual assistant**. As Vox\njournalist Rani Molla points out, drivers should not be using touchscreens\nwhen driving—and voice assistants seem like a perfect replacement.26 Today’s\ncar voice assistants are often connected through smartphones, which may take\nadvantage of cars with built-in connectivity through programs like Apple\nCarPlay and Android Auto, but can also function without those built-in\nfeatures. This gives Siri and Google Assistant an advantage in the auto space,\nbut Amazon is fighting back, planning a plug-in called Echo Auto for cars\nwithout built-in voice assistants.27 As Amazon already has a reputation for\nmaking standalone virtual assistants that are built for a particular\nenvironment, this may prove to be an effective product.\n\nA few years ago, Toyota previewed a concept car with a more functional\nautomotive virtual assistant called Yui.28 Yui would have all kinds of unique\n**informational context** that it would use when helping direct you where to\ngo. If you always visit the grocery store on Tuesdays after work, it would\ndirect you there at the usual time—unless, of course, your digital calendar\nwas booked. It could switch back and forth between allowing the user to drive\nand driving itself. It would remember your preferences—perhaps remembering\nwhether you tend to prefer the surface streets or the expressway—and use it\nwhen recommending places to go and ways to get there. It might even learn\nthose preferences based on its analysis of your facial expressions. That is,\nit would incorporate a version of **informational context** based on the user.\n\nUnderstanding Context is Essential to Supporting the User\n\nGAVIN: Yui is just a concept, and it’s a long way from being a reality. But it\ngives us a preview of what virtual assistants might do in the long term. It’s\nproactive and incorporates different types of context. For context of use, it\nseems focused specifically on helping the user drive. For informational\ncontext, it seems to have a wealth of information about the user and because\nit is context driven, assistance can be given without the user explicitly\nrequesting a command.\n\nBOB: We’ve already got the building blocks of proactivity in cars. Think about\na feature in many new cars, “lane assist.” I recently drove a rental car with\nthis feature. If I drifted outside the current lane without signaling, the car\nresisted (gently) and moved back to the lane and also played an audio signal.\nThis is a first step in proactivity on the part of the car. It was perhaps\nmore aware of my situation than I was—maybe I was distracted and on my phone,\nor I was nodding off. This combination of machine vision, sensors, and\nsignaling saves lives and is an example of a good user experience.\n\nGAVIN: Those are the kinds of UX changes we can make in mobility AI in the\nnear term.\n\nBOB: We’re still a long way away from developing anything like Yui. Probably\n10 years, at least. These are the obvious use cases. I want the AI to either\ndrive my car or be my assistant helping me be aware. For instance, the AI\nshould detect an obstacle in the road even if I don’t see it. Or it might even\nknow if the driver next to me is driving like they have had too much to drink;\nit should help me with my “situational awareness.” Let’s have AI work to help\nme drive safely.\n\nGAVIN: But there are UX solutions for virtual assistants in the car that we\ncould adopt today. The car is an easy scenario for context of use. What are\nusers going to want to ask their virtual assistant in the car? Mainly for\ndirections and to play music and podcasts. They can ask whether they’ve got\nany new messages. That’s really about it.\n\nBOB: True, but that’s short-term thinking. Applying what we know about\ncontext—large multi-ton vehicles moving at high rates of speed—means that\nother things should take precedence. If I ask for the weather forecast, should\nthe AI forego detecting an imminent accident and tell me that it’s going to\nrain instead? Of course not.\n\nGAVIN: Well, would that be the same AI system doing both those tasks? Maybe\nthere’d be one AI pumping the brakes when you’re at risk of an accident and a\nwhole separate AI that answers your questions.\n\nBOB: I don’t know, but if that was the case, they’d certainly have to talk to\neach other. If I keep getting into close calls on the road while talking to my\nassistant, the accident-assist AI should be able to tell the AI assistant that\nit should probably stop letting me ask frivolous questions.\n\nGAVIN: Indeed. Virtual assistants should support the users and understand the\npriorities of safety (in this case) before information and before\nentertainment.\n\nBOB: Changes like that would go a long way toward making virtual assistants\nmore useful.\n\n**The point** For virtual assistants to be successful, they need to\nincorporate three types of context—context of use, conversational context, and\ninformational context.\n\n## Data science and imputation\n\n**Data science** , like AI, has been getting a lot of buzz lately.\n\nDefinition\n\nData science—you might also call it data analytics—is the analysis of large\ndatasets to gather insights.\n\nThe potential to uncover insights from a massive amount of data that would be\nimpossible by a human is alluring. AI a lot of the time is spent on the\nalgorithms, but, surprisingly, not a lot of time is spent on data. All too\noften, data is purchased or obtained from historical archives and the data is\nspun into AI. There is an eagerness to see what happens when the AI algorithm\nthinks. So much interest is focused on the algorithm and what it uncovers, but\nare we spending enough attention on the data that are fed into the machine?\n\nAI has been described as a black box; data goes in and what comes out the\nother side may show no traceability to the inputs (Figure 3-1).\n\n![../images/470825_1_En_3_Chapter/470825_1_En_3_Fig1_HTML.jpg](../images/470825_1_En_3_Chapter/470825_1_En_3_Fig1_HTML.jpg)\n\nFigure 3-1\n\nIllustration of information entering the black box (i.e., AI process) and\ninsight emerges on the other side\n\nDon Norman, part of the team that developed theories of information processing\nthat led to early neural network development, said this about AI, “The problem\nwith modern artificial intelligence is it’s all based on pattern recognition\non huge data. It looks for patterns… it reads all the literature, but it\ndoesn’t know how to reason about it… There’s no understanding.”29 We are blind\nto what is happening with the algorithms. The patterns formed are merely\nstatistical coefficients. It is impossible to determine the rationale that was\nused to build the output.\n\nThe Survey Data Minefield\n\nBOB: When our UX consultancy was purchased by one of the top market research\nfirms, we learned firsthand what big data was really about. There was\nmountains of data—lots of it going back decades. It was all part of being a\nworld-class data company.\n\nGAVIN: Access to 30 years of trended respondent data at that scale must be a\nbeautiful sight for a team building an AI app that could use this data.\n\nBOB: It is actually really useful in understanding trends in market\nresearch—and potentially valuable as AI training data. Some of the questions\nsurvived multiple changes in methodology from in-home interviews to telephone\nsurveys to online.\n\nGAVIN: And if they were using roughly the same questions, that could be really\ninteresting as an AI data source.\n\nBOB: Well, there is a concern. The people using the output generated from the\nAI need to know the dataset. How many developers will stop to ask questions\nabout the multiple methods used to collect that data? The data scientists knew\nhow to weigh the impact of differing methods of data collection. But is this\nlost to AI or just part of the black box?\n\nGAVIN: The dataset has more than data. It has metadata—data about the data.\nSome might be important but not really factor into the data important to the\nalgorithm and stripped to provide a “clean dataset.”\n\nBOB: That’s my fear. In any survey, there’s almost always data missing whether\nby design, technical hiccup, or respondent laziness. What data scientists\nfrequently do is fill the missing data using various techniques. This results\nin a “clean dataset” without missing cells.\n\nGAVIN: Why would data be missing “by design”?\n\nBOB: Back in the 1970s, a 60-minute survey of consumer preferences may have\nbeen an interview over coffee in someone’s home. In the 1980s, this same\nsurvey became a telephone interview, then an online computer survey, and now a\nsurvey via mobile phone.\n\nGAVIN: But, who would take a 60-minute survey on a mobile phone?\n\nBOB: That is the point. Because researchers know that few survey takers will\nspend more than 10 minutes doing a survey on a mobile phone, they break up the\nsurvey into sections. Now, it takes several participants to complete what used\nto happen in the home.\n\nGAVIN: They don’t just merge the data. The holes are filled by some equations\nperformed by data scientists. So, you have missing fields being filled by data\nscientists. Furthermore, some survey methodologists tell me that in some\nsurveys up to 25% of surveys are taken by bots written by people to make “beer\nmoney.”\n\nBOB: Yup, some datasets have a lot of missing data and others are filled—not\nby people—but by bots. This should make everyone pause. At its core, AI looks\nfor patterns. Won’t AI find the highest pattern match when it correlates data\nfrom the underlying algorithms used to fill empty cells or by bots to take\nsurveys? If true, then AI would weigh data made by algorithms over human\ndata?!?!\n\nGAVIN: This is garbage in, garbage out, but we would never know because we\ncannot peek inside the AI black box to see how, why, or where the output was\nderived.\n\n**The point** This is the problem with AI as a black box. It only is as good\nas the data that it receives. The data used is critical because if we\nunderstand what is going in and then what is happening inside, then the\noutcome might be just a recreation of what some data scientist used to fill\nthe holes.\n\nWe need to be extremely careful when AI uses survey data that is based on\nhuman respondents. We actually should approach the data with a high degree of\nscrutiny when we use the data to train AI algorithms. A marketing firm might\nbuild a database of potential customers, or a political campaign might have an\ninternal list of potential voters with lots of parameters and values. This is\nthe data that an AI tool might use to learn from. The data scientist needs to\nunderstand how data elements were obtained and if missing data was filled.\nKnowing the details is vital to the future success of AI because once AI is\ntrained, we cannot look into the “black box” to understand the why or\nrationale behind the solution. Without accurate data, any AI program that\ndepends on data science’s findings is suspicious at best.\n\nLet’s review for emphasis this problem: the data scientists often have missing\npieces of information in their datasets. When the datasets contain information\nfrom surveys or behaviors of human respondents, that raises several issues.\nProper analyses should demand that all the cells in the dataset are filled. If\nresearchers simply delete rows of data that have empty cells, their results\nwill come out distorted or biased, especially if there was some confounding\nvariable that the rows with empty cells had in common.30 So the data\nscientists need to fill these cells in; this is **imputation** .\n\nDefinition\n\nImputation is the insertion of values into a dataset, in place of missing\ncells. Imputation is often needed to run a data analysis.\n\nWhat’s more, survey design often includes built-in **imputation**. Some\nsurveys are so long that it might take 30 minutes to complete. Today, most\npeople are not willing to take a 30-minute survey (and even if they do, the\nquality of their responses is likely to deteriorate toward the end of the\nsurvey. For this reason, survey designers may break up the survey into\nsmaller, say, 10-minute chunks and assign each chunk to an individual subject.\nThen, using the common data (e.g., demographic information), the researchers\nwould impute these subjects’ responses to the chunks of the survey that they\ndid not see. This imputation would be based on the responses to these survey\nchunks from other subjects who had similar demographic information to the\noriginal subject.\n\n**Imputation** is a necessary, reasonable statistical process for survey\nresearch—but could raise havoc as an AI training set. Many large-scale surveys\ninvolve at least some sort of imputation. There are a variety of methods for\nimputing data, many of which include input from an algorithm. These\nalgorithmically generated imputations concern us.\n\nMany data insights are based on trends found in the dataset. If there is an\nalgorithm imputing data, that algorithm will follow a certain pattern in its\nimputation. This would lead to data which would represent a pattern or trend\nartificially present in the data. An AI learning system that was fed such a\ndataset might identify these algorithmic patterns and mistake them for human-\ngenerated trends.\n\nThis, of course, has potentially dangerous consequences if the algorithmically\nimputed data is not clearly marked as different in the data analysis.\n\nThe solution we see is to mark imputed data as imputed and not strip out\nimportant metadata. This will give data scientists context to construct proper\ntraining datasets to give to AI developers so AI systems will analyze data as\nintended. Being skeptical about data gives the AI a better chance to discover\nsomething real rather than delivering some artifact inherent in the data.\n\nThe point\n\nSurvey data can often at least partially be generated by algorithms via\nimputation. Before feeding a dataset based on surveys to AI as training data\nlook into how much of that data is actually generated by an algorithm.\n\n## Recommendation engines\n\nIf you’ve ever listened to Spotify’s “Discover Weekly” playlist, or Apple\nMusic’s similar “New Music Mix,” you’ve interacted with a **recommendation\nengine** .\n\nDefinition\n\n**Recommendation engines** are algorithms that analyze a user’s past behavior\nto determine what new content that user might like and then recommend that new\ncontent to the user.\n\nMusic isn’t the only space where recommendation engines reign—Netflix and Hulu\noffer movie and TV recommendations, YouTube has its infamous right sidebar,\nand Amazon’s website seems to fill every inch of extra space with recommended\nproducts. Even Facebook and Twitter recommend related accounts and pages to\nyou or suggest “people you may know” to connect with.\n\nAt this point, recommendation engines are foundational to the appeal of a wide\nvariety of digital services, that is, if they’re useful. They can also be\nendlessly frustrating if they recommend irrelevant content or content they’ve\nalready viewed. And these engines have a pretty short span with which to\ncapture users’ attention. Netflix research says that users will give up their\nsearch for new content to watch after 60 to 90 seconds.31 If users are\ndisappointed by the offerings of a **recommendation engine** , it may lead to\na case of “failure runs deep,” in which users may quickly learn that a\nrecommendation engine is ineffective and begin to ignore it. But web services\nlike Netflix and Spotify continuously tweak the algorithms to improve the\nsuggestions.\n\nAt their best, recommendation engines serve an effective purpose for both\nparties. Web services want to keep users browsing, listening, watching, and\nshopping, and according to Netflix’s research, the recommendation engine saves\nthe company more than $1 billion by doing just that. Internal research claims\nthat 80% of video views on Netflix come from recommendations rather than from\ndirect search.32 Users want to find new and useful content, and recommendation\nengines help them do this.\n\nIn 2014, Netflix announced that they were updating their recommendation engine\nfrom a simpler engine focused on viewing data to a more complex one based on a\nneural network.33 Netflix’s recommendation algorithm uses data about users’\nviewing habits—including which shows and movies they have watched, how quickly\nthey finished those shows, and which shows they quit watching. They combine\nthis with data from other viewers, and with genre and feature codes assigned\nto each Netflix show by human coders, to group users into one of thousands of\n“taste groups”.34\n\nSpotify’s recommendation engine for its Discover Weekly playlist features a\nsimilar apparatus of data collection and subgenre classification, but it adds\nin another metric: pairings on user playlists.35 A song that many users have\non the same playlist as the Cranberries’ “Dreams” is pretty likely to be\nrelated to “Dreams” somehow. This further limits the chance of coincidental\npairings. Spotify has also described personalizing its recommendations based\non user habits. As of 2015, Spotify has described building a detailed database\nof minuscule subgenres much like Netflix’s taste groups.\n\nSpotify uses a three-pronged process to shape its Discover Weekly\nRecommendations, according to software engineer Sophia Ciocca. It runs a\nmatrix analysis which compares users to other users, it uses natural language\nprocessing data from press coverage of artists and songs to determine what\nadjectives might describe them, and it runs a neural network analysis of the\naudio attributes of each of its songs.36 It then combines these factors with\nhuman guardrails to avoid things like playlists full of children’s music for\nparents.37\n\nAll of this personalization can give Discover Weekly the feeling of a human-\ncurated playlist. In a sense, it _is_ a human-curated playlist—it’s created\nbased on your listening habits and the playlists shaped by other users, and\nthere is human input throughout the process. But deep learning (which we might\nconsider AI) plays a role—and, ultimately, your tracks are chosen by an\nalgorithm.\n\nOne Spotify user, blogger Eric Boam, logged nearly all music recommendations\nhe received for an entire year and wrote a post comparing Spotify’s\nrecommendations with those from media and human sources. He found that Spotify\noffered a large volume of recommendations, but that those recommendations had\na lesser rate of success than recommendations from other humans or from the\nmedia.38 So Spotify’s algorithm doesn’t quite have the quality of a human\nrecommender just yet. Of course, it can spit out a far higher volume of\nrecommendations.\n\nAnd still, Boam describes engaging with Spotify recommendations and sometimes\nfinding favorite albums through the service. Ultimately, engaging users with\nan AI service is the end goal, and recommendation engines achieve this.\nDiscover Weekly will never replace your friends’ recommendations, but it\ndoesn’t have to—those recommendations are probably shared in the form of a\nSpotify playlist anyway.\n\nRecommendation engines exemplify the ways in which AI might fit into a user\nexperience. While only a portion of Spotify’s recommendation engine is in fact\nan AI system, that AI system blends seamlessly with other computing and human\nelements to build an engine that proves valuable to users. At the end of the\nday, this engine is only one part of the appeal of Spotify or Netflix as a\nservice, but Netflix’s numbers indicate that it is an important component of\ncustomer retention. Even if customers don’t always get their music or movie\nrecommendations from Discover Weekly or Netflix’s recommendation service,\nusers still engage with those digitally produced recommendations. And the high\nrecommendation volume a digital recommender can produce can keep users\nengaging with a service before their attention spans expire.\n\nThe point\n\nAI may not be the end-all, be-all that’s ready to replace humans in fields\nlike recommendation, but it can still be a useful component of a user’s\nexperience.\n\n## AI journalists\n\nThe field of journalism is in a unique predicament in the information age:\ndespite being the most important purveyors of news in local and national\nsettings, the news business is in a state of crisis. Newspapers, especially,\nhave been ravaged by the digital era. Many smaller newspapers have shut down\naltogether, and those that have survived have often done so by laying off\nstaffers and cutting pay, leading to a possible reduction of news quality.\nJournalists are already spread thin by the nature of the constantly churning\nnews cycle, and they now have to complete a larger proportion of their paper’s\nwork on stagnant or reduced pay.\n\nMedia owners are searching for ways to cut costs, and journalists are looking\nto cut down on rote work. Enter AI journalists. In 2010, Northwestern\nUniversity researchers released StatsMonkey, a program that could write\nautomated stories about baseball games.39 By 2019, major news outlets\nincluding _The Washington Post_ and the Associated Press were using AI to\nwrite articles.40\n\nNaturally, journalists are afraid of the potential of losing jobs to robots.\nBut AI journalists are not quite capable enough to replace most journalists\nanytime soon. Generally, AI journalists do best at producing high volumes of\nshort recaps on formulaic, data-based events like earnings reports and\nbaseball games. AI journalists do not write articles all on their own,\neither—they’re generally fed scripts on how to write a particular type of\narticle.41 The major AI journalism project RADAR operates much like an expert\nsystem, as journalists must program the format of articles on a certain topic\ninto the system alongside a set of if-then rules.42 Other AI journalists seem\nto operate similarly. While AI journalists can produce large amounts of\ncontent in the subfields for which they have been trained, they are generally\nnot capable of replacing much of the cognitive work journalists do.\n\nIn 2019, the Tow Center for Digital Journalism at Columbia University released\na study in which researcher Andreas Graefe attempted to use an AI journalist\nto automatically generate stories about polling and predictions in the 2016 US\npresidential election.43 Graefe considered the project “very successful,”44 as\nthe AI published thousands of articles on different results. However, it was\nmost successful with relation to specific domains for which it was\nmeticulously trained and when dealing with less complex data.\n\nGraefe found difficulty in training the AI to recognize such qualitative\ncharacteristics as one candidate having “momentum” or a margin being “large”\nor “small.” In an extremely close race, a poll showing a three-point lead for\none candidate might seem “large”; in a race with one dominant candidate, that\nsame lead would be “small.” Since these qualities vary in specific cases, they\nare difficult to quantify, making them difficult to program into the AI.\n\nThis is an effective illustration of the limitations of AI journalists today.\nThey’re essentially expert systems engineered for specific scripts and\ndomains, while being unable to do the sort of adaptive or deep-learning work\nthat might really make them a threat to human journalists. The sort of\nqualitative analysis that can easily be performed by humans is much more\ndifficult for AI. Meanwhile, AI can mass-produce formulaic stories that human\nbeings would otherwise have to individually write out. It leads to a symbiotic\nrelationship.\n\nIn 2016, _The Washington Post_ deployed AI in its election coverage. Even with\nthe limitations of AI journalists, the _Post_ successfully applied theirs to\nwrite some 500 election stories and internally to help alert reporters to\nunexpected shifts in election data and even won an award for their bot\nusage.45, 46 In 2017, the _Post_ hired a new team of human investigative\njournalists.47 We think the timing of those two events may not be a\ncoincidence.\n\nAI as a Beat Writer\n\nGAVIN: If the _Post_ can trust AI to fill out routine stories, they can afford\nto invest more in the sort of cognitive, investigative work that human beings\nare best at.\n\nBOB: Which is exactly what we need to save journalism, right?\n\nGAVIN: Right. _The Washington Post_ can now afford to put additional resources\ntowards in-depth reporting.\n\nBOB: So, AI can do some of the routine work of journalism, writing repetitive\nstories about election polls and baseball games. That frees up resources for\ninvestigative reports.\n\nGAVIN: What happens if the AI screws up? What if it says that my favorite\nbaseball team, the San Francisco Giants, won by four runs, when they actually\nlost by four runs?\n\nBOB: Well, people have a hard time blaming a computer. But I guess that speaks\nto the beauty of AI being limited to less consequential stories for the time\nbeing. If AI reports the wrong score for the Giants’ game for a few hours, the\nworld isn’t going to burn down.\n\nGAVIN: As a Giants fan, I’d be a little upset if I read a whole article about\nhow they won, only to find out they actually lost. Maybe there should be some\nway of notifying me, as someone who read the article, about the mistake. Maybe\nthey could send out a push notification to only users who opened the article,\nsaying that there was an error.\n\nBOB: I’m not sure the technology’s there, but that’s a good example of\ncontext-aware proactivity. For the time being, AI journalists are another\nexample of AI supplementing human employees. AI can only write stories about\nspecific domains, and they’re writing the sort of stories that mainstream\njournalists might not enjoy. When it comes to more complex tasks, real\njournalists are better equipped to handle them. Fears of human journalists\nlosing their jobs to AI are overexaggerated, as of now.\n\n**The point** AI in workplaces like the newsroom can actually elevate the work\nof human employees, allowing them to take on more cognitive tasks uniquely\nsuited to humans’ strengths.\n\n## AI, filmmaking, and creativity\n\nIn this chapter, we’ve tried to emphasize the fact that human beings and AI\nare good at different things. AI can make calculations, gather disparate\nsources of information, and find insights that human beings can’t. But there\nare still plenty of domains where human beings still reign supreme. In order\nto find out how close AI is to beating us at our own game and venturing a look\nat how AI and humans can work together in more qualitative domains, we decided\nto take a look at how AI fares in the creative domain of filmmaking.\n\nBelieve it or not, the 2010s saw the first-ever AI-created movie trailer. In\n2016, IBM’s Watson partially constructed a trailer for _Morgan_ , a horror\nfilm about humans dealing with an AI system gone rogue.48 Watson was trained\nfor the task by watching and analyzing other horror movies, to determine the\ndifferent types of emotion present in each scene, and it then selected ten\nmoments from _Morgan_ that would fit well in a trailer, which were then\ncompiled into a trailer by IBM.49 The trailer has moments that seem scary or\nemotionally resonant, and it has the right mood-setting music, but the trailer\nis also disjointed. The different clips don’t always fit well together, and\nit’s hard to tell what the plot of the film is based on the trailer. The AI-\ngenerated _Morgan_ trailer has an “uncanny valley” quality to it—that is, it\nfeels close enough to a human-created trailer that it isn’t totally\nincomprehensible, but just different enough that it leaves the viewer with an\neerie effect.\n\nFilm editing combines cognitive and affective elements, requiring the editor\nto understand what the emotional resonance of a scene or clip will be and how\nbest to design a trailer or a film to maximize the sort of emotion that they\nwant to evoke. It’s these sorts of tasks, which are both complex and\nemotional, that AI is least equipped to take on. Consider Sherry Turkle’s\ncriticism of the Hello Barbie toy: AI can’t provide empathy.50 Empathy is a\nkey component of a lot of art.\n\nThis clearly illustrates what human beings have that AI is a long way away\nfrom being able to replicate. Humans are capable of constructing coherent\nnarratives that make sense to other human beings, evoke emotions in their\naudiences, convey subtextual messages, and even contain aesthetic beauty. AI\ncan’t do any of those things. It’s difficult to quantify aesthetics.\n\nSo, AI is pretty bad at scriptwriting and not great at generating movie\ntrailers, either. But it’s still proven useful to filmmakers in the realm of\nanimation. AI has made the routine tasks of animation—fixing little details in\na character’s movement or definition—much easier. In today’s special-effects-\nheavy films, this is a vitally important contribution. Today’s actors often\nstar as characters whose physically impossible appearance needs to be\nanimated. At one time, this required these actors to film all of their scenes\nin front of a green screen or inside a recording studio. Today’s AI can\nartificially generate a character’s look based on the actor’s face and allow\nthem to act with the other actors in the film, animating the character on top\nof their real-life appearance. This technology was used in a recent _Avengers_\nmovie. It can even generate the animated character so quickly that the actor\ncan view themselves acting as their animated character while filming.51\n\nIt’s not just filmmaking where AI has made an impact in automating rote tasks.\nAI-human team relationships not unlike the one found in medical AI are all\nover the artistic world. In visual art, the Celsys AI can colorize black-and-\nwhite drawings.52 In music, the Bronze AI can generate an infinite version of\nan individual song that changes a little bit every time it’s played.53 In\nfiction, one author has created an AI program that automatically auto-\ncompletes a writer’s sentences while writing science fiction stories, based on\na corpus of science fiction stories.54 He envisions the program as a kind of\nco-author, which generates ideas that might spark the writer’s human\ncreativity. Tellingly, none of these three projects are widely used. AI in the\narts is not quite ready for prime time yet.\n\nBut these projects offer a preview of how AI might begin to make inroads into\nthose domains that are very conducive to human skills and not so conducive to\nthe quantitative expertise of AI. In almost any domain, there is room for a\nuseful assistant that brings a different perspective and a contrasting set of\nskills. In the arts, there may be fewer quantitative tasks for AI to complete,\nbut there are still specific domains where it can automate tasks that once\ntook humans hours of frustrating effort to complete, freeing them up to\ncomplete other tasks.\n\nThe point\n\nIn the arts, human skills still reign supreme. But artists are already finding\nniches where AI can offer a contribution, bringing the collaborative relation\nto even the more qualitative domains.\n\n## Business AI\n\nBusiness is one of the AI verticals where human-AI relationships will be more\ndifficult to forge. There are AI solutions available for finding new sales\nleads, analyzing job applicants, improving customer service, parsing turgid\nlegal documents, and more.55, 56, 57 But with their profits riding on it, it’s\ndifficult to imagine businesspeople abandoning their tried-and-true sales and\nhiring methods and adopting one of the various AI business solutions on the\nmarket. It’s going to require building an AI system that is especially capable\nof building trust in a business setting.\n\nLuckily, there is already research about what it takes to build an effective\nbusiness relationship, thanks to guidelines designed for human-to-human\ncollaboration. We think this research is a good starting point for building a\nrelationship between AI and humans in the workplace. An important contribution\non the subject comes from the medical field of family practice, but its\nfindings are applicable across many types of businesses. Research from FPM, an\nAmerican Academy of Family Physicians journal, identified seven elements that\nlead to healthy, collaborative relationships: trust, diversity, mindfulness,\ninterrelatedness, respect, varied interaction, and effective communication.58\n\nSix out of seven of these elements are vitally important to human-AI\ninteractions in the business field, and we’ve already discussed three of them.\nBeginning in Chapter [1](470825_1_En_1_Chapter.xhtml), we’ve extensively\ndiscussed the importance of trust for AI. We’ve also discussed the manner in\nwhich AI offers diversity by investigating the collaborative relationship\nbetween AI and human beings. AI can add diversity of perspective to a\ndecision-making process because its computational nature frees it from\ncognitive biases inherent in human reasoning. Interrelatedness is similar to\ncontext—it requires an awareness of the big-picture significance of specific\nactions and collaborators within an organization.\n\nBut it also might make users more comfortable collaborating with a machine, by\nmaking the machine seem more friendly and less intimidating.\n\nThis is another area where Microsoft’s Cortana offers a unique approach that\nmakes its stagnation in the wake of the virtual assistant winter all the more\nfrustrating. At one point, after Microsoft purchased LinkedIn in 2016, they\nwere rumored to be planning to bring its data to Cortana.59 This could have\ncreated an AI assistant that truly had the capability to function in the\nrealms of both business and personal affairs. Unfortunately, this version of\nCortana never came to fruition.\n\nThe FPM researchers describe effective communication as knowing when to apply\ntwo different types of communication: text communication (which delivers less\ninformation and context but is quick and convenient) and face-to-face or over-\nthe-phone communication (which is more informative and context rich, but\ncumbersome). Business AI might apply similar methods in deciding how to\ncommunicate a message to the user. While the option of face-to-face\ncommunication is not present for AI, there are several methods of possible\ncommunication between AI and humans, including typing back and forth,\nselection from a menu, and voice interfaces. An effective business AI might be\nbuilt with information about which ones to use when.\n\nR-E-S-P-E-C-T What IT Means to Me\n\nBOB: That leaves respect as the final one of our seven elements of business\ncommunication.\n\nGAVIN: How does respect differ from **trust**?\n\nBOB: Well, usually, when we respect someone, we also trust them. So, it can be\nhard to separate them. But in my mind, respect is something above and beyond\ntrust. You might trust a business AI tool to do one or two specific things,\nbut not respect it. I might trust my AI to analyze my earnings report and\nwrite up some statistical insights, but that doesn’t mean I respect it. The\nFPM researchers talked about respect as “valu(ing) each other’s opinions.” To\nme, that means that you think of its insights as worth considering in any\ncase. If an AI system that I respect tells me about some insight—even if that\ninsight goes against all my preconceived notions—I’ll listen to it. I’ll hear\nit out.\n\nGAVIN: Respect is more general, and trust is more specific?\n\nBOB: Right. That’s how I would differentiate it. Although it’s difficult,\nbecause we sometimes use “trust” to mean what I would call respect. But right\nnow, AI is built to value your opinion, but I might not value its opinion,\neven if I trust it to do certain things. AI has to do all of these other\nthings—earn trust, apply context, communicate effectively, and maybe even vary\ninteractions—before it can earn the user’s respect.\n\nGAVIN: A lot of that sounds like UX. AI needs to be designed for its users,\nwith respect to how they think and the components that make them trust. What\nmakes you trust another human being isn’t all that different from what makes\nyou trust AI.\n\nBOB: And that respect is especially hard to earn in a business setting. There\nare serious consequences if the AI gets anything wrong.\n\nGAVIN: An element that’s not on this list—but maybe should be, because it’s a\ncomponent of respect—is transparency. I need to have some idea of what AI is\ndoing, and why it’s doing it, before it can earn my respect. I don’t need to\nknow all the details, but I need to know something.\n\nBOB: Sounds like something to add to our AI-UX framework.\n\n**The point** A good user experience can help build trust and respect in the\nAI application.\n\n## Some conclusions and where we’re going next\n\nIn the next chapter, we will go deeper into how to affect change in the areas\nof AI that we can influence, such as the data. Again, the focus is not on a\nparticular AI learning algorithm or code, but what can we do to improve AI\nthrough the data itself? What are the elements where we can have an impact on\nAI? As we identify gaps and concerns, what can be done to pivot problems into\nopportunities to make the product better? What can we learn from those who\nunderstand the issues with datasets, for example, and how the “big players” in\nthe industry approach solutions? In some ways, the answer is not simply to do\nwhat is needed, but to go orders of magnitude greater to give their AI-enabled\nproducts a better chance for success.\n\nFootnotes\n\n1\n\nWitten, Bekah. “Ubiquitous Computing: Bringing Technology to the Human Level.”\nUSF Health [https://hscweb3.hsc.usf.edu/is/ubiquitous-computing-human-\ntechnology/](https://hscweb3.hsc.usf.edu/is/ubiquitous-computing-human-\ntechnology/).\n\n2\n\nMarr, Bernard. “Why the Internet of Medical Things (IoMT) Will Start To\nTransform Healthcare in 2018.” Last updated January 25, 2018. Accessed June 1,\n2019. [www.forbes.com/sites/bernardmarr/2018/01/25/why-the-internet-of-\nmedical-things-iomt-will-start-to-transform-healthcare-\nin-2018/#75cf88e54a3c](http://www.forbes.com/sites/bernardmarr/2018/01/25/why-\nthe-internet-of-medical-things-iomt-will-start-to-transform-healthcare-\nin-2018/%252375cf88e54a3c).\n\n3\n\n“About IBM Watson Health.” Accessed May 29, 2019.\n[www.ibm.com/watson/health/about/](http://www.ibm.com/watson/health/about/).\n\n4\n\n“Deep Learning.” NVIDIA Developer. Accessed May 30, 2019.\n[https://developer.nvidia.com/deep-\nlearning](https://developer.nvidia.com/deep-learning).\n\n5\n\nStrickland, Eliza (2019). “How IBM Watson Overpromised and Underdelivered on\nAI Health Care.” IEEE Spectrum. Last updated April 2, 2019. Last accessed\nNovember 6, 2019. [https://spectrum.ieee.org/biomedical/diagnostics/how-ibm-\nwatson-overpromised-and-underdelivered-on-ai-health-\ncare](https://spectrum.ieee.org/biomedical/diagnostics/how-ibm-watson-\noverpromised-and-underdelivered-on-ai-health-care).\n\n6\n\nStrickland, Eliza. (2019). “How IBM Watson Overpromised and Underdelivered on\nAI Health Care.” IEEE Spectrum. Last modified April 2, 2019. Accessed August\n19, 2019. [https://spectrum.ieee.org/biomedical/diagnostics/how-ibm-watson-\noverpromised-and-underdelivered-on-ai-health-\ncare](https://spectrum.ieee.org/biomedical/diagnostics/how-ibm-watson-\noverpromised-and-underdelivered-on-ai-health-care).\n\n7\n\nMearian, “IBM’s Watson…to diagnose patients.”\n\n8\n\nJha, Saurabh. “Will Computers Replace Radiologists?” Last updated May 12,\n2016. Accessed July 14, 2019.\n[www.medscape.com/viewarticle/863127#vp_3](http://www.medscape.com/viewarticle/863127%2523vp_3).\n\n9\n\nHernandez, Daniela and Ted Greenwald. “IBM has a Watson dilemma.” The Wall\nStreet Journal. [www.wsj.com/articles/ibm-bet-billions-that-watson-could-\nimprove-cancer-treatment-it-hasnt-\nworked-1533961147](http://www.wsj.com/articles/ibm-bet-billions-that-watson-\ncould-improve-cancer-treatment-it-hasnt-worked-1533961147).\n\n10\n\nMearian, Lucas. “Did IBM overhype Watson Health’s promise?” Computerworld.\nLast updated November 14, 2018. Accessed May 31, 2019.\n[www.computerworld.com/article/3321138/did-ibm-put-too-much-stock-in-watson-\nhealth-too-soon.html](http://www.computerworld.com/article/3321138/did-ibm-\nput-too-much-stock-in-watson-health-too-soon.html).\n\n11\n\nMearian, “Did IBM overhype…”\n\n12\n\nHernandez and Greenwald.\n\n13\n\nMiller, D. Douglas, and Eric W. Brown. “Artificial Intelligence in Medical\nPractice: The Question to the Answer?” The American Journal of Medicine,\n131/2(2018): 129–133.\n[https://doi.org/10.1016/j.amjmed.2017.10.035](https://doi.org/10.1016/j.amjmed.2017.10.035).\n\n14\n\nMiller and Brown, “Artificial Intelligence in Medical,” 132.\n\n15\n\nHernandez and Greenwald.\n\n16\n\n20 percent of patients with serious conditions are first misdiagnosed, study\nsays. [www.washingtonpost.com/national/health-science/20-percent-of-patients-\nwith-serious-conditions-are-first-misdiagnosed-study-\nsays/2017/04/03/e386982a-189f-11e7-9887-1a5314b56a08_story.html](http://www.washingtonpost.com/national/health-\nscience/20-percent-of-patients-with-serious-conditions-are-first-misdiagnosed-\nstudy-says/2017/04/03/e386982a-189f-11e7-9887-1a5314b56a08_story.html).\nBernstein, Lenny. April 4, 2017 (retrieved May 22, 2020).\n\n17\n\nBelluz, Julia. “3 ways AI is already changing medicine.” Vox. Last updated\nMarch 15, 2019. Accessed May 31, 2019. [www.vox.com/science-and-\nhealth/2019/3/15/18264314/ai-artificial-intelligence-deep-medicine-health-\ncare](http://www.vox.com/science-and-health/2019/3/15/18264314/ai-artificial-\nintelligence-deep-medicine-health-care).\n\n18\n\nStrickland, Eliza (2019). “How IBM Watson Overpromised and Underdelivered on\nAI Health Care.” IEEE Spectrum. Last updated April 2, 2019. Last accessed\nNovember 6, 2019. [https://spectrum.ieee.org/biomedical/diagnostics/how-ibm-\nwatson-overpromised-and-underdelivered-on-ai-health-\ncare](https://spectrum.ieee.org/biomedical/diagnostics/how-ibm-watson-\noverpromised-and-underdelivered-on-ai-health-care).\n\n19\n\nMilanesi, Carolina (2016). “Voice Assistant, Anyone? Yes please, but not in\npublic!” Creative Strategies. Last modified June 3, 2016. Accessed August 23,\n2019. [https://creativestrategies.com/voice-assistant-anyone-yes-please-but-\nnot-in-public/](https://creativestrategies.com/voice-assistant-anyone-yes-\nplease-but-not-in-public/).\n\n20\n\nBariso, Justin (2019). “Jeff Bezos Gave an Amazon Employee Extraordinary\nAdvice After His Epic Fail. It’s a Lesson in Emotional Intelligence. The story\nof how Amazon turned a spectacular failure into something brilliant.” Inc.\nLast updated December 9, 2019. Accessed May 14, 2020. [www.inc.com/justin-\nbariso/jeff-bezos-gave-an-amazon-employee-extraordinary-advice-after-his-epic-\nfail-its-a-lesson-in-emotional-intelligence.html](http://www.inc.com/justin-\nbariso/jeff-bezos-gave-an-amazon-employee-extraordinary-advice-after-his-epic-\nfail-its-a-lesson-in-emotional-intelligence.html).\n\n21\n\nMost (all?) virtual assistants can be used on mobile platforms, like phones,\nas well.\n\n22\n\nVincent, James. “Amazon’s Alexa can now talk you through 60,000 recipes.” The\nVerge. Last updated November 21, 2016. Accessed July 1, 2019.\n[www.theverge.com/2016/11/21/13696992/alexa-echo-recipe-skill-\nallrecipes](http://www.theverge.com/2016/11/21/13696992/alexa-echo-recipe-\nskill-allrecipes).\n\n23\n\n“We put Siri, Alexa, Google Assistant, and Cortana through a marathon of tests\nto see who’s winning the virtual assistant race—here’s what we found.”\nBusiness Insider. Last updated November 4, 2016. Accessed July 1, 2019.\n[www.businessinsider.com/siri-vs-google-assistant-cortana-alexa-2016-11#the-\nsetup-theres-no-perfect-way-to-evaluate-a-talking-ai-database-let-alone-four-\nof-them-but-i-tried-to-cover-as-many-fundamental-topics-as-i-\ncould-1](http://www.businessinsider.com/siri-vs-google-assistant-cortana-\nalexa-2016-11%2523the-setup-theres-no-perfect-way-to-evaluate-a-talking-ai-\ndatabase-let-alone-four-of-them-but-i-tried-to-cover-as-many-fundamental-\ntopics-as-i-could-1).\n\n24\n\nGrice, H. P. (1975). “Logic and Conversation,” Syntax and Semantics, vol.3\nedited by P. Cole and J. Morgan, Academic Press, and Grice, H. P. (1989).\nStudies in the Way of Words. Harvard University Press.\n\n25\n\nMolla, Rani. “The future of smart assistants like Alexa and Siri isn’t just in\nhomes—it’s in cars.” Vox. Last updated January 27, 2019. Accessed June 26,\n2019. [www.vox.com/2019/1/15/18182465/voice-assistant-alexa-siri-home-car-\nfuture](http://www.vox.com/2019/1/15/18182465/voice-assistant-alexa-siri-home-\ncar-future).\n\n26\n\nMolla, Rani, “The future.”\n\n27\n\n[www.consumerreports.org/automotive-technology/amazon-alexa-isnt-so-simple-in-\na-car/](http://www.consumerreports.org/automotive-technology/amazon-alexa-\nisnt-so-simple-in-a-car/).\n\n28\n\nEtherington, Darrell. “Here’s what it’s like to drive with Toyota’s Yui AI in-\ncar assistant.” TechCrunch. Last updated January 6, 2017. Accessed July 2,\n2019. [https://techcrunch.com/2017/01/06/heres-what-its-like-to-drive-with-\ntoyotas-yui-ai-in-car-assistant/](https://techcrunch.com/2017/01/06/heres-\nwhat-its-like-to-drive-with-toyotas-yui-ai-in-car-assistant/).\n\n29\n\nNorman, Don (2016). “Doing design with Don Norman.” Medium Podcast. August 24,\n2016. Accessed March 18, 2020. [https://medium.com/@uxpodcast/design-doing-\nwith-don-norman-6434b022831b](https://medium.com/%2540uxpodcast/design-doing-\nwith-don-norman-6434b022831b).\n\n30\n\nGelman, Andrew and Hill, Jennifer. “Missing-data imputation.” From Data\nAnalysis Using Regression and Multilevel/Hierarchical Models, Cambridge\nUniversity Press (2006). p. 529–544. Accessed July 3, 2019. doi:\n[https://​doi.​org/​10.​1017/​CBO9780511790942​.​031](https://doi.org/10.1017/CBO9780511790942.031).\n\n31\n\nMcAlone, Nathan. “Why Netflix thinks its personalized recommendation engine is\nworth $1 billion per year.” Business Insider. Last updated June 14, 2016.\nAccessed June 16, 2020. [www.businessinsider.com/netflix-recommendation-\nengine-worth-1-billion-per-\nyear-2016-6](http://www.businessinsider.com/netflix-recommendation-engine-\nworth-1-billion-per-year-2016-6).\n\n32\n\nMcAlone, “Why Netflix.”\n\n33\n\nRussell, Kyle. “Netflix Is ‘Training’ Its Recommendation System By Using\nAmazon’s Cloud To Mimic The Human Brain.” Business Insider India. February 12,\n2014. Accessed June 15, 2019. [www.businessinsider.in/Netflix-Is-Training-Its-\nRecommendation-System-By-Using-Amazons-Cloud-To-Mimic-The-Human-\nBrain/articleshow/30259713.cms](http://www.businessinsider.in/Netflix-Is-\nTraining-Its-Recommendation-System-By-Using-Amazons-Cloud-To-Mimic-The-Human-\nBrain/articleshow/30259713.cms).\n\n34\n\nPlummer, Libby. “This is how Netflix’s top-secret recommendation system\nworks.” Wired. August 22, 2017. [www.wired.co.uk/article/how-do-netflixs-\nalgorithms-work-machine-learning-helps-to-predict-what-viewers-will-\nlike](http://www.wired.co.uk/article/how-do-netflixs-algorithms-work-machine-\nlearning-helps-to-predict-what-viewers-will-like).\n\n35\n\nPasick, Adam. “The Magic that Makes Spotify’s Discover Weekly Playlists So\nDamn Good.” Quartz. Accessed June 15, 2019. [https://qz.com/571007/the-magic-\nthat-makes-spotifys-discover-weekly-playlists-so-damn-\ngood/](https://qz.com/571007/the-magic-that-makes-spotifys-discover-weekly-\nplaylists-so-damn-good/).\n\n36\n\nCiocca, Sophia. “How Does Spotify Know You So Well?” Last updated October 10,\n2017. Accessed June 15, 2019. [https://medium.com/s/story/spotifys-discover-\nweekly-how-machine-learning-finds-your-new-\nmusic-19a41ab76efe](https://medium.com/s/story/spotifys-discover-weekly-how-\nmachine-learning-finds-your-new-music-19a41ab76efe).\n\n37\n\nPasick, “The Magic.”\n\n38\n\nBoam, Eric. “I Decoded the Spotify Recommendation Algorithm. Here’s What I\nFound.” Medium. Last updated January 14, 2019. Accessed June 15, 2019.\n[https://medium.com/@ericboam/i-decoded-the-spotify-recommendation-algorithm-\nheres-what-i-found-4b0f3654035b](https://medium.com/%2540ericboam/i-decoded-\nthe-spotify-recommendation-algorithm-heres-what-i-found-4b0f3654035b).\n\n39\n\n“Program Creates Computer-Generated Sports Stories.” NPR. Last updated January\n10, 2010. Accessed June 16, 2019.\n[www.npr.org/templates/story/story.php?storyId=122424166](http://www.npr.org/templates/story/story.php%253FstoryId%253D122424166).\n\n40\n\nPeiser, Jaclyn. “The Rise of the Robot Reporter.” Last updated February 5,\n2019. Accessed June 16, 2019.\n[www.nytimes.com/2019/02/05/business/media/artificial-intelligence-journalism-\nrobots.html](http://www.nytimes.com/2019/02/05/business/media/artificial-\nintelligence-journalism-robots.html).\n\n41\n\nPeiser, “The Rise.”\n\n42\n\n“Will AI Save Journalism—Or Kill It?” Knowledge @ Wharton, University of\nPennsylvania. Last updated April 9, 2019. Accessed June 17, 2019.\n[https://knowledge.wharton.upenn.edu/article/ai-in-\njournalism/](https://knowledge.wharton.upenn.edu/article/ai-in-journalism/).\n\n43\n\nGraefe, Andreas. “Computational Campaign Coverage.” Tow Center for Digital\nJournalism (2017).\n[https://academiccommons.columbia.edu/doi/10.7916/D8Z89PF0/download](https://academiccommons.columbia.edu/doi/10.7916/D8Z89PF0/download).\n\n44\n\nGraefe, 37.\n\n45\n\nMoses, Lucia. “The Washington Post’s robot reporter has published 850 articles\nin the past year.” Digiday. Last updated September 14, 2017. Accessed June 17,\n2019. [https://digiday.com/media/washington-posts-robot-reporter-\npublished-500-articles-last-year/](https://digiday.com/media/washington-posts-\nrobot-reporter-published-500-articles-last-year/).\n\n46\n\nMartin, Nicole. “Did a Robot Write This? How AI Is Impacting Journalism.”\nForbes. Last updated February 8, 2019. Accessed June 17, 2019.\n[www.forbes.com/sites/nicolemartin1/2019/02/08/did-a-robot-write-this-how-ai-\nis-impacting-\njournalism/#31e620777957](http://www.forbes.com/sites/nicolemartin1/2019/02/08/did-\na-robot-write-this-how-ai-is-impacting-journalism/%252331e620777957).\n\n47\n\nWashPostPR. “The Washington Post to create rapid-response investigations\nteam.” The Washington Post. Last updated January 9, 2017. Accessed June 17,\n2019. [www.washingtonpost.com/pr/wp/2017/01/09/the-washington-post-to-create-\nrapid-response-investigations-\nteam/?utm_term=.5f4864546f4b](http://www.washingtonpost.com/pr/wp/2017/01/09/the-\nwashington-post-to-create-rapid-response-investigations-\nteam/%253Futm_term%253D.5f4864546f4b).\n\n48\n\nAlexander, Julia. “Watch the first ever movie trailer made by artificial\nintelligence.” Polygon. Last updated September 1, 2016. Accessed July 6, 2019.\n[www.polygon.com/2016/9/1/12753298/morgan-trailer-artificial-\nintelligence](http://www.polygon.com/2016/9/1/12753298/morgan-trailer-\nartificial-intelligence).\n\n49\n\n20th Century Fox. “Morgan | IBM Creates First Movie Trailer By AI [HD] | 20th Century FOX.” YouTube. Published August 31, 2016. Accessed July 6, 2019. [www.youtube.com/watch?v=gJEzuYynaiw](http://www.youtube.com/watch%253Fv%253DgJEzuYynaiw).\n\n50\n\nBarbie wants to get to know your child. Vlahos, James. Sept 16, 2015.\nRetrieved May 19, 2020. [nytimes.com/2015/09/20/magazine/barbie-wants-to-get-\nto-know-your-child.html](http://nytimes.com/2015/09/20/magazine/barbie-wants-\nto-get-to-know-your-child.html)\n\n51\n\nRobitzski, Dan. “Was That Script Written By A Human Or An AI? Here’s How To\nSpot The Difference.” Futurism. Published June 18, 2018. Futurism. Accessed\nJuly 6, 2019. [https://futurism.com/artificial-intelligence-automating-\nhollywood-art](https://futurism.com/artificial-intelligence-automating-\nhollywood-art).\n\n52\n\nLee, Dami. “AI can make art now, but artists aren’t afraid.” The Verge. Last\nupdated February 1, 2019. Accessed July 6, 2019.\n[www.theverge.com/2019/2/1/18192858/adobe-sensei-celsys-clip-studio-colorize-\nai-artificial-intelligence-\nart](http://www.theverge.com/2019/2/1/18192858/adobe-sensei-celsys-clip-\nstudio-colorize-ai-artificial-intelligence-art).\n\n53\n\nChristian, Jon. “This AI Generates New Remixes of Jai Paul...Forever.”\nFuturism. Last updated June 4, 2019. Accessed July 6, 2019.\n[https://futurism.com/the-byte/ai-remixes-jai-paul](https://futurism.com/the-\nbyte/ai-remixes-jai-paul).\n\n54\n\n“Writing with the Machine.” robinsloan.com. [www.robinsloan.com/notes/writing-\nwith-the-machine/](http://www.robinsloan.com/notes/writing-with-the-machine/).\n\n55\n\nPower, Brad. “How AI Is Streamlining Marketing and Sales.” Harvard Business\nReview. Last updated June 12, 2017. [https://hbr.org/2017/06/how-ai-is-\nstreamlining-marketing-and-sales](https://hbr.org/2017/06/how-ai-is-\nstreamlining-marketing-and-sales).\n\n56\n\n“Applications of Artificial Intelligence Within your Organization.”\nSalesforce.\n[www.salesforce.com/products/einstein/roles/](http://www.salesforce.com/products/einstein/roles/).\n\n57\n\nGreenwald, Ted. “How AI Is Transforming the Workplace.” The Wall Street\nJournal _._ Last updated March 10, 2017. [www.wsj.com/articles/how-ai-is-\ntransforming-the-workplace-1489371060](http://www.wsj.com/articles/how-ai-is-\ntransforming-the-workplace-1489371060).\n\n58\n\nTallia, Alfred F., Lanham, Holly J., McDaniel, Jr., Reuben R., Crabtree,\nBenjamin F. American Association of Family Practitioners. “Seven\nCharacteristics of Successful Business Relationships.” From _Fam Pract Manag._\n2006: 13(1):47–50. Accessed July 14, 2019.\n[www.aafp.org/fpm/2006/0100/p47.html](http://www.aafp.org/fpm/2006/0100/p47.html).\n\n59\n\nDarrow, Barb. “How LinkedIn Could Finally Make Microsoft Dynamics a Big Deal”\nJune 13, 2016. [https://fortune.com/2016/06/13/microsoft-linkedin-dynamics-\nsoftware/](https://fortune.com/2016/06/13/microsoft-linkedin-dynamics-\nsoftware/) (accessed May 22, 2020).\n\n\n© Gavin Lew, Robert M. Schumacher Jr. 2020\n\nG. Lew, R. M. Schumacher Jr. AI and UX\n<https://doi.org/10.1007/978-1-4842-5775-3_4>\n\n# 4\\. Garbage In, Garbage Out\n\nDoing a disservice to AI\n\nGavin Lew1 and  Robert M. Schumacher Jr. 2\n\n(1)\n\nS Barrington, IL, USA\n\n(2)\n\nWheaton, IL, USA\n\nGiven the ever-evolving nature of AI, programmers need to continuously improve\nand refine their algorithms. In Chapter [1](470825_1_En_1_Chapter.xhtml), we\nsaw how algorithms are improved and often repurposed for different tasks, such\nas the credit card fraud detection system called Falcon that Craig Nies\ndescribed had its roots in a visual system to detect military targets.\nEssentially, the foundation for pattern recognition to differentiate\nbattlefield equipment from surrounding landscapes was applied to recognize\npatterns of fraud in credit card data.\n\nBut, again, let’s assume the AI code works; that is, the AI algorithms that\nfeed all of the modern-day AI systems—whether they are called deep learning or\nmachine learning or some other proprietary name—are capable of doing the job.\nIf this is true, then the focus shifts from the code to the datasets that feed\nthese systems. How much care has been placed into the data that feeds the\nmachine?\n\nWe need to take a bit of a step back here and more clearly define the space\nwe’re talking about. AI is a huge field. The focus of our data-centered\ndiscussion here points at AI-enabled products that rely on human behaviors,\nattitudes, opinions, and so on. Data that are actively solicited (e.g.,\nsurveys) have different properties (and problems) than data that are passively\nacquired. A lot of our discussion in this chapter focuses on data that is\nintentionally acquired from people.\n\nWhat we Feed the Algorithms Matters\n\nBOB: Consider Formula One racing. No matter how good the engine is, success\ndepends on the entire ecosystem around the vehicle: the mix of the fuel, the\nskill of the driver, the efficiency of the pit crew, and so on.\n\nGAVIN: An engine using low-grade fuel will underperform. In the case of AI,\nits fuel is data. While data scientists can massage the data to map to the\nalgorithms for learning, how much care is placed on the dataset? That data\nmight have been purchased from a site where it was “not perfect, but good\nenough.” Or it may be far removed from the researchers who collected it. What\nif the data is no longer high grade?\n\nBOB: As UX researchers, we know a lot about data collected from people—it’s\nmessy—the nuances in the questions, missing cells, context of collection, and\nmore. The problem can be especially concerning if the dataset was not\ncommissioned by the team using it. There’s a lot of trust that the dataset is\nclean.\n\nGAVIN: AI algorithms are fed data initially to learn and train those models;\nthose models are then applied broadly to more data to provide the insight.\n\nBOB: The data that is fed into AI is pretty important for success, especially\nin the training phase when AI is learning.\n\n**The point** It’s not just how good the algorithms are; it’s how good the\ndata is or “garbage in, garbage out.” Let’s spend time giving AI the best data\nwe can.\n\n## Swimming in data\n\nAs researchers, we (the authors) often talk to companies about their data. We\nask about what they know, what they don’t know, and what is currently being\ncollected. We look for gaps and opportunities where more or better data could\nanswer strategic questions. A common issue is that they have more data than\ncan be analyzed. So it’s not about collecting more data, but taking the time\nto think through how to better analyze what they have.\n\nIf this is the case, then the product team developing the AI-enabled\ntechnology must look critically into what data is used for training and what\nis used once the algorithm is trained. AI is an ecosystem of many elements—the\nalgorithm is just one piece. It may be at the center and gets much of the\nattention, but success depends on all the elements being aligned and\nsupportive of the objectives.\n\nCompanies swimming in data should think about how their data was obtained—did\nit come from a vast warehouse of compiled data, or was it gathered for a\nspecific purpose? This is a critical question to help understand that data.\nWas the data specifically collected for the AI-enabled product that targets\nthe key area of interest? If the data was not commissioned specifically for\nAI, then one must spend time to understand more about the dataset itself.\n\nQuestions to ask when evaluating a dataset are as follows:\n\n  * Where did the dataset come from?\n\n  * What was the method of data collection?\n\n  * If it was survey data, what are the assumptions and conditions under which this data was obtained?\n\n  * Were any of the data imputed (missing cells filled algorithmically)?\n\n  * What other datasets could be joined to add supplemental context?\n\n  * What do subject matter experts know about the data and how could this knowledge be beneficial to learning?\n\nThe point\n\nThese simple questions can identify areas of improvement for the training\ndataset that will be used to help AI learn. This is where we give AI a\nfighting chance at success by potentially giving the data more **context**.\n\n## So, how does AI really “learn”?\n\nData that capture human behaviors and interactions are given to machine\nlearning (ML) scientists to train AI systems and algorithms. Whether the data\ncomprises a set of liver-disease diagnoses and outcomes, comes from a consumer\nsurvey on attitudes toward marijuana usage, or derives from active/passive\ndata collection of spoken phrases, AI systems need training data to ensure\ntheir algorithms produce the right outcomes. Custom-built data for AI may not\nbe as common as datasets that were created for other purposes, such as market\nresearch, customer segmentation, sales and financial data, health outcomes,\nand a lot more. Once ML scientists have acquired a dataset, they still need to\nconsider whether it includes what the AI system needs.\n\n### An example of how AI learns\n\nAt one level, AI can be thought of as a pattern recognition system. In order\nfor an AI system to learn, it needs lots of examples. AI algorithm needs data\nto look for patterns, make mistakes, and refine its internal understanding to\nbe better. As a fun example of this, Figure 4-1 illustrates an Internet meme\nthat circulated a few years ago. What’s interesting is that it shows how easy\nit is for people to detect the signal (the Chihuahua) in a very noisy field.\nAI algorithms have a very difficult time with this and these samples are\nuseful to validate patten recognition systems.\n\n![../images/470825_1_En_4_Chapter/470825_1_En_4_Fig1_HTML.jpg](../images/470825_1_En_4_Chapter/470825_1_En_4_Fig1_HTML.jpg)\n\nFigure 4-1\n\nExample of the challenge of pattern recognition and data that might be\nprovided for an AI to learn how to distinguish between a Chihuahua from a\nblueberry muffin.\n\n### Different ways machines learn today\n\nIn general, there are three kinds of machine learning (ML) techniques for\nconstructing AI systems, as follows:\n\n  * **Supervised learning** – in this approach, scientists feed algorithms a dataset comprising data—for example, labels, text, numbers, or images—and then calibrate the algorithm to recognize a certain set of inputs as a particular thing. For instance, imagine feeding an algorithm a set of pictures of dogs, in which each picture contains a set of features that correspond to properties of the picture. Inputs to the algorithm could also include a number of images that are _not_ of dogs—for example, pictures of cats, pigeons, polar bears, pickup trucks, or snow shovels—and the corresponding properties of each of the _not-dogs_ images. Then, based on what the algorithm has learned about classifying images as _dog_ or _not dog_ through the features and properties of images, if you show the algorithm a picture of a dog it’s never seen before, it has the ability to identify that it is, in fact, a picture of a _dog_. The algorithm is successful when it can accurately recognize an image as a dog and reject images that are not dogs.\n\n  * **Unsupervised learning** – this approach attempts to find classes of similar objects in a dataset based on each object’s properties. When scientists give an algorithm a set of inputs that have parameters and values, it tries to find common features and group them. For example, scientists might feed an algorithm thousands of pictures of flowers with various tags such as color, stem length, or preferred soil. The algorithm is successful if it can group all flowers of the same type.\n\n  * **Reinforcement learning** – this approach trains an algorithm through a series of positive and negative feedback loops. Behavioral psychologists used this technique of feedback loops to train pigeons in lab studies. This is also how many pet owners train their animals to follow simple commands such as _sit_ or _stay_ and then reward them with a treat or reprimand them with a _no_. In the context of machine learning, scientists show an algorithm a series of images, and then as the algorithm classifies images—of, say, penguins—they confirm the model when the algorithm properly identifies a penguin and adjust it when the algorithm gets it wrong. When you hear about bots on Twitter that have gone awry, this is typically an example of reinforcement learning where the bots have learned to identify examples incorrectly, but the system thinks they are correct.1\n\nAlthough all ML techniques are useful and applicable in various contexts,\nlet’s focus on supervised learning.\n\n#### All data are not equal\n\nObtaining good training data is the Achilles heel of many ML scientists. Where\ndoes one get this type of data? Getting data from secondary sources is\nsurprisingly easy. There are many sources2 that provide access to thousands of\nfree datasets. Recently, Google launched a search tool to make publicly\navailable databases for ML applications easier to find. But it is important to\nnote that many of these databases are very esoteric—for example, “Leading\nAnti-aging Facial Brands in the U.S. Sales 2018.”3 Nonetheless, data is\nbecoming more accessible. While this supports educational endeavors, the\nability for businesses to use these databases for mainstream applicability may\nbe low.\n\nThese databases have limitations such as the following:\n\n  * They might not have precisely what ML researchers are seeking—for example, videos of elderly people crossing a street compared to children riding bicycles.\n\n  * They might not be tagged appropriately or usefully with the metadata that is necessary for ML use.\n\n  * Other ML researchers might have used them over and over again.\n\n  * They might not represent a rich, robust sample—for example, a database might not be representative of the population.\n\n  * They might lack enough cases/examples.\n\n  * They might not be very clean—for example, they could have lots of missing values.\n\nAs many researchers often say, all data are _not_ equal. The inherent\nassumptions and context that are associated with datasets often get\noverlooked. If scientists do not give sufficient care to a dataset’s hygiene\nbefore plugging it into an ML system, the AI might never learn—or worse, could\nlearn incorrectly, as we described earlier. In cases where the quality of the\ndata may be suspect, it’s difficult to know whether the learning is real or\naccurate. This is a huge risk.\n\nKnowing what we now know about machine learning and the risks and limitations\nof datasets, how can we mitigate these risks? The answer involves UX.\n\nPlaying Catch Up with Computing Speed when we Should be Slowing Down\n\nBOB: Recovering from failure and learning is necessary and part of how AI will\nevolve and succeed. But, recovering from failure requires significant overhaul\nto not do what did not “work” but review what “worked” as well.\n\nGAVIN: Yes. And consider how fast the technology is advancing. The faster AI\nadvances, in some ways, we lose the opportunity to think about ethical\nconsiderations or even about revisiting the foundations.\n\nConsider the evolution of the CPU. Under Moore’s Law, the number of\ntransistors in a CPU doubles every 2 years, but in AI’s case, computing power\nfor AI took advantage of the massively parallel processing of a GPU (graphics\nprocessing unit). These are the new graphics chips associated with making\nvideo games smoother and the incredible action movies we see today. Massively\nparallel processing required to present video games made AI much, much faster.\nAI systems often took months to learn the dataset. When graphics chips were\napplied to AI applications, training intervals dropped to single days, not\nweeks. There is barely enough time to stop and think about the results.\n\nBOB: If AI applications are to learn, they learn from consuming data. When one\nthinks of data, it is easy to assume that the AI application would take all\ndata into consideration. But, practically speaking, consuming data still takes\ntime. If the training sets are consumed faster, have we evolved our thinking\non the data itself or just the processing power?\n\nGAVIN: This is the trap. With all the emphasis on hardware and algorithmic\nadvances, my fear is that this only distracts from getting the foundation\nright first.\n\nBOB: Simply put, “garbage in, garbage out.” We are doing a disservice to AI if\nwe don’t think about what we are feeding it.\n\n**The point** Advances in AI will come, but are we taking time to understand\nthe data that we feed into the machine?\n\n### Getting custom data for machine learning\n\nWhile not all datasets relate to human behavior, the majority of them do.\nTherefore, understanding the behaviors that the data capture is essential.\nOver the last decade, our UX agency has been engaged by many companies to\ncollect data for custom datasets. This means we had to collect precise\nexamples and attribute tags that are necessary to train or prove in their AI\nalgorithms. (In some cases, thousands of data points are needed, which are\nsamples of different things.) Here are some examples of these samples:\n\n  * Video samples of people doing indoor and outdoor activities\n\n  * Voice and text samples of doctors and nurses making clinical requests\n\n  * Video samples of people stealing packages from porches\n\n  * Video samples capturing the presence or absence of people in a room\n\n  * Thumbprint samples from specific ethnic groups\n\n  * Video and audio samples of people knocking on doors\n\nNote that _none_ of this data was available publicly. We had to build each of\nthe datasets through custom research based on the specific intentions and\nresearch objectives of our clients.\n\nCustom Data for AI is a Big Deal\n\nBOB: When we received a request from a client to collect thousands of samples\nfor a custom dataset, we raised an eyebrow at how to approach this from a\npractical perspective. Thousands of people—face to face data collection! We\nwere to capture _in situ_ behaviors.\n\nGAVIN: After reviewing the specifications, the amount of precision required\nwas immense. Because participant demographics are always important to ensure\nsamples are representative to the target population, we would need many\nparticipants. For instance, a facial recognition AI on a smartphone or\ncomputer needs to learn to recognize data from the same participant in\ndifferent situations. They might have changed their appearance. So, we would\ncollect data with and without beards. They could wear different clothing or\ndifferent makeup, or different hair styles, and so on. We would systematically\nask participants to change their look to add additional samples to the data.\nThis would allow AI to learn about people, but also train it to recognize that\nthe same person can look different. We were asked to capture participants in a\nvariety of contexts. The amount of care placed on the ask was well thought\nthrough.\n\nBOB: It also extended to different continents too. At some point, we argued\nthat there were more cost-effective ways to collect this massive amount of\ndata than through a UX firm like ours, as we tended to collect data on a much\nsmaller basis. The response was that they understood, but that most large-\nscale data collection lacked the experimental rigor to capture what was needed\nfor their AI application. They wanted to use precision that was typically used\nin small sample research studies but replicated two orders of magnitude\nhigher.\n\nGAVIN: When we write about using datasets that were used for other projects or\ndatasets that are purchased and used for AI, the difference between that and\ncommissioning a custom dataset for a specific AI application is striking. It\nis one thing to “dust off” an old dataset and entirely another world to\nspecify what your AI would need to consume to properly train.\n\n**The point** While an existing dataset might describe people and behavior, a\ncustom dataset can be tuned to the elements that make AI smarter and better.\nCare for the details in the data makes AI better.\n\nUnderstanding the sheer magnitude of data collection necessary for effective\nML applications, it seems obvious that these datasets should be custom. But,\nhow much time, effort, and money are _actually_ spent on clean datasets\nrelative to the programming?\n\nFor many scientists and researchers, the easy way is to use data that already\nexists. But our clients who commissioned these projects understood a key\nshortcoming of these methods: low data integrity. The project sponsors\nrecognized that the underlying data had to be clean and representative of the\ndomain they were trying to model—carefully considering the nuances of captured\nexperiences. So, we needed to collect the behaviors in context and had to\n_observe_ them—not simply ask for a number on a five-point scale—as is often\nthe case in quantitative data collection. Apart from the obvious problems in\nsurvey research, we, as psychologists, understand that people often cannot\nreport on their own behaviors reliably. That is, we can’t often just ask\npeople to tell us what they did; we must observe and record. Capturing\nbehavior is the prerogative of UX and requires research rigor and formal\nprotocols. What we learned is that UX is uniquely positioned to collect and\ncode these data elements through our tested research methodologies and\nexpertise in understanding and codifying human behavior.\n\n## Data hygiene\n\nWhile this section may seem somewhat redundant with some of the material\ncovered in Chapter [3](470825_1_En_3_Chapter.xhtml), the dataset can be\nfraught with concerns, such as the following:\n\n  * Identification of missing cells that are filled with imputed data where cell tags (i.e., notations that this is imputed) are not passed on to the AI team\n\n  * Data that is purposefully and systematically unsurveyed so multiple participants are combined to complete a full survey (i.e., split questionnaire survey design).\n\n  * Surveys that are completed by bots, acting as humans4\n\nData scientists take data hygiene very seriously. What we are concerned with\nhere is that we have heard from those involved immersed in AI development that\nsometimes the preceding issues are glossed over. Let’s not assume the data is\nfree from elements that can skew the learning.\n\nThe point\n\nLet’s not assume the data—even if no cells are missing—is free from elements\nthat can bias outcomes and create unintended consequences.\n\nDoing a Disservice to AI\n\nBOB: The question for anyone who is working on AI applications is how much\ncare is spent on the data itself?\n\nGAVIN: This is a challenge because so many hands touch the data and when it is\npassed from survey designers to programmers to respondents to data scientists\nthen to AI technologists, who know exactly what was done to the dataset that\nmight be an artifact that will influence the AI application?\n\nBOB: We know that some data scientists tag fields that have been imputed, but\nby the time the data is washed and formatted for training the AI application,\nhas this knowledge been stripped away?\n\nGAVIN: There is a disservice to AI to have it trained on datasets where there\nmay be underlying flaws in the data..\n\n**The point** The dataset deserves a lot of scrutiny—ask questions on the\nmethodology, respondents, questions, design, and so on. This is what the AI\napplication will use to learn and all team members can play a role to give AI\nbetter data.\n\n## Black box implications\n\nAs described in Chapter [3](470825_1_En_3_Chapter.xhtml), one challenge with\nAI is that it does not reveal the meaning or rationale behind what it finds.\nIt is a classic “black box” where data goes in and an answer comes out, but\nthere is no description of why or how the answer came to be.\n\nAs mentioned above, potentially compounding the problem is the concern that\nthe data we think is obtained from humans, just might be from bots. Or in our\nefforts to make a complete dataset, we use imputed data where an equation or\nalgorithms were used to fill data elements. The concern is that any outcome\nobtained might simply be the result of the AI system reverse engineering the\nimputation algorithms used. Because AI is a black box, we are not able to\ninspect the “why” behind AI results. This takes away our ability to walk\nbackward through the AI application’s conclusions to find the underlying\nrationale. This can be problematic, especially considering how fast the\nbusiness world acts on AI findings.\n\nEthics is Best Early not Late in the Discussion\n\nBOB: When I started my career, there were companies that were considered\ninnovators and there were those that adopted a “wait and see” attitude about\n“fast following” of innovations.\n\nGAVIN: Today, these corporate philosophies still exist, but it seems that the\nbrand value of being innovative is much stronger, and this is driving\ncompanies to innovate faster and faster. Consider the practice of producing\nthe minimum viable product (MVP), where startups and monolithic companies\nalike are launching products with a bare minimum feature set hoping to capture\nthe attention of the marketplace and learn quickly from customers.\n\nBOB: One challenge of MVP is what happens if the product has been pared down\nsuch that isn’t very compelling in its MVP state? This is not only a UX and\nvalue proposition issue, but what concerns me more about AI is how quickly\ncompanies are moving to be first to market. Let’s say you are creating an AI-\nenabled application. You rush to get data from sources that make sense. The\ndataset is cleaned and used for training. After AI “trains” and presumably\n“learns,” it identifies an interesting finding. What does a company do next?\n\nGAVIN: A company that believes they are “innovative” will run to build a\nbusiness case, get funding, and build a product where AI is at the core. But\nwhat if the dataset is dodgy or biased due to poor sampling?\n\nBOB: You are talking about ethics in data. This is an area where AI has not\ndeveloped fully. Companies are building AI not for foundational science, but\nfor commercial advantage. The same sorts of issues that arise with bias in the\nculture also exist in the data. So the fear is that AI applications may have\nsubtle—or even not-so-subtle—biases because the underlying data contain\nbiases.\n\n**The point** Organizations are moving fast to build applications, but social\nand ethical considerations inherent in the data need to be addressed,\ndeveloped, and adopted.\n\nNext, let’s take a deeper look at ethics and AI through the lens of privacy\nand bias.\n\n## Ethics and AI\n\nThe ethics of AI is a relatively new area of discussion. Only recently has AI\nbecome mainstream enough that ethical considerations are beginning to take\nshape. There are no formal ethical standards or guidelines for AI. It is very\nmuch the proverbial “Wild West” where technology is being created without\nguardrails.5\n\nThe concern is that the “grist for the AI mill” (the data) could hide ethical\nconcerns. What data was used? Was the data universal? Did it have too much\nfocus on a region or socioeconomic level? If the training data contain bias,\nwould AI have an opportunity to revisit the underlying training or will it\nalways have a bias?\n\nLet’s look at two important points concerning ethics and AI: privacy and bias.\n\n### Privacy\n\nThe data science revolution is the centerpiece of major tech companies. As\noutlined by Facebook investor Roger McNamee, web startups like PayPal,\nFacebook, and Google have made massive inroads through a big-data first\napproach—using data to build more functional and successful products, then\nselling that data.6 Despite being well connected in the tech industry, McNamee\nsounded the alarm about tech companies’ big-data focus. He invoked the idea\nthat user privacy is actively being compromised by major tech firms in a way\nthat outweighs any benefits of their services. While privacy concerns haven’t\nstopped programs like Gmail and Facebook from becoming behemoths, they are an\never-present part of the discussion around issues of big tech, and AI may only\nexacerbate these fears. In 2010, then-Google CEO Eric Schmidt described\nGoogle’s capabilities in terms sure to scare any user concerned about their\nprivacy:\n\n> _We don’t need you to type at all. We know where you are. We know where\n> you’ve been. We can more or less know what you’re thinking about._7\n\nThis quote from a decade ago described how algorithms guided by fallible human\nbeings could extract untold insights from the data we all share online. When\nEric Schmidt was asked during an interview for CNBC’s “Inside the Mind of\nGoogle” special about whether users should be sharing information with Google\nas if it were a “trusted friend,” Schmidt responded:\n\n> _If you have something that you don’t want anyone to know, maybe you\n> shouldn’t be doing it in the first place._8\n\nWhen you consider what is in a dataset and how it is derived from human\nbehavior, this is a clear example of behavior and how it can be used to\nanalyze and predict future behaviors. The message that Schmidt might not be\nexplicitly describing is how much information Google really has. It is\ncertainly more than simply search terms, but geonavigation data, actual\nconsumer purchases, and email correspondences at the very least. And the rub\nof it all is that we give our consent to have this data collected by clicking\nthrough and accepting the policies. We’re all giving up our privacy for the\nputative benefits that the technology offers us.\n\nPrivacy can be divided into three different types:\n\n  * Big Brother privacy (keeping personal information from government or business entities)\n\n  * Public privacy (keeping personal information from coworkers or community)\n\n  * Household privacy (keeping personal information from family or roommates)\n\nEach of these three types of privacy has different impacts on UX.\n\nFor a long time, Big Brother privacy intrusions have mostly been tolerated by\nusers. After all, we’ve all had the experience of clicking through the Terms\nand Conditions for a new account or app without reading them. But, with the\nera of big data fully upon us, the issue seems to be gaining political\nsalience. This is best exemplified by the European Union’s GDPR privacy law,\none of the most prominent attempts to regulate big data. The GDPR is “based on\nthe concept of privacy as a fundamental human right.”9 Privacy and policy\nresearch director Michelle Goddard views the GDPR’s regulations on data\ncollection as an opportunity for data scientists, not a setback. She says the\nGDPR’s focus on ensuring privacy through “transparency” and “accountability”\naligns with privacy practices necessary for ethical research, including\nanonymizing personal data.10 AI, similarly, can focus on transparency to\ndispel user concerns about Big Brother privacy.\n\nPublic privacy is probably the least likely of these three forms to be\nviolated given the current political and mainstream concerns focused on big\nbusinesses like Google and Facebook, so let’s look at household privacy.\n\nHousehold privacy is most salient with programs or devices that are meant to\nstay at home or to be used by one user in particular, such as standalone\nvirtual assistants. If a user buys a virtual assistant device for their\nhousehold, it can lead to violations of household privacy. For example, the\nuser’s roommate might be able to read and respond to their texts, or their\nspouse might stumble upon an update on the delivery status of their secret\nanniversary gift. The desktop computers of a bygone era were a classic case of\npotential household privacy violations, which were resolved by the feature of\nindividual user profiles. A similar solution might help virtual assistants—but\nthe technology for a convenient profile solution on virtual assistants is\nstill evolving.\n\nMattel created a virtual assistant that offers a glimpse at a profile system.\nAristotle was Mattel’s virtual assistant, based on Amazon Alexa, which was\nintended to primarily serve children. The company planned to make Aristotle\ncapable of understanding a child’s voice and of differentiating it from adult\nvoices. Then, the device could offer limited capabilities to child users,\nwhile also offering adults the ability to use Alexa to do more complex tasks\nlike ordering childcare supplies.11 However, Aristotle was canceled in 2017,\nafter consumer advocates, politicians, and pediatricians objected. Big Brother\nprivacy concerns were one major reason for objections to Aristotle, along with\nconcerns about child development.12\n\nWhile Aristotle may not have come to fruition, an AI system like it that can\ndifferentiate users’ voices from one another and associate with an\nindividual’s profile is a solution to the problem of household privacy in\nvirtual assistants. There are other possible solutions, of course—perhaps a\nfuture assistant could determine who it is talking to by discovering whose\nsmartphone is in the room. In 2017, Google Home provided a feature where it\ncould distinguish from up to six different household members13 and Amazon’s\nAlexa followed suit in 2019 with “Voice Profiles.”14\n\nUsers’ expectations of privacy online can be slippery, as Microsoft principal\nresearcher Danah Boyd has pointed out. Boyd has written that users’\nexpectations of privacy online are most obviously violated when the context is\nstripped away from their actions and those actions are released to a wider\npublic than the user had intended. This leads the user to feeling a loss of\n“control over how information flows,”15 which results in user mistrust in the\ntechnology that removed the context.\n\nFor an example of how to build trust, let’s turn back to Spotify. The company\ncites data claiming that it is more trusted than its competitors, including\namong millennials. They cite “discovery” features like Discover Weekly and the\npartially neural-network-powered recommendation engine as a primary reason\nwhy.16 In an article directed at advertisers, Spotify claims that users are\nwilling to give a company personal information so long as it results in a\nuseful feature. Spotify’s recommendations are that useful feature.\n\nThe Spotify recommendation engine is built based only on data from Spotify\nitself, and it even allows users to enter a private mode in which Spotify\nwon’t count their streams. That means that users can simply take their guilty\npleasures elsewhere (might want to stream that Nickelback album in private\nmode or on YouTube) and make sure they don’t affect their recommendations.\nThis helps users trust that Spotify’s data collection serves a purpose for\nthem.\n\nAI Does not know where the “Line” is, So we Need to Draw IT\n\nGAVIN: This is a very difficult question to solve for businesses because\ncompanies have an obligation to their shareholders first, so AI-enabled\nproducts should be made with all data available to produce a compelling\nproduct.\n\nBOB: But, if users rebel, that will hurt the shareholders. Companies must\nstill balance privacy to not negatively impact their brand.\n\nGAVIN: This reminds me of another quote by Eric Schmidt. When he was asked\nabout whether Google would implant technology into the brain to get\ninformation, Schmidt said, “There is what I call the creepy line. The Google\npolicy on a lot of things is to get right up to that creepy line and not cross\nit.”17\n\nBOB: And let’s hope we can trust businesses to know where that line is.\n\n**The point** The need for AI to respect privacy comes from those who develop\nand market AI.\n\nPrivacy implications center around whether the data should be used in AI.\nLet’s explore the concept of bias that creeps into our dataset—even with the\nbest of intentions.\n\n### Bias in datasets\n\nEthical considerations and artificial intelligence date back to 1960 when\nArthur Samuel wrote in _Science_ about the moral consequences about a machine\nsimply making conclusions from logical consequences of the input it is\ngiven.18 Today, much of the focus on AI ethics is on the “what” (principles\nand codes) rather than on the “how” (practical application to AI). Ethics and\nAI have a long way to go.\n\n> _Awareness of the potential issues [of AI] is increasing at a fast rate, but\n> the AI community’s ability to take action to mitigate the associated risks\n> is still at its infancy._\n>\n> —Morley, Floridi, Kinsey, and Elhalal (2019)19\n\nHow Does AI Know what is Important?\n\nBOB: Let’s take a medical example where AI takes in data and learns. One could\nargue that the very best data is from peer-reviewed journal articles. Studies\ndescribed in these articles can be replicable (in theory), and medical science\nand careers advance through peer-reviewed publications.\n\nGAVIN: But let’s also consider generations of medical research from the 1960s\nand earlier where mostly men were participants. We have learned through the\nyears that women have differing symptoms from men for the same disease. For\nexample, women often delay seeking medical attention for a heart attack\nbecause they feel abdominal pain and not chest pain.20\n\nBOB: This opens the question of whether the dataset used for AI applications\nadequately weighs evidence. The process of publications is to build on what is\nknown. When a groundbreaking study is done, while it may be published in a\ntop-tier journal, it takes time for more articles to be published to both\nreplicate and further the science. And how does AI take groundbreaking results\ninto its learning when a preponderance of articles exists on the older\ntreatment?\n\nGAVIN: Yep. When corrections to the science are made post AI learning, does\nthe AI application get updated?\n\n**The point** How does the AI application “keep up with the literature” or\nsimply stay current when new data come to light?\n\nAs an example, in 2018, the FDA fast-tracked and approved a new “tissue\nagnostic” cancer drug for a specific genetic mutation. Oncologists said that\nthis new therapy would change the game, but how many studies need to be\npublished until AI applications adopt it as the therapy of choice?\n\nResearchers at the Memorial Sloan Kettering Cancer Center (MSKCC) who teamed\nup with IBM Watson sought to solve this question by creating “synthetic cases”\nthat were put into training datasets so IBM Watson could learn from their\ndata.21\n\nBias from what some Believe to be True\n\nGAVIN: Essentially, MSKCC and IBM Watson added new cases to their dataset.\nThey created records from their cases and placed them into research datasets\ncontaining other cases.\n\nBOB: Presumably, this would make IBM Watson become smarter because it would\nhave the benefit of MSKCC’s knowledge. This is often referred to as the “Sloan\nKettering way” of treating patients.\n\nGAVIN: So, these “synthetic cases” were given to IBM Watson so it would learn.\nDoesn’t this beg questions about whether these are common or unique cases, or\neven if MSKCC tends to receive a certain type of patient?\n\nBOB: And because this was the “training set” where the AI modeled and\n_learned_ , the bias can permeate future findings.\n\n**The point** Techniques to add “synthetic cases” to improve datasets may also\nadd bias as well.\n\nWe assume that peer-reviewed studies care for certain factors, such as\nrepresentativeness, and control for bias or at minimum state them as\nassumptions/qualifications to the findings. When “synthetic cases” are\ncreated, one must ask questions such as these:\n\n  * Are these synthetic patient cases representative for the domain?\n\n  * Are they typical cases? Or are the edge cases?\n\n  * Did these patients transfer to the institution because they needed the worse/last resort treatment solutions?\n\n  * Is there potential for social, economic, racial, or gender bias in selection for these cases?\n\nWhile this list merely nips around the edges of the potential for bias when an\ninstitution creates artificial or synthetic data to train AI, the need to\napply ethical standards in AI becomes clear and apparent.\n\nLet us be clear: MSKCC is one of the premiere cancer treatment centers in the\nworld, but as Pilar Ossorio, a professor of law and bioethics at the\nUniversity of Wisconsin Law School, argues, “ _[AI] will learn race, gender,\nand class bias, basically baking those social stratifications in and making\nthe biases even less apparent and even less easy for people to recognize.”_\nConsidering that patients who are attracted to MSKCC tend to be more affluent,\nhave a different mix of types of cancer, and have often failed multiple lines\nof treatment and are looking for one last chance,22 these biases are woven\ninto the very fabric of Watson’s AI.\n\nWhen the Watson team were pressed on concerns over the use of ‘synthetic\ncases’ to train IBM Watson, the response was striking. Deborah DiSanzo,\ngeneral manager, IBM Watson Health, replied, “ _The bias is taken out by the\nsheer amount of data we have._23 _”_\n\nConsidering how AI is a black box and we cannot truly know what data elements\nWatson’s AI algorithm used or did not use, the relevance of the volume of data\nas an answer that overcomes potential bias is speculation at best.\n\nThis is the issue with bias. It is often hard to see or incorporate into one’s\nthinking. As an example, Dr. Andrew Seidman, who was the MSKCC lead trainer\nfor IBM Watson, provided this answer to concerns over bias using MSKCC\n“synthetic cases” by proclaiming, _“We are not at all hesitant about inserting\nour bias, because I think our bias is based on the next best thing to\nprospective randomized trials, which is having a vast amount of experience. So\nit’s a very unapologetic bias.”_ This is why an ethical standard is needed and\nshould be applied. It can be difficult for some to be objective.\n\nTraining Data Sets the Foundation for AI Thinking\n\nGAVIN: The underlying concern is how pervasive bias can be when AI learns from\na dataset with a questionable foundation. AI only learns what you feed into\nits training dataset. There is a lot more to successful AI than simply its\nprogramming.\n\nBOB: Whether you bought the dataset and need to manage what is inside or you\ntook the time to curate your own dataset, the data is critical to the process.\nResponsibility is on the product and data scientists teams to ensure good data\nhygiene.\n\nGAVIN: Assume a result forms the basis for a product—one with AI at the core.\nHow many corporations would retrain on a new dataset following the product\nlaunch?\n\nBOB: There is a lot of risk on a complete retrain. What if the AI engine\ndoesn’t produce the same results with new training data? If it’s bad enough,\nit could sink the product. There are a lot of companies or product teams that\nmight not take that risk.\n\n**The point** Ethical standards are relevant today because AI is learning from\ndatasets now. These datasets need to consider inherent bias or risk weaving\nthat very bias into the foundation that is used to power the AI engine.\n\n## Toward an ethical standard\n\nOrganizations are concerned with the lack of an ethical standard for AI. In\n2018, the MIT Media Lab at the Massachusetts Institute of Technology joined\nforces with the Institute of Electrical and Electronics Engineers (IEEE), a\nNew Jersey-based global professional organization dedicated to advancing\ntechnology for humanity, and the IEEE Standards Association to form the global\nCouncil on Extended Intelligence (CXI). CXI’s mission is to promote the\nresponsible design and deployment of autonomous and intelligent technologies.\n\nThe IEEE welcomes engagement from those who wish to be part of the standards\ninitiatives. The IEEE’s Global Initiative’s mission is, “To ensure every\nstakeholder involved in the design and development of autonomous and\nintelligent systems is educated, trained, and empowered to prioritize ethical\nconsiderations so that these technologies are advanced for the benefit of\nhumanity.”\n\nThis organization drafted a downloadable report entitled Ethically Aligned\nDesign: A Vision for Prioritizing Human Well-being with Autonomous and\nIntelligent Systems, First Edition (EAD1e).24 This report sets the foundation\nfor an ethical standard for autonomous and intelligent systems. The IEEE\nP7000™ Standards Working Group standards projects listed as follows:\n\n  * **IEEE P7000** – Model Process for Addressing Ethical Concerns During System Design\n\n  * **IEEE P7001** – Transparency of Autonomous Systems\n\n  * **IEEE P7002** – Data Privacy Process\n\n  * **IEEE P7003** – Algorithmic Bias Considerations\n\n  * **IEEE P7004** – Standard on Child and Student Data Governance\n\n  * **IEEE P7005** – Standard on Employer Data Governance\n\n  * **IEEE P7006** – Standard on Personal Data AI Agent Working Group\n\n  * **IEEE P7007** – Ontological Standard for Ethically driven Robotics and Automation Systems\n\n  * **IEEE P7008** – Standard for Ethically Driven Nudging for Robotic, Intelligent and Autonomous Systems\n\n  * **IEEE P7009** – Standard for Fail-Safe Design of Autonomous and Semi-Autonomous Systems\n\n  * **IEEE P7010** – Wellbeing Metrics Standard for Ethical Artificial Intelligence and Autonomous Systems\n\n  * **IEEE P7011** – Standard for the Process of Identifying and Rating the Trustworthiness of News Sources\n\n  * **IEEE P7012** – Standard for Machine Readable Personal Privacy Terms\n\nThe point\n\nThere is an effort underway to develop ethical standards for AI.\n\n## Conclusion: Where to next?\n\nSo we have covered a couple of the concerns about the inputs to AI-enabled\nproducts. But we think there’s another place where there is opportunity and\nwhat could keep AI applications from getting a bad rap: the user experience.\nOne underlying theme that we touch on here and there is that there is a\ngiddiness, an infatuation at times, with the technology that we forget that at\nthe beginning and the end there is a user, a person. And, because we believe\nan AI application is just another application, we need to ensure that the AI\napplication is tuned not only to the data but to the user’s needs. The final\nchapter then describes the elements we feel will promote user engagement, and\nultimately increase the likelihood of marketplace success.\n\nFootnotes\n\n1\n\n[https://www.theverge.com/2016/3/24/11297050/tay-microsoft-chatbot-\nracist](https://www.theverge.com/2016/3/24/11297050/tay-microsoft-chatbot-\nracist).\n\n2\n\n[https://medium.com/towards-artificial-intelligence/the-50-best-public-\ndatasets-for-machine-learning-d80e9f030279](https://medium.com/towards-\nartificial-intelligence/the-50-best-public-datasets-for-machine-\nlearning-d80e9f030279).\n\n3\n\n[www.statista.com/statistics/312299/anti-aging-facial-brands-sales-in-the-\nus/](http://www.statista.com/statistics/312299/anti-aging-facial-brands-sales-\nin-the-us/).\n\n4\n\nIn fact, in one study we were involved in, as many as 10% of the respondents\nwere estimated to have been from bots built to take the survey to collect the\nincentive.\n\n5\n\nThere is some traction emerging: in June 2019, Oxford University announced a\ndonation of £150M to establish the Schwartzman Centre that will contain the\nInstitute of Ethics and AI. [www.ox.ac.uk/news/2019-06-19-university-\nannounces-unprecedented-investment-\nhumanities](http://www.ox.ac.uk/news/2019-06-19-university-announces-\nunprecedented-investment-humanities) (retrieved March 1, 2020).\n\n6\n\nMcNamee, Roger. “A Brief History of How Your Privacy Was Stolen.” The New York\nTimes. June 3, 2019. Accessed June 3, 2019.\n[www.nytimes.com/2019/06/03/opinion/google-facebook-data-\nprivacy.html](http://www.nytimes.com/2019/06/03/opinion/google-facebook-data-\nprivacy.html).\n\n7\n\nSaint, Nick. Google CEO: “We Know Where You Are. We Know Where You’ve Been. We\nCan More Or Less Know What You’re Thinking About.” Business Insider. October\n4, 2010. Accessed June 25, 2019. [www.businessinsider.com/eric-schmidt-we-\nknow-where-you-are-we-know-where-youve-been-we-can-more-or-less-know-what-\nyoure-thinking-about-2010-10?IR=T](http://www.businessinsider.com/eric-\nschmidt-we-know-where-you-are-we-know-where-youve-been-we-can-more-or-less-\nknow-what-youre-thinking-about-2010-10%253FIR%253DT).\n\n8\n\nEsguerra, Richard. “Google CEO Eric Schmidt Dismisses the Importance of\nPrivacy.” Electronic Frontier Foundation. December 10, 2009. Accessed February\n16, 2020. [www.eff.org/deeplinks/2009/12/google-ceo-eric-schmidt-dismisses-\nprivacy](http://www.eff.org/deeplinks/2009/12/google-ceo-eric-schmidt-\ndismisses-privacy).\n\n9\n\nGoddard, Michelle. “The EU General Data Protection Regulation (GDPR) is a\nEuropean regulation that has a global impact.” International Journal of Market\nResearch 59/6 (2018).\n[https://journals.sagepub.com/doi/10.2501/IJMR-2017-050](https://journals.sagepub.com/doi/10.2501/IJMR-2017-050).\n\n10\n\nGoddard, “The EU.”\n\n11\n\nWilson, Mark. “Mattel is building an Alexa for kids.” Fast Company. January 3,\n2017. Accessed June 25, 2019. [www.fastcompany.com/3066881/mattel-is-building-\nan-alexa-for-kids](http://www.fastcompany.com/3066881/mattel-is-building-an-\nalexa-for-kids).\n\n12\n\nVincent, James. “Mattel cancels AI babysitter after privacy complaints.” The\nVerge. October 5, 2017. Accessed June 25, 2019.\n[www.theverge.com/2017/10/5/16430822/mattel-aristotle-ai-child-monitor-\ncanceled](http://www.theverge.com/2017/10/5/16430822/mattel-aristotle-ai-\nchild-monitor-canceled).\n\n13\n\nBaig, Edward C. “Google Home can now tell who is talking.” USA Today. April\n20, 2017. Accessed February 16, 2020.\n[www.usatoday.com/story/tech/talkingtech/2017/04/20/google-home-can-now-tell-\nwhos-\ntalking/100693580/](http://www.usatoday.com/story/tech/talkingtech/2017/04/20/google-\nhome-can-now-tell-whos-talking/100693580/).\n\n14\n\nJohnson, Jeremy. “How to setup Amazon Alexa Voice Profiles so it knows you are\ntalking.” Android Central. November 26, 2019. Accessed February 16, 2020.\n[androidcentral.com/how-set-amazon-alexa-voice-profiles-so-it-knows-its-you-\ntalking](http://androidcentral.com/how-set-amazon-alexa-voice-profiles-so-it-\nknows-its-you-talking).\n\n15\n\nBoyd, Danah. “Privacy, Publicity, and Visibility.” 2010. Microsoft Tech Fest,\nRedmond, WA. Accessed June 4, 2019.\n[www.danah.org/papers/talks/2010/TechFest2010.html](http://www.danah.org/papers/talks/2010/TechFest2010.html).\n\n16\n\n“Trust Issues: Spotify’s Commitment to Fans and Brands.” Spotify for Brands.\nAccessed June 15, 2019. [www.spotifyforbrands.com/en-US/insights/trust-issues-\nspotifys-commitment-to-fans-and-brands/](http://www.spotifyforbrands.com/en-\nUS/insights/trust-issues-spotifys-commitment-to-fans-and-brands/).\n\n17\n\nSaint, Nick. “Eric Schmidt: Google’s Policy Is To ‘Get Right Up To The Creepy\nLine And Not Cross It’.” October 1, 2010. Last accessed February 16, 2020.\n[www.businessinsider.com/eric-schmidt-googles-policy-is-to-get-right-up-to-\nthe-creepy-line-and-not-cross-it-2010-10](http://www.businessinsider.com/eric-\nschmidt-googles-policy-is-to-get-right-up-to-the-creepy-line-and-not-cross-\nit-2010-10).\n\n18\n\nSamuel, Arthur L. (1960). “Some Moral and Technical Consequences of\nAutomation—A Refutation.” American Association for the Advancement of Science.\n132(3429):741–742, 1960.\n[https://doi.org/10.1126/science.132.3429.741](https://doi.org/10.1126/science.132.3429.741).\n\n19\n\nMorley, J., Floridi, L., Kinsey, L. & Elhalal, A. (2019). “From What to How:\nAn Initial Review of Publicly Available AI Ethics Tools, Methods and Research\nto Translate Principles into Practices” _Science and Engineering Ethics_.\nDecember 11, 2019. Last accessed February 16, 2020.\n[https://link.springer.com/article/10.1007/s11948-019-00165-5#Sec2](https://link.springer.com/article/10.1007/s11948-019-00165-5%2523Sec2).\n\n20\n\nDeFilippis, Ersilia M. “Women can have heart attacks without chest pain. That\nleads to dangerous delays.” Washington Post. February 16, 2020. Last accessed\nFebruary 16, 2020. [www.washingtonpost.com/health/women-can-have-heart-\nattacks-without-chest-pain-that-leads-to-dangerous-\ndelays/2020/02/14/f061c85e-4db6-11ea-9b5c-eac5b16dafaa_story.html](http://www.washingtonpost.com/health/women-\ncan-have-heart-attacks-without-chest-pain-that-leads-to-dangerous-\ndelays/2020/02/14/f061c85e-4db6-11ea-9b5c-eac5b16dafaa_story.html).\n\n21\n\nStrickland, Eliza (2019). “How IBM Watson Overpromised and Underdelivered on\nAI Health Care.” IEEE Spectrum. April 2, 2019. Last accessed November 6, 2019.\n[https://spectrum.ieee.org/biomedical/diagnostics/how-ibm-watson-overpromised-\nand-underdelivered-on-ai-health-\ncare](https://spectrum.ieee.org/biomedical/diagnostics/how-ibm-watson-\noverpromised-and-underdelivered-on-ai-health-care).\n\n22\n\nGorski, D. (2019). “IBM’s Watson versus cancer: Hype meets reality.” _Science\nBased Medicine._ September 11, 2017. Last accessed February 16, 2020.\n[https://sciencebasedmedicine.org/ibm-watson-versus-cancer-hype-meets-\nreality/](https://sciencebasedmedicine.org/ibm-watson-versus-cancer-hype-\nmeets-reality/).\n\n23\n\nStrickland, Eliza (2019). “How IBM Watson Overpromised and Underdelivered on\nAI Health Care.” IEEE Spectrum. April 2, 2019. Last accessed November 6, 2019.\n[https://spectrum.ieee.org/biomedical/diagnostics/how-ibm-watson-overpromised-\nand-underdelivered-on-ai-health-\ncare](https://spectrum.ieee.org/biomedical/diagnostics/how-ibm-watson-\noverpromised-and-underdelivered-on-ai-health-care).\n\n24\n\n[https://ethicsinaction.ieee.org/#set-the-\nstandard](https://ethicsinaction.ieee.org/%2523set-the-standard).\n\n\n© Gavin Lew, Robert M. Schumacher Jr. 2020\n\nG. Lew, R. M. Schumacher Jr. AI and UX\n<https://doi.org/10.1007/978-1-4842-5775-3_5>\n\n# 5\\. Applying a UX Framework\n\nA pathway for AI’s success\n\nGavin Lew1 and  Robert M. Schumacher Jr. 2\n\n(1)\n\nS Barrington, IL, USA\n\n(2)\n\nWheaton, IL, USA\n\nWhen we look at the ground we have covered so far in this book, we see how AI\nand UX have some common DNA. Both started with the advent of computers and\nboth with a desire to create a better world. We saw how UX evolved from a need\nto bring the information age closer to everyone.1 AI grew similarly—with some\nfits and starts—and is now in the mainstream of conversation.\n\nThe benefits AI can bring are legion. However, there is a risk of another AI\nwinter for reasons that have less to do with potential and more to do with\nperception. For many people, there’s still a hesitance, a resistance, to adopt\nAI. Perhaps it is because of the influence of sci-fi movies that have planted\nimages of Skynet and the Terminator in our minds, or simply fear of those\nthings that we don’t understand. AI has an image problem. Risks remain that\npeople will get disillusioned with AI again.\n\nWe believe that AI is ready. AI is more accessible than ever before and not\njust to the big players in industry, but within reach of many from startups to\navid technophiles who are able to experiment with AI tools. This means AI is\nbeing embedded into new product ideas across almost all industries.\n\nBut, AI needs to be more than technology—the prescription for success is to\nnot just embed AI into the product but also to deliver a solid user\nexperience. We believe that this is the key to success.\n\nNo One Sets Out to Build Terrible Experiences\n\nBOB: Let’s get something straight. No company wants to build a product with a\nterrible experience.\n\nGAVIN: But, think about it. One does not have to work too hard to remember\nexperiences where you caught yourself saying, “What were they thinking when\nthey made this?”\n\nBOB: Several years ago, I used to give a talk that was more on the foundations\nof UX. Every application’s user interface presents an experience. What we need\nto realize is that there is a designer behind the application and it’s the\ndesigner’s choice as to what that experience is: it can be amazing or a dud.\n\nGAVIN: I don’t really think that programmers wake up in the morning and say,\n“I’m gonna make things just a little harder for those users.” But here’s the\nthing, if programmers don’t set out to create bad user experiences, why are\nthere so many bad user experiences?\n\nBOB: Yup, that is a paradox. There are a lot of reasons why so many products\nhave such mediocre experiences—cost, awareness of users, time, laziness, and\nso on.\n\nGAVIN: I would say though that most product owners underestimate how hard it\nis to design good experiences.\n\nBOB: Luckily, technology has advanced to where microwaves and clock radios no\nlonger default to flashing “12:00.” Fixing the time after a power outage was\nso annoying. But, just this last Christmas, I spent hours trying to set up new\ngadgets for my house only to scream out in frustration.\n\nGAVIN: I believe experiences matter. A product can promote the most amazing\nfeatures, but thinking of AI-UX principles, when I set it up or use the\nproduct, is the interaction intuitive? Do I trust that the product works?\nThink of products you love and use every day—how much of that is because the\nuser experience isfrictionless and enjoyable?\n\nBOB: Technology has become a commodity. What can set a product apart is good\ndesign. The same logic applies to AI-enabled products.\n\n**The point** Designing for simplicity is hard; it takes commitment.\n\n## What makes a good experience?\n\nEveryone has these digital things scattered around the house in closets,\nbasements, and workspaces. Think of a product recently purchased:\n\n  * Was it easy to order?\n\n  * Was it easy to set up?\n\n  * If you used instructions, did they help? (Were they even necessary?)\n\n  * Could you get the product to work quickly?\n\n  * Did it work like you thought it would?\n\n  * Do you still use it or does it sit idle after a month?\n\nWhat is it about these products that fail? While there can be many reasons for\nwhy products do not meet expectations, all too often, we find ourselves\nshaking our head and asking,\n\n> _What were they [designers, engineers, and product people] thinking when\n> they designed this? It doesn’t work the way I think._\n\nAgain, manufacturers do not set out to make products that disappoint—but they\nexist. Why? Sometimes, the simple answer is that the product creators did not\nspend enough time on the true need. Put in a different way, they built the\nproduct because they felt the technology was so compelling that they assumed\neveryone else would be captivated by it.\n\nOften users find novel ways to use products far different from what the\ncompany or organization expected.\n\nDo People Still Buy Alarm Clocks?\n\nGAVIN: I believe watches have always been fashionable, but now, we don’t use\nwatches to tell time any more. We use them as fashion accessories. The\nfunctionality of my watch has been replaced by my mobile phone.\n\nBOB: And your phone tends to be more accurate than your old Timex watch too!\n\nGAVIN: This is a powerful example because the ubiquity of mobile phones really\nchanged behavior. Think about early mobile phones. Some phones let you set an\nalarm as a feature. Now, you can set multiple alarms on your phone. I can even\nsay, “Hey, Google, set an alarm at 7 a.m.” And she replies, “Got it. Alarm set\nfor 7 a.m. tomorrow.”\n\nBOB: More to the point, did the early phone manufacturers think that their\nproduct would have decreased sales for physical alarm clocks or result in\npeople not wearing watches as often as they did 20 years ago?\n\n**The point** Even the most seemingly mundane features can change behaviors\nand change markets for products. When people use a product, their expectations\nand behaviors change in unanticipated manners.\n\n### Understanding the user\n\nHow do we better understand how people use products? A better understanding of\nthe user experience is the answer. The ultimate purpose of a user-centered\ndesign (UCD) is to build products and services around users and their needs.\n\nWe have been involved in research and design of all manner of applications and\nproducts for decades. We have also seen many different phases and approaches\nto user interface design. The method that we see with most success is a user-\ncentered design.\n\nDefinition\n\n**User-centered design** (UCD) places user needs at the core. At each stage of\nthe design process, design teams focus on the user and the user’s needs. This\ninvolves a variety of research techniques to understand the user and is used\nto inform product design.\n\nUCD2 advocates putting the user and the user’s needs as primary during each\nphase of the design process. While this seems obvious that one would design\nwith the user in mind, the reality is that surprisingly few development\nefforts strictly follow this process end to end.\n\n### Research matters\n\nIt is hard to disagree with the idea that any product is better if the\nintended user has a good experience. Designing these seamless experiences is\nthe result of hard work that maps what the user expects and needs into the\ndesign and interaction models for the product.\n\nUser research is the key method to capturing the needs of the intended user\nand integrating these insights into the product design.\n\nHard to Call a Baby Ugly\n\nBOB: In order for UCD to really work, user research is needed. I have heard\nmany creative and design directors say that “they _know_ what the user needs!”\nBut, the reality is that having evidence describing user needs is better than\none’s belief, however adamant one says it.\n\nGAVIN: And at the very least, designers should be humble as they recognize\nthat iteration can only make things better. Test early versions with\nprospective users. Get feedback early and often. Don’t be afraid to be wrong.\nKnow that the design will be better with this feedback.\n\nBOB: And let someone else do the research. Let an objective party evaluate the\ninitial designs. It is too easy to fall in love with something that you have\npoured time and energy into building. The product becomes “your baby,” and\nlet’s face it—it is hard to hear someone call your baby ugly. You might ignore\nthe criticism, or you might get defensive to protect it.\n\nGAVIN: But this is the best thing to hear—especially early in the design\nprocess. The parts that are “ugly” appear as confusion or outright frustration\nwhen naive users interact with your product. Learn from what users think and\nimprove the design.\n\n**The point** Make mistakes faster by having users interact with early-stage\ndesigns. Then, improve and repeat.\n\n### Does research really matter?\n\nSometimes, the best argument against a belief is a rhetorical question. The\nmerits of user research are often put down by those who believe that\ncreativity and innovative thinking does not need evidence-based UCD by\nrepeating an adage often attributed to Henry Ford:\n\n> _If I had asked people what they wanted, they would have said faster\n> horses._\n\nThe question “does research really matter?” places people into two groups:\nthose who believe true innovation comes from gifted visionaries and those who\nbelieve that understanding what people think matters to sound design.\n\nAs a researcher, this is not even a good question because it is leading (i.e.,\ndirects the answer in a particular direction and is biased). While there are\nsome who might be truly visionary, the reality is that all too often, research\nis the evidence that drives the innovation, the need that can be filled.\nIdentifying this need and designing around it is where user research is best\nsuited.\n\nWhere were the Horses?\n\nBOB: When I think of the Henry Ford’s often recited adage, I shake my head.\nRhetorically, it implies insight cannot be obtained from the user. If Henry\nFord had asked the question, he would have not made the Model T.\n\nGAVIN: I think the time Henry Ford would have said this is in the mid-1920s.\nThink of _The Great Gatsby,_ which was set at around the same time.\n\nBOB: Whether in the book or in its theatrical interpretations, automobiles\nwere prominent.\n\nGAVIN: Exactly. Where were the horses? In _The Great Gatsby_ , no one talked\nabout horses. Driving and owning an automobile was a key theme.\n\nBOB: Remember, Henry Ford did not invent the automobile. He invented a\nconveyor belt system to make cars more efficiently.\n\nGAVIN: And that is why Henry Ford _never said that quote._3 What he did say\nwas, “Any color, so long as it is black.”\n\n**The point** While great innovations can indeed come from gifted designers,\nobtaining user feedback on designs early and often is a recipe for success.\n\n### Objectivity in research and design\n\nUser research is the key method to capturing the needs of the intended user\nand integrating these insights into the product design.\n\n## The UX lens\n\nMany readers of this book may not have the skill (or inclination!) to debug\nPython code, but we believe that there is much more to AI than the code.\nSufficient attention is usually given to the AI engine proper. AI-enabled\nproducts can benefit by shifting attention to everything _around_ the AI\nengine. How can we improve everything else?\n\nLet’s Focus Away from the AI Output and on the Experience\n\nBOB: A common output of AI is a number or coefficient, such as 0.86: a\ncorrelation between 0 and1. Let’s take your credit card fraud example. You’re\nout to dinner and make a payment using your credit card…\n\nGAVIN: So, as my payment transaction is processing, there is an AI program\nanalyzing and thinking about fraud. In this example, AI outputs 0.86 and this\nmeans the transaction could be fraudulent (i.e., the algorithm assumes\nanything above .8 is likely to be fraud).\n\nBOB: That is the extent of AI. The outcome was 0.86. But the _experience_ with\nthe AI-enabled fraud detection product is an alert to my phone in the form of\na text message.\n\nGAVIN: It could read, “WARNING. POTENTIAL FRAUD DETECTED ABOVE 0.80. CODE\nF00BE1DB.”\n\nBOB: Or someone could spend time designing a better interaction**.** The\nmessage could be much more friendly and provide the user with a call to\naction, such as “To authorize this purchase, Reply Yes.”\n\n**The point** Work on what touches the user, such as messaging and user\ninteractions. What AI provides can be quite amazing, but what makes the\nproduct a success is the experience.\n\nWe think AI can be seen through the lens of how we look at the user experience\nof any product or application. AI is no different. To be successful, it must\nhave the essential elements of utility, usability, and aesthetics.\n\nSo what are the principles and what is that process?4\n\n### Key elements of UX\n\nUX is not one thing for all users. It is multivariate. The various constituent\nparts, which we introduce next, combine to provide the user with an experience\nthat is beneficial and maybe even delightful.\n\n#### Utility/functionality\n\nProbably the most important thing that defines any application is what it\ndoes—we call this “utility” or “functionality” or “usefulness.” Basically, is\nthere a perceived functional benefit? In more formal terms, does the\napplication (tool) fit for the purpose it was designed for? Does it do what\nthe user needs? A hammer is good for pounding nails, not so good for putting\non makeup. Any application needs to have the features and functions that a\nuser expects and needs to be useful. These are table stakes for a successful\nproduct.\n\nStrive to Create Frictionless Experiences\n\nBOB: Let’s go back to the car example with a natural language voice assistant.\nThe table stakes here is that AI needs to reliably understand human speech. In\na car, it could be touchless controls like “call mom,” “turn on the heater,”\n“find a destination on the navigation system,” and so on.\n\nGAVIN: Earlier speech recognition in cars was very command driven and\ntypically the user needed to know the exact voice command to say. Do you say,\n“Place call…” or “Make call…” or “Dial…” or “I want to call…” or even “Can you\nplease call…”\n\nBOB: The concept of using your voice has a benefit because the driver can keep\ntheir eyes on the road. But, too often, the rigid structure required the\ndriver to remember the phrase. And, as the driver guesses commands, random\nnoise can interfere. The driver might have been correct on the command, but\nthe external noise caused the system to reply, “Sorry, I did not understand.”\n\nGAVIN: Instead, a better user experience would be to design an interaction\nthat accepts many alternative commands in the presence of typical ambient\nnoises found in cars.\n\n**The point** Designing frictionless or effortless interactions can allow\nusers to experience the utility and functionality. Early attempts to put AI-\nbased speech recognition in cars had poor usage not because voice activation\nwould not be beneficial, but possibly because the driver found it difficult to\nengage.\n\n#### Usability\n\nUtility and usability are often conflated, but they are independent\nconstructs. Let’s consider an absurd example, steering a car. We all can use a\nsteering wheel to turn a car to the right or to the left. But if you think\nabout it, there are other ways in which a car could be controlled. A keyboard\nwhere you would type “turn right” or a joystick or a remote control. There is\na difference between function and how that function is implemented. Some of\nthose ways are more usable than others.\n\nDefinition\n\nUsability is defined as whether a product enables the user to perform the\nfunctions for which it is designed efficiently, effectively, and safely.\n\nBecause car manufacturers aligned to a steering wheel, people who know how to\ndrive can rent a car and, after some seat and mirror adjustments, drive a make\nand model of a car they have never driven before with a high degree of\nproficiency. These are standards that help guide how people interact with\ndifferent instantiations of a system.\n\nBut what about new or systems requiring new controls? How do you create an\n**interaction** that is **usable**?\n\nThe basics of a **UCD process** , involve:\n\n  1. 1.\n\nEarly-stage research (exploratory or discovery) to capture user expectations\nand understand what people think.\n\n  2. 2.\n\nConstruction of a prototype.\n\n  3. 3.\n\nFormative research, such as a **usability test** , to identify areas of\nconfusion and gaps preventing a satisfactory experience.\n\n  4. 4.\n\nIterative design and more **usability tests** to further refine the product.\n\nDefinition\n\nA **usability test** is a qualitative study where intended users who are naive\nto the product are given context and asked to complete tasks for which the\nproduct was designed. Behaviors and reactions are observed. A usability test\ntypically involves small sample sizes and feeds into an iterative design\nprocess.\n\nMore detail will be provided in the **Principles of UCD** section later in\nthis chapter**.**\n\nNatural Gestures are a Myth\n\nBOB: Apple said swiping and pinching on a phone are _natural gestures_ where\npeople just _intuitively_ know how to do it.\n\nGAVIN: Really? I remember 2007 playing with my first iPhone. I saw the\ncommercials by AT&T. This is where I learned to swipe and pinch.\n\nBOB: Those commercials might have been the most expensive user manuals ever\nmade!\n\nGAVIN: Apple has patented scores of gestures that it calls natural. You have\nto take a look at Apple’s patents like “multi-touch gesture dictionary.”5 My\nfavorites are Apple’s patents for _print_ and _save._ For _print_ , you put\nthree fingers together, say, at the center, and branch out to form the points\nof a triangle. But to _save,_ you do the opposite. Three fingers stay apart\nlike points on a triangle and move inwards to the center. How is this\nsomething people know?\n\nBOB: If these gestures were so natural, then why did Apple go to the trouble\nof patenting so many?\n\nGAVIN: This is how myths are made. This is a 2009 study that describes the\ngestures that people know.6 But, we did studies on multi-touch mobile phones\nin 2006 and 2007 where I saw people struggle. In 2009, people may have learned\nand adapted, but to say gestures have always been known and are natural is\nhard for me to reconcile from what I saw in the lab on the LG Chocolate, Palm,\nand first iPhone.\n\n**The point** Not everything is natural or springs to mind when someone\ninteracts with technology. Research and design go hand in hand to make the\nexperience usable.\n\nThere are many products that have the same functions but those functions are\nrendered in different ways; some ways being more usable than others. An\nobvious example is booking a seat on an airline. Many sites will sell the\nexact same seat, they all do it a little differently, and some do it better\nthan others. The function or purpose is the same, but the usability is\ndifferent.\n\n##### UX, AI, and trust\n\nFor a moment, let’s consider one of our **AI-UX principles** : **trust**. One\nof the reasons that AI often fails to deliver for the users is that the\noutputs are off point. That is, we can’t trust what we see or hear. The two\ndimensions of UX (utility and usability) speak directly to trust. If we build\nan app that lacks sufficient utility or suffers from poor usability, users\nwill fail to trust the app and abandon. The same will happen with an AI app.\n\nWe find Alexa to have a lot of utility in some specific areas—giving the time,\nplaying the news, telling the weather, and so on. And for these tasks, Alexa\nis also very usable. Thus, in a narrow, but important way, Alexa serves the\nuser well. Yet, there are tens of thousands of skills available. We have tried\ndozens of skills that we thought would be useful, but they were deleted\nquickly because they were not usable. One new skill Bob tried to get sports\nscores, launched hisRoomba by mistake; needless to say, he appreciated the\nclean floor, but it wasn’t the Roomba app that I launched. It’s aggravating at\nbest.\n\nAdmittedly, designing for voice is hard and, often, the design fails in two\nways on usability; the natural language processing is simply not robust enough\nto handle many cues, and the voice designers have not done a good job of\ndesigning the dialog. Many of these voice skills also fail on utility because\nthey do not sufficiently anticipate the users and the use cases. Thus, Alexa\nis relegated not to do much more than set a timer or gather information for a\nshopping list.\n\nMistakes are Okay for People, But not so with AI\n\nBOB: We both started out in telecommunications. Back when phones were wired\nand could survive a hurricane. Even when the power went out in the\nneighborhood, your phone usually still worked. Back when we were at Ameritech,\nwe did an internal study on dialing. What we found was that people were about\n98% accurate when dialing any single number.\n\nGAVIN: That is pretty accurate, but when dialing seven or now ten digits to\ncomplete a call, an error can occur. Doing the math, it happens maybe once or\ntwice every dozen or so times you manually enter a phone number.\n\nBOB: Precisely. Even with what seems to be a highly practiced task, errors\nhappen. Who can say that when dialing a number from a piece of paper that they\nhave _never_ accidentally made a mistake?\n\nGAVIN: Imagine a **voice assistant** that is always listening for that wake\nword. Even with a high accuracy rate, mistakes happen.\n\nBOB: When a person makes a mistake, we say, “Oh, I fat-fingered the number.”\nBut humans are less forgiving of machines. If machines make “fat-finger”\nerrors, we are more apt to say, “This thing sucks!”\n\n**The point** Because people have such a low tolerance for mistakes made by\nmachines, **trust** is very important to the success of any AI-enabled\nproduct.\n\n**Trust** has a very powerful influence on perceived success. Consider the\nchallenge posed by autonomous driving cars. In 2017, there were over 6.4\nmillion vehicle crashes in the United States. That is a crash every five\nseconds by human drivers. According to Stanford University, 90% of motor\nvehicle accidents are due to human error.7 What is the potential benefit to\nautonomous vehicles? In California, there are 55 companies with self-driving\ncar permits to conduct trials. From 2014 to 2018, there have been a total of\n54 accidents from these autonomous vehicles, and according to an Axios report,\nall but one were due to human driver error—not the AI.8\n\nBut the problem is that humans have **trust** issues with autonomous vehicles.\nAccording to a AAA study, 73% of respondents expressed lack of trust in the\ntechnology’s safety.9 However, the thesis of the user experience playing a\npivotal role in user acceptance still holds. “Having the opportunity to\ninteract with partially or fully automated vehicle technology will help remove\nsome of the mystery for consumers and open the door for greater acceptance,”\nsaid Greg Brannon, AAA’s director of automotive engineering and industry\nrelations. The AAA study further stressed that “experience seems to play a\npivotal role in how drivers feel about automated vehicle technology, and that\nregular interaction with advanced driver assistance systems (ADAS) components\nlike lane keeping assistance, adaptive cruise control, automatic emergency\nbraking and self-parking, considered the building blocks for self-driving\nvehicles, significantly improve consumer comfort level.”\n\nThe point\n\nExperience and trust matter. Positive experiences with AI-enabled driving\nfeatures can improve **trust** in more advanced AI technologies.\n\n#### Weirdness\n\nAs AI-enabled products proliferate, more information is available to the\nsystem to be analyzed. As AI thinks about your data, how should AI engage\n_without making it weird_?\n\nBy weird, we mean situations that can make human to AI interactions\nuncomfortable, awkward, or unusual.\n\nWhen AI Gets Weird…\n\nGAVIN: Imagine your daily commute in your car. There is a lot of data that can\nbe collected about your driving habits, from what you are listening to on the\nradio to your speed to route taken—all time and date stamped. The car even\nknows if you are wearing your seatbelt.\n\nBOB: And based on the key fob you are using, it probably can distinguish\nbetween different drivers. With all of this data—all collected passively in\nthe background while you drive to work.\n\nGAVIN: Now, if an AI system could recognize patterns in your behavior.\nLogically, AI could use this insight to be proactive and save you time and\neffort. For example, say, there is a major traffic jam on your normal route to\nwork, it could inform the driver and even suggest an alternative route.\n\nBOB: I would expect there is high value in this _suggestion_ based on AI\nconstantly thinking about me and my commute to work. This would be a\n_frictionless_ experience for the user.\n\nGAVIN: But, it could suggest a lot of things from my patterns. Perhaps I go to\nthe gym every other day after work. It could inquire if I want directions to\nthe gym. But, where is the line on what to suggest? Say, I go to places that I\nwould rather not have AI _offer suggestions?_\n\nAI making a _recommendation_ could freak me out. There needs to be a\n“weirdness scale” to designate what is appropriate and helpful from what is\ndownright creepy.\n\n**The point** AI has the potential to offer suggestions based on personal\npatterns of behaviors and habits, but where is the line between what is\nacceptable and unacceptable? This is a good example of why AI and UX need to\nbe linked.\n\n##### A weirdness scale\n\nThe concept of creating a continuum of inappropriate to appropriate actions is\nnot about precision, but to present a user-centered mindset to what actions AI\nshould and should not do. This weirdness scale would ground the product team\nto reflect on the appropriateness of AI suggestions. Patterns formed based on\nuser behaviors, both explicit (e.g., a driver entering an exact address for\ndirections) and passive (e.g., the car enters a shopping area and parks for 45\nminutes before heading home), are target-rich opportunities for an AI system.\nKnowing whether or not to trigger an action can be shaped and guided through a\n**UX lens**.\n\nDefinition\n\nWhen a pattern is identified and your AI product is ready to offer a\n_recommendation,_ the event can be called a **trigger**. The actions that\nfollow can be shaped with the user’s needs at the center.\n\nHaving a concept such as this **weirdness scale** during the design stage of\nany AI-enabled product has value. It can set the perspective for the team that\nthere are **triggers** that should and should not be acted upon. It\nestablishes for the product team that there are boundaries that should be\nconsidered. This can lead to **interactions** with the user to help refine the\nAI engine by capturing the times when the user explicitly pressed or said “No”\nto the recommendation. The team can brainstorm and anticipate the possible\n**triggers** to illustrate the breadth of the scale and to define “guardrails”\nthat should be recognized.\n\nWhen you consider the concerns over “Big Brother” fears that AI systems are\nalways _watching_ or _listening_ (even when there is little evidence to\nsupport that **voice assistants** are actually “always listening”10), care\nmust be given by the team designing the AI-enabled product to improve\n**trust** and subsequent adoption and usage. This is not about what AI can\npredict, but the responsibility of the team to the design the product using an\nAI-UX perspective.\n\nThe point\n\nAI has the potential to offer suggestions based on your habits, but where is\nthe line between what is acceptable and unacceptable? This is a good example\nof why AI and UX need to be linked and is in the hands of the team making the\nproduct.\n\n#### Aesthetics/emotion\n\nAs humans, we prefer to use things that are aesthetically pleasing.11 Imagine\ntwo websites that have the same features and same controls (i.e., same utility\nand same usability), but one has a better rendering than the other. Most\npeople would prefer the one that looks nicer. There is some evidence to show\nthat under these exact conditions, users will judge the better-looking site as\nbeing more usable, even though it has exactly the same functions and controls\n(the so-called **aesthetic usability effect**). The emotion from using a\nproduct has a profound effect on our perception of it.\n\nDefinition\n\nThe **aesthetic usability effect** describes how users attribute a more usable\ndesign to products that are more visually pleasing to the eye.\n\nClearly, designing to a high standard for the look and feel of a product or\napplication is important. The problem comes when companies think that all they\nneed to do is provide a beautiful product and ignore the utility and usability\ndimensions. The glitz may sell, but users will remember and punish that\nproduct in the market, especially when it comes time to making buying\ndecisions again.\n\nPutting Lipstick on a Pig\n\nGAVIN: When I go to the Consumer Electronics Show where tens of thousands of\nproducts are showcased, I always find it curious how companies try to capture\nyour attention with the shiny new tech.\n\nBOB: What strikes me as fascinating is the push to anthropomorphize products.\nIs there really a reason for these AI applications to be rendered with faces?\nBut the reason they are, I think, is the designer wanted to evoke emotions in\nthe user. Interacting with a plastic object that has big eyes and a mouth that\nspeaks might be less intimidating than an orb that speaks.\n\nGAVIN: There is nothing wrong with trying to tap into the emotional factor\nwith aesthetically pleasing, anthropomorphic talking heads.\n\nBOB: But, recognize that there is a UX hierarchy at play. Getting the\nfoundation sound is key. Consider the usability of the application. Does it do\nas intended? Is there a perceived usefulness? These are the things that have\nto be done right because how something looks can only take you so far.\n\nGAVIN: The experience has to be good. Or you are just dressing up another\nmediocre product. Know that the **aesthetic usability effect** only takes a\nproduct just so far.\n\n**The point** Focus on getting the foundation right and then apply the visual\ntreatment to make the product stand out and set apart from the rest.\n\n##### UX and the relationship to branding\n\nThis is a good place to inject an issue that we think about a lot: branding\nand its relationship to UX.\n\nThere is this interconnection between brand perception and user experience.\nSometimes “brand” covers a lot of sins in the user experience;12 sometimes a\npoor user experience damages the brand’s value. We’ve come to realize just how\ninfluential a brand is to experience. We’ve seen amazing products that have no\nbrand value get discounted and mediocre products from respected brands get\nlauded. Maybe it does not need to be said, but the most useful and usable\ndon’t always win. If brand marketers are pushing something that the product\ndoes not deliver on, then the market will react. Trust is eroded when the\npromises exceed the reality.\n\nAs we are increasingly commercializing the AI in the product, branding issues\nsurface. Just saying something has a “powerful AI engine” does not guarantee a\nsale. It needs more; it needs UX.\n\nUX Delivers the Brand Promise\n\nBOB: UX intersects with marketing whether we like it or not. And one of the\nthings that we have learned from marketing and branding is that it’s easy to\nsell the sizzle, but you have to deliver the steak too.\n\nGAVIN: That’s why getting the user experience right is so important. Brand\nmarketing can tell us how easy something will be or how a product will change\nour lives, but unless we actually experience that, it’s all just talk.\n\nBOB: Yep, UX is a delivery on that brand promise. If the user doesn’t\nexperience what the brand marketer sold them, then that mismatch will\nundermine the credibility of the brand.\n\nGAVIN: AI has the same problem. Lots of hype, but until it delivers in a\nmeaningful way to change lives, it will be just the sizzle without the steak.\n\n**The point** Designing a product is more than just the product itself. The\nlandscape is littered with failures. Build a better product experience—one\nthat allows AI to show what it can do.\n\n### Principles of UCD\n\nThe key principle of user-centered design is that it focuses the design and\ndevelopment of a product on the user and the user’s needs. There are certainly\nbusiness and technical aspects to engineering and launching a product. The UCD\napproach does not disregard or discount the needs of the business or the\ntechnical requirements. However, **UCD** process places emphasis squarely on\nthe user because the tradeoffs made at the business and technical level for\ntimelines, specifications, and budgets are assumed. The UCD focus ensures that\nthe user is not cast aside easily.\n\nSo let’s turn to the key components: users, environment, and tasks.\n\n#### The users\n\nThe importance of understanding the user’s goals, capabilities, knowledge,\nskills, behaviors, attitudes, and so on cannot be underestimated. There may\nalso be multiple user groups as well. Descriptions of each group should be\ndocumented (sometimes these descriptions are expressed as “personas).” One\ntrap we see is that designers and developers assume they know the user. Or\nthey assume they are just like the user. Many designs fail because the\ndescription of the user was nonspecific, nonexistent, or just wrong.\n\nThe point\n\nDefine the intended user—beyond simply a market description or segment.\nConstruct a persona describing the user’s knowledge, goals, capabilities,\nlimitations and include user scenarios that will both define the experience to\nbe built and include “guardrails” of things to avoid.\n\n#### The environment\n\nWhere are the places and what are the conditions where the users interact with\nthe product? Environment includes the locations, times, ambient sounds,\ntemperatures, and so on. For instance, certain environments (i.e., contexts)\nare better for some modalities (hands-free) than others. An app to be used on\na treadmill while running will have different design characteristics than an\napp used for banking. What this means is that the user experience extends\nbeyond simply what the user sees on the screen. The UX is the whole\nenvironment; the totality of the context of use. In registering for a new\napplication that requires mobile phone use for two-factor authentication, for\ninstance, assumes the user has access to the phone and must account for the\npossibility that the phone is not present or unable to be used. The whole\nprocess is part of the UX. The instruction guide that comes with a new\nsmartphone is part of the UX. These things are not disconnected in the mind of\nthe user, but they are often handled by different groups in the organization.\n\nThe point\n\nDevelop use cases against the environments of use to identify further user\nneeds. This could also illuminate the points where passive and explicit user\ndata can be used to **trigger** additional user benefits.\n\nImportance of Environment\n\nGAVIN: In 2017, France’s official state railway company, SNCF, wanted to build\nan AI chatbot ticket application.13 The design team captured the conversations\nbetween travelers and ticket agents. They trained the AI’s grammar system to\nmodel the experience observed.\n\nBOB: But when they tested the chatbot prototype with users, it failed. When\ncustomers encountered the blank text field, they began with, “Hi. I would like\nto buy a ticket.” Users expected to engage the chatbot in a friendly and\nconversational manner.\n\nGAVIN: So, the chatbot expected, “I would like to buy a ticket from Paris to\nLyon today at 10:00 a.m.” Just like how they were observed at the ticket\nwindow.\n\nBOB: Yep, they never encountered a friendly and conversational experience when\nthey observed transactions at the ticket window!\n\nGAVIN: Ah, this is Paris, right? There was probably a line of customers all\nwaiting impatiently. If a customer walked up and said, “Hi. I would like to\nbuy a ticket from Paris to Lyon or maybe Normandy... What times are\navailable?” Those in the queue would be visibly angry. Such that the next\nperson would walk right up and make the request as precise and efficient as\npossible!\n\nBOB: Exactly! The environment of use was different. A chatbot could be used at\none’s leisure without the pressure from a queue of Parisian travelers!\n\n**The point** The environment of use can change user interactions\ndramatically. Luckily, testing identified the issue and the team was able to\nretrain the chatbot.\n\n#### The tasks\n\nTasks are about what people do to accomplish their goals and involve breaking\ndown the steps that users will take. Depositing a check using a mobile app\nrequires me to log in, to navigate to the right place, enter amounts, take\npictures, and a lot more. Tasks can be expressed as a task analysis or a\njourney map (if detailed enough). Based on a full explication of the tasks,\nthe application requirements can be built.\n\nThe point\n\nUse cases are well defined in product development processes. With AI, where\nthe system presumably can learn and identify new opportunities, design for AI-\nenabled products now has AI-originated use cases which are not defined by the\nproduct team. Tasks have an added dimension with AI that needs to be managed.\n\nWhile much of this can seem elementary, it amazes us how few organizations\ntake the time to describe users, environments, and tasks. Getting to this\ndetail requires user research14 and must be documented in the specifications\nfor the design and development team. The three key elements of UCD form the\nfoundation of knowledge, but the process is what delivers a successful product\nor application that allows AI to show what it can do.\n\n### Processes of UCD\n\nThere are multiple flavors of this, but Figure 5-1 shows the basics of how the\nprocess works. Starting at the top and moving clockwise, early-phase user\nresearch would have defined the users, refined their needs, identified the\ncontext (i.e., environment) of use, and documented tasks in the requirements.\nAt this point, we are ready to start design. Often the tendency among the\ntechnical team is to start programming. In UCD, we want to resist that urge.\nThis is supported in _The Mythical Man-Month_15 by Fred Brooks where he\nwrites, “By the architecture of the system, I mean the complete and detailed\nspecification of the user interface.”\n\n![../images/470825_1_En_5_Chapter/470825_1_En_5_Fig1_HTML.jpg](../images/470825_1_En_5_Chapter/470825_1_En_5_Fig1_HTML.jpg)\n\nFigure 5-1\n\nTypical UCD processes ensure the users and their needs feed into the process\nand then cycle through design and evaluation processes\n\nTaking the research and building the user interface begins next.16 Design\nshould begin by defining with rough sketches so as not to get too wedded to\nearly ideas.17 Be willing to throw out a dozen ideas before settling on one.\nDraw the wireframes and map out the interactions—do they make sense? Do they\nget users closer to their goals?\n\nOnce design begins, conduct usability tests with paper prototypes.\nSuccessively refine them with representative users doing typical tasks. Later\ndevelop digital prototypes to test dynamic interactions, getting the design\ncloser to the final look and feel. There are countless books and websites\ndescribing methods on UI design, conducting usability testing and iterative\nprototyping that we’re not going to cover here. Suffice it to say, usability\ntesting is the single-most important component to perfecting the user\ninterface.\n\nAs Figure 5-1 illustrates, this is an iterative process, a successive\napproximation to a system that meets the needs of the user. Once the design is\nevaluated, based on user feedback, one might adjust the requirements, decide\non different ways to break down the tasks, or change the interface controls. A\nlot can change.\n\nOne key thing is that it is only at the end that we advocate applying\ngraphical and visual treatments. What most people think of as design,\ngraphical treatment, comes only after we have nailed the interaction models in\nthe prototype designs. Colors, shape, size, and others, all support the\nunderlying design. In construction, you can’t paint the house before the walls\ngo up. In application development, you should not create pretty diagrams\nfirst. Design begins in the field with research, not in Photoshop.\n\nA best practice at the outset of design is to describe tangibly what success\nlooks like as it relates to the user needs. For instance, you might set a goal\nthat 95% of users get through the registration page in 2 minutes without\nerror. These targets on critical tasks provide measurable progress that the\ndevelopment team can aim for. More importantly, success on these targets\nduring usability testing is what breaks us out of that loop and moves us to\nthe technical development.\n\nThe real benefit of the UCD model is that through progressive improvements\ndocumented in usability testing, the organization can have much higher\nconfidence in the success of the final deliverable than if there was no, or\nonly casual, feedback from users. Done well, applying the UCD model will make\napplications useful, usable, learnable, forgivable, and enjoyable.\n\nUCD is agnostic to the content; it can, and should, be applied to AI\napplications.\n\n### A cheat sheet for AI-UX interactions\n\nThere has been some very good work exploring the relationship of the\ninteraction of humans and AI from Microsoft and the University of\nWashington.18 The research team there did a very thorough review of guidelines\nand vetted them experimentally. As you can see in Table 5-1, these guidelines\nare mostly geared toward improving the utility and usability of the AI\ninterface.\n\nTable 5-1\n\nGuidelines for AI-UX interactions from Microsoft and the University of\nWashington\n\n#| Guideline| Description  \n---|---|---  \n1| Make clear what the system can do.| Help the user understand what the AI\nsystem is capable of doing.  \n2| Make clear how well the system can do what it can do.| Help the user\nunderstand how often the AI system may make mistakes.  \n3| Time services based on context.| Time when to act or interrupt based on the\nuser’s current task and environment.  \n4| Show contextually relevant information.| Display information relevant to\nthe user’s current task and environment.  \n5| Match relevant social norms.| Ensure the experience is delivered in a way\nthat users would expect, given their social and cultural context.  \n6| Mitigate social biases.| Ensure the AI system’s language and behaviors do\nnot reinforce undesirable and unfair stereotypes and biases.  \n7| Support efficient invocation.| Make it easy to invoke or request the AI\nsystem’s services when needed.  \n8| Support efficient dismissal.| Make it easy to dismiss or ignore undesired\nAI system services.  \n9| Support efficient correction.| Make it easy to edit, refine, or recover\nwhen the AI system is wrong.  \n10| Scope services when in doubt.| Engage in disambiguation or gracefully\ndegrade the AI system’s services when uncertain about a user’s goals.  \n11| Make clear why the system did what it did.| Enable the user to access an\nexplanation of why the AI system behaved as it did.  \n12| Remember recent interactions.| Maintain short-term memory and allow the\nuser to make efficient references to that memory.  \n13| Learn from user behavior.| Personalize the user’s experience by learning\nfrom their actions over time.  \n14| Update and adapt cautiously.| Limit disruptive changes when updating and\nadapting the AI system’s behaviors.  \n15| Encourage granular feedback.| Enable the user to provide feedback\nindicating their preferences during regular interaction with the AI system.  \n16| Convey the consequences of user actions.| Immediately update or convey how\nuser actions will impact future behaviors of the AI system.  \n17| Provide global controls| Allow the user to globally customize what the AI\nsystem monitors and how it behaves.  \n18| Notify users about changes.| Inform the user when the AI system adds or\nupdates its capabilities  \n  \nThe point\n\nConsider developing and applying a set of guidelines, such as those in Table\n5-1, when designing your product’s AI and user interactions. It covers a wide\nrange of topic areas with UX implications.\n\n## A UX prescription for AI\n\nWe have looked at what some of the problems with AI have been in the past. We\nhave also investigated why similar problems might manifest again in the future\nand why UX offers an effective solution to these problems. Now we will look at\nthe “how”: how can we use UX to avoid some of the pitfalls of AI?\n\nThe answer lies in advancing a framework for incorporating UX sensibilities\ninto AI.\n\nWe are going to illustrate the goal by example. Let’s say we want to build a\nsmart chatbot into a desktop application (say, an electronic health record\n(EHR) used by doctors) to offer in-context help or clinical support. The first\nstop is to understand if this is a need that user’s really have? If so, what\ndo we know about those users?\n\n### Understanding users, environments, and tasks\n\nFirstly, we’d want to understand our users; we’d conduct user interviews to\nunderstand how they use their EHR today. This will uncover the user’s mental\nmodel for how they interact with the EHR. Workflows will be identified that\nwill help set the stage for how AI would integrate seamlessly into daily tasks\nalready performed. Moreover, the research will find existing difficulty and\ninefficiencies with the EHR which can be opportunities for AI to be more\nbeneficial.\n\nExample research areas of interest:\n\n  * What are the things you did in your job earlier this week? Ideally, \n\n    * Shadow and observe actual activities; watch the tasks users do and capture the workflow with emphasis on the different sources of information and manual effort.\n\n    * Watch how the user navigates and interacts with systems.\n\n    * Pay attention to the types of help used.\n\n  * Who are the various users? Do they all need/want help for the same things? In the same ways?\n\nTo avoid being too influenced by a small set of users, in a case like this, we\nmight use the qualitative interviews and observations to develop a survey to\nget more information on how users would be best supported by an EHR chatbot.\nThe survey would collect data about the characteristics of the user, where\nhelp is needed in the EHR, under what circumstances help might be welcomed,\nand so on. Often we will take the quantitative data about users and identify\ngroups with similar characteristics using multivariate statistical methods.\nThese groups form the basis of personas, and we will carry out further, more\ndetailed, interviews with people who fall into those groups to flesh out the\npersonas. Our emphasis through all this is to get at the knowledge, skills,\nexpertise, and behaviors of the users to know how best to develop the\napplication to serve them.\n\nNote that after doing these interviews, we may discover that the proposed\nconstruction of the chatbot is not a good solution and that users would be\nbetter aided in other ways.\n\nNext, we move on to understanding the contexts of use, the environments. EHRs\nare used in exam rooms, in hospitals, at reception desk, in homes, and many\nmore. Knowing what users do in each of these settings is important to know the\nkind of context-sensitive help that the chatbot would offer. If the chatbot\noffers the same kind of help to all users in all contexts, it’s likely not to\nbe so useful. Getting information about the environments might be able to be\ndone in the same interviews as those for the users. Better still would be to\ninclude some questions in the preceding questionnaire to get a feeling for the\nenvironments, but then to go and observe the users _in the actual use\nenvironments_. Seeing how users use the application and all the supporting\nmaterials and procedures in _context_ is essential to documenting\nrequirements. This means going to the clinic and watching as many\nrepresentative users use (and get frustrated with) the application.\n\nLast, while gathering information about the users and their environments, data\nabout the specific tasks people do can be gathered. If we know what each\ngroups’ goals are in using the EHR and learn about the environments, then we\ncan begin to identify the various steps in the process to support them.\n\nUnderlying all this is the AI model—presumably based on thousands of\ncases—knows the interaction sequences (e.g., screens viewed), user inputs,\nerror messages, and so on that occur. The AI model can infer where users might\nneed the support of an intelligent bot. While these inferences are probably\ndirectionally correct, they can only be refined by the knowledge gathered from\nusers. Perfecting these models may require tuning through crowdsourcing of the\ninputs and outputs. (More on this in the next section.)\n\n### Applying the UCD process\n\nArmed with this background on users, environments, and tasks, initial design\ncan begin. This design should start with the interaction model, controls, and\nobjects, but at a rough level. Recall that the purpose is to help the user by\nmonitoring the user’s behavior and offer advice/support when it detects\ntrouble. Design specifically when the chatbot should **trigger** and interrupt\nthe user with important information or when decisions need to be made. Set the\n_guardrails_ defining when and how that the chatbot interacts.\n\nThis is very tricky and potentially dangerous as distracting someone during a\ncritical task on an application like an EHR might cause the user to forget to\nenter critical information.\n\nThe design team will go through a series of concepts, perhaps on paper or\ncrude digital prototypes (e.g., using PowerPoint). It is here that the\niterative design, test, revise, test cycle takes over. The design is put\nthrough usability testing with representative users doing representative\ntasks. Giving users the chance to experience how the chatbot interface will\nrespond to actions will be essential in successively refining its behavior.\n\nThe important thing to note is that, up to this point, we are assuming that\nthe algorithms are sound and that the underlying AI is appropriate. During\nuser testing, not only should the experience of interacting with the chatbot\nbe tested, but a test of the AI-based content should be conducted too. The\npoint here being that once the design cycle begins, all aspects of the user\nexperience should be in play. The user testing begins cheap and fast and often\nends with a larger-scale more formal user test.\n\nPoor Clippy—Not!\n\nGAVIN: Microsoft unleashed Clippy on the world in the mid-1990s trying to make\nusing Office applications more friendly. It had a rules-based engine that\nwould detect if you needed help and intervene.\n\nBOB: Yeah, but it would totally stop my work and make me respond to it. It\nquickly became hated by most of us. It’s even a punchline now. People have a\nnostalgia for it!\n\nGAVIN: What’s interesting though is that despite the annoying experience, it\nhad another side. Microsoft ignored their own research—many women thought that\nClippy was male, too male. And not only that, he was a creepy male.19 The\nthing is, men did not feel this emotion—but women did.\n\nBOB: Clippy was an anthropomorphized agent that went wrong. It simply could\nnever deliver the kind of support or build the kind of rapport with the user\nthat was intended. He resurfaced through the years most recently in Microsoft\nTeams, but that was crushed in very short order!20\n\nGAVIN: Yeah, Clippy’s become an exemplar of how not to annoy your users. So\nmany poor decisions!\n\n**The point** User research can identify potential trouble and allow for\ncourse corrections to solve issues.\n\nIn a development like this, the measures of success are perhaps a little\nharder to come by. Clearly, user measures of utility and usability are high on\nthe list. We see this today in many applications—users get asked after many\ninteractions how many stars they would give the quality of service. Other\nobjective measures can also be taken. If this is supposed to be a user\nassistant, we could measure whether the number of successful actions after the\nchatbot interaction has increased or not. It is important to have these\nmeasures established so that we know we have succeeded in the design and can\nproceed with detailed programming.\n\n## What else can UX offer? Better datasets\n\nWe’ve spent a lot talking about the benefits of UX and the UCD process to AI.\nBut there may be other ways UX can benefit AI.\n\nUser experience comes from various disciplines within psychology. One\nimportant thing that psychology and therefore user experience can bring to AI\nis a set of capabilities for collecting better data.\n\n### Giving AI better datasets\n\nAs we discussed earlier in Chapter [4](470825_1_En_4_Chapter.xhtml), one of\nthe biggest drawbacks for AI is getting the right data. Many in UX researchers\nare trained in research methods to collect and measure human performance data.\nUX researchers can assist AI researchers in collection, interpretation, and\nusage of human-behavior datasets for incorporation into AI algorithms.\n\nWhat is the process by which this is done?\n\n#### Identify the objective\n\nThe first task is to understand what the AI researchers really need. What is\nthe objective? What constitutes a good sample case? How much variability\nacross cases is acceptable? What are the core cases, and what are the edge\ncases? So, if we wanted to get 10,000 pictures of people smiling, is there an\nobjective definition of a smile? Does a wry smile work? With teeth, without\nteeth? What age ranges of subjects? Gender? Ethnicity? Facial hair or clean\nshaven? Different hair styles? And so on. Both the in and out cases are\ncomponents that the AI researchers need to clearly define and have all parties\nagree on.\n\n#### Collect data\n\nNext, do the necessary planning for data collection. One of the strengths of\nUX researchers is the ability to construct and execute large-scale research\nprograms that involve human subjects. How to collect masses of behavioral data\nface to face, efficiently, and effectively is not in the core expertise of\nmany AI researchers. In contrast, much of the practice of user research is\nabout setting the conditions necessary to get unbiased data. Being able to\nrecruit sample, obtain facilities, get informed consent, instruct\nparticipants, and collect, store, and transmit data is essential. Furthermore,\nUX researchers can also collect all the metadata necessary and attach that\ndata to the examples for additional support. UX researchers are practiced in\nsorting, collecting, and categorizing data—as is evidenced by a skillset that\nincludes qualitative coding and the many tools that support these types of\nanalysis.\n\n#### Do further tagging\n\nAfter initial data collection, it may be necessary to organize and execute a\ncrowdsourcing program such as Amazon’s Mechanical Turk21 to further augment\nthe data collected so far. For instance, if we were to collect voice samples\nof how someone orders a decaf, skim, extra-hot, triple-shot latte in a noisy\ncoffee shop, there could be several properties that would be of interest for\neach sample. In such cases, we might engage multiple researchers, or coders,\nto review each sample, transcribe the samples, and judge their clarity and\ncompleteness. These coders would then have to resolve any observed differences\nto ensure the cleanliness of the coding.\n\nCollecting Data in Real World\n\nBOB: We recently did a study where we had to get representative users to enter\nvoice commands into a mobile device in a way they would normally speak to a\ncolleague.\n\nGAVIN: Yes, we sampled more than a thousand people through an on-line survey,\ngave them scenarios, and they had to respond with the commands. They spoke the\ncommands into their mobile phone. We were dealing with all sorts of accents,\nso the voice recognition was a challenge.\n\nBOB: Yeah, and there was lots of ambient noise. The speech then had to be\nconverted to text. There was a lot of really good content and voice capture,\nbut there was also gibberish. But, that’s realistic.\n\nGAVIN: Ultimately, there were dozens of coders who listened to the recordings\nand checked the speech to text transcriptions in order to tune the AI\nalgorithms. It was hard work, but necessary to make sure that the AI engines\nget it right in the future.\n\n**The point** AI-enabled apps often need a lot of humans to curate the input.\n\nWell-constructed and executed research programs can help protect against the\nlimitations that may be present in existing databases used to train AI\nalgorithms. Using custom datasets avoids the use of inconclusive, useless, or\nincorrect data whose limitations might not be immediately obvious. UX\nresearchers are well positioned to help ML scientists collect clean datasets\nfor the training and testing of AI algorithms.\n\nSimilarly, one increasing objection to AI is that it has been trained on\nbiased data; we touched on this earlier. By controlling the sample from which\nthe data are collected, datasets can avoid that bias. We once did a large-\nscale study (_n_ =5000) for a company that wanted to collect videos of people\ndoing day-to-day activities. One of the key criteria for them was to get a\nrepresentative sample of the population—by age, gender, ethnicity, and so\non—so that their facial recognition algorithms could be trained on better data\nand their output more accurate.\n\n## Where does this leave us?\n\n### Find the why\n\nWe opened the book with a “Hobbit’s tale” suggesting AI and UX have been on\nquite a journey. One filled with hype and periods of winter. The future for AI\nis quite immense. In the not too distant future, we will witness AI\nintegrating into almost every industry to bring forth better health,\nliberation from mundane or dangerous jobs, and advances beyond what can be\nimagined. However, we believe that success will come more readily and may\navoid failures if more attention were placed on the user experience to\nspecifically improve AI-UX principles of context, interaction, and trust. We\nbelieve AI-enabled products need to not be singularly focused around\ntechnology. As a solution, we recommend using a UCD process—one that places\nthe human, who is the beneficiary of AI’s progress, at the core. However, we\nwould like to end with one additional suggestion—to build AI-enabled products\nwith a purpose, find the _why_ that will give AI purpose that will drive the\ndesign to greater success.\n\nFinding the Why\n\nGAVIN: You know, I started my research career at UCSF studying brainwaves. I\nplaced electrodes on people’s heads and recorded electrical impulses from the\nbrain when the participant heard a “boop” or a “beep” or when they lifted\ntheir index finger.\n\nBOB: You were a lab technician doing basic research!\n\nGAVIN: Exactly. The year was 1991, and during one research session, the\nparticipant finished, and as he left, he shook my hand and said, “I really\nhoped this research goes to find a _cure_.” I smiled and said, “I hope so as\nwell.” When the participant left, I was devastated.\n\nThe participant was HIV+, and at the time, retrovirus treatments were still\nexperimental. I thought that in 10 years, this 21-year-old, bright-eyed, and\nenergetic person might have full-blown AIDS and probably would pass away\nbefore a cure is found.\n\nBOB: You were doing basic research. Comparing his brainwave activity to see if\nit resembled an Alzheimer’s patient or alcoholic’s when a “beep” is heard…\n\nGAVIN: Or when an index finger is raised…\n\nBOB: We still haven’t found a cure for any of those three diseases and that\nwas almost 30 years ago.\n\nGAVIN: You could say that was the day I lost my purpose. I moved into UX\nresearch where I hoped research would have more direct impact for me.\n\nFast forward 10 years. I was with a patient doing a study on a prototype for\nan auto injection device. I remember the patient needed to pause and stand up\nbecause she had such terribly fused joints from her disease. At the end of the\nsession, she did not shake my hand like the HIV patient. She gave me a hug.\n\nBOB: I have never received a hug in a research session before!\n\nGAVIN: Me either! I was surprised. She looked at me and said, “You don’t get\nit, do you?” I shook my head. She continued, “Look at my hands and see how\nhard it is for me to hold things. Currently, I have to reconstitute this\nmedicine. The process is so complicated that I have to do it all on my kitchen\ntable. I have to syringe one drug and mix with another precise amount. Wait\nand then inject. Again, my hands barely work, but this therapy stops the\nprogression and it is a miracle drug for me.”\n\nI said, “The purpose is to understand how it works for you. We want to\nunderstand how we can make the experience match your expectations so you can\nhave a safe and effective dose.”\n\nShe replied, “So I can use this new device for the first time correctly? I can\nwalk into the bathroom and be done in a minute?”\n\nI said, “Exactly.”\n\nShe shook her head and said, “You still don’t get it. You see, my current\nprocess is so complex that I do it on the kitchen table and it takes forever.\nWell, my 5-year-old daughter sits and watches her mother struggle to take her\nmiracle medicine. Now, with this device, I can take my medicine discreetly in\na bathroom.\n\n“Well, you are not just making this safe and effective for people like me,”\nshe said. “You are changing the way a daughter looks at her mother! My\ndaughter does not have to watch her mom struggle with her medicine and shoot\nup to live.”\n\nBOB: Wow. When a product is done well and is appreciated, the reasons can be\nbroader than imagined.\n\nThe device’s _purpose_ for this patient is powerful.\n\nGAVIN. I found _my purpose_. Companies need to also find purpose within a\nproduct and let this help drive better design.\n\n**The point** Have a strong understanding of how the user experience will be\naffected by its usage. Who will be affected? Identify situations of use that\ncan amplify the user benefit. A product’s _purpose_ will be found there. This\nis the “why” that will drive the product team to design a compelling\nexperience rather than falling prey to the hype surrounding the technology _._\n\nOur hope is that a greater recognition of the symbiotic relationship between\npeople and AI can be designed into products. We want AI to become less of a\nblack box and more transparent about its strengths and weaknesses. UX can\nhelp. AI development efforts should involve the user and user’s goals\ndirectly, be aware of the environments, and account for the tasks. We believe\nthat AI products with a user-centered focus will be hands down more successful\nthan those that do not have that focus. Remember,\n\n> _If AI doesn’t work for people, it doesn’t work._\n\nFootnotes\n\n1\n\nThat task has been largely accomplished with the fact that more than five\nbillion people have mobile phones, and over the half of those are smart\nphones. Smartphone ownership is growing rapidly around the world, but not\nalways equally. Laura Silver.\n[www.pewresearch.org/global/2019/02/05/smartphone-ownership-is-growing-\nrapidly-around-the-world-but-not-always-\nequally/](http://www.pewresearch.org/global/2019/02/05/smartphone-ownership-\nis-growing-rapidly-around-the-world-but-not-always-equally/) (retrieved April\n16, 2020).\n\n2\n\nAnother term coined by Norman in the title of his book: _User Centered System\nDesign: New Perspectives on Human-computer Interaction_ (1986). United\nKingdom: Taylor & Francis.\n\n3\n\nVlaskovits, P. (2011). “Henry Ford, Innovation, and That ‘Faster Horse’\nQuote.” Harvard Business Review. August 29, 2011. Accessed May 18, 2020.\n[https://hbr.org/2011/08/henry-ford-never-said-the-\nfast](https://hbr.org/2011/08/henry-ford-never-said-the-fast).\n\n4\n\nWe are not going to go into the details here. There are many websites and\nbooks on how to implement the UCD process into many different development\napproaches. A good place to start is here: [www.usability.gov/what-and-\nwhy/user-centered-design.html](http://www.usability.gov/what-and-why/user-\ncentered-design.html) (retrieved May 13, 2020).\n\n5\n\nElias, J.G., Westerman, W.C., & Haggerty, M.M. (2007). “Multi-touch Gesture\nDictionary.” USPTO US 2007/0177803 A1. Last accessed: June 22, 2020\n[www.freepatentsonline.com/20070177803.pdf](http://www.freepatentsonline.com/20070177803.pdf).\n\n6\n\nWroblewski, L. (2010). Design for Mobile: What Gestures do People Use?\nReferencing Dan Mauney’s Design for Mobile conference. Last accessed: June 22,\n[www.lukew.com/ff/entry.asp?1197](http://www.lukew.com/ff/entry.asp%253F1197).\n\n7\n\nSmith, B. (2013). “Human error as a cause of vehicle crashes.” The Center for\nInternet and Society, Stanford University. Traffic Safety Facts Annual Report\nTables. National Highway Traffic and Safety Administration. December 18, 2013.\nLast accessed: May 19, 2020 [http://cyberlaw.stanford.edu/blog/2013/12/human-\nerror-cause-vehicle-crashes](http://cyberlaw.stanford.edu/blog/2013/12/human-\nerror-cause-vehicle-crashes).\n\n8\n\nKokalitcheva. K (2018). “People cause most California autonomous vehicle\naccidents.” Axios. August 29, 2018. Last accessed: May 19, 2020\n[www.axios.com/california-people-cause-most-autonomous-vehicle-accidents-\ndc962265-c9bb-4b00-ae97-50427f6bc936.html](http://www.axios.com/california-\npeople-cause-most-autonomous-vehicle-accidents-\ndc962265-c9bb-4b00-ae97-50427f6bc936.html).\n\n9\n\nMohn, T. (2019). “Most Americans Still Afraid To Ride In Self-Driving Cars”\nForbes. March 28, 2019. Last accessed: May 19, 2020\n[www.forbes.com/sites/tanyamohn/2019/03/28/most-americans-still-afraid-to-\nride-in-self-driving-\ncars/#5803114632da](http://www.forbes.com/sites/tanyamohn/2019/03/28/most-\namericans-still-afraid-to-ride-in-self-driving-cars/%25235803114632da).\n\n10\n\nLevy, Nat. (2020). “Three Apple workers hurt walking into glass walls in the\nfirst month at $5bn HQ.” Geekwire. February 24, 2020. Accessed May 20, 2020.\n[www.geekwire.com/2020/alexa-always-listening-new-study-examines-accidental-\ntriggers-digital-assistants/](http://www.geekwire.com/2020/alexa-always-\nlistening-new-study-examines-accidental-triggers-digital-assistants/).\n\n11\n\nIn fact, Don Norman wrote a book on the subject. Norman, D. (2004). _Emotional\nDesign: Why We Love (or Hate) Everyday Things_. Basic Books.\n\n12\n\nThink of how powerful the Apple brand is. Now think about it. It has\nnotoriously bad usability. Much has been written on this, for example, the\nrise and fall of iTunes, Apple’s most hated app.\n[www.theverge.com/2019/6/3/18650571/apple-itunes-rip-discontinued-\nmacos-10-15-ipod-store-digital-music-\nwwdc-2019](http://www.theverge.com/2019/6/3/18650571/apple-itunes-rip-\ndiscontinued-macos-10-15-ipod-store-digital-music-wwdc-2019). Porter, Jon. Jun\n3, 2019 (retrieved May 21, 2020).\n\n13\n\nLannoo, Pascal & Gaillard, Frederic (2017). “Explore the future: when business\nconversations meet chatbots.” 13th UX Masterclass. April 22, 2017. Shanghai,\nPeople’s Republic of China.\n\n14\n\nSometimes organizations will rely on market research for this kind of\nanalysis. Market research and insights can be helpful, but they do not\nsubstitute for understanding of the user and user’s needs at the level\nrequired for system specifications.\n\n15\n\nBrooks, F. P., Brooks, F. P. (1975). The Mythical Man-Month: Essays on\nSoftware Engineering. United Kingdom: Addison-Wesley Publishing Company.\n\n16\n\nWe don’t want to imply that business needs or technical needs are secondary;\nthere is always a balance between user needs, business needs, and technical\ncapabilities. We strongly believe that the business and technical teams should\nbe a part of the UCD process.\n\n17\n\nWhile we are introducing many of the concepts of UCD and user interface design\nfor the purposes of showing how they can be beneficial to AI, we are not\nintending this to be a UCD book. There are hundreds of books, articles, and\nresources out there to get you started.\n\n18\n\nSaleema Amershi, Dan Weld, Mihaela Vorvoreanu, Adam Fourney, Besmira Nushi,\nPenny Collisson, Jina Suh, Shamsi Iqbal, Paul N Bennett, Kori Inkpen, et al.\n2019. Guidelines for human-AI interaction. In Proceedings of the 2019 CHI\nConference on Human Factors in Computing Systems. ACM, 3.\n\n19\n\nClippy might never have existed if Microsoft had listened to women. Perkins,\nChris. June 25, 2015. [https://mashable.com/2015/06/25/clippy-male-\ndesign/](https://mashable.com/2015/06/25/clippy-male-design/). Retrieved May\n15, 2020.\n\n20\n\nMicrosoft resurrects Clippy and then brutally kills him off again. Warren,\nTom. March 22, 2019. [www.theverge.com/2019/3/22/18276923/microsoft-clippy-\nmicrosoft-teams-stickers-\nremoval](http://www.theverge.com/2019/3/22/18276923/microsoft-clippy-\nmicrosoft-teams-stickers-removal). Retrieved May 15, 2020.\n\n21\n\n“Crowdworkers” or “Turkers” do work that machines cannot do, yet. A review of\nthe use cases of MTurk reveals that there is a wide range of work available to\nTurkers that supports machine learning datasets. There are potentially as many\nas 500,000 turkers worldwide. AI needs humans.\n\n\nIndex\n\nA\n\nAdvanced driver assistance systems (ADAS)\n\nAdvanced Research Projects Agency (ARPA)\n\nAesthetic usability effect\n\nAffect heuristic\n\nAffordances\n\nAI journalists\n\nAI system\n\nbias in datasets\n\ndata collection\n\nobjective\n\ntagging\n\nblack box\n\ndata elements\n\nethical standard\n\nethics\n\nsynthetic cases\n\nAI-UX interactions\n\nCheat Sheet\n\nenvironments\n\ntasks\n\nUCD\n\nusers\n\nAI-UX principles\n\ncontext\n\ninteraction\n\ntrust\n\ndefinition\n\nrole\n\nSiri\n\nvoice assistant\n\nAI winter\n\nALPAC report\n\nCortana\n\nhorizon\n\nhype curve\n\nvirtual assistants\n\nvoice assistant\n\nAmazon Alexa\n\nAlexa skills kit\n\nAmazon Echo\n\nAmeritech\n\nTest Town\n\nAndroid Auto\n\nApple CarPlay\n\nApple Siri\n\nArtificial intelligence (AI)\n\ndefinitions\n\nfilmmaking\n\njournalist\n\nMusk, Elon\n\nSearle, John\n\nscriptwriter\n\nuser experience\n\nAugmentation Research Center\n\nAutomatic Language Processing Advisory Committee (ALPAC)\n\nB\n\nBehavioral economics\n\nSystem 1\n\nSystem 2\n\nBias\n\nSynthetic cases\n\nBig Brother privacy\n\nBilingual machine\n\nBixby\n\nBoam, Eric\n\nBoyd, Danah\n\nBrannon, Greg\n\nBrooks, Fred\n\nBrown, Eric\n\nBusiness AI\n\nC\n\nCard, Stuart\n\nCathode-ray tube (CRT)\n\nCommercials\n\nAmeritech Test Town\n\nApple\n\nAT&T\n\nYves Saint Laurent\n\nContext\n\ncontext of use\n\nconversational context\n\ninformational/user context\n\nConversational context\n\nConversational norms\n\nCouncil on Extended Intelligence (CXI)\n\nCustom-built data\n\nD\n\nData science\n\nDataset evaluation\n\nData imputation\n\nSeeImputation\n\nDecision-making process\n\nDeep learning systems\n\nDepartment of Veterans Affairs doctor\n\nDiSanzo, Deborah\n\nDynamic intelligence\n\nE\n\nElectronic health record systems (EHR)\n\nELIZA\n\nEngelbart, Douglas\n\nEthically Aligned Design (EAD)\n\nEthics\n\nprivacy\n\nBig Brother Privacy\n\nPublic Privacy\n\nHousehold Privacy\n\nExpert systems\n\nEHR\n\ninference engine\n\ninference machine\n\nknowledge base\n\nknowledge elicitation\n\nlimitations\n\nEuropean Strategic Program on Research and Information Technology\n\nF\n\nFace-to-face communication\n\nFalcon\n\nFeigenbaum, Edward\n\nFilm editing\n\nFord, Henry\n\nModel T\n\nG\n\nGeneral Data Protection Regulation (GDPR)\n\nGeographic context\n\nGeorgetown-IBM experiment\n\nGeorgetown-IBM translator\n\nGestures\n\nApple patents\n\nGibson, James\n\nGoogle Assistant\n\nGoogle Duplex\n\nGoogle Home\n\nGoogle Translate\n\nGraefe, Andreas\n\nGraphical user interface (GUI)\n\nGrice’s Four Maxims of Communication\n\nGrice, Paul\n\n_Great Gatsby, The_\n\nGreenwald, Ted\n\nGrice, Paul\n\nH\n\nHernandez, Daniela\n\nHeuristic\n\naffect\n\naffordance\n\n_Hobbit, The_\n\nHome, Google\n\nHousehold privacy\n\nHuman cognition\n\nHuman-computer interaction (HCI)\n\nHuman-created trailer\n\nHuman-curated playlist\n\nHuman factors\n\nHuman journalists\n\nHuman-to-human collaboration\n\nHutchins, W. John\n\nI\n\nIBM Watson 12\n\nImputation\n\nInformational context\n\nInformation Processing Techniques Office (IPTO)\n\nInstitute of Electrical and Electronics Engineers (IEEE)\n\nInternet-enabled medical instruments\n\nInternet of Medical Things (IoMT)\n\nInternet of Things (IoT)\n\nIoT\n\nSeeInternet of Things\n\nJ\n\nJeff Bezos\n\nK\n\nKahneman, Daniel\n\nSystem 1\n\nSystem 2\n\nKohn, Martin\n\nL\n\nLicklider, J. C. R. (Lick)\n\nLighthill Report (1993)\n\nLighthill, Sir James\n\nLighthill, Sir James\n\nLund, Arnie\n\nM\n\nMachine Learning\n\nsupervised learning\n\nunsupervised learning\n\nreinforcement learning\n\nMachine translation\n\nMan-Computer Symbiosis\n\nMarkoff, John\n\nMaxims of communication\n\nMcCarthy, John\n\nMcNamee, Roger\n\nMedical AI\n\nacademic research\n\nanalytical tasks\n\ndiagnosis/treatment\n\ndiagnostic tasks\n\nfears\n\n_vs._ medical professionals\n\noncology program\n\nMemorial Sloan Kettering Cancer Center (MSKCC)\n\nMenzies, Tim\n\nMicroelectronics and Computer Technology Corporation\n\nMicrosoft Cortana\n\nMilanesi, Carolina\n\nMiller, Douglas\n\nMinimum viable product (MVP)\n\nMinsky, Marvin\n\nML applications\n\ncustom datasets\n\ndatabases limitations\n\nModel T\n\nMoore’s Law\n\nMoran, Thomas\n\nMSKCC\n\nSloan Kettering Way\n\nMusk, Elon\n\nN\n\nNeural-network-powered recommendation engine\n\nNeural networks\n\nNewell, Allen\n\nNies, Craig\n\nNorman, Don\n\nNorman Foster\n\nO\n\nOssorio, Pilar\n\nOver-the-phone communication\n\nP, Q\n\nPalo Alto Research Center (PARC)\n\nPlateau of profitability\n\nPublic privacy\n\nR\n\nRecommendation engine\n\nApple Music\n\nHulu\n\nNetflix\n\nSpotify\n\nYouTube\n\nRegional Bell Operating Company (RBOC)\n\nReinforcement learning\n\nRochester, Nathaniel\n\nRometty, Virginia\n\nS\n\nSamuel, Arthur\n\nSamsung Bixby\n\nSchank, Roger\n\nSchmidt, Eric\n\nScriptwriting\n\nSearle, John\n\nSiedman, Andrew\n\nSiri\n\nShannon, Claude\n\nSNCF, French Railway\n\nSpotify recommendation engine\n\nStrategic Computing Initiative\n\nSupervised learning\n\nT\n\nTaylor, Robert (Bob)\n\nText communication\n\nTognazzini, Bruce\n\nTolkien, J. R. R\n\nTopol, Eric\n\nTrap\n\nTriggers\n\nTrough of disillusionment\n\nTulip craze\n\nTulip mania\n\nTuring test\n\nAI winters\n\nELIZA\n\n_Ex Machina_\n\nGoogle Duplex\n\nhype\n\ntechnology\n\nunder-delivered\n\nweakness\n\nU\n\nUbiquitous computing\n\nAI systems\n\ndynamic intelligence\n\nInternet enabled instruments\n\nUniversity of California, San Diego\n\nUnsupervised learning\n\nUser-centered design (UCD)\n\nbenefit\n\ndesign\n\nprinciple\n\ntasks\n\nusers\n\nprocess\n\nUser-centered framework\n\nenvironment\n\nUser experience (UX)\n\nadjectives/adverbs\n\ndefinition\n\ndesign\n\naffordances\n\nApple’s gestural interfaces\n\nFacebook\n\nfeatures\n\nform/aesthetics\n\nnegative interactions\n\nsignifiers\n\nvision\n\nframework\n\nUser research\n\nUX Adjectives and Adverbs\n\nUX lens\n\naesthetics/emotion\n\nbranding\n\ntriggers\n\ntrust\n\nusability\n\nutility/functionality\n\nweirdness\n\nV\n\nVirtual assistants\n\nAI-enabled vehicles\n\nAlexa\n\ncontext of use\n\nconversational context\n\nCortana\n\ndeployment\n\nGoogle Duplex\n\nGoogle Translate\n\ninformational/user context\n\nSiri\n\nToyota Yui\n\nVoice assistant\n\nAmazon Alexa\n\nAmazon Echo\n\nAndroid Auto\n\nApple CarPlay\n\nGoogle Assistant\n\nGoogle Home\n\nMicrosoft Cortana\n\nSamsung Bixby\n\nSiri\n\nVoice Profiles,101\n\nW\n\nWatson\n\nWatson Health\n\nWeaver, Norbert\n\nWeirdness scale\n\nWeisenbaum, Joseph\n\nELIZA\n\n“Wizard of `Oz” test\n\nX\n\nXerox PARC (Palo Alto Research Center)\n\nY, Z\n\nYui\n\nYves Saint Laurent\n\nY, fragrance\n\n",
    "book_id": "ai_and_ux",
    "book_title": "AI and UX",
    "book_author": "Gavin Lew",
    "topic_id": "ai_theory",
    "topic_label": "theory",
    "chunk_index": 0
  },
  {
    "chunk_full": "![image](../images/00002.jpeg)\n\n\nTo Giselda and Ângelo,\n\nFor a lifetime of unconditional love\n\n\nBe not the slave of your own past. Plunge into the sublime seas, dive deep and\nswim far, so you shall come back with self-respect, with new power, with an\nadvanced experience that shall explain and overlook the old.\n\nRalph Waldo Emerson\n\n\nCONTENTS\n\n[Title Page](part0000.html#tit)\n\n[Dedication](part0001.html#ded)\n\n[Epigraph](part0002.html#epi)\n\n[Prologue: Just Follow the Music](part0004.html#pro)\n\n[1\\. What Is Thinking?](part0005.html#ch1)\n\n[2\\. Brainstorm Chasers](part0006.html#ch2)\n\n[3\\. The Simulated Body](part0007.html#ch3)\n\n[4\\. Listening to the Cerebral Symphony](part0008.html#ch4)\n\n[5\\. How Rats Escape from Cats](part0009.html#ch5)\n\n[6\\. Freeing Aurora’s Brain](part0010.html#ch6)\n\n[7\\. Self-Control](part0011.html#ch7)\n\n[8\\. A Mind’s Voyage Around the Real World](part0012.html#ch8)\n\n[9\\. The Man Whose Body Was a Plane](part0013.html#ch9)\n\n[10\\. Shaping and Sharing Minds](part0014.html#ch10)\n\n[11\\. The Monster Hidden in the Brain](part0015.html#ch11)\n\n[12\\. Computing with a Relativistic Brain](part0016.html#ch12)\n\n[13\\. Back to the Stars](part0017.html#ch13)\n\n[Selected Bibliography](part0018.html#bib)\n\n[Acknowledgments](part0019.html#ack)\n\n[Index](part0020.html#ind)\n\n[About the Author](part0021.html#aut)\n\n[Copyright](part0022.html#copy)\n\n\nPROLOGUE: JUST FOLLOW THE MUSIC\n\nAs the first violin arpeggios emerged from the marble walls of the ample hall\nand capriciously ventured down the stairs from the second floor to the main\nentrance of the deserted medical school building, I could not help but feel\ndisoriented by the total absurdity of the situation. After all, no medical\nstudent would be prepared to find himself listening to a concerto in the\nmiddle of the night while taking a quick break from one of the busiest\nhospital emergency rooms in the world. Yet, my initial uneasiness was soon\nreplaced by music that breathed a whole new life, full of hope and adventure,\ninto a soggy tropical summer evening. Perhaps that is why, even though those\narpeggios seduced my brain nearly a quarter of a century ago, I can still\nvividly recall how the stunning beauty of the melody, not the otherwise\nmeaningless individual notes, composed an earnest collective plea that\nbeckoned me to follow the siren music. I swiftly bounded up the stairs and\nmutely walked through a thin corridor to find myself standing at the entrance\nto the auditorium where the “Vorspiel,” the overture of Wagner’s Parsifal, was\nremorselessly playing. Unable to resist, I followed the music and entered the\nauditorium.\n\nHow disappointing it felt, then, when I realized that, except for an elderly,\nwell-dressed gentleman who was busily working, apparently trying to fix a\nworn-out faulty projector that had mangled one too many of his slides over the\nyears, the auditorium, with all its chandeliers blazing, was completely empty.\nBuilt in the late 1920s, each of the classroom auditoriums at the University\nof São Paulo medical school was a model of elegant economy. At the front, a\ntidy, boxlike stage demarcated the space from which professors lectured. A\nheavy wood table, a sturdy chair, and a long, well-worn sliding blackboard\ncompleted the humble teacher’s domain. The student seating was stacked into\nsteep, straight rows, allowing backbenchers inhabiting the last row—including\nme—to live well beyond the catedraticos’ authoritative stare during the\nunending lectures.\n\nBy now, the old man—his close-cropped white hair matching his pristine lab\ncoat—was startled by the sound as I opened the door to the lecture hall. Yet\nhe turned to me to reveal his effortless Mediterranean smile. Without giving\nup his struggle with the projector, he waved his left hand almost as if we had\nknown each other for years. On the lecturer’s desk I saw, to my dismay, the\nevidence that clearly incriminated that unsuspicious gentleman with that\nnight’s musical recital: a turntable, two expensive-looking loudspeakers, and\nthe covers of a few records by the Berlin Philharmonic.\n\n“Come in. Welcome. We have wine and cheese. I am having some difficulties with\nthe projector tonight, but we will be ready to start in a moment. By the way,\nmy name is Professor César Timo-Iaria. I am the teacher of this course.”\n\nHe had barely finished the sentence when a loud metallic pinging resounded\nfrom the slide projector and light spilled onto the lecture hall’s screen.\nWithout waiting for my reply, he swiftly changed position to stand behind the\nprojector, looking very much like a battle-proofed admiral on his ship’s\nbridge. After dimming the chandeliers and waiting for the second track of the\nrecord to start playing, he began clicking through his slides with a joy that\nI had only seen and experienced as a child while playing soccer in the narrow\nstreets of my old neighborhood. Sitting there alone in the dark, lullabied by\nTannhäuser’s singing, which echoed around the entire auditorium while images\nwholly unrelated to the typical medical curriculum were glancing off the\nscreen, I felt both provoked and enticed like in no other lecture I had\nattended before in my life.\n\n“But what course are you teaching?” I asked.\n\n“Introduction to Physiology,” Professor Timo-Iaria replied, without looking at\nme.\n\nJust to be sure, I looked at the screen again. Like all medical students, I\nhad taken the mandatory introduction to physiology course a few years earlier\nand, as far as I could tell, none of the images I was seeing matched what I\nhad been taught then.\n\n“How come?” I insisted.\n\n“How come what, son?” he rebutted, still not looking at me.\n\n“How could this be ‘introduction to physiology’? Your slides, they are all\nabout, I mean, you are showing only…”\n\n“Yes?” He looked amused by my discomfort, as if he had seen this happen many\ntimes before. “Go ahead. Tell me what is so surprising to you.”\n\nThe music, the images, an old man lecturing in the middle of the night in a\nvast and empty auditorium. Nothing made any sense. Half-perplexed and mildly\nirritated, I finally let him have it.\n\n“Stars, galaxies, those are the images you are showing. Look, now there is a\nradio telescope on the screen. What is this? How could this be an introduction\nto physiology?”\n\n“Well, this was the beginning. It all started there, from the big bang to\nbrains in just about fifteen billion years. Quite a voyage, wasn’t it? I will\nexplain what I mean.”\n\nThrough an endless visual parade of mindless shining spiral galaxies, budding\nstar clusters, playful nebulas, rebellious comets, and exploding supernovas,\nall tendered by music that seemed to have been composed by universal gods, I\nwatched Dr. Timo-Iaria’s slide-by-slide depiction of the epic that led to the\nemergence of the human mind. Planets were formed. Most remained bare, lifeless\nlands. But on at least one, an interesting experiment led to the emergence, a\nfew billion years ago, of the biochemical and genetic mechanisms for\nsustaining and replicating life. And life blossomed, struggled to survive,\nand, always full of hope and aspirations, started to evolve through many\nutterly unpredictable and tenuous roads.\n\nNext, I saw images of the first hominid couples walking, side by side,\nmillions of years ago in the middle of an African night in what today is\nEthiopia’s Afar Desert. And then, at the very moment when Wagner’s Tannhäuser\nwas at last granted freedom from the Venusberg by rejecting immortality for\nthe simple reward of experiencing what it is to be human, I shared the instant\nin which those early ancestors first looked at the infinite bright sky above\nthem, full of awe and fear, while a raging electrical storm crisscrossed their\nbrains searching for answers to the questions that still torment us today. I\nrealized that by looking timidly but curiously to the sky, those first men and\nwomen launched a long and noble relay race that since has united us all in the\nsearch for the fundamental explanations of our existence, our consciousness,\nand the meaning of all that surrounds us. The symbolic birth of science could\nnot have been better chronicled. Clearly, the seasoned admiral on the bridge\nknew very well how to steer his ship.\n\nThe dying notes of Tannhäuser’s “Pilgrim’s Chorus” announced the final slide,\nwhich, after being projected on the screen, lingered there as both of us\nremained in solemn silence. The slide showed a side view of a human brain.\nAfter a couple of minutes, Dr. Timo-Iaria turned on the lights, came down from\nhis station by the projector, and walked calmly toward the auditorium door.\nBefore leaving the room, he turned as if to say good-bye. Instead, he said:\n“This is the first lecture of the introduction to human physiology course. But\nI forgot to mention that I also teach an advanced course on neurophysiology.\nThe first class is tomorrow night. I strongly advise you to take that class,\ntoo.”\n\nStunned by what I had just experienced, I could only think of asking, “And\nwhat do I need to do to enroll in that course?”\n\nSmiling again as he exited through the hall, Dr. Timo-Iaria dispensed his very\nfirst piece of advice to the lifelong student he had by now so effortlessly\nrecruited.\n\n“Just follow the music.”\n\n* * *\n\nFor the past twenty-five years, I have often remembered Dr. Timo-Iaria’s\nunshakable belief that music and the scientific method represent two of the\nmost astounding by-products to emerge from the endless toils and torments of\nthe human mind. That may explain why I decided to dedicate my whole career to\nlistening to a different type of music, the kind of symphonies composed by\nvast ensembles of brain cells.\n\nTechnically speaking, I am a systems neurophysiologist. At least this is the\nway most of my neuroscience colleagues would define the type of work that my\nstudents and I carry out in my laboratory at the Center for Neuroengineering\nat Duke University, in Durham, North Carolina. In general terms, systems\nneurophysiologists spend their lives investigating the physiological\nprinciples that underlie the operation of the large variety of neural circuits\nformed by nerve fibers that emanate from the hundreds of billions of cells\nthat inhabit our brains. Such intricate brain networks, which dwarf by many\norders of magnitude the complexity and connectivity of any electrical,\ncomputational, or machine grid ever assembled by humans, allow each individual\nbrain cell, known as a neuron, to establish direct contact and communicate\nwith hundreds or even thousands of its peers. By virtue of their particular\nmorphology, neurons are highly specialized in receiving and transmitting\nminute electrochemical messages through their cellular contacts, called\nsynapses, which they use to communicate with other neurons. It is through\nthese immensely interconnected and highly dynamic cellular networks, which are\nknown rather prosaically as neural circuits, that the brain goes about its\nmain business: the production of a multitude of specialized behaviors that\ncollectively define what we usually, and proudly, refer to as “human nature.”\n\nBy harnessing massive waves of millivolt electrical discharges, these\nmicroscopic neural grids truly provide for every act of thinking, creation,\ndestruction, discovery, cover-up, communication, conquest, seduction,\nsurrender, love, hate, happiness, sadness, solidarity, selfishness,\nintrospection, and elation ever perpetrated by every one of us, and our\nancestors, throughout humanity’s whole existence. Had the word miracle not\nbeen appropriated by another area of human enterprise, I believe society\nshould grant neuroscientists exclusive rights to use it when reporting the\nwonders that brain circuits can generate on a routine basis.\n\nFor most systems neurophysiologists, like me, the ultimate quest is to\ndecipher the physiological mechanisms that allow these bursts of\nneurobiological electricity to give birth to the vast repertoire of human\naction and behavior. In seeking that holy grail, however, much of neuroscience\nover the past two hundred years has been embroiled in settling the hotly\ncontested dispute over what specific regions of the brain serve a particular\nfunction or behavior. At one extreme, radical localizationists, who are the\nlegitimate but often unclaimed heirs of Franz Gall, the father of phrenology,\nstill firmly believe that distinct brain functions are generated by highly\nspecialized and spatially segregated areas of the nervous system. In the other\ncorner, a smaller but fast-growing crowd, whom I call the distributionists,\nprofesses that rather than relying solely on unique specialization, the human\nbrain calls on populations of multitasking neurons, distributed across\nmultiple locations, to achieve every one of its goals. In defending this\nposition, we distributionists propose that the brain seems to utilize a\nphysiological mechanism that is somewhat equivalent to an election, a neuronal\nvote in which large populations of cells located in many different regions of\nthe brain contribute, albeit each in small and different amounts, to the\ngeneration of a final behavioral product.\n\nOver the past two centuries, both the localizationist and the distributionist\ncamps have elected the cortex—the most superficial component of the brain,\nlying just beneath the bone layer of the skull—as the main neuronal\nbattlefield for their never-ending dispute. The origins of this battle can be\ntraced to the days when phrenologists claimed to be able to recognize key\npersonality traits of an individual simply by palpating the scalp in search of\nskull bumps that reflected the disproportional enlargement of particular areas\nof the cortex that, according to their doctrine, generated attributes such as\naffection, pride, arrogance, vanity, and ambition. According to this doctrine,\neach human emotion and behavior was generated by a particular cortical\nterritory.\n\nAlthough Gall and his pseudoscience were discredited in due time, this general\nframework survived and morphed into one of the key dogmas of twentieth-century\nneuroscience. About one hundred years ago, a glorious series of experiments by\nthe first generation of full-time brain researchers, led by the genial\nSpaniard Santiago Ramón y Cajal, demonstrated that, as in all other organs, an\nindividual cell, the neuron, constitutes the brain’s fundamental anatomical\nunit. Almost by default, the single neuron was also quickly anointed as the\nfundamental functional unit of the central nervous system. The ascension of\nthe single neuron doctrine, combined with a dazzling 1861 report by Pierre\nPaul Broca, the French physician who observed that a localized lesion of the\nleft frontal lobe could lead to a profound loss of speech and paralysis of the\nright half of a patient’s body, temporarily put the distributionist camp in\ndisarray. But just as the distributionists were becoming isolated, Sir Charles\nSherrington came to their rescue. Sherrington argued that even one of the\nbrain’s simplest functions, the spinal cord arch reflex, depended on the\ncollaboration of many neurons and distinct neural circuits to work properly.\n\nIn the last decade, although no decisive blow has yet been delivered, the\ndistributionists have gained the high ground in the battle over the brain’s\nsoul. Discoveries emanating from neuroscience laboratories around the world\nare overturning the localizationists’ model. Among these collective efforts,\nresearch conducted in my lab at Duke University over the last two decades has\nhelped to show categorically that a single neuron can no longer be viewed as\nthe fundamental functional unit of the brain; instead, connected populations\nof neurons are responsible for the symphonies of thought composed by brains.\nToday, we can record the music produced by these neural ensembles and even\nreplay a small fraction of it in the form of concrete and voluntary motor\nbehaviors. By listening to just a few hundred neurons—an infinitesimally small\nsample of the billions of neurons in the brain—we are already beginning to\nreplicate the process by which complex thoughts become instantaneous body\nactions.\n\nWhat principles guide the composition and conduction of these neural\nsymphonies? After more than two decades delving into the workings of neural\ncircuits, I have found myself looking for those principles both outside the\nbrain, beyond the boundaries that have constrained our biological evolution\nout of humble beginnings in stardust, as well as deep inside the central\nnervous system, trying to identify and give voice to the brain’s own point of\nview. Here I propose that, like the universe that fascinates us so much, the\nhuman brain is a relativistic sculptor; a skillful modeler that fuses neuronal\nspace and time into an organic continuum responsible for creating all that we\nsee and feel as reality, including our very sense of being. In the following\nchapters, I will propose that, in the next decades, by combining such a\nrelativistic view of the brain with our growing technological ability to\nlisten and decode even larger and more complex neural symphonies, neuroscience\nwill eventually push human reach way beyond the current constraints imposed by\nour fragile primate bodies and sense of self.\n\nI can imagine this world with some confidence because of the work conducted by\nmy lab to teach monkeys to utilize a revolutionary neurophysiological\nparadigm, which we named brain-machine interfaces (BMIs). Using such BMIs, we\nwere able to demonstrate that monkeys could learn to control voluntarily the\nmovements of extraneous artificial devices, such as robotic arms and legs,\nlocated either close to or very far from them, using only their raw electrical\nbrain activity. This unleashes a vast array of possibilities for the brain and\nthe body that could, in the long run, completely change the way we go about\nour lives.\n\nTo test the different versions of our BMIs, we took advantage of a new\nexperimental approach to read directly and simultaneously the electrical\nsignals produced by hundreds of neurons that belong to a neural circuit. This\ntechnology was initially developed as a way to test the distributionists’\nviewpoint: that populations of single neurons, communicating with one another\nacross different brain regions, are required to generate any brain function.\nBut once we discovered how to listen to some motor neural symphonies played by\nthe brain, we decided to push further: to record, decode, and transmit—all the\nway to the other side of the world—the motor thoughts of a primate cortex. We\nthen translated these thoughts into digital commands to generate humanlike\nmotion in machines that were never designed to acquire such unique human\ntraits. It was at that moment that our BMIs stumbled on a way to liberate the\nbrain from the constraints imposed by the body and made it capable of using\nvirtual, electronic, and mechanical tools to control the physical world. Just\nby thinking. This book tells the story of those experiments and how they have\nchanged our understanding of brain function.\n\nFor the vast majority of people alive today, the full impact of our research\nwith BMIs will be felt primarily in the medical arena. Unraveling the brain’s\nintricate workings by building advanced BMIs will lead to the development of\namazing new therapies and cures for those afflicted by devastating\nneurological disorders. Such patients will be allowed to regain mobility and\nthe sense and feeling in an otherwise lame body through a variety of\nneuroprosthetics, devices the size of a modern heart pacemaker that harvest\nhealthy brain electrical activity to coordinate the contractions of a silk-\nthin wearable robot, a vest as delicate as a second skin but as protective as\na beetle’s exoskeleton—a suit capable of supporting a paralyzed person’s\nweight and making formerly immobile bodies roam, run, and once again exult in\nexploring the world freely.\n\nYet, BMI applications promise to reach way beyond the borders of medicine. I\nbelieve future generations will be in a position to enact deeds and experience\nsensations that few today can imagine, let alone verbalize. BMIs may transform\nthe way we interact with the tools we fabricate and how we communicate with\none another and with remote environments and worlds. To grasp what this future\nworld may look like, you first need to picture how the execution of a few of\nour daily routines will change radically when our brain’s electrical activity\nacquires the means to roam freely around the world pretty much like radio\nwaves sail above us today. For a moment, imagine living in a world where\npeople use their computers, drive their cars, and communicate with one another\nsimply by thinking. No need for cumbersome keyboards or hydraulic steering\nwheels. No point in relying on body movements or spoken language to express\none’s intentions to act upon the world.\n\nIn this new brain-centered world, such newly acquired neurophysiological\nabilities will seamlessly and effortlessly extend our motor, perceptual, and\ncognitive skills to the point that human thoughts can be efficiently and\nflawlessly translated into the motor commands needed to produce either the\nminute manipulations of a nanotool or the complex maneuvers of a sophisticated\nindustrial robot. In that future, back at your beach house, sitting in your\nfavorite chair facing your favorite ocean, you may one day effortlessly chat\nwith any of a multitude of people anywhere in the world over the Internet\nwithout typing or uttering a single word. No muscle contraction involved. Just\nby thinking.\n\nIf that is not enticing enough, how about experiencing all the sensations\naroused by touching the surface of a different planet, millions of miles away,\nwithout leaving your living room? Or even better, how would you feel if you\ncould access your ancestral memory bank and readily download the thoughts of\none of your forefathers and create, through his most intimate impressions and\nvivid memories, an encounter you both would have never shared otherwise? That\nis just a glimpse of what living in a world beyond the boundaries imposed upon\nthe brain by the body may bring to our species.\n\nSuch wonders will soon no longer be the stuff of science fiction. This world\nis starting to take shape before our very eyes, right here and right now. And\nto become immersed in it, as Dr. Timo-Iaria would say, all you have to do is\njust follow the music that begins playing on the very next page.\n\n\n1\n\nWHAT IS THINKING?\n\nBy the time the rainy days of the tropical autumn of 1984 arrived, most\nBrazilians had had enough. For twenty years, their beloved country had been\nruled by a vicious dictatorship, brought to power by a military coup d’état\nthat triumphed, emblematically, on April Fools’ Day 1964. For the next two\ndecades the military regime built an infamous legacy, marked primarily by its\nrampant incompetence, widespread corruption, and shameful political violence\nagainst its own people.\n\nBy 1979, thanks to the growing popular opposition to the regime, the latest\nfour-star general installed in the presidential palace had no alternative but\nto grant amnesty to the political leaders, scientists, and intellectuals who\nhad fled into exile abroad. A gradual, controlled return to civilian rule had\nbeen mapped out by the generals, beginning with popular elections for state\ngovernorships in the fall of 1982.\n\nThat November, the opposition parties won by a landslide. By the next year,\nhowever, that small token of democracy had been all but forgotten. Brazilians\nrealized they had the right and, more importantly, the power to demand more\nthan a dictator’s political bread crumbs. They wanted to oust the military\ngovernment, but not through another coup d’état. Instead, they wanted to vote\nit into retirement through a direct election for president. That is how,\nseemingly out of nowhere, a nationwide movement demanding immediate direct\nelections for president (diretas já in Portuguese) broke loose. The first\nrally took place in the tiny northeastern city of Abreu e Lima on March 31,\n1983. By November, a somewhat shy crowd of ten thousand people had gathered to\nprotest in Brazil’s most populous and wealthy city, São Paulo. From that\npoint, the movement grew exponentially. Two months later, on January 25, 1984,\nthe day São Paulo celebrated its 430th anniversary, more than two hundred\nthousand people were chanting their collective demand for immediate\npresidential elections. In a matter of days, gigantic crowds started to\nconverge on the main squares of Rio de Janeiro, Brasilia, and other major\ncities.\n\nOn the evening of April 16, 1984, more than one million people congregated in\nthe heart of downtown São Paulo to participate in the largest political rally\never staged in the country’s history. In a matter of hours, a river of people,\nmost dressed in the Brazilian national colors, green and yellow, inundated the\nvalley where the city was originally founded. Every new group of people that\narrived immediately joined into an already familiar, two-word rhythmic chant\nthat erupted briskly somewhere in the crowd, every minute or so, and spread\nlike thunder through space: “Diretas já, diretas já” (Elections now, elections\nnow). If you have never taken part in a chorale formed by one million people,\nI recommend the experience. Nothing can prepare you for its penetrating sound,\nand nothing this side of the Milky Way will allow you to forget it. It is the\nsort of sound that carves memories for a lifetime.\n\nPressed by the ever-increasing flow of people, I climbed to the roof of a\nnewsstand and, for the first time that night, gained a panoramic view of the\nentire citizenry that was conquering São Paulo’s Anhangabaú Valley with its\ntwo-word song. For the long-vanished Tupi-Guarani, the native Indian tribe\nthat inhabited that land before the Portuguese arrived in the sixteenth\ncentury, the stream that had run through the valley was known as the “river of\nthe bad spirits.” Not anymore. That night, the only river visible was a mighty\nAmazon of people. No bad spirit would have dared to exert itself in such a\npurposeful human ocean.\n\n“What do we want?” part of the crowd spontaneously asked.\n\n“Diretas” (elections), the rest of us answered.\n\n“Quando” (when)? another group provoked.\n\n“Já, já, já!” (now, now, now!) the whole crowd screamed back.\n\nWhen that million-people choir began to sing the Brazilian anthem, not even\nthe sky could hold its tears anymore. As the traditional São Paulo drizzle\ndescended, I absorbed this resounding demonstration of what a population of\nindividuals can do when they collaborate in harmony to achieve a common goal.\nEven though the message transmitted by the crowd (Diretas já!) was always the\nsame, at any moment in time, a different combination of many voices was\nrecruited to produce the crowd’s roar. People weren’t necessarily able to\nscream every time. Some were talking to their neighbors; others became\ntemporarily hoarse, or were distracted while waving their flags; others simply\ndropped out of the chorale due to sheer emotion. Moreover, even as handfuls of\npeople began to leave the rally later on, the crowd continued to thunder. For\nany observer, the loss of those few protesters did not make any difference at\nall—the overall potential population was so huge that the loss of a few people\ndid not meaningfully alter the result.\n\nUltimately, the voices of those millions of Brazilians were heard. A few days\nlater, I met with my mentor, Dr. César Timo-Iaria, to discuss a paper by David\nHubel and Torsten Wiesel, who had shared the Nobel Prize in Physiology or\nMedicine in 1981 for their groundbreaking research on the visual cortex. Hubel\nand Wiesel had recorded the electrical activity of single neurons in the\nvisual cortex, using the classical reductionist approach that was the norm in\nlabs around the world at the time. I innocently asked Timo-Iaria why we did\nnot do the same. His reply was as forceful as the roar that I had experienced\nas a member of the crowd in São Paulo: “We do not record from a single neuron,\nmy son, for the same reason that the rally you attended a few days ago would\nbe a disaster if, instead of one million people, only one person had showed up\nto protest,” he said. “Do you think that anyone would pay attention to the\nplea of a single person screaming at a political rally? The same is true for\nthe brain: it does not pay attention to the electrical screaming of a single\nnoisy neuron. It needs many more of its cells singing together to decide what\nto do next.”\n\n* * *\n\nHad I been more observant on that historic night in 1984, I may have\nunderstood that the dynamic social behavior of that thundering crowd had set\nbefore me most of the neurophysiological principles that I would obsessively\ninvestigate over the next quarter of a century. But instead of listening to a\nchorus of political protesters, I would be listening for the virtually unheard\nelectrical symphonies created by large populations of neurons.\n\nThese neural ensembles would eventually provide the means for liberating a\nprimate brain from its biological body. But in the mid-1980s, very few\nneuroscientists saw any reason to relinquish the reductionist experimental\nparadigm and its focus on single neurons. Perhaps this was because other\nscientific fields, including particle physics and molecular biology, had\nexperienced extraordinary success with reductionism; in particle physics, for\ninstance, the theory and ultimate discovery of smaller and smaller particles,\nsuch as quarks, proved to be a linchpin in the definition of the so-called\nstandard model, which continues to be the basis of our understanding of the\nphysical universe.\n\nRoughly speaking, in mainstream twentieth-century neuroscience, the\nreductionist approach meant breaking the brain into individual regions that\ncontained a high density of neurons, known as brain areas or nuclei, and then\nstudying the individual neurons and their connections within and between each\nof these structures, one at a time and in great detail. It was hoped that once\na large enough number of these neurons and their connections had been analyzed\nexhaustively, the accumulated information would explain how the central\nnervous system works as a whole. Allegiance to reductionism led most\nneuroscientists to dedicate their entire careers to describing the anatomical,\nphysiological, biochemical, pharmacological, and molecular organization of\nindividual neurons and their structural components. This painstaking and\nwonderful collective effort generated a tremendous wealth of data from which\nmany outstanding discoveries and breakthroughs resulted. With the unfair\nbenefit of hindsight, today one could argue that neuroscientists were trying\nto decipher the workings of the brain in the same way as an ecologist would\nattempt to study the physiology of a single tree at a time in order to\nunderstand the rain forest ecosystem, or an economist would monitor a single\nstock to predict the stock market, or a military dictator would try to arrest\na single protester at a time to reduce the effectiveness of a million-strong\nBrazilian chorus chanting diretas já in 1984.\n\nFor an observer who benefits today from the century-old work of the true\ngiants of brain research, it seems that what much of neuroscience still lacks\nis an experimental paradigm for dealing with the complexity of brain circuits.\nToday, systems formed by large numbers of interacting elements—things like a\npolitical movement, the global financial market, the Internet, the immune\nsystem, the planet’s climate, and an ant colony—are known as complex systems\nwhose most fundamental properties tend to emerge through the collective\ninteractions of many elements. Typically, such complex systems do not reveal\ntheir intimate collective secrets when approached by the reductionist method.\nWith its billions of interconnected neurons, whose interactions change from\nmillisecond to millisecond, the human brain is an archetypal complex system.\n\nPart of the neglect toward exploring the complexity of the brain could be\njustified by the tremendous experimental challenges involved in “listening”\nsimultaneously to the electrical signals produced by large numbers of\nindividual single neurons, distributed across multiple brain areas, in a\nbehaving animal. For example, at the time Brazilian crowds were fighting for\npresidential elections, no one in the neuroscience community was sure what\ntype of sensor could be implanted in the brains of animals so that many of\nthese minute neuronal electrical signals could be sampled simultaneously for\nmany days or weeks, while subjects performed a variety of behavioral tasks.\nMoreover, there was no electronic hardware or sufficiently powerful computer\navailable that neurophysiologists could readily utilize to filter, amplify,\ndisplay, and store the electrical activity generated by tens of individual\nneurons simultaneously. Neurophysiologists wondered, almost in despair, how\nthey might choose which neurons to record from in each brain structure. Worst\nof all, nobody had any idea how to analyze the huge mountain of\nneurophysiological data that would be generated in case these technical\nbottlenecks could somehow be solved.\n\nParadoxically, few neuroscientists ever doubted that the astonishing feats\naccomplished by the human mind—from the production of artificial tools to the\ngeneration of self-awareness and consciousness—arise from the brain’s huge\nnumber of neurons combined with their intricate pattern of massive parallel\nconnectivity. But for decades, any attempt to tackle the technical hurdles to\nlisten to brain symphonies was dismissed as a chimera, a high-tech\nexperimental utopia that might only be realized through an effort on the scale\nof the Manhattan Project.\n\nEssentially, all expressions of human nature ever produced, from a caveman’s\npaintings to Mozart’s symphonies and Einstein’s view of the universe, emerge\nfrom the same source: the relentless dynamic toil of large populations of\ninterconnected neurons. Not one of the numerous complex behaviors that are\nvital for the survival and prosperity of our species—or, for that matter, of\nour close and distant cousins, primates and mammals—can be enacted by the\naction of a single neuron, no matter how special this individual cell may be.\nThus, despite the great deal we have learned about how single neurons look and\nfunction, and despite innumerous scientific achievements of brain research for\nthe past century, the straightforward application of reductionism to brain\nresearch has proven to be insufficient and improper as a strategy to deliver\nthe field’s most cherished promise, a comprehensive theory of thinking.\n\nAll this means that the traditional and well-disseminated view of the brain,\nthe one espoused in artful prose and beautiful illustrations in most of the\nneuroscience textbooks, can no longer stand. In much the same way that\nEinstein’s theory of relativity revolutionized the classic view of the\nuniverse, the traditional single neuron–based theory of brain function needs\nto be categorically replaced by what amounts to a relativistic view of the\nmind.\n\n* * *\n\nThe first step in proposing any new scientific theory is to define a proper\nlevel of analysis for investigating phenomena and testing one’s hypothesis\nabout them. This allows for validating or falsifying the proposed theory—the\nessence of the scientific method. I contend that the most appropriate approach\nto understanding thinking is to investigate the physiological principles that\nunderlie the dynamic interactions of large distributed populations of neurons\nthat define a brain circuit (see [Fig. 1.1](part0005.html#fig1-1)). Neurons\ntransmit information to one another through long, projecting structures—their\naxons—which make discrete, noncontinuous contact (the synapse) with nerve cell\nbodies and their protoplasmic, treelike structures, called dendrites. In my\nview, while the single neuron is the basic anatomical and information\nprocessing-signaling unit of the brain, it is not capable of generating\nbehaviors and, ultimately, thinking. Instead, the true functional unit of the\ncentral nervous system is a population of neurons, or neural ensembles or cell\nassemblies. Such a functional arrangement, in which populations of neurons\nrather than single cells account for the information needed for the generation\nof behaviors, is also commonly referred to as distributed neuronal coding.\n\n![image](../images/00003.gif)\n\nFIGURE 1.1 The architecture of a neural circuit. Reproduction of a Ramón y\nCajal original drawing showing a neural circuit formed by many neurons. One\nsingle neuron and its cellular specializations are highlighted. In general,\ndendrites serve as the main neuronal specialization receiving synapses from\nother neurons. Axon terminals establish the neuron’s synapses with other brain\ncells. (Cajal’s drawing from “Histology of the Nervous System” was reproduced\nwith permission of the Cajal Legacy, Instituto Cajal [CSIC], Madrid, Spain.)\n\nThinking with populations of neurons! Even two of humanity’s most intimate\npossessions—a sense of self and a body image—are fluid, highly modifiable\ncreations of the brain’s mischievous deployment of electricity and a handful\nof chemicals. They both can change or be changed on less than a second’s\nnotice. And, as we will see, they do.\n\nDuring the first half of the twentieth century, so-called single-neuron\nneurophysiologists argued, with seemingly incontrovertible evidence, that\nafter sensory information was sampled from the external world through\nspecialized receptors—the skin, retina, inner ear, nose, and tongue—it\nascended through specific sensory nerve pathways that terminated in specific\ncortical areas. These areas were identified as the primary sites for\nprocessing sensory information in the cortex, with the somatosensory\n(tactile), visual, and auditory areas gaining particular prominence. During\nthe same period, however, an American psychologist, Karl Lashley, emerged as\nthe poster boy for the opposition: the distributionist camp. Lashley’s main\nobsession was to identify the location in which the brain stores a memory,\nwhich he called the engram. In his experiments, he would surgically remove\ncortical tissue from various areas of the brains of rats, monkeys, and apes,\nboth before and after the animals had been taught to perform particular\nbehaviors, which ranged from simple tasks (learning how to identify a\nparticular object visually and then jump or reach for it) to complex problem\nsolving (learning how to navigate an elaborate maze). After an animal was\ntrained, he measured the impact of the cortical lesions he had created on the\nanimal’s capacity to acquire or retain the behavioral skill, or habit, it had\nlearned. With this experimental process, he aimed to understand how\nassociations were built between sensory information and motor behavior.\n\nAccording to Lashley, after animals had been trained in a simple task, much of\nthe remaining cortex could be removed without affecting significantly the\nanimal’s behavioral performance—provided that some volume of the primary\nsensory cortex involved in the task was left intact. In fact, if just one-\nsixtieth of the primary visual cortex remained, the animal would retain a\nvisual-motor habit it had learned. Faced with simple tasks, the brain was\namazingly resilient in handling sensory information. In his classic article,\n“In Search of the Engram,” Lashley summarized his results as the “principle of\nequipotentiality,” whereby memory traces were distributed throughout the\nsensory area, not in a specific neuron or small group of neurons.\n\nYet, Lashley also had found that the brain was less able to recover from\ndamage when faced with more complex behavioral tasks. An animal would begin to\nmake task errors with a small number of lesions, and the amount of errors\nproduced was proportional to the cortical mass removed surgically. Once 50\npercent or more of the neocortex had been removed, the animal began to lose\nthe learned habit altogether, requiring extensive retraining. Based on these\nfindings, Lashley proposed a second principle of memory, the “mass action\neffect,” which stated that “some physiological mode of organization or\nintegrating activity is affected rather than specific associative bonds.”\nComplex problem solving became “disordered” when parts of the cortex were\ntaken out of commission.\n\nMany neuroscientists criticized Lashley’s conclusions. Even today, simply\nmentioning his name in a scientific talk invariably triggers all-knowing\nchortles of derision. Most of the scientific blowback was leveled at his\nexperimental approach, particularly in trying to create brain lesions and then\ncorrelating them with too simplistic or too complex tasks. Still, Lashley was\nable to show that there was more going on in the primary sensory cortices than\nmost neuroscientists were willing to acknowledge.\n\nUsually, academic battles turn out to be so bloody because their stakes are\noften so miserably low. Not in this case. Defining the functional unit of the\nbrain is a solemn endeavor. After all, this quest aims at pinpointing exactly\nwhich piece of organic matter decides, on your behalf, where your body starts\nand ends, what it feels like to be human, what deeper beliefs you hold, and\nhow your children and the children of your children will one day remember who\nyou were and what became of your legacy as a human being. Few human\nenterprises come close in relevance and drama as the ongoing search for the\ntrue reasons that make each of us feel so irrevocably different and unique,\nand yet so strikingly similar to our kin.\n\nA simple analogy helps to clarify the distinction between the two competing\nviews of brain function I am presenting here. Consider the role played by\nmusicians in a symphony orchestra. If you had tickets to hear a concert and\narrived to find out that just one bassoonist had showed up, you would be\nrather disappointed at the end of the night: no matter how proficient the\nmusician had been, and how much effort he or she put into the performance, you\nwould not be able to imagine the symphony’s full score—not even if, instead of\na bassoonist, the glorious violinist Anne-Sophie Mutter or the electrifying\npianist Maria João Pires were onstage. You would only get an appreciation of\nthe entire musical tapestry of the symphony if a significant number of\nmusicians performed together, simultaneously. For the distributionists, when\nthe brain creates a complex message or task using a large number of neurons,\nit is composing a type of symphony.\n\nA neuronal concerto.\n\nCoding a complex neuronal message or task into a large number of small,\nindividual fragments or actions is similar to the work of an orchestra—each\nfragment helps to create the meaningful whole, like the million human voices\nsinging “diretas já” that, with its sheer power, dethroned a dictator. This\nsort of distributed message strategy is often found in nature.\n\nDistributed strategies are present in many aspects of our daily lives. For\ninstance, the production of complex phenotypical traits—how our genetic makeup\nis expressed in, say, our physical appearance—often relies on the concurrent\nexpression of many genes distributed across an array of chromosomes. Another\nnatural distributed strategy involves multiprotein complexes, which operate\nwithin individual cells to perform a variety of functions ranging from DNA\ntranslation and repair to the release of chemicals, known as\nneurotransmitters, by neuronal synapses. Each protein is responsible for one\nspecific subtask and many proteins may interact together to achieve a rather\ncomplex operation. For instance, different protein complexes, embedded in the\nwidth of the lipid membrane of a single neuron, form a variety of membrane ion\nchannels. Each ion channel works like a tunnel through the membrane. When this\ntunnel opens, a particular ion (sodium, potassium, chloride, or calcium) can\nenter or exit the cell. Multiple ion channels cooperate to maintain or alter\nthe electrical membrane potential of a single neuron. A single ion channel\ncannot regulate this process, just like a single neuron cannot produce a\nmeaningful behavior. Instead, a population of diverse ion channels is needed\nfor neuronal cell membranes to work properly.\n\nDistributed strategies work at higher levels, too. For example, African lions\nusually hunt in packs, particularly when they want to capture large prey, such\nas a seemingly vulnerable elephant drinking at a water hole. This pack\napproach ensures that if one of the lions is killed by the elephant, the rest\nof the pack still have a chance to get that valuable elephant steak tartare by\nthe end of the night. Conversely, some of the most preyed-upon species usually\ndefend themselves from potential predators by gathering into dense groups when\nthey roam in search of food for themselves. Thus, flocks of migratory birds\ncrossing the thin air of the Himalayas, schools of fish navigating the glassy\ngreen shallows of the Caribbean, and swarms of capybaras, a South American\nrodent weighing more than one hundred pounds with menacing front teeth but\nlittle else to defend it, each rely on distributed strategies for protection\nagainst predators. By increasing the density of the pack of individuals\ntraveling together, they divide the attention of their nemesis and\nsignificantly reduce the probability that a given individual will be caught.\nBy doing so, the chance of perpetuating the group as a whole increases—a\ndistributed strategy of risk management.\n\nDoes this approach to handling risk sound familiar? When financial managers\nadvise you to diversify your portfolio, spreading your investments across a\nlarge number of companies representing multiple sectors of the economy, they\nare proposing exactly this sort of distributed strategy, without the\ncapybara’s menacing teeth. Even the most influential technology of our time,\nthe Internet, relies on distributed computer grids to fulfill our apparently\nlimitless thirst for information. No single computer controls the flow of bits\nand bytes across the whole system, and there’s no single cable connecting your\ncomputer to Google’s headquarters when you type in a request to find a Web\npage on a particular subject. Rather, huge numbers of interconnected machines\nvery quickly route your Google search to one of the company’s many computer\nservers in Mountain View, California. If one of these machines goes bonkers,\nno problem; the remaining computer network ensures that your query is not\nlost.\n\nBut why do distributed strategies work so well? Why, from proteins to packs of\ncapybaras, does it make sense to rely on large and distributed populations of\nindividual elements? To answer this fundamental question, let’s return to the\nbrain and examine the advantages of such a population-coding scheme for\nthinking.\n\nBy distributing thought across a large population of neurons, evolution has\ndesigned an insurance policy for the brain. In most cases, people do not lose\nan important brain function when a single neuron or a small chunk of brain\ntissue is damaged due to a localized trauma or a minor stroke. Indeed, because\nof distributed coding, a great deal of brain damage has to occur before\npatients exhibit any clinical signs and symptoms of neurological dysfunction.\nConversely, imagine the risks you would incur if only one of the neurons in\nyour entire brain was in charge of conveying a key aspect of your life, say,\nthe name of your favorite Brazilian soccer team. Lose that neuron, and that\ninformation would be forever lost. Yet, throughout our adult lives, individual\nneurons continuously die without any major side effects. The fact that we\nalmost never notice functional or behavioral effects, though these minuscule\nneuronal tragedies take place every day, speaks volumes in favor of\ndistributed coding in the brain. Neuron populations are highly adaptive, or\nplastic; when damaged or dead neurons need to be bypassed, the remaining ones\ncan self-reorganize, changing their physiological, morphological, and\nconnectivity makeup when repetitively exposed to tasks and environments. As my\nfriend Rodney Douglas, of the University of Zurich, recently noted, the brain\ntruly works like an orchestra, but a unique one, in which the music it\nproduces can almost instantaneously modify the configuration of its players\nand instruments and self-compose a whole new melody from this process.\n\nEvolution may also have favored distributed population coding because it is\nfar more efficient at delivering many complex messages than single-neuron\ncoding. Let’s take a simple example. Suppose that a single neuron can convey,\nor in neuroscience jargon, represent, two distinct messages by flipping\nbetween two frequencies of electrical firing, either very rapid firing or very\nslow firing. If just one neuron was devoted to detecting images in the visual\nfield of an animal, the animal’s brain would only be able to respond to two\ndistinct images—firing at a rapid rate when one of the images was detected and\nat a slow rate for the other. Any other images would not be discernible by the\nsingle neuron at that same moment. Now suppose that one hundred different\nneurons were allocated to perform the same job. The number of distinct images\nthat could be detected with the same two firing states would jump to 2100.\n\nIn addition to this dramatic increase in computational power and memory,\ndistributed coding in the brain relies on massive parallel information\nprocessing. Single neurons are capable of establishing an incredible number of\nconnections by giving rise to axon processes that branch and reach many other\ndifferent neurons simultaneously. This intricate mesh of neuronal connections\ncan achieve wondrous things. For example, as part of my doctoral thesis, I\ncreated a simple computer program that could store, in a square matrix format,\nthe direct connections shared between the pairs of brain areas and nuclei that\nform the circuit responsible for controlling cardiovascular functions. I then\nselected the most important forty brain structures that defined this circuit\nand identified which of the related forty nuclei were directly connected by a\nbundle of axons or nerves that uses only one synapse, called a monosynaptic\npathway. In my computer program’s forty-by-forty matrix, rows indicated the\nbrain structures from which neurons gave rise to such monosynaptic pathways;\nthe columns indicated the structures that received them. If structure number 4\nhad neurons that sent direct axonal projections to structure number 38, I\nnoted “1” in the respective matrix position (intersection of row 4 and column\n38). If neurons belonging to structure 38 reciprocated this connection and\nsent axons back to structure 4, another “1” was added to the intersection of\nrow 38 and column 4. If there was no direct connection between a given pair\n(for instance, between nuclei 5 and 24), a “0” was added to the respective\nmatrix position (see a reduced example in [Fig. 1.2](part0005.html#fig1-2)).\nHaving gone through the trouble of building such a detailed matrix of direct,\nmonosynaptic connections, I decided to ask a very simple question: given all\nthe known pairwise connectivity of the circuit, how many neural pathways\nexisted that could connect any pair in this circuit that did not have a direct\nmonosynaptic connection? In other words, was there any way for information to\nflow between two unconnected pairs in the circuit? With that question in mind,\nI set a series of twenty IBM-PC XT microcomputers to run my program, hoping to\nget an answer. Each of these computers was supposed to seek potential\nmultisynaptic pathways linking one of twenty distinct pairs of brain\nstructures that did not share a direct monosynaptic pathway. At the end of\nthis search, each computer would then print out the potential pathways in a\nlist and a summary graph. I then headed out for a five-day holiday, to\ncelebrate that most sacred of Brazilian religious events, Carnival.\n\nImagine my shock when, upon returning to the lab, I found that piles and piles\nof printouts had been generated by half of the computers. The programs running\non those ten computers had identified thousands of potential multisynaptic\npathways for connecting pairs of structures that did not talk directly to each\nother ([Fig. 1.2](part0005.html#fig1-2)). More surprising, of the other ten\ncomputers, some had not yet finished printing the potential pathways, while\nothers had simply run out of paper. Even with just a handful of direct\npairwise nuclei connections, there were hundreds of thousands or even millions\nof potential pathways for exchanging information between pairs of brain\nstructures that did not share a monosynaptic connection.\n\nBy relying on large populations of interconnected neurons and massive parallel\nprocessing to encode information, advanced brains like ours become dynamic\nsystems in which the whole becomes larger than the sum of its individual\nparts. That happens because the overall dynamic interactions of the network\ncan generate complex global patterns of activity, known as emergent\nproperties, which cannot be predicted from the outset by the linear sum of the\nindividual features of individual elements. Such extreme nonlinear behavior\nenhances dramatically the physiological and behavioral outcomes that can\nemerge from the neural networks of the brain. Distributed networks formed by\nmillions or even billions of neurons generate emergent properties such as\nbrain oscillations, complex rhythmic firing patterns that underlie a variety\nof normal and pathological functions including certain states of sleep and\nepileptic seizures. Emergent brain properties also generate highly elaborate\nand complex brain functions, such as perception, motor control, dreaming, and\na person’s sense of self. Our very consciousness, arguably the most awesome\nendowment known to us, likely arises as an emergent property of a multitude of\ndynamically interacting neuronal circuits of the human brain.\n\n![image](../images/00004.jpeg)\n\nFIGURE 1.2 Use of graph theory to study the distribution of pathways linking\npairs of neurons. On top, a square matrix is used to represent the direct,\nmonosynaptic connectivity of a small brain circuit. In this matrix, 1\nrepresents the existence of a direct connection between a pair of brain\nstructures, while 0 depicts its absence. Next to the matrix, a graph is used\nto represent the circuit. Circles with numbers represent the structures and\ndirectional arrows represent the direct connectivity information contained in\nthe square matrix. The histogram below depicts the total number of pathways\nlinking two structures (carotid baroreceptor and cerebellum) that did not\nshare a direct, monosynaptic connection. The X axis represents the number of\nsynapses of the pathways and the Y axis depicts the number of pathways found.\nNotice that millions of pathways were found for this particular example.\n(Courtesy of Dr. Miguel Nicolelis; redrawn by Dr. Nathan Fitzsimmons, Duke\nUniversity.)\n\nBut the new view of the brain I propose involves much more than a simple shift\nin emphasis from a single neuron to populations of connected brain cells. Up\nto now, most neurophysiological theories have consistently ignored the fact\nthat highly elaborated brains do not sit tight and wait for things to happen.\nInstead, these brains take the initiative and actively gather information\nabout the body in which they are embedded and its surrounding world,\ntirelessly and diligently sewing the cloth of reality, opinions, loves, and, I\nam afraid, even prejudices that we proudly, and sometimes blindly, wear every\nmillisecond of our lives, blissfully unaware of where it all comes from. This\nactive information-seeking maintains what I call the “brain’s own point of\nview”: the combination of the brain’s accumulated evolutionary and individual\nlife history, its global dynamic state at a given moment in time, and its\ninternal representation of the body and the external world. All these\ncomponents, which comprise our most intimate mental existence, merge into a\ncomprehensive and exquisitely detailed rendition of reality.\n\nThe “brain’s own point of view” influences decisively the way we perceive not\nonly the complex world around us, but also our body image and our sense of\nbeing. The Cartesian assumption, which poses that our brains passively\ninterpret or decode signals coming from the outside world, without any\npreconceived viewpoint attached to it, can no longer stand up to the\nexperimental evidence. In fact, to fulfill its enormous scientific\npotential—from unveiling the intricate physiological principles that govern\nthe operation of the human brain to developing brain-machine interfaces\ncapable of both rehabilitating patients devastated by neurological diseases\nand greatly augmenting human reach—mainstream neuroscience must divest itself\nfrom its twentieth-century dogma and wholeheartedly embrace this new view.\n\nIn his masterpiece, The Organization of Behavior, published in 1949, the\nCanadian psychologist Donald O. Hebb promoted the concept that cell assemblies\nare the true functional unit of the nervous system. A student of Lashley’s,\nHebb also postulated that no “single nerve cell or pathway [is] essential to\nany habit or perception.” He also pointed out that “electrophysiology of the\ncentral nervous system indicates … that the brain is continuously active, in\nall its parts. An afferent [incoming] excitation [from the body’s periphery]\nmust be superimposed on an already existent excitation [inside the brain]. It\nis therefore impossible that the consequence of a sensory event should often\nbe uninfluenced by the pre-existent [brain] activity.”\n\nI propose that the brain’s work results from the dynamic interplay of billions\nof individual neurons that create a continuum in which neuronal space and time\nseamlessly combine. In a fully behaving animal, as Hebb proposed, no incoming\nsensory stimulus is processed without first being compared against the brain’s\ninternal predispositions and expectations, arduously built through the\ncollection of signals and memories from previous encounters with similar, and\neven not so similar, stimuli earlier in life. The diffuse electrical response\nevoked in the brain of a conscious subject when a novel message arrives from\nthe periphery of the body seems to depend heavily on the internal state of the\nbrain at that particular moment in time. Thus, while the constancy of the\nvelocity of light determines why space and time have to be relativized in\nrelation to the state of motion of a pair of observers in the universe, I\ncontend that evolutionary and individual history, the fixed maximum amount of\nenergy a brain can consume, and the maximum rate of neuronal firing offer the\nconstraints that require an equivalent relativization of space and time within\nour heads.\n\nMost information about the world and our body comes to the brain as a result\nof exploratory actions initiated by the brain itself. Perception is an active\nprocess that starts inside our heads and not at a peripheral site on the body\nwith which the outside world happens to come in contact. Through a variety of\nexploratory behaviors, the brain continuously tests its own point of view\nagainst the new information it encounters. Even though we routinely experience\nthe “feeling” on our fingertips of such tactile attributes as texture, shape,\nand temperature, in reality these sensations are skillful illusions crafted by\nthe brain—“felt” during the split second in which our fingertips make contact\nwith an object and collect and transmit the sensory data via the nerves back\nto the brain. If the feeling doesn’t match the brain’s expectations, it will\ncorrect the mismatch by creating a moment of surprise and discomfort, like the\none that emerges when you reach into a package of bread and drop the slice\nwhen you find it is wet and slippery rather than dry and crumbly. The same\nprocess takes place during our elaborate, simultaneous visual, auditory,\nolfactory, and gustatory “experiences” of the world. All these indisputable\nhuman traits are borne by massive electrical brainstorms, which we usually\nrefer to, more colloquially, as the act of thinking.\n\nBut could we push the definition of thinking any further? I believe so. I\npropose that the brain is actually the most awesome simulator to evolve in the\nknown universe, at least as far as we can independently verify. Like a\nfaithful and patient modeler of reality, the main business of our brains is to\nproduce a large variety of behaviors that are vital for our existence as human\nbeings. In essence, these physiological purposes boil down to:\n\n(a) maintaining our bodies in working order through the global physiological\nprocess called homeostasis;\n\n(b) building and storing very detailed models of the external world, of our\nlives, and of the continuous encounters between the two; and\n\n(c) actively and continuously exploring the surrounding environment in search\nof new information to test and update these internal models. This includes\nlearning from experience and predicting future events and their payoffs by\ngenerating potential expectations for their outcomes, costs, and benefits.\n\nThis short list covers most of the basic functions of the central nervous\nsystem.\n\nBy definition, a good simulation or model allows its user to continuously\nanalyze and monitor all sorts of events to predict future outcomes.\nNeurophysiologists have spent a great deal of time investigating how the brain\nmaintains the body’s homeostasis, and in recent decades there’s been an\nexplosion of research into how the brain encodes sensory, motor, and cognitive\ninformation. But for the most part, because of the difficulty in studying\nthese phenomena experimentally, they have avoided the highly complex behaviors\nthat are encompassed in building and nurturing a model of the world, the\npervasive and primordial human longing to create a detailed account, no matter\nhow mystical and abstract, of how the universe was created, how humanity\nemerged, and why we have the gift of life in this otherwise mundane solar\nsystem. Often, these longings are left to the realm of religious inquiry. But\nthese same complex behaviors also endow humans with an ardent curiosity, a key\nand unique trait of our species, which has led to the emergence of art as well\nas scientific thinking. Complex behaviors also encompass the elaborate social\nand courtship strategies employed by humans to achieve the evolutionary goal\nof transmitting genes to future generations, as well as our continuous\nattempts to imprint our ideas, dreams, beliefs, fears, and passions into the\nmemories of our loved ones, friends, and other members of our species.\n\nBy now you may be thinking that the theoretical shift I am proposing is no big\ndeal. Yet, this issue has played a central role in an ongoing theoretical\nscrimmage that has engulfed neuroscience during a two-hundred-year\nintellectual battle over the brain’s soul. And as it happens, the notion of\nthe brain as a model builder has found significant support outside the\nneuroscience community. In his classic book, The Selfish Gene, the British\nevolutionary biologist Richard Dawkins espouses the view that the brain,\nparticularly that of humans, has evolved the enormously advantageous capacity\nof creating very elaborate simulations of reality. The physicist David Deutsch\ngoes even further by proposing, in his book The Fabric of Reality, that all\n“we experience directly is a virtual-reality rendering, conveniently generated\nfor us by our unconscious mind from sensory data plus complex inborn and\nacquired theories (i.e. programs) about how to interpret them.”\n\n* * *\n\nIn the first paragraph of his masterpiece book, Cosmos, Carl Sagan muses, “The\nCosmos is all that is or ever was or ever will be. Our feeblest contemplations\nof the Cosmos stir us—there is a tingling in the spine, a catch in the voice,\na faint sensation, as if a distant memory, of falling from a height. We know\nwe are approaching the greatest of mysteries.”\n\nAs far as we know, there is only one offspring of this awesome cosmos capable\nof deciphering its majestic language while producing a reel of luxurious\nsensations that our true parents, faraway deceased supernovas, never had the\nprivilege to enjoy. While these progenitors burned away, unaware that their\nstardust would one day blow the breath of life on a small bluish planet\nrevolving around an average star located in a remote corner of a distant\ngalaxy, our brains endow us to lustily consume every bit of our conscious\nexistence while silently carving in our minds the many intimate tales of a\nlifetime.\n\nThus, if ever there has been a scientific battle worth fighting for, it is the\none in which neuroscientists have embroiled themselves for the past two\ncenturies. And if you asked me to take a side, I would not hesitate a\nmillisecond to say that, at the end of this intellectual brawl, as Brazilians\nproved twenty-five years ago, those siding with the inebriant plea generated\nby another huge crowd, one formed by billions of interconnected neurons, shall\nprevail.\n\n\n2\n\nBRAINSTORM CHASERS\n\nSir Edgar Douglas Adrian was keenly aware of the disputes that tormented the\nfounding fathers of neuroscience. Installed at a lectern in the College of\nSaint Mary Magdalene at Oxford University in 1946, Adrian, himself a Cambridge\nman, set out to describe what in his opinion was the first major achievement\nin understanding the brain: an argument about the seat of “intelligence.” He\nintoned that the brain “is a particular structure of nerve-cells and nerve-\nfibres, found in certain animals but not in all, and some animals which have\nno brain in the strict anatomical sense have none the less a complex\nbehaviour, well adapted to the circumstances, a behaviour which we might well\ncall intelligent. And within our own bodies there are many cells swimming\nfreely in the blood and behaving as more or less independent living beings,\navoiding what is bad for them and selecting what is good. Have they any claim\nto minds?” Adrian then related that more than two centuries earlier, at the\nend of the seventeenth century, philosophers at Cambridge and Oxford raged\nover the question of whether intelligence was situated in one part of the\nbody—the brain—or throughout it. The Cambridge dons took the side of a\nsingular site, while the Oxford dons looked farther afield.\n\nTo make his point, Adrian slyly quoted the opening stanzas of the poem “Alma:\nor, The Progress of the Mind,” published in 1718 by Matthew Prior, who had\nearlier been a lecturer in medicine at his very own Cambridge. After mocking\nthe idea that the mind, in a “slap dash” manner beholden to ancient\nAristotelian philosophy, “runs here and there like Hamlet’s ghost” throughout\nthe body, Prior describes the argument in favor of the brain:\n\nThe Cambridge wits, you know, deny\n\nWith ipse dixit to comply.\n\nThey say (for in good truth they speak\n\nwith small respect of that old Greek)\n\nThat putting all his works together\n\nTis three blue beans in one blue bladder.\n\nAlma they strenuously maintain\n\nSits cock horse on her throne the brain\n\nAnd from that seat of thought dispenses\n\nHer sovereign pleasure to the senses\n\nWith this bravura performance of British wit, Adrian simultaneously teased his\nOxford rivals while paying tribute to his Cambridge ancestors, the latter of\nwhom had recognized that the brain was the sole culprit for the vicissitudes\nof the human mind. More than six decades later, I can almost imagine the\ncircumscribed smirk quivering across the great man’s face as he recited\nPrior’s poem to the assembly.\n\nAdrian was granted the latitude to make this academic jab. After all, he had\nbeen the very first neuroscientist to measure exactly how sensory information\nabout the surrounding world and the body is coded by the peripheral nerves\ninto the electrical signals that are the language of the brain—work that had\nwon him a share of the Nobel Prize in 1932. An earlier researcher, Keith\nLucas, had proposed that these electrical sparks, later named “action\npotentials,” were all-or-none in nature. Adrian probed further into this idea\nand discovered that the intensity of the stimuli, whether tactile, olfactory,\ngustatory, or visual, related to the frequency of the action potentials\ntransmitted in the peripheral nerves.\n\nIt was fitting then that in his lecture at Saint Mary Magdalene, Adrian also\nevoked the great dispute between two Italian scientists, the physician Luigi\nGalvani and the physicist Alessandro Volta, to revisit how electrophysiology\nwas born of accident and soon became a significant field of scientific\ninquiry. Around 1783, Galvani had learned that by touching the leg muscle of a\ndead frog with two contacting strips, made of distinct types of metal, he\ncould induce the muscle to contract. He interpreted this finding as proof that\nelectricity was stored in the dead muscle fibers and that he had found the\nsecret driving force of life. Volta was shocked by this naive conclusion and\nstrongly, but respectfully, pointed out that electricity was most likely being\ncreated by the two distinct metal strips contacting the muscle and one\nanother. Volta was a serious scientist, after all, so he knew that he would\nneed to prove his point of view with evidence. According to Volta, the frog\nleg muscle was actually doubling as a conductor and a biological detector for\nthe electrical current generated by Galvani’s probe made of two distinct\nmetals. Convinced of this interpretation, Volta went on to design the first\nelectric battery, also known as a voltaic pile, by substituting brine-soaked\npaper for the frog’s muscle as the conductive material filling the space\nbetween two metal plates made of zinc and silver.\n\nAs Adrian noted, it seemed that Volta was the clear winner of this electrical\ntempest, leaving poor Galvani with the wondrous prospect of being forever\nknown as the senseless experimentalist who could not even correctly interpret\nhis own results. Indeed, very few people today recall that Galvani should be\ncredited for designing the first, rudimentary neuroprosthetic device, capable\nof artificially stimulating a nerve in a muscle—for this very same research.\nFortunately for Galvani, other scientists soon obtained conclusive evidence\nthat both living muscle and nervous tissue generated electrical currents. This\ndiscovery of animal electricity, though, really wasn’t the major shock that\nVolta claimed it to be. In fact, the electrical currents were quite tiny,\nwhich explains why it proved so difficult for so long to measure them\naccurately.\n\nNature, it seems, does not write its riddles with just a few notes. More often\nthan not, it tends to compose a symphony with a variety of tonal and rhythmic\nnuances that invariably sound strange and novel to our rather narrow\nperceptual skills. Scientists who, when faced with new evidence or fringe\nphenomena, resist modifying their pet theories usually find that the noise in\nthe data, no matter how counterintuitive, is telling them something essential.\n\n* * *\n\nIn a mind-boggling way, the fundamental debates in brain research not only\nresemble but are historically intertwined with the classic struggle among\nphysicists to decide whether light should be considered to be formed by a\nwavelike phenomenon or particles, the latter theory favored by both Sir Isaac\nNewton and his gravitational nemesis, Albert Einstein. An extraordinary\nBritish physicist at Cambridge named Thomas Young—who was also an\nEgyptologist, linguist, physician, physiologist, and neuroscientist—played a\nkey role in both of these scientific controversies.\n\nYoung’s unprecedented scientific career tends to provoke the appropriate awe;\none biography of him, written by Andrew Robinson, is titled The Last Man Who\nKnew Everything: Thomas Young, the Anonymous Polymath Who Proved Newton Wrong,\nExplained How We See, Cured the Sick, and Deciphered the Rosetta Stone, Among\nOther Feats of Genius. One of those feats involved his ingenious and now\nclassic double-slit experiment, which is now known simply as Young’s\nexperiment. By flashing light through a thin plate that contained two parallel\nvertical slits separated by a short distance, Young observed that an\nalternating pattern of bright and dark bands appeared on a screen placed\nbehind the slits. Because this pattern resembled the “interference pattern”\nobserved when two waves of water, produced by two stones thrown into a lake at\nthe same time, collide, Young proposed that light was in fact a wave. Many\nphysicists, like the genial Richard Feynman, credit Young’s double-slit\nexperiment as the founding event of quantum mechanics.\n\nIncredibly, just a year after this revolutionary experiment, Young began\nformulating what became his theory of distributed neural coding, called the\ntrichromatic theory of color vision. Not too shabby for the eldest of ten\nchildren of a Quaker family roosted in Milverton, Somerset.\n\nI was initially introduced to Young’s work as a result of my friendship with\nRobert Erickson, whom I met shortly after I arrived at Duke, where he was a\nsenior professor in the Department of Psychology. A well-known gustatory\nphysiologist, Erickson was an ardent supporter of the distributionist notion\nthat brains relied on neural populations to encode information. He was also\none of the few people alive to have traced the origin of the debate between\nlocalizationists and distributionists in neuroscience to the disputes between\nThomas Young and the phrenologist Franz Gall.\n\nBy all accounts, Gall was himself an accomplished anatomist. Just a couple of\nyears before Young published his trichromatic theory in the Philosophical\nTransactions of the Royal Society, Gall began to popularize his purportedly\nclinical method, then called “cranioscopy,” for identifying fundamental\ncharacter traits and mental skills through careful analysis of a person’s\nskull. Gall argued that certain areas of the cerebral cortex would grow\ndisproportionately in people endowed with certain artistic skills, mental\nabilities, and aberrant behaviors. This localized differential growth of the\ncortex, according to Gall, would impinge upon the actual form of the skull,\nallowing a skillful examiner, like himself, to palpate a person’s head and\nannounce the unique aptitudes and shortcomings of his or her brain, even\nwhether it would make the person a gifted writer—or a cold-blooded murderer.\nGall divided the brain into twenty-seven “organs” (translated as skull bumps),\nnineteen of which he said were shared by all animals, up to and including\nhumans. In addition to the organs dedicated to basic emotions such as the\ninstinct of reproduction, the love of one’s offspring, pride, arrogance,\nvanity, and ambition, there were organs specified to dictate a person’s\nreligion, poetic talent, and firmness of purpose and perseverance. According\nto Gall’s schema, people endowed with an unusually accurate memory exhibited\nprotruding eyeballs.\n\nDuring his lifetime, most of the medical and scientific community strongly\ndisagreed with Gall’s outlandish conclusions. That did not stop Gall and his\ndisciples from disseminating his ideas in lectures all over Europe,\nparticularly the notion that mental functions are spatially localized in\nspecialized modules in the cortex. Yet, as Robert Erickson always mentioned in\nhis papers, there is no escaping from history: localizationist neuroscience is\nthe legacy of Gall, just as distributionist neuroscience is the legacy of\nYoung.\n\nAlthough he never mentioned it to me during our years together on the Duke\nfaculty, I later discovered that Erickson carried the heritage of another\nscientific dynasty. Erickson had studied under Carl Pfaffmann, whose\ngroundbreaking work on the gustatory nerves in cats provided evidence that,\neven at the level of peripheral nerves, information could only be coded by the\nconcurrent activation of many broadly tuned nervous fibers. As Erickson later\nrecounted in one of his review articles, Pfaffmann asserted that “in such a\n[gustatory] system, sensory quality does not depend simply on the ‘all or\nnothing’ activation of some particular fiber groups alone, but on the pattern\nof other fibers active.”\n\nPfaffmann’s lab was located in the physiology department at Cambridge\nUniversity. In a small footnote to the article, Erickson shares the delightful\nstory of how his mentor got his start in gustatory research. At Cambridge,\nPfaffmann collaborated on research with Lord Adrian. By then, Adrian had\nbasically claimed for himself the investigation of almost all of the\nperipheral sensory nerves, as well as their central projections. His empire\ncovered the visual, auditory, olfactory, and somatosensory (touch) systems.\nThe only system Adrian did not grab for himself was taste. As Erickson put it,\n“This he assigned to Pfaffmann.” Erickson was clearly gratified that he\nbelonged to such a distinguished scientific family tree. His pride was evident\nwhen, prior to quoting the original formulation of Young’s trichromatic\ntheory, he stated that this “hypothesis for the encoding of color is\nsuccinctly put in what are arguably the two most powerful sentences in the\nhistory of neuroscience.”\n\nEssentially, without any other source of information but that provided by\nYoung’s brilliant logic, the trichromatic theory of color vision predicted the\nexistence of three distinct types of receptors for color vision in the human\neye. Here is the first of the two sentences lauded by Erickson, with his\nclarifications of the scientific jargon noted in parentheses, in which Young\ndefined the trichromatic theory in 1802:\n\nNow, as it is almost impossible to conceive each sensitive point of the retina\nto contain an infinite number of particles (receptors), each capable of\nvibrating in perfect unison (responding) with every possible undulation\n(wavelength), it becomes necessary to suppose the number limited; for instance\nto the three principal colors, red, yellow, and blue, of which the undulations\nare related in magnitude nearly as the numbers 8, 7 and 6; and that each of\nthe particles is capable of being put in motion less or more forcibly, by\nundulations differing less or more from a perfect unison; for instance, the\nundulations of green light, being nearly in the ratio of 6½, will affect\nequally the particles in unison with yellow and blue, and produce the same\neffect as a light composed of those two species; and each filament of the\nnerve may consist of three portions, one for each principal color.\n\nFive years later, Young would go further, arguing that “the different\nproportions in which they (the sensations) may be combined, afford a variety\nof tints beyond all calculations.” It took some time, but late in the\ntwentieth century experimentalists finally demonstrated the existence of\nYoung’s retinal color receptors—the three types of retinal cones.\n\nIn his extremely rich book, Origins of Neuroscience, the historian and\nneuroscientist Stanley Finger details how Young’s trichromatic theory was\nrescued from anonymity by the physician and physicist Hermann von Helmholtz,\nwho provided both the data and a more mathematical formulation to validate it\nfully. Finger also argues that Young’s theory inspired Johannes Müller to\ndevelop his theory of specific nerve energies. This theory postulates that the\nperception of different sensations emerges as a direct result of the\nstimulation of particular receptors and nerves. But as much as I enjoyed\nFinger’s book, I cannot agree with him on this last assessment. If anything,\nYoung’s theory suggests the opposite—that is, that a particular sensation, in\nthis case color vision, depends on a pattern of activation across many\ndistinct nerve fibers.\n\nThis idea is usually understood more readily by examining a graphic display of\nYoung’s model for color-coding in the retina ([Fig.\n2.1](part0006.html#fig2-1)). This graph depicts the bell-shaped response\ncurves of the three retinal receptors postulated by Young. Although each of\nthe receptors responds maximally to the presence of one of three main colors\n(blue, green, or red), it also has the capacity to respond, albeit at lower\nand graded magnitudes, to the presence of other colors. That is the definition\nof a broadly tuned receptor, or neuron: a biological “detector” capable of\nresponding submaximally to the presence of a broad range of values of a given\nphysical entity, such as light, pressure, sound, or chemical concentration,\nand maximally to a particular value of this range.\n\nAn important detail to keep in mind is that the bell-shaped curves that\ndistinguish the response profiles of the three retinal receptors overlap\nconsiderably in the dimension of wavelength (color). This means that a given\ncolor stimulus is likely to trigger a response, albeit a different one, from\neach of the three receptors. This graph also shows how Young’s three retinal\nreceptors would collaborate to indicate the presence of a large set of\ndistinct colors. For each color stimulus delivered to the retina, there is a\nparticular distributed population pattern produced by the sum of the distinct\nelectrical firings generated by each of the three retinal receptors. For\nexample, in the case of a distinct color “P,” the unique retinal response\npattern is defined by an almost maximal signal from receptor 1, a 20 percent\nmagnitude signal from receptor 2, and a nil response from receptor 3.\nConversely, the color “Q” induces an almost maximal signal from receptors 1\nand 2, and a 20 percent magnitude signal from receptor 3. By using just three\n“broadly” tuned receptors, the retina acquires the power to represent the\npresence of an incomprehensibly large set of colors. As we saw in chapter 1,\nthis is one of the most remarkable advantages of distributed neural coding:\nthe stunning capability of representing a huge number of messages that far\nexceeds, by many orders of magnitude, in fact, the number of elements employed\nby the neuronal population. And without any of today’s high-tech instruments,\nThomas Young imagined this capability. Just by thinking!\n\n![image](../images/00005.jpeg)\n\nFIGURE 2.1 Thomas Young’s trichromatic theory of color. A portrait of Thomas\nYoung and a graphic representation of his theory on the right. Notice that any\ncolor stimulus (P, Q, R, or S in the X axis of the plot on the bottom left)\ncan be represented by the graded response of three distinct “color receptors,”\nwhich responds maximally to red, green, and blue respectively, but can also\nrespond submaximally to different colors. (Young’s portrait was reproduced\nwith permission of the National Portrait Gallery, London. The figure was\noriginally published in M.A.L. Nicolelis, “Brain-Machine Interfaces to Restore\nMotor Function and Probe Neural Circuits.” Nature Reviews Neuroscience 4\n[2003]: 417–22.)\n\nWe now know that Young’s broadly tuned neurons exist throughout the primate\nbrain. This fact was not known in the nineteenth century, and a great battle\nfor the “functional soul” of the brain was waged by neuroscientists. In 1861,\nhowever, the localizationists scored a mighty blow with the publication of a\nclinical study by the French physician Paul Broca. He reported the case of a\npatient who had experienced a profound loss of fluent speech—he was only able\nto utter the nonsense word “tan,” no matter what he was trying to say—who was\nalso severely paralyzed down the right side of his body. The man died soon\nafter Broca examined him, so the patient’s brain was recovered and dissected.\nTo his astonishment, Broca found extensive lesions in the middle, convex\nsection of the brain’s left frontal lobe. Here was hard evidence that mental\nfunctions were discretely localized in distinct parts of the brain. Broca\nquickly tried to dissociate his finding from phrenology, stating that this\n“speech” center was not in the same place as the skull bump postulated many\ndecades earlier by Gall and his progeny. Yet, as Finger shrewdly notes, both\nlanguage centers were located in the frontal lobe, and that was enough. There\nwas no escaping Gall’s bulging-eyed ghost.\n\nBroca’s discovery caused a commotion in the medical community, and many\nneurologists became converts to the ideology of a brain comprised of\nspecialized functional areas. Nine years later, two German scientists, Eduard\nHitzig and Gustav Fritsch, delivered what appeared to be the true coup de\ngrâce. By applying mild electrical currents sequentially to different regions\nof the frontal cortex in dogs, Hitzig and Fritsch elicited obvious muscle\ncontractions in distinct parts of the animals’ bodies. The researchers also\ndemonstrated that surgically removing a particular region in one hemisphere of\nthe cortex produced a noticeable, though not complete, deficit in the strength\nand maneuverability of the dogs’ forepaws on the opposite side of the body.\nHitzig and Fritsch used this data to sketch out a complete motor map of a\ndog’s body in a well-defined part of the frontal lobe that became known as the\nmotor cortex. Such topographic representations of an animal’s body are also\nknown as somatotopic maps. A century later, we know that our brains contain\nseveral of these maps, not just in the frontal lobe, but also in several areas\nwithin the parietal lobes and in many subcortical structures.\n\nSpectacular as these findings were, they were soon overshadowed by the growing\ninfluence of a new breed of experts armed with microscopes and brain tissue\nstained by chemical reactions. These histologists would stage their decisive\nshowdown with the last remaining general of the distributionist opposition in\nthe nineteenth century, during the ceremonies marking the award of the 1906\nNobel Prize in Physiology or Medicine.\n\n* * *\n\nLike any year, the twelve months of 1906 witnessed their fair share of\ntragedies, triumphs, and memorable human achievements. In April, San Francisco\nwas shaken by the horrendous earthquake that embroiled the city and killed\nmore than three thousand of its residents. In August, another earthquake\nreduced to rubble the city of Valparaiso, the coastal resort in Chile, where\nthree thousand people died. In Italy, Mount Vesuvius erupted. Lava,\nincandescent rock, and ashes spread over Pompeii and Naples, killing hundreds\nand dislodging thousands of people.\n\nThe night before the San Francisco earthquake hit, the great Italian tenor\nEnrico Caruso sang the role of José in the opera Carmen at the Tivoli Opera\nHouse. Awakened by the enormous jolt of the quake, Caruso ran down the stairs\nof the Palace Hotel to seek refuge in the street. Legend has it that, holding\na picture autographed by President Roosevelt as his only form of\nidentification, Caruso managed to escape the burning city by gaining passage\non a boat to New York City. In November, Caruso was brought to face a New York\nCity judge and accused of an indecent act allegedly committed in the monkey\nhouse of the New York City Central Park Zoo. Mrs. Hannah Graham alleged that\nCaruso had pinched her bottom—unceremoniously and uninvitedly. In his defense,\nthe singer, making sure he spoke in a way that would preserve his voice for an\nupcoming staging of La Bohème at the Metropolitan Opera, claimed that most\nlikely Mrs. Graham was pinched by a monkey. The judge did not buy his story.\nCaruso was fined ten dollars and released.\n\nCaruso’s best American friend, President Theodore Roosevelt, also had a busy\nyear. After becoming the first U.S. president to travel abroad to visit the\n“ditch,” his favorite nickname for the Panama Canal, in December, to his own\nastonishment, he was informed that he had been awarded the Nobel Peace Prize\nfor his role in mediating a truce agreement in the war between Russia and\nJapan. To his delight, he was now part of a very special and select group of\nindividuals and an official guest to a ceremony that, by all accounts, changed\nthe future of brain research.\n\nOn the characteristically chilly but scientifically red-hot Swedish night of\nDecember 10, 1906, the great hall of the Swedish Royal Academy of Music in\nStockholm was filled by the nation’s royal family, members of parliament,\ndistinguished scientists, and—as one of the laureates of the night wrote in\nhis memoir—“many elegant ladies.” They, along with members of the late Alfred\nNobel’s family, waited solemnly for His Majesty the King of Sweden to formally\naward that year’s Nobel Prizes. Likely, few of the people assembled would have\nbeen unaware of the tension between the two men who would share the prize\ngiven in medicine that evening—the first time the Nobel Committee had not\nselected a single individual to receive the honor.\n\nCount Karl Axel Mörner, the rector of the Karolinska Institute, which had been\ncharged with selecting the prize recipients, announced the winners. He\npronounced, “This year’s Nobel Prize for Physiology or Medicine is presented\nfor work accomplished in the field of anatomy. It has been awarded to\nProfessors Camillo Golgi of Pavia and Santiago Ramón y Cajal of Madrid in\nrecognition of their work on the anatomy of the nervous system.” As the count\ncontinued to recite his graciously worded presentation, he reminded his\nlisteners—including the men honored that night—about how much of the brain’s\nactivity remained a mystery. After a brief foray into describing the complex\nanatomy of the nervous system, he returned to the prizewinners, who, he said,\nhad given birth to an entirely new branch of medicine.\n\nWhen it finally came time to discuss the specific research that had brought\nGolgi and Ramón y Cajal to Stockholm, a sneaky international diplomacy\nprevailed. Addressing Golgi in Italian, Count Mörner said, “Professor Golgi.\nThe Staff of Professors of the Karolinska Institute, deeming you to be the\npioneer of modern research in the nervous system, wishes therefore, in the\nannual award of the Nobel Prize for Medicine, to pay tribute to your\noutstanding ability and in such fashion to assist in perpetuating a name which\nby your discoveries you have written indelibly into the history of anatomy.”\nMörner then turned to Cajal and switched to Spanish. “Señor Don Santiago Ramón\ny Cajal. By reason of your numerous discoveries and learned investigations,\nyou have given the study of the nervous system the form that it has taken at\nthe present day, and by means of the rich material which your work has given\nto the study of neuroanatomy, you have laid down a firm foundation for the\nfurther development of this branch of science,” he said. “The Staff of\nProfessors of the Karolinska Institute is pleased to honor such meritorious\nwork by conferring upon you this year’s Nobel Prize.” With those historic\nwords, neuroscience was baptized.\n\nBorn in Petilla de Aragón, Santiago Ramón y Cajal was obsessive and\nautocratic, a stubborn genius who single-handedly launched brain research into\nits modern era by demonstrating unequivocally that brains, like other organs,\nare made of collections of individual cells. A fanatical microscopist, he\ncombined exquisite technical skills with a panache for drawing and creative\ninsights.\n\nFew would believe, if told, that this man receiving the Nobel award from the\nhands of King Oscar II that night had published his first paper on the central\nnervous system only eighteen years earlier, when he was an anonymous professor\nat the University of Valencia. Early in his career, Cajal had not been able to\nwrite in German, the language of the leading anatomists of his time. So he\nfounded a scientific magazine, Revista Trimestral de Histología Normal y\nPatológica, in which he could report his findings in Spanish. It helped that\nhe continued to finance and edit the publication throughout its existence—it\nmade for a very straightforward editorial review. In 1896, he repeated the\nexperiment and created the first classic scientific journal of brain research,\nRevista Trimestral Micrográfica. The debut issue contained six papers authored\nby Don Santiago himself. Years later, German anatomists decided to learn\nSpanish just to be able to read Cajal’s original masterpieces.\n\nTo this day, Cajal remains one of the most frequently cited authors in\nneuroscience. His ingenuity and resilience as an experimentalist were first on\ndisplay when he adapted a staining method, called la reazione nera (the black\nreaction), for his revolutionary investigation of the organization of brain\ntissue. Cajal ceaselessly tinkered with the black reaction, hardening blocks\nof brain tissue in a compound of either potassium or ammonia dichromate\ncrystals, and then moving the tissue to a solution of silver nitrate, where\nthe tissue structures would slowly “blacken” against the translucent yellow of\nthe firm block. Cajal then sectioned the blocks into thin sections, which\ncould be viewed under a microscope. The patient histologist could then easily\nidentify the splendorous brain-cell bodies, dendrites, and axons scattered\nacross the tissue. Cajal tried using the technique on embryonic and newborn\nbrain tissue as well as adult specimens. It took years for him to find the\nright combination of solution baths, slice thicknesses, and brain tissues. His\nmost stupendous results came from the brains of birds, reptiles, and small\nmammals that he mostly caught himself and then prepared in his wife’s kitchen.\nThere were likely very few chickens pecking around the Cajals’ backyard.\n\nHaving perfected the black reaction, he next developed a new illustration\nmethod, which consisted of drawing on a single sheet of paper every single\nindividual cell he could observe as he shifted the focal plane of each slice\nof tissue he placed under his beloved Zeiss microscope ([Fig.\n2.2](part0006.html#fig2-2)). These comprehensive microscopic images of brain\ntissue were stunning and unprecedented. His precise and pioneering images of\nbrain circuits gave an inkling that neuroscientists would soon be able to\ndisclose the intimate secrets and long-lost tales of the human mind.\n\nYear by year, Cajal unveiled a series of unique discoveries about the\nmorphology of the brain’s cells, each embellished by his inventive\nillustrations and interpretations. These coalesced into his famous law of\ndynamic polarization, which stated that nerve cells were functionally\npolarized, that is, they were formed by a recipient region—represented by\ntheir dendrites—and a transmitting component—the axon. Using this concept,\nCajal predicted that an electrical impulse would be received by the dendrites\nof a cell and, after passing through the cell body, be transmitted via the\naxons to other cells. Although there is no evidence that he ever witnessed a\nphysiological recording of an electrical potential traveling across an axon or\nfiring up from a dendrite or a cell body, three decades later\nelectrophysiological recordings would prove him to be right. The story told in\nMadrid, where Cajal later worked, was that on the day God decided to create\nthe brain, He got very excited and decided to phone Professor Cajal to\ndescribe His ideas on how the brain should work. After attentively listening\nto God’s plan, Don Santiago simply said: “Not bad—but, please, come down to\nMadrid and let me show Thee some slides so that Thou might learn how the brain\nThou are about to create will actually work.”\n\n![image](../images/00006.jpeg)\n\nFIGURE 2.2 Putting the neuron’s supremacy in ink. Don Santiago Ramón y Cajal\nat his favorite place, in front of a microscope, and a few of his masterpiece\ndrawings of different portions of the central nervous system. (Cajal’s three\ndrawings from “Histology of the Nervous System” and the photo were reproduced\nwith permission of the Cajal Legacy, Instituto Cajal [CSIC], Madrid, Spain.)\n\nThe crown jewel of Cajal’s career was the enunciation of a series of laws that\ndefined what became known as the neuron doctrine. According to his theory, the\nbrain is formed by large numbers of individual cells that communicate with one\nanother by discrete contacts. To Cajal’s chagrin, though, he had not himself\ncome up with the name “neuron” to designate this critical unit of the nervous\nsystem’s anatomy. In 1891, a German anatomist, Wilhelm von Waldeyer-Hartz, had\ndone so, in a review article that received a great deal of attention. But the\nname was coined, and Cajal had to endure it.\n\nGiven the dimension of Cajal’s achievements, you may be wondering why in\nheaven the Nobel committee decided that he must share the prize for medicine\nwith Camillo Golgi. In total opposition to Cajal’s conclusions, Golgi ardently\nsupported the competing theory of the brain, the “reticular theory” originally\nproposed by the German anatomist Joseph von Gerlach. The reticular theory\npostulated that the brain was formed not by individual cells but by a\ncontinuous, vast mesh of brain tissue. While Gerlach favored the idea that\nfused dendrites were the main component of this reticulum, Golgi believed\nfused axons or widespread neural networks—so-called nerve nets—primarily\nserved to organize the brain’s tissue, making Golgi one of the few\nneuroscientists of his day to abhor the idea that mental functions were\nlocalized in discrete regions of the cortex. He was out of step with the\nscientific times. But as it happened, Golgi had invented the crucial reazione\nnera, back in 1873. The Nobel committee could not ignore him. It was an ironic\ndestiny, in the tradition of Galvani: an Italian, seemingly the last\ndescendant of Thomas Young, had devised a fabulous new method but seemed to\nhave badly misunderstood his own data. Indeed, despite their shared accolade,\nthe practice of la reazione nera, a love for using their wives’ kitchen as a\nlab, and a devotion to Zeiss microscopes were the only things on Earth that\nunited Cajal and Golgi.\n\nBy the time of the Nobel presentation on December 10, Cajal and the neuron\ndoctrine had long dominated the newborn field of neuroscience; the\nexperimental evidence was overwhelmingly in the Spaniard’s favor. Yet Golgi\nresisted. In fact, on December 11 he delivered a defiant Nobel lecture, titled\n“The Neuron Doctrine—Theory and Facts,” in which he argued that the doctrine\nwould soon be in decline. He then dissected the theory, point by point,\noccasionally raising his own ideas as an alternate approach. Midway through\nhis presentation, he openly mocked those who took the neuron doctrine for\ngranted: “I shall therefore confine myself to saying that, while I admire the\nbrilliancy of the doctrine which is a worthy product of the high intellect of\nmy illustrious Spanish colleague, I cannot agree with him on some points of an\nanatomical nature which are, for the theory, of fundamental importance.”\n\nThe next day, a very upset Cajal took his place at the front of the Swedish\nlecture hall, armed with his irrefutably elegant pictures of distinct neurons\nand their delicate processes. He began by invoking the “tradition” of the\nNobel lectures as a time for a scientist to present his own results. But Cajal\nwas not above drawing blood. “We mourn this scientist who, in the last years\nof a life so well filled, suffered the injustice of seeing a phalanx of young\nexperimenters treat his most elegant and original discoveries as errors,” he\nsaid in closing.\n\nFor most observers, Cajal won the day. Throughout the twentieth century,\nlocalizationists would proceed to divide the cerebral cortex into visual,\nauditory, tactile, motor, olfactory, and gustatory centers. These original\nsites were then subdivided into specialized regions for color, motion\ndetection, face recognition, and other complex functions. Soon, individual\nneurons were being labeled as visual neurons, mirror neurons, face neurons,\ntouch neurons, even grandmother neurons.\n\nFew areas were left uncharted, but how a whole brain worked remained a deep\nand obscure mystery. After dividing and subdividing the brain into its minute\nunits, neuroscientists still lacked a way to explain how those units came\ntogether to produce the seamless perceptual experiences that define human\nlife. Ironically, like his countryman Galvani, it now appears Golgi may have\naccurately seen the big picture, even though he could not identify it in the\ndetails of his black reaction slides. Moreover, in recent decades, scientists\nhave even found that several areas of the brain, including a structure called\nthe inferior olive, which is involved in motor control, and some classes of\nneurons, such as the inhibitory interneurons of the cortex and the mitral\ncells in the olfactory bulb, do form continuous networks. These networks are\nlinked by cytoplasmic bridges known as gap junctions, similar to Golgi’s\nversion of the reticular theory. In a very unexpected way, though, Golgi did\nexact his revenge, albeit almost in silence: he helped coin the term nerve\nnetwork and the general concept of a brain that thinks through the collective\nwork of vast distributed neuronal circuits. The “Golgi neural network” that\nwas so ridiculed in 1906, as it turned out, inspired generations of\ndistributionists, including Lashley, Pfaffmann, Hebb, and Erickson, to resist\nand endure.\n\nGalvani’s and Golgi’s stories remind me of what a famous Brazilian soccer\ncoach once said: “These Italians can win a fight in very surprising ways.”\nJust ask the millions of Brazilian fans—who watched in despair as the Italian\nstriker Paolo Rossi scored three straight goals and kicked their team out of\nthe 1982 World Cup—whether they still have nightmares about that game.\n\nI certainly do.\n\n\n3\n\nTHE SIMULATED BODY\n\nFollowing the crowning triumph of the neuron doctrine during the 1906 Nobel\nceremony, neuroscience witnessed an unstoppable and, for the most part,\nunmatched rise of the localization view of brain function. This impetus was\nparticularly intense among those who focused on unraveling the organization of\nthe cortex, the highly convoluted block of tissue that comprises the outer\nlayer of the cerebral hemispheres. The opening years of the twentieth century\nsaw the rise of cytoarchitectonics, which relied on a variety of staining\ntechniques, including the Nissl method of dying the negatively charged RNA\nfound in organelles within cells, to study the distribution and clustering of\nneurons.\n\nCytoarchitectonics was launched into prime time, at least in part, by the\nRussian histologist Vladimir Betz’s discovery in 1874 that the motor cortex,\nthe cortical region that Hitzig and Fritsch had tagged as the source of body\nmovements, contained a peculiar horizontal layer packed with large pyramid-\nshaped neurons. Known since then as Betz cells, these pyramidal neurons give\noff very long axons that bundle together and descend all the way to the spinal\ncord, forming the corticospinal tract, one of the bulkiest and most important\nneural pathways. The corticospinal tract carries voluntary motor signals,\ngenerated in the motor cortex, to pools of interneurons, the cells that give\nrise to axons that make connections locally, and lower motor neurons located\nin the brain stem and the spinal cord. The axons of the lower motor neurons in\nthe brain stem terminate in our facial muscles, while those in the spinal cord\nproject to muscles throughout the rest of the body. When lower motor neurons\nfire electrical bursts, our body muscles readily oblige and contract. By\ntransmitting detailed motor instructions to these lower motor neurons, the\ncorticospinal tract exerts executive control on the generation of concrete\nmovements. The corticospinal tract allows our inner voluntary motor intentions\nto be communicated to the surrounding world.\n\nCytoarchitectonic studies carried out at the end of the nineteenth century had\ndetermined that the cortex could be divided into six layers of neurons, each\nlamina stacked atop another. These cortical layers are numbered, from the\nouter to the inner layer, using the roman numerals I through VI. By measuring\nthe thickness and number of layers per cortical area, the density and\ndistribution of different cortical cell types across these layers, and other\nparameters, several histologists had proposed schemes for dividing the cortex\ninto distinct areas or fields by the early twentieth century. Among these\npioneers, the German neurologist Korbinian Brodmann introduced, in a series of\npapers published between 1903 and 1914, a comprehensive cytoarchitectonic\nclassification, based on Nissl staining, suggesting that the mammalian cortex\ncomprised fifty-two cortical fields ([Fig. 3.1](part0007.html#fig3-1)). Though\nin one of his initial studies Brodmann reported data from a single lemur’s\ncortex, in his classic 1909 paper he described and documented with\nillustrations data obtained from multiple animal species. Based on these\nfindings, he identified forty-nine distinct cortical areas in the human brain.\n\n![image](../images/00007.jpeg)\n\nFIGURE 3.1 Korbinian Brodmann’s cortical cytoarchitectonic map and the six\nlayers of the cortex. A side view of the human brain with the original numeral\ndesignations of cortical areas created by Brodmann is on the right. A\ncomparison of the six layers of a section through the primary motor (M1) and\nvisual (V1) cortices is shown on the left. Cytoarchitectonically speaking, the\nM1 is characterized by the presence of large pyramidal-looking neurons (Betz\ncells) in layer V, while V1 exhibits a very dense cluster of neurons in the\nbottom part of layer IV and upper part of layer VI. (Cajal’s two drawings from\n“Histology of the Nervous System” were reproduced with permission of the Cajal\nLegacy, Instituto Cajal [CSIC], Madrid, Spain. Brodmann’s Areas, originally\npublished in 1910, are in the public domain.)\n\nIn Brodmann’s classification scheme, each of the cortical fields was\nidentified by a number. In some cases, the particular distribution of neurons\nin a given cortical layer provided the main feature for allocating the field’s\nnumber and function. For example, Brodmann noted the prominent occurrence of\nBetz cells in layer V, in which his own area 4 was located. Area 4, in his\nview, contained the primary motor cortex. Similarly, layer IV appeared to be\ndensely packed with cortical neurons that were the main final target of the\nmajor sensory pathways (tactile, visual, and auditory), which carry\ninformation from the body periphery to the cortex. Brodmann used the existence\nof this densely packed layer IV in different cortical regions to identify the\nprimary somatosensory (areas 3, 1, and 2), visual (area 17), and auditory\n(areas 41 and 42) cortices. The anatomophysiological correlations identified\nby Brodmann have stood the test of time. Yet, cytoarchitectonics suffered from\nits own obsession with more and more intricate subdivisions of the cortex.\nIndeed, at the same time that Brodmann was publishing his studies, his own\nteachers, Cécile and Oskar Vogt, proposed an alternative scheme that included\nmore than two hundred distinct cortical areas. Even as cytoarchitects moved to\nother features and techniques—staining myelin fibers, for instance—they were\nnot able to provide a definitive coherent functional guide to how the brain\nworks in a behaving beast.\n\n* * *\n\nSir Charles Sherrington is considered by many to be the father of modern\nsystems neuroscience. During the first two decades of the twentieth century,\nhe and a series of students and collaborators at Oxford University employed a\nphysiological approach to study the cortex. At the time, that meant relying on\nelectrical stimulation of cortical regions while measuring the animal’s\nbehavior. Using this method, Sherrington and his colleagues demonstrated that\nthe cortex of the primate frontal lobe contained a complete “motor map” of the\nanimal’s body. These studies, which were summarized in an eighty-seven-page\npaper published in the Quarterly Journal of Experimental Physiology in 1917,\nincluded experiments with twenty-two chimpanzees, three gorillas, and three\norangutans. Sherrington had found the primary motor cortex of great apes in\nthe precentral gyrus, a cortical area located in front of the central sulcus,\nwhich separates the frontal and parietal lobes.\n\nThe true impact of these studies was only felt when one of Sherrington’s\nstudents, the American neurosurgeon Wilder Penfield, shared some unusual\nobservations taken during neurosurgical procedures on patients suffering from\nepileptic seizures. After studying with Sherrington, Penfield had interned\nunder the legendary American neurosurgeon Harvey Cushing at Yale, where he\nhoned his skills in the operating room. Penfield then moved to Montreal to\nwork at McGill University, where he founded and directed the Montreal\nNeurological Institute. Over a period of nineteen years, he collected data\nfrom more than four hundred craniotomies, in which, under local anesthesia, a\npiece of a patient’s skull is removed, allowing the cortex to be exposed.\nSince manipulation or electrical stimulation of the cortex does not generate\nany pain, Penfield’s patients remained conscious during the operation and\ncould report what they felt when he stimulated different spots in their cortex\nin his attempt to locate the source of their epileptic seizures. During the\nprocedure, Penfield and his collaborators, including the Canadian psychologist\nDonald Hebb, were able to map the type of tactile sensations elicited by\nelectrical stimulation of cortical regions located in front of (precentral)\nand behind (postcentral) the central sulcus. Penfield found that 75 percent of\nthe spots that provoked tactile sensations were located in the postcentral\ncortex, which contained the primary somatosensory cortex according to\nBrodmann; the remaining 25 percent were located in the precentral gyrus, where\nthe primary motor cortex resides. More surprising, the sensations elicited by\nstimulation of the precentral cortex continued even in the few cases that\nrequired that the surgeons remove the postcentral gyrus in order to quell\nseizures. He also reported that stimulating the postcentral gyrus created body\nmovements, even when the precentral gyrus was removed. Penfield believed this\nprovided evidence that the sensations reported when the motor cortex was\nstimulated were not generated by collateral electrical activity or creeping\nnerve fibers from the primary somatosensory cortex. The primary motor and\nsomatosensory cortices appeared to share their functions. While each area\nexhibited a clear functional bias, the two sides of the central sulcus\ncontributed to similar sensorimotor behaviors.\n\nSuch an arrangement suggests the notion that cortical areas could exhibit a\nsignificant degree of functional specialization (in this case, either more\nmotor responses elicited from the motor cortex or more tactile sensations from\nthe somatosensory cortex) while still contributing to other brain functions or\nbehaviors. In this context, a given cortical area, like the primary motor\ncortex, would normally have a higher probability of participating in the\nexecutive generation of motor behaviors while contributing in a secondary way\nto the genesis of tactile sensations. Conversely, under normal conditions, the\nprimary somatosensory cortex would have a much higher probability of being\ninvolved with the generation of tactile sensations than motor responses. Yet,\ncontrary to the dominant dogma sustained by the defenders of a strict\nsegregation of functions in the cortex, the chance that neurons in the primary\nmotor or somatosensory cortices could be recruited for these other tasks would\nnot be zero.\n\nPenfield proceeded to test what he had learned from his epileptic patients. He\nreconstructed the sequence of body sensations reported, slowly moving the spot\nat which he electrically stimulated the brain from the medial to the lateral\nportion of the postcentral cortex, immediately behind the central sulcus. He\ndiscovered that as he shifted the stimulation, the location of the tactile\nsensations progressively moved, too, starting at the toes, then the foot, then\nthe leg, hip, trunk, neck, head, shoulder, arm, elbow, forearm, wrist, hand,\neach of the fingers, the face, the lips, the intraoral cavity, and finally the\nthroat and the intra-abdominal cavity. When plotted across a cross section of\nthe cortex, this sequence provided a topographic map of the human body, which\nbecame known as the sensory “homunculus.” Although Penfield wrote the study,\nthe depiction of the homunculus was drawn by Mrs. H. P. Cantlie. It took her\ntwo tries to satisfy the neurosurgeon. But in the end, she pulled off what\nbecame one of the most reproduced illustrations in the annals of medical\nliterature ([Fig. 3.2](part0007.html#fig3-2), left side).\n\nThe homunculus that so pleased Penfield does not resemble anyone you might see\nwalking down the street or sitting at your dinner table. Instead, Mrs.\nCantlie’s homunculus appears severely, eerily distorted. This distortion is\nthe result of a developmental process called cortical magnification that\noverrepresents the areas of the body with the highest density of\nmechanoreceptors, a series of highly adapted peripheral nerve endings\nresponsible for translating tactile stimuli into electrical potentials, the\nlanguage of the brain. So the homunculus bulges at the fingers, hands, and\nface, most especially the area around the mouth and the tongue. Other body\nareas, including the chest and the trunk, seem squeezed—as though they are on\na sensory diet—despite occupying larger proportions of our skin. Because the\nfingers, hands, and face contain so many mechanoreceptors, they are our most\nrefined tactile organs, the ones we normally use to create a tactile image of\nour surrounding world. That’s also why it’s so difficult to tell what an\nobject is when it’s rubbed across the skin of the back.\n\nThe phenomenon of cortical magnification is not a privilege of the human\nspecies. In every mammal examined in the past seventy years, it has been\ndocumented extensively. In rats, the “rattunculus” involves a large\nrepresentation of the array of facial whiskers, with the forepaws supersized\ncompared to the rear ones ([Fig. 3.2](part0007.html#fig3-2), right side). In\nthe case of the platypus, the semiaquatic, egg-laying Australian mammal, it is\nthe animal’s beak that is overmagnified in the somatosensory cortex’s body\nmap.\n\nThese somatotopic representations are also not unique to the cortex. Each of\nthe subcortical relays of the bundle of axons that form the somatosensory\nnerve pathways carrying tactile information from the body periphery and\nproprioceptive feedback from muscles and tendons to the central nervous system\ncontain such maps. For this reason, it seemed that these topographic maps were\na fundamental physiological tool employed by the brain to shape tactile\nperception—but then a paradox emerges. There is no doubt that our most\nextraordinary tactile experience is a sense of inhabiting a body that we\nrecognize as our own. From just a few months of age, humans are able to\ndistinguish their own bodies from other objects and other people. And\nthroughout our lives, we experience and interact with the world from our\nbody’s first-person perspective. Yet, none of our routine tactile experiences,\nnot even this most private and meaningful one, resembles Mrs. Cantlie’s\nhomunculus. The drawing feels eerie because it doesn’t match our sense of\nself.\n\n![image](../images/00008.jpeg)\n\nFIGURE 3.2 The homunculus meets the rattunculus. The drawing depicts an\nimpossible meeting between the reconstruction of the cortical “homunculus,”\nthe distorted representation of a human’s body in the primary somatosensory\ncortex, based on Wilder Penfield’s studies, and a cortical “rattunculus,” the\nequivalent distorted representations of the rat’s body, in the rodent primary\nsomatosensory cortex. Notice the overrepresentation of the lips and hands in\nthe homunculus and the facial whiskers, snout, and forepaws in the\nrattunculus. The cheese is Swiss. (Illustrated by Dr. Nathan Fitzsimmons, Duke\nUniversity.)\n\nSo how in heaven is the body image we actually experience defined, if not by\nthe toil of the homunculus imprinted in the brain? To begin to answer this\nquestion, we have to enter the mental landscape of “experienced”\nphenomena—near-death encounters and phantom limbs—that cannot be explained by\northodox, localizationist neuroscience.\n\n* * *\n\nOne morning, late in my final year at the University of São Paulo medical\nschool, one of my good friends, a young vascular surgeon working at the\nHospital das Clinicas, its main teaching hospital, invited me to visit the\northopedics ward. His invitation was unusual, to say the least.\n\n“Today we will talk to a ghost,” he said in a solemn tone. “Do not get\nfrightened. Try to stay calm. The patient has not accepted what has happened\nyet, and he is very shaken.”\n\nOf course, I had never met a ghost in my life, though my Italian great-\ngrandmother had repeatedly assured me that phantasms floated invisibly around\nus and did not particularly like children who refused to go to bed before the\nend of the Wednesday night soccer broadcast. I decided to check Dona Ada’s\ntheory empirically.\n\nDespite many previous calls at the hospital emergency room, which on some\nnights resembled a rumbling war zone, I was not prepared for my visit to the\nalways austere orthopedic institute. As we entered a small, isolated\ninfirmary, we were received by the tired gaze of a stocky middle-aged woman\nwho had just risen from her chair and was sobbing. The woman’s round and\nreddish face was covered with deep lines; together with her leathery hands,\nthey betrayed a life of sorrow and hardship. Sitting beside her in a reclined\nbed was a boy of about twelve years, his face dripping with sweat and\ncontorted in an expression of horror. The child’s body, which I now watched\nclosely, writhed from excruciating pain.\n\n“It really hurts, Doctor; it burns without a break. It seems as if something\nis crushing my leg,” he said. I felt a lump in my throat, slowly strangling\nme.\n\n“Where does it hurt?” I ventured.\n\nHe did not hesitate: “In my left foot, my calf, the whole leg, everywhere\nbelow my knee!”\n\nMoved by an uncontrollable reflex, and a sickening feeling of revolt that a\nchild had been lying in pain in a hospital bed in the middle of the day, I\nstarted to lift the sheets that covered his sweat-soaked body. My indignation\nwas immediately replaced by disorientation when I noticed that half of the\nboy’s left leg was missing. As my colleague later told me, the boy’s leg had\nbeen amputated below the knee after he had been run over by a car.\n\nOutside the ward, my surgeon friend tried to calm me. “It was not him\nspeaking,” he said. “It was his phantom limb.”\n\nAt that time I did not realize that at least 90 percent of amputees—millions\nworldwide—experience these phantom limb sensations: the uncanny feeling that a\nmissing body part is still present and attached to their bodies. In some\ncases, the part moves; in others, it is locked in place. Usually, such ghostly\nappendages are often defined by a diffuse tingling sensation that extends\nthroughout the amputated limb and effectively reconstructs it. These phantoms\nare often very painful and terrifyingly vivid. In some cases, they persist for\nyears.\n\nThe phenomenon of phantom limbs has been reported for centuries. During the\nMiddle Ages, European folklore glorified the restoration of sensation in\namputated limbs in soldiers. In one classic account of mysterious cures in the\nport of Aegea, in the Roman province of Siria, during the fourth century,\npatients who had lost their arms and legs supposedly sensed the angelic\npresence of their missing limbs thanks to the “miraculous” works of twin\nbrothers who were later canonized as saints by the Catholic Church. According\nto the Church’s documentation, Saints Cosmas and Damian had “restored” the\nsensation of a missing leg by transplanting the leg of a dead person into the\nstump. Legend had it that any amputee who evoked the brothers’ names would,\nonce again, feel their missing limbs.\n\nIn the sixteenth century, phantom limbs moved from the realm of religion to\nthat of medicine. The French military surgeon Ambroise Paré, whose improved\nsurgical techniques boosted the survival rate for amputees, noted many cases\nof phantom limbs among soldiers returning from European battlefields. Although\nhe believed his patients, Paré feared that people would think he had lost his\nmind while attending them—which may explain why he published his findings in\nFrench and not in Latin, the scientific language of his day, and why his\nobservations were neglected for more than three centuries.\n\nThat neglect may provide some insight into the particular heroism of the\nBritish admiral Horatio Nelson, whose legacy includes a remarkable self-\ndescription of a phantom limb. During the Battle of Santa Cruz de Tenerife, in\n1797, soon after coming off a small boat and landing on shore, Nelson was shot\nin the right arm by a Spanish musket ball. The injury was severe. Most of his\narm was amputated.\n\nEight years later, on the eve of the Battle of Trafalgar, Lord Nelson foresaw\nthe victory of the British armada over the combined French and Spanish navies.\nIn a letter he drafted to his queen, he revealed that he had received a divine\npresage: a vivid sensation that he could still hold high the sword with which\nhe had sworn to defend the Crown of England—with the limb he had lost in\nTenerife. The next morning, armed with his ghost sword (and more than two\nthousand cannons, I may add), Nelson defeated Napoleon’s forces. Later that\nday, he died, the victim of a shot that this time was lethal.\n\nThe modern clinical investigation of phantom limbs, however, had to wait for a\nbloodier battle. A few days after the Battle of Gettysburg, the American\nneurologist Silas Weir Mitchell documented case after case of phantom limbs,\nmany among Confederate amputees who felt compelled to reenact their\nparticipation in Pickett’s charge, an uphill assault on the Union army and the\ndeadliest skirmish of the battle. Hopelessly lying in their medical barrack\nbeds, the amputees experienced the sensation of endless throbbing, almost like\nthe pain of running past the point of no return. It was Mitchell who coined\nthe term “phantom limb.”\n\nInterviews with thousands of amputees have been written up since the Civil\nWar. Their cases suggest that severe limb pain before the amputation, due to a\nsevere fracture, deep ulcer, burn, or gangrene, is a major risk factor for\nlater developing phantom pain. More than 70 percent of patients find their\nphantom limbs painful immediately after surgery; in up to 60 percent, the\nthrobbing persists for years. Phantom limbs sometimes perform phantom\nmovements. Recent amputees may even wake up screaming that their nonexistent\nlegs are trying to walk off the bed and run away on their own. In one-third of\nafflicted people, the absent limb becomes completely paralyzed, often\nagonizingly so—for instance, embedded in an ice cube, permanently twisted in a\nspiral, or tortuously bent against the back.\n\nResearchers now know that phantom sensations can occur in any excised body\npart, not only the arms or legs; people who have lost their breasts, teeth,\ngenitals, and even internal organs can experience them. Women with\nhysterectomies have reported illusory menstrual pain and uterine contractions,\nsimilar to those felt during labor. Curiously, male transvestites who have\nundergone sex reassignment surgery do not experience a phantom penis, which\nsuggests that, to their brains, these men already live in a woman’s body.\n\n* * *\n\nDespite intense investigation over the past century, neuroscientists have yet\nto pin down the origins of phantom limbs. An early hypothesis, put forth by\nthe British neuroscientist Patrick Wall, who held a professorship at the\nMassachusetts Institute of Technology (MIT), proposed that the phantom limb\nphenomenon originated as a result of spurious activity generated by severed\nnerve fibers in the scarred region of the stump. These severed fibers formed\nnodules, or neuromas, which were thought to send erroneous signals through the\nspinal cord to the brain. Based on Wall’s hypothesis, neurosurgeons began to\ndesign treatments aimed at removing this peripheral source for the\nmisinterpreted signals. But when they cut the sensory nerves leading to the\nspinal cord, severed the nerves in the spinal cord itself, or even removed the\nparts of the brain that received the sensory neuronal tracts, the phantoms\npersisted. A patient’s pain might vanish temporarily, but it always\nreturned—with a vengeance. As these clinical observations accumulated, many\nneuroscientists rejected the notion that neuromas, or any other abnormality at\nthe level of peripheral nerves, could explain the richness of symptoms of the\nphantom limb syndrome.\n\nThe main voice of opposition came from the great Canadian psychologist Ronald\nMelzack, who had studied under Donald Hebb. In 1965, Melzack and Patrick Wall\nwere working together at MIT when they introduced a daring proposition, which\nbecame known as the gate control theory of pain. According to this theory, the\npain sensation associated with a peripheral noxious stimulus, that is, a\nstimulus that generates some kind of body injury, can be modulated, or “gated\nout,” at the level of the spinal cord. This happens when there is concurrent\nactivity in other peripheral nerve fibers, such as those that carry light\ntouch information, or even in nerves that descend from the cortex and other\nhigher brain centers to the spinal cord but are not themselves associated with\nsignaling pain. Melzack postulated that the central brain structures played a\nfundamental role in the control of pain sensation. His hunch was dramatically\nproven a few years later when significant analgesia was produced by\nelectrically stimulating the periaqueductal gray matter, a small region buried\ndeep in the brain whose neurons extend axons to the very spinal cord regions\nwhere peripheral “pain fibers” converge. Next, researchers found that this\nanalgesic effect was mediated by endogenous opiates produced by the brain\ncells—endorphins.\n\nThis chain of discoveries, initially triggered by the gate control theory of\npain, revolutionized pain research by unequivocally demonstrating that the\nsensation of pain is an internal construct of the brain. The brain is capable\nof modulating a noxious stimulus arriving from the periphery, according to its\nwill as the ultimate modeler of reality (and its sorrows). This shifted the\nreference point for understanding pain from the peripheral pain receptors and\nnerves to the brain’s own point of view. Neurobiology could now begin to\nexplain why soldiers filled with a belief in a true and legitimate moral cause\n(e.g., ridding the world of the Nazis) continue to fight for their countries\nafter suffering devastating, painful wounds; why marathoners continue to run,\nmile after mile, despite throbbing foot injuries; and why, as Melzack\ndemonstrated experimentally, Italian mothers tend to scream much more than\nIrish ones during regular baby deliveries.\n\nFollowing their work on the gate control theory, in the 1980s Melzack and his\ncolleagues developed an alternative explanation for the phantom limb\nphenomenon: the elaborate illusions experienced by amputees emerged not from\nperipheral neuromas but from widely distributed neuronal activity within the\npatient’s brain. Dedicated “pain fibers” and “pain pathways,” as the\nlocalizationists had called them, simply did not exist. Instead, pain—and the\nentire realm of sensations and emotions associated with it—exemplifies the way\nin which the products of our brain’s intricate neuronal circuits are\nconceived, interconnected, informed, and delivered to our consciousness. Pain\ncan strike suddenly, when you first notice blood dripping, even though the cut\nwas sliced several moments or minutes earlier. And it can grow into anguish,\nand sometimes out of control, as it matures into a lingering memory.\n\nMelzack’s explanation of phantom limbs challenged the classic dogma of\nperception. He proposed that in addition to detecting sensory signals from the\nbody, the brain also generates a pattern of activity, or “neural signature,”\nthat defines the body’s image or schema at any given moment of our lives. He\nsuggested that this internal brain representation, extending well beyond the\nreach of the homunculus that Penfield had identified in the motor and\nsomatosensory cortices, endows each of us with a sense of our own body’s\nconfiguration and borders and establishes the definition of our very sense of\nself. According to Melzack, the brain’s image of the body and its limits would\npersist even after the removal of a body part, creating the anomalous yet\nvivid sensation of a phantom limb.\n\nThe dynamic sculpturing of the neural signature, according to his new theory,\nfalls to a large network of neurons that Melzack has named the “neuromatrix.”\nThe neuromatrix includes the somatosensory cortex, located at the surface of\nthe brain on the top of the head, and its associated regions of the parietal\nlobe. In addition, it encompasses multiple neural pathways, including one that\nconveys tactile information from the periphery of the body to the thalamus, a\nsensory relay station deep in the brain whose neurons send information to the\nsomatosensory cortex, and another that traverses the brain’s limbic system, a\ngroup of buried brain structures that governs emotions such as those\nassociated with phantom limbs.\n\nDamage to part of the neuromatrix can result in a loss of ownership of part or\nall of one’s body. For instance, extensive injuries to the right parietal lobe\ncaused by brain trauma, a tumor, or a stroke can lead to a complex\nneurological condition, known as left hemibody neglect syndrome, in which\npatients become indifferent to the entire left side of their bodies and, in\nmost cases, the environment surrounding that side. Patients with the condition\nhave been known to fail to put on the left sleeve of a shirt or a left shoe.\nWhen asked about their behavior, they typically deny that the left arm or leg\nis theirs—it belongs to someone else’s body.\n\nThe clinical symptoms of this syndrome are usually transient, but they can\ncause commotion. Take an incident described to me by a NASA astronaut who\nvisited my lab at Duke several years ago. During the initial orbit of his\nfirst space mission, he said, the pilot of the space shuttle started\ncomplaining to his colleagues. “Stop poking your hand on my left control\npanel!” When the pilot was told that nobody was poking their hand in his panel\nand that the hand in question was his own left hand, he shrugged and replied,\n“The hand on the left panel is certainly not mine.” A few hours later, to the\nrelief of the rest of the crew (and mission control in Houston), the pilot\nsuddenly reported: “Just relax, guys. I have found my missing left hand on the\ncontrol panel!”\n\nMelzack argues that the basic structure of the neuromatrix may be already\npresent at birth, its blueprint defined by genetic instructions. This\ncongenital network would explain why, as Melzack reported in 1997, phantom\narms and legs appear in at least one-fifth of children born without these body\nparts, and in half of the children whose limbs were amputated at a very young\nage. These remarkable findings suggested that the human brain is capable of\ngenerating a well-defined model of the subject’s self even in the absence of\nsomatic sensory signals derived from the physical body.\n\nFor more than half a century, since Penfield’s neurosurgical observations, the\nexistence of the homunculus map of the body’s surface had been accepted.\nNeuroscientists generally believed that this somatotopic map, like other\ntopographic representations that had been found in the primary visual and\nauditory cortices, was only malleable during a brief period of early postnatal\ndevelopment known as the critical period. After that point, the consensus was\nthat the brain’s topographic map “crystallized,” remaining stable throughout\nthe rest of the person’s life. This notion was based on evidence provided by\nNobel Prize winners David Hubel and Torsten Wiesel regarding the segregation\nof ocular dominance columns, clusters of neurons of the primary visual cortex\nthat carry signals from either the left or right eye. Based on this work,\ncortical maps were believed to be incapable of exhibiting “plastic” functional\nreorganization during adulthood.\n\nThat picture started to change in 1983 when two American neuroscientists, Jon\nKaas of Vanderbilt University ([Fig. 3.3](part0007.html#fig3-3), left panel)\nand Michael Merzenich of the University of California, San Francisco,\nannounced that traumatic amputation of the middle finger of an adult monkey\nled to a remarkable functional reorganization of the somatotopic map in the\nprimary somatosensory cortex. Instead of remaining silent after the\namputation, after a few weeks or months the cortical neurons that had\nrepresented the finger started to respond to any tactile stimulus delivered to\nan adjacent region of the hand, such as the index and ring fingers ([Fig.\n3.3](part0007.html#fig3-3), right panel). All of a sudden, old monkey neurons\ncould learn new tricks. Few people realized that more than a decade earlier,\nPatrick Wall, who had originated the gate control theory of pain, and his\nstudents had published a small study in the journal Nature claiming to have\ninduced plasticity in the somatosensory thalamus, the main subcortical relay\nof the neural pathways that carry tactile information from the skin to the\ncortex, in adult rats.\n\n![image](../images/00009.jpeg)\n\nFIGURE 3.3 Revolutionary plasticity experiment in owl monkeys by Jon Kaas and\nMichael Merzenich. On the left, Jon Kaas appears with a close collaborator. On\nthe right, the top shelf describes how, after a traumatic amputation of the\nthird finger, the territory representing this finger in the primary\nsomatosensory cortex of an owl monkey does not remain silent. Instead, the\nregion that was occupied by finger 3 is now invaded by enlarged\nrepresentations of fingers 2 and 4. The lower shelf shows that enlargement of\nthe representations of fingers 1–4 can be obtained by repetitive, selective\nstimulations of these digits in detriment of finger 5. To see the effect, just\ncompare the map on the bottom middle (before selective stimulation) with the\ncortical map on the bottom right (after stimulation). (Adapted from, M. M.\nMerzenich, J. H. Kaas, J. Wall, R. I. Nelson, M. Sur, and D. Felleman,\n“Topographic Reorganization of Somatosensory Cortical Areas 3B and 1 in Adult\nMonkeys Following Restricted Deafferentation,” Neuroscience 8, no. 1 [1983]:\n33–55, with permission from Elsevier.)\n\nThe findings of Kaas and Merzenich triggered a true revolution in the field.\nClearly, the mammalian brain had evolved to be plastic. Yet, some of the later\nproponents of cortical plasticity subsequently resisted the notion that\nsubcortical structures were capable of functional reorganization, too. So it\ncaused a stir when in 1993, my postdoctoral research adviser, John Chapin, now\nat SUNY Brooklyn, and I showed that the plastic reorganization process started\nimmediately after a small dose of local anesthetic was injected subcutaneously\nto block neural activity from a small area of skin (and thus more easily—and\nless traumatically—mimic the effects induced by a finger amputation). Our\nstudies showed that the reorganization process occurred at the subcortical\nlevel, in structures such as the thalamus. Soon afterward, Tim Pons, a\nneuroscientist then at the National Institutes of Health (NIH), conducted\nstudies in monkeys whose entire arm—not just a finger—had been deafferented\nmany years prior. Deafferentation involves severing the connection between all\nsensory afferents (nerves) and the spinal cord. Pons reported that long-term\ndeafferentation prompted a widespread reorganization, in which the neurons\npreviously assigned to the hand switched to react to signals from the face,\nwhich is represented next to the arm in the brain’s map. He and his colleagues\nalso noted that the reorganization process occurred in the thalamus and brain-\nstem relays of the somatosensory system.\n\n* * *\n\nWhat do all these observations in monkeys have to do with a potential\nexplanation for the occurrence of phantom limbs in amputees? The connection\nbecame evident when V. S. Ramachandran, a physician and neuroscientist at the\nUniversity of California, San Diego, documented the occurrence of plastic\nreorganization of the topographic body map present in the somatosensory cortex\nof patients whose arms had been amputated. Using an imaging technique called\nmagnetoencephalography (MEG), which measures the magnetic fields produced by\nelectrical activity in the brain, in the early 1990s Ramachandran and his\ncolleagues showed that tactile stimulation delivered to regions of these\npatients’ faces activated what was supposed to be the hand area of their\ncortical body map. As Ramachandran explains in his enlightening book, Phantoms\nin the Brain, when he touched particular spots on the amputees’ faces, the\npatients instantly claimed that they felt sensations in their phantom hands.\nFurthermore, Ramachandran’s team found that tactile stimulation of specific\npoints on the face elicited sensations at specific points on the phantom hand.\nThe type of sensation—whether hot, cold, rubbing, or massaging—was the same in\nboth locations. The brains of these patients connected their existing face to\ntheir phantom hands. The link between adult brain plasticity and phantom limb\npain was corroborated in a 1995 study in which neuroscientist Herta Flor at\nthe University of Heidelberg in Germany and her colleagues employed MEG to\ndetect the degree of cortical reorganization in thirteen amputees. There was a\nstrong correlation between the amount of functional cortical restructuring and\nthe magnitude of phantom pain.\n\nSpurred by this evidence, Ramachandran and his collaborators developed a very\nsimple but ingenious treatment for phantom limb syndrome based on two main\ntheoretical pillars: first, that the adult brain’s body maps are malleable,\nand second, that the brain’s internal workings, and not the feed-forward flow\nof tactile signals from the peripheral nervous system, dictate the sculpturing\nand maintenance of the perception of body identity and uniqueness. Their\ntherapy involved building a “mirror box” that patients could use to practice\ncalming a phantom arm. A vertical mirror was inserted into a cardboard box\nfrom which the top had been removed. The amputees were then instructed to\ninsert their intact arm in the front side of the box so that the arm’s\nreflection in the mirror overlaid the perceived location of the phantom limb.\nThis created a visual illusion that the phantom arm had been resurrected,\nalmost like the miraculous effect credited to Saints Cosmas and Damian. In\nthis case, though, the effect was experimentally recorded—and had long-lasting\nresults. When patients moved their existing arms, they felt the “phantom” arm\nobey these same motor commands. Six of the patients who used the mirror box\nsaid they could feel as well as see their phantom arm moving, generating the\nimpression that both arms could now be moved. Four of the patients used this\nnewfound ability to relax and open a clenched phantom hand, providing relief\nfrom painful spasms. In one patient, a routine of ten minutes of practice per\nday with the mirror caused his phantom arm and elbow to “disappear” completely\nwithin three weeks. And when the limb vanished so did the pain. The visual\nillusion apparently corrected the tactile one, suggesting that the activity of\nthe central visual circuits could modify the activity of Melzack’s\nneuromatrix.\n\nAlmost a decade later, psychologist Eric Brodie of Glasgow Caledonian\nUniversity and his colleagues reported hints of success in a test of a mirror\nbox modified for phantom legs. Forty-one lower-limb amputees watched a\nreflection of movements of their intact leg in a mirror as they tried to move\ntheir phantom leg. Another thirty-nine amputees tried to move both their\nphantom and real legs without the mirror. Both efforts, which involved ten\ndifferent movements each repeated ten times, diminished phantom limb\nsensations, including pain. Although the mirror did not enhance this effect,\nit did produce significantly more phantom limb movements and more vivid\nawareness of the phantom leg than did the exercise without the mirror.\nProlonged mirror treatment might be effective in fighting phantom pain, Brodie\nproposes, because it reverses the plastic reorganization of the brain.\n\nResearchers are now trying to ameliorate phantom limb pain with immersive,\nthree-dimensional computer simulations—so-called virtual reality (VR)—that can\nproduce illusions similar to those created by a mirror. The technology can\ndisplay a patient’s entire body, including the phantom limb, and enable a\npatient to perform complex movements of the fingers, toes, hands, feet, arms,\nand legs that are not possible with mirror therapy alone. In a preliminary\nstudy conducted in 2007, psychologist Craig Murray at the University of\nManchester and his colleagues exposed two upper-limb amputees and one lower-\nlimb amputee to a simulation that transposed the users’ existing limb\nmovements onto those of a virtual limb, which overlaid their phantom limb in\nthe virtual environment. All three amputees, who each participated in two to\nfive VR sessions, reported feeling sensations in the phantom. In each case,\nphantom pain decreased during at least one of the sessions, suggesting that VR\ntherapy might offer pain relief as well.\n\nThe clinical evidence obtained from these patients underscores that our body\nimage, that inexpugnable refuge of our carefully groomed individuality and\nmental uniqueness, emerges graciously as a dynamic by-product of the\ncollective electrical activity of brain circuits to remain malleable and\nresponsive to events occurring within, on, and beyond the physical boundaries\nof our mortal skin. Just like any good and sensible modeler of reality, the\nbrain has endowed us with what feels like a true and concrete physical\ninstantiation of the self, a simulated body.\n\nBut if our body image is just a simulation, how does the brain create and\nmaintain such a convincing illusion over the course of a whole life? How\neasily can such an internal neural model be changed, and how far can the\nlimits of our self reach?\n\nRecent experiments have started to address these key questions and, to the\nsurprise of many in the neuroscience community, the emerging answers are\nstunning. The conclusion from more than two decades of experiments is that the\nbrain creates a sense of body ownership through a highly adaptive, multimodal\nprocess, which can, through straightforward manipulations of visual, tactile,\nand body position (also known as proprioception) sensory feedback, induce each\nof us, in a matter of seconds, to accept another whole new body as being the\nhome of our conscious existence.\n\nTake, for instance, the so-called rubber hand illusion, first demonstrated by\ncognitive neuroscientist Jonathan Cohen, now at Princeton University ([Fig.\n3.4](part0007.html#fig3-4)). Subjects are asked to sit in a chair and rest\ntheir left arms close to the left edge of a small table placed in front of\nthem. An opaque screen is then positioned in front of the person in order to\nhide the resting left arm from his or her sight. Then, a full-size dummy\nrubber arm and hand are placed on the same table, but a bit closer to the\nsubject. Care is taken to place the dummy arm and hand so that they occupy a\nposition where the subject could plausibly imagine that they might be his or\nher own arm—which remains out of sight, behind the screen. The subject is then\ntold to fix his or her gaze on the rubber hand and arm, as an experimenter\nsimultaneously and synchronously uses two paintbrushes to touch an analogous\nlocation on the rubber hand and his or her real left hand. After a few\nminutes, nearly everyone who has been subjected to the experiment has reported\nexperiencing the strokes not on the left hand, which received the tactile\nstimulation, but on the site where they fixated their gaze—the dummy rubber\nhand. Indeed, most of the subjects reported feeling that during the\nstimulation, the rubber hand felt as though it was their own true hand.\n\n![image](../images/00010.jpeg)\n\nFIGURE 3.4 The brain adopts a rubber hand. The drawing depicts the\nexperimental apparatus used to induce the “rubber hand” illusion. See the text\nfor details. (Illustrated by Dr. Nathan Fitzsimmons, Duke University.)\n\nIn a subsequent experiment, the same group of subjects endured a longer period\nof paintbrush strokes. Afterward, they were asked to close their eyes and move\ntheir right index finger on the surface of the table until it had reached the\nindex finger of their left hand. Their left index finger reaches were headed\ntoward the rubber hand’s finger rather than the index finger of their own\nbiological hands for as long as the rubber hand illusion remained in their\nhead.\n\n* * *\n\nIf the lasting effects of the rubber hand illusion do not convince you of the\ndynamic nature of our body image, perhaps an “out-of-body” experience will do\nthe trick. Like phantom limbs, reports of out-of-body phenomena, the vivid\nperception of exiting the body or even experiencing the body from an outside\nviewpoint, appear throughout history. Out-of-body experiences can be induced\nby a number of events, including brain trauma, near-death experiences, car\ncrashes, major surgical procedures, anesthesia with a drug called ketamine,\nconsumption of psychedelic drugs, deep meditation, sleep or sensory\ndeprivation, and sensory overload, just to mention a few. Olaf Blanke of the\nBrain and Mind Institute of the Ecole Polytechnique Fédérale de Lausanne, in\nLausanne, Switzerland, and his colleagues discovered that multiple aspects of\nan out-of-body experience can be replicated in healthy subjects by noninvasive\nstimulation of the junction region of the right temporal and parietal lobes,\nusing a technique known as transcranial magnetic stimulation (TMS).\n\nBuilding on this finding, Henrik Ehrsson of the Karolinska Institute in\nStockholm, Sweden, used a virtual reality apparatus to manipulate visual and\ntactile signals in healthy individuals. In the experiment, the subjects were\nable to experience the weird feeling of existing outside of their own bodies,\nassume ownership of an entirely new body, and “swap” bodies with someone else.\nTo achieve this, Ehrsson first manipulated the so-called first-person\nperspective of his subjects by asking them to wear a head-mounted display that\nprojected a true stereoscopic image provided by two cameras positioned on the\nhead of a dummy, placed in front of them. The cameras were positioned so that\nthey could offer to the human subjects what would be the mannequin’s “first-\nperson perspective,” that is, a view of the mannequin’s chest and abdomen.\nThen, in a maneuver reminiscent of the rubber hand illusion, an experimenter\nstepped between the subject and the mannequin, while staying out of view of\nthe cameras and, hence, out of the subject’s view. Using two rods, the\nexperimenter stroked simultaneously the subject’s and the mannequin’s abdomens\nfor several minutes, keeping the strokes as synchronous as possible. During\nthis stimulation, the subjects were able to see the rod touching the\nmannequin’s abdomen. Strikingly, when asked to describe what they experienced\nduring the experiment, most subjects reported sensing the touch of the rod on\nthe mannequin’s abdomen rather than on their own bodies. In fact, the\nsubjects, for the most part, reported feeling that the mannequin’s body had\nbecome their own. So vivid was this assimilation of the mannequin’s body that,\nwhen the authors “threatened” to cut the mannequin’s abdominal skin with a\nknife, the human subjects observing the scene through the head-mounted camera\nexhibited a significant increase in evoked skin conductance response,\nsuggesting that the “threat” to the mannequin’s body spawned a great deal of\nanxiety in them. Similar effects were provoked using other parts of the\nmannequin’s body, such as the hands. However, if objects that did not resemble\na human body were utilized, the out-of-body experience did not take place.\n\nUsing the same apparatus, Ehrsson and his research team went further,\ndemonstrating that people were able to swap bodies with another person through\nan out-of-body experience created by another manipulation of visual and\ntactile information. In the experiment, the subjects wore the same head mount.\nThis time, however, the images the subjects saw were generated by a camera\npositioned on the head of the experimenter, who sat right in front of the\nindividuals and looked directly at them. This clever arrangement allowed the\nsubjects to view their own hands from the “first-person” perspective of the\nexperimenter. Then, each subject was asked to stretch out his or her right arm\nand shake hands with the right hand seen in front of them. At this moment, the\nsubject could see and recognize his or her own right arm moving toward the\nright hand of the experimenter. Because the head mount allowed the subject to\nassume the experimenter’s perspective, when the experimenter moved his or her\nright hand, the subjects saw this movement as coming from the right side, as\nwell. The subject was then asked to squeeze hands synchronously with the\nexperimenter for a couple of minutes. When asked to describe what had been\nfelt during these two minutes, the subjects predominantly reported feeling\nthat the experimenter’s arm, not their own, belonged to their bodies. They\nalso said that their bodies were located behind and to the left of the\nexperimenter’s arm, not in front of the experimenter. Their real bodies had\nbeen forgotten.\n\nTo make things more persuasive, the experiment was repeated while a second\nexperimenter used a knife to threaten either the experimenter’s or the\nsubject’s arm while the two shook hands. Remarkably, subjects experienced a\nmuch higher increase in evoked skin conductance when the experimenter’s right\narm, and not their own real arm, was threatened by the knife. Interestingly,\nneither the transfer of body ownership to a mannequin, nor the swapping with\nanother human body, was influenced by gender. Men could swap bodies with women\nand women could swap bodies with men, a finding that demolished a few\nneurobiological dogmas. So much for the cult of the body!\n\nThe rubber hand illusion and the laboratory induction of out-of-body\nexperiences indicate that the brain actively shapes the sense of self and a\nbounded physical existence. At the center of this new view of body image is\nthe fact that nothing in our regular daily experience and perception of our\nbodies, throughout our entire lives, resembles the distorted somatotopic maps\ntraced as a result of Penfield’s neurosurgical recordings. If anything, those\nmaps are as strange to us in real life as they look in print. Indeed, when\nphysiological recordings are carried out in behaving animals, the map of the\nsensory body obtained is vastly more dynamic than the homunculus; even the\nmost precise stimulus to the body’s periphery produces a spatiotemporal wave\nof neuron activation that quickly and widely spreads across the primary\nsomatosensory area, called S1, and other cortical fields. Such experimental\nevidence, now reproduced by many laboratories worldwide, directly rebuts the\nmosaic view of the brain built by Brodmann’s cytoarchitectonics. And in order\nto achieve the multimodal integration required to define an internal body\nimage, the brain has to recruit widely distributed neural networks dispersed\nacross most of the neocortex, not to mention the subcortical territories,\nwhose contribution to the sculpturing of a coherent sense of body ownership\nremains mostly unexplored.\n\nAs we will see later in chapter 9, the definition of a body image does not\nseem to end at the limits of the last layer of epithelial cells that cover our\nfragile primate bodies. Instead, a series of studies suggest that as monkeys\nand humans acquire proficiency in the use of artificial tools, their brains\nassimilate these tools as true and seamless extensions of their own biological\nbodies. That implies that part of the process of becoming an exceptional\nviolinist, pianist, or soccer player involves the gradual incorporation of\nspecialized tools of the trade, such as violins, pianos, and soccer balls, as\nadd-ons to the neuronal representations of fingers, hands, feet, and arms that\nexist in the brain.\n\nBut it is not just the brains of virtuosos and world-class athletes that are\ncapable of such mesmerizing tricks. In each of us a brain is constantly at\nwork, frenetically assimilating everything that comes into our proximity and\nmorphing our self-image based on a ceaseless flow of information. Our primate\nbrains have the distinguished and unique capability of being not only one of\nthe few, and certainly the most sophisticated, toolmakers bred by natural\nselection, but also an equally voracious tool incorporator. Our brains are\nconstantly busy, adding our clothes, watches, shoes, cars, computer mice,\nsilverware, and every other instrument we use into a dynamically expanding and\ncontracting body representation.\n\nTaking these ideas to the limit, such findings and theories support the\ncontention that as we learn to utilize brain-machine interfaces in which our\nbrains interact directly with artificial tools, located either next to or far\naway from our biological bodies, our brains will incorporate these devices as\npart of us. For some, the potential future amalgamation between brains and\nmachines may sound frightening and even seal the end of humanity as we know\nit. I could not disagree more. In fact, I believe that this tool-incorporating\nhunger of the brain will open a new chapter in evolution, offering us ways to\nextend our bodies, perhaps reaching immortality, in a very particular form: by\npreserving our thoughts for posterity.\n\nThis neurobiological principle of assimilation may reach even deeper into our\nlives. Evidence suggests that as we fall in love, it is our brains, not our\nhearts, that seamlessly merge scents, touches, sounds, and tastes to transform\nthe body of our beloved into a passionate and vivid extension of ourselves.\nThat is why I believe that Cole Porter actually got the big picture right when\nhe wrote “I’ve Got You Under My Skin.”\n\nTo hear the “chairman of the board” Frank Sinatra belt out Porter’s lyrics\nduring an innocent summer night spent dancing with your Gala in an otherwise\nempty parking lot is enough to understand how painful it is to be deprived of\nthe touch of a true love. If you had any doubt before, you can rest assured:\nthe pain of love is real, very likely because, for our brains, to lose the\nobject of our love is to sever a real part of our lonely selves.\n\nIt is no wonder, then, that millions of people cannot entertain the thought of\nbeing separated, not even for a minute, from their beloved BlackBerrys. Once\nthe primordial feelings aroused by the simulated body are released, the brain\nembraces them without boundaries.\n\n\n4\n\nLISTENING TO THE CEREBRAL SYMPHONY\n\nI have always been amazed at how most people, including a large number of\nneuroscientists, readily embrace the notion that the central nervous system\nneeds to be organized in some sort of hierarchical and orderly manner.\nSomehow, though, this type of rigid, military-like structure appeals to us\nhumans as being natural, “the way things ought to be.” Yet, in my opinion, the\nassumption that the brain follows an operational hierarchy has more to do with\nideology than with the way nature really operates. Perhaps it’s not so\nsurprising that science historian Philip Pauly traces the localizationist\ndogma back to a particularly orderly source in his paper, “The Political\nStructure of the Brain: Cerebral Localization in Bismarckian Germany.” Pauly\nnotes how both Eduard Hitzig and Gustav Fritsch, the discoverers of the motor\ncortex, eagerly borrowed metaphors from their beloved Prussian bureaucracy to\nelucidate the workings of the central nervous system.\n\nThere is no doubt that a hierarchical brain, exulting order from the single\nneuron up to Korbinian Brodmann’s fifty-two cortical areas, was more palatable\nto most of the neuroscientists who established the field in the late\nnineteenth and early twentieth centuries. Ideology aside, the limitations of\nusing language may also be responsible for the human tendency to categorize\nand organize how thinking happens. Certainly naming something in the brain was\nthe fastest road to immortality a nineteenth-century neuroanatomist could\ntake. Robert Erickson, my colleague at Duke, insisted that because we always\nsearch for words to define natural phenomena, including the types of behaviors\nand functions carried out by the brain, we almost imperceptibly make the\nlogical jump that each of these corresponding and discrete “word-function”\ncategories, as Erickson called them, should be represented by a particular\nbrain region. As my research over the past two decades has shown, however,\nbrain activity involves overlapping yet widely distributed neural circuits,\ninteracting across time. Words alone may not do justice to the brain’s work,\nwhich is much more probabilistic than any of the world’s many languages that\nwe use to express our thoughts.\n\n* * *\n\nOne of the first scientists to recognize the distributed nature of thinking\nwas Sir Charles Sherrington, who proved that basic neurological functions,\nsuch as the spinal reflexes, depended on the cooperation of multiple\nperipheral and central structures of the nervous system, a finding that earned\nhim the other half of the 1932 Nobel Prize in Physiology or Medicine. By\ndefining this neural collaboration as the work of an integrated system,\nSherrington helped launch the area of brain research now called systems\nneuroscience. Sherrington was not a man purely devoted to the laboratory\nbench. In his book Man on His Nature, he captured the brain’s inner life in\nmagnificent, poetic prose: “The brain is waking and with it the mind is\nreturning. It is as if the Milky Way entered upon some cosmic dance. Swiftly\nthe head-mass becomes an enchanted loom where millions of flashing shuttles\nweave a dissolving pattern, always a meaningful pattern though never an\nabiding one; a shifting harmony of subpatterns.”\n\nSherrington’s integrative neuroscience rekindled the use of a distributed\nstrategy to describe the physiology of thinking. But as often happens in\nscience, the technology available for investigating the global function of the\nbrain was meager, to say the least, in the opening decades of the twentieth\ncentury when Sherrington’s work made its mark.\n\nThis started to change in 1924, when Hans Berger, a German physician and\nprofessor at the University of Jena, made a stunning discovery. Berger had at\none time been a collaborator of Oskar Vogt and Brodmann, the paramount\ncytoarchitects. Upon his return from World War I, Berger had become frustrated\nby his inability to use measurements of the blood circulation in the brain to\nestablish a link between neural activity and psychological behavior. So he\ndecided to venture in a different direction: to measure the brain’s electrical\nsignals. Richard Caton, an English scientist, had recorded the electrical\nactivity produced by the brain from the exposed cortices of experimental\nanimals back in 1875, and the Russian physiologist W. Prawdicz-Neminski had\nrecorded similar activity from the intact skulls of dogs just before the war.\nBerger decided to utilize the same technique on humans. After first trying to\nmake recordings by inserting silver-wire electrodes into a person’s scalp,\nBerger realized—to the relief of his subjects (including his own son)—that he\ncould obtain a reading on the small electrical potential generated by the\nhuman brain simply by attaching silver-foil electrodes on the scalp and\nconnecting them to a galvanometer. Berger proceeded to record brain activity\nin a variety of conditions, including in patients suffering from epilepsy, and\nlearned that a series of global brain rhythms were associated with routine\nbehaviors. One of his first discoveries was the alpha rhythm, an oscillation\nof ten cycles per second (or ten hertz) in brain potentials recorded over the\noccipital bone when patients sit quietly and immobile with their eyes closed.\nBerger named his method the electroencephalogram, or EEG (see [Fig.\n4.1](part0008.html#fig4-1)).\n\nToday, the EEG is routinely employed as an essential diagnostic and research\ninstrument. Moreover, with the discovery of the EEG, neurophysiologists began\nto realize that the cortex was capable of producing global patterns of\nelectrical activity, including a wide range of rhythms that were correlated\nwith a variety of normal internal dynamic brain states and behaviors, such as\nattentiveness or relaxed wakefulness. Pathological cortical conditions,\nincluding different types of epileptic seizures, could also be detected in EEG\nrecords. The EEG gave neurophysiologists the ability to document the\nintegrative activity of an awake brain.\n\n![image](../images/00011.jpeg)\n\nFIGURE 4.1 Sample of first EEG recording ever obtained by Hans Berger. The\ntrace represents a few seconds of the electrical activity of Berger’s own son,\nrecorded using scalp sensors. (From Hans Berger, “Über das\nElektrenkephalogramm des Menschen,” European Archives of Psychiatry and\nClinical Neuroscience 87, no. 1 [1929]: 527–70, with permission from\nSpringer.)\n\nDespite the development of EEGs and another method, the sensory evoked\npotential, to measure cortical electrical activity triggered by a sensory\nstimulus, the tradition of “looking locally” for brain functions continued\nunrepentantly as neuroscientists, led by Lord Adrian and his Cambridge\ncolleagues, simultaneously mastered techniques for recording the electrical\nsignals, also known as action potentials, produced by individual neurons.\nTheir tool of choice was the microelectrode (see [Fig.\n4.2](part0008.html#fig4-2)). In its most traditional incarnation, a\nmicroelectrode consists of a long, thin, rigid metal rod with a very sharp\ntip. With the exception of its tip, the entire microelectrode shaft is encased\nin an isolating material, for example, a resin, glass, or plastic. Once the\nsurface of the brain is exposed, a single microelectrode can be lowered into\nthe brain tissue, creating a penetration track along which one can record the\nelectrical activity of a single neuron in the extracellular space where the\nmicroelectrode’s tip rests. A ground wire, usually connected to the dura, is\nused to provide a reference point for the electrical signals that are picked\nup. Since the extracellular electricity generated by neuronal action\npotentials is very small—around one millivolt—the signals registered by the\nmicroelectrode must be filtered and amplified in order to record the activity\nof a given single neuron.\n\n![image](../images/00012.jpeg)\n\nFIGURE 4.2 The single microelectrode recording method. A single metal\nmicroelectrode, positioned in the extracellular space next to two neurons, is\ncapable of recording the extracellular action potentials of both cells.\nInspection of the recording trace in the oscilloscope reveals that the action\npotentials of the two neurons have different shapes and magnitudes, allowing\nthe distinction between the two cells. (Illustrated by Dr. Nathan Fitzsimmons,\nDuke University.)\n\nUsing a microelectrode, neurophysiologists can monitor the activity of a\nneuron for a period spanning from a few minutes to, at most, a couple of\nhours. Once a neuron’s action potentials are isolated and recorded, the\nmicroelectrode can be moved a few micrometers deeper into the brain to record\nthe activity of another cell. Neuroscientists use such a serial sampling\nprocedure, which can be repeated several times during a recording session, by\nadvancing or retracting the microelectrode or by initiating new tissue\npenetration in another area, to characterize the properties of a population of\nneurons sequentially.\n\nIn the late 1940s, a variation of this method allowed neurophysiologists to\nrecord, for the first time, electrical activity from the intracellular space\nof neurons. These neuronal recordings required a new breed of microelectrodes.\nMade of thin glass pipettes, filled with a conductive electrolytic solution\n(e.g., potassium chloride), and very sharp tips, the microelectrodes could\npenetrate the membrane and cytoplasm of an individual neuron without causing\nmuch damage, allowing neuroscientists to measure accurately the neuron’s\nresting membrane potential—the tiny electrical dipole that is maintained by\nthe differences in concentration and flow of positively and negatively charged\nions that exist between the extracellular environment and the intracellular\nneuronal space.\n\nWithin a decade, intracellular recordings were being used to describe the\nmyriad of synaptic currents that continuously bombard the neuron’s membrane\nand that can lead to the production of action potentials. Once triggered,\naction potentials propagate very rapidly to the neuron’s axon and through all\nits branches, which make synaptic contact with other neurons in a brain\ncircuit. That is how the firing of a given neuron can influence the physiology\nof every neuron connected to it.\n\nMuch of the pioneering experimental work on synaptic potentials and their role\nin generating action potentials was conducted by one of Sherrington’s former\nstudents, the Australian neurophysiologist and Nobel laureate John Eccles.\nTaking full advantage of a new method for in vivo cellular recordings, Eccles\ndiscovered that synaptic potentials, generated by the stimulation of distinct\nperipheral nerve fibers that projected to neurons in the spinal cord, could\nexert either excitatory or inhibitory influences on the membrane potential of\nthose neurons. He also observed that if the overall sum of these synaptic\ninfluences crossed a particular voltage threshold, a spinal cord neuron would\nrespond by producing an action potential.\n\nArmed with their beloved microelectrodes, for two decades neuroscientists\nconquered a variety of brain structures employing Adrian’s experimental\napproach, with a fervid devotion to the primary sensory fields that had been\nso instrumental in validating Brodmann’s cytoarchitectonics. One of the main\nprotagonists of that era, the legendary American neuroscientist Vernon\nMountcastle, made his name wielding microelectrodes across the primary\nsomatosensory cortex (S1) of anesthetized cats and monkeys. During a series of\npainstaking experiments, reported in the Journal of Neurophysiology in 1957,\nMountcastle and his students at Johns Hopkins University penetrated the\nsurface of the S1 cortex with a microelectrode and serially recorded the\ntactile responses of the individual neurons they encountered as they advanced\nthe microelectrode down into the depths of the brain tissue. By showing that\nmost of the neurons they encountered during each of these penetrations shared\nsimilar physiological properties—for instance, the neurons would all fire when\nthe same skin region was stimulated mechanically—Mountcastle provided\nphysiological evidence for the hypothesis that S1 was formed by a series of\nfunctional columns, a “vertically linked group of cells,” which he proposed\nconstituted the “elementary unit for cortical function.”\n\nIn his classic paper, Mountcastle took proper care to acknowledge that this\ntype of vertical cortical modular organization had previously been proposed by\nthe prodigious Spanish neuroanatomist Rafael Lorente de Nó. At the age of\ntwenty, de Nó had kicked off his career by picking a fight—a scientific\none—with none other than Don Santiago Ramón y Cajal, by submitting to one of\nCajal’s magazines an article that argued forcibly, contrary to Cajal’s\nopinion, that the organization of the mouse cortex was virtually identical to,\nand as rich as, that of the human cortex. Rather than balking, Cajal published\nthe article, although the relationship between both Spaniards was strained.\nSadly, Mountcastle’s painstaking footnote was, for the most part, overlooked\nby the many scientists searching to uncover the mysteries of the mammalian\ncortex.\n\nMountcastle was far from alone in his physiological mission. The industrious\nCanadian-Swedish partnership formed by David Hubel and Torsten Wiesel was also\nbusy recording single neurons in anesthetized cats, but in the primary visual\ncortex (V1) rather than S1. Using the same serial sampling technique, Hubel\nand Wiesel markedly expanded on the work of their mentor, the American\nneuroscientist Stephen Kuffler. Kuffler had mapped the receptive fields (RFs)\nof retinal ganglion neurons, which are positioned near the surface of the\nretina of the eye, and had found them to be nearly circular.\n\nYet, when Hubel and Wiesel sampled single V1 neurons, they discovered that\neach of them primarily responded to a bar of light that was presented at a\nparticular angle or orientation. Other V1 neurons responded more strongly if\nthe bars of light moved in a particular direction. Hubel and Wiesel proceeded\nto identify the orientation preference of a sequence of neurons that they\npenetrated in the V1 of cats, creating a complete map of these preferences. In\nthe map they spotted a large number of cortical columns, each of which\ncontained, throughout its vertical depth, neurons that gave a similar maximal\nand specific firing response when a bar of light of a given orientation hit\nthe eye. Hubel and Wiesel’s V1 map provided substantial ballast for the\ntheoretical framework defended by Horace Barlow, a neuroscientist at Trinity\nCollege, Cambridge. The great-grandson of Charles Darwin, Barlow promoted the\nnotion that single neurons worked as “feature detectors,” responding to\nspecific components of a complex stimulus.\n\n* * *\n\nFrom the very earliest recordings of single neurons made by Lord Adrian in the\n1920s to the breakthrough experiments conducted by Hubel and Wiesel to the\nheyday of the technique in the 1980s, the basic experimental approach used to\ninvestigate how the brain generated perceptual experiences did not evolve\nmuch. For starters, perceptions were always investigated from the perspective\nof an external observer who delivered a well-controlled, unimodal stimulus and\ntargeted it at a particular peripheral receptor (e.g., the skin, retina, inner\near, or tongue) of a subject animal. The experimenter was also responsible for\nmeasuring the evoked reaction produced by the animal’s brain and controlling\nthe brain’s internal state. That explains why, in the vast majority of the\nstudies, animals were kept in a deep state of anesthesia. Experimental\nconditions were easy to control that way.\n\nThis experimental approach biased neuroscientists toward the notion that the\nbrain rests static, devoid of any historic recollections and patiently waits\nto decode the information embedded in external physical stimuli by breaking\ndown these messages into their constituent features. In the case of a visual\nstimulus, for example, these compartmentalized features include orientation,\ncolor, and motion. That reductionist paradigm removed any possibility of\nincorporating the brain’s internal point of view, which in even a simple,\nsingle-stimulus experiment involves:\n\n• its internal dynamic state at the moment of its encounter with the stimulus,\nas well as the internal expectations created by the brain just prior to the\nencounter;\n\n• the accumulated evolutionary and individual perceptual history of the\nsubject, which summarizes the brain’s multiple previous encounters with\nsimilar and dissimilar stimuli;\n\n• its adaptive ability, which allows it to change in response to an encounter\nwith a novel perceptual experience;\n\n• the emotional value associated with the stimulus; and\n\n• the brain’s production of a series of motor behaviors, including eye, hand,\nand head movements, aimed at actively sampling the stimulus.\n\nInstead, experimenters focused on identifying neurons that could be driven to\nfire maximally in response to a primitive stimulus, and the notion took hold\nthat the individual features of the stimulus would trigger only a particular\nand specialized group of cortical neurons, located in discrete cortical areas.\nGall would certainly have been proud of this next generation of\nlocalizationists. In short order, cortical neurons were being categorized by\ntheir theoretical “feature-extracting” capability. These included, of course,\nHubel and Wiesel’s specialized V1 neurons for detecting oriented lines. And\nmany other neurons were joining them, with cascading effects.\n\nCells for detecting color or motion were discovered in cortical visual fields\nother than V1, leading to the establishment of a strict visual cortical\nhierarchy that originated in V1 and formed distinct streams of visual\nprocessing, known as the dorsal and the ventral pathways through the\noccipital, parietal, and temporal lobes. Because it was presumed that the\nlocation of the neurons in these two streams determined their roles in visual\nprocessing, the two systems were initially dubbed the where/how (dorsal) and\nwhat (ventral) pathways of the visual system. Further investigation, however,\ndemonstrated that there was significant cross talk between these streams.\nToday many question the usefulness of this scheme.\n\nSimilar segregated functional pathways were proposed for the auditory system\nand, with less emphasis, the somatosensory and motor systems. Yet, none of\nthese attempts could rival the grandiose schema built by the physiologists\nprobing the parietal, temporal, and occipital lobes where the visual cortex\nwas said to reside.\n\nThrough the 1980s and into the early ’90s, there were few souls brave enough\nto confront Gall’s inheritors. Systems neuroscientists almost unconsciously\nfell into the trap of classifying each and every bewitching neuron, drawing\ntheir names from a set of proverbial, and sometimes anthropomorphic, labels.\nFor instance, a neuron that vigorously responded to the flash of a familiar\nface—say, the picture of the person’s grandmother—was baptized, in 1969, as\nthe “grandmother neuron.” The homely grandmother neuron was joined by a\nneuronal celebrity in 2005. That was when the electrical discharges of the\n“Halle Berry neuron,” which responded “to the concept, the abstract entity, of\nHalle Berry,” were found in the inferotemporal cortex of a male patient and\nrecorded in all their splendor at the University of California, Los Angeles.\nWhere else would a celebrity neuron make its first appearance?\n\nAt first glance it probably seems natural that the activity of an individual,\nfeature-tuned neuron would play a starring role when the image of such a\npotentially beloved and admired lady as your grandmother or Academy Award\nwinner Halle Berry enters your visual field. In reality, however, these names\nmerely identify the best visual stimulus that experimenters were able to\nutilize to get a single neuron to fire maximally, and some of the recorded\nneurons also responded, albeit at lower intensity, to other visual stimuli.\nYet, by focusing only on one “high-profile” stimulus, neurophysiologists\nbecame more and more fixated on the target of their microelectrodes’\nmeasurements: the single neuron. As one of the classiest American\nneurophysiologists and historians, James T. McIlwain of Brown University, put\nit: “The widespread use of microelectrodes focused experimental research on\nthe behavior of single neurons and the possibility that their individual\nproperties could account for much of what the brain does.… I can testify\npersonally to the seductiveness of this … view. As you sit in a darkened\nlaboratory with your attention riveted to the sounds of the audiomonitor and\nprobe a neuron’s receptive field with a tiny visual stimulus, it is easy to\nforget that the cell you are listening to is but one of many that are\nresponding to the stimulus.”\n\nNotwithstanding the sheer, simple elegance—and seeming experimental success—of\nlabeling the physiological properties of single neurons, in the 1980s the\nGerman computer scientist Christoph von der Malsburg exposed a fundamental\nlimitation of the feature-extraction model. Malsburg’s challenge, which became\nwidely known as the binding problem, goes like this: If the brain really\ntreats a novel sensory stimulus by first decomposing its overall complex\nstructure into a series of discrete, primitive features, each of which is\nrepresented by a single specialized group of neurons located in a given\ncortical area, how can the brain put back together all this information, which\nhas been broken down (by feature) and distributed (spatially) across the\ncortex to reconstruct the original stimulus and generate the full perceptual\nexperience of a complex object that we know from daily life?\n\nThat was a good question!\n\nThe proponents of the feature detector theory of single neurons had no\nimmediate answer for Malsburg. It was as though he had opened a chasm in the\nfield, much like the one between general relativity and quantum mechanics.\nYet, after the initial furor provoked by Malsburg’s inconvenient but rather\npointed challenge, most neuroscientists returned to using the same old\ntechniques, the same old terms, and the same old thinking.\n\n* * *\n\nSince the early 1950s, there have been a few mavericks willing to stray from\nthe plodding of a single microelectrode into more adventurous attempts to\nsample the activity of populations of neurons. None of these early dissenters\nwas more daring than the American neuroscientist, philosopher, and writer John\nCunningham Lilly.\n\nAfter graduating, on an academic scholarship, from the California Institute of\nTechnology in 1938, Lilly earned a medical degree from the University of\nPennsylvania, where he also trained in psychoanalysis. After World War II, he\nlanded at the National Institutes of Health in Bethesda, Maryland, as the\nchief scientist of the exotically bureaucratic section called Cortical\nIntegration. Lilly’s years at NIH gave an early indication of a life committed\nto exploring unconventional ideas. Over the subsequent five decades, he was\nassociated with a number of unusual and often highly controversial research\ninitiatives, several of which raised eyebrows.\n\nLilly had a long-standing interest in human consciousness. In 1954, he plunged\ninto an investigation of how the human brain would react to an environment\ndevoid of sensory stimulation. In his quest he designed and extensively used\nan apparatus he named the sensory isolation tank. Lilly and one of his\nfriends, Edward Evarts, served as both the first subjects and the principal\ninvestigators. In the original studies, which later inspired the Hollywood\nfilm Altered States, starring William Hurt as a scientist devolving into a\nblob of consciousness, Lilly and Evarts took turns lying inside the tank,\nwhich was soundproofed and contained slowly flowing, warm salt water. Their\nbodies were suspended so that only the top of their heads remained above\nwater. A mask covered their heads to further reduce sensory stimulation. While\nunderwater, normal breathing was facilitated by attaching a tube to the mask.\nAfter a few training exercises to get accustomed to the conditions, Lilly and\nEvarts experimented with the tank. They would be isolated inside for a couple\nof hours, during which they were supposed to relax and restrict their body\nmovements. At the end of each session, they would file a report of their\nimpressions of the experience. Following a decade of self-experimentation with\nthe isolation tank, Lilly expanded his daredevil research by placing himself\nin this environment, either alone or in the company of live swimming dolphins,\nanimals that he was very fond of, after taking the psychedelic drug LSD.\n\nLilly’s experiments with LSD and other drugs, as well as his attempts to\ndemonstrate possible ways to establish direct communication between humans and\ndolphins (one of his most exotic ideas), strained his relationship with more\northodox scientists. As a result, his more mundane, albeit revolutionary\nneurophysiological studies have all but disappeared from the neuroscience\nliterature.\n\nStarting in 1949, Lilly and a few coworkers set out to find a new method for\nrecording large-scale electrical brain activity and for chronically\nstimulating brain regions with electrical pulses without producing tissue\ndamage in the process. Lilly’s goal was to unite neurophysiology and\nexperimental psychology, which remained completely balkanized in academia. In\nhis view, until the two fields came together, it would be impossible to\ngenerate “[an] accurate time-space description of central nervous system\nelectrical activity and behavior in the very short-term intervals.” The first\nstep would be to probe the brains of animals that had not been anesthetized.\n\nEver the pioneer, Lilly built an experimental setup capable of implanting\nelectrodes in unanesthetized animals, adding to this new paradigm the\ncapability of recording the electrical activity of the brain. And instead of\nlimiting himself to only a single electrode, which could be placed in a given\nbrain structure to record the electrical signals of a sequence of neurons over\ntime, Lilly designed a multi-electrode array that allowed him to sample\nsimultaneously electrical potentials of more than two dozen spots on the\ncortical surface. (His subjects in this experiment were cats and monkeys,\nrather than himself.) Lilly named his apparatus the “twenty-five-channel\nbavatron” and the brain electrical profiles he obtained with it, “electro-\niconograms.”\n\nTo render data from behaving animals, in his (mostly) precomputer age, Lilly\nhad to overcome enormous technological bottlenecks. In an early version of the\nbavatron, he threaded twenty-five metal wire electrodes into an array of\ntwenty-five glass tubes, arranged in a five-by-five square, with two\nmillimeters of space between each electrode. Each of the glass tubes was\nembedded in a Lucite cylinder, and the cylinder was mounted in a stainless\nsteel barrel; the barrel was screwed into a hole, three-quarters of an inch in\ndiameter, drilled into an animal’s skull. During an experiment, the animal was\nhoused in a soundproof box that itself was placed in a room shielded with\nmetal, in order to reduce the amount of stimulus disruption received by the\nanimal and the amount of electromagnetic “noise” generated by sources outside\nthe animal’s brain—including radio broadcasts from Washington, D.C. To further\nminimize noise, the glass tubes were filled with a saltwater solution and made\nto rest gently on the cortical surface. Once the barrel and the electrodes\nwere in place, each of the wires was connected to a twenty-five-channel\npreamplifier situated next to the animal. The outputs obtained by this\npreamplifier were fed via long cables to twenty-five amplifiers located\noutside the shielded room. A scientist who worked at NIH for many years once\ntold me that these constituted the entire stock of amplifiers available at the\ntime to the whole staff of the National Institutes of Health.\n\nLilly certainly was up to something big.\n\nFrom each of the twenty-five channels amplified and filtered by his array,\nLilly recorded the difference between the potential obtained from a given\nelectrode and the average potential obtained from the full array. This design\npermitted signals that were common to all of the electrodes in the array to be\ncanceled out from each individual electrode recording, allowing only the\nrelevant local electrical brain activity to be isolated from each sensor. This\ntechnical innovation, known as differential recording, is still used today to\nremove movement artifacts and other strong biological signals from\nneurophysiological recordings in behaving animals.\n\nLilly’s insight and creativity reached their pinnacle in his system for\ncapturing and recording the spatiotemporal waves of brain activity with the\ntwenty-five-channel bavatron. Without the luxury of computers or other large\ndata-storage devices, Lilly linked each of the outputs of his twenty-five\namplifiers to a twenty-five-square array of glow tubes, disposed into the same\nfive-by-five arrangement as the twenty-five electrodes resting on the cortical\nsurface. In this amazing apparatus, the intensity of the light emitted by each\nglow tube was modulated, either above or below an averaged value, by the\ndifferential electrical signal generated by the corresponding electrode in the\narray connected to a given bulb. Thus, if the electrical signal coming from a\ngiven electrode was negative (in relation to the mean potential of the full\narray), the glow lamp would brighten. Conversely, if the signal of one\nelectrode was positive, the light emitted by its corresponding glow tube would\ndim. Using this strategy, Lilly began to observe spatiotemporal waves of light\nthat corresponded to spatiotemporal patterns of electrical brain activity\nbeing recorded from a particular spot on the cortical surface while his animal\nsubject performed some type of movement, listened to auditory stimuli, or\nsimply went to sleep and then awoke. Through the sort of boundless, timeless\nlink that only human endeavors, like science and art, can establish, John\nLilly, secretly sequestered in a lab in Maryland, became the first\nneurophysiologist to glimpse the dynamic blossoming, spreading, and vanishing\nof unabiding, dissolving neuronal patterns as they were carefully woven by an\nenchanted mental loom, much like the one envisioned, five decades earlier, by\nthe great Sir Charles Sherrington.\n\nAs if this was not enough wizardry, in order to generate permanent records of\nthese complex spatiotemporal patterns of cortical activity, Lilly employed an\nelectrically driven, sixteen-millimeter Bell and Howell 70 G super-speed\nmotion picture camera to tape continuously the patterns of light produced by\nhis glow-tube array. In one of his papers describing this method, Lilly\nimplies that the camera made so much clatter that he had to find a spot for it\nin the building as far as possible from the shielded room with his animals so\nas not to distract them with the noise of making a “brain film.” (Later, Lilly\nwould devise a method for converting individual frames from his brain films\ninto careful woodcarvings: solid renderings of fluid brain dynamics.)\n\nIn the early days, Lilly carried out his experiments on anesthetized cats.\nThat provided a simple test of the electrode array and the entire recording\napparatus. He shared these findings in a series of papers published in leading\nneurophysiological journals. During this period Lilly also introduced a new\nmethod for stimulating the brain chronically, using a biphasic, charge-\nbalanced electrical pulse that did not damage tissue, a technique sometimes\nstill referred to as a Lilly wave. Eventually, Lilly progressed to experiments\ninvolving awake, behaving monkeys, whose heads were restrained by the\nrecording apparatus though they could either freely move their upper and lower\nlimbs or attend to sensory stimuli, such as a click sound. Unfortunately, much\nless is known about the results he obtained in these studies. In a single book\nchapter, Lilly briefly describes one of his most elaborate\nexperiments—implanting an astounding 610 cortical electrodes across most of\nthe cerebral cortex of an adult rhesus monkey. Since Lilly’s apparatus limited\nhim to recording only 25 electrodes, he was never able to collect the\nsimultaneous brain activity from all 610 sensors.\n\nAlthough in the last years of his life he may not have been aware of (or cared\nabout) the evolution of the field he helped to create, it is fair to say that\nLilly’s neurophysiological experiments marked his life deeply. Indeed, while\nwriting about his time in the isolation tank, Lilly recalled that during his\nfirst LSD trip he experienced the feeling of navigating within his own brain,\nof seeing his neurons firing. Those long nights at NIH making brain films left\nenduring memories in Lilly’s brain.\n\nDuring the 1960s, Edward Evarts, who collaborated with Lilly in the isolation\ntank studies, introduced his own method to perform brain recordings in awake\nand behaving nonhuman primates. His approach, which became the gold standard\nin primate neurophysiology, focused on single neurons that were recorded while\na monkey performed a particular behavior task. Most of what is known about the\nphysiological properties of individual cortical and subcortical neurons in\nprimates has been gathered using this method or variations on it. Its success\nis undisputable. Yet, without a multi-electrode array, neurophysiologists\ncannot understand how populations of neurons in a brain circuit operate in\nreal life—and that poses several significant limitations.\n\nFor starters, as we have seen, until very recently these single neuron\nrecordings were obtained using a single microelectrode that was slowly moved\nthrough the depths of a particular cortical or subcortical structure so that\nelectrical activity of individual neurons could be recorded, one at the time,\nin a sequential way. Second, in the traditional version of the experimental\nsetup, these single-unit recordings start only at the end of the animal’s\nbehavioral training. By the time any neuronal activity is recorded, the animal\nis overtrained in the task of interest. This leads to the confounding problem\nthat by encountering single neurons whose firing properties correlate to\nparticular task contingencies imposed by the animal’s behavior training, an\nexperimenter cannot decipher whether these findings reflect intrinsic\nphysiological attributes assigned to these neurons or simply depict the fact\nthat these highly adaptable neurons have been conditioned to fire according to\nthe salient features of the task. In some cases, neurophysiological\nexperiments seem to have morphed into tautological exercises, in which authors\nreport, usually with great excitement, that after many months of intense\nbehavioral training and painstaking analysis of neuron firing patterns, they\nwere capable of identifying a few neurons whose activity correlates with some\nof the task’s main attributes. Given that the behavioral task was usually the\nmost relevant activity carried out each and every day by the animals studied,\nthis is not at all surprising. The much tougher question to answer is whether\nthe neuronal firing is causally linked to the execution of the task. In\nEvarts’s paradigm the focus is also often placed on sampling neuronal activity\nin a single brain structure. Because of the technological constraints imposed\non experimental research, Sherrington’s integrative brain was once again\nsliced into its multitudinous parts and microscopic constituents, rather than\nbeing allowed to express its richness as a whole complex neural circuit.\n\nYet, few believed there was a way to break this traditional approach to\nprobing the brain.\n\n* * *\n\nOne afternoon circa 1987, during a break from an experiment that was not going\nwell, I found myself browsing through a book that I had recently purchased at\nmy favorite bookshop in downtown São Paulo. The book, written by the Savilian\nChair of Astronomy at Oxford University, Joseph Silk, was titled The Big Bang:\nThe Creation and Evolution of the Universe. As I distractedly flipped page\nafter page, I fell upon an illustration that grabbed my attention: a three-\ndimensional depiction of radio wave sources obtained by a milelong radio\ntelescope formed by placing smaller radio telescopes in phase at a site in\nEngland (see [Fig. 4.3](part0008.html#fig4-3)). By plotting the magnitudes of\nthe radio sources as Z-axis peaks distributed on a two-dimensional space\nrepresented on the X and Y axes of the plot, the resulting “map” identified\nthe location of radio galaxies and quasars in a sector of the universe. The\nmore I inspected the plot, the more I felt that the same approach could be\napplied to studying brain activity.\n\n![image](../images/00013.jpeg)\n\nFIGURE 4.3 3-D image of a small patch of the sky produced by an array of radio\ntelescopes in Cambridge, England. Peaks identify galaxies that emit radio\nsignals. The height of each peak indicates the magnitude of the radio signal\nproduced by each galaxy. (Originally published in Joseph Silk, The Big Bang\n[San Francisco, Calif.: W. H. Freeman, 1980], with permission from Mullard\nRadio Astronomy Observatory [MRAO] and Cavendish Laboratory, Cambridge.)\n\nSpurred by Silk’s book, my idea was to implant several “sensors” in multiple\nlocations of a given brain circuit, to create a similar 3-D neurophysiological\nmap. Furthermore, if a fourth dimension, representing time, could be added to\nthe map, I would have devised a completely new way to visualize, monitor, and\nmeasure the electrical activity of whole brain circuits in behaving animals.\n\nWeeks later, I mustered enough courage to talk to my mentor, Dr. César Timo-\nIaria, about my neurophysiological chart work. It did not take long for him to\ninspect my humble sketches and peruse the couple of pages that timidly and\nnaively outlined a new experimental approach in systems neurophysiology: to\nwatch the brain’s work from the brain’s own perspective.\n\n“I think it is time for you to finish your thesis, leave the laboratory, and\ngo abroad.” His verdict was swift and blunt.\n\n“Have I done anything wrong?” I could barely believe his reaction.\n\n“Nothing at all. You are simply ready to go. And what you want to do, neither\nI, nor anyone else in Brazil, can help you achieve.”\n\nFifty application letters and ten months later, I found myself one afternoon\ncontemplating a large welcome envelope from a young associate professor, John\nChapin, at Hahnemann University in Philadelphia. Inside, I found a series of\ngoodies. The first was a detailed research plan, funded by a grant from the\nNIH, for creating a new neurophysiological method that might one day produce\nthe spatiotemporal map of the brain I had in mind. Original and daring, the\nplan involved a level of technical innovation I had not encountered in any of\nthe dozens of scientific papers I had read in graduate school. Chapin aimed to\nmove neuroscience away from single-neuron recordings to a new technique that\nwould allow for simultaneously monitoring populations of individual neurons in\nbehaving animals, not just for a few hours but for several weeks, or even\nmonths, at a time. He was reaching for the holy grail of neurophysiology, and\nin the next five years!\n\nAt first glance, every step of the project looked like an impossible,\nborderline-crazy endeavor. For instance, instead of employing rigid metal\nmicroelectrodes, the classic tool utilized by neurophysiologists for almost\nhalf a century, Chapin proposed to take advantage of a new type of recording\nsensor, built in the format of arrays or bundles containing eight or sixteen\nhairlike, flexible microwires composed of stainless steel. For electrical\nisolation, each of these microwires would be encased in a thin layer of Teflon\nthat covered all but the microwire’s blunt tip. When implanted in the brain,\nChapin was certain that these devices would allow us to monitor tens of\nneurons simultaneously in anesthetized or freely behaving rats alike. To\nachieve this feat, the microwire arrays or bundles would need to be implanted\nin a given brain structure, such as the rat somatosensory cortex, during what\nlooked like, even after several of my readings, a pretty demanding\nneurosurgical procedure. Contrary to the norm at the time, the entire\nrecording apparatus would remain implanted—permanently—in the animal’s brain.\nThat was quite a departure from the status quo.\n\nPreviously, some neuroscientists had tried to leave rigid, sharp-tipped\nmicroelectrodes in the brain as an attempt to record the activity of neurons\nfor longer periods of time. Virtually all of these attempts had failed\nmiserably. After a couple of days in the brain, the microelectrodes mostly\nstopped working and no neuronal signals could be recorded from them; the\ninflammatory reaction triggered by implanting a foreign object in the animal’s\nbrain produced deposits of proteins and cells along the entire microelectrode\nsurface, particularly at its sharp and bare tip. These deposits blocked\nneuronal electrical signals from reaching the only exposed area of the sensor.\nMoreover, as the brain moved slightly inside the animal’s head, the\nmicroelectrode’s rigidity tended to create, over time, large tissue lesions\nthat led to the degeneration of the neurons lying next to the sensor.\n\nChapin believed both these issues could be solved, or at least mitigated, by\nusing flexible microwires with blunt tips (see [Fig.\n4.4](part0008.html#fig4-4)). Because of the larger exposed area of the\nmicrowire tips, he postulated that the inflammatory reaction would not be\nsufficient to block a tip’s entire surface. Instead, the inflammatory deposits\nwould actually improve the quality of the microwire’s recordings over the\nfirst two weeks or so, morphing it from an electrode with low to one with high\nresistance. The reaction would also anchor the filament to the brain. And\nbecause the microwires were rather flexible, once they became anchored they\nwould move in synchrony with the brain and therefore induce no significant\nbrain lesions.\n\nDuring the surgical implantation, Chapin’s microwire arrays would be pushed\ngently into the brain tissue so that their exposed blunt tips would rest in\nthe extracellular space surrounding many neurons. This arrangement would allow\naction potentials produced by many individual neurons to be recorded\ncontinuously by multiple amplifiers. In his proposal, Chapin hinted that a\ncompletely new piece of hardware would have to be built to handle the\noverwhelming amount of data recorded in the experiments. He also realized that\na variety of new analytical techniques would have to be developed to make\nsense of the recorded data.\n\n![image](../images/00014.jpeg)\n\nFIGURE 4.4 Engineering a better way to listen to the brain. On the left, a\nhigh-power magnification of a multi-electrode array produced at the Duke\nUniversity Center for Neuroengineering (DUCN) by Gary Lehew and Jim Meloy.\nNotice that multiple thin metal filaments are clustered in a matrix format.\nSuch filaments are flexible and can be chronically implanted in the brain,\nremaining active for months to years. On the right, a sample of different\ntypes of multi-electrode arrays created at the DUCN in the past decade.\n(Courtesy of Dr. Miguel Nicolelis.)\n\nBased on his preliminary findings, which were published as part of his\ndoctoral dissertation, Chapin noted that rats recovered quickly from this\nneurosurgical procedure. In fact, they went on to live normal rodent lives\nafter the microwire arrays were implanted. When they returned to their normal\nroutines, doing everything a regular rat might do in the lab, including\nmastering elaborate behavioral tasks, he could record their brain activity.\n\nReading the NIH grant, I quickly grasped the enormous influence that the\nexperiments Chapin was proposing might have on the field of neurophysiology if\nthey were to truly succeed. For the first time, there was a scientific road\nmap to the most unknown frontiers of the brain: the place where the electrical\nstorms of distributed neuronal populations combined into streams of thoughts.\nGetting there would be very difficult, perhaps impossible. But it was a trip\nworth attempting, one filled with pure adventure.\n\n* * *\n\nI arrived at Hahnemann in 1989 as a postdoctoral fellow. There, as the newest\napprentice of the Merlin of multi-electrode recording, I joined Chapin in his\nobsession with a much more mundane challenge.\n\nBoth of us desperately wanted to know how in heaven rats manage to escape from\ncats.\n\n\n5\n\nHOW RATS ESCAPE FROM CATS\n\nAs the computer-controlled sliding doors instantaneously opened, revealing the\npitch-dark yet familiar chamber, Eshe did exactly what was expected of her\nafter all those demanding weeks of training. Without hesitation—and likely\ncounting on the reward she was certain to receive given her superb performance\nof late—she lunged inside the narrow room at full speed, aiming for the\nopposite wall.\n\nClearly, she was ready to show off.\n\nThe experimental trial started at once, as Eshe’s head crossed an infrared\nlight beam located in front of an aperture positioned directly in her running\npath. The aperture, formed by the small arms of two T-shaped metal bars, each\nof which protruded from the lateral walls of the chamber, defined an opening\nthrough which Eshe had to pass in order to reach the far side of the room (see\n[Fig. 5.1](part0009.html#fig5-1)). Although she had already mastered her\nroutine, Eshe’s job was far from trivial. First, she had to place her nose\ninto a hole in the wall located just across from the aperture. Then, she had\nto estimate, in a single attempt and as quickly as possible, the aperture’s\ndiameter. To make things more complicated and interesting, this diameter\nvaried randomly from trial to trial. Thus, in order to receive the reward she\ncraved so much, Eshe had to determine whether the present aperture’s diameter\nwas narrower or wider than the one she had explored just a few seconds\nearlier. All in total darkness.\n\nWithout being able to see the bars, Eshe had only one way to achieve her goal:\nshe had to rely entirely on her exquisite sense of touch and the accumulated\nexperience of performing the task again and again in the course of the past\nmonth. Amazingly, in 90 percent of the trials, Eshe could correctly decide\nwithin 150 milliseconds whether the aperture she was touching was narrower or\nwider than it had been in a prior trial—even when the difference in diameter\nwas a mere couple of millimeters.\n\nEshe achieved her virtuoso tactile performance not by using the tips of her\npaws, but by touching the edges of the two bars with the tips of her facial\nwhiskers, the prominent long hairs (also known as mystacial vibrissae) that\nsprout from both sides of a rat’s face. Any man trying to solve a similar task\nby rubbing his mustache or beard on the same aperture would fail miserably.\n(That would go for a woman making the attempt, too.)\n\nHumans, of course, use their fingertips in problems involving tactile\ndiscrimination. This works very well because the skin covering our fingertips\ncontains a very high density of mechanoreceptors, whose elaborate and diverse\nmorphological structure allows us to perceive minute forces applied to the\nbody’s surface. When the receptive field of each of our mechanoreceptors is\nactivated by external forces, the information contained in a tactile stimulus\nis translated into the electrical language of the brain. Thus, the tactile or\nsomatic receptive field defines the amount of skin that, when stimulated,\nleads a peripheral mechanoreceptor or a neuron in the central nervous system\nto respond by producing a salvo of action potentials. The process of relaying\naction potentials from mechanoreceptors, known as sensory transduction,\nensures that as a mechanical stimulus is applied to the skin of a fingertip,\nswift sequences of action potentials are produced to signal the location,\nintensity, and duration of the stimulus. Mechanoreceptors generate tactile\nimages of the world that exists immediately around us.\n\n![image](../images/00015.jpeg)\n\nFIGURE 5.1 An experimental setup designed for testing the ability of freely\nbehaving rats to use their facial whiskers to discriminate the diameter of an\naperture in the dark. Eshe is seen in the right frame performing the task with\ngusto! (Originally published in D. J. Krupa, M. C. Wiest, M. Laubach, and\nM.A.L. Nicolelis, “Layer Specific Somatosensory Cortical Activation During\nActive Tactile Discrimination,” Science 304 [2004]: 1989–92.)\n\nAs we learned earlier, tactile messages are then conveyed through peripheral\nnerves to the central nervous system for further processing. The ascending\nbundles or nerve fibers that climb up first toward the subcortical relays and\nthen the cortical areas are usually referred to as “feedforward somatosensory\npathways.” These feedforward pathways are matched by so-called feedback nerve\nprojections, which flow in the opposite direction; they originate in the\nsomatosensory cortex and project down to several subcortical structures with a\nvariety of strange names, such as the thalamus and brain-stem nuclei. All of\nthe sensory systems contain a similar arrangement of feedforward and feedback\nprojections. Without knowing it, Eshe had been participating in an experiment\ndesigned to address one of the very basic functions of the interplay between\nthese feedforward and feedback pathways. Originally conceived by David Krupa,\na neurophysiologist working in my lab at Duke, Eshe’s tactile discrimination\ntask had been created to investigate the interactions in this somatosensory\nbrain circuitry during the active exploration of objects.\n\nThat Eshe decided to use her facial hair to solve this task was therefore only\nproper. After all, Eshe was a rat. And when rats really need to escape from\ncats, running through an aperture of unfamiliar location and diameter—say, a\nhole in the wall of the attic—the rhythmic movements of their facial hair\noffer the best hope for success, as frustrated cats would surely tell us, if\nthey could.\n\nLike the fingertips of primates, the hair follicle of each facial whisker of a\nrodent contains a high density of mechanoreceptors, which translate any minute\nmechanical deflection of a hair into electrical signals for transmission to\nthe feedforward nerve pathways and eventually the central nervous system. This\ntask is performed by the trigeminal system, a subdivision of the somatosensory\nsystem specialized in conveying and processing tactile signals from the face.\n\nUnderstanding how rats use their whiskers to perform their tricks of evasion\nis an interesting scientific question in its own right. However, many\nneurophysiologists realized early on that addressing the basic question of how\nlarge populations of trigeminal neurons process tactile information carries\nmuch more weight than simply learning how anxious rats elude hungry cats.\nIndeed, since the early 1970s, the rodent trigeminal system has become one of\nthe favorite experimental models of neurophysiologists interested in\nresearching neural coding. That was exactly what motivated Janaina Pantoja, a\ngraduate student working in my lab, to spend her days “listening” to neurons\nfrom the trigeminal system. Such “brain listening” was made possible by a\nneurophysiological technique designed and implemented during my days as a\npostdoctoral fellow in John Chapin’s laboratory and later expanded in my own\nlaboratory at Duke.\n\nIn its current incarnation, the technique for chronic, multisite,\nmultielectrode recordings enables neuroscientists to continuously and\nsimultaneously monitor the electrical activity produced by up to five hundred\nsingle neurons, located at multiple interconnected brain structures that\ndefine a neural circuit, for periods ranging from days to many years. Over the\nlast twenty years, the spatial and temporal sampling ranges of this method\nhave provided an unparalleled tool for probing the brain, compared with other\nmajor technologies, as can be seen in an illustration created by Terry\nSejnowski, a computational neuroscientist at the Salk Institute in San Diego\n(see [Fig. 5.2](part0009.html#fig5-2)). A generation of scientists at Duke and\nelsewhere has now used this method to record the concurrent activity of\nhundreds of single neurons, distributed across many of the brain structures\nthat define the trigeminal system, while rats performed a variety of\nbehavioral tasks.\n\n![image](../images/00016.jpeg)\n\nFIGURE 5.2 Temporal and spatial resolution of the single electrode versus\nmultielectrode recording methods. The top graph relates the spatial and\ntemporal resolution of most techniques used to investigate brain function. The\nlower graph compares the single and multi-electrode recording method using the\nsame parameters. (Adapted from A. Grinvald and R. Hildesheim, “VSDI: A New Era\nin Functional Imaging of Cortical Dynamics.” Nature Reviews Neuroscience 5\n[2004]: 874–85, with permission from Macmillan Publishers Ltd.)\n\nBut the possibility of carrying out such rewarding experiments did not come\nabout instantaneously. It required almost a decade of intense technological\ndevelopment, arduous data collection, and the publication of many studies\naimed at demonstrating the validity of our new method. Actually, it took a lot\nof publishing and talking to persuade a significant fraction of the\nneurophysiology community, so used to recording from a single neuron at a\ntime, to accept our findings.\n\n* * *\n\nThis persuasion began in 1989, when I joined John Chapin’s lab at Hahnemann\nUniversity in Philadelphia, the City of Brotherly Love. During the subsequent\nfive years, my goal was to implement and test this new neurophysiological\napproach systematically. A sample of the type of neuronal data obtained in\nthis approach can be seen in [Figure 5.3](part0009.html#fig5-3).\n\nMy postdoctoral research had also been designed to examine the validity of a\nneural coding scheme favored by most somatosensory neurophysiologists working\nwith rodents at the time. Known as the labeled-line model, this neural coding\nscheme was a variation on functional localization. It posited that sensory\ninformation generated at the rat body’s periphery was conveyed, through\nmultiple parallel and segregated feedforward somatosensory pathways, all the\nway to the neocortex. The model, therefore, purported that sensory information\nwas processed by the brain through a strict feedforward circuit, which\nconnected the peripheral mechanoreceptors in the skin surrounding the whisker\nfollicles to higher-order structures in the central nervous system.\n\nIn the early 1970s, the labeled-line model received a boost, thanks to the\nAmerican and Dutch neuroscientists Tom Woolsey and Hendrik Van der Loos, then\nat Johns Hopkins University School of Medicine. In a seminal study, they\nextracted blocks of tissue containing the entire mouse primary somatosensory\ncortex (S1), flattened the blocks, and cut thin tangential tissue sections,\nspanning the entire depth of the cortical tissue. Next, they employed a\nhistological staining method to reveal the existence of a particular spatial\ndistribution of cortical neurons that contain high levels of a mitochondrial\nenzyme—cytochrome oxidase, or CO—in the S1 cortex.\n\nAs in other mammals, the mouse cortex depth can be divided into six layers,\nnumbered I to VI. After staining the tissue, Woolsey and Van der Loos analyzed\ntheir thin brownish S1 sections sequentially from the top (layer I) to the\nbottom (layer VI). At midcortical depth (i.e., layer IV), they were surprised\nto discover the presence of multiple, clearly identifiable clusters of CO-rich\nneurons, forming a well-delineated matrix arrangement in which both rows and\ncolumns were clearly visible. Woolsey and Van der Loos named each of these CO-\nrich neuronal clusters a “barrel,” and the entire matrix arrangement the\n“barrel fields.” To everyone’s astonishment, these barrel fields defined a\nbeautiful, if slightly distorted, map of the entire mouse facial vibrissa. In\nthis topographic map, each of the barrels identified the position of a\nparticular, single facial whisker; the observed rows and columns of the barrel\nfields precisely matched the rows and columns that characterized the spatial\ndistribution of whiskers on the mouse snout. Whisker rows run through the\nvertical axis of the face, from the topmost (A) to the bottommost (E) row,\nwhile whisker arches run through the horizontal axis, from the tail-most (1)\nto the snout-most whisker (5 to 10, depending on the row). Thus, each facial\nwhisker could be identified by its row and arc position in the neuronal\ncluster. For instance, whisker C2 (a pet of many research projects) is the\nsecond whisker in the third row.\n\n![image](../images/00017.gif)\n\nFIGURE 5.3 Many single neurons. Computer screen image depicting the action\npotentials produced by a sample of 394 cortical neurons recorded\nsimultaneously in a freely behaving primate. The leftmost half of the picture\nshows four distinct families of action potentials, recorded simultaneously\nfrom a single microelectrode of an array, demonstrating the electrical\nactivity of four different cortical neurons sampled simultaneously. These four\ndistinct neurons are shown in isolation in the lower left corner. (Courtesy of\nDr. Miguel Nicolelis.)\n\nWoolsey and Van der Loos’s map triggered a flurry of scientific interest in\nthe rodent trigeminal system. Soon, a similar barrel-field arrangement was\nfound in the rat (see [Fig. 5.4](part0009.html#fig5-4)). And it wasn’t limited\nto S1. Topographic maps existed in the main thalamic relay nucleus of the rat\ntrigeminal system, the ventral posterior medial (VPM) nucleus, as well as the\nmain subdivisions of the trigeminal brain-stem complex. In the VPM, these CO-\nrich clusters were named “barreloids,” while in the brain stem they became\nknown as “barrelets.” Overall, these histological studies uncovered stacks of\ntopographic whisker maps at each of the subcortical relays of the trigeminal\nsystem, linking the peripheral organs, including the facial whiskers, to the\nS1 cortex. Further experiments revealed that neurons located within a given\nVPM barreloid—let’s say the one representing whisker C2—tended to project\nprimarily, though not exclusively, to the core of the cortical barrel\nrepresenting the same whisker in layer IV of the S1 cortex. This supported the\nnotion that the rodent somatosensory system represented the quintessential\nexample of a labeled-line system. In such a system, the single most important\nprediction derived from such a neural coding scheme was that individual\nneurons located within each of the cortical barrels, thalamic barreloids, and\ntrigeminal barrelets should respond significantly only to stimulation of the\nindividual whisker that each of them represented in the overall map. That\nwhisker became known as the “principal whisker.”\n\n![image](../images/00018.jpeg)\n\nFIGURE 5.4 The whisker map of the rat’s face. The left panel depicts the\ndistribution of facial whiskers in the rat snout on four rows and many\ncolumns. The right half of the figure depicts a horizontal section through\nlayer IV of the rat primary somatosensory cortex (S1) that contains the entire\n“rattunculus,” including a whisker representation (barrel cortex), nose (N),\nlower jaw (LJ), forepaw (FP), and hindpaw (HP), and is stained for the\npresence of a mitochondrial enzyme found in neurons. Dark clusters represent\nclusters of neurons in layer IV. Notice that the barrel cortex contains an\nisomorphic representation of whisker rows and columns. Circles identify\nwhisker C2 both in the rat face and the S1 cortex.\n\n(Courtesy of Drs. John Chapin and Rick Lin.)\n\nSure enough, initial measurements obtained in deeply anesthetized rats lent\nfurther support to the labeled-line model by showing that single neurons\nlocated within a given cortical barrel responded strongly, by producing a\nbrief sequence of action potentials, to the mechanical displacement of the\nfacial vibrissa represented by that CO-rich cluster of neurons. Over the next\ndecade, single neuron recordings, all obtained separately from individual\ncortical barrels, thalamic barreloids, and brain-stem barrelets, seemed to\nmove the labeled-line model into the realm of established scientific theory.\n\nBy the late 1980s, however, a few breaches started to appear. Leading the\ncharge was the British neurophysiologist Michael Armstrong-James, then at\nUniversity College London, who decided to record signals from single neurons\nlocated in multiple cortical barrels of anesthetized rats. Although Armstrong-\nJames was able to identify the principal whisker of most of these cortical\nneurons and demonstrate that it corresponded to the barrel in which the neuron\nwas located, he learned that these same neurons were also capable of\nresponding to the mechanical deflection of whiskers surrounding the principal\none. In what was considered at the time an almost heretical conclusion in the\nsmall but feisty rat somatosensory community, Armstrong-James and his team\nsuggested that the RF of neurons in the rat barrel cortex was not confined to\na single principal whisker. Instead, they said a few “surrounding whiskers”\nalso drove neurons to produce weaker and slower but nonetheless significant\nresponses.\n\nIt was in the summer of 1991, in the middle of this tumultuous state of\naffairs, that John Chapin and I decided we were ready to apply our technique\nfor multi-electrode recording to the question of the principal whisker. We had\nspent two long years testing circuit boards and building microelectrode\narrays. We decided to measure the RFs of individual neurons located in the\nmany barreloids of the rat VPM nucleus of the thalamus, the main source of\nascending somatosensory thalamic fibers to the S1 barrel field. The rat VPM\nwas chosen purposely because it contains only one class of cells, named\nthalamocortical (TC) neurons. TC neurons have extensive and elaborated\ndendritic trees that receive many hundreds of synaptic contacts from ascending\nnerve fibers originating in the trigeminal brain-stem nuclei. In addition to\nthese gorgeous and busy dendrites, TC neurons have long axons that exit the\nthalamus and project all the way to the barrel fields of the rat S1, where\nthey form excitatory synapses with the dendrites of cortical neurons. This\nthalamocortical pathway completes the last feedforward section of the major\nneural highway that connects the array of whiskers on the rat’s face to the\nrat’s cortex (see [Fig. 5.5](part0009.html#fig5-5), left panel).\n\nThere is another component to this feedforward circuit, though, that is worth\nmentioning. Before reaching the S1, the axons of TC neurons branch and give\nrise to an axonal collateral that terminates in a shell-like, thin layer of\nneurons known as the reticular (RT) nucleus, which encases, like an onion\nskin, most of the neuronal mass that defines the thalamus. The RT nucleus\ncontains only neurons that primarily utilize the neurotransmitter gamma-\naminobutyric acid (more familiarly called GABA), which inhibits the neurons’\nexcitability. Curiously, the axons of these GABAergic RT neurons return the\nfavor by projecting back to the VPM nucleus and providing the only inhibitory\nsynapses linked with TC neurons in the VPM. This comes in handy when a\nneurophysiologist wants to test a new method, since any neuron recorded in the\nrat VPM is, by definition, an excitatory TC neuron belonging to a given VPM\nbarreloid. Each of these TC neurons provides the main source of excitatory\nthalamic inputs to equivalent cortical barrels as well as to the RT nucleus,\nwhose neurons account for all of the inhibition detected in the VPM thalamus.\n\n![image](../images/00019.jpeg)\n\nFIGURE 5.5 The left half of the figure depicts the connectivity of some of the\nmain brain structures that define the rat trigeminal somatosensory system.\nExcitatory (+) and inhibitory (-) connections are shown. The mechanical\nstimulation of facial whiskers triggers electrical responses of neurons in the\ntrigeminal ganglion (Vg). Vg neurons project to two distinct trigeminal nuclei\nin the brain stem: the spinal (SpV) and principal (PrV) nuclei. These two send\nnerve pathways to three thalamic nuclei: the ventroposterior medial nucleus\n(VPM), the posterior medial (POM) nucleus, and the zona incerta (ZI). The\nthalamic reticular nucleus (RT) provides inhibition to the VPM and POM. VPM,\nPOM, and ZI provide thalamic nerve fibers to the primary somatosensory cortex.\nOf those, the ZI is the only one that sends inhibitory afferents to the S1\ncortex. In the right half of the figure, a stack of 3-D graphs illustrates the\nsimultaneously recorded tactile-evoked responses of populations of individual\nneurons at different levels of the trigeminal system. (Adapted from M.A.L.\nNicolelis, L. A. Baccala, R.C.S. Lin, and J. K. Chapin, “Sensorimotor Encoding\nby Synchronous Neural Ensemble Activity at Multiple Levels of the\nSomatosensory System.” Science 268 [1995]: 1353–58; and from M.A.L. Nicolelis,\nA. A. Ghazanfar, B. Faggin, S. Votaw, and L.M.D. Oliveira, “Reconstructing the\nEngram: Simultaneous, Multisite, Many Single Neuron Recordings.” Neuron 18\n[1997]: 529–37, with permission from Elsevier.)\n\nThis peculiar “wiring diagram” decisively influenced the design and goals of\nour experiments. The initial plan was simple: to simultaneously record the\nactivity of about two dozen VPM neurons, dispersed across the multiple\nneuronal clusters or barreloids that form the thalamic nucleus, and then\nrecord the electrical responses of these neurons to repetitive mechanical\nstimulation, conducted in a sequential and random order, of most of the facial\nwhiskers on a bunch of lightly anesthetized rats. To ensure that we could\nsimultaneously document the electrical activity of multiple VPM TC neurons, we\nbuilt custom microwire arrays and bundles that could yield nice, robust\nrecordings beginning a week after their careful surgical implantation. Careful\nand slow, I should say.\n\nIt took a while, but after a few failed implantations, we realized that to\nobtain the best results, we had to implant the microwires in the rat brain\nvery slowly, which helped guarantee that we did not damage the tissue. As we\nperfected the procedure, we slowed down a lot: to one hundred micrometer-per-\nminute maneuvers into the volume of the VPM nucleus, interleaved with one-to-\nthree-minute resting periods between each step to give the tissue time to\nadjust to each of our micropenetrations. It was around this time that, while\nlistening to Philadelphia’s radio stations day and night, I became closely\nacquainted with two local icons: the Philadelphia Philharmonic Orchestra and\nthe Philadelphia Phillies. After months of practice, with maestro Eugene\nOrmandy, slugger John Kruk, and relief pitcher Mitch Williams as my close\ncollaborators, the implants started to go smoothly.\n\nNext, it was time to wait for the rats (and me) to recover from the surgery\nand to test what kind of neural signal each of the implanted sensors yielded.\nAs John Chapin had predicted in his NIH grant application, after the recovery\nperiod, when the animals were brought to the laboratory, we could readily\nidentify the neuronal firing from most of the implanted microwires. It is\ndifficult to describe what I felt when the fast-flowing, fluorescent green\ntrace of an oscilloscope revealed the identity of the first action potentials\nrecorded from those hard-conquered VPM implants. By using a specially built\nmultichannel amplifier, we could filter, amplify, and store the electrical\nsignals produced by every single neuron that cared to fire next to the tip of\nthe microwires in the rat’s brain. That gave us the crisp, simultaneous\nsignals of nearly two dozen single neurons. During our recording sessions, I\nroutinely sent the amplifier’s output to a loudspeaker, and, with the turn of\na knob, endowed the neuronal electricity flowing in front of us with a voice\nso enticing, so melodic, that like Odysseus, one would have to fill one’s ears\nwith wax not to fall in love with it.\n\nThe singing of neurons, Dr. Timo-Iaria used to call it. After three years of\nwaiting, I was finally listening to their serenade.\n\nAlthough untrained ears listening to the sound of the neuronal firing coming\nout of our loudspeaker would refer to it as “popcorn-popping over a noisy AM\nstation,” those neurons were virtuosos of the tiny sparks of action potentials\nplaying across the brain. Here in our small, soundproof bunker of a lab in\nHahnemann’s Department of Physiology and Biophysics, we were getting close to\nunveiling the secrets of a brain circuit in real time. At once, we launched\ninto an intense spree of experimentation that included twelve- to sixteen-hour\nrecording sessions. I vividly recall one occasion when, at 5 A.M., after\nspending an entire day and night debugging an Eclipse minicomputer, Chapin and\nI paused and realized that between us we were holding an implanted rat, a\nscrewdriver, and a computer printout. The only thing I could think to say to\nmy bewildered but deliriously happy mentor was: “At least we can both share\nthe same divorce lawyer.”\n\nThe siren song of those bursting neurons left us no alternative but to shrug\noff the joke and go back to our neuronal symphonies.\n\n* * *\n\nHaving succeeded in recording the electrical signals produced by many VPM\nneurons simultaneously, the next obstacle we faced was creating a delicate way\nto deflect individual facial whiskers. This would allow us to measure\nquantitatively how each of the simultaneously recorded VPM neurons responded\nto a well-controlled mechanical stimulus. After a few days of tinkering,\nChapin and I came up with a low-tech, low-cost solution that could have come\noff the shelves of a Radio Shack and your neighborhood drugstore. To\nmanufacture the device, each morning in the lab I removed the cotton covering\non the end of the long wooden shaft of a typical hospital-grade Q-tip. Then,\nusing a Swiss Army knife, I carved the very tip of the Q-tip until it\nresembled a sharp needle. Using Krazy Glue, I cemented the cylindrical edge of\nthe Q-tip to the flat surface of a thick steel washer, which was tightly\nfitted to the metal shaft of a small electrical motor. This motor was then\nplaced in a small metal box wrapped in a copper mesh. When connected to a\nground wire, the mesh allowed us to eliminate the electrical noise generated\nby the motor itself. By using a simple stimulator to drive the electrical\nmotor, we could produce a very precise movement of the motor shaft—which\ncreated an equally accurate displacement of the Q-tip rod and its sharpened\ntip.\n\nOnce the day’s apparatus was ready for service, usually by mid-afternoon, the\ndemanding part of the experiment started. After anesthetizing a rat that had\nbeen previously implanted with our microwire array, we laid it on a small\ncushioned platform inside a Plexiglas recording chamber. I then connected the\nrat’s array to the hardware responsible for amplifying, filtering, displaying,\nand storing the electrical neuronal activity of the VPM neurons. Finally, I\nwas ready to stimulate about twenty of the rat’s whiskers, all located on the\nside of the face contralateral to the hemisphere of the brain with the\nimplanted array.\n\nStimulating one single whisker isn’t easy. Looking through a mounted\nmagnifying glass, I had to position the sharpened tip of the wooden rod a mere\nten millimeters from a whisker’s follicle root. At this point, the whisker\nsimply had to rest on the tip, so that I could confirm its position on the\narray. Once this was done, I turned on the motor powering our whisker\nstimulator. Each whisker stimulus lasted one hundred milliseconds and produced\na half-millimeter (or three-degree) upward movement in the whisker, followed\nby a downward movement that returned the whisker to its resting position. This\nwas repeated 360 times at one hertz, that is, once per second, leaving a gap\nof nine hundred milliseconds between each consecutive whisker stimulus. After\nthat, I’d find the next whisker to stimulate.\n\nWhile the whisker stimulation progressed, the ensuing electrical activity\nproduced from the VPM TC neurons I had been able to identify was recorded in\nperfect synchrony with the mechanical whisker stimulator. Because of the long\nperiod between stimulation trials (from the brain’s point of view, nine\nhundred milliseconds is very long), we also got a chance to record the\nspontaneous activity of these VPM neurons. At the end of each experimental\nsession, we could reconstruct the RFs of each of the neurons recorded, as well\nas the whisker map embedded in the entire VPM. Although this map had already\nbeen described, I was looking forward to creating the first topographic\nrepresentation of it based on our simultaneous multi-electrode recordings.\n\nAccording to the labeled-line model, there was no doubt that the RF of each of\nthe VPM TC neurons would strictly be confined to a single, principal whisker\nrepresented by the barreloid within which each given neuron resided. Yet, the\nresults that emerged from those lonely summer nights tweaking whiskers were\nfar from what was expected. After eighteen months of analysis, Chapin and I\nwere able to demonstrate that single VPM neurons were capable of responding\nsignificantly to the stimulation of many whiskers. When we quantitatively\nmeasured the number of action potentials produced by many single VPM neurons\nin response to the independent stimulation of many single whiskers, we saw\nthat the evoked firing response of the neurons was significantly above their\nresting activity. Then, just by counting the total number of whiskers that\ndrove each VPM to produce a statistically significant tactile response, we\nreached an unavoidable conclusion: VPM neurons had humongous, multiwhisker\nreceptive fields. The prediction made by the labeled-line model was not even\nclose. As we reported in two articles, one published in the Proceedings of the\nNational Academy of Sciences in 1993 and the other in the Journal of\nNeuroscience in 1994, some of the VPM RFs were so large that they pretty much\ncovered the entire face of the rat.\n\nA graphical representation of what happened when we stimulated the C1, D1, D2,\nE1, and E2 whiskers helps to explain our findings. [Figure\n5.6](part0009.html#fig5-6) depicts the type of data set obtained in the\nexperiments, plotted in a classic graph known as the peri-stimulus time\nhistogram (PSTH), which I used to quantify the RF size of the single VPM\nneuron. The histograms represent the frequency of action potentials (on the\nY-axis) produced by a given VPM neuron around the time (on the X-axis) of\nstimulation of a given whisker, with t = 0 ms being the time at which I\nstarted stimulating the whisker and t = 100 ms being the time at which I\nstopped. In these histograms one can observe several interesting things.\nFirst, before the whisker stimulation started, this VPM neuron fired very few\naction potentials—its firing frequency prior to t = 0 ms was very low. Yet,\nabout five milliseconds after the stimulus started, the neuron produced a\nvigorous excitatory electrical response that reached an instantaneous\nfrequency of almost 50 hertz. This so-called short-latency response was very\nbrief and decayed rapidly, likely due to the inhibitory action of the\nGABAergic RT neurons linked to this VPM cell. And given the reciprocal\nconnectivity between VPM and RT neurons, that inhibition was likely triggered\nby the VPM neuron’s strong firing.\n\nInspecting hundreds of such histograms, I discovered that the same VPM neuron\nresponded, albeit with a different magnitude and timing, to the mechanical\nstimulation of many other single whiskers. Moreover, when I looked closely at\nthe temporal dimension of these responses I noticed a rich pattern that, in\ntwo spatial dimensions, showed that the RFs of the VPM neurons also changed\nover time, after the onset of the stimulus. This is depicted in what I called\na spatiotemporal receptive field plot (see [Fig. 5.7A and\nC](part0009.html#fig5-7)).\n\nEach of these illustrations ([Fig. 5.7A, C](part0009.html#fig5-7)) begins in\nthree dimensions, with the distribution of whiskers on the rat’s face\nrepresented by its row and column spatial grid (the X\\- and Y-axes), plotted\nagainst the firing frequency generated by a single VPM cell when each of these\nwhiskers is individually stimulated (the Z-axis). Then, mapping a sequence of\nthese three-dimensional graphs as a function of post-stimulus time, I created\na 4-D plot that measured, for the first time, how the structure of the RFs of\neach of these VPM neurons varied in sequential steps of five to ten\nmilliseconds.\n\nReviewing these plots, Chapin and I recognized that something very intriguing\nhad emerged in our experiments. By inspecting the first three-dimensional\nslice, which described the spatial domain of the RF of a single VPM neuron at\nfive to ten milliseconds ([Fig. 5.7A](part0009.html#fig5-7)) after the whisker\nstimulus onset—the earliest possible time in which the neuron could have\nreceived information about the mechanical stimulation of the whiskers—we saw\nthat the cell was already firing significantly. Even at this very short\nlatency, the neuron’s RF ranged across a huge spatial area, formed by somewhat\nweaker responses triggered by the tweaking of surrounding whiskers.\nSpecifically for the example in [Fig. 5.7A](part0009.html#fig5-7), the spatial\ndomain of the RF was located in the farther-out, caudal region of the whisker\npad (principal whisker E1), close to the curvature of the mouth where the\nupper and lower lips join.\n\n![image](../images/00020.jpeg)\n\nFIGURE 5.6 VPM peri-stimulus time histograms. Four peri-stimulus time\nhistograms illustrate typical averaged electrical responses of single VPM\nneurons following the deflection of facial whiskers. In each histogram, the X\naxis represents the peri-stimulus time, 0 indicates the time of whisker\ndeflection. The Y axis depicts the number of spikes produced by the cell.\n(Courtesy of Dr. Miguel Vieira, Duke University.)\n\nBecause VPM neurons responded at distinct post-stimulus times to the tweaking\nof different whiskers, it was also clear that the overall spatial domain of\neach of these neuron’s RFs varied significantly depending on when the stimulus\nhappened. By twenty-five to thirty-five milliseconds after the stimulus onset,\nthe spatial center of the RF of one VPM neuron had steadfastly migrated from\nwhisker E1, in the caudal portion of the mouth, to whisker E4 (see [Fig.\n5.7A](part0009.html#fig5-7), last plot on the right), a hair located at the\nfront of the rat snout. And the surrounding RF had been distributed, too;\nwhereas it had started around whiskers C1, C2, D1, D2, and E2, it had moved to\nwhiskers C3, D3, D4, and E3. Not only were the RFs of these VPM neurons huge,\nbut their spatial domains wandered capriciously across the rat’s face over\ntime.\n\n![image](../images/00021.jpeg)\n\nFIGURE 5.7 Spatiotemporal RF and maps. (A) Spatiotemporal receptive field (RF)\nof a single VPM neuron. Each 3-D graph represents the spatial domain (RF) of a\nsingle VPM neuron at a particular post-stimulus time interval (5–10 ms, 20–25\nms, 35–50 ms). In each 3-D graph the X and Y axes depict the position of\nwhiskers in the rows and columns found in the rat’s face. The Z axis\nrepresents the magnitude of the VPM neuron’s firing response when one\nparticular whisker is mechanically deflected. Notice that at 5–10 ms, whisker\nE1 elicits the strongest firing response of the VPM neuron, while stimulation\nof other whiskers produces somewhat smaller responses. Yet, at 35–50 ms after\nthe whisker stimulus, whisker E4 triggers the strongest response of the same\ncell. Thus, the spatial center of the RF of this VPM neuron shifts as a\nfunction of post-stimulus time. (B) Spatiotemporal histogram depicting the\ntactile responses of a population of simultaneously recorded VPM neurons.\nPost-stimulus time is represented in the X axis, with 0 marking the onset of\nwhisker stimulation. The Y axis depicts a number of individual VPM neurons\nrecorded simultaneously. The gray-shaded Z axis illustrates the magnitude of\nfiring of these VPM neurons as a function of time. (C) Spatiotemporal RF of a\nsingle neuron located in the rat primary somatosensory cortex. In each of\nthese 3-D graphs, the X axis represents whisker columns, the Y axis represents\nwhisker rows, and the Z axis (gray-scale) represents the magnitude of a single\ncortical neuron response. Each 3-D graph depicts a particular post-stimulus\ntime interval (8–12, 12–16, 16–20, 20–24, 24–28 ms). Notice that, like in the\nVPM, the spatial domain of the RF changes as a function of post-stimulus time.\n(Adapted from M.A.L. Nicolelis, L. A. Baccalá, R.C.S. Lin, and J. K. Chapin,\n“Sensorimotor Encoding by Synchronous Neural Ensemble Activity at Multiple\nLevels of the Somatosensory System.” Science 268 [1995]: 1353–58; from M.A.L.\nNicolelis, A. A. Ghazanfar, B. Faggin, S. Votaw, and L.M.O. Oliveira,\n“Reconstructing the Engram: Simultaneous, Multisite, Many Single Neuron\nRecordings.” Neuron 18 [1997]: 529–37, with permission from Elsevier; from\nM.A.L. Nicolelis, and J. K. Chapin, “The Spatiotemporal Structure of\nSomatosensory Responses of Many-Neuron Ensembles in the Rat Ventral Posterior\nMedial Nucleus of the Thalamus.” Journal of Neuroscience 14 [1994]: 3511–32,\nwith permission; and from A. A. Ghazanfar and M.A.L. Nicolelis,\n“Spatiotemporal Properties of Layer V Neurons in the Rat Primary Somatosensory\nCortex.” Cerebral Cortex 9 [1999]: 348–61, with permission from Oxford\nJournals.)\n\nHidden deep in the rat brain, space and time had fused to such a level of\nintimate amalgamation that to speak of the probable spatial domain of a given\nVPM neuron’s RF was meaningless, unless you also specified, in the same\nbreath, the precise moment of post-stimulus time to which you were referring.\nMoreover, given that single VPM neurons did not fire the same number of action\npotentials during each trial of whisker stimulation, the Z-axis of each of the\n3-D plots did not represent absolute firing magnitudes, but rather a simple\nestimate of the probability of each neuron to fire, in response to a given\nwhisker stimulus, at a particular moment in time (see another example of the\nRF of S1 cortical neuron in [Fig. 5.7C](part0009.html#fig5-7)).\n\nI call this the uncertainty principle of neurophysiology, one of ten\nprinciples that describe how a relativistic brain generates thinking from its\nown point of view.\n\nTHE UNCERTAINTY PRINCIPLE OF NEUROPHYSIOLOGY\n\nOne cannot define the spatial domain of a particular neuronal receptive field\nwithout specifying a particular moment in time. In other words, the spatial\nand temporal domains of neuronal firing are tightly coupled, defining a\nneuronal space-time continuum.\n\nI propose that this space-time coupling emerges as a result of the fact that,\nat each moment in time, different combinations of neuronal afferent signals\nconverge on neurons.\n\nOur discovery of spatiotemporal RFs directly challenged the labeled-line\nestablishment. Furthermore, instead of corroborating the notion that the VPM\nsomatotopic map was defined by parallel feedforward neural pathways ascending\nfrom the periphery, our data suggested that such a tactile representation was\nthe result of an asynchronous interplay of three main neural systems: the\nexcitatory trigeminothalamic feedforward pathway, the excitatory\ncorticothalamic feedback projection, and the potent inhibitory inputs provided\nby the RT neurons. These three major influences converged on different\nlocations of the dendritic trees of the VPM neurons at distinct moments in\ntime to define a dynamic spatiotemporal map (see [Fig.\n5.7B](part0009.html#fig5-7)).\n\nThis asynchronous convergence principle is the second of the ten\nneurophysiological principles defining the relativistic brain.\n\nTHE ASYNCHRONOUS CONVERGENCE PRINCIPLE\n\nThe receptive field of an individual neuron and the “maps” embedded in brain\nregions are defined by the asynchronous spatiotemporal convergence of multiple\nascending, local, and descending influences provided by a myriad of other\nneurons. Receptive fields and maps can only be properly defined by coupling\ntheir spatial and temporal domains in a single space-time continuum.\n\nTaken together with the uncertainty principle, asynchronous convergence\noverturns the classical definition of both receptive fields and somatotopic\nmaps, in which time plays no role. Instead, I propose that receptive fields\nand maps are nothing but dynamic and fluid spatiotemporal distributions of the\npotential probabilistic neuronal population firing patterns.\n\n* * *\n\nWe soon realized that there was yet another major aftershock coming out of our\nexperiments: that a dynamic VPM nucleus map should have the potential to endow\nVPM neurons with the ability to reorganize or remap their tactile responses\nquickly—indeed, immediately after any manipulation that changed the ascending\nflow of tactile information generated at the whisker pad. Our prediction could\nbe tested directly by anesthetizing small patches of skin on the rat face, and\nthen measuring the effects of this peripheral blocking on the RF of the VPM\nneurons. So we quickly performed a second series of experiments in the same\nanimals.\n\nLo and behold, a couple of seconds after a patch of facial skin was\nanesthetized with lidocaine, a local anesthetic, a widespread functional\nreorganization of the spatiotemporal RFs of VPM neurons was triggered. As a\nconsequence, the entire whisker map embedded in that nucleus reorganized\nitself to a new equilibrium point, and it did so almost instantaneously. Once\nagain, by recording many individual neurons simultaneously across the whole\nVPM nucleus, we documented in exquisite detail, from the tips of our\nmicrowires, how the somatotopic map of the rat face graciously shifted to\nreflect the new reality of the animal’s periphery.\n\nFor some systems neurophysiologists, these results, published in Nature in\n1993, were even more shocking than our initial study. In the early 1990s, very\nfew believed that a subcortical structure like the VPM thalamus could exhibit\nthe kind of adult plastic reorganization extensively documented at the\ncortical level. A couple of years later, however, we were able to observe this\ndynamic and distributed view of brain function throughout the entire\ntrigeminal system. Using yet another new strategy, I had decided to implant\nmultiple arrays of microwires in the brains of the rats, eventually succeeding\nin taking recordings from up to forty-eight single neurons in a session. The\nneurons were distributed across the main structures that form the rat\ntrigeminal system, including the trigeminal ganglion, two trigeminal brain-\nstem nuclei, the VPM, and the S1 cortex (see the example in [Fig.\n5.5](part0009.html#fig5-5), right panel). It was the first time in the history\nof systems neurophysiology that a neuronal sample from a whole neural circuit\nhad been visualized and measured in a freely behaving mammal. Yes, you read it\nright: freely behaving—or, in this case, freely awake and whisking.\n\n* * *\n\nThe moment had come to take recordings from awake animals and demonstrate that\nsingle whisker deflections triggered complex spatiotemporal waves of\nelectrical activity, which spread across multiple CO-rich clusters within each\nof the neural structures we were simultaneously monitoring. The effect was\nmost prominent at the level of the VPM and S1, but could also be observed in\none of the subdivisions of the trigeminal brain-stem complex, meaning that\nindividual neurons located in most of the relays of the rat trigeminal system\n(with the exception of the trigeminal ganglion and the main nucleus of the\ntrigeminal complex) responded to the stimulation of multiple individual facial\nwhiskers. At long last, we had come into contact with a distributed\nrepresentation, a population neural code, playing its neural symphonies right\nbefore our eyes and ears.\n\nNothing could have been farther removed from the labeled-line model. Instead\nof highly specialized neurons (like the infamous grandmother neuron) that fire\nin response to a single stimulus attribute (e.g., the face of one’s\ngrandmother), distributed neural representations are formed by broadly tuned\nneurons, which convey small amounts of information. As such, any individual\nneuron’s instantaneous firing activity, when taken in isolation, is incapable\nof either discriminating between multiple stimuli or sustaining any behavior.\nHowever, when large populations of broadly tuned neurons are working together,\nprecise computations can be achieved. For instance, in the 1980s the Greek-\nAmerican neurophysiologist Apostolos Georgopoulos, then at Johns Hopkins,\nreported that individual neurons in the primary motor cortex of rhesus monkeys\nwere broadly tuned to the direction of arm movements. Georgopoulos went on to\ndemonstrate that each of these cortical neurons fired significantly, and at\ndifferent magnitudes and time spans, before a variety of movements were made.\nThis made it impossible to predict the direction in which a monkey’s arm was\nabout to move from the activity of one neuron. Yet, when Georgopoulos combined\nthe activity of hundreds of neurons, he was able to obtain precise predictions\nof movement direction and generate accurate arm trajectories—in a single\ntrial. A similar distributed scheme appeared to be employed in the rat\ntrigeminal system for representing tactile stimuli. By combining the activity\nof large populations of single neurons that displayed broad, multiwhisker RFs,\nwe could extract precise and meaningful information about the rat’s immediate\nsurrounding environment—just as the rat brain was doing.\n\nBut there was more to this distributed framework than large populations of\nbroadly tuned neurons.\n\nSince my new multisite recording strategy allowed me to sample neuronal\nactivity across most of the rat trigeminal system in freely behaving animals,\none day, out of curiosity, I decided to observe how this circuit operated when\na fully awake rat simply stood in the recording chamber without receiving any\nmechanical stimulation to its whiskers. In principle, it was a calibration\nexperiment, a way to test the recording setup for the demanding whisker\nstimulation routine I would be performing on a perfectly immobile rat later in\nthe day.\n\nA few minutes into the recording session, I realized that the neuronal signals\nI was hearing had nothing to do with a passive brain participating in a\nroutine calibration. That rat brain was certainly engaged in something. As the\nrat stopped moving around the chamber and attentively stood its ground, a very\nrhythmic sound poured out of the lab’s loudspeaker. Paying close attention to\nthe rat’s state of “attentive immobility,” I switched the neuronal amplifier\nsignal to other cortical, thalamic, and brain-stem neurons. Most of the cells\nI was recording, across the entire trigeminal system, were firing at the same\nfrequency. In fact, with the exception of the neurons in the trigeminal\nganglion and in one nucleus of the trigeminal brain-stem complex, most of the\ncortical and subcortical structures in the trigeminal somatosensory system\nwere expressing the same pattern of rhythmic firing.\n\n![image](../images/00022.jpeg)\n\nFIGURE 5.8 Examples of 7–14 Hz rhythmic mu oscillations observed in the rat\ntrigeminal somatosensory system. In the left panel, different traces obtained\nsimultaneously illustrate that mu oscillations start in the S1 cortex, spread\nto the VPM, and later to the spinal complex of the trigeminal brain-stem\ncomplex (SPV) before whisker twitching movements begin. In the right panel, a\nsimilar illustration of the relationship between mu rhythm and whisker\ntwitching is made between the barrel cortex (rat S1 whisker area), the VPM,\nthe basal ganglia (CP), and the hippocampus (HI). (Originally published in\nM.A.L. Nicolelis, L. A. Baccala, R.C.S. Lin, and J. K. Chapin, “Sensorimotor\nEncoding by Synchronous Neural Ensemble Activity at Multiple Levels of the\nSomatosensory System.” Science 268 [1995]: 1353–58.)\n\nAfter a few seconds, the rat, totally oblivious to my neuronal eavesdropping,\ndelicately—and synchronously—moved the long whiskers on both sides of its\nface. During each movement cycle, the whiskers rapidly moved forward, then, a\nfew tens of milliseconds later, retracted back to their original position. A\nnew whisking cycle would begin. The overall amplitude of these whisker\nexcursions was very small, suggesting that vibrissa behavior was distinct from\nthe large-amplitude whisker movements used by rats to explore the objects they\nencounter when walking around. The most conspicuous feature of these small-\namplitude whisker movements, however, was their frequency: around ten cycles\nper second, the same frequency of brain oscillatory activity that had preceded\nthem.\n\nOnce they started, these discrete, low-amplitude whisker movements, which I\ncall whisker twitches, somewhat modulated the rhythmic neuronal firing of the\nentire trigeminal system. As long as the rat remained in an immobile posture,\nboth the brain oscillations and the whisker twitching could proceed\nuninterrupted (see [Fig. 5.8](part0009.html#fig5-8)). When the rat finally\ndecided to explore the recording chamber, its whisker movements increased\ndramatically in amplitude, up to four to six hertz, half the frequency\nobserved during the whisker twitching.\n\n* * *\n\nHaving collected a couple of hours of my “calibration data,” I spent several\nweeks poring over the neuronal recordings. To my amusement, the rhythmic\nneuronal firing, whose frequency power was distributed across a seven-to-\ntwelve-hertz range, always started in some part of the S1 cortex. After\nspreading across most of S1, a process that took about ten to twenty\nmilliseconds, the synchronous waves of rhythmic firing started to appear in\nthe VPM thalamus, where almost at once most of the thalamocortical neurons\nwere recruited into the rhythmic firing. Similar oscillations could be seen in\nanother thalamic nucleus, and in one of the trigeminal nuclei in the brain\nstem (see [Fig. 5.8](part0009.html#fig5-8)). Even before the rat had a chance\nto produce its whisker-twitching movements, most of its trigeminal system was\nseemingly invaded by the cortex’s seven-to-twelve-hertz firing wave. The\nfiring was flowing in the opposite direction to the ascending feedforward\npathways of the trigeminal system.\n\nIn a paper published in Science reporting these findings in 1995, Chapin and I\nproposed the existence of the dynamic and distributed spatiotemporal\nrepresentations of tactile information that ranged across the whole of the rat\ntrigeminal system. Moreover, we suggested that this large-scale rhythmic\nneuronal firing could represent an internal temporal reference signal produced\nby the rat brain to synchronize the activity of multiple, spatially dispersed\nneural structures into a cohesive circuit. This temporal signal could be\nresponsible for generating a highly attentive state that allowed rats to\nanticipate and, most likely, better discriminate any incoming tactile\ninformation that could be acquired by rhythmic whisker movements before they\nstarted their next bout of active exploration.\n\nThat was my first encounter with a typical manifestation of a living brain’s\nown point of view. By listening to those enchanting and ever-changing rhythmic\nsymphonies of neuronal firing over our loudspeakers, I had arrived, almost by\naccident, at an unexplored territory of brain research.\n\nI was eager to plunge into this new world and discover how far I could push\nits boundaries.\n\n* * *\n\nThose were the last experiments I carried out as a postdoctoral fellow at\nHahnemann University. Still not quite knowing how rats escape from cats, in\nthe fall of 1994 I found myself setting up a laboratory in the recently\ncreated Department of Neurobiology at Duke University. Soon after my arrival\nat Duke, a young Pakistani-American graduate student, Asif Ghazanfar, who had\njust graduated with a double major in philosophy and biology from the\nUniversity of Idaho at Moscow, joined my research group. The Brazilian-\nPakistani core of our newly inaugurated lab became kindly known in the\ndepartment as the “lab from nowhere.”\n\nFor the next two years, Ghazanfar and I worked furiously to test the ideas\nthat had erupted as a result of my research at Hahnemann. For instance,\nGhazanfar, who is now an associate professor at Princeton University,\nconfirmed that single neurons in the rat S1 also exhibited large, multiwhisker\ndynamic RFs in which the spatial dimension of the RF varied as a function of\npost-stimulus time (see [Fig. 5.7C](part0009.html#fig5-7)). Moving further, he\ndemonstrated in quantitative terms that populations of neurons with large,\nmultiwhisker RFs could accurately predict the location of a single whisker\nstimulus, in a single trial. This was done by inputting the activity of many\ncortical neurons, obtained during stimulation of multiple single whiskers,\ninto a series of pattern recognition computational algorithms, known as\nartificial neural networks (ANNs). In these experiments, Ghazanfar trained an\nANN to employ the spatiotemporal firing patterns produced by populations of\ncortical neurons to classify correctly the location of a single whisker\nstimulus. Once the algorithm reached a high level of accuracy in classifying\nthis “training set,” he introduced a new database of trials that had never\nbeen presented to the ANNs. When the activity of populations of single neurons\nwas fed to the ANNs, the identity of the stimulus (i.e., which whisker had\nbeen deflected) was accurately predicted, but when the activity of single\nneurons in isolation was fed into them, the ANNs failed.\n\nBy then, other laboratories were obtaining data from a variety of experimental\nmethods that strongly supported our electrophysiological findings. For\nexample, the Israeli neurophysiologist Ron Frostig, at the University of\nCalifornia, Irvine, employed a brain imaging method called intrinsic optical\nimaging to measure the spread of activation in the rat S1 cortex produced by a\nsingle whisker deflection. He was able to show that a tiny stimulus induced a\ncomplex spatiotemporal response that swept across most of S1. Moreover, in\nvivo intracellular recordings of S1 neurons conducted by Chris Moore, Sacha\nNelson, and Mriganka Sur at MIT, and independently by Barry Connors’s lab at\nBrown University, revealed that single cortical neurons, no matter which\ncortical layer they belonged to, received afferent information from many\nwhiskers. Thus, each neuron was bombarded by synaptic currents that were\ntriggered by the stimulation of many single whiskers. As in our results, the\nRFs of these neurons were multiwhisker.\n\nGhazanfar went on to show that VPM and S1 neurons could also integrate\nmechanical stimuli delivered to multiple whiskers at once, and that blocking\nS1 neuronal activity decisively affected the multiwhisker tactile responses of\nVPM neurons. A few months later, David Krupa, then a postdoctoral fellow in my\nlab, observed that blocking the so-called corticothalamic pathways decreased\nthe ability of VPM neurons to exhibit plastic reorganization following\nanesthesia of a few facial whiskers. These findings gave credence to the\nasynchronous convergence theory by demonstrating, unequivocally, that the\nfeedback somatosensory projections, originating in the S1 cortex, that target\nthe VPM thalamus sometimes play a major role in managing the flow of tactile\ninformation through the thalamus. Based on these various results, we proposed\nthat the highly dynamic, multiwhisker tactile responses of both S1 and VPM\nneurons were determined by the asynchronous convergence of a multitude of\nascending, descending, local, and modulatory afferents that converge to each\nof these neurons at a different moment in time.\n\nMany of the predictions that could be derived from the asynchronous\nconvergence principle required extensive testing. For instance, while a\ngraduate student in my lab in the late 1990s, Erika Fanselow, now at the\nUniversity of Pittsburgh, designed a clever way to measure how S1 and VPM\nneurons in freely behaving rats would respond to similar tactile stimuli\ndelivered under different behavioral conditions. By implanting a tiny “cuff”\nelectrode around the infraorbital nerve (ION), the branch of the trigeminal\nnerve that innervates the facial whiskers, Fanselow delivered precise\nsequences of electrical pulses to the ION while simultaneously recording the\nevoked responses of populations of single neurons. She then employed this\napparatus to measure how these neuronal responses varied according to the\ntypical, and stereotypical, behaviors exhibited by rats during their daily\ncycle.\n\nWhen rats were moving their whiskers, their cortical and thalamic neurons\nresponded to tactile stimuli in a way that was very different from the\nresponses observed when the same animals were quiet and immobile. Instead of\nthe classical cycles of excitatory responses followed by profound, long-\nlasting inhibition, the cortical and thalamic neurons of the rats responded in\na more sustained manner to a single electrical nerve pulse and did not exhibit\nany post-excitatory inhibition—no matter what sort of whisker movement they\nwere producing. This prompted Fanselow to deliver to the nerve a sequence of\ntwo electrical pulses instead of one pulse. What she noticed was astounding.\nWhen rats were awake, immobile, and not producing any whisker movements, their\ncortical and thalamic neurons could only respond to the first electrical\npulse; the second pulse was masked by the neurons’ post-excitatory inhibition.\nOn the other hand, when rats were actively moving their whiskers, their S1 and\nVPM neurons could respond well to both electrical pulses; they could even\nrespond to pulses separated by as little as twenty-five milliseconds.\nObviously, whisking allowed both the cortex and the thalamus to represent\nfaithfully a sequence of tactile stimuli, something that was not possible when\nthe rat was simply awake but immobile.\n\nFanselow’s results clearly showed that tactile responses varied depending on\nthe animal’s behavior state. Of course, her subjects were not engaged in a\nmeaningful tactile task. So the question arose: how would the somatosensory\nsystem of the rat behave when it needed to use its whiskers to perform a\nmeaningful and demanding task, such as using its facial hair to judge the\never-changing diameter of a hole? As every household cat knows, this is a task\nrats perform with gusto.\n\nWhile David Krupa worked on designing an appropriate experimental task to\naddress this issue, another member of our research team, Marshall Shuler, now\nan assistant professor at Johns Hopkins, discovered that a large percentage of\nS1 neurons outside the barrels of layer IV responded to the stimulation of\nwhiskers located on both sides of the rat’s face. These bilateral responses\nwere first observed in lightly anesthetized animals. A few years later,\nShuler’s experiments were repeated with awake rats by Mike Wiest, a\npostdoctoral fellow in my lab at Duke, now an assistant professor at Wellesley\nCollege. He confirmed Shuler’s findings: rats judged the diameter of a hole by\nintegrating tactile information generated by whiskers on both sides of the\nface.\n\nBy then, Krupa had found a way to train rats to perform the task that Eshe\nlater mastered so well. This allowed us to explore whether tactile responses\nin S1 and the VPM nucleus varied according to whether a multiwhisker stimulus\nwas delivered passively to the rat or whether it resulted from the active\nengagement of the animal’s whisker in a tactile discrimination task in which\nsuccess was rewarded. To control for the possibility that the whiskers could\nbe stimulated differently when the animal actively touched the aperture, Krupa\nbuilt the ingenious apparatus in which the aperture created by two bars can\nmove toward the face of an awake but immobilized rat. In this setting, the\nanimal’s whiskers are rubbed by the edges of the bars in a way that is\nvirtually identical to what would happen if a rat rushed into a box and\ntouched the bars by itself. The only difference was that in the active task,\nthe rat had to use its whiskers to discriminate the aperture’s diameter and\nbehave a certain way to receive a liquid reward.\n\nThe experiment showed that if multiple whiskers were stimulated passively,\neither by a multiwhisker stimulator or by the movement of the entire apparatus\ntoward the animal’s face, both S1 and VPM neurons produced short-duration,\nphasic excitatory responses. Very few pure inhibitory responses were observed.\nHowever, when animals actively engaged their whiskers to judge the diameter of\nthe aperture in exchange for a reward, a large percentage of their S1 and VPM\nneurons exhibited intense, long-duration excitatory responses (see [Fig.\n5.9](part0009.html#fig5-9)). Moreover, a large percentage of cortical neurons\nexhibited pure long-lasting inhibitory responses that had never been seen in\neither anesthetized or awake-but-immobile rats.\n\n![image](../images/00023.jpeg)\n\nFIGURE 5.9 Peri-event histograms depict the firing response pattern of a\nsingle cortical neuron in the rat primary somatosensory cortex under three\ndifferent behavioral conditions: active tactile discrimination in a freely\nbehaving animal (leftmost panel), awake but immobilized (center panel), and\nimmobilized and passive discrimination (rightmost panel). Notice how the\npattern of this neuron’s responses is totally different according to the\nanimal’s behavioral context. For each histogram, the X axis represents peri-\nevent time, with 0 indicating the onset of facial whisker mechanical\nstimulation, and the Y axis represents the electrical firing response of the\nneuron in spikes per second. (Originally published in D. J. Krupa, M. C.\nWiest, M. Laubach, and M.A.L. Nicolelis, “Layer Specific Somatosensory\nCortical Activation During Active Tactile Discrimination.” Science 304\n[1989–1992, 2004].)\n\nWhen an analysis of the responses across the different cortical layers was\nconducted, Krupa discovered that the neuronal firing in layers II/III and V/VI\ntended to increase or decrease early in the trial, long before the rat’s\nwhiskers had made contact with the bars. More surprising, modulations in the\nneuron firing rate started before any neuronal firing could be seen in layer\nIV, the main target of ascending thalamocortical fibers that carry tactile\ninformation from the periphery to S1. By using a variety of physiological\nmeasurements, Krupa went on to show that during the rat’s execution of the\ntask, neurons located in different cortical layers behaved distinctly. His\nfinding challenged the notion, championed by the illustrious neurophysiologist\nVernon Mountcastle, that neurons located across a vertical column of S1 fire\nsimilarly to the same tactile stimulus. At least in the rat somatosensory\ncortex, when rats use their whiskers to solve a tactile puzzle, the functional\nunit of thinking is not a column of neurons, but populations of neurons\ndistributed across the entire 3-D volume of S1 cortex.\n\nAs a final demonstration, Krupa fed an artificial neural network with the\nspatiotemporal firing patterns generated by populations of individual neurons\nduring the execution of this behavioral task. This allowed him to show that\nthe combined activity of up to fifty cortical neurons, which exhibited either\nlong-lasting excitatory or inhibitory tactile responses, could predict with\ngreat accuracy whether rats were going to correctly identify a wide versus a\nnarrow aperture diameter, in a single trial.\n\nUsing the same aperture assessment task, Janaina Pantoja, Mike Wiest, and Eric\nThomson showed that some of the anticipatory neuronal firing in layers II/III\nand V/VI emerges during the rat’s training. Even after the animals cease to\ntouch the bars with their whiskers, S1 neurons, and to a lesser degree VPM\ncells, continue to exhibit firing patterns that represent the identity of the\ntactile stimulus for several hundred milliseconds. In fact, this sustained\nactivity lasts until the animal is rewarded. The spatiotemporal activity of\npopulations of S1 neurons even provides reliable predictive information about\nthe animal’s expectation of a reward, and thus whether it is likely to\nidentify the aperture’s diameter successfully.\n\nAfter a decade of tweaking whiskers and listening to the brains of rats, my\nresearch team at Duke was getting close to understanding how sneaky Jerry the\nmouse always escapes Tom the unlucky cat. Yet, despite all the evidence\ngathered in these experiments, we could barely entice any of the main\nproponents of the labeled-line, feedforward model of sensory processing to\nconsider that the brain is not a passive decoder of information but a dynamic\nand distributed modeler of a reality comprised of a multitude of feedback,\nlocal, modulatory, and feedforward neural pathways conjuring a vast and\nelaborate organic spatiotemporal grid. For us, the rat somatosensory system\nwas the epitome of a new paradigm of brain function, one in which an active,\never-changing, ever-adapting brain was always ready to express its own point\nof view and its expectations about the surrounding world, even before real\ninformation about this world reaches its central structures via an array of\nsensory channels. But outside the small and highly inbred community\nspecializing in studying the “rat barrel cortex,” very few thought that our\ndata mounted a credible challenge to the holy canon of neurophysiology.\n\nTo prove our point, we decided to do just as Clint Eastwood had done in the\n1982 thriller Firefox, when he managed to steal a secret plane from the Soviet\nUnion using a helmet that allowed him to think in Russian and fly the aircraft\nwithout moving a finger. Over a Philly cheesesteak dinner, John Chapin and I\ndecided that we, too, would link a brain to a machine and make that machine\nobey the brain’s voluntary motor will through the force of thinking alone.\n\nAs they say in academic circles, peer-review support is the best tonic a\nscientist can get for his ideas. Sure enough, we immediately received ours\nthat evening when a truck driver, who had been listening attentively to our\nconversation from a nearby booth, promptly gave us a thumbs-up and said: “That\nain’t a bad idea at all!”\n\n\n6\n\nFREEING AURORA’S BRAIN\n\nSitting comfortably in her favorite laboratory chair, taking a few sips of\nfruit juice, she seemed absolutely relaxed. For the past few weeks, she had\nbeen at the top of her game. And she made everyone around her aware of that.\n\nNo insecurity, no inferiority complex. That night, like on many recent nights,\nshe exuded confidence. She was the quintessential go-getter, ready to make her\nindelible mark in the world of science.\n\nDespite being relegated to sidekicks, we played along. Because above all, we\nloved Aurora’s larger-than-life persona.\n\nShe had overcome disappointments, difficulties, and in some cases bare\ninjustice—and she was quite willing to let you know it. Like other pioneers\nbefore her, she had endured these trials with a love for adventure and\ndiscovery. Tonight, she would reach the pinnacle of her scientific career.\n\nIndeed, in a mysterious and strange way, Aurora had become one of us. A\ncoinvestigator, decisively contributing to research that was pushing beyond\nthe limits of our understanding of the brain. She was a member of a team\nassembled to demonstrate what, just a few months earlier, had seemed\nimpossible. Yet, no one involved in the often cruel business of academic\nresearch could claim that her achievement was anything but spectacular.\n\nNot at all.\n\nAn unproven middle-aged worker who had failed at most everything she had\npreviously tried in life, Aurora had been forced to jump-start her career. She\nhad proved her scientific mettle through her own hard, meticulous, and\nsometimes dull laboratory work. No free lunches. No big break. No free ride\njust because she was cute.\n\nWhich she was, by the way.\n\nAnd a serious flirt too, I might add.\n\nBy a long shot Aurora had become the golden girl of the lab. Indeed, many of\nher male peers seemed pretty jealous of the VIP status she enjoyed. But life\nwas not always easy for Aurora. At the end of her initial training, she had\nnot had much to show for her labors. She had no scientific papers to gloat\nover, no good data to publish. Her citation index was, mildly put,\nunmentionable. Her funding track record was, well, next to nil. That wasn’t so\nuncommon. But to her total dismay and dejection, she had been denied a full-\ntime job in a prestigious federal research institution, located in the suburbs\nof Washington, D.C. In the terminology used by that prominent institution, she\nwas known simply as a “Reject.” In their report, they said that she was\nhopelessly opinionated, overambitious, and too creative for her own good.\n\nIn fairness, she had not been easy to mentor. Every experienced scientist who\ntried to guide her to some task that needed to be investigated quickly\nrealized that Aurora held very high standards for what work she considered\nworth her attention. For no clear reason, nearly every scientific project\noffered to her by senior scientists was deemed totally unsuitable. If she\nthought your idea was bad, she wouldn’t work on it.\n\nIt was a problem that Aurora was a bit narcissistic. She had done more than\nher share of waiting. A slow learner, she fell behind her peers’ performance\nand often had a tough time recovering from her mistakes. While she was\nstruggling, many of her peers (males, I should point out) were being labeled\n“star performers.” They seemed almost disdainful of her. Some even bet, behind\nher back, that she would never make it.\n\nShe was so tough-minded that it came off as a display of a ferocious disregard\nfor authority. She could be really mean, without showing a drop of remorse in\nher face or conscience. Occasionally, out of sheer desperation, people in her\nlab would beg on their knees for her to perform accordingly. Others cried like\nchildren, to no avail. She showed no mercy and even contributed to some people\ndropping out of science outright. Or so the Aurora legend went.\n\nHer perseverance and principles paid off, though, and the memories of her\nhumbling start made her current prospects even sweeter. Looking from the top\nof her chair, with those penetrating and rather defiant black eyes, she would\nmumble something, drink her fresh orange juice, maybe take a brief nap,\noblivious to all and everyone, a free spirit with no respect for weaklings or\ntraditional behavioral neuroscientists.\n\nAbove all, Aurora was about action. Once in the lab, she unleashed pure,\nmassive amounts of adrenaline. She wanted to perform every test, every game,\nall the time, and super fast. Forget about absurdly complex or boringly\nrepetitive oculomotor tasks. She wanted experiments that provoked novelty,\nexcitement, risk, and intensity. As later events proved, even a “Reject” like\nAurora, given another chance and a nurturing environment, can hope to stamp a\nmajor contribution into the big book of science.\n\n* * *\n\nOne day, during a phone conversation with a great friend of mine, who happens\nto work in a reputable federal research institution, I mentioned, casually,\nthat I was looking for a collaborator on a new project.\n\nHe immediately and enthusiastically offered to send Aurora to work for me.\n\nThat was very nice of him, I thought.\n\nMy friend, however, did not elaborate too much on Aurora’s past record of\nachievements in his lab.\n\nI have to admit that my first meeting with her was neither auspicious nor\npleasant. She displayed a very arrogant demeanor. No camaraderie was shared\nbetween us. In fact, I came away from that meeting with the impression that\nalthough she was supposed to be a pretty reasonable worker, I had better not\ntry to impose my way of doing things on her. Her scrutinizing stare implied\nthat we could be colleagues, maybe friends, if I let her be herself.\n\nSo I did exactly that.\n\nDuring her first few months in the lab, she simply abhorred befriending\npostdoctoral fellows, technicians, graduate students, even the cleaning staff.\nAnyone who tried to talk her into psychological nonsense and modern techniques\nof behavioral training was condemned to scorn. She was not about to give up\nher long-held beliefs unless the payoff was substantial, and sweet—very sweet.\nAurora, as we later discovered, was really addicted to fruit juice. In\nparticular, she would do anything you asked if a pitcher of Brazilian orange\njuice was set down next to her.\n\nThen, something quite unexpected happened. Out of nowhere, one night in the\nfall of 2001, Aurora decided we were worth her attention and collaboration.\nShe even began to simulate an easy half-smile around the graduate students.\n\nTrue, she could still be touchy. As she was throwing one of her renowned\ntantrums, she once tried to scratch a coworker. Fortunately, she did not\nsucceed. Some of us were shocked, but to be honest, most of us knew that if\nyou messed with Aurora, particularly when she was having her lunch, you did so\nat your own peril.\n\nDuring a severe admonishing session, in which she seemed not to pay a second\nof attention, I told her this type of behavior would not be tolerated. It\nnever happened again. At least not in front of any of us. You see, Aurora\ncould be sneaky, too.\n\nStill, within months, Aurora had been amazingly transformed into the best\nperformer at any task I asked of her. Night after night, she was a full member\nof our team in the little bunker of a lab at Duke.\n\n* * *\n\nMy own motivation for accomplishing our research program surged with the\narrival of unexpected news from Brazil: Dr. César Timo-Iaria had been\ndiagnosed with a terrible neurological disorder, amyotrophic lateral sclerosis\n(ALS), a disease that would claim his life four years later.\n\nFor most of us, it is virtually impossible to imagine how terrifying it would\nbe to lose control over our body movements, a muscle at a time, through an\ninexorable process of wasting that culminates with the failure of our most\nresistant musculature, the one that allows us to breathe. That is exactly the\ndestiny of patients who suffer from ALS, which is best known to the public for\nits devastating progression in the life of the beloved Yankees slugger Lou\nGehrig.\n\nIn one of those incredible ironies that only life can stage, Timo-Iaria had\nbegun his career working on a new method to diagnose ALS. In the late 1950s,\nas a postdoctoral fellow in New York City, he became one of the first\nneurophysiologists to observe that patients suffering from ALS started to\nexhibit a continuous decrease in the conduction velocity of peripheral nerves.\nThat meant that as the disease progressed, it took more and more time for an\nelectrical impulse to travel through a nerve and activate a muscle. Forty\nyears later, Timo-Iaria, then an emeritus professor of physiology at the\nUniversity of São Paulo Medical School, calmly informed me that his diagnosis\nhad been confirmed using a modern variation of the test he had perfected as a\nyoung scientist.\n\nThroughout his last years, Timo-Iaria followed with great interest the\nexperiments we were conducting at Duke. The primary reason for his interest,\nhowever, was not the possibility that he could benefit from what we were\ndoing. Being a very experienced and accomplished neurophysiologist, he knew\nthat there would be no time to translate our discoveries in time for him. But\nhe was thinking of the possibilities for future patients and the impact these\nexperiments could have in the field of neuroscience.\n\n* * *\n\nA couple of years before I met Aurora, John Chapin and I had decided to create\na real-time platform to demonstrate that populations of neurons, rather than a\nsingle brain cell, should be considered the true functional unit of the\ncentral nervous system. Because most of our past work had been conducted in\nthe rat somatosensory system, several of our colleagues in neurophysiology had\nopenly challenged whether the neural coding scheme we had proposed, known as\ndistributed coding, was actually significant when animals were sustaining\nmeaningful behaviors, such as moving around or identifying objects in their\nsurrounding environment. Rats, after all, do not have the habit of talking\nback to the scientists training them, so we had no way to verify what our\nrodent subjects were feeling, in terms of tactile experience, as\nspatiotemporal waves of electrical activation spread across their cortices\nwhen one of their facial whiskers was deflected. In fact, some skeptics argued\nthat the spatiotemporal complexity of both the single-neuron receptive fields\nand the somatotopic maps we had identified in the cortical and subcortical\nrelays of the trigeminal pathway were pretty much meaningless when it came to\nreal rat behaviors. From a perceptual point of view, they said, the rat brain\ncould basically ignore all this undesirable complexity simply by taking into\naccount the strongest tactile responses of a very small number of cortical\nneurons to decide what type of tactile message a whisker stimulus conveyed. In\nthis process, known as “response thresholding,” an arbitrarily high activity\nthreshold would be set by the brain to consider a given neuronal response as\nsignificant for eliciting a perceptual experience. In the rat trigeminal\nsystem, this threshold could be set so that only the high-magnitude firing and\nshort-latency responses elicited by the stimulation of the principal whisker\nof a given neuron would be taken into account by the brain to build a tactile\nimage of the external world. Accordingly, the smaller, long-latency responses\nof the dynamic “surrounding” components of the neuron’s RFs and the maps\nwould, rather conveniently, be filtered out and eliminated as the\nsomatosensory system interpreted tactile sensory information.\n\nThat was one quick and sanitized way to dispense with the headaches set off by\nour findings.\n\nYet, despite finding this elegant solution for handling our “troubling” data,\nour colleagues were a bit more vague about how the brain would actually\ndetermine the level of this threshold and how networks of neurons would know\nthe difference between so-called useful action potentials and those that\nshould be filtered out. Why, we asked, should such rich complex neuronal\ndynamics be considered a problem, a villain, something that should be swiftly\neliminated from the cortex (as well as from our theories)? By proposing that a\nresponse thresholding took place across the rat somatosensory system, our\ncolleagues were in essence removing the temporal dimension—the fourth\ndimension—from tactile responses and somatotopic maps. It was as if a dynamic\nmammalian brain was too undesirable a nuisance to enter into their models of\nhow touch sensations emerge. Only static spatial relationships could, in their\nview, give rise to tactile perception. In their model, the sensation of touch\nsolely originated at the skin’s epithelium and then through parallel\nfeedforward labeled lines extended, more or less completely segregated,\nthrough the stack of multiple, distorted topographic maps all the way to layer\nIV of the S1 cortex, where it blossomed into a precious pattern of electrical\ninteractions between neurons that reproduced the spatial nature of the\nperipheral tactile stimulus. These neuroscientists were still following the\ndogma established by Vernon Mountcastle in his studies in the 1950s: spatial\norder, not temporal chaos, was the stuff from which the sense of touch\nemerged. So, if you passed your fingertip over an embossed letter A, your\ncortical neurons would precisely reproduce the spatial organization of A\ninside your head. According to this view, time played no role in the brain’s\nrepresentations of the external world.\n\nTo prove that a completely antagonistic theory was closer to reality, Chapin\nand I had to demonstrate that populations of neurons, working together as part\nof widespread neural circuits, could encode enough information, using dynamic\nspatiotemporal patterns of activity, to sustain a motor behavior. We could no\nlonger straightforwardly observe and quantify the physiological properties of\nthe individual cells we happened to record while our freely behaving animals\ncarried out a particular motor task. Although this was the classical way in\nwhich virtually all cortical physiologists, including those who defended the\nrelevance of population coding schemes in the motor cortex, operated, our\nexperimental strategy had to be reinvented. To convince our colleagues, we\nwould have to introduce a completely new experimental paradigm to the field of\nmotor cortical physiology, which is how we came up with the idea of the brain-\nmachine interface (see [Fig. 6.1](part0010.html#fig6-1)).\n\nIn our original conception, chronic, multisite, multi-electrode recordings\nwould be used to sample simultaneously the activity in as many cortical\nneurons as possible in an individual animal subject. Our chronic implants\nallowed us to maintain viable recordings of single neuronal activity for weeks\nto months, depending on the animal species utilized in a given experiment. All\nwe needed to do was record populations of neurons, spatially distributed\nacross multiple cortical areas in the frontal and parietal areas, while our\nanimals learned to perform clear and easily quantified movements of their\nlimbs in a well-controlled behavioral task.\n\n![image](../images/00024.jpeg)\n\nFIGURE 6.1 The debut of the brain-machine interface. Systems engineering\ndrawing depicting the general organization of a brain-machine interface.\nMulti-electrode arrays and microchips are used to record large-scale brain\nactivity. Signal processing techniques are then used to translate raw brain\nactivity into digital commands that can be employed to reproduce, in a robotic\narm, the voluntary motor intentions generated by the brain. Visual, tactile,\nand proprioceptive feedback signals from the robotic actuator are then sent\nback to the subject’s brain. (Originally published in M.A.L. Nicolelis and M.\nA. Lebedev, “Principles of Neural Ensemble Physiology Underlying the Operation\nof Brain-Machine Interfaces.” Nature Reviews Neuroscience 10 [2009]: 530–40.)\n\nMoreover, instead of simply measuring the physiological properties of each of\nthese neurons, we came up with a different approach altogether. During each of\nour experiments, after amplifying and filtering the individual action\npotentials generated by the cortical neurons we were monitoring, our\nmultichannel recording system would start to send these neuronal signals to a\nmicrocomputer. There, the continuous stream of neuronal data would be fed, as\nclose to real time as physically possible, into a series of simple\nmathematical algorithms designed to optimally combine the spatiotemporal\npatterns of neuronal activity so that one could extract, from the overall\nneuronal population activity, the type of motor control programs that were\nnormally used by the subject’s brain to generate limb and hand movements for a\nparticular task. Although there are tens of millions of neurons in a monkey’s\nprimary motor cortex alone, our technology limited us to sampling only about a\nhundred neurons from one cortical region at a time. That meant that our first\nBMI would be driven by something close to 0.000001 percent of the overall\nneuronal population available to the motor cortex.\n\nFor animals to operate a BMI consistently, the neuronal activity had to be\ntranslated into digital control signals very quickly—in this case, in no more\nthan two hundred to three hundred milliseconds. This narrow timing window was\nnot accidental. We wanted to reproduce limb movement in an artificial device\nunder the same strict limits that governed an animal’s typical reaction time,\nand in rats as well as monkeys it takes about that much time for the brain to\ngenerate a motor plan and enact it through the movement of the limbs.\nInterestingly, we would later learn that if the time for executing a BMI’s\noperations was much longer than two hundred to three hundred milliseconds, the\nsubject animal promptly became uncooperative and, in most cases, gave up on\nthe experiment. Assuming the continuous outputs of our simple mathematical\nmodels carried enough information about the key motor parameters involved in\nthe generation of limb movements, we predicted that a fully functional BMI\ncould efficiently transfer these digital brain signals to a robotic device—for\nexample, a prosthetic arm—and make every inch of its metal, plastic, and wire\nfulfill the ultimate dream of machines: to suddenly, and almost miraculously,\nmorph into a living and feeling clump of purposeful living flesh, capable of\nfaithfully carrying out the desires of its newly assigned master, an animal’s\nbrain.\n\nBut there was more to our BMI than simply making a limb move. To make its\nbiological master aware of its loyal performance, the prosthetic limb would,\nat each moment in time, reciprocate its continuous blessing by sending sensory\nsignals back to the brain. Given the technical difficulties at the time of\naccurately mimicking tactile feedback from a prosthetic limb to a rat or a\nmonkey brain, we opted to restrict ourselves to visual feedback information\ndelivered to the brain by including the robotic arm, or the outcome of its\nmovements, in the animal’s visual field. Thus, every time the robotic arm\nmoved under the command of the animal’s brain signals, the subject would be\nable to evaluate directly, through visual information, how well the BMI was\noperating.\n\nThe challenges facing us were enormous. First, we had to get access to enough\ngood neurons from our chronic brain implants. Since there was no possible way\nto record the entire population of neurons in the primary motor cortex (M1),\nwe had to rely on a relatively small sample of neurons to demonstrate our\nthesis that ensembles of neurons rather than single cells define the true\nfunctional unit of the brain. Second, our neuronal recordings had to last long\nenough for our animals to learn and master the motor tasks we designed to test\nthe operation of our BMIs. Third, no one had ever described a computational\nalgorithm robust enough to extract multiple motor commands out of raw brain\nactivity in real time, or efficient enough to run on a budget-conscious Dell\nworkstation, the only computer hardware we could then afford. Finally, no one\nknew how animals would react to the scene of having an artificial limb\nperforming a task that they had been supposedly trained to complete using\ntheir own biological limbs.\n\n* * *\n\nSurprisingly, despite the reviewer’s pugnacious reaction to our proposal, NIH\nactually funded the original BMI experiments that John Chapin and I outlined.\nWith some seed money secured through this small research contract, by 1997 we\nwere able to carry out the first experiments that allowed us to measure the\nperformance of a few rats operating a true brain-machine interface. Right off\nthe bat, our BMI design included a complete closed-control loop apparatus,\nwhich meant that the BMI utilized brain-derived signals to control the one-\ndimensional movements of a basic artificial device while permitting the rat to\nhave continuous access to the device’s performance through information gleaned\nin its visual field.\n\nOver several weeks, Chapin and his team trained six rats, a tediously slow\nprocess that required all the patience a neurophysiologist could muster.\nFirst, the rats had to learn to press a bar with one of their forepaws, and\nnot with their furry butts, the preferred, if unorthodox, method usually\nchosen by most rodents when faced with solving this particular behavioral\nriddle. Once the rats had figured out how to make the forepaw movements we\nwanted them to use to press the bar, they then had to learn how to repeat\nthese movements over long periods of time—persevering for several minutes per\nrecording session, in order to give us a reasonable amount of data to feed\nthrough our computers into the BMI.\n\nIn this setup, rats pressed a bar that was electronically connected to a metal\nlever equipped with a tiny cup. If the rat’s forepaw pressing was delicate\nenough, the lever moved so that the cup was positioned under a water-dripping\ntube. By holding the cup in that position for a second or so, the rat could\nuse the lever to collect a drop of nice, cool Philadelphia tap water. Then, by\nslowly releasing the pressure applied by its forepaw on the bar, it could make\nthe lever bring the cup all the way back to its mouth so that it could enjoy a\ndrink. Once a rat mastered this simple motor task, a microwire array was\nimplanted in its M1 so that we could harvest the neuronal electrical activity\nfor the BMI. Our next goal was to get the rat to repeat the whole operation of\nreceiving its drop of water, but now by moving the lever controlled by the BMI\ninstead of its forepaws. To do so, the rat had to use its brain activity to\ncontrol the lever’s movements and deliver some refreshing water into its\nmouth.\n\nIt was then that we entered what could only be described as the “twilight\nzone” of neurophysiology, a place where the main question hovered in the air:\ncould a rat grasp the concept of having to rely on thinking alone to get that\ndrop of water, literally without moving a whisker?\n\nFollowing a couple of weeks of postsurgical recovery, Chapin’s rats returned\nto the experimental setup and were prepared, over a few days, for the first\ntest of the BMI. Careful inspection of each of the brain implants revealed, to\nour total delight, that in each of the animals we could identify up to forty-\nsix motor cortical neurons firing robustly during the execution of the bar-\npressing task. By monitoring the simultaneous activity of these neurons while\nthe animals moved their forelimbs to press the bar, we soon realized that most\nof these neurons exhibited “premovement activity”—the preparations made by the\nM1 cortex in the two hundred to three hundred milliseconds prior to producing\na body movement. This meant that we would be able to record the high-quality\nbrain activity necessary to make a BMI work.\n\nNow, as the rat commanded its forelimb to press the bar, Chapin recorded the\naction potentials produced by the microwire array’s sample of motor cortical\nneurons. A parallel array of electrical resistors had been assembled into an\nintegrator board so that the contribution of each individual neuron could be\nappropriately weighted—and given the computer’s processing power, this had to\nbe done manually during each recording session. These weighted contributions\nwere then summed up to generate a single, continuous analog “motor control\nsignal” output, which would create a brain-derived signal capable of\npredicting the rat’s voluntary forepaw movements. By feeding this motor signal\nto a lever controller, we could then move the metal lever, which for all\npurposes was now capable of reproducing, in real time, the voluntary motor\nintentions of the rat’s brain.\n\nA few days after the rats started to show signs that they were able to\ntransition back and forth between either using their forepaws or their brains\nto control the water-carrying lever, Chapin decided to play the ultimate trick\non his furry friends: he disconnected the bar from the lever. At that point,\nthe rats would press the bar, but the lever would remain still. Frustrated,\nthe rats would press the bar repeatedly, but to no avail. The lever would no\nlonger move. It was then that something almost unimaginable took place.\n\nAs Chapin turned on the BMI that allowed the rat brain activity to feed\ndirectly to the lever, the rats reacted to the sudden introduction of hope\ninto their predicament as any of us would: they tried very hard to find a way\nto move the lever, not by pressing the bar with their forepaws, but just by\nthinking about doing it.\n\nInitially, most of these movements were cautious, and the rats were unable to\nreach that rewarding drop of water. Yet, a few of their attempts succeeded,\nand the more a rat was able to drink via this most unlikely water delivery\nsystem, the more it realized that it could reproduce a complete lever movement\nusing its brain activity alone. Although none of the rats knew what was going\non, they were producing spatiotemporal patterns of neuronal firing activity\nthat were somewhat similar to those generated when they used their forepaws to\ncontrol an intact bar-lever system. After a few minutes of interacting with\nthe BMI apparatus, most of the rats stopped pressing the bar with their\nforepaws altogether. Through trial and error, the animals discovered that if\nthey just looked at the bar and, we posited, imagined the movements needed for\ntheir forepaws to press it, they could somehow get all the water they wanted.\nOf course, the four rats that succeeded had no idea of the trouble we—and most\nespecially John Chapin—had experienced to interpret and enact their thirsty\nmotor intentions. What they appreciated was that they had become the first\nrats in their colony to get free drinks every time they were placed in the\nlab’s experimental setup.\n\n* * *\n\nAurora owed a great deal of her scientific success to an owl monkey named\nBelle, who three years earlier ushered primates into the age of brain-\nactuating technology.\n\nBelle was a video game junkie, just like Aurora. Over several months, she had\nmastered a game that involved using her right hand to grasp a joystick while\nshe watched a horizontal series of lights flash in front of her on a plain\ndisplay panel. During her training, she had quickly figured out that if a\nlight suddenly flashed on the screen, and she moved the joystick left or right\nin the direction of the light, a solenoid valve would open and a drop of fruit\njuice would splash into her mouth. More than any other of our owl monkeys,\nBelle loved to play this game. It seemed she was a juice junkie, too.\n\nWhile Belle played her game, she wore a cap, glued to the top of her head with\na smooth layer of surgical cement, a material normally used to repair the loss\nof skull bone in patients. Under the cap there were four plastic connectors,\neach of which received signals from rectangular arrays of Teflon-coated metal\nmicrowires implanted a couple of millimeters deep into different areas of\nBelle’s frontal and parietal lobes. The cortical areas chosen for those\nimplants were known to be involved in the type of visual-motor planning\nrequired for primates to translate visual cues, such as flashing lights, into\nthe hand movements needed to move a joystick toward a target. A couple of\nthese implants were located in Belle’s M1 and other motor cortical areas.\nTheir combined signals amounted to a small sample of the detailed motor\nprogram needed by Belle’s arm and hand muscles to produce the elaborate\nmovements that her brain had conceived in response to her video game–juice\nchallenge. As she reflected on how to perform her motor tricks, the implants\ntook a privileged glimpse of the sweeping electrical brainstorms that spread\nacross her cortex as action was, spark by spark, forged out of abstract\nthinking.\n\nTo ensure a clear recording, each of the blunt metal tips of the microwires\nwas nicely laid into the slightly salty, fluid-filled extracellular space that\nsurrounded Belle’s delicate cortical neurons. Strategically positioned, these\nnaked sensors listened attentively, like a respectful and privileged\nconfessor, to the brief electrical murmurs produced by her neurons.\n\nInside Belle’s brain, each time one of the cortical neurons located next to an\nimplanted microwire produced an action potential, a tiny amount of electrical\ncurrent would flow through the extracellular space and be detected by the\nmicroelectrode’s tip. The electrical discharges from all the implanted\nmicrowires would then feed into the microchip connected to one of the\nconnectors situated atop the microwire arrays. This microchip, which we named\nthe neurochip, contained the electronics needed for amplifying and filtering\nthe tiny electrical signals produced by Belle’s neurons.\n\nFrom each of the neurochips, a small wiring bundle ran from Belle’s head cap\nto an electronic closet next to the soundproof chamber in which she was\nplaying the video game. The electronic closet, in turn, was linked to a master\nmicrocomputer that was responsible for translating Belle’s thoughts into a\nstream of digital signals that would control the movements of two robot arms,\nfollowing the voluntary motor intentions produced by Belle’s brain.\n\nBut how could that translation of raw brain activity into digital motor\nsignals be achieved? Although many in those early days thought that this\nhurdle would be the most difficult one to overcome, it turned out that the\nanswer to this problem was simpler than one might have thought. I vividly\nrecall the day that Johan Wessberg, a Swedish neurophysiologist from Göteborg\nwho at that time was conducting his postdoctoral research in my lab at Duke,\nwalked into my office and told me, in his calm but confident manner, that he\nhad figured out how to implement a real-time computational algorithm for our\nBMI. His audacious insight, which undoubtedly opened the path to BMIs, came\nwhile he was playing around with some neuronal data, the sort that we\ncollected day in and day out. After some careful analysis, Wessberg discovered\nthat if he used a relatively simple algorithm, known to statisticians as\nmultivariate linear regression and to engineers as the Wiener filter, to sum\nlinearly the electrical activity produced by the simultaneously recorded\ncortical neurons, he could generate a surprisingly accurate prediction of\nBelle’s hand position. The algorithm would be able to identify an optimal\nmeans for weighing the contribution of electrical activity from each of the\nrecorded neurons. Then it would sum these weighted contributions to generate a\ncontinuous motor output signal that could be used to reconstruct the\ntrajectory of Belle’s wrist in a robotic device (see [Fig.\n6.2](part0010.html#fig6-2)). It was a huge step forward from John Chapin’s\nmanual calculations.\n\nFirst and foremost, Wessberg had to come up with a way to obtain a good\nestimate of the free arm movements he was trying to predict from monkey brain\nactivity alone. Since we were using owl monkeys—small primates with delicate\nlittle arms—that requirement posed severe constraints on the type of\ntechnology that could be utilized to achieve precise kinematic measurements.\nHe overcame this obstacle by adapting a device known as the shape-tape, a\nflat, narrow, and very flexible plastic tape containing fiber-optic sensors,\nto the dimensions of an owl monkey arm. By strapping the tape along the\nmonkey’s forearm and wrist, the sensors embedded in this device could detect\nthe monkey’s arm movements; every time a monkey made a given movement, the\nflexible tape attached to the arm would bend somewhat proportionally. By\ncontinuously recording the bends of the tape, Wessberg could reconstruct the\ntrajectory of Belle’s wrist as she performed her motor tasks.\n\n![image](../images/00025.jpeg)\n\nFIGURE 6.2 General algorithm for translating raw neuronal electrical activity\ninto digital commands that can be employed to predict kinematic parameters and\ncontrol artificial tools based on brain activity alone (see text for details).\n(Illustrated by Dr. Nathan Fitzsimmons, Duke University.)\n\nHaving solved this first big problem, Wessberg pressed on to investigate the\nbest computational strategy for combining the dynamic activity of a population\nof cortical neurons. He began by considering how the primate brain solves\nproblems computationally to make a muscle move. His intuition was that we had\nto be able to handle the signals that initiate a voluntary motor command a few\nhundred milliseconds before any body muscle moves. Once a motor plan is\ngenerated by the brain, the large cortical pyramidal neurons transmit the\ncommands to sizable pools of neurons in the spinal cord via their long, thick\naxons, which are highly myelinated and hence pretty fast conductors of\nelectrical activity. The final product generated by the spinal cord neurons\nafter they have received this electrical bombardment from the cortex is the\nequivalent of a final, ready-for-execution motor script.\n\nInspired by biology’s highly efficient strategy for generating voluntary\nmovements, Wessberg created an algorithmic analog for transforming thoughts\ninto actions. To predict the continuous spatial trajectory of Belle’s wrist as\nshe moved her arm, he analyzed the electrical activity generated by each of\nthe one hundred cortical neurons we were recording, going as far back as a\nfull second before the movement started. He then divided this one-second\nperiod into ten consecutive segments of one hundred milliseconds. For each\ncortical neuron, he counted the number of action potentials the cell generated\nin each of these segments, also called “time bins.” At the end of this\nprocess, he had a temporary database of ten bins per cortical neuron recorded,\neach of which contained the number of spikes produced by the neuron in a\nparticular moment in time prior to the onset of Belle’s arm movement. So, as\nBelle moved her arm over the course of a few minutes, Wessberg recorded her\nwrist’s position, looking backward in time into his neuronal database every\ntime a movement started. This component of his algorithm is known as “data\nbinning.”\n\nThe time bins were then combined into two large data sets, also known as time\nseries: one containing a time-varying trajectory of Belle’s wrist in three-\ndimensional space and the other containing the “binned” cortical electrical\nactivity produced one second prior to Belle’s arm reaching that particular\nposition. These data sets were used to measure the linear correlation between\nthe cortical neuronal firing produced by Belle’s brain and her arm\ntrajectories.\n\nThis linear correlation turned out to be highly statistically significant.\nAfter receiving the time series data, the Wiener filter returned a large\nnumber of optimal regression coefficients, each of which related to one of the\nten bins describing the firing activity of a given neuron. The value of each\nof these coefficients directly reflected the relevance of a given bin of past\nneuronal firing in predicting the future position of Belle’s wrist. Thus, if\nthe regression coefficient was a high number, the neuronal firing contained in\nthat one-hundred-millisecond bin was highly predictive of Belle’s wrist\nposition in the future. Conversely, if the coefficient was very small and\nclose to zero, the firing activity contained in that bin had no predictive\npower whatsoever and could be dropped from the calculations. Moreover, if the\nregression coefficient was positive, the firing rate produced by a neuron at a\nmoment in the past was directly correlated with changes in arm position in the\nfuture. If the regression coefficient was negative, the rate was inversely\ncorrelated with arm position. Using a large number of regression coefficients,\nwe could derive a multivariate linear equation to describe the level of\ncorrelation between our sampled neuronal population firing and Belle’s arm\nmovement. We could then train the algorithm to linearly convert the time-\nvarying activity of a sample of cortical neurons into a time-varying motor\ntrajectory. Furthermore, after long training sessions, the coefficient values\nbecame stable. They seemed to reach an optimal level of performance. For this\nreason, the Wiener filter could, to some degree, artificially reproduce the\nkind of complex neurophysiological task normally performed by the spinal cord\n(see [Fig. 6.3](part0010.html#fig6-3)).\n\nBut Wessberg was not done. In an attempt to find the limits of his clever new\ncomputational technique, he decided to investigate whether running multiple\nversions of his algorithm, fed by spatiotemporal data from the same pool of\nneurons, could generate simultaneous predictions of multiple motor behaviors,\nsuch as the time-varying positions in three-dimensional space of the wrist,\nelbow, and shoulder. Against all odds, it worked. The only caveat was that,\nfor each kinematic parameter it needed to predict, the Wiener filter had to\nproduce a different set of regression coefficients. Still, this was a\nspectacular finding. Simply by mixing the patterns of cortical activity in\nslightly different ways using distinct weighted linear sums, Wessberg could\ngenerate multiple, simultaneous kinematic signals. There was no doubt that\nBelle’s cortex was capable of some serious multitasking.\n\n![image](../images/00026.jpeg)\n\nFIGURE 6.3 Where will Belle’s wrist go? The graphs illustrate real-time\npredictions, derived from Belle’s and Carmen’s brain activity, and how well\nthey reproduce the actual position of these two owl monkeys’ hands. The bottom\npanel shows that the same kinematic predictions can be used to control\nsimultaneously a robot arm located next to the animals at Duke University or\nremotely, at MIT. (Originally published in J. Wessberg, C. R. Stambaugh, J. D.\nKralik, P. D. Beck, J. K. Chapin, J. Kim, S. J. Biggs, M. A. Srinivasan, and\nM.A.L. Nicolelis, “Real-Time Prediction of Hand Trajectory by Ensembles of\nCortical Neurons in Primates.” Nature 408 [2000]: 361–65.)\n\nThe next question was whether this linear computational strategy would command\na robot arm to move in the skillful, biological manner of Belle’s monkey arm.\nTo find out, we had to apply Wessberg’s optimal and stable regression\ncoefficients for one thousand time bins to the algorithm. First, the algorithm\nwould multiply each bin by its corresponding regression coefficient, which had\nbeen calculated during the training phase. Once all the necessary products had\nbeen calculated, the algorithm would sum them up, plus a fixed constant, to\ngenerate the predicted value for a particular movement in time. The operation\nwould be repeated continuously, for each subsequent time period. If the\nalgorithms could keep up with the data, a master computer at our lab would\ndisseminate the motor control signals derived from Belle’s brain to two\ncomputers, one located in a room next to our lab at Duke and the other at MIT\nin Cambridge. Each of these computers was in charge of delivering digital\ncommands to a multijointed robot arm. As with John Chapin’s rat experiments,\nwe needed to translate Belle’s neuronal activity into robot commands in just\ntwo to three hundred milliseconds—the natural temporal delay that normally\nseparated the emergence of motor signals in the cortex and the instant in\nwhich her limb started to move.\n\nAfter several months of grueling work, finally it was showtime.\n\nAs Wessberg turned on the experimental setup and lights started to flash\nrandomly, one at a time, in front of her, Belle immediately started to move\nher joystick back and forth. For the next thirty minutes, which actually felt\nlike the closest thing to the infinite that I have experienced, the master\ncomputer busily cranked out preliminary sets of linear regression coefficients\nuntil the algorithm’s subroutines indicated that an optimal set had been\nobtained. At that point, with the coefficients changing very little, we were\nready to turn on the BMI.\n\nFor a few nerve-racking, nail-biting, soul-searching moments, nothing out of\nthe ordinary happened. Belle continued to play her game. Fruit juice continued\nto drop into her mouth. And the most profound silence I had heard in my life\nfilled the room.\n\nWithout any warning, the delicate robot arm at Duke started to move its metal\njoints and rubber tendons. On the main computer’s screen, we started to see\ntwo bright lines being plotted simultaneously. A red one tracked the gentle\nmovements of Belle’s biological arm as she played away. Then, appearing out of\nnowhere, a line of Duke navy blue began to plot the trajectory of the robot\narm, which appeared to be trying as hard as it could to keep pace with its\nmore senior primate counterpart. During the first few seconds, the two lines\nwere separated by a considerable distance, suggesting that the predictions\ncoming out of the BMI were not very accurate. Then the lines started to\nconverge, until they almost completely overlapped.\n\nI turned my attention to the phone, where my colleagues in Massachusetts\nawaited my word.\n\n“Is it moving yet?” I said, puzzled that I wasn’t greeted by an excitement to\nmatch my own.\n\n“Nope. It is dead! Pretty dead! As dead as it could be.”\n\n“How could that be? Ours is moving just fine next door.”\n\n“Up here, nothing is happening. The arm is sitting there, frozen. Immobile,\nactually. Not even a jerk or a twist.”\n\nAlthough I was trying to keep my cool, I could see that around the room\ntension and uneasiness were growing steadily as everyone listened to my side\nof the conversation with the MIT team. Turning back to the phone, I searched\nfor an explanation for the trouble we were experiencing. “I have no idea what\nis going on. It should be working. Have you checked the transmission line?”\n\n“Yes, I have checked everything. I have gone through the checklist three times\nby now. It reminds me of Apollo 13. I should be saying, Durham, we have a\nproblem!”\n\nBy then, I could only come up with one last idea. “Well, usually, when things\nlike this happen at home, we start by checking whether the power switch is\non.”\n\nFor a second, it sounded like my interlocutor was no longer paying attention\nto anything I had to say. Frustrated beyond belief, he seemed instead to be\ntalking to himself.\n\n“I have reinitialized the computers and tried all but.… Now, wait a minute.\nWait just a damn minute. I have forgotten one thing. How could I not have\nchecked this?”\n\n“What? What have you forgotten?” Now it was I who could not contain the angst\nany longer.\n\n“I forgot to turn the robotic arm’s power switch on!”\n\nThe loud and synchronous screaming coming from the phone, like the sound one\nhears in a packed soccer stadium when the home team unexpectedly scores a\ngolaço—a really beautiful goal—told me that the arm in the MIT lab was finally\nmoving.\n\nAt this stage in our research, Wessberg’s algorithm could not come close to\nmimicking all of the complex physiological tasks performed by a brain’s neural\ncircuit. But it was more than enough to generate the continuous motor signal\noutputs to reproduce the gracious movements generated by the slender upper\nlimbs of our owl monkey Belle.\n\nAs our MIT colleagues began to receive the motor control signals from Belle’s\nbrain, a third line appeared on our screens. Now, it was the MIT robot arm\nthat started tracking Belle’s movements in its brief but adventurous voyage\ninto the annals of neuroscience history. Choking with the significance of the\nmoment, I could only think of what Galileo Galilei had allegedly murmured in\nhis own defense during his trial before the Italian Inquisition:\n\n“Eppur si muove”—And yet it moves.\n\n* * *\n\nBy the winter of 2002, our research team’s setup was ready for Project\nMANE—the Mother of All Neurophysiological Experiments. It was an acronym that,\nnot coincidentally, reminded me of my boyhood idol, Manoel Garrincha, the star\nof the 1958 and 1962 Brazilian soccer teams that, in winning back-to-back\nWorld Cups, set new standards of excellence for the beautiful and most popular\nsport in the world. Born with severe bone deformations that made his knees and\nlegs point and bend in different directions, Garrincha learned to take\nadvantage of this unusual conformation to create a dribbling technique—full of\nhip twists and body fakes. He taught his teammates, by example, to dance their\nway through the game, creating an exquisite ballet in which the ball became an\nextension of the player’s feet. Garrincha’s nickname, the name by which he is\nknown to all Brazilians, is Mané.\n\nThe MANE experiment was far from trivial to design, implement, and execute.\nSeveral computational tasks and engineering components, each of which\ncontained many new procedures and parts, had to work flawlessly, in real time,\nin order for the data we collected to be of use. Besides, like anything that\nhas never been tried before, it was difficult to predict the outcomes of the\nexperiment. Moreover, if not for the strong and visionary support of Alan\nRudolph, an accomplished scientist who worked as a program director at the\nU.S. Defense Advanced Research Projects Agency (DARPA) at the time, we would\nhave never have mustered the funding needed to run MANE.\n\nAfter months of assembling large pieces of hardware, generating new computer\nprograms, fixing computer glitches, and convincing Aurora to play video games,\nof all things, the time for the first complete test had arrived.\n\nAs we prepared to “fire up” the experiment, our conversation was sporadically\ninterrupted by Aurora’s voice, pestering us to hurry up. Nobody could blame\nher for being a tiny bit impatient. After weeks of intense training, several\npromising attempts to run the experiment had been recently aborted. For a\nchange, that night our computers, spread out on a large lab bench in the\nexperiment’s control room, did not crash and eagerly swallowed the gargantuan\namounts of data they were being fed.\n\nAurora was, as usual, ensconced in her favorite chair, equipped with a fancy\njoystick, a flat-panel LCD screen, and a fruit juice dispenser, so that she\ncould drink as much juice as she wanted as she went through the experiment.\nNearby, a 512-channel multineuronal acquisition processor (MNAP), the largest\nmachine of its class in the world, sat equally ready to record Aurora’s brain\nactivity. For this run, we utilized only ninety-six of its channels. The other\nmembers of the team—Jose Carmena, a hyperactive former electrical engineer and\nSpaniard who loved to operate multiple microcomputers, each fitted with two\nmonitors, and Mikhail (Misha) Lebedev, a Russian physicist turned\nneuroscientist with a soothing capacity for problem solving—were poised around\nthe bunker. Both were big soccer fans—that was a given. But Lebedev also\nbrought a particularly useful skill to the experiment: he had worked with\nAurora in her previous job and knew firsthand how difficult she could be.\n\nThings were moving steadily in the control room. While Carmena frantically\ncirculated among the computers, checking their status, I continuously observed\nAurora’s “body language” through a series of TV monitors, each of which\ndisplayed the view from a different video camera.\n\nBehind the control room, we had placed a sturdy industrial robot arm. The arm\nitself had a freedom of movement of seven degrees; the rudimentary hand at its\nend contained just two fingerlike appendages capable of grasping simple\nobjects and holding them secure. The robot lay immobile, its artificial\nshoulder and elbow partially stretched, its hand fully opened—a beautiful\npiece of machinery, almost pleading for someone to take control of its inert\njoints and motors and generate purposeful and coordinated movements, capable\nof accomplishing something meaningful.\n\nCooing gaily, Aurora—or, I should say, Aurora’s brain—prepared to unleash the\nunimaginable. In previous attempts Aurora had not shied from letting us know\nthat if the robot arm was not working, the fault was not hers. Tonight,\nhowever, she was performing without complaint. Lebedev entered the lab and,\nsliding past the cables and hardware next to Aurora’s chair, whispered\nsomething in Russian into her ear. He never told us what he said, but we\nbecame convinced that it inspired her. Staring intently at the computer screen\nin front of her, Aurora made faces at one of the cameras recording her. She\nwas having fun, we could see. Yet, her inquisitive eyes conveyed that she was\nrunning out of patience and wanted to start the experiment.\n\nAs soon as Lebedev closed the door of Aurora’s chamber, Carmena gave the “go”\nto start MANE. At that moment, the cumulative years of research and the hopes\nof thousands of severely paralyzed people who dreamed of one day regaining\nsome degree of their former mobility became deeply intertwined.\n\nWe knew very well that most of the experiment depended on the work of the\nMNAP, a few computers, and—mainly—Aurora’s very own stubborn brain. As she\ngently grabbed the joystick with her left hand and applied her intriguing and\ncharming voluntary motor intentions to her favorite video game, our attention\nwas glued to the sleek, swift blinking matrix on the computer screen that\nrepresented the flow of electrical activity produced by the population of\nninety-six cortical neurons we were recording. I had also rigged up a\nloudspeaker in the control room, so that I could listen to her brain symphony,\nwhich sounded something like a violent thunderstorm of sheer beauty as it\ncrisscrosses a soft tropical summer night sky. Lost in a contemplative awe\nakin to the amazement one unexpectedly encounters in the presence of a miracle\ncreated by nature, we were gripped by the intimacy and revelation that Aurora\nhad so generously granted to us.\n\nThe sample of Aurora’s motor thinking was obtained simultaneously from\nmultiple cortical areas of her brain. In the previous three decades, studies\ncarried out by many laboratories had identified which of the cortical areas in\nthe frontal and parietal lobes are involved in the generation of the type of\nneural motor plans required for someone like Aurora to produce precise arm and\nhand movements. According to the rules of the experiment, Aurora needed to use\nsuch precise arm-hand movements to manipulate her joystick and control the\ntwo-dimensional trajectories of the cursor on her computer screen. By moving\nthis cursor, Aurora could intercept and grab the target object—a large, filled\ncircle—which randomly appeared in one of a variety of locations on the screen\nat the beginning of each experimental trial. Using her left arm and hand,\nAurora had learned to play the video game quite well, particularly because, if\nshe succeeded and grabbed the target in less than five seconds, she received a\nvery satisfying reward: a drop of her beloved fruit juice. Thus, every time\nAurora got the game right, a high-frequency sound, produced simultaneously\nwith the opening of the solenoid valve that delivered the juice to her mouth,\nfilled the control room (see [Fig. 6.4](part0010.html#fig6-4)).\n\nAs Aurora started to play the game, we were able to use the MNAP to record the\nspatiotemporal patterns of electrical signals produced by the sample\npopulation of neurons distributed across the six cortical areas of Aurora’s\nbrain. Each of the action potentials typically lasted about one millisecond\nand was then plotted on a computer in the control room, allowing us to monitor\nAurora’s brain activity in real time. We eagerly tracked these spatiotemporal\npatterns as they unfolded in front of us, trying to decipher the precise\nprocess that Aurora’s brain used, like a maestro, to compose its neuronal\nsymphony.\n\n![image](../images/00027.jpeg)\n\nFIGURE 6.4 Aurora loses her joystick and frees her mind. On top, the\nexperimental setup employed for Aurora to use her brain activity alone to\ncontrol the movements of a robotic arm. On the bottom left, a sample of the\nelectrical activity of 96 cortical neurons from Aurora’s brain. On the Y axis,\neach vertical bar represents an action potential produced by a single cortical\nneuron. The X axis represents time (10 seconds). In the right panel, a graphic\nrepresentation of the task Aurora performed and examples of predictions of arm\nmovements based on combined brain activity. (Originally published in J. M.\nCarmena, M. A. Lebedev, R. E. Crist, J. E. O’Doherty, D. M. Santucci, D. R.\nDimitrov, P. G. Patil, C. S. Henriquez, and M.A.L. Nicolelis, “Learning to\nControl a Brain-Machine Interface for Reaching and Grasping by Primates.”\nPublic Library of Science 1 [2003]: 193–208.)\n\nOn the monitor we could see large waves of the electrical discharges generated\nby the populations of neurons that formed Aurora’s brain circuits. That\nbrewing sea of electricity, an unstoppable flow of electrical pulses, each of\nthem meaning very little, but all together defining a vast brainstorm,\nregistered the epic of her lifetime: every motion, every sensation, every\ndream, every memory, and every sorrow, but also all the joy that made her such\na unique character. The very stuff that defines the human brain, making each\nmember of our species so unique and yet so similar. We were witnessing the\nfluid life cycle of thinking, from its rapid and unpredictable seed at birth,\nto its waving and warping rampant spread, all the way to its agonizing\ndisappearance, in the still mysterious hills and valleys of the cortical\nmantle.\n\nWe sat there for minutes, irremediably and unconditionally falling in love\nwith the small fragments of Aurora’s thoughts. Now we had the ultimate proof\nthat Aurora had joined our team. By sharing her most precious possession, her\nthinking, Aurora was not only displaying her camaraderie toward us; she was\noffering an unrivaled example of altruism by devoting her own existence, and\nthe exploration of her brain, to the future benefit of millions of people.\n\nBut we needed to get to work to ensure that solemn offering was not made in\nvain. Our first goal was to sample and record the activity of as many of\nAurora’s cortical neurons as possible while they were contributing to the\nsymphony of motor control signals involved in playing her video game and\nwinning liters of fruit juice. Then, we had to run the electrical signals\ngenerated by Aurora’s brain through a series of simple mathematical models and\nextract from these signals the motor commands required for Aurora to move her\nleft arm and hand. These included, for instance, the continuously changing\nspatial position and velocity of her wrist, elbow, and shoulder, as well as\nthe gripping force she applied to the joystick handle. The commands could also\nbe derived exclusively from linear combinations of her neuronal activity—a\nvery simple calculation, given the parameters involved.\n\nThirty minutes into the experiment, the first good news came: we realized we\ncould produce very accurate real-time predictions of the cursor movements that\nAurora needed to generate in order to reach and grab a new circular target as\nit appeared on the computer screen—just with our ninety-six-neuron sample of\nAurora’s brain activity. In other words, we could reproduce the type of\nmovements Aurora used to get her juicy reward.\n\nBut this was nothing compared to what came next.\n\nWhile the sounds rendered by the electrical discharges of Aurora’s brain\nfilled the control room, we slowly recognized that changes were occurring in\nthe firing frequency of her neurons even before she started to move her arm or\nhand. We could recognize which movement Aurora was planning to make long\nbefore she actually contracted any muscle in her arm—a few hundred\nmilliseconds before any movement happened. The brain signals that we were\nhearing contained enough of the motor plan needed for Aurora to play her video\ngame.\n\nAs Aurora’s movements became more and more precise, her performance improved,\nto the point that she was reaching and grabbing the circular target virtually\nevery time she played a new trial of the video game. And as Aurora improved,\nso did our mathematical models. By now, they were converging into an optimal\nperformance, meaning that, using only brain-derived signals as inputs, our\nmodels could generate time-varying outputs that predicted, amazingly well,\nwhat arm and hand trajectories Aurora’s brain was going to execute, before\nAurora moved a single muscle in her arm.\n\nNext, we started routing the outputs of these models to the robot arm located\nin the control room. After a few seconds of indecision—apparently even the\nrobot arm could sense the anticipation of the moment—the machine began to\ngenerate movements that mimicked those being produced by Aurora’s left arm and\nhand. Aurora’s motor thoughts were now directly controlling not only her own\narm but also the robotic one, which imitated its biological counterpart. And\njust as easily as her arm followed the commands of her brain, the robot arm\nfollowed them, too. Indeed, there was not much disparity between her voluntary\nmotor intentions and the robot arm’s performance. If anything, the robot was a\nbit faster in materializing its mistress’s desires.\n\nProgressively, the movements of the robot arm became more precise, which\npresented an opportunity to see exactly how powerful Aurora’s motor commands\nwere. Without a second thought, Lebedev went to the experimental room and\ngently removed the joystick from Aurora’s reach. After wishing her good luck,\nhe left her alone, with the game still playing on the computer screen. Next,\nCarmena switched the control over the cursor movements, from Aurora’s old\njoystick to the robot arm’s wrist, which was commanded by the BMI. That meant\nthat from now on, to move the computer cursor, intercept and grab the target,\nwin the video game, and get her rewarding drop of fruit juice, Aurora had only\none option. Neither the joystick nor her own arm movements would do the trick\nany longer. Instead, she would have to perform this complex motor task by\noperating a BMI.\n\nAs with Belle’s BMI setup, this BMI had been created to generate upper-limb\nmotor movements just by thinking. Aurora would have to imagine the arm\nmovements she wanted the robot to use to intercept the target. Her thoughts\nwould guide the robot arm, rather than her own limb, to direct the cursor to\nthe exact spot on the screen where she could grab the circle. Of course, that\ncircle, though it was right in front of her, was also a virtual object, so\nAurora would also need to imagine the arm touching a ball in space.\n\nWe had not trained Aurora to handle this part of the experiment; she had to\nfigure out how to operate the BMI on her own. Things had changed dramatically.\nAt first she looked puzzled and surprised. After hesitating, and then\ncommitting a few errors, Aurora rose to the occasion better than any\npreviously rejected and discredited scientist. Cooing to tell us that she had\nunderstood the idea, she relaxed her long, muscular arms, resting them around\nher chair. After taking aim at the screen, her attentive eyes fixed on the\ncursor, waiting for a new circular target to pop up. Back in the control room,\nwe began to hear and see what many scientists had told us would be impossible:\nas the initial salvos of a brainstorm charged across her cortex, the sound and\nflashes produced by hundreds of action potentials filled the room as Aurora’s\nmotor intentions were decoded in real time by our mathematical models. Even\nbefore Aurora could visualize the end result of her thoughts, our BMI was\ntransferring the motor commands extracted from her brain activity to the robot\narm. Just as a new target appeared on the screen, the robot arm started to\nmove, seeking in the empty space of the control room an elusive and unseen\nobject whose location had registered only within Aurora’s eyes and brain. Back\non the screen placed in front of Aurora, the computer cursor—now under the\ncontrol of the robot wrist—slid into a beautiful and intentionally curvy\ntrajectory aimed right at the target’s center. That first target, and the many\ntargets that followed, was grasped with almost human gusto by the mechanical\narm, whose graceful movements had been generated by voluntary brain activity\nalone. At long last, Aurora’s brain had been set free from the limiting\nconstraints of her biological body.\n\nAurora was now playing her video game just by thinking. No need to use her own\narms anymore. Her brain activity, free of her body and self-sufficient, was\ncarrying out, across laboratory walls, every bit of the burden generated by\nher voluntary will. But something even more stunning was unfolding right in\nfront of us. As Aurora’s neurons directly controlled the movements of the\nrobot arm, her brain began to assimilate that piece of machinery into the\nneuronal image of her body, almost as if the robot had become an extension of\nher self.\n\n* * *\n\nAurora’s achievement with the BMI demonstrated a third principle of neural\nensemble physiology, the distributed coding principle.\n\nTHE DISTRIBUTED CODING PRINCIPLE\n\nAny type of information processed by the brain involves the recruitment of\nwidely distributed populations of neurons.\n\nSince Aurora’s achievement, this principle has been validated in an extensive\nseries of experiments, carried out by many laboratories worldwide, that have\nmeasured how brain circuits process information related to perceptual, motor,\nand cognitive tasks. Essentially, by sampling populations of neurons,\ndistributed across vast areas of Aurora’s brain, we were able to\nmathematically translate the probabilistic nature of her neurons into\ndeterministic motor behavior.\n\nAs we saw in chapter 5, the flavor of distributed coding that I envision\ninvolves a dynamic understanding of spatiotemporal receptive fields and\ncortical maps. As such, the regional functional specialization of cortical\nareas, initially determined by the way the cortex was built during early\ndevelopment, is probabilistic. That also means they never commit 100 percent\nof their allegiance to any particular function. Thus, although neural circuits\nspatially distributed in the S1 cortex may have a very high probability of\nfiring in relation to a particular tactile stimulus, the probability that they\nmay also fire in relation to a stimulus from a different sensory modality is\nnot zero. Moreover, in certain contexts, particularly those involving changes\nin body constraints—such as Aurora’s loss of control over the video game by\nusing a joystick—neurons may fire robustly in response to stimuli other than\nthose to which they were originally assigned. Any change in the original body\nconstraints (e.g., blindness), modifications in experience (e.g., learning to\nplay the piano), or increase in task demands (e.g., playing in the Majors\ninstead of in triple-A ball) will readily change the shape of these\ndistributions and reallocate functions across the cortex. I propose that the\nnotion of distributed processing is a universal coding strategy employed by\nthe entire neocortex, being valid either for local subregions or across its\nwhole.\n\nBecause of this neurophysiological distributed coding, we believed far-\nreaching opportunities would immediately present themselves once we were able\nto demonstrate unequivocally that a BMI could generate movements in a robotic\ndevice controlled by brain activity. We proposed that BMIs would lead to the\ndevelopment of a new generation of neuroprosthetic devices aimed at restoring\nthe mobility of millions of patients suffering from devastating levels of body\nparalysis, including that caused by the ALS that had conquered Dr. César Timo-\nIaria’s body. In addition to providing a completely new experimental tool to\nprobe the basic mechanisms that govern the operation of brain circuits, the\nMother of All Neurophysiological Experiments had fulfilled its goal of making\nthis a credible clinical possibility to be studied.\n\nBack in the control room, the high-frequency noise of the solenoid continued\nto burst unimpeded as Aurora got drunk on an ocean of fruit juice. By the\nbright look of her face, it was clear she was savoring each millisecond of her\ntriumph over the unexpected task we had presented to her. Indeed, when we had\nfinished cheering and hugging and had calmly returned to our chairs in the\ncontrol room, I could almost swear that Aurora flashed those shiny and\nmischievous black eyes away from her computer screen to stare at us, through\none of the video cameras, and, flirtatiously, as only she could, gave us a\nwink.\n\nApparently, no one but me saw that wink. Both Lebedev and Carmena promptly\nargued that such a thing was utterly impossible. But then, everything we had\naccomplished that night had once been deemed impossible. Therefore, I would\nnot be surprised, if on detailed analysis, our videos one day reveal that\nAurora did in fact manage to pull off yet another of her rhesus monkey tricks.\n\n\n7\n\nSELF-CONTROL\n\nIn the mid-1960s, a handful of scientific reports surfaced indicating that\nhuman subjects were capable of achieving exquisite voluntary control of the\nmuscle fibers innervated by the axon of a single alpha-motor neuron located in\nthe ventral horn of the spinal cord. This launched the era of biofeedback\nresearch. The people who had participated in these studies had exhibited this\nlevel of control over Sherrington’s single “motor unit” by using visual or\nauditory feedback of the activity recorded by electrodes inserted into their\nmuscles, conveyed using either a light pulse on a monitor or a sound pulse\nfrom a speaker. After just fifteen to thirty minutes of training in these\nbiofeedback experiments, most human subjects became highly proficient. Further\ntraining allowed the same people to repress the motor unit they had earlier\nrecruited and select another one for voluntary control.\n\nAround that same time, Drs. James and Marianne Olds, then at the University of\nMichigan, found that they could artificially increase the firing rate of\nindividual neurons, located in many distinct sensory and motor cortical areas\nin animals, by stimulating the brain’s reward-pleasure system in lightly\nanesthetized rats. In their experiments, every time a single neuron they were\nrecording produced an action potential, the rat received an exogenous\nelectrical stimulus directly to a brain structure that generates powerful\nhedonic sensations, such as those associated with eating or mating.\nAccordingly, every time the monitored cortical neuron fired, the rat received\nsome near-orgasmic biofeedback. The Oldses observed that this ingenious\nreinforcement loop resulted in a significant increase in the neuron’s firing\nrate.\n\nInspired by these studies, the German-American neurophysiologist Eberhard Fetz\ndecided to apply biofeedback to his own innovative experiments with primates.\nAfter working under the renowned pain neurophysiologist Patrick Wall, and\ngraduating from MIT’s physics department, Fetz had joined the Department of\nPhysiology and Biophysics and the Regional Primate Research Center at the\nUniversity of Washington in Seattle. A promising assistant professor, he also\nlearned the new art of recording single neurons in behaving monkeys. Moreover,\nby collaborating extensively with research psychologist Dom V. Finocchio, an\nexpert in operant conditioning, Fetz gradually realized that there was yet\nanother way to investigate the physiological properties of cortical neurons.\n\nIn one of his first studies at the University of Washington, published in\nScience in 1969, Fetz used his new technique to investigate individual neurons\nin the primary motor cortex of awake rhesus monkeys. Although a few scientists\nwere dismissive of Fetz’s approach, the experiment laid the early foundation\nfor the creation of brain-machine interfaces three decades later.\n\nLike all leading primate neurophysiologists of his time, Fetz started by\nmonitoring, for a few hours each day, the extracellular electrical activity\nproduced by a single neuron using a solitary tungsten microelectrode. Once\nthat neuron was recorded, the microelectrode was slowly lowered a few hundred\nmicrons into the M1 cortex using a hydraulic microdrive. But such serial\nrecording was the only orthodox aspect of Fetz’s experimental approach. In a\nstrike of audacity and ingenuity, he decided to link the amount of food reward\nhis monkeys received to the animal’s ability to generate high levels of firing\nfrom the single cortical neuron he was recording. This meant the level of the\nanimal’s own brain activity dictated how much reward it could gain. The reward\nmay not have been as orgasmic as a microstimulation of the pleasure center,\nbut it was still pretty good: a banana-flavored food pellet.\n\nFetz’s experimental apparatus combined both operant conditioning and\nbiofeedback. After isolating a single neuron in the M1 cortex of a monkey,\nFetz recorded the action potential produced, and each time the action\npotential crossed a particular voltage threshold, a trigger mechanism produced\na voltage pulse. These voltage pulses were combined by a simple resistor\nvoltage integrator, which Fetz called the “electronic activity integrator.”\nWhen the integrator’s voltage hit a high enough total level, a feeder released\nthe food pellet near the monkey’s mouth. This created a direct link between\nneuronal firing rate and food reward.\n\nTo assist his subjects in gaining their banana-like feast, Fetz provided\nauditory and visual feedback that indirectly signaled the level at which their\nmonitored cortical neuron was firing at a given moment. After a few training\nsessions, all of the animals learned to associate high-intensity auditory\nclicks or illuminated meter movements with the imminent delivery of a\ndelicious meal. Shockingly, he had discovered that nonhuman primates could,\nlike humans, learn to voluntarily control the firing of individual M1 neurons.\n\nOn close inspection, Fetz saw that the monkeys were able to spur a burst of\nfiring in the monitored neuron for one hundred to eight hundred milliseconds,\nwhich, as he wrote in Science, “were sometimes accompanied by specific,\ncoordinated movements such as flexion of the elbow or rotation of the wrist.”\nBut only “sometimes”! Fetz emphasized in later papers that often he would\nrecord a neuron in the motor cortex for which an increase in firing rate did\nnot generate any discernible muscle contractions. More puzzling, this neuron\nwas invariably surrounded by other cells that usually did fire when a specific\nmuscle was contracted.\n\nTo uncover what was happening in those cases, in the early 1970s Fetz and Dom\nFinocchio decided to enhance the electronic activity integrator. In this\nupgraded version, the typical primate chair was rigged so that a monkey’s head\nwas restrained from moving and its left arm was embedded, in a semi-prone\nposition, in a cast molded to the shape of the arm. So that the monkey would\nstill be able to produce isometric arm-muscle contractions (generating muscle\nforce without changing the muscle length or the angle of the joint associated\nwith it), the cast was locked with the animal’s elbow fixed at 90 degrees and\nits wrist and fingers fully extended at 180 degrees. Pairs of braided\nstainless-steel electrodes, inserted in each of four arm muscles, allowed for\ncontinuous electromyograph (EMG) recordings.\n\nThe most substantial change introduced, however, was the way in which the\nelectronic integrator itself operated. Now, instead of feeding the integrator\nwith only the electrical activity of a single neuron, Fetz and Finocchio added\na few new inputs to their device: voltage pulses corresponding to the\nelectrical activity generated by each of the four individual muscles being\nrecorded. In this arrangement, the contributions of each of the inputs—whether\na single muscle or the single neuron—could be weighted. This meant that Fetz\nand Finocchio could change the relevance of the activity of a given muscle (or\nneuron) in determining the final voltage level of the integrator—and what\nelectrical activity would garner a food reward for a monkey.\n\nAfter determining that their monkeys could handle this modified apparatus,\nFetz and Finocchio investigated what happened when a single muscle, a single\nneuron, a subgroup of muscles, or even different combinations of muscle and\nneuronal activity were used as the main input source to the integrator. From\nthe start, the monkeys, which continued to receive auditory and visual\nfeedback cues, typically tried to reach the desired high voltage by\nconcurrently contracting all muscles at once. They understood the new rules of\nthe game and were trying to outsmart their primate cousins. To direct their\nsubjects’ behaviors, Fetz and Finocchio changed the feedback mechanism so that\na set of colored lights was used to indicate which of the individual muscles\nhad been contracted. This allowed the researchers to reinforce the contraction\nof a particular muscle while eliminating the other muscles from the\nintegrator’s voltage sum. The monkeys soon recognized they would only get\ntheir food pellet if they contracted the chosen muscle.\n\nAs each monkey learned to limit its voluntary motor intentions to a single\nmuscle, Fetz and Finocchio continuously recorded the activity of one of its\nsingle M1 neurons. They found that most of the single neurons in the monkey M1\ncortex fired in response to passive movements of the joints of the animal’s\ncontralateral arm in the absence of EMG activity, which meant that sensory\ninformation coming from the body periphery affected most neurons in the motor\ncortex.\n\nWhen Fetz and Finocchio started to use the apparatus to reinforce isometric\ncontractions in each of the four distinct muscles, they noticed that the\ncortical neurons they recorded exhibited some unexpected properties. For\nexample, a large number of the single neurons seemed capable of modulating\ntheir firing rate prior to or during the contraction of multiple muscles.\nIndeed, some neurons were coactivated, either with the same or with different\nfiring intensities, with all four muscles. As with the dynamic receptive\nfields of rat whiskers, Fetz and Finocchio had demonstrated that individual M1\nneurons fired before the contraction of several muscles and that the\ncorrelations between neuronal firing and muscle contraction changed\ndramatically, depending on the context and type of movements being generated.\nAs a result of these findings, Fetz later coined the term muscle field to\ndefine the set of muscles coactivated by the firing of a single cortical\nneuron. This muscle field undermined the idea that parallel “labeled lines”\nlinked individual M1 neurons to specific individual body muscles.\n\nThe most astonishing result to come out of Fetz and Finocchio’s experimental\nsetup was obtained when they used the weighting options provided by the\nelectronic activity integrator in an attempt to dissociate the observed\ncorrelations in the firing of individual neurons and individual muscles. To do\nthis, they simply reinforced high firing rates of a single cortical neuron\nwhile suppressing the muscle activity in the final integrator voltage. After\nonly a few minutes the monkeys were capable of selectively raising the firing\nrate of only the neuron, without generating any simultaneous electrical\nactivity (and contraction) in the muscle field, including the single muscle\nmost strongly correlated to that particular M1 neuron. Through selective\nreinforcement, Fetz and Finocchio had trained their monkeys to completely\ndissociate cortical neuron activity from peripheral muscle contractions.\n\nNot yet satisfied by their evidence, they next tried to obtain the opposite\neffect, by selectively reinforcing muscle contraction while suppressing the\nfiring of a single neuron. Even though the neuronal-muscle pair chosen for\nthis experiment (an M1 neuron and the biceps) exhibited a very strong\ncorrelation in firing, and despite the fact that at the time of testing their\nsubject was tired and totally satiated with banana-like pellets, Fetz and\nFinocchio reported a 10 percent suppression in neuronal firing accompanied by\na 300 percent increase in bicep activity. Despite the fact that they were\nmonitoring only one cell in the primary motor cortex at a time, and so could\nnot document the full dynamic activity of M1, they had powerfully shown that\nit truly was possible to dissociate the mind from the body’s flesh.\n\nWith this series of experiments, Fetz and Finocchio unveiled a much more\nmalleable relationship between M1 neuronal firing and muscle activity. Indeed,\nin one of those peculiar coincidences that sometimes happen in science, almost\nexactly a hundred years after the original discovery of the motor cortex by\nthe German scientists Eduard Hitzig and Gustav Fritsch—a finding that became a\ndecisive plank for the localizationists—a team led by a young German-American\nphysiologist now claimed that bursts of action potentials, produced by single\nneurons located in the same primary motor cortex, did not necessarily lead to\nmuscle contractions at the body periphery or dynamically change the control\nexerted over multiple muscles. Cortical function was not so localized and\npredetermined after all. There was plenty of room for flexibility and fine\nadjustments, even in the motor cortex.\n\n* * *\n\nWhile Eb Fetz had been perfecting the pleasurable delivery of banana-like food\npellets just by thinking, a few laboratories had been experimenting with the\npossibility of conditioning animals and people to be able to enhance rhythmic\nneuronal activity in the brain. Using a variety of operant conditioning\ntechniques similar to those utilized by Fetz and Finocchio in their monkeys,\nas well as the noninvasive scalp EEG, several neurophysiologists recorded\nrhythms in the visual and sensorimotor cortices to provide subjects with\nbiofeedback of their ongoing brain activity. Pleasant sounds, flashing lights,\nand even the projection of pleasant images were commonly used to reward humans\nwhen they produced the EEG rhythmic activity requested.\n\nUsing variations on this basic approach, researchers including Joe Kamiya at\nthe Langley Porter Neuropsychiatric Institute in San Francisco reported that\nhuman subjects could not only learn to control a specific motor unit, they\ncould learn to control the appearance of their alpha rhythm, an eight-to-\nthirteen-hertz oscillating activity that usually appears in the visual cortex\nwhen a subject closes the eyes and settles into a state of relaxation.\nSimilarly, M. Barry Sterman and his collaborators in the Department of Anatomy\nand Neurology at the University of California, Los Angeles, found that cats\nlearned to control the production of their mu rhythm, a seven-to-fourteen-\nhertz oscillating activity detected over the sensorimotor cortex when the\nanimal is idle or has ceased all limb movements, when they were reinforced to\ndo so with a reward of either food or a pleasurable direct brain stimulation.\n\nThings got so wild that one author, Edmond Dewan, said that he had become so\ngood at controlling his alpha rhythm activity that he could send Morse code\nmessages to a computer using his own EEG.\n\nAlthough at the time neither Fetz nor the researchers using EEGs could have\npredicted it, this innovative work with biofeedback split the scientific\ncommunity in a very peculiar way. Unlike the intellectual disputes over\nwhether a single neuron or a population of neurons served as the basic\nfunctional unit of thinking, this divide involved those who, like Fetz,\nutilized “invasive” intracranial recordings to obtain brain signals and those\nwho preferred to rely on “noninvasive” methods, such as the scalp EEG. The\nfundamental differences inherent in these two general approaches for sampling\nbrain activity define the almost insurmountable chasm between those who pursue\ninvasive and noninvasive brain-machine interfaces today.\n\nThe first person to formulate how biofeedback might one day blossom into the\ninvention of BMIs was not one of the pioneers of biofeedback but yet another\ncortical neurophysiologist employed by the National Institutes of Health. In a\npaper published in the Annals of Biomedical Engineering in 1980, Edward\nSchmidt presented a scientific manifesto for the creation of a new field aimed\nat, among other things, building a new generation of prosthetic devices\ndesigned to restore mobility to severely handicapped patients.\n\nUnfortunately, Schmidt’s daring proposition immediately hit two major walls:\none theoretical and the other experimental. Just a few years later, Apostolos\nGeorgopoulos and his collaborators categorically demonstrated that the firing\nof a single neuron in the motor cortex was not accurate enough to generate an\nunambiguous prediction of the direction in which a monkey intended to move its\narm. A population of neurons was needed in order to produce an accurate\nprediction of a time-varying primate arm trajectory in space. To transform\nSchmidt’s vision into clinical reality, neurophysiologists would need to\nrecord the extracellular activity of large numbers of single neurons\nsimultaneously—and there were few, if any, neurophysiologists in the mid-1980s\nwho believed that such neural ensemble recordings could be obtained in the\nforeseeable future.\n\nBy the time John Chapin’s rats in Philadelphia and our monkeys at Duke started\nto operate the sort of gizmo that Schmidt had envisioned, almost twenty years\nhad elapsed.\n\n* * *\n\nStarting in 1998, a series of scientific breakthroughs helped pave the way to\nAurora’s mastery over the robot arms. That year, the neuroscientist Philip\nKennedy and the neurosurgeon Roy Bakay, both then at Emory University,\nreported the case of a patient suffering from a variation of the “lock-in\nsyndrome,” a neurological condition in which subjects are completely paralyzed\nbut still have an intact or partially functional central nervous system, who\ncould move a computer cursor with the activity of a single cortical neuron.\nAfter some consultation, the patient had consented to an experimental device,\ncalled a cone electrode, to be implanted in his cortex. The electrode was\ndesigned to record the activity of neuronal processes that would,\ntheoretically, interweave with the electrode’s surface. Not much raw data was\ndisclosed in that original report, and subsequent experiments in animals and\nother patients did not support the cone electrode’s efficacy. Yet, it was\nclear that the time was fast approaching for the translation of basic\nneurophysiological techniques and concepts into clinical applications.\n\nIndeed, a year later enthusiasm grew when Niels Birbaumer of the Psychology\nInstitute at the University of Tübingen, Germany, reported in Nature that\nlocked-in patients had been taught to control a computer-aided spelling system\nusing their EEG rhythmic activity. Using this early “brain-computer interface”\nsystem, patients became able to write letters and e-mails—in some cases,\ncommunicating with their loved ones and the external world for the first time\nin years.\n\nShortly afterward, John Chapin, his students, and I published our study\nshowing that rats could learn to use their brain activity to directly control\na robotic lever. Although our rats rapidly learned to slake their thirst, it\nwas obvious to us that the future of BMI research lay in primate experiments.\n\nWhen we first began to plan our experiment with Belle and her owl monkey\nfriends, Johan Wessberg and I had conservatively thought that getting the\nmonkey to move a joystick to the left or to the right in response to a visual\ncue would be the most we could expect to achieve. During those early wobbly\ndays of BMI research, we, along with most of the other handful of neuroscience\ngroups trying to make their way into this emerging new field, would have been\nthrilled beyond belief if someone could have assured us that some species of\nmonkeys, let alone our cute little owl monkeys, would get this far in their\nfirst shot at controlling an upper-limb BMI apparatus. So Belle’s stellar\nperformance in her first contact with our BMI immediately raised the feeling\nin all of us that she could handle something trickier.\n\nThis impression led Wessberg to test whether our newly unveiled BMI-controlled\nrobot arm could reproduce the type of free arm movements that monkeys use in\nthe “real” world. At first glance, this sounded like quite a challenge for a\ngentle owl monkey, born somewhere in the idyllic canopy of the South American\nrain forest and now sitting in a laboratory bunker in North Carolina. Yet,\nfollowing our instincts, Wessberg and I believed we could generate much more\nnatural, and consequently much more variable, arm movements in our\nexperiments.\n\nLooking at how eagerly and inquisitively Belle went for the joystick every day\nwhen she was brought to the lab setup with its juice dispenser, Wessberg\nsensibly figured out that the real-world task that we should study was food\ngrabbing. In this new task, our monkey would first have to sit attentively in\na chair and, for a few seconds, face an opaque Plexiglas barrier. Suddenly,\nthe barrier would lift, revealing a square tray with a juicy chunk of\nfruit—Belle’s favorite meal treat—placed enticingly in one of its corners.\nUpon seeing the fruit, Belle would have to use her right arm to reach, grab,\nand bring back the food to her already salivating mouth, and do so before the\nbarrier came mercilessly down. If all this happened as planned, the monkey\nwould have completed a single task trial correctly and been treated to a fair\nreward. Then, as soon as that delicious fruit chunk had been gobbled up, Belle\nwould be ready to start her search for gustatory pleasure all over again,\nuntil the moment when, fully satiated, she would do what every wise primate\nusually does after such a feast: she would take a nice afternoon siesta.\n\nAs we had done in our earlier experiment, we wanted to demonstrate that our\nBMI could translate the raw electrical activity produced by about one hundred\ncortical neurons from Belle’s brain into meaningful, three-dimensional robot-\narm movements that closely matched those produced by the monkey reaching for\nthe fruit. This is exactly what happened. As Belle used her arms to reach\ntoward the fruit chunk, so did the robot arms at Duke and MIT. As the\nmultijointed robot arms accurately mimicked Belle’s arm based on a\nridiculously small, random sample of neurons, it was difficult not to ponder\nthe implausibility of what we were witnessing: the robot arm’s trajectories\nwere about 70 percent similar to the monkey’s free arm movements. Based on\nthese results, 95 percent accuracy could be achieved with a much smaller\npopulation of neurons than we had ever imagined. The question remained, how\nmany?\n\n* * *\n\nThe neurophysiological data obtained in further experiments revealed another\nimportant finding. Since in Belle and another owl monkey, Carmen, multiple\ncortical areas had been implanted with a microelectrode array, we were able to\nmeasure the contribution of individual neurons as well as cortical ensembles\nto the real-time prediction of arm movements in the BMI. To quantify this\nrelationship, Wessberg created yet another new analytical technique, now known\nas the neuron-dropping curve (NDC) (see [Fig. 7.1](part0011.html#fig7-1)),\nwhich measures the accuracy of a particular BMI’s computational algorithm in\npredicting a given motor parameter as a function of the number of single\nneurons recorded simultaneously. NDCs are computed by first measuring the\nperformance of the entire sample of simultaneously recorded neurons from a\ngiven brain region. Once this “maximum” performance is obtained, the same\ncalculations are repeated after individual neurons are randomly dropped from\nthe original sample. This step of randomly dropping neurons is repeated many\ntimes until the overall population is reduced to a final, single neuron.\n[Figure 7.2](part0011.html#fig7-2) shows pairs of NDCs highlighting the\ncontribution made by populations of neurons, located in two different cortical\nareas (primary motor cortex and posterior parietal cortex), to the\nsimultaneous prediction of two time-varying motor parameters during operation\nof a BMI by a rhesus monkey. This figure illustrates how the predictions of\ntwo such parameters, hand position and gripping force, varied as a function of\nthe size of the recorded neuronal population.\n\n![image](../images/00028.jpeg)\n\nFIGURE 7.1 How many neurons does it take? Two neuron-dropping curves correlate\nthe number of neurons (X axis) with the accuracy of real-time prediction\nobtained for two distinct parameters (hand position and hand gripping force).\nThe same sample of neurons from the primary motor cortex of a monkey was used\nto construct these curves. (Originally published in J. M. Carmena, M. A.\nLebedev, R. E. Crist, J. E. O’Doherty, D. M. Santucci, D. R. Dimitrov, P. G.\nPatil, C. S. Henriquez, and M.A.L. Nicolelis, “Learning to Control a Brain-\nMachine Interface for Reaching and Grasping by Primates.” Public Library of\nScience 1 [2003]: 193–208.)\n\nSimple as they are, the development of NDCs allowed us to obtain many useful\ncomparisons from our recorded neurons. For starters, we were able to compare\nhow well neural ensembles located in different cortical areas predict a\nparticular motor parameter or overall behavior. We also reaped a quantitative\nmeasurement of the performance of neural ensembles of different sizes as well\nas of the effect produced by combining all simultaneously recorded neurons,\nirrespective of their anatomical location. Furthermore, we now had the means\nfor detecting the average contribution of individual neurons, and for\ncomparing the performance of similarly sized populations of neurons, located\nin distinct cortical areas, in predicting a variety of animals’ behaviors,\nsuch as one-dimensional versus three-dimensional arm movements. But since\nnothing is perfect in this universe of ours, NDCs also posed a major\ninconvenience for a scientist working on a tight research budget: they\nrequired significant computational power (or a lot of patience) to be\ncalculated. At the time, not having the funds to obtain a faster computer, we\nhad to rely on Wessberg’s Swedish patience and resourcefulness to get all the\ncurves calculated and plotted.\n\n![image](../images/00029.jpeg)\n\nFIGURE 7.2 Gathering motor commands all over the brain. Neuron dropping curves\nare used to compare the accuracy of prediction of populations of neurons\nsimultaneously recorded in the primary motor cortex (M1, traced line) and the\nposterior parietal cortex (PP, dotted line) for two parameters (hand position\nand gripping force). Notice that information about both parameters is\navailable in both cortical regions, but that M1 contains more information, for\nequal neuronal populations, for hand position. Yet, both M1 and PP yield\nsimilar levels of accurate predictions for gripping force, considering\nneuronal populations of the same size. (Originally published in J. M. Carmena,\nM. A. Lebedev, R. E. Crist, J. E. O’Doherty, D. M. Santucci, D. R. Dimitrov,\nP. G. Patil, C. S. Henriquez, and M.A.L. Nicolelis. “Learning to Control a\nBrain-Machine Interface for Reaching and Grasping by Primates.” Public Library\nof Science 1 [2003]: 193–208.)\n\nDespite our computing power limitations, Wessberg persisted and, using every\ndrop of data generated by Belle and Carmen in their efforts to consume as much\nfruit as possible, produced the first NDCs ever published. These plots, which\nappeared in Nature in 2000, grabbed the attention of the neuroscience\ncommunity by revealing a couple of very intriguing and provocative findings.\nFirst, although neural ensembles from different cortical areas exhibited a\nclear degree of specialization in predicting one-dimensional arm movements,\ninformation about the same motor behaviors could be simultaneously derived\nfrom each of the cortical fields from which we recorded neuronal activity in\nour owl monkeys. Second, as individual neurons were randomly removed from the\noriginal recorded population in each cortical area, the NDCs indicated very\nlittle effect on the overall performance of the computational algorithm in\npredicting a particular movement. In other words, despite losing a few\nindividual elements, the remaining neuronal population was resilient enough to\nsustain performance at a level very close to the original population.\n\nAs we continued to remove neurons, one by one, the performance of the ensemble\ndegraded gradually. This continued until just ten to twenty neurons remained.\nAt that point, the neural ensemble’s performance declined rapidly, so that by\nthe time only a handful of neurons were left, the level of behavioral\nprediction obtained was extremely poor. Indeed, whenever only one neuron was\navailable, no reliable prediction of movements could be returned by the BMI\nalgorithm.\n\nSince our NDCs were produced by pulling random neurons from the original\nneuronal sample, these last results implied that, on average, the electrical\nactivity of any single cortical neuron recorded in our experiments could not\nunambiguously predict the type of movement our monkeys intended to make a few\nhundred milliseconds into the future, not even if the neuron was located in\nthe primary motor cortex. To generate purposeful arm movement, or to reproduce\nthat movement in an artificial machine, the brains of our two owl monkeys\nrelied on the cooperative work of a population of neurons. This defines the\nneurophysiological principle of single neuron insufficiency.\n\nTHE SINGLE NEURON INSUFFICIENCY PRINCIPLE\n\nNo matter how well tuned a single neuron may become to a particular parameter,\nits individual firing rate is insufficient to sustain a particular function or\nbehavior mediated by the cortex. Since the contribution of most single\ncortical neurons varies significantly from moment to moment, their lack of\nindividual statistical reliability means that no brain-machine interface can\nbe operated consistently, over long periods of time, based on a single\nneuron’s firing rate, and that the basic functional unit of thinking cannot be\na single neuron, but instead a population of neurons.\n\nAs exciting and demanding as the experiments with Belle and Carmen were, there\nwas a key element that could not be tested during their execution: how would\nprimates react to a feedback signal that informed them in real time of the\nmotor performance of the robots they were controlling with their brain\nactivity?\n\nBy the time Aurora had decided to cooperate with us, our experimental\napparatus had been upgraded in many different ways to try to address this\nquestion. For starters, the development of high-density microelectrode arrays,\nspearheaded by our resident “manufacturing magician,” Gary Lehew, with the\nhelp of Jim Meloy, had opened the way to monitoring up to 512\nmicroelectrodes—potentially as many as 2,048 single neurons simultaneously.\nSuch technological capabilities allowed us to observe how Aurora’s brain\nadapted to the demands of learning new tasks and interacting with the BMI,\nwhich had also been enhanced to provide feedback information about the\nperformance of a robot arm, placed in another room in our laboratory, in real\ntime.\n\nIn a way, Aurora had already been trained to receive feedback from a machine.\nFor example, since the joystick we had given her for playing the video game\nmoved easily, she had to learn how to control her full-of-wild-purpose hand\ngrips and arm movements to produce the desired trajectory for the cursor,\nusing the visual information on the screen. Since her reward depended on how\nquickly she grabbed the circle target, which randomly popped up on the\nmonitor, she also learned the importance of not wasting time or energy by\nmaking erroneous and futile guesses before the target appeared. Instead, she\ncontrolled her impulses and sat in an attentive state.\n\nThe night we turned on the BMI, removed the joystick, and left her to figure\nout the new game on her own, she moved her own arm and hand for the first few\nminutes, as if trying to reach into the screen to grab and squeeze the target.\nWhen she stopped making arm movements whatsoever and started to receive her\njuice reward as before, we realized that she had made the transition from the\njoystick to BMI control. And because we were continuously monitoring the EMG\nactivity produced by several of Aurora’s arm and back muscles, we were able to\ndocument the precise moment in which Aurora’s brain activity dissociated\nitself from her body muscles.\n\nAs in Eb Fetz’s experiments, both the visual feedback and the juice reward\nworked as powerful reinforcers in this process of dissociating Aurora’s brain\nand her muscle activity. Curiously, however, we never instructed Aurora to opt\nfor this dissociation, in the way that Fetz and Finocchio had designed their\nexperiments with primates. In fact, as long as Aurora continued to make the\ncursor cross the target she would receive a drop of fruit juice, whether her\nmuscles were still or contracting. Yet, as soon as she figured out that her\narm movements were not needed to win the reward she craved, she took the\ninitiative and refrained from moving her body, only using her mind to\naccomplish her feat. Nonhuman primates could voluntarily opt to dissociate the\nproduction of brain activity containing voluntary motor intentions from the\ntranslation of these intentions into true muscle action. Indeed, since our EMG\nrecords indicated that no muscle contraction occurred in the multiple arm\nmuscles we were monitoring, it appeared that none of the motor neurons in the\nspinal cord that projected to these muscles were activated either. Somehow,\nAurora could almost completely prevent the motor instructions generated in her\ncortex from being “downloaded” to her spinal cord.\n\nOver a few weeks, Aurora improved her behavioral performance while using the\nBMI in direct brain-control mode. By the end of this second training period,\nshe could get as many trials correct and drink as much fruit juice as she had\ndone using the joystick to play the game. Moreover, she was able to reduce the\ntime needed to generate the cursor trajectories using the BMI until she\nreached the same delay observed when she played the game with a joystick. In a\nmere 250 milliseconds, Aurora’s brain activity could be recorded from her\nbrain, rerouted to a central computer, fed into several mathematical models,\ntranslated into motor digital commands that could be understood by a machine,\ntransmitted to the robot arm, used to guide a cursor’s trajectory, and finally\nreturn, most of the time in triumph, to her eyes and to her mouth, where she\ncould taste her victory. About thirty days into her training regime, Aurora\nrealized that she could even use her arms and hand for other important\npurposes, such as scratching her back or grabbing a passerby neuroscientist,\nwhile using her brain alone to play the video game.\n\nOnce she had mastered her favorite game, we trained Aurora to perform two\nother motor tasks. One involved showing on the computer screen a static visual\ntarget formed by two concentric circles with different diameters, plotted one\ninside the other. In this task, the difference between the two circle\ndiameters indicated how much gripping force Aurora had to produce to receive a\ndrop of fruit juice. After learning to solve this puzzle by calibrating the\namount of force she used to grip the handle of the joystick, Aurora made a\nrelatively easy transition to employing the BMI to produce the right amount of\ngripping force just by thinking. Once again, she realized she could fulfill\nthe task requirement without generating overt movements of her own hand.\n\nIn a final display of her experimental prowess, Aurora learned to solve a much\nmore elaborate task, one that combined the most arduous elements of her\npreceding two experiments. As in the first game, Aurora had to guide a\ncomputer cursor to intercept a circular target that appeared in a distinct,\nrandomly selected location on the computer screen. However, by the time the\ncursor reached the target, its shape would morph into the two concentric\ncircles that represented how much gripping force Aurora had to generate in\norder to hold the target for a few hundred milliseconds. Now, to receive her\njuice reward Aurora not only had to make the cursor reach the target, but she\nalso had to make the cursor “hold it,” applying the correct amount of gripping\nforce. It took a while, but Aurora eventually learned to perform this more\ncomplex operation using pure thought, no arm movements required.\n\nAurora’s spectacular performances yielded a ton of neurophysiological data\nthat could be analyzed using the neuron dropping curves. In this case, NDCs\ncould be calculated for each of many different motor parameters simultaneously\npredicted in real time by our models. We could also take into account each of\nthe six cortical areas from which we recorded single neurons as Aurora\nmastered her tasks, learned to use the BMI, and dissociated her brain and\nmuscle activity.\n\nThe NDCs based on Aurora’s data reinforced the observations we had gleaned\nfrom our experiments with Belle. In fact, we did not need to dig too deeply\ninto the data to verify that predictive information about Aurora’s arm\ntrajectories could be derived from populations of neurons, but not from\nindividual cells, in all six cortical areas surveyed. Using the level of\naccuracy in predicting the real-time variation of each of the individual motor\nparameters modeled in our experiments as an initial criterion, we observed\nthat although samples of neurons from different cortical areas displayed\ndifferent levels of specialization in predicting each of these parameters,\nneuronal ensembles from every cortical area concurrently carried at least some\nmeaningful information. Take, for example, the NDCs plotted in [Figure\n7.2.](part0011.html#fig7-2) These curves compare the number of neurons needed,\nper cortical area sampled, to predict hand position or hand gripping force. A\nmuch smaller sample of M1 neurons can be used to predict hand position, at\nmuch higher levels of accuracy, than an equivalently sized population of\nneurons recorded in a region of the posterior parietal (PP) cortex. However,\nwhen we compare the contribution of the same cortical areas to the real-time\nprediction of hand gripping force, neuronal samples from the PP cortex\ngenerated predictions almost as accurate as M1 ensembles of the same size. If\nwe could have recorded more PP neurons, it is conceivable that larger neuronal\nensembles from this region could perform just as well as the M1 ensembles. Not\nonly were the general thoughts about Aurora’s arm trajectories generated\nacross vast extensions of her frontal and parietal cortex, but, very likely,\nmany of the cortical neurons participating in one particular motor task could\nlend their firing activity to the computation of several motor parameters,\nsimultaneously.\n\nThat finding led me to develop yet another principle of the relativistic\nbrain, the neuronal multitasking principle.\n\nTHE NEURONAL MULTITASKING PRINCIPLE\n\nIndividual cortical neurons and their probabilistic firing can simultaneously\nparticipate in multiple functional neural ensembles. That means that the\nspikes produced by a single cortical neuron can be utilized by distinct neural\nensembles to encode multiple functional and behavioral parameters. Thus, even\nthough in a particular moment in time a single cortical neuron may exhibit\nsharper tuning to a particular motor or sensory parameter, its spiking may\nconcurrently participate in the encoding of a different parameter, performed\nby another subset of neurons. The neuronal multitasking principle predicts\nthat the entire cortex is capable of exhibiting cross-modal sensory responses,\nand that individual neurons are capable of encoding multiple motor, or other\nhigher cognitive, parameters.\n\nThere was no sign of strict and precise localization of motor functions in\nAurora’s brain. Instead, cortical specialization, although evident, was\nrelative; it coexisted with a high level of function sharing. No hint of an\nencounter with a grandmother neuron seemed apparent either. When neuronal\npopulations from all cortical areas were reduced to a single neuron, none of\nthem individually generated the type of meaningful predictions of Aurora’s\nmotor behavior that could drive the BMI to work continuously, trial after\ntrial, with a decent level of accuracy.\n\nThe main conclusion that emerged from these experiments was rather\nstraightforward. Within Aurora’s brain, the phrenological inheritors to Franz\nGall and the single-neuron acolytes were both, at long last, utterly defeated\nby nature’s clever idea of relying on highly distributed populations of\nneurons to sculpt an animal’s behavior.\n\nNeuronal democracy rather than single neuron dictatorship was the slogan\nwritten across Aurora’s brain.\n\n* * *\n\nThe next step in our analysis was to compare how individual cortical neurons\nreacted when Aurora made the delicate transition from playing the video game\nwith the joystick and her own limbs to the brain-control mode of operating a\nBMI without producing even the slightest contraction of those same muscles. To\naccomplish this, we analyzed tuning curves that measured how each individual\ncortical neuron firing correlated with the velocity and direction of movement\nof Aurora’s biological arm and the robot arm at moments in time before,\nduring, and immediately after the execution of the movement. [Figure\n7.3](part0011.html#fig7-3) depicts three such tuning curves, each built with\ndata derived from a distinct task condition: joystick control mode, brain\ncontrol mode with concurrent arm movements, and brain control mode with\nmovements of the robot arm without movements of Aurora’s own arm.\n\nBased on a vast literature of neuroscientific research published over the last\nforty years, we expected a large percentage of the recorded neurons to exhibit\nfiring patterns that were somewhat correlated with some aspect of Aurora’s arm\nand hand movements. This was, indeed, what we found. The cortical neurons\nmodulated their electrical activity in a variety of ways in relation to these\nmovements; a single neuron could fire in anticipation of the onset of these\nmovements or change its firing up and down while the movements were being\nexecuted. Such distinct firing patterns could be seen, in more or less\ndifferent frequencies, in every one of the cortical areas we sampled.\n\n![image](../images/00030.jpeg)\n\nFIGURE 7.3 Fine-tuning of cortical neurons that control the movements of the\nbody and machines. Gray-scaled polar plots of the firing rate of one M1 neuron\nas a function of both arm velocity (key, top left) for different lags with\nrespect to instant of instantaneous arm velocity measurement (0 ms). Velocity\n= 0 is at the center of each circle and maximum velocity (14 cms/second) is at\nthe perimeter of the circle. Firing patterns were obtained during different\nmodes of operation (pole control and brain control with and without hand\nmovements), and during the use of different actuators (hand or robot\nmovements, see legends). Each of the circles also encodes the neuron’s\npreferred direction (see scale). Gray shading indicates firing rate (minimum\nis white, maximum is dark gray). (A) A neighboring neuron that exhibits strong\nvelocity and direction tuning only when the animal is using its hand to play\nthe video game, but not the robot. (B) A single M1 neuron that displays both\nvelocity and direction tuning during both pole and brain control and when the\nanimal is using its hand or only the robot to play the video game. (C) A\nneighboring M1 neuron that exhibits enhanced velocity and direction (dashed\narrows) tuning only when the monkey is preparing to use its brain activity to\nmove the robot arm, but not when it moves its own biological arm. (Originally\npublished in M. A. Lebedev, J. M. Carmena, J. E. O’Doherty, M. Zacksenhouse,\nC. S. Henriquez, J. Principe, and M.A.L. Nicolelis. “Cortical Ensemble\nAdaptation to Represent Velocity of an Artificial Actuator Controlled by a\nBrain Machine Interface.” Journal of Neuroscience 25 [2005]: 4681–93, with\npermission.)\n\nFurther analysis of the velocity and direction tuning curves revealed several\ninteresting properties. First, we identified a population of cortical neurons\nthat modulated their firing only when Aurora employed her own limb and hand to\ngenerate a movement (see [Figure 7.3A](part0011.html#fig7-3)). Invariably,\nthese neurons exhibited clear and broad velocity and direction tuning prior to\nthe onset of her arm movements. Often, both the velocity and direction tuning\nof these cells changed dynamically as the movement unfolded over time. Again,\nthese dynamic changes resembled the flexibility of the spatiotemporal tactile\nreceptive fields of the cortical and subcortical neurons recorded in our\nwhisking rats a decade earlier. Second, once Aurora stopped making movements\nwith her own arm, this group of cortical neurons stopped firing altogether. No\naction potential spikes were created. As a consequence, these neurons showed\nno velocity or direction tuning to the movements of the robot arm, despite the\nfact that these movements were actually under the control of Aurora’s brain\n(see [Fig. 7.3A](part0011.html#fig7-3)).\n\nBut this highly specific covariance of neuronal firing with biological arm\nmovements was only one of the patterns that we were able to see within the\nelectrical activity in Aurora’s cortex. Another sizable subset of her cortical\nneurons displayed velocity as well as direction tuning in response to\nmovements of both her biological arm and the robot arm, even when Aurora was\nmanipulating the BMI without making any muscle contractions (see [Fig.\n7.3B](part0011.html#fig7-3)). Sometimes the velocity and direction tuning\nchanged as Aurora made the transition from moving her own arm to directly\ncontrolling the robot limb with her brain alone. But in many cases, the\nvelocity and direction tuning of a single cortical neuron remained the same or\nvery similar in both conditions. Clearly, some of Aurora’s neurons could fire\nin the absence of any overt movement in her arm, as Eb Fetz observed among his\nmonkeys. Moreover, our experiments showed that these cortical neurons could\nalso sustain their original tuning characteristics when Aurora needed to\ncontrol the movements of the robot arm with her brain alone.\n\nEasily, this latter observation could have emerged as the most relevant\nfinding in all of our experiments with Aurora. It turned out, however, that it\nwas soon overshadowed by the identification of a third class of cortical\nneurons whose physiological behavior stunned us beyond belief. In a total\ndeparture from anything I had thus far seen or imagined in my career as a\nneurophysiologist, this subset of cortical neurons did not fire at all when\nAurora moved her own arms or hands, meaning that they did not exhibit any\nvelocity or direction tuning when Aurora was controlling the joystick with her\nhand. Yet, the moment Aurora started to employ her brain activity alone to\ndrive the BMI and control the robot arm, these neurons started to fire like\ncrazy and soon acquired gorgeous tuning for the velocity and direction of\nmovement of the robot arm (see [Fig. 7.3C](part0011.html#fig7-3), dashed\narrows). Curiously, these neurons were found throughout the cortex, even in\nM1, sitting next to other neurons that fired only when Aurora moved her own\narm. So much for the existence of order and structure in the primate primary\nmotor cortex. Essentially, we interpreted this observation as if a fraction of\nAurora’s cortical neurons sat quietly in her sensory motor cortex until the\nmoment they could seamlessly assimilate a tool or, in this case, a robot arm\nas part of her brain’s internal simulation of her body image.\n\nFor those of us committed to developing BMIs, Aurora’s results sounded like\nthe final movement of Beethoven’s Ninth Symphony: pure hope and joy. The\nreason for our optimism was simple. If Aurora could dissociate the production\nof motor activity in her brain from the contraction of her body muscles, it\nwas very likely that patients suffering from severe lesions of the spinal cord\nor a devastating peripheral neurodegenerative disorder, which render them\nparalyzed without affecting the rest of their brains, could learn to use their\ncortical activity to control the movements of a neuroprosthetic device\ndesigned to restore full body mobility.\n\nAlthough few noticed at the time, Aurora’s valiant efforts in these classic\nexperiments created a unique convergence of ideas, dreams, and semi-lost\ncauses that had been stalking the marginal, often neglected corridors of\nmainstream neuroscience for nearly two hundred years. By fully embracing\nThomas Young’s distributed coding insight, Karl Lashley and Donald Hebb’s\nconcept of regional equipotentiality and cell assemblies, John Lilly’s\nobsession to see and hear as many neurons flashing simultaneously as possible,\nEb Fetz’s pioneering use of feedback, and Edward Schmidt’s dream of\nneuroprosthetic devices, the Aurora experiments showed that it was possible to\nredefine the model of self that only a primate brain can create.\n\nIn the proverbial great relay race of science, it seemed that the hope of\nturning the final corner was finally within arm’s reach.\n\n\n8\n\nA MIND’S VOYAGE AROUND THE REAL WORLD\n\nMelding brains and machines had, for decades, appeared to be a far-fetched\ndream or, at best, the stuff of science fiction. With the publication of our\nresearch with Belle and Aurora, however, brain-machine interfaces had crossed\nthe threshold into the rarified halls of “real” science. Scientific American\nand the MIT Technology Review both highlighted the development of BMIs. A\nspecial issue of Nature in 2001 was dedicated to reviewing state-of-the-art\nresearch and technologies with the potential to influence the future of\nsociety. In the article I wrote for that issue, I revealed for the first time\na systems-engineering diagram describing the components of our closed-loop\ncontrol BMI and the steps necessary to transform such a device into a\nneuroprosthetic device. With all of this attention, neuroscience laboratories\naround the world started to shift their research and resources to the field.\n\nAt that time, the argument about BMIs was dominated by the discussion over\nwhether it was better to employ noninvasive methods, like the scalp EEG, or\nmore invasive chronic brain implants of microelectrode arrays, which, as we\nhad seen with Belle and Aurora, had created a BMI that could effectively use\nvoluntary brain activity to control a robotic arm.\n\nThose working with the EEG-based brain-computer interface (BCI) contended that\nbecause EEG signals could be obtained without the need for any invasion of\nbrain tissue, they offered the best balance between clinical risk and clinical\nbenefit. Invariably, defenders of the noninvasive approach pointed to Niels\nBirbaumer’s success in getting “locked in” patients to communicate with the\nexternal world through the first clinically relevant application of an EEG-\nbased BCI. A few years later, several groups, including one led by my good\nfriend Klaus Müller, a computer scientist at the Technical University of\nBerlin, expanded on Birbaumer’s BCI concept and developed applications in\nwhich healthy subjects could utilize their EEG activity to play simple video\ngames. More recently, EEG-based BCIs have allowed severely paralyzed patients\nto steer a wheelchair.\n\nDespite these extremely useful applications, EEG-based BCIs had limitations.\nBecause EEGs averaged the synaptic and firing activity of tens of thousands of\ncortical neurons, the signals fed into BCIs lacked the spatial resolution\nneeded for a prosthetic device to be able to mimic natural limb function.\nEssentially, since the EEG signal carries very little specific neuronal\ninformation, BCIs based on these signals can handle just a couple of bits of\ninformation. The debate finally reached a truce when the two camps realized\nrecently that BMIs could be built by taking the best approaches from\nnoninvasive and invasive techniques. In addition, in patients suffering from\nspinal cord injuries, BMIs could one day be used together with other\nrevolutionary therapies like stem cells to restore full body mobility.\n\nIn late 2002, my lab at Duke obtained preliminary human intraoperative data\nthat demonstrated the viability of using the BMI approach in clinical\napplications. We had decided to get as practical as possible, enlisting Dennis\nTurner, a neurosurgeon at Duke’s medical school, along with two of the\nresidents studying under him, Dragan Dimitrov and Parag Patil. Our chief\ntechnology developer, Gary Lehew, worked with the neurosurgical team to adapt\na commercially available microelectrode to test the concept of a BMI during a\nstandard surgical procedure for patients suffering from advanced symptoms of\nParkinson’s disease, a degenerative disorder in which neurons rich in the\nneurotransmitter dopamine, a key chemical utilized by the motor system to\ninitiate and produce smooth voluntary movements, die over time. The surgery\ninvolved implanting a deep brain stimulator, an electrode about the size of a\nheart pacemaker that is used to block the abnormal nerve signals that cause\ntremors, stiffness, walking difficulties, and other Parkinson’s symptoms.\nBecause it does not halt the degeneration of dopaminergic neurons, deep brain\nstimulation (DBS) does not offer a cure for Parkinson’s disease; it is simply\nthe most effective treatment available for patients who no longer respond to\npharmacological dopamine-replacement therapy.\n\nNormally, Turner and his team gain access to the brain by opening the skull\nand the meninges while the Parkinson’s patient remains conscious. This is\npossible because direct manipulation of brain tissue does not generate any\nsensation of pain; the tissue does not contain pain receptors, or nociceptors.\n(Thus, even though the brain continuously signals the pain of the body that\nhouses it, it never senses its own sorrows.) Turner next identifies a very\nsmall spot, deep in the brain, in which continuous electrical stimulation,\ndelivered through the chronically implanted electrode, will ameliorate some of\nthe patient’s most serious motor symptoms.\n\nThe most crucial step is the correct placement of the stimulator in the\npatient’s brain. During the surgery, in addition to asking the patient about\nthe effects of the stimulating electrode, Turner guides its placement by\ncontinuously monitoring the electrical activity of the neurons he encounters\nas he penetrates the brain with a probe. Usually, this process is carried out\nusing the classical single-microelectrode technique in which one neuron (or\neven multineuronal activity) is recorded serially as the tip of the electrode\nis slowly lowered into the cortex. In the event that Turner cannot find a\nusable spot during the first brain penetration, the electrode must be\nwithdrawn and the process is repeated with a new brain penetration. Usually\nmore than one brain penetration is made before the patient confirms that the\nstimulation has quelled his or her tremors and other Parkinson’s symptoms.\n\nLehew helped Turner and his colleagues improve on this routine by adapting a\ncommercially available probe that had been approved for neurosurgical\nprocedures to record many more spots simultaneously. In fact, rather than\nbeing limited to one spot at each depth of the brain at which he had parked\nhis probe, Turner could now monitor thirty-two spots. Lehew had made this\npossible by bundling together thirty-two microwires into a single recording\nprobe that could be lowered into the brain with a guiding tube. This microwire\nbundle immediately reduced the time necessary to complete the procedure.\nIndeed, in most cases the neurosurgeons could find a spot that worked in a\nsingle penetration.\n\nAfter we were able to demonstrate the clear clinical benefit of Lehew’s\nmicrowire innovation, we were granted permission by the Duke Institutional\nReview Board—and, as important, eleven patients—to test a simple version of\nthe BMI we had used with Aurora during these procedures to implant a deep\nbrain stimulator in Parkinson’s patients. Since the surgeries were now faster,\nwe were given a limited amount of time to explore the efficacy of our\napparatus. We therefore decided to train the patients to play a very simple\nvideo game the day before surgery was scheduled. In this game, the person used\none of his or her hands to squeeze a rubber ball. By applying different\namounts of gripping force to the ball, the patient could move a computer\ncursor back and forth on a track line on a monitor. The aim: to get the cursor\nto hit a rectangular target that popped up at a location along the track. It\ntook each patient just a few minutes of practice to master this task.\n\nDuring the surgical procedure, the same patients were asked to play the game\nfor about five minutes while their brain activity was recorded by the thirty-\ntwo-microwire probe as it slowly penetrated the brain. Unlike in our\nexperiments with Aurora, in these intraoperative recordings we could\nsimultaneously monitor at most fifty neurons. Moreover, since patients with\nsevere Parkinson’s symptoms often tire rapidly during the DBS procedure, we\ntried to minimize their fatigue by limiting our study to the collection of\ndata needed to verify whether the BMI would become capable of predicting the\ncursor’s trajectory using the patient’s brain activity alone. In the end, that\nmeant we recorded just ten minutes of data in each of the patients—five\nminutes dedicated to training BMI and five minutes to test it.\n\nEven with this small window for recording, the results were resounding. The\nBMI algorithms pioneered by Aurora and her monkey friends also worked well\nwhen they were fed with human brain activity. Johan Wessberg’s mathematical\nmodels could predict simple hand movements for humans, too. More important, as\nthe number of neurons employed to feed the BMI dropped from fifty neurons to\nthirty or twenty, the accuracy of the hand-movement predictions fell\nprecipitously, to a point at which they became useless. It was true that our\nstudy had recorded activity from subcortical motor structures, such as the\nthalamus and the subthalamic nucleus, rather than the M1 cortex. But there was\nno indication that very small samples of neurons in M1 would behave\nsignificantly different from those in these structures.\n\n* * *\n\nAfter we successfully completed the experiments with Aurora, it was essential\nto explain what it means to say that BMIs can liberate the brain from the\nphysical constraints imposed by the body that lodges it. One way to do this is\nto look for clinical applications of our BMI experiments.\n\nBefore we could move safely and confidently to more clinically oriented work,\nhowever, the laboratory team at Duke needed to explore further animal\nexperiments. By the fall of 2006, we had made some of our first steps into the\nremaining uncharted territory. Thanks to the superb work of my graduate\nstudent Nathan Fitzsimmons, who also illustrated this book, we had learned\nthat direct cortical microstimulation, using the same microwire arrays we used\nto record brain activity, could potentially deliver feedback messages directly\nto the brain of owl monkeys like Belle. Yet, such an approach still needed to\nbe implemented in rhesus monkeys, during the operation of a closed-loop BMI,\nbefore we could be sure of this outcome. Since these demanding experiments\nwould require extensive behavioral training and a series of control studies,\nwe decided to move forward with another project based on a fundamental\nobservation derived from our work with Belle and Aurora.\n\nWe had learned from Aurora that the utilization of a BMI could allow a subject\nto change simultaneously the normal scale of three physical parameters\ninvolved in the generation of primate motor behaviors: space, force, and time.\nBy directly controlling the movements of a robot arm located far away from her\nbody, Aurora had increased dramatically the spatial reach of her voluntary\nmotor intentions. Since the robot was capable of generating stronger forces\nthan those normally produced by her own arm, Aurora also scaled up the force\nthat resulted from her motor thinking. Finally, through direct control of our\nupper-limb BMI, Aurora could generate movements in the robot arm a bit faster\nthan the normal neurobiological process through which she moved her own arms,\namping up the time scale, as well.\n\nBy the early months of 2007, the main questions we were debating in our\nlaboratory gatherings, usually over some generous servings of Greek-Lebanese\nfood at George’s Garage, our favorite restaurant in Durham, hovered around two\nmain issues. The first related to whether BMIs could be used to produce more\nthan upper-limb movements. In other words, could BMIs ever serve as a platform\nfor restoring all sorts of motor behaviors? The second issue dealt with how\nfar we could possibly push the scaling of space, force, and time involved when\na subject operates a BMI.\n\nAs things in science usually go, the answer we were looking for did not\npresent itself in the circumspect and well-behaved environment that usually\ndominates a typical systems neurophysiology lab. In this case, the hard-sought\nepiphany came to me in the middle of an intensely contested soccer match,\nplayed (and barely won) by my beloved Sociedade Esportiva Palmeiras, in the\nPalestra Itália stadium in São Paulo where I have religiously attended some\nhistoric, as well as some not so memorable, games since my newborn days. That\nSunday afternoon, as I watched in distress as the Palmeiras players missed one\neasy scoring opportunity after another, the desperate crowd repeatedly shouted\nthem down with one of the worst epithets available to them: pernas de\npau—literally, wooden legs. It suddenly dawned on me in which new direction we\nshould steer our BMI research program. Back in my hotel room, after a brief\ncelebration of Palmeiras’s razor-thin victory, the only thing on my mind was\nwhether we could possibly make our rhesus monkeys learn to operate a BMI\ndesigned for reproducing bipedal locomotion in a remotely located robot using\nonly cortical neuronal activity as the driving signal.\n\nWithout much noise, while I was on a sabbatical at the Swiss Institute of\nTechnology of Lausanne during most of 2006 and 2007, one of my postdoctoral\nfellows, Andrew Tate, had been training rats to walk on a treadmill in an\nattempt to measure the concurrent patterns of neuronal electrical activity\nproduced by both cortical and subcortical motor structures known to be\ninvolved in the production of locomotion. Traditionally, neurophysiologists\ninterested in the neural mechanisms of locomotion had primarily focused their\ninvestigations on subcortical pools of neurons that exhibited a certain\nrhythmic firing—the so-called central pattern generators—related to the\nproduction of a quadrupedal gait cycle. Most of these studies, conducted in\ncats, had found central pattern generators in the spinal cord and brain stem.\nIt seemed that the existence of central pattern generators at the spinal cord\nlevel explained why cats were able to sustain a quadrupedal locomotion pattern\neven after the spinal cord had been severed from the rest of the animal’s\nbrain: place a cat with a severed spinal cord on a moving treadmill, and the\ncat can keep up. In primate evolution, most of these central pattern\ngenerators had moved into the brain, which is why a spinal cord injury or\nlesion induces an irreversible paralysis of the body musculature below the\nlevel of damage. Even if the rest of the body is suspended, a human patient\nwith such an injury will not be able to keep up with the treadmill’s pace.\n\nBecause of the long tradition of focusing on subcortical central pattern\ngenerators in experimental animals, as well as the difficulties involved in\nperforming chronic cortical recordings in freely ambulating animals, the\npotential role of the motor cortex in controlling the gait cycle in nonhuman\nprimates had been pigeonholed as a stubborn scientific challenge. To our\nsurprise, however, Andrew Tate’s rat studies revealed that cortical neurons\nlocated in both the M1 and S1 cortices did modulate their firing rate when the\nanimals walked at normal speed on a moving treadmill.\n\nThis revelation spurred us to take the risk and invest in building a\ncompletely new apparatus that would allow us to test a BMI for creating\nlocomotion. To begin, though, we needed to demonstrate that rhesus monkeys\ncould actually walk bipedally on a treadmill. Then, we had to demonstrate that\nwe could record the electrical activity simultaneously produced by a few\nhundred monkey cortical neurons as it walked. No study that we were aware of\nhad reported that rhesus monkeys could perform such a behavioral trick, and\nnone had recorded chronic cortical activity in freely walking monkeys.\n\nLuckily, an answer to one of our problems came from a most unusual source when\nMisha Lebedev, my long-time collaborator and the newly appointed director of\nour primate laboratory, dug out some early-twentieth-century reports\ndescribing how Russian circuses had trained rhesus monkeys to “walk bipedally\non a platform.” Bingo! The secret was in providing enough support to the upper\ntorso of the animal so that it felt secure enough to stand on its hind limbs\nand start walking.\n\nWe assigned this impossible job to our technology whiz Gary Lehew. By now used\nto our absurd demands, Lehew did not take long to figure out how to build a\n“walking-monkey” setup, consisting of a hydraulic treadmill outfitted with a\nPlexiglas support for the monkey’s upper body, allowing it to walk peacefully\nalong with the direction, speed, and slope at which the treadmill was set (see\n[Fig. 8.1](part0012.html#fig8-1)). By using a hydraulic mechanism instead of\nan electric motor, he conveniently eliminated a major potential source of\nelectrical noise from our neuronal recordings. The treadmill was placed in a\nsoundproof, shielded room, further ensuring that the monkey would be protected\nfrom any distraction during its daily strolls.\n\n![image](../images/00031.jpeg)\n\nFIGURE 8.1 Experimental setup employed to create a brain-machine interface for\nlocomotion. (Originally published in N. Fitzsimmons, M. A. Lebedev, I. Peikon,\nand M.A.L. Nicolelis, “Extracting Kinematic Parameters for Monkey Bipedal\nWalking from Cortical Neuronal Ensemble Activity.” Frontiers in Integrative\nNeuroscience 3 [2009]: 1–19.)\n\nLehew also designed a clever tethering system to hold some of our recording\nhardware right above our subject’s head. This adaptation ensured that the many\ncables connecting the implanted microwire arrays to the preamplifiers did not\nget tangled by the walking monkey. To allow the monkey to receive feedback\nfrom the machine it was controlling via the BMI, he installed a system to\nproject video feeds on the wall in front of the treadmill. Any walking monkey\nlooking straight ahead while strolling on the treadmill—the most likely\nscenario, given the Plexiglas support—would see the movements generated by a\nremote robotic device controlled by the BMI.\n\nAs soon as this new apparatus was ready, we chose the first monkey to test it.\nLike her predecessor Aurora, Idoya had shown early signs of a “go-getter”\nnature in her encounters with other members of our rhesus monkey colony.\nAlthough she had never ever seen anything remotely close to Lehew’s walking-\nmonkey setup, she was not intimidated by the equipment and, in just a few\nweeks of daily training, became an expert bipedal walker. By the end of her\ntraining period, Idoya had learned how to shift the direction of her\nlocomotion back and forth, and how to accelerate or slow down her walking\nspeed as soon as the treadmill’s speed was adjusted. As long as her fruit\nreward was dispensed at the end of a few correct steps, she had no problem\nwalking for an hour or more per day.\n\nAt this stage, we implanted several microwire arrays in some cortical areas of\nIdoya’s brain. A few days after the surgery, she was back walking, like any\naddicted primate jogger. The difference now was that our implants were\nyielding recordings from hundreds of gorgeous neurons, which showed a clear\nmodulation in their firing rate as Idoya walked along, unconcerned. These\ninitial recordings revealed that single neurons in both the S1 and M1 cortices\ntended to fire prior to the initiation of each step cycle. When combined into\na population and plugged into the linear multivariate models employed in our\nupper-limb BMI, these cortical neural ensembles generated highly accurate,\nreal-time predictions of Idoya’s strides. That brought us to the next major\nbottleneck in our attempt to generate true locomotion patterns out of a\nprimate’s raw brain activity: which artificial walking device could possibly\nutilize in real time the type of brain-derived control signals that we could\nrecord from our primate’s brain? That was a tough one. To make sure that\neveryone grasped the implications of our experiments, we needed to demonstrate\nwalking in some sort of humanoid robot.\n\n* * *\n\nAs it happened, a few years earlier I had actually met a humanoid robot who\nfit the bill. That robot resided in the lab of Gordon Cheng, the founder of\nthe Department of Humanoid Robotics and Computational Neuroscience at the\nAdvanced Telecommunications Research Institute (ATR) in Kyoto, Japan. In 2005,\nI had met Cheng and ATR’s director, Mitsuo Kawato, during a visit to see a\nstill-under-construction robot named Computational Brain, Model 1, or CB-1 for\nshort.\n\nDesigned by Cheng and built by the American company Sarcos, CB-1 looked pretty\nhumanoid, with its two legs and arms, despite being powered by a hydraulic\nsystem. The robot was primarily going to be used to study the possibility of\nreproducing realistic, humanlike motor behaviors, including locomotion. Using\na previous generation of the same robot family, Kawato had already achieved\narm and hand movements that were highly coordinated and precisely aimed—a\nwonderful triumph. They had even been programmed to play Ping-Pong and execute\na shuffling version of the steps of several traditional Japanese folk dances.\nI was sure I had the right roboticist and the right robot to marry with our\nwalking monkey’s brain. So I called Cheng up one day to make a proposal—one\nthat I had not even mentioned in a recent grant proposal for fear of being\nridiculed by the reviewing panel.\n\n“So, my friend,” I began tentatively. “We have figured out that one of our\nrhesus monkeys, Idoya, can walk bipedally on a treadmill, and that the\ncollective neuronal activity we record from her cortical areas can predict her\ngait cycle. That means we have built a BMI that in theory could allow Idoya’s\nbrain activity to drive your CB-1 robot, in real time, to be the first\nhumanoid robot to ever walk under the control of a primate brain.”\n\nThat was the easy part.\n\n“We still need to figure out how to get the brain data to Japan quickly enough\nso that the robot can walk as fast as it takes Idoya’s legs to respond to her\nbrain. We also need to figure out how to return a video feedback signal from\nthe robot’s legs to Idoya so that she is aware of what is going on at every\nmoment. As you know, both the transmission of the brain signals from the\nUnited States to Japan and the return of the visual feedback from Japan to the\nUnited States have to occur within the same time delay as the animal’s\nreaction time.”\n\nThat was the hard part. As we had seen with Aurora, we had just a few hundred\nmilliseconds to play with. Longer than that, and the machine and the brain\nwould fall out of sync, and the BMI would fail.\n\n“Sure. Let’s do it. I like the sound of the idea. Believe me, I can get the\nsignals back and forth between Durham and Kyoto faster than 250 milliseconds.”\n\n“Are you sure?”\n\n“Yeah. Let’s start tomorrow.” That is why I like Cheng so much; he is always\nready to take on the almost inconceivable.\n\nBefore I was up the next morning and could tell the team in the Duke bunker\nabout this good news, Cheng was busy coding a way to get around the massive\nInternet traffic jam that confounded us in order to hit his 250-millisecond\nmark. For the next few months, in addition to his own lab projects, he worked\non the problem every night.\n\nThe data transmission speed wasn’t the only hurdle placed before our\ntranscontinental human-robot team. For instance, Ian Peikon, a brilliant\nundergraduate engineering student, almost single-handedly designed and\nimplemented a completely new system for measuring the continuous three-\ndimensional spatial position of Idoya’s leg joints. Peikon’s kinematic\nrecording system involved placing on the ceiling above the treadmill a number\nof video cameras, each of which measured the light reflected by a layer of\nfluorescent paint applied to Idoya’s right hip, knee, and ankle. This tactic\ngenerated a continuous stream of precise information about the position,\nvelocity, and acceleration of Idoya’s leg joints as she walked on the\ntreadmill. We then asked Cheng to add to his list of projects a robotics\ninterface for utilizing the brain-derived data that predicted this kinematic\ndata.\n\nPeikon’s kinematic data and the concurrent recordings of the electrical\nactivity of a couple of hundred of Idoya’s cortical neurons were then fed into\nthe same computational algorithms that we had employed in Belle’s and Aurora’s\nexperiments. When we analyzed Idoya’s brain activity as she walked at constant\nvelocity on a treadmill, we found that steps which appeared identical from a\nkinematic point of view were preceded by distinct, fine-grain spatiotemporal\npatterns of neural ensemble firing—measurements that were made with just a\ncouple of milliseconds of temporal resolution. If we integrated the activity\nof this small number of neurons at a time interval on the order of one hundred\nmilliseconds, however, the variability of these patterns of cortical activity\nwas reduced significantly. This implied that the 250-millisecond interval\ncould be successfully employed in a BMI.\n\nIt also suggested something more profound. Since billions of neurons are\navailable to participate in producing a solution to any given problem at any\ngiven moment in time, the brain has the luxury of being able to recruit a\ndifferent combination of neurons each time it desires to produce a motor\nbehavior. Indeed, I propose that throughout our lives, no matter how many\ntimes we repeat the same motor behavior, the fine-grain spatiotemporal\nneuronal pattern that carries this particular voluntary motor desire will\nnever be exactly the same. This finding, which resembles ideas originally put\nforward by Karl Lashley and Donald Hebb, is encapsulated in the neural\ndegeneracy principle, a term coined by the American Nobel laureate Gerald\nEdelman, who compared this strategy to the degeneracy, or redundancy, observed\nin the genetic code.\n\nTHE NEURAL DEGENERACY PRINCIPLE\n\nA particular brain outcome—whether a motor action, a perceptual experience,\neven complicated behaviors produced by the brain such as singing or solving an\nequation—can be generated by a very large variety of distinct neuronal\nspatiotemporal patterns of activity.\n\nIn the genetic analog, distinct nucleotide triplets, known as a codon, in a\nmessenger RNA molecule can call on a given amino acid to join a polypeptide\nchain in a ribosome, the granules within cells where proteins are synthesized.\nNotice that there is no ambiguity in the genetic code since there is never\ndoubt about which amino acid a particular codon calls for. In the brain,\nhowever, multiple neuronal solutions may exist for encoding any goal-oriented\nbehavior the brain needs to produce.\n\nWith this insight, by the summer of 2007 we had discovered that by feeding the\nbrain activity that Idoya experienced as she walked on the treadmill at fixed\nand variable speeds into twenty-one linear mathematical models running in\nparallel, we could generate the necessary continuous stream of motor commands\nthat would make CB-1 walk like a monkey (see [Fig.\n8.2](part0012.html#fig8-2)). Later that summer, Cheng proved that he was up to\nthe 250-millisecond challenge. He had established a special Internet\nconnection between our two laboratories, a masterpiece of technological\ninvention that circumvented the idiosyncrasies of the Duke and ATR firewalls,\noutmaneuvered delays caused by the servers and hubs around the world that\nwould route the data between the two campuses, and documented the efficiency\nof both directions of data transmission at the same time.\n\nThe usual protocol at this point would have been to run the experiment,\ncollect the data, publish a paper in some specialized scientific journal, and\nthen disclose the findings to the public, about two years down the road. But\nwe felt that the mind-bonding between Idoya and CB-1 deserved a streamlined\nprocess, so we resolved to accelerate the peer review system a bit and run the\nexperiment with the New York Times sitting on the sidelines in Durham and\nKyoto. The benefits of getting our results disseminated broadly far exceeded\nthe risks of the experiment not working or upsetting our more traditional\npeers. If we could show that CB-1 was walking under the control of Idoya’s\nbrain, it would help bolster the argument that it was within the reach of\nscience to build a BMI, perhaps within a decade, that would allow a person\nsuffering from a devastating neurological disorder to walk again.\n\n* * *\n\nOn a colder than average January morning in 2008, we decided to stage what I\nhad been calling our “little moon walk.” Idoya, the primate poised to make\nthis next great leap, was escorted into the room housing her familiar\n“walking-monkey” setup. She may have noticed that those assembled in the lab\nlooked conspicuously graver than they had been at her previous walking\nsessions. As with most major technological endeavors, the Duke team turned to\nour checklists to prepare for countdown. The level of worrying in the room\ncould be calibrated against the number of times one of our graduate students\ninspected the many computers involved in sending and receiving data from\nIdoya’s brain and the Kyoto lab, as well as the number of people who asked me\nif I was sure we had enough raisins and Cheerios—Idoya’s preferred food\nrewards—in stock.\n\nWith photographers documenting every moment, Idoya was gently placed on the\ntreadmill. Before she was allowed to walk free, we linked the plastic\nconnectors, which held the microwire arrays we had implanted in her brain\nseveral months earlier, to our hanging preamplifiers. On the projection\nscreen, she could see the sharp, colorful image of CB-1’s humanoid legs, which\nhad been magnified a few times so that Idoya’s entire visual field was\nprimarily occupied by them.\n\n![image](../images/00032.jpeg)\n\nFIGURE 8.2 Kinematic predictions, derived from combined raw brain activity,\nfor different types of bipedal locomotion behaviors: slow forward walking (top\nshelf), fast forward walking (middle shelf), and variable speed (lower shelf).\nBlack trace depicts actual position of Idoya’s leg whereas the gray traces\nyield real-time predictions of this kinematic parameter obtained from her\nbrain activity alone. (Originally published in N. Fitzsimmons, M. A. Lebedev,\nI. Peikon, and M.A.L. Nicolelis, “Extracting Kinematic Parameters for Monkey\nBipedal Walking from Cortical Neuronal Ensemble Activity.” Frontiers in\nIntegrative Neuroscience 3 [2009]: 1–19.)\n\nWhen the lights were dimmed, the fluorescent paint glowed green at Idoya’s\nhip, knee, and ankle. The conditions seemed to urge Idoya into action. She\nrealized it was time to play her favorite game and that she was likely to get\nloads of raisins and Cheerios if she played it well. With that incentive,\nIdoya did not hesitate for a millisecond and eagerly started to walk,\nfollowing the gentle, constant speed of the treadmill.\n\nThe video cameras spread across the room’s ceiling immediately picked up the\nlight reflected by the fluorescent markers, transmitting individual video\nframes to a computer programmed to calculate the three-dimensional spatial\nposition of Idoya’s moving legs. Meanwhile, a continuous parallel stream,\nconsisting of the thousands of tiny sparks of voluntary brain electricity made\nby hundreds of Idoya’s cortical neurons, promptly lit up one of the large\ncomputer monitors placed in the control room. As we observed this sample of\nmotor thinking, our computers crunched the linear mathematical models that\nwould match Idoya’s brain activity to the kinematic parameters derived from\nher leg movements. Back in Kyoto, Gordon Cheng had already positioned CB-1 on\ntop of a spiffy Japanese treadmill of its own. Suspended by its back, CB-1\nwould, in this first experiment, be limited to walking on air, a simplified\nrobotic emulation of a distinguished North Carolinian who used to do this for\na living, playing basketball, first for Duke’s eternal rival, the University\nof North Carolina, and later on for the NBA (see [Fig.\n8.3](part0012.html#fig8-3)).\n\nAs usual, the training of the mathematical models continued for a few minutes\nuntil the linear regression coefficients started to converge and stabilize.\nWhen the optimal set of regression coefficients was identified, we turned our\nattention to another computer screen, where Idoya’s brain activity appeared to\nalready be predicting her walking movements quite well. The two lines, one red\nand the other white, moved closer and closer together, until they almost\ncompletely overlapped. It was time to start sending the brain-derived\npredictions to Kyoto.\n\nA few seconds passed while Cheng turned on the control system responsible on\nhis end for streaming the BMI’s signals to CB-1. Oblivious to the tension that\ngripped the humans around her, Idoya continued to maintain her stride without\nmissing a step. Suddenly, the video frames projected on the wall acquired an\nalmost human sense of purpose. A little twelve-pound, thirty-two-inch-tall\nrhesus monkey was using her mind’s electricity to control the primatelike baby\nsteps of a two-hundred-pound, five-foot-tall humanoid robot on the other side\nof the Earth. It was impossible not to deem it “one small step for a robot,\none giant leap for primates.”\n\n![image](../images/00033.jpeg)\n\nFIGURE 8.3 Idoya and CB-1’s great leap across the globe. General schematic\nrepresentation of the experiment that allowed a monkey on the U.S. East Coast\nto use its brain activity to control the leg movements of a humanoid robot\n(CB-1) in Kyoto, Japan, while receiving visual feedback back in Durham, North\nCarolina, from the robot locomotion. (Courtesy of Dr. Miguel Nicolelis.)\n\nAurora would be proud of her little cousin. After all, in just these few\nsteps, Idoya had pushed the scaling of space and force of brain-machine\ninterfaces to the edge of what was theoretically conceivable, but not yet\ndemonstrated, at the time.\n\nBut that was not all. As Cheng quickly reminded me over the phone, we still\nneeded to measure the total time delay elapsing between sending brain-derived\nsignals from Durham to Kyoto and returning video images from Kyoto back to\nDurham. “It’s 230 milliseconds,” he reported. “I told you we would do it below\n250 milliseconds!” After months of exhausting work, Cheng could not hide his\nsatisfaction. Together with Fitzsimmons, Lebedev, Peikon, and the rest of our\nteam, he had managed to establish a direct, bidirectional functional link\nbetween a primate brain and a pair of robot legs that accomplished its task a\nfew tens of milliseconds faster than the time it took for the electrical\nactivity generated in Idoya’s brain to trigger a muscle contraction in her own\nleg. With so much to celebrate, we decided we could take yet another risk.\n\n“On my sign, just stop the treadmill. Okay: stop it.”\n\nAs the treadmill slowed to a stop and Idoya assumed a typical immobile\nposture, all eyes in Durham were glued to the monitor showing CB-1 in Kyoto.\nIdoya, too, seemed intrigued as she continued to stare at the images projected\nin front of her.\n\nPerhaps she wanted to prove something. For, as long as we stood witness, the\nonly thing we could see was that son of a gun CB-1 robot, only recently\npromoted to a bipedal walker, continuing to take its rhythmic strides floating\nin midair. CB-1 just walked and walked, following the instructions that\ncontinued to flow, uninterrupted, from Idoya’s brain.\n\nBack in Durham, nothing was happening, at least in movement terms. All of us,\nincluding Idoya, remained still and silent, admiring in awe those robot\nstrides projected on the screen before us, each step finely crafted, just a\nfew hundred milliseconds earlier, by the raw electrical breath of life that\nemerged, almost as a divine gift, from a rightly joyful, because it was now a\nliberated, primate brain.\n\n\n9\n\nTHE MAN WHOSE BODY WAS A PLANE\n\nStarting a few million years ago, when the first hominids began to roam\nthrough the valleys of East and North Africa, the brains of some of our Homo\ngenus ancestors underwent a series of morphological and physiological\ntransformations that resulted in the emergence of a cascade of mental\nprocesses and behaviors never before witnessed in the animal kingdom. Among\nother changes, this complex brain remodeling included an enormous differential\ngrowth of the frontal and parietal cortical lobes and the many parallel and\nreciprocal neuronal pathways that link these regions to one another and to a\nvariety of subcortical structures. This huge evolutionary expansion of the\nfrontoparietal circuitry yielded a suite of unique neurophysiological\nadaptations culminating in new perceptual, motor, and cognitive behavioral\nskills that contribute to the unique attributes of what we commonly define as\nhuman nature.\n\nThe ability to produce and comprehend oral language surged during this\nevolutionary quantum leap in brain architecture. Since many interesting\nscholarly articles and books have been written on the subject of language and\nits role in subsequent human evolution, here I will focus instead on two other\nemergent concurrent adaptations whose inception in the primate brain’s\ncognitive tool kit has been equally decisive, if not essential, in the\nunfolding of human evolution. The first of these involves the ability of our\nspecies and its ancestors to become the ultimate master toolmakers on Earth.\nIndeed, the presence of many artifacts in close proximity to the early hominid\nfossil remains found at Olduvai Gorge, in Tanzania, was so conspicuous that\nthe great paleontologist Louis Leakey named the species he unearthed Homo\nhabilis—literally, “handy man.” The mental abilities required for toolmaking\nstand as one of our most amazing evolutionary riddles.\n\nCuriously, a second, and perhaps even more revolutionary, behavioral\nadaptation arising from the burgeoning of the frontoparietal circuitry has\nattracted less attention in the neuroscience community. Such an attribute\nensures that besides being the ultimate toolmakers in the evolutionary history\nof our planet, the human species has acquired the capability of seamlessly\nincorporating the artifacts we fabricate into our sense of self, as true\nexpansions of the elaborate and intimate simulated model of our bodies\ngenerated by our brain. Although this may sound more incredible than feeling\nthe presence of a phantom limb or having an out-of-body experience, a range of\npsychophysical, imaging, and neurophysiological experimental evidence for the\nphenomenon of tool incorporation has been simulated in both human and nonhuman\nprimates. In this chapter I will review this rather stunning\nneurophysiological evidence. But first I would like to tell a story that\nillustrates the profound relationship that we, as a species, have established\nwith artificial tools. Created first in our minds, as mere dreams, and soon\nafter translated into tangible mechanical, electronic, and, more recently,\ncomputational and virtual artifacts, for the past six million years tools\nbuilt by the human brain have helped extend our collective reach to the limits\nof our imagination, allowing, among other things, our terrestrial bodies to\nconquer the very skies that endowed us with the basic elements of life.\n\n* * *\n\nIn the daily routine of Parisians during the opening months of the twentieth\ncentury, the publicized intention of a young expatriate scientist to test his\nlatest invention was hardly reason for special notice. After all, Paris was at\nthe time one of the leading world centers in science, home to world-renowned\nphysicists, mathematicians, engineers, and inventors. Just a dozen years\nearlier, Gustave Eiffel had erected the tallest man-made structure in the\nworld, an emblematic demonstration of France’s engineering power and\ningenuity. Yet, for the crowd of onlookers who, on the windy Saturday\nafternoon of October 19, 1901, opted to stroll absentmindedly through the\nimmaculate boulevards and manicured gardens of their enchanting city, history\nhad reserved quite an unforgettable treat.\n\nThat frigid fall day, Alberto Santos-Dumont, a diminutive but impeccably\ndressed Brazilian man, defied the normal protocol of discovery by doing\nsomething that was as foreign to the scientific community then as it is today:\nhe performed one of his most daring experiments in public, in broad daylight,\nso that all of Paris could verify whether he had succeeded or failed in his\nlatest attempt to fulfill his childhood dream. He had spent the best part of\nthe past four years, as well as a small but meaningful chunk of his personal\nfortune, to do so. He had almost been killed a few times, too. Fortunately for\nSantos-Dumont, he was the son and heir of one of the richest coffee farmers in\nthe world, so money—if not life—was without limit. He did not need to expend\nmuch valuable time away from his workshop trying to convince skeptical peers\nor funding agencies of the merits of his revolutionary approach to solving one\nof the greatest obsessions of his age.\n\nThat obsession was flight.\n\nSo it was that the flâneurs crossing the recently inaugurated Pont Alexandre\nIII bridge happened to witness an astonishing, dead-on view of a strange\nflying object that seemed to be approaching the Eiffel Tower from the\ndirection of the Bois de Boulogne. Parisians stopped wherever they stood,\ntracking the object’s trajectory. For generations to come, these awestruck\nspectators would proudly brag that they were there when this son of a\nBrazilian coffee farmer flew his airship through the skies, like a lone\nsoaring bird full of purpose, defeating the wind to steer its body at will.\nThat afternoon Alberto Santos-Dumont single-handedly launched the age of\ncontrolled flight when he guided the airship he had conceived and perfected\nalong a path circling the Eiffel Tower and then returned to the very point\nfrom which he had taken off.\n\n* * *\n\nAs its mundane name suggests, No. 6, the airship that Santos-Dumont flew in\nhis demonstration, was the sixth in a sequence of experimental dirigibles that\nthe young Brazilian and his crew of Parisian mechanics, led by his close\ncollaborator Albert Chapin, had built. Being the most evolved member of its\nclass, No. 6 included all of their innumerable aeronautical innovations.\n\nBefore Santos-Dumont, men had only left Earth’s ground by ascending in\nspherical balloons filled with hot air, helium, or hydrogen gas. These\nballoons could not be steered, so their pilots had to be content to fly at the\nwill of the wind and to use the release of ballast or gas to control the\nballoon’s ascent and descent. As such, these aviators never controlled the\nballoon’s trajectory or its landing place. They flew at the mercy of nature.\n\nSantos-Dumont was clearly aware of how achieving controllable flight would\nreshape human life. For him, such a conquest defined a fundamental struggle,\nwell worth the sacrifice of the inventor’s own life: to liberate humanity from\nits “terrestrial prison” and provide the means to freely explore the far-off\nlimits of the universe. A self-taught engineer who learned to design, improve,\nand improvise machines on his father’s farm, he wanted to build the airships\nthat his idol, the nineteenth-century writer Jules Verne, had dreamed would\none day take man all the way to the moon.\n\nAfter a succession of intermediary prototypes, Santos-Dumont invented the\nsingle-pilot dirigible, an airship that included a variety of technological\nadvances in its shape, design, construction materials, steering technology,\nand instrumentation. From the beginning of his experiments with these\nairships, he had aimed to create a machine that could be freely driven and\nsteered by its pilot, just as he could do when he toured France in one of the\nfirst cars to become available there. In a stroke of genius he decided to\nadapt petroleum car engines and add them, together with a long propeller, to\nhis flying machines. In fact, to reach the output needed to power his\nairships, Santos-Dumont asked his mechanics to combine two car engines in a\ndesign that alarmed his friend Albert Chapin. Thankfully, he also had the\npresence of mind to bend down the engine’s exhaust pipe so that the hot fumes\nand sparks coming off his motor would be ejected away from the dirigible’s\ngasbag full of flammable hydrogen.\n\nTo enable the small car engines to move his dirigible, he had to find new ways\nto reduce the airship’s weight. Being five-feet-four and very thin, his body\nframe was more than suitable for traveling in the smaller, lighter airships he\nwanted to build. He chose lightweight Japanese silk, which proved to be\nextremely resistant, for constructing his dirigible’s gasbags, and bamboo\nsticks and pine wood for constructing its frame. Then, to improve the\nmachine’s aerodynamics, he departed from the classical spherical shape used in\nballoon gasbags and built an elongated, cigarlike gasbag that would “cut\nthrough the air.”\n\nHe then struggled to find a way to control the steering of his airship at\nwill—something no other aviator had yet experienced. After much tinkering and\ntesting, he installed two aeronautical navigation devices that he thought\nwould do the trick. The first of these devices was a large and movable\ntriangular (later hexagonal) rudder, also made of Japanese silk, that was\nplaced on top of a light frame. The frame was attached to the back of the\nairship’s keel, or sometimes directly onto the gasbag. Santos-Dumont was able\nto control his dirigible’s horizontal motion by manipulating a rudder. This\nrequired him to pull on a system of rope cords. Later on he adapted bicycle\nhandlebars to serve as a steering device.\n\nNext he turned to the issue of the airship’s vertical equilibrium. As Paul\nHoffman describes in his wonderful account, Wings of Madness, the system\nreflected Santos-Dumont’s uncanny ability to find elegant solutions to\nproblems that had defied a generation of aviators. According to Hoffman, the\nessential breakthrough came when Santos-Dumont realized that he needed to find\na way to “tilt the airship, raising or lowering its nose, so that the motor\nwould drive the balloon’s ascent or descent.” To do this, he would engineer “a\nsystem of movable weights by which the center of gravity of the airship could\neasily be shifted. The weights were merely two bags of ballast, one front and\none aft, suspended from the balloon envelope.” Santos-Dumont could move the\ndirigible’s center of gravity by pulling one of the two weights into the\npilot’s basket. “If the front weight was drawn in, the nose of the airship\nwould point up and if the aft weight was pulled in, the nose would point\ndown,” Hoffman explains.\n\nAs before, Santos-Dumont was willing to push the limits, including his own\nsafety. In one prototype he went so far as to replace the traditional balloon\nbasket with a bicycle saddle and frame, attached to a thirty-three-foot-long\nbamboo pole. In this configuration, people watching from a distance had the\nimpression that Santos-Dumont was flying “like a witch sitting on a stick”\nthat was tenuously held to the gasbag by an intricate network of cords. Not\nyet satisfied, he later decided to utilize piano strings instead of ropes and\ncords to suspend and reinforce the keel of his airship. This significantly\nreduced the airship’s weight, and its air resistance.\n\nAt this point Santos-Dumont’s dirigibles had become very complex machines, in\nwhich a multitude of ropes, handlebars, and even bicycle pedals controlled\ndistinct piloting maneuvers. His dirigible was an unorthodox concoction of a\ncigar-shaped gasbag, a bamboo keel, a mesh of piano strings, two petroleum\nengines, a triangular silk rudder, and a system of shifting ballast bags. It’s\nno wonder his aerial acrobatics in this engineering hodgepodge attracted\nattention, and that by the turn of the century he had become one of the most\nwell known personalities in the world. Though he freely disseminated the\nblueprints for his airship, the collective memory of the press and Parisians\nserved as the only record of his scientific experiments. In fact, Santos-\nDumont never believed in patents. Instead, he said that his discoveries\nbelonged to the whole of humanity.\n\nNever shy about showing off his latest inventions to the public, Santos-Dumont\nregularly tested his prototype flying machines in the skies above Paris,\nalways wearing a freshly pressed suit, a tightly knotted tie, and his favorite\nvapor-sculpted Panama hat. What Santos-Dumont desperately needed to silence\nhis remaining critics was proof that this airship would fly under his control\nand decisively free the human body from the purely terrestrial habitat in\nwhich it had lived for millions of years.\n\n* * *\n\nHis mission to demonstrate the possibility of man-controlled flight was\nsuddenly boosted when the oil tycoon Henri Deutsch de la Meurthe issued a\nchallenge to his fellow aeronautics enthusiasts. During a meeting of the Paris\nAero Club in April 1900, Deutsch announced that he would pay, out of his own\npocket, the sum of one hundred thousand French francs to the first airship\nthat could take off from the Parc d’Aerostation of the Aero Club at Saint-\nCloud and, without touching the ground, circumnavigate the Eiffel Tower and\nreturn to the departure point using only resources available on the airship in\nno more than thirty minutes. As estimated by Hoffman, to win this first\nofficial international prize in aviation, an airship would have to fly at\nabout fourteen or fifteen miles per hour to cover the round-trip journey—a\nhurdle that would put even Santos-Dumont, the only credible contender for the\npurse, to the test.\n\nAlthough many believed Deutsch’s original intention was to win the prize\nhimself, Santos-Dumont never doubted that the honor was his to lose. As a\nmatter of fact, just before the prize was announced, the Brazilian had\nreceived permission from the Aero Club of Paris to build the infrastructure\nneeded to fly his dirigible out of the Saint-Cloud airfield. He equipped\nhimself with an extensive workshop for fabricating the airship’s parts and a\nhydrogen plant for generating the balloon’s gas. He also erected a large\naerodrome and a balloon hangar—the first hangar ever constructed. The hangar,\ncomplete with movable doors that he had, of course, invented, gave Santos-\nDumont a tremendous advantage over his competitors for the Deutsch Prize. He\ncould now store his balloon inflated, saving the time (and money) involved in\nfilling it with gas before each and every test flight.\n\nFor the next eighteen months, he flew all over the city, experimenting until\nhe converged on the final design of what became No. 6. To get there, he had\nsurvived a couple of harrowing accidents. In the first, which occurred in\nJuly, his airship fell from the sky into the garden of Edmond de Rothschild’s\nestate. The tree that had luckily broken the balloon’s fall also proved to be\na hazard. But a month later, he faced a much more serious danger. After\ncrashing his dirigible into the side of the Trocadero Hotel, he found himself\nhanging for his life along some of the airship’s mercifully resistant piano\nstrings.\n\nNot even this brush with death would deter the little Brazilian from pushing\nforward. Mere details seemed to separate him from a prize most of Paris\nalready believed he had earned through sheer courage. Thus, by the time the\nAero Club judging committee was summoned to observe yet another attempt at\ncircumnavigating the Eiffel Tower on the afternoon of October 19, the only\nquestion that lingered was whether he could make the trip in thirty minutes or\nless. The winds of Paris were unpredictable, a nemesis that had caused him\nmany headaches in his tests. But regardless of the conditions, his dirigible\nwould have to perform at the outer limit of its speed, aerodynamics, and\nsteering capabilities. Plus, he would have to survive the journey, an\nuncertain prospect after his experience on the wall of the Trocadero Hotel.\n\nAfter an aborted attempt, Santos-Dumont’s solo voyage into history began\nprecisely at 2:42 P.M., as No. 6 took off from the Parc Saint-Cloud and headed\nstraight toward the Eiffel Tower. As the dirigible became visible in the\ncity’s sky, Parisians of all ranks and social position, without any ceremony,\ndropped whatever they were doing to secure a better vantage point for\nfollowing the intrepid aviator’s latest attempt. The rush of bodies and\nemotions produced “a wild stampede of foot-passengers, cabs, automobiles, and\ncyclists racing towards the Champs of Mars.”\n\nStill, according to Hoffman’s reconstruction of those epic minutes, at one\npoint the members of the French Twenty-fourth Regiment Band, who were marching\non the Champs-Élysées and “serenading the visiting King of Greece and five\nhundred other dignitaries,” heard the crowd’s roaring chants of “Santos-\nDumont, Santos-Dumont.” Without hesitation, and taken by a synchronous\ncollective reassurance that witnessing history more than justified the court-\nmartial penalty for insubordination, they unceremoniously dropped their\ninstruments and joined the running masses. About five thousand Parisians had\narrived at the Trocadero Gardens by the time Santos-Dumont began to contour\nthe “[Eiffel] Tower’s lightning rod at a precariously close distance of forty\nfeet.” Aside from some slight trouble with a wind current when No. 6 crossed\nthe Seine, the incoming flight had been faultless. He had even set a new speed\nrecord for that part of the circuit, at eight minutes, forty-five seconds. Not\nhaving a watch to check on his timing—something his friend Louis Cartier\nsolved a few months later by designing the very first wristwatch for\nhim—Santos-Dumont relied on the cheers of the crowd to judge his progress and\nwhether he was still within range to finish the trip in time. The moment he\ncleared the Eiffel Tower (see [Fig. 9.1](part0013.html#fig9-1)) and shot back\ntoward Saint-Cloud, the huge crowds in the streets threw hats into the hair\nand hugged their fellow bystanders. The final dash of humanity to conquer the\nforbidden skies had begun.\n\n![image](../images/00034.jpeg)\n\nFIGURE 9.1 Alberto Santos-Dumont and his flying machine. A photograph of the\naviator (on the left) and the historic moment when he circumnavigated the\nEiffel Tower on October 19, 1901. (Reprinted with permission from the National\nAir and Space Museum, Smithsonian Institution Archives, Washington, D.C.)\n\nDespite the buoying enthusiasm below him, Santos-Dumont found the returning\nleg to be nothing close to trivial. Slowed dramatically by a strong headwind\nand three engine stalls in a row—each of which Santos-Dumont calmly fixed,\nmidair, while still maintaining control of the dirigible direction—No. 6 lost\nprecious time. As he began his approach to the aeropark, he decided to make a\nfinal dive toward the ground. The ultimate seconds of the voyage were\nexhilarating. According to Hoffman:\n\nThe official timekeeper clocked twenty-nine minutes and fifteen seconds.\nAnother minute and twenty-five seconds passed while Santos-Dumont turned the\nairship around and brought it back to the starting point, where his workmen\ngrabbed the guide rope and reeled him in. When his basket was low enough for\nhis voice to be heard over the applause, he leaned over the side and yelled,\n“Have I won the prize?”\n\nHundreds of spectators responded in unison, “Yes! Yes!” and swarmed the\nairship. [Santos-Dumont] was showered with flower petals that swirled like\nconfetti. Men and women cried. The Comtesse d’Eu [the former Empress of\nBrazil, then living in exile in Paris] dropped to her knees, raised her hands\nto the heavens, and thanked God for protecting her fellow countryman. The\nCountess’ companion, the wife of John D. Rockefeller, squealed like a school\ngirl. A stranger presented Santos-Dumont with a small white rabbit, and\nanother handed him a steaming cup of Brazilian coffee.\n\nUnfortunately, the Aero Club committee could not ratify Santos-Dumont’s\nvictory at once, since—under a small but pivotal change in the prize rules\nenacted barely a month before—the timekeeper’s clock could only be stopped\nwhen the dirigible’s guide rope was grabbed by the workmen on the ground, not\nwhen the airship crossed the departure line. Santos-Dumont claimed that he had\npurposely overshot the line to demonstrate his disregard for what he\nconsidered a blatant attempt to make the task ever more difficult.\n\nAt long last, on November 4, the Aero Club committee awarded the Deutsch Prize\nto Alberto Santos-Dumont. Upon receiving it, he immediately donated half of\nthe money to the poor of Paris, thirty thousand francs to his workmen, and\ntwenty thousand francs to his most enthusiastic supporter, the mathematician\nEmmanuel Aimé. In the days that followed, Santos-Dumont acquired a hero’s\nstatus, an emblematic adventurer of the new century and the anticipated new\nworld order that would come with it. What that order would bring, nobody could\ntell. But at least one thing was now sure: the twentieth century would have\nmachines that allowed people to fly at their own will and under their\nvoluntary control. And thanks to the introduction of the telegraph and the\ntelephone, this newsworthy event could now be transmitted to people around the\nworld in a matter of hours.\n\nOver the following days and weeks, articles detailing all aspects of Santos-\nDumont’s accomplishments filled the press reports. Almost certainly, the news\nreached the secluded paradise of Kitty Hawk, North Carolina, where, during the\nfall of 1901, two brothers from Dayton, Ohio, had returned for one more season\nof aeronautical experiments. Unlike in the previous year, when their efforts\nwere restricted to playing with a particular type of kite, during the 1901\nseason on the Carolina coast, Orville and Wilbur Wright spent their entire\ntime flying a glider at the will of the wind. At no moment during that fall\ndid the Wright brothers achieve anything remotely close to a controlled\nflight. That did not happen for another two years, when on December 17, 1903,\nthe Wright brothers flew their heavier-than-air plane, Flyer I, from the crest\nof a sand dune on the same beach in North Carolina.\n\nIn my mind, there is no doubt that the Wright brothers deserve all due credit\nfor inventing and flying the first heavier-than-air plane. Yet, if we center\nthe question on who was the first to fly an airship that could be voluntarily\ncontrolled and navigated through the infinite skies, not as a simple and\npassive slave of the imposing and unpredictable moods of heaven’s wind, but\ninstead at the mercy of the voluntary motor desires of a pilot’s brain, there\nis no doubt that the honor belongs to the Brazilian. Santos-Dumont’s choice of\na lighter-than-air craft to make his demonstration simply reflected his\npreference to follow the early aviators, including Count Ferdinand von\nZeppelin, who in the summer of 1901 had preceded him in building a dirigible,\nbut had not found a way to control it.\n\nNeedless to say, the pioneering experiments conducted by Santos-Dumont, the\nWright brothers, and many others launched a technological transformation, one\nin which the scale of human transportation, exploration, communication,\ncommerce, and true social and cultural integration around the globe was vastly\nincreased. Unfortunately, so was the ability to wage war and commit horrendous\ncrimes on a scale never before witnessed. Among the millions of victims of\nthis transformation of airships into killing machines was none other than\nSantos-Dumont himself. After much emotional distress, in part from the\nknowledge that planes were being used by the Brazilian government to put down\na civilian insurrection in São Paulo, on July 23, 1932, he took his own life.\n\nLess than seven decades after Santos-Dumont circled the Eiffel Tower, the\nrevolution in human reach attained a new zenith when Neil Armstrong walked on\nthe surface of another celestial body on the very day that would have been\nSantos-Dumont’s ninety-seventh birthday. A few years later, the late Brazilian\nreceived an even greater gift when the International Astronomical Union named\na small impact crater, located in the Montes Apenninus range at the eastern\nedge of the vast lunar sea known as Mare Imbrium (the Sea of Rains), after\nhim.\n\n* * *\n\nSantos-Dumont’s flying machine illustrates in vivid and concrete terms an\nessential property of the human brain: namely, its ability to design,\nfabricate, and utilize tools to enhance its reach and the interaction of our\nspecies with the surrounding world. What neither Santos-Dumont nor his fellow\naviator pioneers realized is that, around the time they were starting to\nventure into the forbidden skies in their flying machines, the concept of tool\nassimilation by the brain’s “body schema” was starting to take shape in the\nminds of pioneering neuroscientists. In its earliest conception, posited by\nthe British neurologists Henry Head and Gordon Holmes in 1911, a “body schema”\nwas a creation of the human mind and, as such, this schema incorporated\ncommonly used artificial tools. Head and Holmes had observed that patients\nsuffering from lesions at different cortical levels of the somatosensory\nsystem experienced abnormal tactile sensations. In a presentation of their\nfindings to the Royal College of Physicians, they proposed that the body\nschema helped to account for the strange perceptions reported by their\npatients, as well as the rich range of tactile sensations experienced by those\nunaffected. “We are always building up a postural model of ourselves which\nconstantly changes,” they wrote. “Every new posture or movement is recorded on\nthe plastic schema, and the activity of the cortex brings every fresh group of\nsensations evoked … into relation with it.” Head and Holmes likened this\nprocess of comparison and translation to how “on a taxi-meter the distance is\npresented to us already transformed into shillings and pence.” In essence, the\nbody schema was the brain’s own point of view surrounding tactile information.\n\nHead and Holmes reinforced their hypothesis by looking at cases where an\nextensive lesion destroyed a section of the cortex holding the original body\nschema. In patients who had experienced a phantom-limb sensation for many\nyears, the loss of the schema eliminated the phantom.\n\nIf this was not shocking enough for their Victorian audience, Head and Holmes\npredicted that if the brain could maintain a nonexistent limb in its schema,\nthen surely it must adopt the trappings of the body. How else could it be\nexplained that humans had been able not only to master tools, but to feel\nthings through them, even without the use of other senses:\n\nIt is to the existence of these “schemata” that we owe the power of projecting\nour recognition of posture, movement and locality beyond the limits of our own\nbodies to the end of some instrument held in the hand. Without them we could\nnot probe with a stick, nor use a spoon unless our eyes were fixed upon the\nplate. Anything which participates in the conscious movement of our bodies is\nadded to the [brain] model of ourselves and becomes part of these schemata: a\nwoman’s power of localization may extend to the feather in her hat.\n\nThere is no indication in the society’s notes of whether Head and Holmes’s\nuninvited invasion of Victorian women’s fashion was welcomed. But they\nconcluded their paper on a similarly bold note, arguing that the somatosensory\ncortex is “the storehouse of past impressions,” out of which a schematic of\nreality was created, often below the level of consciousness. All of our\nsensations, they argued, “rise into consciousness charged with a relation to\nsomething that has happened before.” In fact, Head and Holmes did not simply\npropose that reality is born out of a body-centered cortical model of the\nworld but directly implied that the component of this model that simulates our\nown physical body is our brain’s ready capability to alter its spatial\nconfiguration by assimilating tools into the experience of its own flesh.\n\n* * *\n\nSince my first reading of their paper, I have always enjoyed the boldness with\nwhich Head and Holmes put forward their ideas. I was certainly not the first\nscientist to respond this way. As we saw in chapter 3, Head and Holmes’s body\nschema reappeared in the neuroscience literature several decades later, in the\nshape of the “neuromatrix” theory proposed by Ronald Melzack. In the broadest\nterms, these theories shared a rejection of the notion of pure perception,\nthat the neural representations of an animal’s body are exclusively defined by\nthe one-way feedforward information ascending from the periphery to the S1\ncortex. Both theories also assigned to widely distributed networks of\nfrontoparietal cortical neurons a central role in defining the familiar\nexperience of belonging to a body. Yet neither of these challenges to the\nlocalizationist status quo made much headway for the best part of the\ntwentieth century.\n\nA large reason for this lack of support originated from a very fundamental\nshortcoming: a corresponding lack of validating experimental data. As a matter\nof fact, nothing in Head and Holmes’s clinical cases directly supported their\nmain hypothesis.\n\nMoreover, as Angelo Maravita and Atsushi Iriki point out in their excellent\nreviews of the research on tool incorporation, Head and Holmes’s original\nconcept of a body schema was based on an unconscious integration made by the\nbrain of a sequence of proprioceptive signals alone. By so limiting the\nbrain’s simulation to information generated within the body, they left future\ngenerations of neuroscientists with a conundrum: how could we make sense of\nthe wealth of action potentials that seemed to arise from the large number of\nneurons in the frontal and parietal areas that exhibit multimodal receptive\nfields, which combine somatosensory, visual, and proprioceptive signals? One\nof the first steps forward was made by Ronald Melzack, even before single-\nneuron recordings could be taken in behaving primates, when he included\ntactile signals and motor activity in his neuromatrix. As both Maravita and\nIriki point out, the mounting evidence for multimodal RFs, coming out of more\nand more sophisticated recording techniques, opened the way for testing a\nmultimodal model of the self. In this model, “multiple fronto-parietal\nnetworks integrate information from discrete regions of the body surface and\nexternal space in a way which is functionally relevant to specific actions\nperformed by different body parts.”\n\nAt long last, sensory neurophysiologists had little choice but to realize that\nthere was no point in studying the perceptual capabilities of an animal in a\ndeeply anesthetized state. Because a brain’s representation of the body and\nits relationship to the space surrounding it requires a merging of concurrent\nvisual, somatosensory, proprioceptive, and motor signals, the new research\nparadigm insisted that studies had to be conducted in awake, behaving animals,\npreferably during experiments involving meaningful tasks. Once all the\nrelevant information was mixed with the mnemonic traces of past experience,\nthe subject’s brain would be able to make its best prediction about the always\nuncertain future—just like a human brain does in real life. And only under\nthese conditions could the neuronal circuitry of the brain generate the type\nof spatiotemporal patterns of activity that define the conscious experience of\nthe self, the abstract concept usually known as “body image.”\n\nIt was not until the late 1990s that the first experimental evidence was\nobtained to demonstrate that body image could be altered by introducing a tool\ninto a subject’s daily routine. In this innovative study conducted in 1996 by\nAtsushi Iriki and his collaborators at the Tokyo Medical and Dental\nUniversity, Japanese macaque monkeys were trained to use a simple rake to\ncollect a food pellet placed outside hand’s reach (see [Fig.\n9.2](part0013.html#fig9-2)). Despite not being known for using tools in their\nnormal, outside-the-lab lives, after a couple of weeks of training, these\nmonkeys became quite proficient in using the rake to grab and enjoy their\ntreat. After this initial phase, Iriki and his colleagues serially recorded\nthe activity of single neurons in the parietal cortex while the monkeys\nperformed their newly learned trick. From the beginning, the team observed\nthat some of the parietal cortical neurons exhibited both a somatosensory RF,\nlocated somewhere in the monkey’s hand, and an equivalent visual RF, centered\non the external space immediately surrounding the hand. In scientific jargon,\nthese neurons are called “bimodal cells” because they typically respond to\nstimuli from two distinct sensory modalities. Since the portion of the extra-\npersonal space adjacent to the limits of the body is usually known as peri-\npersonal space, the bimodal neurons identified by Iriki’s group primarily\nrepresented visual stimuli generated within the peri-personal space of the\nmonkey’s hand.\n\nTo their amazement, Iriki and his colleagues also observed that as the monkey\nmoved its hand to a new location in space, the somatosensory RF of the\ncortical neuron remained focused on the same skin region, whereas the visual\nRF migrated to represent the distinct peri-personal space that now surrounded\nthe animal’s hand. The neuron’s visual RF had instantaneously and properly\nupdated the hand’s position. No matter where the animal placed its hand, the\ncortical neurons would always keep the somatosensory and visual RFs in\nregister. Clearly, the animal’s hand position was the reference point used to\ndefine these neurons’ physiological properties.\n\nThat, by itself, would have counted as a spectacular neurophysiological\nfinding. Yet, what Iriki’s group found next was even more impressive. After a\nmonkey successfully used the rake to collect food pellets for about five\nminutes, the visual RF of the same bimodal cortical neurons suddenly enlarged\nto include the peri-personal space surrounding the entire tool, in addition to\nthe space around the animal’s hand. Plus, this dramatic enlargement of the\nvisual field only occurred when the monkey was actively using the rake (in\nthis case, to collect food pellets). If by chance the animal was simply\nholding the rake without actively using it, there was no change in the\nneuron’s visual RF (see [Fig. 9.2](part0013.html#fig9-2)).\n\nIn these studies, Iriki also described a second class of bimodal parietal\ncortical neurons whose somatosensory RFs were located in the shoulder of the\nmonkey. Before handling the tool, these neurons’ visual RF included the three-\ndimensional, peri-personal space that the animal’s unaided arm might encounter\nif it moved. However, following just a few minutes of playing with the rake to\ncollect food pellets, the visual RF of the same neurons enlarged to include\nthe entire potential, three-dimensional, peri-personal spatial range of\nmovement of the animal’s arm plus the rake. As Iriki and his colleagues\nrightly concluded, their data strongly suggested that the monkey’s brain was\nassimilating the rake as an extension of the animal’s arm. So precise was this\nassimilation that when Iriki measured the effect of rake usage on another\ngroup of bimodal cortical neurons whose tactile RF was located in the monkey’s\nfingers, he observed no change in the corresponding visual RF. Apparently, the\nmonkey would have to use a tool that required more specific finger movements\nfor the visual RF to be transformed. As far as I know, no one has yet recorded\nfrom the brain of a monkey playing the violin or a piano to test this\ninteresting hypothesis. Nevertheless, the prediction makes sense.\n\nIriki’s group continued to turn out milestone findings in the search for the\nneurophysiological correlates of tool assimilation in the primate brain. For\ninstance, in 2001 they showed that the same enlargement of visual RFs in\nbimodal cortical neurons could be obtained by projecting images of a rhesus\nmonkey’s hand on a video monitor while the animal manipulated objects without\nbeing able to directly see its hands, which were blocked from its view by an\nopaque barrier. Using this setup, Iriki found that the visual RF of cortical\nparietal neurons was centered on the video image of the animal’s hand and the\nperi-personal space adjacent to that image. When a virtual tool was placed in\nthe virtual hand, the visual RFs of these bimodal neurons expanded to\nencompass the virtual tool as expected. And when the size, position, and shape\nof the virtual hand were manipulated, an equivalent change in the visual RF of\nthose neurons was elicited. Iriki could make the neurons represent a monkey’s\nhand and a tool held by it in any way, shape, or form in which he configured\nthe virtual image on the monitor.\n\n![image](../images/00035.jpeg)\n\nFIGURE 9.2 Summary of the experiments carried out by Dr. Atsushi Iriki and\ncolleagues showing that the visual receptive field of a parietal cortical\nneuron expands when the animal employs a simple tool to perform a task. On the\ntop shelf, a single neuron with both a tactile and visual RF centered on the\nhand changed its visual receptive field to include the entire tool used by the\nanimal to retrieve some food reward. Notice that when the animal merely holds\nthe tool, but does not utilize it to perform a task, the visual RF remains\ncentered on the animal’s hand alone. On the bottom shelf, another neuron with\nthe tactile RF centered on the animal’s shoulder and a broad visual RF shows\nthe same expansion of the visual RF when the animal utilizes a tool in a 3-D\nspace. Notice that the visual expansion of the RF includes the entire space in\nwhich the tool can reach. (Adapted from A. Maravita and A. Iriki, “Tools for\nthe Body (Schema).” Trends in Cognitive Sciences 8, no. 2 [2004]: 79–86, with\npermission from Elsevier.)\n\nDespite these striking observations, there was one important question that\ncould not be answered: was the enlargement of the visual RF an adaptation due\nto repetitive tool use or effective tool use? This was the difference between\na rote and a proficient behavior, and in the latter case, the expanded RF\nmight be a necessary step for learning to use the tool properly. Although\nIriki had clearly shown that the cortical parietal neurons changed their\nvisual RFs to incorporate the tool and the space around it, his single neuron\nrecordings only began after monkeys had learned how to use the rake. As such,\nhe could not say when the change took place, whether during the monkeys’\ntraining phase or just as the monkeys mastered the task of retrieving the food\npellets. In addition, since Iriki could not selectively disrupt the activity\nof this particular class of bimodal neurons, he could not demonstrate a causal\nlink between the enlargement of visual RFs and the proficient operation of an\nartificial device. Consequently, one could still argue that the enlargement of\nthe visual RFs was a mere consequence of any tool use, no matter how skilled\nit was.\n\n* * *\n\nTo be fair to Atsushi Iriki and his collaborators, the task of establishing a\ncausal link between an observed pattern of brain activity and the production\nof a particular behavior is one of the most difficult challenges a\nneurophysiologist faces. In this specific case, however, the data on Aurora’s\nBMI, which we published a couple of years after Iriki’s paper appeared, shed\nsome light on this unanswered question.\n\nAs we saw in chapter 7, the velocity and direction-tuning profiles of\nindividual cortical neurons reveal two interesting classes of neurons that\ncould be found in every one of Aurora’s cortical areas. One of these neuronal\ngroups displayed velocity and direction-tuning properties that were somewhat\nsimilar when Aurora used either her own biological arm or the external robotic\narm to play the video game. The second class of neurons, though, only\nexhibited clear velocity and direction tuning when Aurora was controlling the\nrobot arm movements using her thinking alone. This enhanced neuronal firing,\nwhich showed clear and sharp velocity and direction tuning, occurred a few\nhundred milliseconds before the onset of the robot arm’s movements, which is\nwhy the BMI allowed Aurora to intercept the target with the cursor and win\ngallons of delicious fruit juice. Indeed, without these two classes of\ncortical neurons, our BMI simply could not work; the remaining neurons\nrecorded from Aurora’s cortex tended to shut down when she stopped making\nmovements with her arm. Interestingly, the moment we turned off the BMI and\nAurora returned to using her arm to play the game, the cortical neurons that\nfired prior to the movements of the robot stopped firing altogether. Unlike in\nIriki’s experiments, our recordings had been initiated from the very start of\nAurora’s training with the BMI.\n\nAs Aurora transitioned from moving her own arm to controlling the robot arm by\nthinking alone, the ninety-six cortical neurons we were simultaneously\nrecording displayed a three- to sixfold increase in the co-variance of their\nfiring, measured in sequential one-hundred-millisecond bins. This meant that\nthe direct operation of the robot arm by this random sample of neurons was\ncorrelated with a distinct, broad temporal pattern of similar firing among\nthese neurons. To make things even more intriguing, this effect was not\nrestricted to neighboring neurons. Instead, it extended to groups of cortical\ncells that were far apart, as if by reducing the variability of their\nindividual firing times, these neurons could create closely related, though\nspatially dispersed, circuits that shared a similar task. Yet again, the\nmoment the BMI was turned off and Aurora returned to using her own arm while\nplaying the video game, the neuronal firing covariance structure broke off,\ntoo, and the firing across the recorded sample returned to the original,\ntemporally dispersed pattern. This use of time to “glue” space appeared to be\na trick of the brain to allow widely dispersed populations of neurons to\ninteract, ever so briefly, during the generation of a fundamental exploratory\nbehavior.\n\nAlthough some neurophysiologists could argue that the combined findings from\nIriki’s and my own lab do not constitute definitive proof that observed\nchanges in cortical neuronal firing patterns and RFs determine the ability of\nmonkeys to become proficient tool users, our research certainly pointed in\nthat direction. We gained some extra support from a series of studies\nconducted with humans. For example, Lucia Riggio and her colleagues at the\nUniversity of Padua have demonstrated that humans can distinguish a tactile\nstimulus equally well, regardless of whether it is delivered at the finger’s\nskin surface or at the end of a long tool. In another study, conducted at the\nUniversity of Milan, Angelo Maravita and his team observed that a visual\ndistractor, such as a flashing light, interfered with receiving another\nsensory signal, in this case a tactile stimulation, regardless of whether the\nsubjects were using their hands or long tools to report what they felt. More\nrecently, a group led by Lucilla Cardinali at INSERM (the National Institute\nfor Health and Medical Research) and the Université Claude Bernard in Lyon,\nFrance, reported that using a mechanical grabber to perform a simple task\nsignificantly alters the kinematic properties of subsequent free-hand\nmovements. The study revealed that, after use of this tool, people misreported\nthe length of their own biological arms by indicating that touches\nsimultaneously applied to their elbow and middle finger were further separated\nthan in the period prior to tool utilization—implying that using the tool gave\nthe subjects a feeling that their arms had been physically extended. Can you\nimagine what arm length Roger Federer would report following an hours-long\nbattle employing his customized racket against his archrival, Rafael Nadal?\n\nFurther support for the thesis that tools readily become a part of our\ninternal, brain-based self-representation comes from a series of clinical\ncases involving patients who have suffered extensive cortical lesions. For\ninstance, Anna Berti of the University of Turin, and Francesca Frassinetti, of\nthe University of Bologna, both in Italy, report that, following a massive\nstroke, one patient had developed a huge lesion in the right cortical\nhemisphere, which limited the blood flow to a large area of the right parietal\nlobe. The patient developed a hemineglect syndrome, ignoring the leftmost side\nof both her body and the world—down to the leftmost letters in a word and the\nleftmost words in a sentence she was asked to read. Upon a closer examination,\nconducted a month after the cortical event, the neurologists discovered that\nthe patient’s hemineglect had evolved. When an object was placed about twenty\ninches away from the left side of her body, the patient could not perceive its\npresence; however, if the same object was placed farther away, about forty\ninches, the patient could suddenly sense it. The hemineglect had been\nrestricted to a spatial terrain quite near to her body.\n\nTo further characterize the hemineglect syndrome, the research team next\ntested the patient on a line bisection task. In this task, straight lines were\ndrawn on a piece of paper mounted on a board and placed in different spatial\npositions in front of the patient. Then the patient was asked to use her right\nindex finger to point to the middle of a straight line positioned in her\nneglected space. Typically, patients suffering from left spatial neglect\ndisplayed a rightward bias when trying to bisect a straight line positioned on\ntheir left side. In their examination, however, Berti and Frassinetti\nintroduced two new ways in which the patient could bisect the line, in\naddition to using her index finger: for nearby lines she could now use a laser\npointer, and for lines placed farther from her body she could use either a\nlaser pointer or a forty-inch-long stick. As predicted, when the thin black\nline was placed on the patient’s near-left extra-personal space, she misjudged\nthe middle point of the line, whether using her fingertip or a laser pointer,\nby placing it much farther to the right. Yet, when the line was placed in the\nfar-left extra-personal space, the patient could bisect the line flawlessly\nwhile using the laser pointer (as she did when she used her own right finger),\nbut committed the same rightward errors when she used the forty-inch-long\nstick. Berti and Frassinetti proposed that these latter errors occurred\nbecause, through the use and incorporation of the long stick, the patient’s\nbrain had somehow converted the far-left space into near-left space, a region\nin which the patient’s spatial neglect fully manifested itself.\n\n* * *\n\nAll together, the neurophysiological, psychophysical, and clinical findings I\nhave thus far described constitute a small sample of the vast scientific\nliterature that upholds, with a great deal of certainty, Head and Holmes’s\noriginal hypothesis of tool assimilation into the brain’s model of the body.\nAlthough many animals, from insects to mammals, exhibit some capability to\nutilize natural tools or artifacts to their advantage—a phenomenon Richard\nDawkins calls the “extended phenotype”—several million years of evolution\nendowed the human central nervous with the ability to outperform all other\nspecies. By harnessing a precious combination of cortical algorithms with the\ndexterous multimodal sensorimotor ability to grasp, reach, and manipulate a\ncreated tool and dynamically represent extra-personal space around an ever-\nmoving body, the human brain has produced the most adaptive and complex model\nknown to us: one’s sense of self.\n\nThe overall impact of this relativistic, brain-derived model is tremendous.\nOut of this explosive catalytic mix of mental creativity, motor dexterity, and\nunending assimilation of extra-personal space and artifacts, for the past few\nmillion years the human species has gradually climbed a very unusual\nevolutionary path. Far from merely allowing us to develop new technologies\nthat extend our individual and collective reach, enlarge our habitat, and\nenhance our means of producing food, surviving diseases, and enduring natural\ndisasters, this exuberant brain-based simulation assures that all of our\nfuture technological developments, whatever they may be, will be continuously\nand actively assimilated into the sense of self of each generation to come.\nThat may sound outlandish to some, but after spending the past two decades\nreviewing the accumulated findings of a century of neurophysiological\nresearch, I found this to be the only parsimonious way to describe how brains\ngenerate our ever-shifting sense of being. In fact, I would go even further.\nBecause of its unsurpassable and pragmatically effortless powers to employ and\ndisguise complex artifacts as extensions of our human flesh, the human brain\npossesses the only biological algorithm capable of wrestling away from our\ngenes a significant chunk of responsibility for defining the future of human\nevolution.\n\nIt will take a multitude of experiments and many years of debate to test the\nvalidity of my theoretical assertion, but the weight of the evidence collected\nthus far points like a beacon in this direction. For instance, we have already\nseen that experimental evidence supports the notion that one’s sense of self\nis not limited to the outermost layer of the epithelium. Instead, it likely\ncomprises the clothing, wristwatch, rings, socks, tie, gloves, shoes, hearing\naids, dental fillings, prosthetic limbs, pacemaker, glasses, contact lenses,\nfake nails, wig, dentures, artificial eyes, necklaces, earrings, bracelets,\nbody piercings, silicone implants, and every other add-on applied to or within\nthe body. Furthermore, one’s sense of self likely encompasses all the tools we\ncommonly, and not so commonly, utilize, directly or remotely, provided the\nmovements of these tools are somewhat correlated with the movements of one’s\nown body parts. For most people, one’s sense of self surreptitiously expands,\nover the course of a lifetime, to include technological tools with which we\nare actively engaged, such as a car, bicycle, motorcycle, or walking stick; a\npencil or a pen; a fork, knife, spoon, whisk, or spatula; a tennis racket,\ngolf club, basketball, baseball glove, or football; a screwdriver or hammer; a\njoystick or computer mouse; and even a TV remote control or a BlackBerry, no\nmatter how weird that may sound.\n\nFor a subset of people with specialized proficiency, the self can enlarge to\ninclude a musical instrument, say a violin, piano, flute, or guitar; a medical\nimplement such as a scalpel or a microelectrode; or a transportation device\nalong the lines of a sailboat, backhoe loader, lunar capsule, or plane. Which\nis why Alberto Santos-Dumont’s adventures with his balloons, dirigibles, and\nplanes can tell us a great deal about tool assimilation.\n\nAccording to Santos-Dumont’s own written and oral reports, by continuously\npulling, with his own hands, the many ropes that controlled the components of\nhis dirigibles, and immediately experiencing the airship’s steering responses\nas he traveled through the Parisian sky, he began to feel as if the airship’s\nmovements were his own. That was quite a departure from the sensation he had\nexperienced in his previous flights as a passenger in the basket of a\ntraditional balloon that moved at the mercy of the wind. Santos-Dumont’s sense\nof self became so entangled with his flying machines that, later on, when he\nwas working in 1908 on the blueprints for the Demoiselle, the most reliable\nsingle-pilot heavier-than-air plane of its time, he made sure to create ways\nin which different parts of his body could be directly attached to the\nspecific controls of his beloved new airship. Thus, by simply moving his body\nback and forth or laterally, he could steer his plane. Knowing what we know\ntoday, one can speculate that Santos-Dumont began to experience the first\nclosed-control feedforward/feedback loop among millions of interconnected\nhuman neurons and a flying machine, and that in the process his brain\nincorporated the entire plane into its image of his petite body. The exquisite\nlevel of tactile sensitivity that Santos-Dumont experienced most likely\nrivaled the mastery of the Formula 1 racers Ayrton Senna and Nelson Piquet,\nwho said they were able to detect minute changes in the asphalt surface of a\nracetrack while driving their cars at more than 150 miles per hour. A similar\nprocess may explain why Pelé rarely looked directly at a moving soccer ball\nbefore initiating a dribble, pass, or commanding shot into the goal. The brain\nof the greatest of all soccer players had long assumed that the football was\nnothing but an extension of his moving feet (see [Fig.\n9.3](part0013.html#fig9-3)).\n\n![image](../images/00036.jpeg)\n\nFIGURE 9.3 The left panel shows Pelé in one of his characteristic shooting\nmaneuvers in the 1960s. On the right, a drawing represents how the theory\nproposed in this book predicts what Pelé’s sensorimotor cortex would look\nlike. According to this view, the soccer ball would be incorporated into\nPelé’s foot representation in the cortex. (From Swedish press image in the\npublic domain.)\n\nThese master tool wielders incorporated their tools into their brains’ body\nimage. Thus, as Santos-Dumont flew No. 6 or his “petite Demoiselle,” and as\nPelé took possession of the ball in each of the 1,363 professional games he\nplayed, each man’s brain newly adopted the respective tools into his body\nschema, adjusting his sense of self and the related sensory receptive fields\nin real time. This process encompasses yet another neurophysiological\nprinciple of the relativistic brain, the plasticity principle.\n\nTHE PLASTICITY PRINCIPLE\n\nThe representation of the world created by populations of cortical neurons is\nnot fixed, but remains in flux, throughout our lives, continuously adapting\nitself to new learned experiences, new models of the self, new simulations of\nthe outside world, and newly assimilated tools.\n\nThe plasticity principle encompasses all of the mechanisms of cortical\nreorganization that underlie the ability of animals to learn new tasks,\nincluding the incorporation of artificial tools as expansions of the internal\nmodels of one’s self. As such, it constitutes one of the main reasons brain-\nmachine interfaces actually work—the brain does not discriminate between a\ntool that is, for example, sensed in a biological hand and a virtual one.\n\n* * *\n\nAt this point, I have to admit that my focus so far on the tools incorporated\nby the brain does not completely satisfy me. In my sincere opinion, one’s\nsense of self can reach far beyond these limits. Although the experimental\nevidence remains scant, I firmly believe that, in their perfectionist drive to\nachieve the ultimate simulation of the self, our brains also incorporate, as a\ntrue part of each of us, the bodies of the other living things that surround\nus in our daily existence. The refined neural simulation routine I am\nproposing may be better understood if we call its final product by its more\ncolloquial and well-publicized name: love.\n\nThink about how love, and its more intense incarnation, passion, arise in us.\nThere’s love at first sight, the murmur of sweet nothings, a mother’s tender\nembrace. Each of these involves classical sensory channels (visual, auditory,\nand tactile). There’s also the cascade of hormones that come with the ever-\nsensitive chemical senses, olfaction and taste, the examples for which I will\nleave to your extraordinary human imagination. The brain receives a continuous\nflow of multimodal signals in each of these situations and strives to\nincorporate that flow into its existing model of reality and its sense of\nself, built on its prior experiences, just as it would when faced with the\nfeedforward and feedback information of a video game joystick or a BMI. In\nthis approach to defining the relativistic brain, one’s sense of self would\nhave to include parents, spouses, and children, and to a lesser degree,\nrelatives, friends, and perhaps minor social acquaintances. Even our pets\ncould be part of this list.\n\nThere are some indirect hints, obtained by the investigation of social\nbehaviors in our species as well as other mammals, that lend some initial\nsupport for this far-fetched idea that the brain assimilates other living\nbodies into its internal image. Take, for example, the behavior of the North\nAmerican prairie vole. When a young adult of this species encounters a mate it\nreally likes, its brain tends to release a great deal of dopamine, a molecule\nthat mediates a strong, pleasant, and rewarding sensation. After this initial\npassionate meeting, a typical prairie vole establishes a very strong social\nbond with its partner, leading to a relationship that usually lasts a whole\nlife. Thus, despite the fact that both female and male prairie voles continue\nto engage in fortuitous sexual escapades with a variety of other individuals,\nthey maintain a much stronger bond with a particular companion with which they\nlive. Further studies have shown that when prairie vole couples are in direct\nsocial contact, they produce high levels of the hormone oxytocin, the same\nchemical released when women breast-feed their infants. Once released,\noxytocin can bind to specific receptors located in the limbic areas of the\nbrain and induce dopamine release. Consequently, a well-acquainted prairie\nvole couple is likely to experience a very pleasant, long-lasting feeling of\nreward, something they do not necessarily experience in their briefer\nencounters with occasional sex partners. Interestingly, blocking the oxytocin\nreceptors in a female vole who has just delivered a litter makes it “socially\nblind” to its new offspring. It follows, therefore, that if oxytocin reception\nwas blocked in a prairie vole couple, the animals would break their bond and\nfill their lives with a never-ending series of one-night stands.\n\nWhile that’s all well and good for prairie voles, brain imaging studies\ncarried out in young human couples who report being in the early stages of a\npassionate romantic relationship have revealed a similarly intense activation\nof dopamine-rich areas of the brain. Moreover, it is now known that oxytocin\nis released when people embrace their loved ones, such as a spouse or child,\nwhen couples have sex, and even when one of us meets up with a close and\nbeloved friend. Oxytocin release may also contribute to the pleasant feeling\npeople experience when they caress a pet or enjoy a massage. These and other\nobservations have raised the possibility that oxytocin, among other hormones,\nplays a key role in intermediating social human bonding through a sequence of\npositively reinforcing, hedonic responses. These responses would be initially\ntriggered by behaviors that elicit physical body contact—things like hand\nholding, kissing, hugging, and sex—or social encounters with an object of\ndesire. Then, the hormones and chemicals released by the brain would create an\nintensely pleasant sensation, eventually establishing a long-lasting bond.\nThis bond would be nurtured by the brain’s simulated reality, until it became\nintegrated as part of the very model that defines one’s sense of self.\n\nIn my view, this perceptual-chemical transduction mechanism for body\nassimilation and plasticity could define a causal chain of events through\nwhich the human brain expands its neural model. Surprising as it may sound,\nthis would imply that our sense of self also encompasses a vivid\nrepresentation of the social network of individuals with whom we share our\nlives, a true amalgam of bodies that is actively and dynamically maintained in\nneuronal space by a crowd of touches, embraces, kisses, and caresses\ndistributed to those we love. This might even explain in neurophysiological\nterms why it is so painful to cope with the end of a romantic relationship or\nthe death of a loved one. Basically, I propose that such a terrible, all-\nconsuming pain emerges because, for our ever-meticulous modeling brains, such\na loss truly represents an irrevocable abdication of an integral part of the\nself.\n\nBut is the assimilation of other living beings the final limit to which one\ncan stretch the sense of self? Precarious as this may sound, I believe the\nanswer is a resounding no. The emergence of brain-machine interfaces, in\ncombination with new technologies for remote operation, or tele-operation, of\nthe most varied sort of mechanical, electronic, and virtual tools located at\nconsiderable distances, or even in different spatial scales, from the physical\npresence of their biological operators, all but guarantees that the unique\nability of the human brain to assimilate the tools it creates has the\npotential to expand the limits of the self to domains never before visited by\nany representative of our species.\n\nWhat I have in mind goes much further than Aurora’s ready assimilation of a\nrobot arm, located a few yards away, or Idoya’s assimilation of a pair of\nrobotic legs, located on the other side of the planet. For instance, could the\nself be extended so that it would be possible for it to experience the\nunfamiliar surface of another planet through a mechanical device that has been\nsent there years, decades, or centuries earlier? It is at least theoretically\nconceivable that such an inconceivable frontier may be crossed in a generation\nor two. And when the time comes, it is plausible—in fact, it is almost\ncertain—that our grandchildren will not comprehend why the earthbound\ngenerations that preceded them thought it was so stunning to wander the red\ndunes of Mars, and feel the cold, flaky sand being pushed underfoot, from the\ncomfort of a chair in their own terrestrial living rooms.\n\n\n10\n\nSHAPING AND SHARING MINDS\n\n“Has anyone tried this before and survived peer review to tell the story?”\nThrough an irritating muffling caused by the international phone call’s\nbackground noise, I could detect some polite hesitation in certifying my\nlatest unconventional idea. Having stayed awake the whole night sketching out\nan experimental setup I had been ruminating over for months, I now faced the\nreal struggle: getting someone else to believe my strategy would work.\n\nIt wasn’t going to be an easy sell.\n\n“Do you really think a brain-to-brain interface can be built? Connecting two\nliving brains? Hard to believe!” Although my interlocutor seemed mildly\nalarmed, he was not quite ready to move to safer ground, like, for instance,\nour favorite topic: the status of the Palmeiras soccer team. That was my first\ngood sign.\n\nAs I had done many times in the past thirty years, I was testing the waters\nwith my childhood friend Luiz Antonio Baccalá. This time, however, I had not\nchosen this protocol purely out of safe habit. An accomplished and\nintellectually gifted electrical engineer with a PhD from the University of\nPennsylvania, Baccalá can dissect the most complex scientific theory with a\ncrystalline sharpness.\n\n“I will fax you some drawings of what I have in mind,” I replied. Now my\nfriend knew I was not joking. Drawings had always been a trial for me. But I\nhad realized they would be the fastest way to convey my ideas for a brain-to-\nbrain interface.\n\n“No problem. Send me the drawings. I will let you know what I think as soon as\nI have some time available. I may have time this weekend. Or maybe not. I will\nsee what I can do.”\n\nAs the call ended, without a single mention of Palmeiras’s customary\nmistreatment of its loyal fans, I grasped that Baccalá was intrigued.\n\nOther people had toyed with the idea of connecting the minds of two people.\nFor instance, in his 1994 book The Quark and the Jaguar, the Nobel\nPrize–winning physicist Murray Gell-Mann wrote:\n\nSome day, for better or for worse … a human being could be wired directly to\nan advanced computer (not through spoken language or an interface like a\nconsole), and by means of that computer to one or more other human beings.\nThoughts and feelings would be completely shared, with none of the selectivity\nor deception that language permits.… I am not sure that I would recommend such\na procedure at all (although if everything went well it might alleviate some\nof our most intractable human problems). But it would certainly create a new\nform of complex adaptive system, a true composite of many human beings.\n\nHaving convinced myself that Gell-Mann’s fear was unjustified, and that in the\nfuture such a technology could be extremely beneficial to humanity (see\nchapter 13), I devoted a great deal of effort to creating a legitimate\nstrategy for testing a true brain-to-brain interface. Now I just awaited\nBaccalá’s seal of approval to unleash the project.\n\nThe following Monday, I spotted an e-mail message, sent early that morning,\nfrom Dr. Luiz Antonio Baccalá, professor at the Escola Politecnica, University\nof São Paulo: “Call me at once.” Besides this urgent plea, nothing else was\nwritten.\n\nAs usual, he did not answer his cell phone. After a few tries, I got lucky and\nreached him at his office. Sounding very irritated, he started by explaining\nthat he did not have much time to talk; he was busy grading student exams. But\nhe needed to tell me something important. He then fell into a deep silence.\nThe next few moments stretched unbearably.\n\n“What is it?” I begged.\n\n“It is crazy, in the good sense of the word: highly unpredictable, disruptive,\nbut extremely attractive. If it works, nothing will be the same in your field.\nIf it does not work, there’s nothing to lose, except perhaps your hard-earned\nreputation. But that is meaningless compared to what will happen if it does\nwork.”\n\nComing from Baccalá, such an unconditional vote of support meant a lot. He had\ncarefully examined the drawings, analyzed the technical issues. For him, the\nlogic of the experiments was sound. And that was all I needed to hear.\n\n* * *\n\nContemporary neuroengineering is quickly approaching the ability to link two\nand even many brains together. As we have seen, the successful operation of a\nBMI dedicated to controlling the movements of a machine requires the\nimplementation of two concurrent components: one that samples brain activity,\nextracts voluntary motor information from it, and redirects the resulting\ncommand signals to an artificial device (the efferent component), and another\nthat provides feedback information describing the performance of the actuator\nback to the subject’s brain (the afferent component). Most of my descriptions\nof BMI experiments so far have focused on the first component and the ways in\nwhich brain-derived signals could be employed to extend the reach of the human\nbrain through the assimilation of artificial tools. In most of these examples,\ndirect or remote visual feedback from the machine’s movements was used in the\nsecond component of the BMIs. Although vision plays a fundamental role in the\nbrain’s natural process of tool incorporation, BMIs that rely on other sensory\nmodalities have been built and operated. In fact, the most successful\nneuroprosthetic device ever built is the cochlear implant, which has already\nallowed tens of thousands of deaf patients worldwide to regain a significant\nfunctional level of hearing. The implant employs electrical stimulation of the\nremaining fibers of the auditory nerve to elicit its clinical effects.\n\nIf anything, it is fair to say that visual feedback became the preferred\nchoice for feedback in BMIs because it is easy to implement in a laboratory\nsetting. Nonhuman primates handle visual feedback very efficiently and exhibit\nno difficulty interacting with video screens. But there is no reason why other\nsensory modalities should not be equally utilized. In fact, in recent years,\nNathan Fitzsimmons and another graduate student at Duke, Joseph (Joey)\nO’Doherty, have shown that tactile stimulation of a monkey’s skin can readily\nsubstitute for visual feedback as the afferent component of an upper-limb BMI\nlike the one operated by Aurora. For instance, in the presence of ambiguous\nvisual information, monkeys easily learn how to use tactile cues to decide in\nwhich direction to move the robotic arm being controlled directly by their\nthinking.\n\nStill, such a method of delivering feedback information relies on the highly\nspecialized sensory apparatuses of the body. It’s hard to put forward a\ncredible claim that BMIs thoroughly liberate the brain from the body when\nthat’s the case. To truly push beyond these limits, one would have to find a\nway to implement the BMI’s afferent feedback component without any\nintermediation by the body’s peripheral sense organs.\n\nIt so happens that a few key modifications of electrical brain stimulation—the\ntechnique used by Eduard Hitzig and Gustav Fritsch when they discovered the\nmotor cortex in 1870, and one of the most common experimental approaches used\nby neurophysiologists in the past century—offer a very convenient starting\npoint for solving this dilemma. In our first attempt to create a brain-\nmachine-brain interface (BMBI) for motor control, we decided to adapt this\nmethod to communicate directly with the brains of our monkeys and then\ninvestigate whether they could learn to decode simple instructional or sensory\nfeedback messages delivered directly to the cortex. Though we were using a\ntypical BMI at this stage, we adopted the term brain-machine-brain interface\nfrom a 1969 study that first outlined a device that would allow bilateral\ncommunication between a subject’s brain and an artificial actuator without any\ninterference from the subject’s body. The original implementation involved an\nautomatic interaction between two subcortical brain areas, mediated by an\nanalog computer. Our BMBI required the subject to voluntarily control the\ndevice through a well-defined motor task.\n\nMore than a hundred years of electrical brain stimulation experiments offered\nnot only inspiration but also a vast repertoire of practical tips in applying\nelectrical brain stimulation to our problem. After all, most leading\nneuroscientists, including Sir Charles Sherrington, Sir Edgar Adrian, and\nWilder Penfield, had tinkered, in one way or another, with electrical\nstimulation to probe different parts of the central and peripheral nervous\nsystem. Yet none of these giants took the technique as far as the largely\nforgotten Spanish neurophysiologist José Manuel Rodriguez Delgado, who\ndeserves most of the credit for launching the modern era of using chronic\nbrain implants in freely behaving animals and humans from his lab at Yale\nUniversity.\n\nIn one of his favorite experiments, carried out in 1969, Delgado demonstrated\nthe automatic operation of the first bidirectional BMBI using Paddy, a female\nrhesus monkey, and a tiny device he had invented, the “stimoceiver,” which\nallowed wireless radio transmission of electrical signals between the brain of\na freely behaving subject and a machine. Because of its size, multiple\nstimoceivers could be implanted at the same time, so distinct brain regions\ncould be simultaneously recorded and stimulated. In his experiments, Delgado\nrelied on chronically implanted EEG-recording electrodes to sample the\nelectrical activity produced by neurons located in the almond-sized deep brain\nstructure named the amygdala that appears to be involved in regulating\nemotions. The stimoceivers relayed the amygdala’s raw brain signals to an\nanalog computer, located in a room adjacent to Delgado’s lab. By programming\nthis computer to detect a particular pattern of rhythmic brain activity, the\nso-called amygdala spindles, that resulted from the activation of a coherent\nensemble of amygdala neurons, Delgado set a clear criterion for triggering the\nfeedback component of his BMBI: every time an amygdala spindle was detected,\nthe computer sent a radio signal to the stimoceiver instructing it to trigger\nan electrical stimulation of a separate brain area, a portion of the monkey’s\nreticular formation, which he had previously identified would inflict a\nnegative reinforcement mechanism.\n\nThis cunning arrangement allowed Delgado to set the BMBI in motion and simply\nobserve how the interactions between these two subcortical brain areas,\nmediated by a machine, played out. To his astonishment, just a few hours after\ninitiating the BMBI, Delgado observed a 50 percent reduction in amygdala\nspindle activity. Over the next six days, Paddy spent two hours each day\ninteracting with the BMBI. By the end of the period, her amygdala spindling\nhad been reduced to an incredible 1 percent of its normal levels. At this\npoint Paddy became quieter, withdrawn, and less motivated to participate in\nfurther behavioral testing, and so Delgado discontinued the experiment. Within\na few days her amygdala spindles, as well as her cheerful demeanor, bounced\nback to normal levels. This led Delgado to predict that physicians in the not-\nso-distant future would directly link the human brain with computers to treat\nneurological disorders.\n\nSadly, it wasn’t long before Delgado and his research met with scientific\nostracism. According to a 2005 Scientific American article by John Horgan,\nDelgado evinced strong, almost visceral antagonism from his peers and the lay\npublic alike. Not particularly known for his subtlety, he certainly did not\nshore up his reputation by choosing Physical Control of the Mind: Toward a\nPsychocivilized Society as the title of the book he published in 1969 to\nsummarize his experimental findings. But it was his particular vision for the\nfuture of brain implants—using them to modulate physiological and pathological\nbehaviors in both animals and humans—that stoked fear that neuroscientists\nwere gaining the knowledge and technology necessary to craft an effective\nmethod for mind control. After all, he was working at the peak of cold war\nparanoia, and any conspiracy theory, particularly one involving a scientist\nmessing with someone’s mind, sounded terrifyingly plausible. Had the\nscaremongers read Delgado’s book, however, they would have learned that his\nexperiments had daringly explored using intracranial electrical stimulation to\nprobe cortical and subcortical circuits, primarily for the purposes of\nacquiring basic knowledge of the brain and, perhaps eventually, developing\nclinical therapies for severely ill patients. There is no doubt, though, as\nHorgan notes, that Delgado was fascinated with the possibility of finding a\nway to communicate directly with the human brain.\n\nI still recall the day, in the fall of 1994, when I first pulled Physical\nControl of the Mind off a library shelf that seemed to be visited exclusively\nby spiders and the occasional homeless termite. Having just begun my career as\nan assistant professor at Duke, I had decided it was about time that I read\nsome of the classic books of neuroscience. My curiosity had been stirred when\nI heard that Delgado had staged an experiment on the inhibitory behavioral\neffects of electrical brain stimulation in a bullfighting arena, of all places\n(see [Fig. 10.1](part0014.html#fig10-1)).\n\nThis fantastic setting is brought to life through a quick sequence of black-\nand-white photographs taken at a ranch in Cordoba, Spain. A lean, mean bull,\nwhose ancestors had been carefully bred for many generations to enhance a\nsingle personality trait—a ferocious dislike of men holding red capes—plays\nout its scripted role, charging at full speed at the neurophysiologist who, at\nfirst glance, seems to be armed only with the typical red dress cape worn by\nthe archetypal matador, the gold-swathed torero who, in Bizet’s immortal\nopera, manages to steal Carmen from Don José, every time.\n\nInitially, the enormous bull stands at the edge of the arena, aiming its\nlethally sharp horns at Delgado, who stares at his experimental subject\nattentively while valiantly holding his red cape with his right hand. In his\nleft hand Delgado holds an object that had likely never before been seen in a\ncorrida de toros: a radio-like device sporting a long antenna. Along the\narena’s wooden ring sits a mysterious helper, who displays no tense sign of\nconcern or distress about the events that are about to unfold. The bull next\nlaunches its seemingly irreversible charge, its horns pointed directly at the\namateur torero’s torso. Nothing can prepare one for the relief and surprise\nwhen the next snapshot reveals the bull sliding to a halt, just a couple of\nyards away from Delgado who, a mere second earlier, has wisely dropped his\nuseless cape and, without releasing his eyes from the hurtling bull, shifted\nhis entire motor will (and his prayers) to the task of pressing a button on\nhis strange device. The now tamed bull turns away from Delgado, who can be\nseen waving the shamed animal away with his right hand. It’s then that one\nappreciates that the helper has hardly moved a muscle throughout the entire\nframe-by-frame account, betraying that this was, to him, a rather pedestrian\nexhibition of a well-rehearsed trick. If it had not been, as Dr. César Timo-\nIaria liked to say, the historical record of that amazing experimental\nencounter would have exposed a trembling graduate student, rather than\nDelgado, as the courageous torero in custody of the radio-like equipment.\n\nNo matter who was controlling the device, Delgado had found a way to stop a\ncharging bull, just moments before the animal was set to gore him, apparently\nby pushing a single button. That, by itself, looked incredible. In fact, what\nDelgado demonstrated that day, in a rather extravagant way, was that by\nelectrically stimulating particular regions in the bull’s brain, including the\nstriatum of the basal ganglia, a major conduit for motor information, he was\nable to induce a state of “motor behavior arrest” in the animal. Being an\ningenious technologist, he had used a radio-wave frequency to activate a\nstimoceiver he had previously implanted in the bull’s brain.\n\n![image](../images/00037.gif)\n\nFIGURE 10.1 The neuroscientist in the bull ring. This sequence of photographs\nillustrates the classical experiment carried out by Dr. José Delgado in which\na charging bull was made to stop its charge using deep electrical brain\nstimulation. (Photographs used with permission of Dr. José Delgado.)\n\nDelgado’s unorthodox experiments with electrical brain stimulation did not end\nin the bullfighting arena. He was the first to study how the use of electrical\nstimulation to contain the aggressive behavior of the dominant “alpha” monkey\nof a colony affected the status of that monkey and other members of its social\ngroup. In monkey colonies, a single alpha male imposes its will on the lower-\nranking “delta” members by displaying a range of threatening behaviors, such\nas staring directly at other animals, baring its teeth, emitting a vocal\nwarning, and changing its physical posture to indicate a potential attack.\nEven in captivity, this behavioral arsenal ensures that the alpha monkey\nmaintains key privileges in the colony, including much of the cage space, a\nchoice of the females for mating, and first access to the food provided by\ncaretakers. As Mel Brooks might say, it’s no fun being a delta monkey.\n\nTo start, Delgado implanted his usual stimulating electrode gear in Ali, the\ndominant alpha monkey. This allowed Delgado to remotely stimulate the caudate\nnucleus—a brain region that is associated with motor control—of Ali’s basal\nganglia as the monkey interacted with delta members of his colony. During one\nhour each day, Ali’s brain was stimulated for five seconds, once a minute, and\nduring this hour of intermittent stimulation, Ali’s aggressiveness diminished\ndramatically. As Ali’s new demeanor was gradually recognized by the rest of\nthe colony members, the lower-ranking monkeys started to assert themselves and\nmoved to strip Ali of his territorial and other privileges. While Ali’s brain\nwas being stimulated, the delta monkeys spread themselves around the cage,\neven crowding around Ali, who did not seem to care about what, at other times,\nwould have been a severe affront to the colony’s strict hierarchical order, an\nact worthy of punishment.\n\nThe relaxed gang of delta monkeys did not get to enjoy their party for long,\nhowever. About ten minutes after Delgado terminated the stimulation of Ali’s\nbrain, the old order was readily reestablished. Back to his usual aggressive\nself, Ali reconquered his territory and all the sweet perquisites that came\nwith being an alpha primate. In a subsequent series of experiments, Delgado\ndecided to measure what would happen to the social structure of the monkey\ncolony if lower-ranking animals had access to a lever that, when pressed,\ntriggered the electrical stimulation of Ali’s caudate nucleus. At first, a few\nof the delta monkeys pressed the lever tentatively. After a while, though, a\nfemale named Elsa discovered that every time Ali threatened her, she could\npress the lever to get him off her case. Elsa’s own behavior also evolved; she\nsoon began to stare directly at Ali when she pressed the lever. Although she\ndid not become the new alpha monkey of the colony, Elsa clearly gained control\nover Ali’s aggressiveness and kept his attacks to a minimum.\n\nDuring his career, Delgado employed the same general method to electrically\nstimulate a large variety of cortical and subcortical brain structures related\nto key brain circuits, such as the motor and the limbic systems, in animals\nand up to twenty-five human subjects suffering from severe psychiatric and\nneurological disorders. He could induce and block a range of behaviors:\ncomplex motor actions, perceptual sensations, emotions such as aggression or\naffinity toward others, euphoria, tameness, and lust. Through these studies,\nhe swiftly realized the many pitfalls involved when employing electrical brain\nstimulation to produce a particular behavior. As Horgan notes in his profile,\nDelgado dropped most of his research on humans when he observed that the\neffects of electrical brain stimulation varied greatly from case to case and\neven in the same patient, from moment to moment.\n\nIt is easy to imagine how this kind of research eventually got Delgado into\nvery hot water. Indeed, as Horgan suggests, these experiments look\nfrighteningly reminiscent of the worst possible scenarios peddled by science\nfiction writers. However, as Delgado revealed in an interview with Horgan, his\nmain intention in pursuing electrical brain stimulation in humans was\nprimarily motivated by the fact that, at the time, schizophrenic patients who\nexhibited bouts of aggressive behavior were commonly subjected to a gruesome\nsurgical procedure—the prefrontal lobotomy—that involved disconnecting from\nthe rest of the brain, removing, or destroying most of the patient’s\nprefrontal cortex. Tragically, it took many years for neuroscientists to\nuncover that the lobotomy reduced the patient to a mental state characterized\nby emotional indifference, lethargy, a profound indifference to pain and other\nfeelings, and a marked lack of initiative and drive. Delgado was horrified by\nthe procedure. Such good intentions alone, however, did not appease his\ndetractors in the scientific community. Nor did they calm members of the\ngeneral public, who started to question the aims of his work, on many fronts\nand on multiple grounds.\n\nBy 1974, just five years after his controversial choice for a book title had\nobfuscated most of his technological and scientific discoveries, Delgado left\nthe United States to accept a position created specially for him at the\nUniversidad Autónoma de Madrid, in Spain. There, he continued his work,\nisolated from much of the mainstream neuroscience community, focusing\nprimarily on noninvasive methods to stimulate the brain. His experiments,\nparticularly those involving the generation and blockade of motor behaviors,\nindirectly paved the way for the introduction, a generation later, of deep\nbrain stimulation (DBS) in the treatment of Parkinson’s and other neurological\ndiseases. Yet, for the next two decades, Delgado’s name and legacy slowly\nvanished from the neuroscience literature.\n\nCuriously, nothing Delgado did or published ever suggested that he would\nbecome capable of controlling someone’s voluntary will, let alone someone’s\nmind. It is interesting to note, though, that the usual suspects who routinely\ndenounce the supposedly innumerable creative ways in which scientists are\ncontributing to the extinction of human nature remain utterly untroubled by\nthe repetitive demonstrations that much more efficient forms of brainwashing\nand mind control abound in modern society. Contrary to these imaginative\nplots, none of these entail any last-generation brain chip developed in the\nlaboratory of a neuroscientist, even one as eccentric as José Delgado.\n\n* * *\n\nDespite Delgado’s troubles, electrical brain stimulation continued to be\nwidely employed by neurophysiologists as a key method to stimulate brain\ntissue, neural pathways, and peripheral nerves. However, for a long time few\nneuroscientists dared to come close to the type of experiments Delgado\nperformed in the 1960s. That placid scenario took a drastic turn, however,\nwhen my former adviser, John Chapin, and his students stunned the neuroscience\ncommunity with their extensive experiments with “roborat” (see [Fig.\n10.2](part0014.html#fig10-2)).\n\nAlthough I had known about their work from the time of their initial\nconception, very few experimental demonstrations impressed me more than the\ncollection of video clips featuring roborat as the leading protagonist. One of\nthese videos depicted roborat climbing a rubber mesh, peppered with holes the\nsize of roborat’s paws, that was installed almost perpendicular to the ground.\nAnother showed roborat successfully navigating the entire sequence of\ndemanding obstacles that formed the open-field testing track in San Antonio,\nTexas, that DARPA used at the time to evaluate the limits of the most advanced\nautonomous robots being built.\n\nAt this point, you may be wondering what could be so sensational about a\nparticular robot performing these tricks. After all, Mitsuo Kawato and Gordon\nCheng’s robots could sing, dance, and play Ping-Pong, couldn’t they? While\nthat was true, those robots had been programmed to carry out specific tasks;\nnone of them could sing new songs or shuffle a samba or defeat a Ping-Pong\ngold medalist without the aid of a roboticist’s diligent code. And that\ncertainly held true for finishing the full extent of the DARPA track’s\nobstacles. An autonomous robot might get stuck in the sand trap, ambushed on\nthe rubble pile, or stymied by the very steep and equally slippery ramp.\n\nMore than anything, though, roborat’s performance irked a number of rival\nroboticists for the simple fact that it was not a robot at all. It was just a\nrat, not of the Philadelphian sewer variety, but a proud member of the Long-\nEvans strain, raised, groomed, and slightly enhanced in John Chapin’s lab at\nthe medical school of the State University of New York, Brooklyn. When Chapin\nentered the DARPA robot track carrying an animal cage, a few people chuckled\nin disbelief. More of them chuckled when from that cage he carefully pulled\nout, not a gizmo, but a black-hooded rodent. After spending a few minutes\ngently patting the rat’s back in some kind of Zenlike maneuver, he carefully\npositioned the animal at the track’s starting line.\n\nWhen Chapin stepped away from his pupil, everyone who was paying attention\ncould now see that his hands were tightly grasping a commonplace laptop.\nSuddenly, all chuckling and mocking ceased. Chapin’s experiment, it should be\nsaid, went much further than anything his illustrious Spanish predecessor had\naccomplished. For several months, he had been testing an elaborate new\nelectrical brain stimulation paradigm. Rather than merely attempting to block\nor induce sporadic body movements by stimulating a particular brain region,\nChapin planned to use electrical pulses to instruct his rat on how to steer\nthrough a complex maze. To deliver this kind of instruction, he chronically\nimplanted a single stimulating electrode in a cortical region he knew very\nwell: the whisker representation of the primary somatosensory cortex. Since\nthese stimulating electrodes would be used to inform the rat which direction\nit should turn its body, Chapin implanted one electrode in the right S1 cortex\nand one electrode in the left. Different from most previous studies that\nutilized electrical stimulation of S1, Chapin also implanted a set of\nstimulating wires in the medial forebrain bundle (MFB), which he knew would\nproduce a very intense pleasant sensation when it was stimulated. After the\nrat recovered from the implantation procedure, Chapin rigged the animal with a\n“backpack” that could receive commands from a straightforward radio\ntransmitter and relay a tiny electrical pulse to any one of the rat’s\nimplants.\n\nThe secret, though, was in how Chapin had defined the temporal sequence in\nwhich these pulses were delivered, either to each cortical hemisphere or to\nthe MFB. Basically, he had discovered that his rats could learn to associate a\npulse delivered to the right S1 cortex with an instruction to turn right,\nwhereas a pulse delivered to the left S1 cortex signaled an instruction to\nturn left. Such learning was possible, and rather quick, because every time\nthe rat followed Chapin’s instruction correctly, it received a single\nelectrical pulse to its MFB. Using this strategy, he managed to train his\nroborats to navigate any maze they faced—becoming the first hybrid living\nthing to smash the DARPA testing track’s record.\n\n* * *\n\nWith his usual modesty and candor, even after such a spectacular performance,\nJohn Chapin recognized pretty well the limits of his otherwise unique strategy\nto communicate with his subject’s brain and reward it for a job well done.\nThus, despite later introducing an instruction for forward movement, among\nother innovations, he was fully aware that adapting his approach in an upper-\nor lower-limb BMI would require many more commands, and far more nuanced\npatterns of electrical stimulation, to be delivered to the somatosensory\ncortex. Regardless, the incredible adventures of roborat helped solidify my\nhunch that electrical brain stimulation might be used in a next generation of\nBMIs. It was time to dislodge these conjectures from the drawing board and try\nthem out in a series of experiments.\n\n![image](../images/00038.jpeg)\n\n![image](../images/00039.jpeg)\n\nFIGURE 10.2 John Chapin and roborat. In the left panel, Dr. John K. Chapin and\nhis creation. Below, roborat walks over a metal mesh. (Courtesy of Dr. John\nChapin.)\n\nBefore reaching for the most fanciful of these ideas, I decided to attempt\nsomething relatively easy: could a monkey learn to interpret a binary message\ndelivered by direct cortical electrical stimulation and use the information\nacquired to solve a behavioral task? This question grew out of a project that\nhad been recently concluded by Aaron Sandler, a graduate student in my lab,\nwho had demonstrated that owl monkeys could utilize tactile stimuli, delivered\nto the skin of either their left or right forearm, to identify which of two\nboxes, placed in front of them, they needed to search for food once an opaque\nPlexiglas door, which blocked their access, was raised. The two monkeys he\ntrained had no trouble in associating a stimulus on the right arm with finding\na food treat in the right box and a stimulus on the left with a treat in the\nleft one. Moreover, after learning this first rule, the monkeys could easily\nhandle a reversal, with a left-arm stimulus meaning the food was on the right\nand a right-arm stimulus meaning it was on the left.\n\nHaving this useful information available, as well as detailed behavioral data\nthat Sandler had carefully gathered during his food-retrieval experiments,\nNathan Fitzsimmons started to explore ways of delivering an electrical\nstimulus to the S1 cortex of the same monkeys (see [Fig.\n10.3](part0014.html#fig10-3)). Sandler had implanted multiple microwire arrays\nin several cortical areas of his two owl monkeys. This had allowed both\nSandler and Fitzsimmons to obtain chronic simultaneous recordings of the\nelectrical activity of close to one hundred cortical neurons for six years in\na row. For his thesis project, Fitzsimmons selected a few microwires in the S1\ncortex of each animal to deliver electrical cues to the monkeys in a new\nversion of the box-identification task. Moreover, he decided to try out a\nlarger variety of “encoding schemes” for delivering the spatial cues directly\nto the monkey’s cortex. His first scheme followed a simple rule: if a high-\nfrequency electrical microstimulation was delivered to the right S1 in the\nperiod before the Plexiglas door was raised, the food pellet would be found in\nthe right box, and if no electrical stimulation occurred during that period,\nthe food would be delivered in the left box.\n\n![image](../images/00040.jpeg)\n\nFIGURE 10.3 Talking back to the brain. Top shelf illustrates the experimental\nparadigm utilized to deliver “electrical messages” to the brain of a monkey.\nChronically implanted microelectrode arrays are employed to deliver\nspatiotemporal electrical patterns that represent different messages. Middle\nshelf illustrates the different types of patterns utilized to deliver messages\nto the primate brain: basic amplitude discrimination, temporal discrimination,\nor spatiotemporal discrimination. Lower shelf illustrates learning curves for\neach of the three methods to deliver messages. (Adapted from N. Fitzsimmons,\nW. Drake, T. Hanson, M. Lebedev, and M.A.L. Nicolelis, “Primate Reaching Cued\nby Multichannel Spatiotemporal Cortical Microstimulation.” Journal of\nNeuroscience 27 [2007]: 5593–602.)\n\nAlthough the same two monkeys had been extensively trained in the tactile-\nstimulation version of the task, it took both animals about forty days to\nlearn this new rule to retrieve their food. That was odd. No previous study\nwith cortical electrical microstimulation had ever reported such a long\nlearning period for mastering such basic electrical cues. Still, once the\nmonkeys picked up the association, they performed the task as well as they had\nwhile receiving tactile stimulation of their forearm skin. Eagerly, we moved\nto the next step, which consisted of reversing the rule the animals had just\nmastered. Now, the monkeys took much less time to learn the cues—about fifteen\ndays. It was time to switch coding schemes again. For this round, Fitzsimmons\nselected two distinct temporal patterns of electrical microstimulation to\ntransmit the two messages: when the monkeys received an electrical stimulus\nformed by 150-millisecond-long pulses separated by 100-millisecond pauses,\ntheir goodies could be collected in the right box; when they received\n300-millisecond-long pulses separated by 200-millisecond pauses, the food was\nin the left one. With the exception of these small differences in frequency,\nin this discrimination contingency all other features of the\nstimulus—including the total charge, duration, and cortical location—were\nidentical. Though this was the first time the two monkeys had to discriminate\nbetween two slightly different stimuli to select which box they should attend\nto in order to collect a food pellet, they mastered the new rule in about a\nweek.\n\nHaving gone this far, we decided to push the limits of instructing primate\nbehavior with cortical electrical microstimulation. Instead of continuing to\nemploy a single microwire to deliver the electrical pulses, Fitzsimmons\nenlisted four pairs of neighboring microelectrodes. Then, instead of using\ndistinct frequencies to communicate distinct cues, he chose two traveling\nwaves of electricity, moving in opposite directions across the microwires, to\nnotify the monkeys of where the food pellet had been placed. We suspected our\ntwo monkeys would have a much harder time in discriminating between the two\npatterns in this spatiotemporal contingency.\n\nOur concerns proved to be totally unnecessary. In a matter of three to four\ndays, both owl monkeys figured out the subtle distinction between the two\nspatiotemporal cues and could find their coveted food pellets with the same\nlevel of assurance they had attained when facing the three previous coding\nschemes. They were actually getting faster at learning the cues, as well. It\nseemed that once the monkeys got the gist of the message we were trying to\nconvey to them—that is, the location of the pellet—they continuously improved\ntheir capability of applying this general rule to any new contingency we threw\ntheir way.\n\nBy identifying a clever way to block the electrical noise created by his\nstimulator, Fitzsimmons also managed to obtain simultaneous neuronal\nrecordings from the S1 and M1 cortices of his two monkeys around the time the\nmonkeys were receiving the electrical cue revealing which of the two boxes\ncontained the food pellet. When he fed these highly valuable neuronal signals\nthrough the same multilinear regression algorithms we regularly used to drive\nour BMIs, he noticed something interesting. Looking at the linearly combined\nneuronal activity produced by S1 neurons during the period between the end of\nthe electrical stimulation and the beginning of the animal’s movement toward\nthe box, he could predict, solely based on this electrophysiological data,\nwhich stimulus had been delivered to the monkey somatosensory cortex just a\ncouple of hundred milliseconds earlier. Encouraged, he fed the activity\nproduced by all recorded M1 neurons during the same trial period into the\nlinear model. This allowed him to predict, with equal precision, to which box\nthe monkey would move its hand, before any sign of movement could be measured\nin the animal’s muscles. As the monkeys mastered each of the task\ncontingencies, Fitzsimmons’s predictions also improved. And that was how he\nwas able to document, in great neurophysiological detail, the temporal\nsequence through which these primate brains decoded the messages embedded in\nan exogenous electrical stimulus and then seamlessly transformed that\nprivileged information into a decisive act of voluntary will.\n\nOur attempt to establish a direct dialogue with a primate brain had found\n“ready to listen” neuronal ears. Without a doubt, this was an auspicious\nstart.\n\n* * *\n\nBy the time Fitzsimmons’s results with cortical microstimulation were\npublished, Joey O’Doherty had devised a new series of experiments with rhesus\nmonkeys that utilized cortical electrical stimulation with an upper-limb BMI.\nO’Doherty and I realized these monkeys could become the first subjects to test\na radical new paradigm, baptized in honor of José Delgado as a brain-machine-\nbrain interface. This BMBI would allow the subjects, our monkeys, to interact\nwith a particular artificial device through a closed-control loop that\nprecluded any interference from the body whatsoever—neither the efferent\n(motor control) nor the afferent (sensory instruction or feedback) components\nof the interface would rely on any cellular tissue other than the small sample\nof cortex that was sending and receiving information. The monkeys would\nreceive instructions or sensory feedback directly to their brains, with no\nneed to involve the biological sensors or peripheral neural pathways that\nprimates normally rely upon to gather information from their moving body.\n\nCertainly this was a steep ladder to climb, particularly considering that the\nfirst version of such a BMBI afforded a rather impoverished makeshift channel\nof communication as a substitute for the body’s vast sensory apparatus. As in\nFitzsimmons’s experiments, replacing the senses would require chronically\nimplanted microwires to deliver simple patterns of electrical stimulation, to\neither the primary somatosensory or the posterior parietal cortex of the\nmonkeys. Our goal was to test whether these monkeys could learn to maximize\nthis single “artificial sensory channel” to decode instructions using their\nbrain activity alone. Later, we would employ the same strategy to give animals\nfeedback about the movements of the machine they were controlling.\n\nTo make things a bit less challenging, our first trials were limited to a\nbinary movement direction instruction, which a monkey had to use to determine\nwhether to move a computer cursor to a target on the left or the right of a\nscreen placed in front of the animal. During the early phases of training, the\nmonkey indicated the direction it extracted from the pattern of\nmicrostimulation by manipulating a joystick that controlled the cursor’s\nmovement. When the animal became highly proficient in executing this maneuver,\nwe gradually removed the joystick and shifted control to the BMI component of\nour apparatus, so that the animal controlled the cursor’s movement with brain\nactivity alone. Typically, the monkey started each attempt by centering the\ncursor on a starting point, projected in the middle of the screen. Once the\ncursor was centered, two visually identical circular targets appeared on the\nmonitor, each an equal distance from the cursor’s starting point.\nSimultaneously, an electrical stimulus, which we had coded to represent either\nthe left or right target, was delivered directly to the monkey’s S1 cortex.\nThe monkey then had to interpret the microstimulation and generate the pattern\nof motor brain activity that moved the cursor to the appropriate target (and\ndelivered its truly juicy reward).\n\nLike Fitzsimmons and Sandler before him, O’Doherty launched his experiments by\nmeasuring how long the same monkey took to solve the same task when an\ninstruction was delivered to the skin on the animal’s arm rather than through\ncortical microstimulation. These control experiments allowed us to compare the\nskin and the brain as conduits for receiving the binary instructions needed by\nthe monkey to solve the task. Furthermore, O’Doherty had set up the experiment\nso that one of the monkeys received electrical messages directly into the S1\ncortex, while the other received them into the PP cortex, just a few\nmillimeters behind the S1. This meant we could also compare which cortical\nregion was a better target for guiding the animal’s decision through cortical\nmicrostimulation.\n\nSimilar to what we had experienced during our experiments with owl monkeys, it\ninitially took a few weeks of training for the monkeys to learn the task when\nthe instructions were delivered tactilely. After this start-up period, a sharp\ndivide emerged between their performances based on whether the monkey was\nslated for S1-directed messages or PP-directed ones. The monkey receiving\nmicrostimulation to the S1 cortex soon reached the same level of proficiency\nin selecting between the two targets as it had achieved with a tactile\nstimulus. Things were murkier with the other monkey. Although this animal also\nlearned to decode the information delivered by tactile stimulation of the\nskin, it did not get a handle on how to interpret the electrical messages\ndelivered to the PP cortex. Though we cannot rule out the possibility that\neither different types of electrical signals or longer training periods are\nnecessary for monkeys to learn how to handle electrical instructions delivered\nto the posterior parietal region, O’Doherty’s experiment opened several\nscenarios for applications in future BMBIs.\n\n* * *\n\nOur team at Duke now turned to the possibilities for establishing a direct\ncommunication channel to deliver novel messages to a living brain. Long before\nmy phone conversation with Luis Baccalá in 2005, I had recognized that these\nnew technologies promised to take us far beyond a strict dialogue with the\nnervous system of our laboratory animals—indeed, they could reshape the\nbrain’s own point of view—based on our accumulated body of research. This\nconcept, which I have introduced throughout this book, is what I call the\nrelativistic brain hypothesis.\n\nTHE RELATIVISTIC BRAIN HYPOTHESIS\n\nWhen faced with new ways to obtain information about the statistics of the\nsurrounding world, a subject’s brain will readily assimilate those statistics,\nas well as the sensors or tools utilized to gather them. As a result, the\nbrain will generate a new model of the world, a new simulation of the\nsubject’s body, and a new set of boundaries or constraints that define the\nindividual’s perception of reality and sense of self. This new brain model\nwill then continue to be tested and reshaped throughout the subject’s life.\nSince the total amount of energy the brain consumes and the maximal velocity\nof neuronal firing are both fixed, it appears that neuronal space and time\nwould have to be relativized according to these constraints.\n\nTo test this hypothesis, we have been designing two experimental paradigms\nthat have never before been tried in brain research. The first of these\nexperiments, still in its early stages, entails creating, on a small scale, a\n“new world”—a very unusual, far from natural environment where newly immersed\nsubjects, in this case adult rats, had never been, let alone lived, in their\nwhole existence. We have chosen to establish a “magnetic” world, a place where\nall relevant environmental features—things like the world’s overall spatial\nboundaries; the location of food, water, and noxious stimulus sources; the\nplaces for nesting and socializing with other animals; and the site where\npredators (and fear) reside—are indicated by different magnetic field sources\n(see [Fig. 10.4](part0014.html#fig10-4)). To investigate whether subjects can\nlearn to navigate in such an environment, a small magnetic sensor will be\nimplanted on the frontal bone of the rat’s skull. When the “augmented” rat\nvisits the new world, the implanted sensor will signal the nearby presence of\na particular magnetic source by triggering the delivery of a unique\nspatiotemporal electrical stimulation to the animal’s S1 cortex via\nchronically implanted microwires.\n\n![image](../images/00041.jpeg)\n\nFIGURE 10.4 Drawing depicting a future experiment in which an R6-T rat will be\nimplanted with a magnetic field sensor that delivers electrical\nmicrostimulation to the animal’s primary somatosensory cortex proportional to\nmagnetic fields of different magnitudes. Each of these different magnetic\nfields identifies different objects, things like food, water, and the location\nof a toy rat. (Illustrated by Dr. Nathan Fitzsimmons, Duke University.)\n\nAs the rat explores the magnetic world, it will receive two sets of rewards\nevery time it correctly identifies a spot containing a positive stimulus: the\n“natural” reward (food, water, or social interaction) and an extra incentive\n(an electrical pulse delivered to the MFB, the brain structure utilized by\nJohn Chapin to train his roborats). Conversely, each time a rat mistakenly\nenters the “predator’s compartment” or another negative stimulus, a loud alarm\nwill reprimand the careless wanderer. To ensure that, from moment to moment,\nthe rat will only navigate the environment using the unique magnetic signature\nof each spot, the locations of the hedonic and unpleasant areas will be\nrotated frequently. Careful control experiments will also be performed to rule\nout the possibility that rats detect other sensory cues to the location of\nrewarding places.\n\nWe have named our subjects the “sixth-sense magnetic rats,” or R6-T, the T\nrepresenting the tesla, the unit employed by the International System of Units\nto designate the strength of a magnetic field. As we build this magnetic world\nand prepare to introduce the first R6-T to its new home, there are many\ninteresting questions that come to mind. Will R6-Ts be able to learn to\ninterpret the magnetic messages delivered to their S1 cortex? Will the animals\nlearn to live in this totally unnatural world and rely solely on their newly\nacquired magnetic sense to find food and water and avoid predators and other\nawful encounters, while guiding themselves to their housing quarters or the\ncompany of other rats? And if R6-Ts can learn all these things, will a full\nrepresentation of the magnetic world emerge in their S1?\n\nMy prediction is that the R6-Ts will eventually be able to learn some, if not\nall, of the crucial parameters of this brave new magnetic world, and that\nclear magnetic receptive fields will emerge to supplement the rats’ typical\ntactile responses. I am also confident that these magnetic RFs will not impair\nthe ability of R6-Ts to use their whiskers to discriminate the normal range of\ntactile stimuli that all regular rats normally perceive. In the context of the\nrelativistic brain, this will be possible because the animals will incorporate\nthe statistics of the magnetic world on top of those that describe their\nnatural environment, the one that has been assimilated in their brains since\nearly postnatal life.\n\nIt is important to emphasize that there is nothing special about selecting\ndifferent magnetic sources for building the new-world environment of these\nexperiments. As a matter of fact, if my theory is correct, one would obtain\nthe same results if the experiments were carried out in an “infrared” or in a\n“ultrasound” world. Recent research further suggests that electrical\nstimulation could also be replaced as the method of choice to deliver\nenvironmental information to our rats’ brains. A very good candidate for such\na job is optogenetics, a revolutionary new method introduced in 2006 by Karl\nDeisseroth, an associate professor of bioengineering and psychiatry at\nStanford University. In optogenetics, a light stimulus is used to modulate the\nelectrical activity of populations of cortical neurons. This isn’t as simple\nas flashing strobe lights, however. First, the cortical neurons need to be\ninfected with a virus that carries the genetic information needed for the\nsynthesis of particular proteins that form unique ion channels that respond to\ncertain wavelengths of light. For instance, if a cortical neuron has been\ngenetically engineered to express Channelrhodopsin-2—a protein that is known\nto direct movement in response to light in green algae—a light stimulus of a\ndistinct wavelength will open ChR-2’s sodium channel, triggering a massive\ninflux of sodium ions into the neuron and making it fire an action potential.\nConversely, if the cortical neuron has been instructed to produce a light-\ngated chlorine channel, using a different light-sensitive protein, another\nstimulus of a different wavelength will inhibit this cell’s firing. By mixing\nup these two types of light-gated ion channels in the S1 cortex of our rats,\nwe could map each of the magnetic signatures found in the magnetic world with\na pattern of light stimuli that generates a particular pattern of cortical\nelectrical activity. The great advantage of using optogenetics in such studies\nis that it does not produce artificial electrical artifacts and does not cause\ntissue damage.\n\n![image](../images/00042.jpeg)\n\nFIGURE 10.5 The explorer rat and the decoder rat. Illustration of a true\nbrain-to-brain interface linking an explorer rat, which uses its facial\nwhiskers to discriminate the diameter of a variable aperture, with a decoder\nrat whose main function is to indicate the diameter of the aperture based on\nthe pattern of brain activity transmitted by the explorer rat, without ever\ntouching the aperture with its whiskers. The brain-to-brain interface connects\nthe brains of the two rats. (Illustrated by Dr. Nathan Fitzsimmons, Duke\nUniversity.)\n\nSetting these technical considerations aside, I envision that long-term\nexposure to a magnetic world will not only lead to a seamless incorporation of\nthe magnetic field sensor into the R6-Ts’ brains, but also to the emergence of\na new sense of worldly and bodily reality in these animals’ minds. Admittedly,\nit would be rather difficult to prove this in an animal that cannot express\nits views, let alone its thoughts, with rhetorical vehemence. Yet, these\npredictions follow logically from the experiments we have conducted thus far.\n\n* * *\n\nThe second of our experiments is focused on testing the first brain-to-brain\ninterface (BTBI). In our preliminary design, a rat previously implanted with\narrays of microwires in the whisker representation of the S1 cortex will be\ntrained to use its long facial vibrissae to perform the tactile discrimination\ntask that Eshe faced in chapter 5. Once the rat has learned how to\ndiscriminate the relative width of two openings, judging one as “narrow” and\nthe other as “wide,” it will be promoted to “explorer rat” status (see [Fig.\n10.5](part0014.html#fig10-5)). At this point, the explorer rat’s patterns of\nneuronal electrical activity, recorded from the whisker representation of S1,\nwill be wirelessly transmitted to another location, a closed behavioral box,\nwhere another animal, the “decoder rat,” will reside, in complete darkness,\njust waiting. We will then activate a multichannel electrical stimulator or a\nlight source grid implanted in the decoder’s head, triggering a spatiotemporal\nwave of electrical or light stimulation targeting populations of neurons\nwithin the whisker representation area of the decoder’s S1.\n\nIn the first version of this experiment, which is in the planning phase, the\ndecoder rat will have been independently trained in the tactile discrimination\ntask so that it has a general idea of the task goals. However, by the time the\nexplorer’s brain activity is transported into the decoder’s brain, the decoder\nwill not be able to fall back on its training and use its whiskers to judge an\nopening’s width, simply because there will be no similar opening in the box\nwhere the decoder rat is placed. Instead, the lonely decoder will have to\nindicate behaviorally, by poking its nose at one of two spots in the box’s\nwalls, whether the opening sensed by the explorer rat’s whiskers was a wide or\na narrow one. The decoder rat will have to rely solely on its own brain’s\ntransduction of a fragment of the electrophysiological correlates of the\nexplorer rat’s tactile experience to make a judgment about the diameter of an\nopening that its own whiskers have never brushed. To make things more\ninteresting, every time the decoder reports the correct diameter of the\nopening and gets rewarded for a job well done, the explorer rat will receive\nan extra amount of reward for successfully transmitting its perceptual\nexperience to its partner.\n\nClearly, there are many potential problems that might derail the execution of\nthis complex experiment. But assuming we can settle all the technical details,\nand both animals are capable of learning how to interact virtually with each\nother, there will be many implications for testing and developing more\nsophisticated BTBIs. My prediction is that individual S1 neurons in the\ndecoder’s brain will become responsive to any mechanical stimulation of the\nexplorer’s whisker, even when this pair of rats is not engaged in a proper\ntactile discrimination task. For instance, I expect that the decoder rat’s\ntactile receptive fields will expand to include not only its own facial\nwhiskers, but also the whiskers belonging to the explorer rat. If this\nhappens, it will indicate that even a rudimentary BTBI will extend the brain’s\ninternal representation of the body to include the body of the other brain\nconnected to it.\n\n![image](../images/00043.jpeg)\n\nFIGURE 10.6 A different version of a brain-to-brain interface involving an\nintermediary layer of rats between the explorer and decoder rats. (Illustrated\nby Dr. Nathan Fitzsimmons, Duke University.)\n\nSuch a finding would be nothing short of astounding. It would be the first\nsuccessful demonstration that two living brains can be functionally linked\nand, as a result of this communion, collaborate harmoniously toward the\ncompletion of a mutually beneficial goal.\n\nSo far, however, I have only considered the simplest of the imaginable BTBIs,\none in which the brain of one subject, the explorer, communicates with the\nbrain of another subject, the decoder, in a unidirectional manner. Yet, the\ndrawings I sent to Luis Baccalá in 2005 contained a number of variations on\nthis original scheme, one of which is illustrated in [Figure\n10.6.](part0014.html#fig10-6) What if the decoder rat is granted the right to\nsend its own brain activity back to the explorer rat? In this arrangement,\ncould the two brains eventually reach a consensus, let’s say, about the\nidentity of a complex object explored only partially by each rat? Would the\nrats literally share their minds to build vicarious sensations, through a sort\nof touchless, Vulcan mind-melding ritual, in order to overcome the limits of\ntheir individual brains? Considering current state-of-the-art BMI technology,\nsuch a bidirectional brain-to-brain exchange would constitute an experimental\ntour de force. Yet its full implementation is at least theoretically feasible,\nparticularly if our unidirectional BTBI proves to be successful and easily\noperated by a pair of rats or, down the road, a pair of primates.\n\nBut I do not think we will be limited to the possibility of bringing together\ntwo living brains in direct communication. Imagine, for instance, that instead\nof being broadcast to a single decoder rat, the explorer rat’s brain activity\nfinds its way to each of the brains of a group of animals, identified as\n“intermediary rats” (see [Figure 10.6](part0014.html#fig10-6)). These\nintermediary rats also do not have access to direct information about the\ndiameter of the real-world opening. They can only gather the information that\nis broadcast by the brain of the explorer rat, which is the only animal that\ncan actually use its whiskers to measure the width of the opening. Upon\nreceiving the explorer rat’s brain activity, each of the intermediary rats\nmust decide to take a particular course of action: poking its nose to either\nthe left or the right to indicate whether it believes the pattern of\nelectrical or light messages indicates the detection of a narrow or a wide\nopening. As the intermediary rats make their individual choices, neuronal\nelectrical activity from each of their brains is then broadcast to a decoder\nrat, whose new job is to evaluate the collected “brain opinions” of the\nintermediary animals and determine if the opening is narrow or wide. As fair\npayment for an extraordinary job collectively achieved, the pack of\nintermediary rats and the curious explorer would each receive a sweet reward,\ndelivered promptly every time the decoder identifies the correct diameter.\n\nThat would surely be a stunning adaptation, a true consensus formed out of the\nvirtually experienced, yet preciously unique, fleeting traces of a communal\nelectrical brainstorm.\n\n\n11\n\nTHE MONSTER HIDDEN IN THE BRAIN\n\nIn different periods of my life, I have looked at the brain with different\neyes. For instance, back in middle school, I thought the brain resembled a\nhighly elaborate supercomputer, one so complicated and mysterious that not\neven the great Mr. Spock, of the Starship Enterprise, could hope to grasp it.\nI reasoned that if the Vulcan scientist who once intoned, “I am endeavoring,\nma’am, to construct a mnemonic circuit using stone knives and bearskins,” was\noften mystified by the logic employed by the human brain, the whole endeavor\nof understanding how that brain actually worked was pretty much fruitless.\nIntimidated, I dropped the subject and focused instead on sharpening my\ndefensive midfield soccer skills, dreaming that one day I could play for\nPalmeiras. Unfortunately, that first career option did not pay off as\nintended.\n\nBy high school, a casual and life-changing encounter with Isaac Asimov’s 1964\nbook The Human Brain rekindled my interest. On each page, as Asimov related\nthe macroscopic and microscopic structures that formed the brain, my\nastonishment grew. But I could not believe, when I reached the end, that there\nwas no chapter that actually clarified how these structures, with their quaint\nLatin and Greek names, toiled together.\n\nJust a few years later, in medical school, I literally dissected the brain, to\nwhich Asimov had introduced me, along many competing layers. Depending on the\nclass you attended, the brain was undressed according to an anatomical,\nhistological, physiological, pharmacological, neurological, or psychiatric\npoint of view. Once again, no one came forward, not even my scientific hero,\nDr. César Timo-Iaria, to tell us how all these levels of organization\nassembled themselves into thinking. At the time, to be a legitimate\nneuroscientist, you had to become an expert in one—and only one—of these\ndisciplines, and dedicate your life to working within the boundaries of the\naccepted canon of your chosen field.\n\nAfter graduating from medical school, I decided to pursue a PhD in physiology\nrather than apply for medical residency. With the total support of Timo-Iaria,\nhowever, I embarked on the risky research project that I described in chapter\n1: using graph theory and computer programs to analyze a brain circuit.\nHalfway through my thesis project, the American cognitive scientist Marvin\nMinsky published his famous treatise The Society of Mind. Eagerly anticipating\nthat the leading figure in the field of artificial intelligence had found the\nultimate answer, I dove into the book, only to discover that what I sought\ncould not be found there either. Minsky did not seem to care much about real\nbrains, just about the higher-order operational processes that may take place\ninside them.\n\nClose to the time of my thesis defense, I attended a lecture by the American\nphysicist John Hopfield. In the mid-1980s, Hopfield had invented associative\nartificial neural networks, and I remember being fascinated as we talked for\nalmost an hour about his opinion of what the future held for neuroscience.\nThat was when I became interested in the notion that I could only vaguely\ndefine then as “neural dynamics.” Later, the conceptualization and\nimplementation of brain-machine interfaces emerged as my attempt to search for\nthe physiological principles that govern the dynamic operation of vast\nensembles of neurons in freely behaving animals.\n\nFor the past few years, deeply influenced by the findings of two decades of\nresearch effort, I have found myself looking at the brain in a very different\nway. Somewhat surprisingly, even for me, today I like to compare the brain to\na special type of ocean, a never-idle sea of electricity, held together by\nmultiple synchronous waves of neuronal time and capable of remembering\neverything that sails through its mysterious gray waters. Using this liquid\nmetaphor, I mentally visualize how the true biological substrate of a\nrelativistic brain actually works. Just as someone who is interested in\nunderstanding the behavior of ocean currents, whirlpools, maelstroms, and\ntsunamis would be unable to explain these macroscopic phenomena by analyzing\nthe behavior of water molecules or atoms, focusing exclusively on the\nproperties of single neurons becomes a major distraction if one wants to\nunderstand the behavior of the whole neuronal ocean. Given this new metaphor\nfor the brain, it is only appropriate that some of my recent findings in\nunderstanding thinking began with the unforgettable moment my students and I\nspotted a mythological monster deeply buried in the meandering patterns of\nmammalian brain dynamics.\n\n* * *\n\n“It looks like the Loch Ness monster!” Improbable as it sounded, that was\nexactly the image that came to my mind when I saw the strange, two-dimensional\ndistribution plot of electrical activity in the brains of rats that had just\nbeen handed to me.\n\n“How in heaven can you see a fake Scottish dragon in this data?” A few\ninevitable giggles surfaced from the lab’s benches.\n\n“Look at the plot. On the top right corner you can almost see the monster’s\nelongated, triangular head. The head is connected to a very long neck, like a\nplesiosaur’s, that runs diagonally. Then the neck coalesces into a massive\nelliptical body, right at the plot’s center. And from the bottom of the body\nyou can clearly see what looks like hind paws—adapted for swimming!” Even I\nwas amazed by how readily my brain assigned all these anatomical parts to a\npicture that, in truth, contained a bunch of little black dots.\n\n“I was thinking about calling it the Great Brain Attractor.” Shih-Chieh “CJ”\nLin, a Taiwanese graduate student in the lab now a researcher at NIH, had\nclearly thought long and hard about what to name the main product of his\nmonths of arduous mathematical toil alongside Damien Gervasoni, a French\npostdoctoral neuroscientist from the city of Lyon, and Sidarta Ribeiro, who,\nin his doctoral thesis at Rockefeller University, had categorically implicated\nsleep as a key contributor to memory consolidation.\n\nUpon joining my laboratory, Gervasoni and Ribeiro had decided to employ our\napproach for chronic, multisite, multi-electrode recordings to investigate the\nnormal wake-sleep cycle of freely behaving rats. First, they implanted\nmicrowire arrays in four distinct brain structures: the somatosensory cortex,\nthe thalamus, the hippocampus, and the caudate-putamen complex, an important\ncomponent of the basal ganglia. After the animals recovered from that\nprocedure, they habituated each rat to live in a comfortable experimental\nchamber where it could have access to food and water with liberty. Each rat\nspent up to five days in this cozy spot, on a twelve-hour day/night cycle,\ndoing everything regular laboratory rats do as they go about their lives.\nMeanwhile, Gervasoni and Ribeiro aimed two high-resolution video cameras on\nthe rat’s chamber, continuously monitoring, Big Brother–style, every detail of\nbehavior. In addition, they recorded the rat’s neural activity, in the form of\nmultiple local field potentials (LFPs)—the sum of electrical signals from a\nchunk of tissue—from each of the four implanted brain areas. The LFPs and\nvideo recordings were synchronized to within milliseconds. Since LFP activity\ntends to encompass a very large pool of neurons in any given brain location,\nGervasoni and Ribeiro took very extensive recordings, up to ninety-six hours,\nof large-scale cortical and subcortical activity, which they could then\ncompare to the variety of behaviors produced by the rats as they shuffled\nthrough hundreds of wake-sleep cycles. They also recorded the electrical\nactivity of tens of single neurons widely spread around each of the four\nstructures from which they sampled LFPs, as a check.\n\nInterestingly enough, although a vast number of neurophysiological correlates\nof sleep had been identified by the early 2000s, there was no approach in\nplace for predicting an animal’s overt behavior, at each moment of the wake-\nsleep cycle, based solely on the electrical activity of its brain. For\ninstance, neurophysiologists knew that, during different segments of the wake-\nsleep cycle, particular types of neural oscillations emerged (see [Fig.\n11.1](part0015.html#fig11-1)). These oscillations are so ubiquitous that\nneuroscientists have often tried to implicate them in a variety of functions,\nincluding a definition of “consciousness.” We were a bit more humble in\nsetting up our experiment.\n\nDuring the wake-sleep cycle, each of the neural oscillations is typically\ncharacterized by a distinct amplitude and dominant frequency range. For\nexample, when rats are in a “quiet awake” state—meaning that they are standing\nstill on their four paws, but not producing rhythmic whisker\nmovements—cortical recordings log a low-amplitude, high-frequency oscillation\nprimarily distributed in the beta (ten-to-thirty-hertz) and gamma (thirty-to-\neighty-hertz) ranges. These low-voltage, high-frequency oscillations usually\ndefine a state of widespread cortical desynchronization. I compare this to a\nquiet ocean, with no major waves observed on its surface (see [Fig.\n11.1](part0015.html#fig11-1)).\n\n![image](../images/00044.jpeg)\n\nFIGURE 11.1 Spectrograms (top four 3D graphs) and corresponding raw local\nfield potentials (5 graphs in the lower shelf) illustrate the frequency and\ngeneral time pattern of different types of brain oscillations observed under\ndifferent types of behaviors: active exploration, quiet waking, whisker\ntwitching, slow wave sleep, and paradoxical REM sleep. In the spectrogram (top\nshelf) graphs, X axis is time (with the period of each behavioral state\nmarked) and the Y axis depicts the frequency of oscillations. Gray scale in\nthe Z axis depicts the magnitude or power of a particular frequency of\noscillation for each state. (Adapted from D. Gervasoni, Shih-Chieh L., S.\nRibeiro, E. S. Soares, J. Pantoja, and M.A.L. Nicolelis, “Global Forebrain\nDynamics Predict Rat Behavioral States and Their Transitions.” Journal of\nNeuroscience 24 [2004]: 11137–47.)\n\nAs rats start moving around to explore their surrounding environment, a state\nwe labeled “active exploration,” they sniff around, lick around, and make\nlarge-amplitude whisker movements. In their brains, a conspicuous five-to-\nnine-hertz oscillation rhythm, known as the theta rhythm, arises in the\ncortical and subcortical structures, in addition to the beta and gamma\nactivity of the quiet-awake state (see [Fig. 11.1](part0015.html#fig11-1)). As\nwe saw in chapter 5, a distinct pattern of rhythmic cortical firing is evident\nwhen awake and immobile rats start producing delicate, low-amplitude “whisker\ntwitching” movements. This whisker-twitching state registers oscillations at\nseven to twelve hertz, the same frequency as the whisker movements, first in\nthe animal’s primary somatosensory cortex and soon afterward spreading to the\nsomatosensory nuclei of the thalamus and other subcortical structures.\n\nThis pattern of brain oscillations is suddenly transformed, however, when\nalert rats descend into the early stages of sleep. At moments when the video\ncameras capture the animal becoming drowsy, a high-amplitude, highly\nsynchronous neural oscillation, known as “sleep spindles,” appears.\nClassically, sleep spindles are described as waxing and waning, seven-to-\ntwelve-hertz waves of rhythmic activity that ride on top of much slower one-\nto-four-hertz delta waves. Thus, as a rat plunges deeper into a “slow-wave-\nsleep” state, sleep spindles steadily disappear and delta waves increasingly\ndominate the cortical activity. By then, the rat is lying down, eyes closed,\nenjoying a nice nap—but not yet dreaming. As it progresses into its sleep\ncycle, the rat starts to dream, albeit in a different way than humans do, as\nfar as we know: rather than displaying rapid-eye-movement (REM) sleep, rats\nvibrate their facial whiskers! In the meantime, the rest of their body’s\nmusculature relaxes, in a serene rodent atony. Such unusual behavioral\npatterns of REM sleep in rats, which should be more properly designated as\n“rapid whisker movement” sleep, were originally reported in 1970 by my mentor,\nDr. César Timo-Iaria, the first sleep neurophysiologist to suggest that,\ndifferent from the visually dominated sleep experiences of primates, rats\nlikely delight in rich tactile dreams. That peculiarity notwithstanding, the\nelectrophysiological signature of REM sleep in rats, as in other mammals, is\nmarked by the recurrence of low-amplitude, high-frequency oscillations that\nare virtually identical to the cortical desynchronization patterns observed\nduring rat wakefulness. Intriguingly, theta oscillations—the frequency seen in\nactive exploration—appear in the hippocampus during REM sleep.\n\nThough these details of the wake-sleep cycle had been known for some time, no\none had devised a way to illustrate this electrophysiological information in a\nsingle, two-dimensional graph. Lin succeeded—graphically representing the\nintrinsic global dynamics of the brain, a central component of the\nrelativistic brain’s own point of view. He started by using a well-known\nalgorithm, the fast Fourier transform, to identify the frequency bands in the\ncortical and subcortical LFP recordings that contributed the most, in terms of\npower (or amplitude), to the frequency spectrum of the ubiquitous neuronal\noscillations that permeate the rat wake-sleep cycle. That analysis yielded\nfour frequency ranges of interest: 0.5 to 20 hertz, 0.5 to 55 hertz, 0.5 to\n4.5 hertz, and 0.5 to 9.0 hertz. Next, he tested which potential relationships\nbetween these frequency bands could most clearly differentiate between the\ndifferent brain states exhibited during the wake-sleep cycle. After an\nexhaustive search, he settled on two spectral amplitude ratios, with one\ncalculating the ratio of 0.5-to-20-hertz oscillations versus 0.5-to-55-hertz\nand the other the ratio of 0.5-to-4.5-hertz oscillations versus\n0.5-to-9-hertz. Since in these ratios the numerator was always part of the\nfrequency range of the denominator, the resulting values conveniently fell\nbetween zero and one.\n\nHaving identified the appropriate metrics, Lin calculated the two spectral\namplitude ratios for every consecutive second of each of the many hours the\nLFPs were obtained concurrently in the cortex, hippocampus, thalamus, and\nstriatum of the rats. For each animal, four extremely long time series (one\nfor each brain area recorded) were obtained for each ratio, with one value,\nbetween zero and one, calculated for each second in time. To generate a two-\ndimensional plot depicting all this information, Lin took advantage of a\nmultivariate statistics technique to linearly combine the data from the two\nclusters into a single record. Then it was a simple matter of plotting the\nvalues on an X-Y “state-space” graph.\n\nThe Loch Ness monster I saw during my first encounter with Lin’s Global Brain\nAttractor is shown in [Figure 11.2](part0015.html#fig11-2). It is immediately\nclear that the individual dots are not distributed around the graph in any\nsort of random pattern. Instead, they define a highly organized image in which\na few very dense clusters of dots are separated from one another by much\nlighter and smeared “belts” of points. When we subsequently analyzed the plot\nof brain oscillations against the videos, we discovered that each of the dense\nclusters could be mapped to one of the major behavioral states seen in a\nnormal rat’s wake-sleep cycle, and the lighter, smeared “belts” of dots\ncoincided with a rat’s transition from one of these states to another. Because\neach point in the state-space corresponds to one second of brain activity, the\ndenser dot clusters—those that represent the main behavioral states—correspond\nto where and how the animal’s brain spends most of its time. The distribution\nof time spent in each of the awake-sleep states can be seen in a plot called\nthe hypno-map, shown in [Figure 11.3](part0015.html#fig11-3). In contrast,\ntransitions between these states occur much faster, which is why they appear\nsmeared on the graph.\n\nLin’s elegant plotting of state-space opened the way for predicting an\nanimal’s general behavior during the wake-sleep cycle from raw\nelectrophysiological records alone. For example, the very dense ellipsoid\ncluster at the center of the state-space, which resembles the Loch Ness\nmonster’s body, depicts the time when the animal is fully awake. If the\namplitude of spectrum ratios matches up with a dot that falls into the\nrightmost two-thirds of the ellipsoid, the rat will very likely be awake,\nstanding on its four paws but not moving at all, not even its whiskers. On the\nother hand, if the internal brain dynamics produce a dot in the leftmost third\nof the same ellipsoid, that same rat will very likely now be awake and moving\naround, exploring its surrounding environment. If the dot appears in what\nlooks like the back of the monster’s head, the rat is likely becoming drowsy,\nready to begin a nap, as it falls into slow-wave sleep. A more lightly\npopulated “neck” connects the “awake” body to the “sleepy” head since the\nbrain oscillates in this region much less than it does in the other two. That\nis because the neck represents the dynamic transition between being awake and\nthe early stages of sleep.\n\nNow, if you look closely, you will see a much smaller but still dense cluster\nof dots next to the monster’s body that looks like something of a wing. This\nsmaller ellipsoid is connected to the monster’s head by a faint, curvy cloud\nof dots. It turns out that this small ellipsoid matches up with the time when\nthe rat is experiencing REM sleep, whereas the faint cloud corresponds to the\ntransition between slow-wave sleep and REM sleep, a state that became known as\n“intermediary sleep.” (REM sleep is also connected to the quiet-awake state by\na brief and faint transitional state; unfortunately, it is difficult to\ncapture it distinctly in a two-dimensional plot, but it stands out in a three-\ndimensional rendition.)\n\n![image](../images/00045.gif)\n\nFIGURE 11.2 The Loch Ness Monster, as depicted in the classic “fake”\nphotograph, appears in the top shelf. Its brain relative is depicted in the\nstate-space of the lower left shelf. On the lower right panel, the different\nbrain states that correspond to different locations in the state-space are\nmarked by ellipses. A transition state between quiet awake and slow-wave sleep\nis also depicted. (Adapted from D. Gervasoni, Shih-Chieh L., S. Ribeiro, E. S.\nSoares, J. Pantoja, and M.A.L. Nicolelis, “Global Forebrain Dynamics Predict\nRat Behavioral States and Their Transitions.” Journal of Neuroscience 24\n[2004]: 11137–47, with permission.)\n\nFinally, in the lower leftmost quadrant of the state-space, a dense cluster of\ndots is connected to the central ellipsoid that defines the awake state. These\nare the dots I liken to the monster’s foot, adapted for fast swimming in sweet\nScottish lake water. This “foot” represents the whisker-twitching state, the\nperiod in which fully awake and attentive rats produce small-amplitude, high-\nfrequency whisker twitches. Meanwhile, the dots connecting the foot to the\nmain ellipsoid—a sparse smear of dots that looks something like a\nleg—corresponds to the transition between the quiet-awake immobile state and\nthe period in which rats produce such whisker twitches.\n\nOver the past five years, in every rat we have studied, we have mapped the\nsame general Loch Ness–looking wake-sleep state-space. With the exception of\nthe whisker-twitching state, for which the precise position in the state-space\nvaries somewhat across subjects (a problem that is resolved by mapping the\ndots in three dimensions), the locations of all of the major behavioral states\nand transitions remain very constant, from rat to rat. Even in mice and\nmonkeys, the general shape is basically the same. Thus, we believe that when\ndata are available for the human brain, we will also see a Loch Ness\nmonster—minus the whisker-twitching state, of course.\n\nTo test our findings, we generated other plots by analyzing the data from the\nfour cortical and subcortical regions separately, rather than averaging them\ntogether. Even in this scenario, the Scottish predator’s body seemed to emerge\nfrom the mist of dots. This suggests that no one precise spatial location in\nthe brain—at least in the subdivision called the diencephalon—from which the\ndata were sampled defined the brain’s global internal state of dynamics.\nUltimately, the wake-sleep cycle can be retrieved from any of the diencephalic\nstructures. And by combining multiple brain structures together, the\nresolution of the final image increases significantly, appearing almost like a\nhologram, much as the distinguished neurosurgeon and neurophysiologist Karl\nPribram, a student of Karl Lashley, imagined it a few decades ago.\n\n![image](../images/00046.jpeg)\n\nFIGURE 11.3 The hypno-map. This 3-D graph illustrates the time spent by rats\nin each of their main behavioral states. The X and Y axes of the graph depict\nthe state-space shown in [Figure 11.2](part0015.html#fig11-2), while the gray-\nscaled Z axis represents the time spent by rats in each state. Notice that for\nmost of their lives, our furry friends sleep, without dreaming, in the slow\nwave sleep state. Quiet awake is only the second most common brain state in\nthe life of rats. (Courtesy of Dr. Shih-Chieh Lin, National Institute on\nAging, NIH; and Dr. Damien Gervasoni, Claude Bernard University, Lyon,\nFrance.)\n\nPerhaps more stunning, we could create a real-time animation of the Loch Ness\nstate-space, which allowed us to envision how the distribution of oscillation\npoints is “built” in freely behaving animals, mapping the rat’s brain\nelectrical activity as the animal shifts, from moment to moment, from one\nphysiological brain state to another. This animated version of the state-space\nrevealed that the rat brain navigates the frequency state-space using highly\nstereotyped and preferred dynamic trajectories. By the same token, we\ndiscovered that there are some trajectories that simply cannot occur. In many\nways, these trajectories matched what has long been observed in animal\nbehavior, that animals do not jump, for instance, from whisker twitching or\nquiet attentiveness into REM sleep, or from active exploration into slow-wave\nsleep. So, if any of those forbidden frequency trajectories does pop up in the\nbrain, it signals a problem—perhaps a neurological disorder.\n\nWhen Gervasoni, Ribeiro, and Lin added the third dimension to the state-space\nplot, it also allowed us to include pooled coherence, a global estimation of\nthe second-by-second level of coherent synchronous firing between populations\nof neurons located across all four brain structures. We focused our estimate\non the seven-to-fifty-five-hertz range, and found that as the pooled coherence\ncalculated in a given one-second segment increased, the synchrony in this\nfrequency range became stronger. To our delight, this further enhanced the\nspatial separation between the dense clusters—those that identified the key\nbehavioral states—in the state-space graph. In fact, the pooled coherence\nallowed us to ascertain, with great certainty, the frequency associated with\nwhisker twitching, since this behavior exhibits the highest level of\nsynchronous firing we observed across the brain structures we sampled.\n\nThis simple modification also revealed that most of the quick transitions\nbetween major states were characterized by abrupt, strong changes in firing\nsynchrony. For example, when the rat was shifting from the quiet-awake state\nto slow-wave sleep, or from slow-wave sleep to REM sleep, the brain structures\nall appeared to start firing more synchronously at the sleeping-spindle\nfrequency range of seven to twenty hertz. In the case of the transition to REM\nsleep, this abrupt increase in coherence is defined as the state of\nintermediary sleep. The animation of the state-space also showed that\ntrajectories between a “departing” state and a “target” state (e.g., from\nattentive wakefulness to slow-wave sleep) could fail, making the brain\nactivity return to the departing state. We later realized that these failures\noccurred primarily because the level of coherent synchronous firing did not\nreach the threshold necessary for going all the way to the target state.\n\nOver the past half century, multiple laboratories have found evidence that the\ntransitions between different states in the wake-sleep cycle are determined by\nthe interplay of a series of modulator neurotransmitters. These chemicals,\nwhich include acetylcholine, noradrenaline, serotonin, dopamine, and, most\nlikely, GABA, are produced by clusters of neurons located in a variety of\nsubcortical structures. By means of their widespread axonal projections, these\nneurons deliver the modulator neurotransmitters throughout the brain. Most\nstudies have so far focused on the role of these chemicals in determining a\nparticular state of sleep or wakefulness, but I believe that it is the\ncollective action of these modulatory structures that triggers the shift from\none brain state to another, leading to an equivalent change in animal\nbehavior.\n\nIn this way, the Loch Ness state-space is a condensed depiction of global\nbrain dynamics and how these dynamics are determined by the energy available\nto the brain. From moment to moment, a subject’s global brain dynamics varies\nas it responds to the neuromodulatory influences that “push” the generation of\ncontinuous electrical activity among billions of interconnected neurons. The\nbrain, however, can only move from one stable dynamic state to another equally\nstable state, assuming, that is, that the cortical circuits can together reach\nthe necessary energy threshold to do so. This happens when the forebrain as a\nwhole attains a high level of coherent and synchronous neuronal activity.\n\nBy delivering this level of descriptive detail, it’s no wonder that the Loch\nNess monster quickly became our favorite lab pet.\n\n* * *\n\nAlthough the state-space may sound abstract, it opened the way to\nunderstanding what happens when identical physical stimuli from the outside\nworld arrive to the body periphery during different states of global brain\ndynamics. In other words, the state-space plot gave us a window into the\nbrain’s own point of view.\n\nWe have already seen how information from the outside world is detected and\ntransduced by the body’s peripheral sensory organs into a spatiotemporal\nstream of incoming electrical activity that ascends through the sensory\npathways to the brain (see chapter 5). Naturally, when this incoming\nspatiotemporal signal hits the cortical circuitry, the brain is settled into a\nparticular dynamic state. Following the relativistic brain hypothesis, it is\nthe collision of these two spatiotemporal signals, the incoming peripheral\nsignal and the internal dynamic state of the brain, at a given moment in time\nthat generates the actual pattern of electrical activity that morphs into\none’s perception of the world. Accordingly, we would expect that identical\nascending peripheral sensory stimuli reaching the cortex at two distinct\ninternal dynamic states should produce totally different patterns of activity,\nand therefore induce different perceptual experiences in the same subject.\n\nAs we saw in chapter 5, even before we had discovered a way to recognize the\nLoch Ness monster in our state-space plots, we passively delivered identical\ntactile stimuli to the infraorbital nerve, the branch of the trigeminal nerve\nthat innervates the rat’s facial whiskers, during the quiet-awake, active-\nexploration, and whisker-twitching states. When these identical stimuli hit\nthe brain at these distinct states, they evoked very different sensory\nneuronal responses, at both the cortical and thalamic levels. In a follow-up\nexperiment, we looked at whether it made a difference that the stimuli had\nbeen delivered passively. We again observed that both cortical and thalamic\nneuronal tactile responses differ dramatically—particularly when the animal\nhas actively engaged the tactile stimulus through whisking. In a further\nexperiment, conducted with a neighboring laboratory led by my best buddy,\nSidney Simon, at Duke’s Department of Neurobiology, Jennifer Stapleton\ndemonstrated that the physiological properties of the sensory responses of\nsingle neurons in the gustatory cortex also differ dramatically, depending on\nwhether an animal is actively licking a tube to sample a tastant or simply\nreceiving a sample of the same chemical passively inside its mouth.\n\nOur findings have been corroborated by examinations of brain activity during\nslow-wave sleep, when the thalamocortical loop becomes entrained by highly\ncoherent synchronous sleep spindles. This results in an almost complete\ninterruption of the sensory volleys coming into the cortex. Indeed, some\nneuroscientists have classified this phenomenon as a “functional\ndisconnection” between the peripheral and the central nervous system, since no\nascending sensory signal seems able to cross the thalamic level and reach the\nprimary sensory cortices. Similar findings have been reported for both the\nvisual and auditory systems, where the flow of incoming sensory stimuli is\naffected by the animal’s brain state.\n\nHowever, global brain dynamics define only one component of the brain’s own\npoint of view. Embedded in these internal dynamics are a heap of memories,\naccumulated throughout the animal’s previous life experience. This mnemonic\ninformation also contributes to the spatiotemporal collision of incoming\nperipheral signal and internal dynamic states. According to the relativistic\nbrain hypothesis, the animal’s mnemonic existence is felt even before the\nincoming sensory signal hits the forebrain, by dictating the generation of an\nanticipatory signal across the cortex and likely most of the forebrain. This\nelectrical distortion, which likely includes components of the spatiotemporal\nmotor signals created by the animal as it explores its environment, may\naccount for the fact that neuronal activity is modulated across all layers of\nthe rat S1 cortex, as well as the somatosensory thalamic nuclei, several\nhundred milliseconds prior to the moment when the animal touches any object\nwith its whiskers. Under the influence of this “expectation” signal, which can\neither increase or decrease the firing rate of single neurons across a\npopulation, the internal brain state is pre-adjusted, creating the brain’s\ninitial model of the external world. In this sense, the relativistic brain\n“sees” before it “watches,” and the brain exerts its own point of view.\n\nI believe that the matches and mismatches between these two spatiotemporal\nsignals, one generated inside the brain and the other from the outside world,\nultimately define what we perceive as reality. That implies that there is no\nabsolute truth, because the brain is not a mere slave to what, for example,\nour retinas report to have seen. This neurophysiological collision is\nencapsulated in the context principle.\n\nTHE CONTEXT PRINCIPLE\n\nThe way the cortex responds as a whole, to an incoming stimulus or the need to\nproduce a particular motor behavior, depends on the global internal state of\nthe brain at that instant; that is, ongoing brain dynamics are essential in\ndefining the optimal solution the brain derives for the generation of any\ngiven behavior.\n\nI should emphasize that the Loch Ness state-space, in all likelihood, reveals\njust a few of the internal or hidden dynamic states of a brain that supply the\ncontext for thinking. In reality, there must be tens of other states embedded\nin our plot that cannot yet be as readily distinguished. For example, smaller,\nmore fine-tuned dynamic brain states likely reside within the large crowd of\ndots that defines the time during which animals are awake. That remains a\nquestion for further experimentation and analysis.\n\n* * *\n\nAt this point it may be useful to introduce a graphic model I have been using\nfor the past few years to visualize these spatiotemporal collisions, the\nenergy limitations on the brain, and, above all, my metaphor of the brain as a\nsea of electricity bound by waves of neural time. I call this graphic\nrepresentation the “wire and ball model” (see [Fig.\n11.4](part0015.html#fig11-4)).\n\nIn the wire and ball model, a closed wire loop ascends, dives, and twists all\nover a three-dimensional volume, tracing the viable pathways that the brain\ncan take within the Loch Ness state-space to move from one internal brain\ndynamic state to another. As such, the bending and twisting of this closed-\nloop wire can be thought of as the track of a dynamic roller-coaster of\nelectrical activity that the brain rides during its wake-sleep cycle.\n\n![image](../images/00047.jpeg)\n\nFIGURE 11.4 The wire and ball model (see text for explanation). (Illustrated\nby Dr. Nathan Fitzsimmons, Duke University.)\n\nNow, let’s add a ball to this model. Conveniently, the ball has a hole in it,\nso it can slide freely along the wire, wherever the wire will take it on its\nneuronal roller coaster ride. The ball represents a large random sample of\ncortical neurons, a good chunk of the brain, if you will, that belongs to a\nparticular brain region, such as the primary somatosensory cortex. By\ndefinition, the sphere’s total volume is kept constant as it slides along the\nwire loop. That’s because the ball represents the maximum number of spikes\nthat this sampled neuronal mass can produce, which is capped by the amount of\nenergy available to the brain. The most interesting property of the ball,\nhowever, is that its overall shape varies as a function of the pattern of\nelectrical activity generated by this particular neuronal population. Thus, as\nthe ball rides along the wire, moving from different internal brain states by\nclimbing or descending through the transitions that flank them, its three-\ndimensional shape will change considerably, though its overall volume will\nremain constant. Thus, when the ball is within the boundaries of the quiet-\nawake state, characterized by low-amplitude, high-frequency neuronal\noscillations, the ball’s surface is relatively smooth, much like the surface\nof the sea on a peaceful, windless day; just small, high-frequency ripples\nappear. In contrast, when the ball moves into the slow-wave-sleep state, it\nmust first cross through a high-coherence transition, with high-amplitude\nspindle waves riding on top of low-frequency delta oscillations. The sphere’s\nsurface in this case would be distorted, with crisp, spiky crests spreading\nacross it while exposing the troughs within the ocean currents. That is\nbecause as we track the ball’s trajectory through the wire loop, the three-\ndimensional representation of the spatiotemporal activity of the neuronal\npopulation is constantly influenced by the changes in internal brain dynamics\nthat define the state-space.\n\nNow, suppose we add a fourth dimension to the ball. This is done by creating a\ncolor scale that measures how fast each neuron of the sample is firing in a\nfixed period, say one second, around the time an incoming spatiotemporal\nstimulus collides with the ball as it travels along its never-ending wire\nroller coaster. In this color-coded scheme, dark gray represents a very high\nfiring rate compared to average, and light gray represents a very low firing\nrate, with intermediary shades assigned between these two extremes. The gray\nshades of this imaginary, and admittedly very strange, ball change depending\non where it is on the wire when a sensory stimulus from the outside world\nhappens to hit it. If the animal is in the quiet-awake state—alert, but not\nactively exploring the world—the arrival of the incoming stimulus will cause\nthe individual S1 neurons represented by the ball to quickly increase their\nfiring rates. This firing will be sustained for a very brief period of time,\nbefore the rate rapidly falls below its original level, characterizing a\nperiod of post-excitatory inhibition that lasts for tens of milliseconds. The\nwire and ball model is up to the challenge of depicting this well-documented\nsequence of events. First, a dark gray bulge suddenly emerges from the surface\nof our sphere, representing the initial intense firing response to the\nincoming stimulus. This deformation is rapidly followed by the appearance of a\nlight gray sinking hole. The more individual neurons that are recruited by the\nincoming stimulus, the larger the dark gray bulge and the light gray sink.\n\nThings would look very different, however, if an identical incoming\nspatiotemporal tactile stimulus hit the same ball of cortical tissue while it\nrides the region of the wire analogous to active exploration. During this\nbrain state, neurons modulate their firing rate up and down long before an\nincoming tactile stimulus hits the S1 cortex. For that reason, our ball’s\nsurface would start bulging and sinking in advance of its collision with the\nstimulus. By the time of the collision, the ball would be deformed, in a\ncomplex and rather colorful manner, so that it barely resembled the ball as it\nlooked when a stimulus occurs during the quiet-awake state. This would\nespecially be true because different numbers of neurons would be recruited to\nrespond to the same whisker stimulus depending on whether the rat was in the\nquiet-awake or active-exploration state.\n\nAs you can see, everything is relative, when it comes to the processing of\nincoming sensory signals by the internal dynamic state of the brain. For\ninstance, [Figure 11.5](part0015.html#fig11-5) depicts samples of a single\nneuron’s firing at one-second time periods. The top trace shows a sample of\nthe firing of this neuron during the quiet-awake state. As with any other\nneuron, this cell has a limit on how fast it can fire, defined by its\nrefractory period, the period that follows the last spike produced by the\nneuron and which is used by the neuron to recharge its membrane capacitor.\nAssuming this neuron’s refractory period is two milliseconds, its maximum\ninstantaneous firing rate (or spike velocity) is limited to five hundred\nspikes per second. Notice how, in the top portion of this figure, the\nindividual action potentials produced by the neuron during the quiet-awake\nstate are spread out over the entire second we analyzed, and that most of the\ntime intervals between consecutive spikes are very long, usually involving\ntens of milliseconds. This neuronal spontaneous firing pattern approximates a\nPoisson process, in which a sequence of events occurs continuously but\nindependently.\n\nNow imagine that at the exact end of the first one-second period a strong\ntactile stimulus collides with the internal dynamic state of our chosen\nneuron. The neuron responds robustly to the colliding tactile stimulus, with\nseveral action potentials generated early in the second firing we recorded, as\nseen in the middle portion of the figure. Instead of being dispersed over the\nwhole second of time, as one might expect from the neuron’s previous activity,\nthe distribution of spikes shifts dramatically, so that many of the spikes are\ncrowded around the onset of the stimulus. This means that each consecutive\naction potential in this early portion of the time segment is separated from\nthe next by just a couple of milliseconds. The neuron is firing at a much\nfaster velocity, based on its own internal time reference. This appears to\ncontinue until the neuron reaches its maximum instantaneous firing velocity of\n550 spikes per second, a biophysical wall based on the limits of the neuron\nand the overall energy available to the brain.\n\nAfter this initial salvo or burst of action potentials, there is a long\nperiod, lasting up to one hundred milliseconds, in which no spikes are\nproduced by the neuron. According to the relativistic brain hypothesis, this\nsilent period is imperative: since the firing is bounded by a cap in the total\nnumber of spikes a neuron can produce—akin to a “neuron spike budget”—and a\nmaximum instantaneous spiking velocity, to take part effectively in the\nrepresentation of an incoming sensory signal the neuron has to “borrow” a few\naction potentials from its total spiking budget. Normally, the neuron spike\nbudget would be spread across a somewhat long period of time—one second, as in\nthe example above. Now, however, these potentials need to be delivered in a\nhurry, effectively warping the neuron’s internal timing reference.\n\n![image](../images/00048.jpeg)\n\nFIGURE 11.5 Patterns of single neuronal firing during normal firing (top\nshelf) and during the occurrence of a sensory stimulus (middle shelf). The\nrefractory period, illustrated in the bottom shelf, defines the maximum firing\nrate that a neuron can reach. Throughout these examples, the total firing rate\nproduced by the whole brain has to be maintained below a maximum cap.\n(Illustrated by Dr. Nathan Fitzsimmons, Duke University.)\n\nFinally, the wire and ball model can illustrate exactly what happens when a\ntactile stimulus is delivered while an animal is maintained under deep\nanesthesia. When oscillations are recorded from anesthetized animals, the\nentire Loch Ness state-space all but collapses, transforming it into a single,\nimpoverished mental blob. Since different anesthetics affect neuronal pathways\nin a variety of ways, and many GABAergic neurons inhibit postsynaptic neurons,\nwhen a subject is anesthetized the neurophysiological measurements of the\nreceptive fields of single neurons and the resulting sensory maps are severely\ncircumscribed and sculptured. This results in a highly localized\nrepresentation of information, wherein the dynamic firing patterns of\npopulations of neurons are not captured because those neurons have been\ndeprived of their means to interact. Far from being dynamic, this impoverished\nstate renders the experimental subject unconscious and unresponsive, and robs\nthe brain of its mind.\n\nWhen these firing constraints and behaviors are applied to all of the single\nneurons interacting in a neural ensemble, two more interrelated\nneurophysiological principles of the relativistic brain are established: the\nconservation of neural ensemble firing principle, which has been verified in\nmultiple species and multiple cortical areas in our laboratory, and the\nneuronal mass effect principle, derived from our analysis of the neuronal\ndropping curves calculated during operation of BMIs.\n\nTHE CONSERVATION OF NEURAL ENSEMBLE FIRING PRINCIPLE\n\nNot only is there a maximum limit of firing that an ensemble of neurons can\nreach, but the global ensemble firing rate tends to stay constant, hovering\naround a mean, due to a variety of compensatory mechanisms that create a\nstable equilibrium. If single or multiple cortical neurons increase their\nfiring rate instantaneously, an equivalent mirror image reduction in firing is\nsoon produced by other members of the neural ensemble so that the overall\nenergy budget of the brain stays constant over the long run.\n\nTHE NEURONAL MASS EFFECT PRINCIPLE\n\nAs the size of cortical neural ensembles grows beyond a certain large number,\nthe amount of information embedded in the neural ensemble tends to asymptote,\nslowly converging to its maximum information capacity. This effect is\nreflected by an important reduction in the statistical variance of prediction\nderived from large neuronal ensembles. The neuronal mass effect principle is\none potential way to explain how the considerably higher variance of a single\nneuron’s firing ends up being washed away whenever that individual neuron’s\ncontribution is averaged across a large neural ensemble to which the single\nneuron becomes functionally attached during the execution of a particular\nbehavior.\n\nTo confirm the conservation of neural ensemble firing in the human cortex,\nneuroscientists will need to turn to functional magnetic resonance imaging\n(fMRI), a technique used to measure blood flow—that is, brain metabolism and\nenergy consumption—in real time. My prediction is that for each cortical spot\nfound to have increased its metabolic activity, there should exist an\nequivalent mass of neurons that show a decreased metabolism.\n\n* * *\n\nGiven the constraints imposed by fixed spike budgets and fixed maximum\nneuronal spike velocities, the shape of the distribution of spike timing has\nto be adjusted to allow a neuron to represent a tiny fraction of the\ninformation contained in the stimulus. Moreover, the spatial recruitment of\nneurons that signals a particular collision of an incoming stimulus with the\nongoing activity of a particular internal dynamic brain state varies from\nmoment to moment. Indeed, in a series of experiments we have seen that if\nidentical tactile stimuli are delivered at different internal brain states,\nthe overall spatial size of the cortical population recruited to participate\nin the representation of this stimulus varies. In addition to the overall size\nof the population, the particular spatial distribution of firing within this\npopulation also changes from moment to moment. The relativistic brain\nhypothesis suggests that a great deal of this variability is dictated by a\ndelicate energy balance. Thus, if a particular neuron “borrows” a lot of\nspikes from its own budget to produce an exuberant firing response to a\nstimulus, another neuron needs to reduce its spiking rate to “finance” the\nfiring of its neighbor. To some degree this sounds like how the global\nfinancial system works today: while a few bankers suck all the money from the\nmarket as if they had some sort of nuclear-powered vacuum cleaner, millions\nfind themselves, all of a sudden, without any money whatsoever.\n\nThis idea owes a great deal to one of the classical concepts in systems\nphysiology, the milieu interieur (later known as body homeostasis), developed\nby the incomparable French physiologist Claude Bernard. An almost forgotten\ncommodity in the reductionist-obsessed medical school curriculum, homeostasis\nplays a key role in regulating how our dynamic body states are managed and\nmaintained at each moment of our entire conscious life. Effectively, this\nmeans that mechanisms that maintain internal brain homeostasis, particularly\nenergy consumption, may dictate the limits of complex information processing\nby the brain. I propose that in a relativistic brain the information generated\nby a subset of billions of neurons is a function of the amount of energy these\nneurons collectively consume at each moment in time.\n\nFinding the mathematical relationship between these two quantities, if it\nactually exists, would be a major breakthrough in modern neuroscience. But\nwould it unlock the mysteries of the egotistical human brain?\n\nOnly time will tell.\n\n\n12\n\nCOMPUTING WITH A RELATIVISTIC BRAIN\n\nOn June 21, 1970, in the midst of a heat wave that insisted on sucking what\nlittle oxygen remained at the high altitude of Mexico City, the Brazilian and\nItalian national soccer teams played what is considered by many as the most\nmemorable championship game in the history of the World Cup. Although the game\nwas going to be played on just another sultry summer afternoon at Azteca\nStadium, a morning shower had made the pitch slick and somewhat unpredictable.\nBoth the Brazilians, led by Pelé, and the Italians, captained by the legendary\nleftback Giacinto Facchetti of Internazionale de Milano, were playing for\ntheir third title and a right to keep the famous Jules Rimet trophy for good.\n\nAfter a hard-fought first half that ended in a 1–1 tie, the Brazilian team,\nwearing their traditional canary yellow jerseys stamped with green numbers,\npulled off a formidable showing in the second half. With the game clock\ncounting down the last four minutes of regular play, the score stood at 3–1\nBrazil. The win was all but guaranteed.\n\nBut winning had never been enough for the Brazilians. That may explain why\ntheir last big play of the match seemed to unfold almost like magic, when\nTostão sneakily robbed the ball from a tired Italian forward, next to the left\nedge of the Brazilian penalty box. Having gained possession, he flicked an\ninconsequential pass to Piazza, who passed the ball, without much fuss, to\nClodoaldo, who himself promptly passed the ball to Pelé, deep in Brazil’s half\nfield. Upon making contact with his lifelong companion, Pelé employed one\nsimple foot caress to deliver the ball to Gérson, who hastily transferred it\non to Clodoaldo. Soon enough, it became evident that the usually cerebral\nGérson had created an ill-posed problem for Clodoaldo’s motor cortex.\nSurrounded by four Italian players, all desperate to strip away the ball,\nClodoaldo did not flinch. Upon securing the ball with his reliable right foot,\nhe started dribbling his way out of that crowd of rivals, one by one. For the\nnext five seconds, he danced with the ball until the last of his opponents\nretreated in despair.\n\nRecovering his balance and cool, Clodoaldo spotted Roberto Rivelino, free on\nthe left wing. Unselfishly, he passed the ball to La patada atomica\n(literally, the atomic kick), a nickname received in honor of Rivelino’s\nprodigious and lethal left shooting foot. Despite these accolades, Rivelino\nlooked tired when he received the ball. Mercifully, he did not need to hold it\nfor long. Jairzinho—Furacao (the hurricane)—was waiting, with plenty of energy\nto spare, in the left flank of the Italian defense. Without his looking at the\nball, Jairzinho’s brain assimilated the tool of his trade with a quick\nsequence of touches, then daringly made a move to his right, where,\nunexpectedly, a defensive midfielder was stalking. Jairzinho had no choice but\nto resort to an ugly, childhood trick, the “toe poke,” to pass the ball\nstraight to the immortal right foot of Pelé, who had run the extension of the\npitch to position himself immediately in front of the Italian penalty box,\npretty much like a jaguar ready to feast on its prey.\n\nPausing to caress the ball with his foot, Pelé barely noticed when Tostão,\nfending off a defender, screamed and pointed anxiously at an opening to score.\nPelé acknowledged Tostão’s warning by redirecting the ball, with almost\neffortless disdain, to a vast open area of field to his right. Just as those\nwatching started to think the king of football had gone mad, the ball reached\nthe empty spot that Pelé’s brain had plotted out for it. There, it was\ninstantaneously met in midair by the charging right foot of Carlos Alberto,\nthe Brazilian team captain, who had played with Pelé for the best part of ten\nyears. What came next was the only thing predictable in that exuberant display\nof collective play: Goooaaalll! As the announcer’s crescendo echoed, an entire\ncountry plunged into the streets, dancing.\n\nThat was quite a goal. Thirty seconds of uninterrupted play. Eight different\nplayers touching the ball. None having the slightest idea, beforehand, what\nthe outcome of their collective interaction would be. That unforgettable\nsequence of movements, and the 4–1 score it sealed, demonstrate the\ninsurmountable power unleashed by the emergent behavior of a dynamic complex\nsystem. No matter how proficient and intelligent each of those players were\nindividually, the play they engendered together could never be predicted or\nplanned a priori—it simply emerged from their combined voluntary actions, in\nhundreds of milliseconds, before they even realized it into a conscious\nthought.\n\nFaced with the unpredictable behavior of such a complex system, the classical\nreductionist strategy would break it down into its smallest freestanding\ncomponents and then, after fully characterizing the properties of these\nelements, try to derive the success of the entire system from the principles\nunderlying its individual parts. That approach would completely miss how the\nBrazilians made their play and won their third World Cup that afternoon. For\nexample, let’s imagine, rather logically, that each player is the smallest\nindividual component comprising the Brazilian team. Reductionists would\nprescribe that, in order to understand the mechanisms that generated that\nplay, one should collect all of the data available for each of the players\ninvolved in the goal, from their physiological descriptors to their individual\npassing, shooting, and scoring records. Next, one could try to describe each\nplayer’s average reaction time, muscle metabolism, and motor control\ncapabilities. The players’ behavior in previous championship games would be\nstudied, to characterize their decision-making processes. A particularly\nenthusiastic devotee might suggest that a complete answer would only be\npossible by analyzing each player’s entire genome. Pretty soon, mountains of\ninformation about each player would be assembled. As the British physicist\nJohn D. Barrow writes in his delightful book, Impossibility, “All the examples\nwe have listed are made of atoms if you look at a low enough level, but that\ndoes not help us to understand the distinction between a book and a brain.”\n\nDespite all this effort, no one feeding the accumulated data into a computer\nwould be able to predict how a team made up of those players would have\nbehaved in a soccer match. The behavior of the Brazilian team would be labeled\nas yet another of the noncomputable phenomena of nature. No one would be able\nto reconstruct or predict that dramatic example of team play by using a\nreductionist strategy because the play emerged from the unpredictable, dynamic\ninteraction of eight interconnected variables, who happened to be the best\nsoccer players in the world in the year 1970.\n\nTake, for instance, the huge number of potential plays that can be generated\nby the interactions of the eleven positions of a regulation soccer team—a\nnumber so large that it, too, is incalculable. In his book, Barrow describes\nthe enormous degree of complexity that can be generated by a system formed by\na large number of connected elements. “Complex structures seem to display\nthresholds of complexity which, when crossed, give rise to sudden jumps in the\ncomplexity,” he argues. “One person can do many things; add another person and\na further relationship becomes possible; but gradually add a few more people\nand the number of complex interrelationships grows enormously. Economic\nsystems, traffic systems, computer networks: all exhibit sudden jumps in their\nproperties as the number of links between their constituent parts grows.” And\nas complexity jumps, so does the unpredictability and the information content\nof the system. Such jumps are part of the history of the universe. In the\nbeginning there was only physics. Then chemistry appeared. Biology evolved\nnext and when consciousness emerged, everything in our part of the cosmos\nchanged dramatically. At least from our own humble perspective.\n\nIf it seems so difficult to predict the collective behavior of some subset of\neleven people, how could it ever be done for a dynamic, ever-shifting\npopulation of neurons drawn from the tens of billions in the human brain? This\nis the dilemma that systems neuroscientists have confronted for most of the\npast century. By focusing on single neurons, neuroscientists may have gathered\nan impressive wealth of information about the biological properties of the\nindividual processing units of brain circuits—a rewarding and very useful\nendeavor. Perhaps looking at the operation of single neurons was the only safe\nand technically feasible path to move forward, for a long time. But, as Barrow\nappropriately states, “consciousness is the most spectacular property to\nemerge” out of a complex system. By turning the gaze of ever more powerful\ntools onto the single neuron alone, neuroscience relinquished, almost without\nknowing, any real concrete chance of understanding the physiological\nmechanisms of thinking, the main product of those vast neuronal galaxies that\ndefine the conscious inner universe that exists within our heads.\n\nAs we have seen over the course of this book, in the past two decades my lab\nhas been part of a new wave of neuroscience that is changing the way we\napproach the brain, using new measurements of complex neural ensemble behavior\nto trace the interwoven sources of thinking, including all of the\ninterdependent, relativistic dynamics that change, from moment to moment, the\nway in which neuronal space and time fuse, bending and shifting the\ncontribution of each component of the circuits that define the nervous system.\nTo return to my soccer analogy, while no one could predict how that particular\nplay at Azteca Stadium emerged from the collective work of the Brazilian team,\nwe certainly can say a few things about the particular conditions in which the\nplay took place. For instance, the players had an ultimate, shared goal: to\nscore more goals than their rivals, win the game, and carry the World Cup\nhome. Moreover, the team was formed by very experienced players, some of whom\nhad played hundreds of games together, “gelling”—an expression used by soccer\nfans to describe a team that plays well together. These Brazilian artists had\nlearned, over their careers, how to select the kind of team strategy that\nwould work (or not) under the conditions of that pivotal match. Having watched\na few of the Italians’ previous games during earlier World Cup rounds, the\nBrazilians also shared knowledge of their opponents’ usual defensive and\noffensive tactics. Thus, prior to their encounter with the Italians, the\nBrazilian team had built an elaborate array of expectations, a “mental game\nmodel,” of what the other team might muster in a variety of situations. Now,\nfacing only four minutes left in play, and seeing that the Italians were\nvisibly tired, the Brazilians adjusted their game model and selected an\noptimal “last-dash” strategy to seal their victory. Their magnificent play was\ninfluenced by a common, overarching goal; a foundation of previously built\nexpectations; a game model shaped by physical potential and constraints as\nwell as adaptability; and the collective ability to interpret the particular\ncontext encountered at a particular moment. As any soccer “scholar” would\nattest, the context presented to the Brazilians dictated as the optimal\nstrategy that the team spread itself across the pitch and use long but highly\nprecise ball passes to “stretch” the Italian defenders as well. That ensured\nthat each tired Italian had to run a lot farther for the mere opportunity to\nsteal the ball from a Brazilian. Once you take into account all these factors,\nthe number of viable plays that could have emerged from the masterful\nBrazilian team is significantly reduced—not enough to predict the precise\nplay, but enough to suggest that some plays were more likely to happen than\nothers.\n\n* * *\n\nThroughout this book, I have defended the position that the brain’s own point\nof view decisively influences the way each of us constructs a model of\nreality. Like the dynamic, interconnected players of the 1970 Brazilian soccer\nteam, I believe the brain achieves its goal through the emergent properties of\na highly complex system.\n\nIn soccer, the players interact on the field of play, following an\ninfrastructure (the rules of the game) and pushing against physical\nconstraints (from the power produced by their bodies, to their maximal speed\nacross the field, to the force of gravity). They are presented with a fielding\nor scoring opportunity, and produce an optimal solution based on their game\nmodels. Throughout, the system remains relativistic, with actions emerging\nfrom collective thoughts in the continuously shifting context of space and\ntime.\n\nI propose that the brain similarly achieves its point of view, working within\nits own operational constraints and physiological infrastructure to create a\nvariety of behaviors out of the complexity of the nervous system. So far I’ve\nintroduced ten principles that go a long way, in my opinion, to describing how\nthought comes alive, in the truest, electrical sense. These are somewhat like\nthe rules of the game, but underlying them all are two simple anatomical and\nphysiological facts that, like gravity and electromagnetism in the larger\nnatural world, define the organic universe in which the brain operates:\n\n1\\. Billions of neurons produce electrical currents that are capable of\nspreading through the continuous, salty, and hence highly conductive space\nbetween and around the brain’s densely packed cells, generating widespread\nelectromagnetic fields that, despite being very tiny in absolute magnitude,\ncan still influence neighboring neurons.\n\n2\\. A hugely complex network of tens of thousands of potential long-range\nfeedforward and feedback connections, which include multiple multisynaptic\ncortical and subcortical loops, provides thousands or even millions of ways\nthrough which neurons in a given cortical area can readily communicate with\nother neurons located at a relatively great distance, far away in the brain (a\nfact I discovered when my thesis research produced those cascades of printouts\nof possible neuron-to-neuron connections).\n\nThese two basic elements help to explain, for instance, findings like those\nthat shocked Eberhard Fetz and his colleagues in 1998. In their study, Fetz’s\ngroup learned they could recover information about a visual cue that had been\nused to train a monkey to make an arm movement. That may not sound at all\nsurprising, given the experiments I have recounted—except that they were\nrecording the activity of interneurons located in the intermediary layers of\nthe spinal cord. Before these results, no neurophysiologist would have claimed\nthat the spinal cord had anything to do with vision, yet visual information\ncould be recovered from cells there. Furthermore, these same general\nproperties of brain connectivity may explain why two postdoctoral fellows in\nmy lab, Romulo Fuentes and Per Petersson, and I discovered that stimulation of\nthe dorsal surface of the spinal cord can dramatically reduce the tremors,\nakinesia, and other symptoms of a neurological syndrome, similar to\nParkinson’s, that affects mice and rats depleted of dopamine. Stimulating the\nspinal cord appears to produce a powerful disruption in the patterns of\nepileptic-like electrical activity crossing the animals’ motor cortex and the\nbasal ganglia. In the relativistic brain, it seems, many roads take you from\nnowhere to everywhere else.\n\nMore proof that the brain does not respect the borders created by cortical\nlocalizationists comes from repeated independent observations of cross-modal\ncortical processing in primary sensory fields—in stark contrast to classical\nhierarchical doctrine, which states that cross-modal processing should only\ntake place in so-called higher-order associative areas in the cortex. In the\nmid-1990s, instances of cross-modal processing in the visual cortex began to\nbe reported among patients suffering from definitive visual deficits (for\ninstance, congenital or acquired blindness) or people submitted to temporal\nvisual deprivation during experiments. In one study, published in 1996,\nNorihiro Sadato, Alvaro Pascual-Leone, and others working in Mark Hallett’s\ngroup at the National Institute of Neurological Disorders and Stroke (NINDS)\nemployed a brain-imaging technique known as positron emission tomography (PET)\nto demonstrate that both the primary and secondary visual cortex were strongly\nactivated in people who, after becoming blind in early life, had become\nproficient Braille readers, when they performed tasks that required fine\ntactile discrimination. A year later, the same NINDS research team decided to\n“disrupt” activity in the V1 cortex while a blind person was presented with\nBraille letters or embossed Roman numbers to read. This disruption was induced\nby a technique called transcranial magnetic stimulation (TMS), which, as the\nname implies, delivers magnetic pulses, in a noninvasive way, to a specific\ncortical region, causing an interference in normal neuronal activity. In\nsubjects with no visual impairment, TMS targeted at the V1 cortex led to\nproblems with the visual recognition of letters, but had no effect on the\nability to discriminate between different tactile information. When the\nresearchers presented blind individuals with the analogous task while the V1\ncortex was disrupted, however, the subjects committed significant\ndiscrimination errors—even though the task involved tactile information. This\nsuggested that the enhanced ability displayed by blind individuals in tactile\ndiscrimination tasks, such as Braille reading, may emerge because the visual\ncortex is recruited to help out—what is called cross-modal recruitment.\n\nIn another study, David C. Somers and his colleagues at Boston University\nshowed that after a mere ninety minutes using a blindfold, the primary visual\ncortex of otherwise normal-seeing human subjects became activated when\nsubjects performed tactile tasks. Such a short period of deprivation,\ninsufficient for a change in gene expression or the creation of new anatomical\nconnections, favored the idea that cross-modal responses were based on\npreviously existing somatosensory afferents in the visual cortex. Blindfolding\nplainly, if ironically, unmasked the ability of the V1 cortex to process\ntactile information.\n\nMore recently, Sidarta Ribeiro’s group at the Edmond and Lily Safra\nInternational Institute of Neurosciences of Natal, Brazil, has extended these\nfindings by demonstrating that cross-modal responses are present in both the\nprimary visual and primary somatosensory cortices of intact rats—meaning rats\nwithout any transient or permanent sensory deprivation (see [Fig.\n12.1](part0016.html#fig12-1)). Ribeiro identified individual neurons in the S1\ncortex that readily respond to visual stimuli and single neurons in V1 that\nreadily respond to the stimulation of a rat’s whiskers. Ribeiro observed\ncross-modal neuronal responses like these even under anesthesia. As these rats\nawake and start performing different tactile discrimination tasks in the dark,\nlarge numbers of individual neurons in V1 continue to fire in response to pure\ntactile stimulation of the animal’s facial whiskers. More extraordinarily,\nwhen an intact rat uses its whiskers to discern the size of an opening\ndiameter in total darkness, the cross-modal tactile responses across V1\nneurons contained enough information to predict the opening’s diameter, and\nthe performance of these V1 ensembles was just as good as that of an\nequivalent population of S1 neurons.\n\nRibeiro’s results have been bolstered by experiments conducted by Yong-Di Zhou\nand Joaquín Fuster at the University of California, Los Angeles, who reported\nthat visuotactile cross-modal associations develop in the primary\nsomatosensory cortex of intact rhesus monkeys as these animals are trained to\nperform tasks that require associations involving both vision and touch. And\nit appears these cross-modal responses in S1 can be enhanced by training\nanimals in tasks whose contingencies, by design, include cross-sensory\nassociations.\n\nSuch a view is also supported by studies involving the primary auditory cortex\n(A1). In a beautiful series of experiments in rhesus monkeys, Asif Ghazanfar\nhas shown that multisensory integration of dynamic facial attributes and\nvoices in A1 may contribute to the way primates, including humans, communicate\nwith one another.\n\nEven the neurons of the primary gustatory cortex respond to a vast repertoire\nof multimodal sensory responses.\n\nAll of this is a far cry from the cytoarchitectonic divisions of the brain\nperpetuated by Brodmann and generations of localizationists, who have claimed\nthat rigid anatomical and functional borders exist within the cortex. Such a\nfunctional model of the brain, devoid of time and internal dynamic brain\nstates, served us well for a century. Lately it has become a major hindrance\nfor progress in our thinking about how the cortex processes information in\nnatural ethological conditions.\n\n![image](../images/00049.jpeg)\n\nFIGURE 12.1 Cross-modal processing in the primary somatosensory and visual\ncortices of rats. Peri-event histograms display the isomodal and cross-modal\nsensory-evoked responses of individual S1 and V1 neurons. In the left panel,\nthe traditional visually evoked responses of V1 neurons and tactile-evoked\nresponses of S1 neurons are shown. In the right panel, samples of V1 neurons\nthat respond to a tactile stimulus and S1 neurons that respond to a visual\nstimulus are plotted. Korbinian Brodmann would be shocked! (Courtesy of Dr.\nSidarta Robeiro, International Institute of Neuroscience of Natal, Brazil; and\nDr. Damien Gervasoni, Claude Bernard University, Lyon, France.)\n\nAt this crucial juncture you may be asking yourself, “But what about Paul\nBroca’s patient in the nineteenth century who could not speak anymore\nfollowing a lesion of the left frontal lobe? Doesn’t that finding still\nsupport strongly the localizationist point of view?” Actually no. Today we\nknow that speech production depends heavily on the concurrent interaction of a\nmultitude of cortical and subcortical brain regions. The reason cerebral\nstrokes, like the one documented by Broca, produce aphasia is likely that they\ndestroy, in addition to the gray matter, huge portions of the underlying white\nmatter, which contains dense packs of the nerve fibers that connect this huge\nnetwork of areas with the frontal lobe. Such a massive destruction of key\ncommunication cables amounts to a catastrophic functional disconnect of this\nspeech production network. Although Broca’s patient survived this rather\nmassive stroke, he may have lost his speech through a catastrophic cortical\ndisconnection. With this explanation, at long last, Broca’s ghost and its\nnever-ending haunting of distributionists can be put to rest.\n\nThe combined evidence for cross-modal processing and the effects of internal\nbrain states mounts a fatal challenge to the idea that the cortex is rigidly\ndivided into functionally specialized areas and that distinct cortical regions\nare purely unimodal. Other than the neatness that it provides for the\nlocalizationists’ brain maps, there’s no profit afforded to such a unimodal\napproach, simply because there is no experience in our lives that can be\ndefined as purely unimodal. Moreover, the real world is not made of a\nconflagration of circular flashes or rectangular bars of light, localized skin\nindentations, pure auditory tones, or primitive tastes and odors, and most of\nthe time we live our lives as freely behaving beings, not in a deeply\nanesthetized state. Only in neurophysiology laboratories have these conditions\nbeen created, and more and more I believe that these artificial worlds have\nmeant that we have been studying a completely different kind of\nbrain—certainly not the one that all of us depend on to survive day in and day\nout.\n\nI propose that the brain we do use—the relativistic brain—is more akin to a\nmedium in which neuronal space and time fuse into a physiological space-time\ncontinuum, which can be recruited in a variety of ways to perform all the\ntasks assigned to it. Depending on the status of the peripheral sensory\norgans, the task demands, and the brain-state context in which behaviors have\nto be produced, this physiological space-time manifold can be dynamically\ntwisted, bent, and shaped in an optimal information processing configuration\nthat, at any given moment, endows us with our best neuronal shot to achieve\nour goal-oriented behaviors. This notion of a cortical neuronal space-time\ncontinuum is totally compatible with the existence of ripples of probabilistic\nregional functional specialization. Yet, in this new conception such ripples\nare neither absolute nor immutable for the duration of one’s life. Instead,\nthey can shift quickly, according to the task at hand.\n\nTHE NEURONAL SPACE-TIME CONTINUUM HYPOTHESIS\n\nFrom a physiological point of view, and in direct contrast to the classical\ntwentieth-century canon of cortical neuroanatomy, there are no absolute or\nfixed spatial borders between cortical areas that dictate or constrain the\nfunctional operation of the cortex as a whole. Instead, the cortex should be\nembraced as a formidable, but finite, neuronal space-time continuum. In this\ncontinuum, functions and behaviors are allocated or produced respectively by\nrecruited chunks of neuronal space-time, according to a series of constraints,\namong which are the evolutionary history of the species, the layout of the\nbrain determined by genetics and early development, the state of the sensory\nperiphery, the state of the internal brain dynamics, other body constraints,\ntask context, the total amount of energy available to the brain, and the\nmaximum speed of neuronal firing.\n\nEssentially, the cortex should cease to be treated as a hierarchical mosaic of\ndiscrete, segregated, highly specialized, and virtually autonomous cortical\nareas.\n\nUnlike previous incarnations—notably Karl Lashley’s equipotentiality theory—my\nconcept of a neuronal space-time continuum has no qualms in accepting that\nthere is some degree of cortical specialization, dictated mainly by the\ngeneral strategy through which the cortex and thalamocortical connections were\nlaid down during early postnatal development. But development is not destiny,\nand populations of neurons can be recruited as needed once the initial\ncortical layout is set down. Those ontogenetic specializations, like a\nfeatured soloist, sit atop a powerful symphony of multimodal and dynamic\ncortical interactions that dictate how a relativistic brain works throughout\nits unique existence.\n\nRecently, during one of my talks, a very distinguished cognitive\nneuroscientist, who seemed befuddled by my notion of a neuronal space-time\ncontinuum, asked me to explain one nagging paradox he had detected. Why would\nnature invest so much energy during early development to building all these\nhighly segregated sensory pathways, not to mention topographically organized\ncortical maps, and then, out of nowhere, decide to relinquish these efforts\nfor the relativistic mess of brain dynamics I proposed? In response I\nvolunteered that, as far as I could say after twenty-five years of seeing,\nlistening, and recording brainstorms, waves of cortical spikes do not appear\nto stop at, or care about, the aesthetically pleasing borders of old-fashioned\ncytoarchitectonics. Instead, they simply pass through them, as if those\nborders were mere fantasies created in someone else’s brain.\n\n* * *\n\nAssuming that the relativistic brain hypothesis and the neuronal space-time\ncontinuum deserve further exploration, I have dedicated the next and final\nchapter to speculate on what, if anything, could emerge from the interfacing\nof a relativistic brain with the most elaborate and smart computer ever dreamt\n(but not yet built) by humanity. But before we get there, I would like to\ndiscuss why I believe relativistic is the best word to describe the way our\nprimate brains operate.\n\nIn an engaging analysis of the “main philosophical impulses” of relativism,\nthe Irish philosopher Maria Baghramian lists three attributes, among many\nothers, that are pertinent to a relativistic view of the brain: context-\ndependence, mind-dependence, and perspectivalism. Context-dependence refers to\nthe fact that many (if not most) human decisions and judgments, as well as the\nexpression of one’s dearest beliefs, are influenced by “events that happen at\na particular time and place and to a particular person.” Mind-dependence\ninvolves the long history of philosophical thought that espouses the position\nthat the human view of reality, as well as our judgments, beliefs,\nexplanations, and scientific theories, is irrevocably tainted by the heavy\ntouch of bias imposed by the human mind, since the only perspective from which\nwe can look at the world is from inside our own brains. Given that an\nobjective “view from nowhere” is not at our disposal, perspectivalism further\nextends this argument by emphasizing that even in the case of what, at first\nglance, appears to be an objective, context-independent assertion about the\nnatural world—things like “there are nine planets in the solar system”—turns\nout to be, in Baghramian’s view, “a statement made from a human perspective\nand … informed by human perceptions and conceptions.” Accordingly,\nperspectivalism dictates that our judgments and decisions are constrained by\n“the position we occupy in time and space, as well as our interests and\nbackground knowledge.” Since considerable experimental evidence now suggests\nthat brain functions can be heavily influenced by context, relativism stands\nas a plausible theoretical framework from which to seek a better understanding\nof the vagaries of the human mind, and the complex brain from which it\nemerges.\n\nRelativism does not come naturally or easily to science. The Cartesian way of\nexamining the world could not accommodate any form of relativism in its credo.\nSolidly founded on the belief that the newly coined scientific method endowed\nmen with the ability to uncover universal facts and laws about nature, it in\nfact professed, according to Baghramian, that “the mind, i.e the inner, has\nthe function of representing the outer—the mind-independent world.” As we have\nseen, this full-hearted embrace of objective scientific truth and dominance\nover the fallible and subjective human senses and mind shaped the experimental\napproach that dominated twentieth-century neurophysiology. So, in addition to\ncarving up the brain into the smallest possible building blocks of sensory\ninterpretation, neuroscientists went out of their way to remove the unwelcome\n“confounding variables” created by that brain’s own point of view—context-\ndependence and mind-dependence. From the measurements of the receptive fields\nof individual neurons and sensory maps embedded in different brain structures,\nneuroscientists tried to infer the way the brain represents, precisely as the\nCartesians predicted, the minimalist replica of the real world created inside\nthe lab.\n\nGiven the multiple intellectual earthquakes that shook the scientific world\nfrom the mid-nineteenth century to the early decades of the century that\nfollowed, Baghramian and John Barrow properly assert that plenty of fertile\nground existed for a radical questioning of this assumed certainty in\nphilosophical and scientific thinking. After all, in 1859, Charles Darwin’s\ntheory of evolution ripped out of the ground of credibility the last roots of\nBible-based cosmology, and just five years later James Clerk Maxwell unveiled\nthe electromagnetic nature of light and predicted that its velocity in a\nvacuum is a universal constant. Soon after, faith in the existence of absolute\ntruth in nature, as well as the belief in man’s ability to prove what truth\nthere is, were both shaken to their foundations. Almost as if they had\naccepted leading roles in a play written by Nietzsche, first, in 1925, Werner\nHeisenberg and his uncertainty principle of quantum mechanics, which\npostulated that the better one measures the position (or momentum) of a given\nparticle, the worse one will estimate the momentum (or position) of the same\nparticle, pushed physics further into the realm of the very small, well beyond\nour daily perceptional abilities. Then, just a couple of years later, the\nAustrian mathematician Kurt Gödel’s incompleteness theorem unsettled the\norderly world of mathematics and logic by revealing that there are statements\nwithin arithmetic that are true but cannot be proven to be true. Moreover, the\ndiscovery of non-Euclidean geometry in the first half of the nineteenth\ncentury dislodged one of the most massive bedrocks of Cartesian thinking:\nNewton’s theory of gravity. About sixty years after the German mathematician\nBernhard Riemann’s doctoral work amazed even his adviser, the formidable\nJohann Carl Friedrich Gauss of Göttingen, a four-dimensional space-time\ncontinuum of non-Euclidean geometry built upon Riemann’s work offered the\nfundamental framework needed for a former Bern patent office clerk to reinvent\nthe laws of physics yet again.\n\nUndoubtedly, Albert Einstein’s special and general theories of relativity\nrepresent the most unabridged success of relativistic thinking ever engendered\nby a human mind. In its special version, the theory of relativity proposes\nthat, given that the velocity of light is constant in a vacuum, both space and\ntime must be perceived differently by observers who are moving at a constant\nspeed in relation to each other. Essentially, neither time nor space is\nabsolute. Instead, they must be relativized in relation to the state of motion\nof a pair of observers, moving at constant speed in relation to each other.\nSuch relativization of time and space accounts for a series of\ncounterintuitive effects, things like time dilation, wherein observed time—in\nthe classic example, two clocks carried by the pair of observers—goes out of\nsync, and for Lorentz’s length contraction, wherein observed objects contract\ndue to relative velocity. However, as the American physicist Brian Greene\nexplains in his book, The Elegant Universe, one would have to be moving at a\nsignificant fraction of light speed to be able to document that a watch is\nregistering much less time elapsed (time dilation), since the beginning of the\ntrip, than the one left with a friend who stayed on Earth. By the same token,\nonly at similarly huge speeds would an observer on Earth be able to\ndemonstrate that the length of a spaceship seems to have been reduced\nsignificantly (Lorentz length contraction). These experiences are certainly\nnot part of the relatively slow velocity of daily life. “Special relativity is\nnot in our bones—we do not feel it,” Greene writes. “Its implications are not\na central part of our intuition.”\n\nDespite the near-full embrace of the theory of relativity, the concept of\nrelativism remained highly contentious and controversial. Relativistic\nthinking generated an intense debate, involving highly contradictory views of\nwhat scientific inquiry really means. In this never-ending contest, in one\ncorner Baghramian pits the argument that scientific knowledge is universal,\nsince it can be verified anywhere at any time. For instance, according to\nNobel Prize winner Sheldon Glashow, scientists “affirm that there are eternal,\nobjective, extra historical, socially neutral, external and universal truths,\nand that the assemblage of these truths is what we call physical science.\nNatural laws can be discovered that are universal, invariable, inviolable,\ngenderless and verifiable.” Curiously enough, Glashow finishes his manifesto\nwith, “This statement I cannot prove.… This is my faith.” In the other corner\nshe places Heisenberg, who—naturally—maintained a pose of uncertainty. “The\naim of this research is no longer an understanding of atoms and their\nmovements ‘in themselves,’” he wrote in The Physicist’s Conception of Nature.\n“From the very start we are involved in the argument between nature and man in\nwhich science plays only a part, so that the common division of the world into\nsubject and object, inner world and outer world, body and soul, is no longer\nadequate and leads us into difficulties.” Here, man confronts himself alone.\n\nIn this debate, as in many other scientific issues, I defer remorselessly to\nthe views of another unsuspicious believer in the scientific method, the late\nStephen Jay Gould. Despite not subscribing to the philosophical school of\nrelativism, Gould argued that “our ways of learning about the world are\nstrongly influenced by the social preconceptions and biased modes of thinking\nthat each scientist must apply to any problem. The stereotype of a fully\nrational and objective ‘scientific method’ with individual scientists as\nlogical (and interchangeable) robots is self-serving mythology.” Instead, in\nGould’s view, “impartiality (even if desirable) is unattainable by human\nbeings.… It is dangerous for a scholar even to imagine that he might attain\ncomplete neutrality for then one stops being vigilant about personal\npreferences and their influences—and then one truly falls victim to the\ndictates of prejudice. Objectivity must be operationally defined as fair\ntreatment of data, not absence of preference.” Here, the heavy weight of\nGould’s argument sails without difficulty through the tricky hurricane winds\nunleashed by Gödel’s incompleteness theorem:\n\nSince all discovery emerges from an interaction of mind and nature, thoughtful\nscientists must scrutinize the many biases that record our socialization, our\nmoment in political and geographic history, even the limitations (if we can\nhope to comprehend them from within) imposed by a mental machinery jury-rigged\nin the immensity of evolution.\n\n* * *\n\nIn my particular definition of the brain’s own point of view, the set of\nphysiological constraints that natural evolution imposed on our brains plays\nthe equivalent role that light has on the theory of relativity: it defines the\nuniversal biological constant around which our daily, brain-created models\nhave to be relativized. Animal evolution in general, and the evolutionary\nhistory of mammals and primates in particular, has to be considered as the\nsource of the constraints around which thinking revolves because the\nanatomical and functional organization of our brains has been sculpted by the\nprocess of natural evolution. Indeed, thanks to an unpredictable series of\nenvironmental events that unfolded across hundreds of millions of years, this\nprocess has yielded an optimal blueprint for the emergence of the type of\nprimate brain that each of us enjoys, from the compact, convoluted arrangement\nof the human cortex, dictated by the necessity of limiting the size of a\nnewborn’s head so that it could slide through a mother’s birth canal, to the\nmesh of connectivity of its billions of individual neurons, which communicate\nelectrically at the mercy of metabolism, biochemistry, and physiology.\n\nFor example, the cerebral vascular system must weave through this huge mesh of\nneurons, limiting the amount of oxygen transported to the brain by red blood\ncells, and thus the oxidation process through which neuronal mitochondria\nproduce adenosine triphosphate, the main energy delivery molecule in cells.\nFor this reason, primate brains operate within the boundaries of a tight\nenergy budget. Since electrical signaling through action potentials is very\ncostly in terms of energy, our brains can only produce, as we have seen, a\nfinite number of action potentials at each given moment in time to represent a\nparticular type of message. Let’s call this primary constraint on how our\nbrain operates the fixed energy budget constraint.\n\nThere are many other such biological constraint factors. These biological\nlimits mean that, despite the wondrous feats that it can achieve, there are\nfinite and specific boundaries for what a human brain can do and how it can do\nit, restrictions defining the type and amount of raw information it can\nprocess and handle and the varieties of thinking, logic, and behaviors it can\nproduce. In this context, ruptures from the concept of an absolute truth, such\nas those provoked by Heisenberg’s uncertainty principle and Gödel’s\nincompleteness theorem, may primarily point to the existence of mental\nbarriers that the human brain may not be able to cross, a territory that will\nforever remain incomprehensible to our primate minds. That is, unless they are\nsomeday aided by a fabulous new tool that, created by the brain, helps its\ncreator to surpass its own biological prison.\n\nObviously, natural evolution also defines the biological limits of the human\nbody that the brain inhabits. These include not only the physical limits of\nour biological actuators, muscles, tendons, and bones, but also the precise\nrange and sensitivity of the arrays of peripheral body sensory organs—true\nbiological transducers—that sample information from the outside world (and\nfrom inside our bodies) to keep the central nervous system well informed. Due\nto the functional limits of our eyes, ears, skin, tongues, and noses, we see,\nhear, feel, taste, and smell just a small fraction of the world that exists\nout there. That explains why, contrary to special relativity, natural\nevolution is literally in our bones, as it is in everything else that defines\nhuman nature. This I define as the body constraint.\n\nBecause our collection of body sensors informs the brain about the current\nconditions of the outside world, the brain is also able to map the\nenvironmental constraints that limit the kinds of behaviors from which it can\nselect to reach a particular goal. Yet, evolution also granted to the human\nbrain access to a precious glimpse of past experience. Buried deep within our\nbrains, there are traces of the statistics of a planet Earth that no longer\nexists, but continues to influence how our brains operate, since it helped to\nshape the range of neurophysiological strategies and behaviors that we use to\nguarantee the fulfillment of our most fundamental goals, such as surviving and\nreproducing—and extracting the most pleasure from the, alas, few brief\ninterludes separating those two demanding tasks. As John Barrow writes,\n“Whether or not living things are aware of it, they are embodiments of\ntheories about the laws of Nature drawn from the part of Nature that they have\nencountered.” In chapter 9, I discussed how our brains have, in practical\nterms, stolen a great chunk of the control of their own future evolution from\nour genes by acquiring the capability of creating powerful tools, which work\nat all spatial scales of nature, and expand the reach of the human body. By\ncombining this toolmaking capability with a lifelong potential for learning\nand adapting, the human brain has mastered the unique art of incorporating the\nvery artifacts it creates as a seamless extension of the mental model of the\nbody it inhabits.\n\nThis plastic capacity also endows the brain with the privilege of storing in\nits vast heaps of distributed memories the unique series of events that marked\nthe progression of an individual existence. Such a preciously exclusive\npersonal biography, a unique time-varying random walk through the vagaries of\nlife, includes each of our individual encounters with the outside world, all\nsocial relationships we establish with other members of our species and many\nothers, and our immersion and absorption into the prevailing culture and\nphilosophy of our time. As such, our individual history, from birth to death,\nsculpts and constrains the collection of internal brain models. I refer to\nthis variable as the individual history record.\n\nThe question then becomes, what is relativized around these three constraints\nto understand the world from the relativistic brain’s own point of view? I\npropose that, like a soccer team formed by billions of mildly related players,\ngiven a set of fixed constraints and the mandate to produce a particular goal-\noriented behavior under a particular brain-state and environmental context,\nthe relativistic brain selects, out of a huge number of possibilities,\ngroupings of spatiotemporal patterns of neural ensemble electrical activity\nthat can accomplish the job at hand. Here, by the “spatial domain” I mean the\nthree-dimensional mass of neurons (the ball in the wire and ball model) that,\nat any given moment in time, is recruited to achieve a goal, and the time\ndimension refers to the temporal distribution of spiking activity within this\nneuronal population. By relativizing this neuronal space-time continuum, the\nprimate brain has found a way to constantly and optimally select viable\nsolutions for what is known to be a typical inverse problem: that is, given an\nobserved behavioral outcome, what finite combination of brain activity should\none choose out of a gargantuan set of options to produce the desired outcome.\nIn this case, the biggest issues are which neurons to recruit, from what parts\nof the brain, and what spatiotemporal firing patterns these neurons will\nproduce. From an outside observer’s point of view, a sequence of arm movements\n(or whatever action or behavior is triggered by the neuronal firing) may look\npretty much identical. Yet, from the brain’s own point of view, the neural\nensemble firing patterns that generate this movement will be similar but never\nthe same. Rather than being a passive and faithful painter of what the outside\nworld looks like, as the Cartesians believed, the human brain actively exerts\nits own probabilistic point of view on everything upon which it puts its eyes\nor hands.\n\nFurthermore, consider the definition of a complete perceptual experience,\nresulting from observing a scene outside a moving train. According to the\nrelativistic brain hypothesis, what emerges in our minds, as a kind of mental\nmovie of that scene, results from the product of a relentless collision of\nincoming multidimensional information, containing limited data samples from\nthe external world, and the brain’s own point of view created a priori as a\nresult of a long and random history of previous encounters with similar\nscenes. This fateful collision forges the real taste and touch, as well as the\nmeaning and emotions, associated with each and every one of the exquisite\nspectrum of human sensations and feelings experienced during our succinct\nconscious existence. This is why I said, in the previous chapter, that a\nrelativistic brain starts seeing before it actually watches. Taken to its\nlimits, this shift in reference raises some intriguing consequences. For\ninstance, it challenges two of the scientific obsessions of our times: the\nquest to reproduce human consciousness through artificial intelligence and the\nclaim that a so-called Theory of Everything that could compress all that\nexists in the cosmos into some manner of universal mathematical formalism.\n\nThe arguments favoring the emergence of a relativistic brain strongly suggest\nthat the primate central nervous system, and the human mind in particular, may\nnot be compressed into any type of classical computational algorithm. In other\nwords, the human brain as a whole is simply noncomputable. As pointed out by\nBarrow, there is no equation that could ever generate things like beauty,\npleasure, and good poetry, just to mention three examples from what is likely\nan infinite list. But even though I contend that a relativistic brain is not\ncomputable as a whole, Gödel’s famous incompleteness theorem may allow us to\ncompute plenty of intelligence with the electrical storms that emerge from\nsubsets of neuronal space-time—enough, perhaps, to let an artificial device\nascend into the realm of humanity. Yet, for that to happen, such a machine\nwould have to resign itself to becoming an assimilated part of a brain model\nthat defines a unique human self.\n\nOf course, if the human mind is noncomputable, there seems to be little hope\nthat theoretical physicists will be able to untangle a radically reductionist\nTheory of Everything from the deepest ten-dimensional realm of\n10-33-centimeter-long vibrating strings. What are the chances of doing so,\nwhen computing the masterpiece play manufactured by the Brazilian soccer team\nseems impossible already? Not even the great Pelé could have imagined that,\nwith a lovely magic flick of his right foot, he would have demonstrated, as\nJohn Barrow professed, that “prospective properties are beyond the reach of\nmere technique. They are outside the grasp of any mathematical Theory of\nEverything. That is why no non-poetic account of reality can be complete.”\n\nStill, we may be able to explore the deeper consequences of having our\nambitious relativistic brains freely interacting with machines, and perhaps\namong themselves, through more profound media than traditional spoken language\nor virtual chat rooms. What that future of such interfaces may bring to each\nindividual person and for our whole species is the topic of the next chapter.\nThere, I will freely speculate about what may happen in the future when about\nfourteen hundred grams of relativistic gray matter acquires the full-fledged\npower to liberate itself from its body prison and, after mingling with other\nsources of gray matter, decides to embark on a reunion trip around the\ncelestial confines from which it was born.\n\nIf such a voyage ever happens, it would crown the long epic of improbable odds\nthe human brain has endured thus far to forge concrete reality for the\nentirety of human evolutionary and personal history, having as raw material\nonly billions of noisy, biophysically challenged, probabilistic neurons. From\nits humble, stardust seeds, and after millions of years evolving quietly on a\nmodest but cozy water-splashed rock enchanted by the gravitational pull of a\nslowly decaying third-rate star, who could possibly imagine that, through\nnatural evolution, the human brain could acquire the privilege to capture and\nhold the very relativistic core of the cosmos?\n\nWhen this extraordinary time comes, when human brain activity can be freely\nbroadcast to the universe, a few may argue that we inadvertently risk giving\naway the intimate secrets of our humanity to whoever out there cares to\nlisten. I do not fear that. For whatever galaxies those waves of thoughts sail\nthrough, our audiences will be far more intrigued by the fact that when the\nmoment finally came to create the human brain, it seems that our gods had no\nchoice but to become master dice throwers.\n\n\n13\n\nBACK TO THE STARS\n\nThe ritual, although well known by its participants, never seemed to grow old\nas the years went by. Late each afternoon, when the lazy tropical sunset began\nto inhibit the children’s outside play, I could not wait for the moment when,\nalways in silence, Lygia would stroll graciously toward her favorite spot in\nthe living room to become my secret and willing accomplice. Inside her\nhandsome white house with the genuine Tupi-Guarani hammock slung across the\nsecond-floor balcony, tucked into an amiable cul-de-sac, in Moema, a southern\nsuburb of São Paulo, Lygia was certain I would never even be late for our\ndaily musical rendezvous.\n\nShe was right. More than anything, every day, as I passed through the front\ndoor of her house without announcing myself, what I really wanted to witness\nwere her imposing steps and the plume of rosewater she left behind as she\nmoved elegantly to the box piano she had befriended through life’s happy and\nnot-so-happy moments.\n\nLygia Maria Rocha Leão Laporta had always been a beautiful, charming woman,\nand even if aging had brought a single shock of white to her otherwise\nimmaculate black hair, it had not diminished the brightness that emanated from\nher light green eyes. Her hands, though delicate, carried the purpose and\nwisdom of someone who over many decades had explored the infinite possible\ncombinations of exquisite movements, each and every one of them carefully\ndesigned and obsessively rehearsed, first in her head, and later by her\nfingers, in order to translate a long sequence of written notes, mixed with\nintimate emotions and memories, into a personal expression of music composed\nby someone else’s brain hundreds of years earlier.\n\nListening and playing music was a large part of Lygia’s life after her\nretirement. The rest of her time was devoted to learning anything and\neverything that she could in a single, fleeting life. Lygia knew very well the\npreciousness of time. At thirty-eight years of age, after losing the love of\nher life, her husband Vicente Laporta, to a brain tumor, she found herself in\ncharge of her entire family. Alone, she raised two daughters and supported her\nmother, father, and brother, while working as a career civil servant.\nUnbeknownst to many of her friends and relatives, during those hard years\nLygia managed to keep the most cherished of Vicente’s dreams alive: the\ntechnical commercial school he had founded in 1943 in São Paulo. Vicente’s\nambition, the one he described to his bride on the day they first met, was to\ncreate similar schools around the country, so that students who could not\nafford or could not gain access to the few universities that then existed in\nBrazil would have an opportunity to claim better jobs and a better life\nthrough education. Although Vicente did not live long enough to pursue his\nfull ambitions, Lygia continued to carry them for as long, and as far, as she\npossibly could.\n\nWhen Lygia retired from her job in the mid-1960s, I became her sole student,\nin an informal school that matched Vicente’s aspirations in its own special\nway. During those years, she was my best friend, my teacher, my true love, the\nperson I could trust unconditionally to explain everything that I did not\ncomprehend. So it was that the first museum I ever visited was the Museu do\nIpiranga, on the day in 1972 when we celebrated the 150th anniversary of\nBrazil’s independence from Portugal. The first opera I ever heard was La\nBohème, at São Paulo’s Teatro Municipal. The first time I saw the placid, lazy\nwaves of the Atlantic Ocean was in the harbor city of Santos. In each of these\nunforgettable events, it was Lygia’s hand that carried me into a completely\nnew world, full of adventure, magic, and captivating people. But nothing was\ncomparable to what I learned in Lygia’s neat office, where I boldly traveled\nto places far beyond, out there, where only Captain Kirk and Dr. Spock, the\nRobinson family, and Dr. Zachary Smith and his robot dared to go. In Lygia’s\noffice I discovered how men learned to fly, and then, still not satisfied, how\nthey decided, as Jules Verne imagined, to venture into the vast and empty\ncosmos. When Neil Armstrong landed on the moon in that magic summer of 1969,\nLygia and I were sitting together in front of a black-and-white TV, both\ntrying hard not to laugh as the people around us asserted that the broadcast\nwas a Hollywood hoax.\n\nBy far, however, my favorite experiences came when Lygia, full of intense\npassion and tenderness, kindly bowed to her soccer-muddy but loyal follower\nand took her seat at the humble piano.\n\nKnowing that I was observing every minuscule gesture of her performance, she\nseemed to take great care in elaborating every hand movement, as if attempting\nto carve a memory to last a lifetime. Clearly, she succeeded, for even now I\ncan remember the way she rested her two delicate but determined hands on the\nkeyboard before she started each piece. Indeed, I can still recall the\nsensation I felt when time seemed to cease ticking and the air in the room\nvoluntarily stilled in anticipation of the moment when the first few notes\nwould blast from the piano, like a tropical thunderstorm exploding in the\natmosphere.\n\nAlthough Lygia’s choice of music changed from day to day, she more often than\nnot devoted that opening salvo to express her devotion to the illustrious\nPolish composer Frédéric Chopin. Today, more than forty years after the\nafternoon when Lygia played her last concerto for me, I cannot listen to the\nfirst few notes of Chopin’s Polonaise héroïque without revisiting those\nevenings in my grandmother’s living room, where I discovered that, apart from\nmemory, learning was the greatest of our brain’s gifts. Unfortunately,\nChopin’s music will also be forever associated in my brain with the memories\nof my first unexpected and stunning acquaintance with the irremediably\ndevastating effects that an insidious neurological disorder, progressing\ncovertly, can have on someone’s life. For it was in that same living room on a\nsummer afternoon, that instead of playing, Lygia inexplicably stared at the\npiano for a few minutes in silence, until she turned to me and, without\nshifting her delicate hands from the keyboard, let two rivers of tears run\nfrom her puzzled eyes to tell me that she could no longer remember the\nsequence of movements she had repeated daily for most of her conscious adult\nlife. The memories of how to produce the music she loved had abandoned her\nmind.\n\nUnknown to her and to all of us at the time, Lygia had been suffering for\nseveral years from a continuous series of very small stroke episodes that\ngradually and mercilessly destroyed most of the upper layers of her frontal\nand parietal cortices. These cortical strokes resulted from thousands of\nminuscule blood clots—in medical terms, “emboli”—clogging the small blood\nvessels in her brain. These emboli did not produce any clear symptom until the\ndestruction of brain tissue reached a critical level around the time Lygia’s\nconcerts came to an abrupt end. For years thereafter, she experienced a\ngradual and inexorable decay in her fine motor skills and memory, both of\nwhich contributed to occasional bouts of severe depression and sudden episodes\nof frightening self-awareness, in which she stated, to the despair of all of\nus and herself, that the person who once was Lygia was no more.\n\nAs Lygia’s lifetime of memories, desires, loves, plans, and dreams slowly but\nemphatically evanesced forever, first from her brain, then from her mind, she\nstarted to lose grasp of the threads of conscious contact that she had shared\nwith the people and world surrounding her. The last time we embraced, I felt,\nfor a moment, that she did not recognize me.\n\nMy grandmother Lygia had a long and productive life. She did many things and\nleft many happy memories in the minds of people who knew her. In our last\nlong-distance conversation, after a few minutes of small talk, she became\naware that it was her loyal student on the other end of the line. Without\nwasting a second, she straightened her voice and shouted:\n\n“You know what time it is? You are late again, boy!”\n\n“For what, Lygia, for what?” I simply could not understand what she meant.\n\n“For Chopin, my son. For Chopin.”\n\n* * *\n\nOver the past three decades, almost every time one of my scientific\nmanuscripts returned from the mandatory peer-review process, I had to cope\nwith the inevitable recommendation that all scraps of speculative thinking\nabout our ability to interface brains and machines should be removed from the\npaper. During those painful reckonings, I would fantasize about the day when I\ncould rescue those speculative ideas, and liberate them for others to consider\nand contemplate. That opportunity has finally arrived.\n\nSuch an exercise is not trivial given that, during the time I have spent\nconfronting the ultraconservative culture of academia, a number of science\nfiction writers and movie directors have speculated unreservedly, and at times\noverindulged in the excesses of their fertile imaginations. During 2009 alone,\ntwo Hollywood mega-productions, Surrogates and James Cameron’s Avatar,\nportrayed the stereotype of science being used to control, harm, kill, and\nconquer people with their technological wizardry. In these movies, BMIs\nallowed human beings to live, love, and fight by proxy. Their full-body\navatars were left to do the hard work of roaming the universe and, in some\ncases, seeking to annihilate a whole alien race, on behalf of their human\nmasters. Similarly violence-minded pop-culture renditions, from Firefox to the\nMatrix trilogy, help to reinforce the fear and anxiety spread by\n“futurologists,” who warn us that humanity’s doomsday waits just around the\ncorner, since a revolutionary generation of extremely smart machines are about\nto take over our planet and make slaves of us all.\n\nHere, I want to present an alternative view. After working and thinking long\nand hard about the impact of BMIs, I see a future filled with blunt optimism\nand eager anticipation, rather than with gloom and calamity. Perhaps because\nso little about the true dimensions of this future can be conceived with\ncertainty, I feel an intense calling to embrace the alluring opportunities\nthat freeing our brains from the limits of our terrestrial bodies will bring\nto our species. In fact, I wonder how anyone could think otherwise, given the\ntremendous humanistic prospects that BMI research promises to unleash.\n\nBut before I get to my vision of this future, I would like to allay some of\nthe concerns voiced about the prospect of supremely intelligent machines to\nemulate, surpass, and dominate the many gifts of the human mind. Although I do\nnot doubt for a minute that very sophisticated forms of machine intelligence\nmay emerge someday, there is one virtually insurmountable impediment that any\ncreator of such machines will face: it is extremely unlikely that any\nbootstrapped computing routine will ever be able to capture the precise\ntemporal sequence of historical contingencies, at either the personal or\nevolutional time scale, that conspired to generate the human brain. In his\nbreathtaking book Wonderful Life, Stephen Jay Gould masterfully lays down the\nbasis for this argument by proposing the thought experiment he dubbed\n“replaying the life tape.” In his view, no matter how many billions of\nmicroprocessors, teraflops, and terabytes or how many millions of artificial\nnucleotides are at one’s disposal, the humongous efforts to create artificial\nintelligence will fail miserably, if the main and only goal is to build a mind\ncomparable to our own. Here’s how the experiment would unfold, according to\nGould:\n\nYou press the rewind button and, making sure you thoroughly erase everything\nthat actually happened, go back to any time and place in the past—say, to the\nseas of the Burgess Shale. Then, let the tape run again and see if the\nrepetition looks at all like the original. If each replay strongly resembles\nlife’s actual pathway, then we must conclude that what really happened pretty\nmuch had to occur. But suppose that the experimental versions all yield\nsensible results strikingly different from the actual history of life? What\ncould we then say about the predictability of self-conscious intelligence?\n\nThen, Gould offers his favored prediction for the most probable outcome of the\nexperiment:\n\nAny replay of the tape [of life] would lead evolution down a pathway radically\ndifferent from the road actually taken. But the consequent differences in\noutcome do not imply that evolution is senseless, and without meaningful\npattern; the divergent route of the replay would be just as interpretable,\njust as explainable after the fact, as the actual road. But the diversity of\npossible itineraries does demonstrate that eventual results cannot be\npredicted at the outset. Each step proceeds for cause, but no finale can be\nspecified at the start.… Alter any early event, ever so slightly and without\napparent importance at the time, and evolution cascades into a radically\ndifferent channel.\n\nThe particular array of contingencies that determined the evolution of the\nhuman brain may never be revisited, ever again, anywhere in the universe.\nSilicon-based consciousness, if it ever emerges, will almost certainly\nmanifest itself in ways that are very distinct from our human version. As\nsuch, it is easy to see that our peculiar history also cannot be compressed\ninto any computational algorithm, dashing any hope that machines, computer\nprograms, or artificial forms of life could be subjected to an identical roll\nof evolutionary pressures generated by any computer code or other man-made\ngizmo. Effectively, one could say that, as a fair quid pro quo for carrying\nhistory’s bequest within its circuits, the brain has been afforded the\nultimate immunity against attempts to mimic or reproduce its most intimate\nsecrets and skills.\n\nThe shielding provided by historical contingencies does not, however,\nguarantee that advanced machines may not one day come to dominate or even\ndecimate the human race. Yet, I would rank the probability of such an event\never taking place at a level much lower than a multitude of other more\npalpable catastrophes that might account for the collapse of humanity.\nEnvironmental destruction, pandemics, famine, nuclear war, climate change,\nlack of freshwater, another meteor collision, the depletion of the ozone\nlayer, even an alien invasion clearly have a higher probability of occurring\nand determining the downfall of our species than a potential coup d’état by\nthe machines. In the very unlikely cataclysmic event that playing the “tape of\nlife” could wreak such a destiny upon us, we can at least rest assured that\nour silicon conquerors will never, as John Barrow asserted, be able to\ncomprehend the immortal meaning of such human verses:\n\n“Fear, O Achilles, the wrath of heaven;\n\nthink on your own father and have compassion upon me, who am the more\npitiable,\n\nfor I have steeled myself as no man yet has ever steeled himself before me,\n\nand have raised to my lips the hand of him who slew my son.”\n\nThus spoke Priam, and the heart of Achilles yearned as he bethought him of his\nfather.\n\n* * *\n\nPersonally, I would rather discuss how in the future humanity might take full\nadvantage of the talents of the relativistic brain—its ability to simulate\nreality and its avid appetite to assimilate artificial tools, both to bypass\nneurological damage and to augment our reach and perceptions. As in my\nexperiments recording neural ensembles, time will be my trusty guide. I will\ntherefore begin by describing the biomedical applications of BMIs that will\nlikely emerge in the next ten to twenty years. I will then move to a more\ndistant future, perhaps several decades from now, when BMIs will be more\ncommonplace, allowing us to merge with computational and virtual tools,\ndevices, and environments. I will end my speculative voyage with what the\nremote future may bring to our species, when our mental allegiance to mortal\nbodies becomes less and less determinant of our way of living. Though I will\nnot go into the details of the possible neuroengineering my vision entails as\nI look farther into the future, I am confident that the technological\nsolutions will be found to make it a reality.\n\nIn the next two decades, brain-machine-brain interfaces, built by connecting\nlarge chunks of our brains through a bidirectional link, may be able to\nrestore aspects of humanity to those who have succumbed, as in the case of my\ngrandmother Lygia and my mentor Dr. César Timo-Iaria, to devastating\nneurological diseases. Possibly within a decade or two, BMIs will likely begin\nto restore neurological functions to the millions of people who can no longer\nhear, see, touch, grasp, walk, or talk by themselves.\n\nAn international research consortium, the Walk Again Project, which I\ncofounded, offers a first glimpse of this future. Conceived a few years after\nBelle and Aurora demonstrated the feasibility of linking living brain tissue\nto a variety of artificial tools, the project aims to develop and implement\nthe first BMI capable of restoring full body mobility to patients suffering\nfrom severe body paralysis, whether it resulted from traumatic lesions of the\nspinal cord or from a neurodegenerative disorder (see [Fig.\n13.1](part0017.html#fig13-1)). To accomplish this lofty goal, we are\nengineering a neuroprosthetic device that will allow paralyzed patients to use\na BMI to control the movements of a full-body exoskeleton (see [Fig.\n13.2](part0017.html#fig13-2)). This “wearable robot”—designed by none other\nthan Gordon Cheng, now at the Technical University of Munich, the genial\nroboticist who made CB-1 learn how to walk under the control of Idoya’s motor\nthoughts—will grant the patient control over his or her upper and lower limbs,\nand sustain and carry his or her body, as dictated by the person’s voluntary\nwill.\n\n![image](../images/00050.jpeg)\n\nFIGURE 13.1 Cortical Neuroprosthesis for Restoring Motor Functions. A drawing\nillustrates how a cortical neuroprosthetic device may one day help patients\nparalyzed due to a spinal cord lesion (see text for details). (Adapted from\nM.A.L. Nicolelis, “Brain-Machine Interfaces to Restore Motor Function and\nProbe Neural Circuits.” Nature Reviews Neuroscience 4 [2003]: 417–22.)\n\nWe are basing this feat of neuroengineering on the ten neurophysiological\nprinciples, derived empirically from our BMI experiments with Eshe, Aurora,\nIdoya, and many other animals, that have been described throughout this book.\nThe BMI-controlled exoskeleton will require a new generation of high-density\nmicroelectrode cubes that can be safely implanted in the human brain and\nprovide reliable, long-term simultaneous recordings of the electrical activity\nof tens of thousands of neurons, distributed across multiple brain locations.\nIndeed, to make BMIs clinically relevant and affordable, such large-scale\nbrain activity recordings will have to remain stable for at least a decade\nwithout any need for surgical repair. Custom-designed neurochips will be\nchronically implanted in the skull, which will allow us to condition and\nprocess the brain’s electrical patterns into signals capable of driving the\nexoskeleton. To reduce the risk of infection and damage to the cortex, these\nneurochips will also have to incorporate low-power, multichannel wireless\ntechnology, capable of transmitting the collective information generated by\nthousands of individual brain cells to a wearable processing unit, about the\nsize of a modern cell phone. This unit will be responsible for running\nmultiple, independent computational models designed to optimize the real-time\nextraction of motor parameters from the brain-derived electrical signals. It\nwill also control all of the training programs employed as the patient learns\nhow to operate the neuroprosthetic device.\n\n![image](../images/00051.jpeg)\n\nFIGURE 13.2 Design of the whole-body exoskeleton to be utilized by the Walk\nAgain Project. (Courtesy of Dr. Gordon Cheng, Technical University of Munich.)\n\nThe populations of neurons we sample to feed into this BMI will be distributed\nacross multiple cortical and subcortical structures. Time-varying, kinematic,\nand dynamic digital motor signals will be extracted from this collective\nelectrical activity of the brain and used to control the actuators distributed\nacross the joints of the robotic exoskeleton. Following the current state-of-\nthe-art techniques available for controlling such a device, high-order, brain-\nderived motor commands will interact with local electromechanical circuits,\ndistributed across the exoskeleton, in order to mimic spinal-cord arc\nreflexes. This will permit the patient to initiate the walking step cycle,\nadjust gait speed, and trigger postural and gait adjustments in response to\nunexpected changes in the terrain. Meanwhile, low-level motor adjustments will\nbe handled directly by the exoskeleton’s electromechanical circuits. This will\ncreate a continuous interplay between brain-derived signals and robotic\nreflexes, a mode known as shared brain-machine control. I also envision force\nand stretch sensors, distributed throughout the exoskeleton, that will\ngenerate a continuous stream of artificial touch and proprioceptive feedback\nsignals to inform the patient’s brain of the device’s performance. This array\nof signals will be delivered via multichannel cortical electrical\nmicrostimulation or through multiple light sources that stimulate light-\nsensitive ion channels deployed directly into the patient’s cortex. Based on\nour lab experiments with BMIs, I expect that after a few weeks of interaction,\nthe patient’s brain will completely incorporate, via a process of experience-\ndependent plasticity, the entire exoskeleton as a true extension of the\nperson’s body image. At that point, the patient will be able to use the BMI-\ncontrolled exoskeleton to move freely and autonomously around the world.\n\nAs the Walk Again Project takes off, other lines of research are starting to\nshow promise for the development of analogous devices to treat the symptoms of\nneurological disorders. For example, I mentioned in chapter 12 that Romulo\nFuentes, Per Petersson, and I discovered in 2009 that high-frequency\nelectrical stimulation of the surface of the spinal cord restored locomotion\nin both mice and rats that had been previously depleted of the\nneurotransmitter dopamine. Following this initial dopamine depletion, the\nrodents became severely rigid and immobile, exhibiting extreme difficulty in\ninitiating any type of voluntary body movements—symptoms typical of\nParkinson’s disease. Because we could record the electrical activity of\npopulations of neurons distributed across multiple cortical and subcortical\nstructures in these animals, we observed that as the mice and rats became\nrigid, populations of neurons located in the motor cortex and the striatum\nstarted to fire in synchronous bursts of action potentials. All together, this\nsynchronous activity produced an intense low-frequency neuronal oscillation\nthat resembled an epileptic seizure (see [Fig. 13.3](part0017.html#fig13-3)).\nInterestingly, when we gave L-DOPA, the drug of choice to treat the early\nstages of Parkinson’s disease, to the mice and rats, these oscillations were\ndisrupted within a few minutes. Body rigidity gradually melted away, and the\nanimals could move again. Once the effect of L-DOPA diminished, usually within\na couple of hours, or once the animals developed a tolerance to the drug,\nusually within a few weeks, the rigidity returned with a vengeance.\n\nTen years earlier, Erika Fanselow and I had investigated methods for\npreventing epileptic seizures in rats. At that time, we had demonstrated that\nelectrical stimulation of the trigeminal nerve disrupted the synchronous\noscillations that marked the seizures we induced in our rats. This allowed the\nrats to escape from the behavioral arrest of an epileptic attack, and even\nprecluded the onset of a new one. When I saw that the Parkinson’s-like\nrigidity in the mice and rats was caused by synchronous oscillations that\nresembled those of an epileptic attack, I suggested to Fuentes and Petersson\nthat we try the same approach with our dopamine-depleted rodents.\n\nWe first tried to electrically stimulate the trigeminal nerve. That produced\nsome relief of facial rigidity, but no major effect on the rest of the body.\nWhat initially looked like a failure, in reality turned out to be the precious\nhint that we needed. Soon afterward, we switched the target of our electrical\nstimulation to the dorsal surface of the spinal cord. This new target had\nseveral advantages: it was much easier to reach, through a much less invasive\nsurgical procedure, and it afforded us the opportunity to stimulate a huge\nnumber of large nerve fibers that run through that region on their way to the\ncentral nervous system, where they influence a big chunk of the rat parietal\nand frontal cortices. It took only a brief trial to realize this was an\noptimal solution to the intense oscillations that were afflicting our mice and\nrats. As long as we kept the stimulation going, the rodents could roam around\ntheir cages, free from the rigidity produced by their Parkinson’s-like\ndisorder. Moreover, by continuously applying this stimulation, animals with a\nsevere depletion of dopamine could be treated with a much smaller dose of\nL-DOPA. That lower dose reduced the side effects from the drug and the risk of\ndeveloping a tolerance to it.\n\n![image](../images/00052.jpeg)\n\nFIGURE 13.3 Treating a Parkinson’s disease-like syndrome in rats using\nelectrical stimulation of the spinal cord. On the top shelf, the stimulating\nelectrodes and the implantation approach to place them on the dorsal surface\nof the spinal cord. Middle shelf shows an implanted rat that exhibits signs of\nParkinson’s disease. On the bottom shelf two circles are used to identify\nbursts of the epileptic activity observed in a spectrogram of the rat’s brain\nactivity, that are correlated with the akinesia produced by Parkinson’s\ndisease. At 0 time, the electrical stimulation of the spinal cord started,\nusing the implanted electrodes. Notice that the epileptic activity disappears\nand, as a result, the rat was able to walk freely again (not shown). The\nspectrogram X axis represents time (0 start of stimulation) and the Y axis\nrepresents frequencies. The gray scale of the Z axis represents power or\nmagnitude of brain activity at a given frequency (see scale on the right).\n(Adapted from R. Fuentes, P. Petersson, W. B. Siesser, M. G. Caron, and M.A.L.\nNicolelis, “Spinal Cord Stimulation Restores Locomotion in Animal Models of\nParkinson’s Disease.” Science 323 [2009]: 1578–82.)\n\nUnexpectedly, we had uncovered a very simple, minimally invasive, and\ninexpensive procedure—spinal cord stimulation—that may become the basis of a\nnew therapeutic alternative for Parkinson’s patients. Similar research is\nbeing feverishly conducted around the world in the hope of developing\nneuroprosthetic devices that interface with living brain tissue to treat other\nmajor neurological disorders, such as epilepsy and depression, and to restore\nvision, hearing, and speech.\n\n* * *\n\nIn the near future, most BMI research will likely focus on the creation of\nnovel therapeutic and rehabilitation tools. In all likelihood, however, the\nfield will also contribute to a much deeper understanding of the\nneurophysiological principles that underlie the functioning of our\nrelativistic brain in its efforts to compose and distort its model of reality.\n\nI fully expect that BMI research will help to elucidate how the neuronal\nspace-time continuum forms and operates, in a tight and cohesive way,\nthroughout the course of our lives. To some degree, this issue pertains to the\ndebate of the famous binding problem, described in chapter 4, a conundrum that\nhas been haunting neuroscientists for quite some time now. By simply changing\nthe frame of reference from incoming stimuli, generated by the outside world,\nto the vantage of the brain’s own point of view, the binding problem might\ndisappear altogether, since in the relativistic brain, there is no need to\nbind anything, because no incoming stimulus was broken into discrete sensory\nbits of information to begin with. In a relativistic brain there is simply a\nsingle dynamic model of the world that is continuously refreshed by the\nconstant collisions between the brain’s internal dynamics and the matching and\nnonmatching information sensed by the body’s periphery.\n\nBesides solving the binding problem, the relativistic brain theory may also\noffer a potentially viable truce for the intellectual war waged between the\nlocalizationist and distributionist camps of cortical physiology. I believe a\ncompromise may be reached at long last if one accepts the notion that strict\nlocalization of functions in the cortex, as well as pure unimodal cortical\nrepresentations, thrive only during the early development of the central\nnervous system or during reduced and artificial states of internal brain\ndynamics. We have seen how deep anesthesia, for instance, induces a collapse\nof internal brain dynamics, artificially limiting the complexity of individual\nneuronal sensory responses and whole cortical representations. Furthermore, it\nis only when animals actively engage in the exploration of their surrounding\nenvironment that the full dynamic splendor of the cortex is revealed. In the\ncompromise I propose, discrete localization of functions and unimodal\nrepresentation dominate the early postnatal developmental stages of the\ncortex, most likely because this is when the brain’s connectivity is\nconsolidated and the central nervous system gingerly crafts its internal\nmodels of reality. That gradual building up of the simulator and its models\nmay account for the relatively long developmental periods of human childhood\nand adolescence. Indeed, this may explain why it takes several years for\nchildren to become capable of merging multimodal information describing the\nsame object, such as the sound that is associated in their native language to\nthe image of a corresponding letter or number.\n\nAt the end of this anatomofunctional maturation process, multiple\nmultisynaptic cortical and subcortical loops connect populations of neurons\nacross the cortex, giving rise to a single neuronal sea, ready to conduct the\nwaves of neuronal time that make it tick. As the brain’s own point of view\nevolves, the topographic maps, cortical structures, and strict cortical\nhierarchies become less dominant, until a point of no return is reached during\nearly adulthood. By then, the brain tissue that still betrays a vestigial\nremnant of its anatomical modules—things like the cortical barrels of the\nrodent S1, ocular dominance columns, and cytoarchitectonic borders—speak\nprimarily of times past, as indelible developmental scars of the organic\nscaffolding through which the brain had to crawl to assemble itself. Now these\nstructures impose only minor constraints on how the neuronal space-time\ncontinuum functions. Accordingly, to obtain a satisfactory comprehension of\nthe mechanisms that generate a conscious and self-aware mind out of a blob of\nneural tissue, systems neuroscientists must shift their attention away from\nthese mirages of the developmental past to follow more closely the waxing and\nwaning waves and ripples of the brain’s electrical ocean.\n\nSuch a newly acquired reliance on emergent internal brain dynamics, rather\nthan strict allegiance to Brodmannian cortical geography, will, I believe,\nlead to a much more comprehensive understanding of neurological illness, since\nbrain dynamics will become the medium through which we view all disturbances\nof the human mind. Neurological and psychiatric disorders will become linked\nto specific alterations in the timing of brain circuits and their\ninteractions. If during its regular operation the brain experiences only\ndelicate synchronous waves, during its altered states this neuronal sea may\nwitness strange whirlpools and maelstroms of firing across the neuronal space-\ntime continuum. Just as classical epilepsy is defined by a distinct pattern of\nsynchronous brain activity, many other dysfunctions of the central nervous\nsystem may one day be ranked according to the level of pathological coherent\nfiring in the brain. Considered from this perspective, the classical\ndistinction between neurological and psychiatric disorders may simply vanish.\nAs such, a better understanding of the principles of neural ensemble\nphysiology will allow us to transcend both the ignominy of our collective\nignorance about this group of particular mental states and the social stigma\nthat surrounds those who live under their cloud. Ultimately, we may be able to\nrecognize these disorders as what they truly are: mere disturbances of\ninternal brain dynamics.\n\nPreliminary support for this bold claim is already emerging in the work\ncarried out by my former graduate student and postdoctoral fellow, Kafui\nDzirasa, now an assistant professor of psychiatry at Duke’s medical school.\nDzirasa has systematically investigated variations in internal brain dynamics\ninduced in a variety of transgenic mice, most of which have been created in\nDr. Marc Caron’s lab at Duke. In each one of these mice, a gene has been\nselectively removed. The mice have then been subjected to a sequence of\npharmacological manipulations during adulthood. This allowed Dzirasa to\nengender in the mice a collection of stereotypical behaviors that resemble\nthose observed in human patients suffering from a variety of cognitive\ndysfunctions and psychiatric disorders. By recording local field potentials\nand populations of single neurons in up to ten different brain structures in\nthe mice, Dzirasa identified specific alterations in dynamic brain\ninteractions that seem to be tightly correlated with the emergence of the\nabnormal phenotype expressed in each of the animals.\n\nAlthough it is still difficult to establish a causal link between such\nneurophysiological alterations and expressed behaviors, Dzirasa has some\nstriking evidence that may cross this pretty high threshold—including the\nidentification of a potential neurophysiological basis for some of the\nstereotypical motor behaviors associated with obsessive-compulsive and bipolar\nsyndromes. As in our experiments with dopamine-depleted rodents, the brains of\nthese transgenic mice reveal differences in the level of coherent synchronous\nfiring across the neuronal space-time continuum. Since we are now able to\nrecord large-scale brain activity for up to one year in these mice, we have\ndocumented, for the first time in the history of neuroscience, the progressive\nand inevitable neurophysiological transformation that an otherwise healthy\nbrain undergoes when it takes a fateful turn toward one of the mental cul-de-\nsacs in which minds sometimes become stuck.\n\nIn the future, we expect to incorporate the information we are collecting in\nmany strains of transgenic mice into a new framework, a more elaborate,\nmultidimensional version of our Loch Ness state-space. This unabridged\ndepiction of normal and altered dynamic brain states may allow us to classify\nmost classical neurological and psychiatric disorders, similar to how we now\nassociate different rat behaviors with distinct clusters on the three-\ndimensional state-space plot. In the long run, this research could allow\nneurologists to measure how a patient’s brain dynamics behave and predict,\nwell in advance of the appearance of any symptoms, whether there is a\nsignificant chance that a person may someday wake frozen by Parkinson’s\ndisease, depressed, or living in a completely new reality, dictated by full-\nblown mania, paranoia, or delirium. And by the same token, such a unified\ndynamic framework may allow physicians to evaluate quantitatively whether\ntheir treatments are working efficaciously and benefiting their patients.\n\n* * *\n\nThe near-term prospects of basic and applied BMI and neuroscience research\nwill entail an explosive acceleration in the convergence of disciplines\nranging from computer science to biology, from engineering to medicine, and\nfrom mathematics to philosophy. As rising generations of neuroscientists\nemploy a much broader bag of intellectual and technical tools, a multiplicity\nof transformative technologies will emerge. Traditional neuroscience\ndepartments and brain research institutes will have to adapt, in order to\nprovide for the free interaction of experimental data, large-scale computer\nsimulations, and theoretical work that will become the rule rather than the\nexception.\n\nAlready, many brain research collaborative initiatives have been established\nin an attempt to adapt to the neuroscience of the future. In fact, in March\n2003, I launched a large-scale scientific endeavor of my own: the founding of\na “Campus of the Brain,” that today is known as the Edmond and Lily Safra\nInternational Institute of Neuroscience of Natal (ELS-IINN), a nonprofit\nacademic effort located in the little town of Macaiba in the underdeveloped\nnortheast coastal region of Brazil. The campus is dedicated to a threefold\nmission: to push the envelope of brain research to the limit; to celebrate the\nhuman brain’s amazing achievements, manifested in terms of art, science, and\nculture; and to disseminate the resulting knowledge of the brain to the local\nsociety, through a series of social and economic projects—including a\nchildren’s science education project, a women’s health program, and an\nindustrial research and technology park—aimed at uplifting the education,\nhealth, and living standards of the towns and communities close to the campus.\nNeuroscience as an agent of social transformation: I bet you never imagined\nthat such a concept existed. One of the most ambitious projects will be the\npublic school attached to the Campus of the Brain, in which children will be\nenrolled at the moment their pregnant mothers begin attending the prenatal\ncare program offered by the campus’s health center. By now, you may have\nguessed the name it will bear: the Lygia Maria Rocha Leao Laporta Public\nSchool.\n\nIn years to come, I sincerely hope the Brazilian Campus of the Brain becomes a\nmodel for establishing the types of multidisciplinary collaborations needed to\nactualize the future of brain-machine interfaces, since such scientific\nnetworks may greatly facilitate the receptivity to employing BMIs beyond the\nrealm of rehabilitative medicine. Take, for example, what could happen within\na few decades if we master technologies that allow humans to utilize the\nelectrical activity of their brains to interact with all kinds of\ncomputational devices. From tiny personal computers that we carry with—or\npossibly within—us, to remote distributed networks aimed at mediating our\ndigital social interactions, from the most mundane text processing to the most\nelaborate simulations of our secret dreams, our daily lives will look and feel\nsignificantly different from what we are accustomed to today.\n\nFor starters, interacting with the operating system and software of one’s\npersonal computer will likely become an embodied adventure, as our brain\nactivity is used to grab virtual objects, trigger programs, write memos, and,\nabove all, communicate freely with other members of our favorite brain-net, a\nconsiderably upgraded version of online social networking. The fact that\nIntel, Google, and Microsoft have already created their own brain-machine\ndivisions shows that this idea is not far-fetched. The main obstacle is the\nneed to develop a noninvasive method to sample the high-resolution brain\nactivity needed to make such BMIs a reality. I feel confident that a solution\nwill be found in the next couple of decades.\n\nThen, what may sound unimaginable will become routine as augmented humans make\ntheir presence felt in a variety of remote environments, through avatars and\nartificial tools controlled by thought alone. From the depths of the oceans to\nthe confines of supernovas, even to the tiny cracks of intracellular space\ninside our own bodies, human reach will finally catch up to our species’\nvoracious ambitions to explore the unknown. It is in this context that I\nenvision our brains will eventually complete their epic journey of\nemancipation from the obsolete terrestrial bodies they have inhabited for\nmillions of years and utilize bidirectional, thought-driven interfaces to\noperate a myriad of nanotools that will serve as our new eyes, ears, and hands\nin the many tiny worlds crafted by nature. Worlds, made of clumps of atoms or\nballs of cells, into which our bodies could never have penetrated, but our\nthoughts certainly will, unopposed, unimpeded, and without any hesitation.\nTraveling in the opposite direction, we will likely be able to operate\nremotely controlled envoys and ambassadors, robots and airships of many shapes\nand sizes, sent on our behalf to explore other planets and stars in distant\ncorners of the universe and capable of placing strange lands and scenery at\nthe tip of our mental fingertips. With each step in our boundless\nexplorations, the tools created by our descendants for these mind voyages will\ncontinue to be assimilated by their brains as further extensions of their\nselves, defining a brain’s own point of view that is far, very far, beyond\nanything we can possibly imagine today. This thought, I must confess, brings\nme an enormous feeling of elation and awe, a profound emotion that, I presume,\nmay resemble only something that a humble Portuguese sailor, five hundred\nyears ago, may have experienced when, at the end of a long and life-\nthreatening journey, he found himself staring at the bright sandy shores of a\ncompletely new world.\n\nAt this point in our speculative trip into the future, we might wonder how\nthis tremendous expansion in human action and perception will affect the very\nrendering of reality that emanates from each of our progeny. Will they see and\ninterpret the universe as we do, or will their daily experiences, ethics,\nculture, and science be so different from ours that a dialogue would be as\nimpossible and meaningless as trying to debate the situation of the world’s\neconomy with a friendly group of Neanderthals today?\n\nUltimately, the most astounding consequence of liberating the human brain from\nthe human body could be the unleashing of a powerful and unexpected set of\ncontingencies capable of decisively influencing both the direction and the\nspeed with which our species’ “life tape” plays in the distant future. In\nother words, by the sheer power of what it can accomplish, emancipated from\nthe constraints and vulnerabilities imposed by the human body, our\nrelativistic brains may come to conquer the most coveted prize available to\nthem in the universe: a hand in steering the evolution of our species.\n\nCould such a complete liberation of the brain allow us to blur, or even\neliminate, the once inexpugnable physical borders that define an individual\nhuman being? Could we one day, down the road of a remote future, experience\nwhat it is to be part of a conscious network of brains, a collectively\nthinking true brain-net? Assuming for a moment that somehow, through some\namazing and harmless future technology, this brain-net became real, could the\nindividuals participating in it not only communicate back and forth with one\nanother just by thinking, but also vividly experience what their counterparts\nfeel and perceive, as they seamlessly adhere to this true “mind meld”? Very\nfew people today would likely choose to venture into these unknown waters, but\nit is impossible to know how future generations will react, given the\nopportunity to experience such a literally mind-boggling experience.\n\nAccepting that all these stunning scenarios could actually take place, and\ntaking for granted that such a collective mind meld became consensually\naccepted as an ethical way through which future generations interact and share\ntheir humanity, could these descendants of ours wake up one morning and simply\nrealize that they had peacefully given birth to a different human species\naltogether? It is not inconceivable that our human progeny may indeed muster\nthe skills, technology, and ethics needed to establish a functional brain-net,\na medium through which billions of human beings consensually establish\ntemporary direct contacts with fellow human beings through thought alone. What\nsuch a colossus of collective consciousness may look like, feel like, or do,\nneither I nor anyone in our present time can possibly conceive or utter. Like\nthat goal of the Brazilian soccer team at the 1970 World Cup, this is the kind\nof greatness that one can only fully appreciate by experiencing its result as\nit unfolds, in all its complexity. It may, without our expecting it, proffer\nthe ultimate human perceptual experience: to discover that each of us is not\nalone after all, that our most intimate thoughts, experiences, anguishes,\npassions, and desires, the very primordial stuff that defines us as humans,\nare shared by billions of our fellow brothers and sisters. The tremendous\ncomfort that this might bring to so many, who feel like prisoners of their own\nhaunting thoughts of isolation, inferiority, prejudice, misconception, and\nsocial inappropriateness, is difficult to imagine.\n\nAlthough I am very aware that my particular optimistic viewpoint may not qualm\nall anxieties, I have no doubt that the rapacious voracity with which most of\nus share our lives on the Web today offers just a hint of the social hunger\nthat resides deep in human nature. For this reason, if a brain-net ever\nbecomes practicable, I suspect it will spread like a supernova explosion\nthroughout the fabric of human societies. It’s true that as individuals\nreadily use their thoughts to control a huge range of artificial devices and\ncommunicate with one another, they may come to resemble something very unlike\nwhat we today call the human race. To this, I say: Given that our own species\nlife’s tape will continue to play its unpredictable tune, independent of what\nwe think about the future, and that evolution will certainly not halt its\ncourse at some arbitrary stage down the road, why should we worry about who or\nwhat will succeed us thousands or even millions of years from now?\n\nHaving struggled with this questions for a long while, I believe the main\nreason we should worry about this future is neither grounded in any fear for\nour own particular destiny as a species, nor based on a reflexive repulsion to\nthe idea that our species and way of life may, one day in the remote future,\nbe replaced. Rather, I believe we owe to the safeguarding of the human\nheritage the same high standards of ethical and moral conduct we should\ndevote, but unfortunately do not often do, to the preservation of every single\nform of life that inhabits our humble planet. From the stunning clouds of\ninsects, communities of plants, squadrons of blue macaws, and packs of\ncapybaras that roam through every cubic foot of the vast tropical rain forest,\nto the polar bears of the North Pole, to the spotted owls of western North\nAmerica, and even to the last strains of the dreaded smallpox virus,\npreserving the diverse ways in which life manifests itself on our planet is\nour best way to pay tribute to the extraordinary circumstances that gave birth\nto the conscious mind. Preserving this biological patrimony is one of the\nfirst steps we can take to bestow a moral legacy to future generations, a\nlegacy that by necessity must embrace not just every contextual trace, but\nevery tiny bit of thinking, every imagined deed, good or bad, and each drop of\nthe improbable neuronal elixir that bestows us with our sense of being.\n\nHow could we ever succeed in portraying the remarkable diversity of human\nexperiences that compose the tale of our species’ unique odyssey?\nCapriciously, the answer to this challenge may rest on the talents of our\nrelativistic brain.\n\nBack in 1945, the great Kurt Gödel stunned the scientific world, yet again, by\nproposing a new solution to Einstein’s equations of general relativity.\nAccording to Gödel’s solution, traveling back in time should be considered a\ndistinct and real possibility in a relativistic universe governed by a space-\ntime continuum and Riemannian geometry. Yet, despite being a mathematical\npossibility, traveling back in time would be far from trivial in practical\nterms and, as far as we can tell, is not the kind of experience that abounds\nin the universe—that is, unless you changed your frame of reference to another\nuniverse, another space-time continuum, the one inhabiting the space between\nyour ears! There, within the confines of our inner neuronal universe, time\ntravel becomes quite a trivial exercise; what any theoretical physicist would\nconsider an astounding feat out in the space-time fabric of the stars, any of\nus can accomplish by merely swimming through the memories accumulated and\ncarefully kept by the fabric of notes of our neuronal symphony, the waves that\ncross the space-time continuum of our minds.\n\nIf the future depicted in this chapter materializes, it takes just a minor\nleap of imagination to contrive that, in the midst of their newly acquired\nwisdom, our progeny may also decide to cross yet another Rubicon in our\nspecies’ epic history and strive to document, for the benefit of future\ngenerations and the posterity of the cosmos, the richness and diversity of\ntheir human inheritance. Such an inestimable treasure could only be assembled,\nI suggest, by preserving the irreplaceable, first-person narrative of each and\nevery single human lifetime story, the unique account of our mortal existence\nthat, after a brief temporary stay in one’s mind, is irremediably lost at the\nend of our lives, in a rare wasteful lapse of nature.\n\nI can envision how a more thoughtful future society would concede to the\ndownload and storage of these lifetime chronicles, not only as a rite of\npassage at life’s end, but also as a final tribute to yet another exceptional\nhuman life that inhabited this universe. Thereafter, each of these perennial\nrecords would be revered as a uniquely precious jewel, one among billions of\nequally exclusive minds that once lived, loved, suffered, and prospered, until\nthey, too, became immortalized, not clad in cold and silent gravestones, but\nreleased through vivid thoughts, intensely lived loves, and mutually endured\nsorrows.\n\nBy then, the same wondrous technology and ethical covenant that will permit\nthoughts to be preserved forever will also allow them to be broadcast back to\nthe edges of the universe, bringing, in the end, the ultimate sense of closure\nand solace to our kind that only a return to the maternal womb can afford. At\nthis remote future juncture, I can still conceive one final eventful turn, as\nthe fair and proper apotheosis for man’s most improbable voyage through the\ncontingencies of immemorial times: the crowning of our relativistic brain as\nthe only meaningful trinity ever to bestow its blessings upon us. For, in\naddition to lodging the skillful sculptors of our sense of reality and self,\nand being the loyal guardians of our memories, it will be up to our brains,\nfrom there on, to effortlessly share, at light’s speed, human symphonies,\ncarefully composed over the course of a lifetime, with whoever and whatever\nare intrigued enough, anywhere in the vast cosmos, to just follow this music.\n\nSitting on a hill, at the construction site of the Campus of the Brain, as the\never bright equatorial sun prepares to take its daily well-earned rest, I can\nonly wonder how someone, deep in our future, may one day react when he or she\nexperiences, for the first time in the midst of his or her otherwise tranquil\nlife as part of a collective consciousness, what it was to see, through mortal\neyes made of flesh, a waving sea of palm trees, gently swaying back and forth,\nfollowing the unrelenting brushing of the wind, as if they simply intended to\nblow a good-night kiss to a blossoming garden of cactus lying at their\ndelicate, imperial feet. Perhaps, if listening with care, this distant cousin\nof ours would notice that the same wind, as it swirled through the concrete\nand steel foundations of the Lygia Maria Rocha Leao Laporta Public School,\nalso seemed to whisper in my ears something I should already know: that I am\nlate again. That it is time to stop playing in the muddy street and run, run\nas fast as I can, back to that house with the always-open door, and, once\nagain, listen to Chopin.\n\n\nSELECTED BIBLIOGRAPHY\n\n1\\. WHAT IS THINKING?\n\nDawkins, Richard. The Selfish Gene. Oxford and New York: Oxford University\nPress, 1989.\n\nDeutsch, David. The Fabric of Reality: The Science of Parallel Universes—and\nIts Implications. New York: Allen Lane, 1997, pp. 120–21.\n\nFreeman, Walter J. How Brains Make Up Their Minds. New York: Columbia\nUniversity Press, 2000.\n\n______. Mass Action in the Nervous System: Examination of the\nNeurophysiological Basis of Adaptive Behavior through the EEG. New York:\nAcademic Press, 1975.\n\nGaspari, Elio. A Ditadura Envergonhada, vol. 1, Coleção As Ilusões Armadas.\nSão Paulo: Cia da Letras, 2002.\n\nHebb, Donald O. The Organization of Behavior: A Neuropsychological Theory. New\nYork: Wiley, 1949.\n\nHubel, David H. Eye, Brain, and Vision. New York: Scientific American\nLibrary/W. H. Freeman, 1995.\n\nKauffman, Stuart A. The Origins of Order: Self-Organization and Selection in\nEvolution. New York: Oxford University Press, 1993.\n\nLashley, Karl. “In search of the engram.” Society of Experimental Biology\nSymposium 4 (1950): 454–82.\n\n______. The Neuropsychology of Lashley: Selected Papers, ed. by Frank A. Beach\net al. New York: McGraw-Hill, 1960.\n\nMitchell, Melanie. Complexity: A Guided Tour. Oxford and New York: Oxford\nUniversity Press, 2009.\n\nNicolelis, Miguel A. L., Gisela Tinone, Koichi Sameshima, et al. “Connection,\na microcomputer program for storing and analyzing structural properties of\nneural circuits.” Computers and Biomedical Research 23, no. 1 (1989): 64–81.\n\nNicolelis, Miguel A. L., Chia-Hong Yu, and Luiz Antonio Baccalá. “Structural\ncharacterization of the neural circuit responsible for the cardiovascular\nfunction control in high vertebrates.” Computers in Biology and Medicine 20,\nno. 6 (1990): 379–400.\n\nSagan, Carl. Cosmos. New York: Random House, 1980, p. 4.\n\nShepherd, Gordon M. Neurobiology, 2nd ed. New York: Oxford University Press,\n1988.\n\nWeidman, Nadine M. Constructing Scientific Psychology: Karl Lashley’s Mind-\nBrain Debates. New York: Cambridge University Press, 1999.\n\nZeki, Semir. A Vision of the Brain. Oxford and Boston: Blackwell Scientific\nPublications, 1993.\n\n2\\. BRAINSTORM CHASERS\n\nAdrian, Sir Edgar Douglas. The Physical Background of Perception: The\nWaynflete Lectures Delivered in the College of St. Mary Magdalen, Oxford.\nOxford: Clarendon Press, 1947.\n\nBroca, P. Paul. “Loss of speech, chronic softening and partial destruction of\nthe anterior left lobe of the brain.” First published in Bulletin de la\nSociété Anthropologique 2 (1861): 235–38. Trans. by Christopher D. Green, York\nUniversity, Toronto, Ontario, Canada, 2003,\n<http://psychclassics.yorku.ca/Broca/perte-e.htm>.\n\nDe Carlos, Juan A., and José Borrell. “A historical reflection of the\ncontributions of Cajal and Golgi to the foundations of neuroscience.” Brain\nResearch Reviews 55 (2007): 8–16.\n\nErickson, Robert P. “The evolution and implications of population and modular\nneural coding ideas.” Progress in Brain Research 130 (2001): 9–29.\n\n______. “A study of the science of taste: On the origins and influence of core\nideas.” Behavioral and Brain Studies 31 (2008): 59–75.\n\nFinger, Stanley. Origins of Neuroscience: A History of Explorations into Brain\nFunction. New York: Oxford University Press, 1994.\n\nFritsch, Gustav, and Eduard Hitzig. “On the electrical excitability of the\ncerebrum” (1870). In Some Papers on the Cerebral Cortex, trans. by Gerhardt\nvon Bonin (pp. 73–96). Springfield, Ill.: Thomas, 1960.\n\nGall, François [Franz] Joseph. On the Functions of the Brain and of Each of\nIts Parts: With Observations on the Possibility of Determining the Instincts,\nPropensities, and Talents, or the Moral and Intellectual Dispositions of Men\nand Animals, by the Configuration of the Brain and Head, 6 vols. Trans. by\nWinslow Lewis Jr. Boston: Marsh, Capen & Lyon, 1835.\n\nGolgi, Camillo. “The neuron doctrine—theory and facts.” Karolinska Institute,\nStockholm, Sweden, December 11, 1906,\n<http://nobelprize.org/nobel_prizes/medicine/laureates/1906/golgi-\nlecture.html>.\n\nGrant, Gunnar. “How the 1906 Nobel Prize in Physiology or Medicine was shared\nbetween Golgi and Cajal.” Brain Research Reviews 55 (2007): 490–98.\n\nMörner, K. A. H. “Presentation speech.” The Nobel Prize in Physiology or\nMedicine, Karolinska Institute, Stockholm, Sweden, December 10, 1906,\n<http://nobelprize.org/nobel_prizes/medicine/laureates/1906/press.html>.\n\nRamón y Cajal, Santiago. Histology of the Nervous System of Man and\nVertebrates, vols. 1 and 2. Trans. by Neely Swanson and Larry W. Swanson. New\nYork: Oxford University Press, 1995.\n\n______. Recollections of My Life. Trans. by E. Horne Craigie and Juan Cano.\nCambridge, Mass.: MIT Press, 1989.\n\n______. “The structure and connexions of neurons.” Karolinska Institute,\nStockholm, Sweden, December 12, 1906,\n<http://nobelprize.org/nobel_prizes/medicine/laureates/1906/cajal-\nlecture.html>.\n\nRobinson, Andrew. The Last Man Who Knew Everything: Thomas Young, the\nAnonymous Polymath Who Proved Newton Wrong, Explained How We Can See, Cured\nthe Sick, and Deciphered the Rosetta Stone, Among Other Feats of Genius. New\nYork: Pi Press, 2006.\n\nYoung, Thomas. A Course of Lectures on Natural Philosophy and the Mechanical\nArts. London: Taylor and Walton, 1845.\n\n______. The Bakerian Lecture: “On the theory of light and colours.”\nPhilosophical Transactions of the Royal Society 92 (1802): 12–48.\n\n3\\. THE SIMULATED BODY\n\nBlanke, Olaf, Christine Mohr, et al. “Linking out-of-body experience and self\nprocessing to mental own-body imagery at the temporoparietal junction.”\nJournal of Neuroscience 25, no. 3 (2005): 550–57.\n\nBlanke, Olaf, Stephanie Ortigue, et al. “Stimulating illusory own-body\nperceptions.” Nature 419 (2002): 269.\n\nBotvinick, Matthew, and Jonathan Cohen. “Rubber hands feel touch that eyes\nsee.” Nature 391 (1998): 756.\n\nBrodie, Eric E., Anne Whyte, and Catherine A. Niven. “Analgesia through the\nlooking-glass? A randomized controlled trial investigating the effect of\nviewing a ‘virtual’ limb upon phantom limb pain, sensation and movement.”\nEuropean Journal of Pain 11, no. 4 (2007): 428–36.\n\nBrodmann, Korbinian. Localisation in the Cerebral Cortex (1909). Trans. by\nLaurence Garey. London: Smith-Gordon, 1994.\n\nEhrsson, Henrik, Birgitta Rosén, et al. “Upper limb amputees can be induced to\nexperience a rubber hand as their own.” Brain 131 (2008): 3443–52.\n\nHerman, Joseph. “Phantom limb: From medical knowledge to folk wisdom and\nback.” Annals of Internal Medicine 128, no. 1 (1998): 76–78.\n\nJasper, Herbert, and Wilder Penfield. Epilepsy and the Functional Anatomy of\nthe Human Brain, 2nd ed. Boston: Little, Brown and Co., 1954.\n\nJeannerod, Marc. “The mechanism of self-recognition in humans.” Behavioural\nBrain Research 142 (2003): 1–15.\n\nKemper, Thomas Le Brun, and Albert M. Galaburda. “Principles of\ncytoarchitectonics.” In Cerebral Cortex, ed. Alan Peters and Edward Jones,\nvol. 1 (pp. 35–57). New York: Plenum Press, 1984.\n\nLeyton, Albert S. F., and Charles Scott Sherrington. “Observations on the\nexcitable cortex of the chimpanzee, orang-utan and gorilla.” Quarterly Journal\nof Experimental Psychology 11 (1917): 135–222.\n\nMakin, Tamar R., Nicholas P. Holmes, and H. Henrik Ehrsson. “On the other\nhand: Dummy hands and peripersonal space.” Behavioural Brain Research 191\n(2008): 1–10.\n\nMelzack, Ronald. “From the gate to the neuromatrix.” Pain, suppl. no. 6\n(1999): S121–26.\n\n______. “Phantom limbs.” Scientific American 266, no. 4 (1992): 120–26.\n\n______. The Puzzle of Pain. New York: Basic Books, 1973.\n\nMelzack, Ronald, and Patrick D. Wall. “Pain mechanisms: A new theory.” Science\n150, no. 3699 (1965): 971–79.\n\nMerzenich, Michael, Jon Kaas, et al. “Progression of change following median\nnerve section in the cortical representation of the hand in areas 3b and 1 in\nadult owl and squirrel monkeys.” Neuroscience 10, no. 3 (1983): 639–65.\n\n______. “Topographic reorganization of somatosensory cortical areas 3b and 1\nin adult monkeys following restricted deafferentation.” Neuroscience 8, no. 1\n(1983): 33–55.\n\nMurray, Craig, Stephen Pettifer, et al. “The treatment of phantom limb pain\nusing immersive virtual reality: Three case studies.” Disability &\nRehabilitation 29, no. 18 (2007): 1465–69.\n\nNicolelis, Miguel A. L. “Living with ghostly limbs.” Scientific American Mind\n18 (2007): 53–59.\n\nNicolelis, Miguel A. L., Rick C. S. Lin, et al. “Peripheral block of ascending\ncutaneous information induces immediate spatiotemporal changes in thalamic\nnetworks.” Nature 361 (1993): 533–36.\n\nPenfield, Wilder, and Edwin Boldrey. “Somatic motor and sensory representation\nin the cerebral cortex of man as studied by electrical stimulation.” Brain 60\n(1937): 389–443.\n\nPenfield, Wilder, and Theodore Rasmussen. The Cerebral Cortex of Man: A\nClinical Study of Localization of Function. New York: Hafner Publishing\nCompany, 1950.\n\nPetkova, Valeria I., and H. Henrik Ehrsson. “If I were you: Perceptual\nillusion of body swapping.” PLoS ONE 3, no. 12 (2008): e3832.\n\nPons, Tim, Preston E. Garraghty, et al. “Massive cortical reorganization after\nsensory deafferentation in adult macaques.” Science 252, no. 5014 (1991):\n1857–60.\n\nRamachandran, V. S., and Sandra Blakeslee. Phantoms in the Brain: Proving the\nMysteries of the Human Mind. New York: William Morrow, 1998.\n\nWall, Patrick D. Pain: The Science of Suffering. New York: Columbia University\nPress, 2000.\n\nWall, Patrick D., and Ronald Melzack, eds. Textbook of Pain, 4th ed. Edinburgh\nand New York: Churchill Livingstone, 1999.\n\n4\\. LISTENING TO THE CEREBRAL SYMPHONY\n\nBerger, Hans. “Über das Elektrenkephalogramm des Menschen.” Archiv für\nPsychiatrie und Nervenkrankheiten 87 (1929): 527–70.\n\nChurchland, Patricia Smith, and Terrence J. Sejnowski. The Computational\nBrain. Cambridge, Mass.: MIT Press, 1992.\n\nEvarts, Edward V. “Effects of sleep and waking on spontaneous and evoked\ndischarge of single units in visual cortex.” Federation Proceedings 19, Suppl.\nno. 4 (1960): 828–37.\n\n______. “A review of the neurophysiological effects of lysergic acid\ndiethylamide (LSD) and other psychotomimetic agents.” Annals of the New York\nAcademy of Sciences 66 (1957): 479–95.\n\n______. “Temporal patterns of discharge of pyramidal tract neurons during\nsleep and waking in the monkey.” Journal of Neurophysiology 27, no. 2 (1964):\n152–71.\n\nHubel, David, and Torsten Wiesel. “Receptive fields, binocular interaction and\nfunctional architecture in the cat’s visual cortex.” Journal of Physiology 160\n(1962): 106–54.\n\nLilly, John C. “Correlations between neurophysiological activity in the cortex\nand short-term behavior in the monkey.” In Biological and Biochemical Bases of\nBehavior, ed. Harry F. Harlow and Clinton N. Woolsey (pp. 83–100). Madison:\nUniversity of Wisconsin Press, 1958.\n\n______. “Instantaneous relations between the activities of closely spaced\nzones on the cerebral cortex: Electrical figures during responses and\nspontaneous activity.” American Journal of Physiology 176 (1954): 493–504.\n\nLilly, John C., George M. Austin, and William W. Chambers. “Threshold\nmovements produced by excitation of cerebral cortex and efferent fibers with\nsome parametric regions of rectangular current pulses (cats and monkeys).”\nJournal of Neurophysiology 15, no. 4 (1952): 319–41.\n\nLilly, John C., and Ruth B. Cherry. “Surface movements of the click responses\nfrom acoustic cerebral cortex of cat: Leading and trailing edges of a response\nfigure.” Journal of Neurophysiology 17, no. 6 (1954): 521–32.\n\nMcIlwain, James T. “Population coding: A historical sketch.” Progress in Brain\nResearch 120 (2001): 3–7.\n\nMountcastle, Vernon B. “Modality and topographic properties of single neurons\nof cat’s somatic sensory cortex.” Journal of Neurophysiology 20, no. 4 (1957):\n408–34.\n\nNiedermeyer, Ernst, and Fernando Lopes da Silva. Electroencephalography: Basic\nPrinciples, Clinical Applications, and Related Fields, 3rd ed. Baltimore:\nWilliams & Williams, 1993.\n\nPauly, Philip J. “The political structure of the brain: Cerebral localization\nin Bismarckian Germany.” Electroneurobiología 14, no. 1 (2005): 25–32.\n\nSherrington, Charles Scott. Man on His Nature, 2nd ed. Cambridge: Cambridge\nUniversity Press, 1951.\n\nSilk, Joseph. The Big Bang: The Creation and Evolution of the Universe. San\nFrancisco: W. H. Freeman, 1980.\n\n5\\. HOW RATS ESCAPE FROM CATS\n\nFox, Kevin. Barrel Cortex. Cambridge and New York: Cambridge University Press,\n2008.\n\nGeorgopoulos, Apostolos P., Andrew B. Schwartz, et al. “Neuronal population\ncoding of movement direction.” Science 233, no. 4771 (1986): 1416–19.\n\nGhazanfar, Asif A., Christopher R. Stambaugh, et al. “Encoding of tactile\nstimulus location by somatosensory thalamocortical ensembles.” Journal of\nNeuroscience 20, no. 10 (2000): 3761–75.\n\nNicolelis, Miguel A. L., ed. Methods for Neural Ensemble Recording, 2nd ed.\nBoca Raton, Fla.: CRC Press/Taylor & Francis, 2007.\n\nNicolelis, Miguel A. L., Luiz Antonio Baccalá, et al. “Sensorimotor encoding\nby synchronous neural ensemble activity at multiple levels of the\nsomatosensory system.” Science 268, no. 5215 (1995): 1353–58.\n\nNicolelis, Miguel A. L., Asif A. Ghazanfar, et al. “Reconstructing the engram:\nSimultaneous, multisite, many single neuron recordings.” Neuron 18, no. 4\n(1997): 529–37.\n\nNicolelis, Miguel A. L., and Sidarta Ribeiro. “Seeking the neural code.”\nScientific American 295, no. 6 (2006): 70–77.\n\nWelker, C. “Microelectrode delineation of fine grain somatotopic organization\nof SmI cerebral neocortex in albino rat.” Brain Research 26, no. 2 (1971):\n259–75.\n\n6\\. FREEING AURORA’S BRAIN\n\nCarmena, Jose M., Mikhail A. Lebedev, Roy E. Crist, et al. “Learning to\ncontrol a brain-machine interface for reaching and grasping by primates.” PLoS\nBiology 1, no. 2 (2003): 193–208.\n\nCarmena, Jose M., Mikhail A. Lebedev, Craig S. Henriquez, et al. “Stable\nensemble performance with single-neuron variability during reaching movements\nin primates.” Journal of Neuroscience 25, no. 46 (2005): 10712–16.\n\nChapin, John K., Karen A. Moxon, et al. “Real-time control of a robot arm\nusing simultaneously recorded neurons in the motor cortex.” Nature\nNeuroscience 2 (1999): 664–70.\n\nNicolelis, Miguel A. L., and John K. Chapin. “Controlling robots with the\nmind.” Scientific American 287, no. 4 (2002): 24–31.\n\nNicolelis, Miguel A. L., Dragan Dimitrov, et al. “Chronic, multisite,\nmultielectrode recordings in macaque monkeys.” Proceedings of the National\nAcademy of Sciences 100, no. 19 (2003): 11041–46.\n\nWessberg, Johan, Christopher R. Stambaugh, et al. “Real-time prediction of\nhand trajectory by ensembles of cortical neurons in primates.” Nature 408\n(2000): 361–65.\n\n7\\. SELF-CONTROL\n\nFetz, Eberhard E. “Operant conditioning of cortical unit activity.” Science\n163, no. 870 (1969): 955–58.\n\nFetz, Eberhard E., and Dom V. Finocchio. “Operant conditioning of specific\npatterns of neural and muscular activity.” Science 174, no. 7 (1971): 431–35.\n\nNowlis, David P., and Joe Kamiya. “The control of electroencephalographic\nalpha rhythms through auditory feedback and the associated mental activity.”\nPsychophysiology 6, no. 4 (1970): 476–84.\n\nOlds, James, and Marianne E. Olds. “Positive reinforcement produced by\nstimulating hypothalamus with iproniazid and other compounds.” Science 127,\nno. 3307 (1958): 1175–76.\n\nSchmidt, Edward M. “Single neuron recording from motor cortex as a possible\nsource of signals for control of external devices.” Annals of Biomedical\nEngineering 8 (1980): 339–49.\n\nWyricka, W., and M. Barry Sterman. “Instrumental conditioning of sensorimotor\ncortex EEG spindles in the waking cat.” Physiology & Behavior 3 (1968): 703–7.\n\n8\\. A MIND’S VOYAGE AROUND THE REAL WORLD\n\nBirbaumer, Niels, Nimr Ghanayim, et al. “A spelling device for the paralysed.”\nNature 398 (1999): 297–98.\n\nBirbaumer, Niels, Andrea Kubler, et al. “The thought translation device (TTD)\nfor completely paralyzed patients.” IEEE Transactions on Rehabilitation\nEngineering 8, no. 2 (2000): 190–93.\n\nBlakeslee, Sandra. “Monkey’s thoughts propel robot, a step that may help\nhumans.” New York Times, January 15, 2008.\n\nFitzsimmons, Nathan, Mikhail A. Lebedev, et al. “Extracting kinematic\nparameters for monkey bipedal walking from cortical neuronal ensemble\nactivity.” Frontiers in Integrative Neuroscience 3 (2009): 1–19.\n\nNicolelis, Miguel A. L. “Actions from thoughts.” Nature 409 (2001): 403–7.\n\nPatil, Parag G., Jose M. Carmena, et al. “Ensemble recordings of human\nsubcortical neurons as a source of motor control signals for a brain-machine\ninterface.” Neurosurgery 55, no. 1 (2004): 27–35.\n\nPeckham, P. Hunter, and Jayme S. Knutson. “Functional electrical stimulation\nfor neuromuscular applications.” Annual Review of Biomedical Engineering 7\n(2005): 327–60.\n\nPeikon, Ian D., Nathan Fitzsimmons, et al. “Three-dimensional, automated,\nreal-time video system for tracking limb motion in brain-machine interface\nstudies.” Journal of Neuroscience Methods 180 (2009): 224–33.\n\nSerruya, Mijail D., Nicholas G. Hatsopoulos, et al. [including John P.\nDonoghue]. “Instant neural control of a movement signal.” Nature 416 (2002):\n141–42.\n\nTaylor, Dawn M., Stephen I. Helms Tillery, and Andrew B. Schwartz. “Direct\ncortical control of 3D neuroprosthetic devices.” Science 296, no. 5574 (2002):\n1829–32.\n\n9\\. THE MAN WHOSE BODY WAS A PLANE\n\nBerti, Anna, and Francesca Frassinetti. “When far becomes near: Re-mapping of\nspace by tool use.” Journal of Cognitive Neuroscience 12 (2000): 415–20.\n\nCardinali, Lucilla, Francesca Frassinetti, et al. “Tool-use induces\nmorphological updating of the body schema.” Current Biology 19, no. 12 (2009):\nR478–79.\n\nFisher, Helen E. Why We Love: The Nature and Chemistry of Romantic Love. New\nYork: Henry Holt and Company, 2004.\n\nHead, Henry, and Gordon Holmes. “Sensory disturbances from cerebral lesion.”\nBrain 34 (1911): 102–254.\n\nHickok, Gregory, and David Poeppel. “The cortical organization of speech\nprocessing.” Nature Reviews Neuroscience 8 (2007): 393–402.\n\nHoffman, Paul. Wings of Madness: Alberto Santos-Dumont and the Invention of\nFlight. New York: Hyperion, 2003.\n\nIriki, Atsushi, Masaaki Tanaka, et al. “Coding of modified body schema during\ntool use by macaque postcentral neurones.” Neuroreport 7, no. 14 (1996):\n2325–30.\n\nLebedev, Mikhail A., Jose M. Carmena, et al. “Cortical ensemble adaptation to\nrepresent velocity of an artificial actuator controlled by a brain-machine\ninterface.” Journal of Neuroscience 25, no. 19 (2005): 4681–93.\n\nMaravita, Angelo, Charles Spence, et al. “Multisensory integration and the\nbody schema: Close to hand and within reach.” Current Biology 13, no. 13\n(2003): r531–39.\n\nYoung, Larry J. “Being human: Love: neuroscience reveals all.” Nature 457\n(2009): 148.\n\nYoung, Larry J., and Zuoxin Wang. “The neurobiology of pair bonding.” Nature\nNeuroscience 7, no. 10 (2004): 1048–54.\n\n10\\. SHAPING AND SHARING MINDS\n\nChapin, John K., Karen A. Moxon, et al. “Real-time control of a robot arm\nusing simultaneously recorded neurons in the motor cortex.” Nature\nNeuroscience 2, no. 7 (1999): 664–70.\n\nDelgado, José M. R. Physical Control of the Mind: Toward a Psychocivilized\nSociety. New York: Harper & Row, 1969.\n\nFitzsimmons, Nathan, Weying Drake, et al. “Primate reaching cued by\nmultichannel spatiotemporal cortical microstimulation.” Journal of\nNeuroscience 27, no. 21 (2007): 5593–602.\n\nGell-Mann, Murray. The Quark and the Jaguar: Adventures in the Simple and the\nComplex. New York: W. H. Freeman, 1994.\n\nHorgan, John. “The forgotten era of brain chips.” Scientific American 293, no.\n4 (2005): 66–73.\n\nNicolelis, Miguel A. L., and John K. Chapin. “Controlling robots with the\nmind.” Scientific American 287, no. 4 (2002): 46–53.\n\nO’Doherty, Joseph E., Mikhail A. Lebedev, et al. “A brain-machine interface\ninstructed by direct intracortical microstimulation.” Frontiers in Integrative\nNeuroscience 3 (2009): 1–10.\n\nSerruya, Mijail D., Nicholas G. Hatsopoulos, et al. “Instant neural control of\na movement signal.” Nature 416, no. 6877 (2002): 141–42.\n\n11\\. THE MONSTER HIDDEN IN THE BRAIN\n\nBuzsáki, György. Rhythms of the Brain. Oxford and New York: Oxford University\nPress, 2006.\n\nDzirasa, Kafui, Sidarta Ribeiro, et al. “Dopaminergic control of sleep-wake\nstates.” Journal of Neuroscience 26, no. 41 (2006): 10577–89.\n\nGervasoni, Damien, Shih-Chieh Lin, et al. “Global forebrain dynamics predict\nrat behavioral states and their transitions.” Journal of Neuroscience 24, no.\n49 (2004): 11137–47.\n\nLlinas, Rodolfo R. I of the Vortex: From Neurons to Self. Cambridge, Mass.:\nMIT Press, 2001.\n\nStapleton, Jennifer R., Michael L. Lavine, et al. “Rapid taste responses in\nthe gustatory cortex during licking.” Journal of Neuroscience 26, no. 15\n(2006): 4126–38.\n\n12\\. COMPUTING WITH A RELATIVISTIC BRAIN\n\nAnokhin, Peter K. Biology and Neurophysiology of the Conditioned Reflex and\nIts Role in Adaptive Behavior. Trans. by Samuel A. Corson. Oxford and New\nYork: Pergamon Press, 1974.\n\nBaghramian, Maria. Relativism. London and New York: Routledge, 2004.\n\nBarrow, John D. Impossibility: The Limits of Science and the Science of\nLimits. Oxford and New York: Oxford University Press, 1998.\n\nCasti, John L., and Werner DePauli. Gödel: A Life of Logic. Cambridge, Mass.:\nPerseus, 2000.\n\nCohen, Leonardo G., Pablo Celnik, et al. “Functional relevance of cross-modal\nplasticity in blind humans.” Nature 389 (1997): 180–83.\n\nEgiazaryan, Galina G., and Konstantin V. Sudakov. “Theory of functional\nsystems in the scientific school of P. K. Anokhin.” Journal of the History of\nthe Neurosciences 16, no. 1 (2007): 194–205.\n\nEinstein, Albert. Relativity: The Special and the General Theory. N.p.:\nQuality Classics, 2009.\n\nFrostig, Ron D., Ying Xiong, et al. “Large-scale organization of rat\nsensorimotor cortex based on a motif of large activation spreads.” Journal of\nNeuroscience 28, no. 49 (2008): 13274–84.\n\nFuentes, Romulo, Per Petersson, et al. “Spinal cord stimulation restores\nlocomotion in animal models of Parkinson’s disease.” Science 323, no. 5921\n(2009): 1578–82.\n\nGaleano, Eduardo H. Soccer in Sun and Shadow. London and New York: Verso,\n1998.\n\nGhazanfar, Asif A., Chandramouli Chandrasekaran, and Nikos K. Logothetis.\n“Interactions between the superior temporal sulcus and auditory cortex mediate\ndynamic face/voice integration in rhesus monkeys.” Journal of Neuroscience 28,\nno. 17 (2008): 4457–69.\n\nGhazanfar, Asif A., and Charles E. Schroeder. “Is neocortex essentially\nmultisensory?” Trends in Cognitive Sciences 10, no. 6 (2006): 278–85.\n\nGlashow, Sheldon. “We believe that the world is knowable.” Presentation at The\nEnd of Science?, 25th annual Nobel Conference at Gustavus Adolphus College,\nSaint Peter, Minnesota, October 3–4, 1989, as quoted in Baghramian,\nRelativism.\n\nGould, Stephen Jay. Full House: The Spread of Excellence from Plato to Darwin.\nNew York: Three Rivers Press, 2007.\n\nGreene, Brian. The Elegant Universe: Superstrings, Hidden Dimensons, and the\nQuest for the Ultimate Theory. New York: W. W. Norton, 1999, p. 25.\n\nHeisenberg, Werner. The Physicist’s Conception of Nature. Trans. by Arnold J.\nPomerans. Westport, Conn.: Greenwood Press, 1970, as quoted in Baghramian,\nRelativism.\n\nIsaacson, Walter. Einstein: His Life and Universe. New York: Simon & Schuster,\n2007.\n\nKelley, Patricia H. “Stephen Jay Gould’s winnowing fork: Science, religion,\nand creationism.” In Stephen Jay Gould: Reflections on His View of Life, ed.\nWarren D. Allmon, Patricia H. Kelley, and Robert M. Ross (pp. 171–188). New\nYork: Oxford University Press, 2009.\n\nMerabet, Lofti B., Joseph F. Rizzo, David C. Somers, and Alvaro Pascual-Leone.\n“What blindness can tell us about seeing again.” Nature Neuroscience 6 (2005):\n71–77.\n\nMerabet, Lofti B., Jascha D. Swisher, et al. [including David C. Somers].\n“Combined activation and deactivation of visual cortex during tactile sensory\nprocessing.” Journal of Neurophysiology 97 (2007): 1633–41.\n\nNicolelis, Miguel A. L., and Mikhail A. Lebedev. “Principles of neural\nensemble physiology underlying the operation of brain-machine interfaces.”\nNature Reviews Neuroscience 10 (2009): 530–40.\n\nPerlmutter, Steve I., Marc A. Maier, and Eberhard E. Fetz. “Activity of spinal\ninterneurons and their effects on forearm muscles during voluntary wrist\nmovements in the monkey.” Journal of Neurophysiology 80, no. 5 (1998):\n2475–94.\n\nRibeiro, Sidarta, et al. “Neurophysiological basis of metamodal processing in\nprimary sensory cortices.” In press, 2010.\n\nSadato, Norihiro, Alvaro Pascual-Leone, et al. “Activation of the primary\nvisual cortex by Braille reading in blind subjects.” Nature 380 (1996):\n526–28.\n\nTimo-Iaria, César, Nubio Negrào, et al. “Phases and states of sleep in the\nrat.” Physiology & Behavior 5, no. 9 (1970): 1057–62.\n\nZhou, Yong-Di, and Joaquín M. Fuster. “Somatosensory cell response to an\nauditory cue in a haptic memory task.” Behavioral Brain Research 153, no. 2\n(2004): 573–78.\n\n13\\. BACK TO THE STARS\n\nBarrow, John. The Constants of Nature: The Numbers That Encoded the Deepest\nSecrets of the Universe. New York: Vintage, 2009.\n\nDzirasa, Kafui, H. Westley Phillips, et al. “Noradrenergic control of cortico-\nstriato-thalamic and mesolimbic cross-structural synchrony.” Journal of\nNeuroscience 30, no. 18 (2010): 6387–97.\n\nDzirasa, Kafui, Amy J. Ramsey, et al. “Hyperdopaminergia and NMDA receptor\nhypofunction disrupt neural phase signaling.” Journal of Neuroscience 29, no.\n25 (2009): 8215–24.\n\nGould, Stephen Jay. Wonderful Life: The Burgess Shale and the Nature of\nHistory. New York: W. W. Norton, 1989, pp. 48, 50, 51.\n\nHomer. The Iliad of Homer. Trans. by Samuel Butler. New York: E. P. Dutton &\nCompany, 1923, p. 413.\n\nKurzweil, Ray. The Singularity Is Near: When Humans Transcend Biology. New\nYork: Penguin, 2007.\n\nLebedev, Mikhail A., and Miguel A. L. Nicolelis. “Brain machine interfaces:\nPast, present and future.” Trends in Neuroscience 29 (2006): 536–46.\n\nNicolelis, Miguel A. L. “Building the knowledge archipelago.” Scientific\nAmerican online, January 17, 2008,\n<http://www.scientificamerican.com/article.cfm?id=building-the-knowledge-\narchipelago>.\n\nPribram, Karl H. Brain and Perception: Holonomy and Structure in Figural\nProcessing. Hillsdale, N.J.: Lawrence Erlbaum Associates, 1991.\n\nSoares, Christine. “Building a future on science.” Scientific American 298,\nno. 2 (2008): 72–77.\n\n\nACKNOWLEDGMENTS\n\nDuring the past twenty-seven years many people have participated in the events\nand experiments described in this book. A few of them have been briefly\nmentioned here, but a large number of professors, mentors, students,\ncolleagues, collaborators, and friends remained anonymous despite their\nprofound contributions to my work as a scientist. Thus, first and foremost, I\nwould like to thank all of them for allowing me the privilege of working with\nthem or simply being part of their personal and intellectual lives. In\nparticular, I would like to thank one of my postdoctoral mentors, Dr. Rick\nLin, for introducing me to many aspects of neuroscience, and one of my\nscientific heroes, Dr. Jon H. Kaas, who by his innumerous acts of kindness and\namazing intellectual deeds has defined the model of scientist I wanted to\nemulate.\n\nSome of my former students and close collaborators were kind enough to read\nthe original manuscript and made invaluable suggestions, criticisms, and\nwarnings. I am, therefore, very grateful to Drs. Asif Ghazanfar, Marshall\nShuler, Sidarta Ribeiro, and Mikhail Lebedev for their rich and insightful\ncontributions. It goes without saying, though, that any remaining outrageous\nand unorthodox thoughts, comments, beliefs, or metaphors left in the\nmanuscript are not their fault but, instead, should be credited to the\nauthor’s vast list of sins. Another former student, Dr. Nathan Fitzsimmons,\ngave me the great pleasure of being in charge of all the illustrations of the\nbook. Nathan’s superb job served as a wonderful closing of his academic career\nand the starting point of new adventures in which he will certainly be as\nsuccessful as he was as a Ph.D. student.\n\nFor the past two decades I have discovered, to my surprise, that my parents\nhad hidden from my sister and me, their only natural children as far as we\nknew, that I had many true “brothers” spread around the world. This postnatal\nbrotherhood has played an essential role in my scientific and personal life\nand, as is often the case, allowed me to learn many things about friendship\nand science that no university or school can ever teach. In this context, my\ntwo American “brothers,” Drs. Alan Rudolph and Sidney “Sampras” Simon have\nbeen an integral part of my scientific and personal adventures for the past\nseventeen years, involving three continents and many unforgettable stories.\nWithout their friendship and continuous encouragement and support, this book\nwould have never been finished, simply because there would be no story to\ntell. I am also grateful to my Israeli brother, Dr. Idan Segev, who since we\nfirst met in the middle of nowhere in the Negev Desert and, soon afterward,\nduring a stunning Bastille Day on the streets of Paris, has become my true\nbenchmark of what a humanist should be and do. In the same rank, I would like\nvery much to thank my Egyptian-Swiss brother, the great mathematician and\nphilosopher of science Dr. Ronald Cicurel, who in our uncountable lunches over\npizza napoletana and Diet Coke (regular Coke for him) at Da Carlo’s restaurant\nin Lausanne, taught me more mathematics, physics, and philosophy than I ever\ndreamed to learn. Without Ronald’s eagerness to teach and sheer generosity to\nshare his immense intellect, I am afraid crucial parts of this book would have\nnever made the light of the day.\n\nBy the same token, I am profoundly indebted to a series of key individuals\nwhose kindness and dedication made it possible for this project to become a\nreality. First, I would like to thank Dr. James Levine, my literary agent, and\nall his wonderful colleagues at Levine and Greenberg in New York City, for\ntheir enormous kindness and support for the past two years. Without Jim’s calm\nsteering and generous experienced guidance to his rookie author, it is\ndifficult to believe how this project could have succeeded. Similarly, I would\nlike to profusely thank my editor, Robin Dennis, who diligently interacted\nwith me, day in and day out, for almost two years to make sure that I could\nlearn to write “short,” and not “long” like my Brazilian ancestors, and go\ndirectly to the point without too many adjectives and acrimony, like my Latin\nAmerican genes would prefer. Working with Robin, a real pro, was not only a\nthrilling pleasure, but the most important writing experience of my life.\nThus, I thank her for all the generosity, diligence, and understanding that\nwere required to edit and sharpen the original manuscript into a final\nproduct. Having learned so much from her, I like to think that the unusual\nexperience of working with a Brazilian writer may also have left something\nRobin can remember. After all, following our dealings she has somehow morphed\ninto a truly fanatical soccer fan, incapable of missing a single game of the\n2010 World Cup. I am also very grateful for the wonderful support and\nencouragement I received from everyone at Henry Holt and Times Books,\nparticularly from Paul Golob, who oversaw this project since its first days,\nand Serena Jones, who took over from Robin in the final stages of production\nof the book. Many thanks to Christine Soares and the editors of Scientific\nAmerican for helping me in my initial attempts to write to a lay audience and\nfor allowing me to reproduce here some excerpts of those articles.\n\nThroughout my career I have had the privilege to work or collaborate in some\nof the most distinguished academic institutions in the world. I would like to\nthank my colleagues at my laboratory at the Department of Neurobiology and the\nCenter for Neuroengineering at Duke University, where I have spent the past\nseventeen years of my career, as well as my friends and close collaborators in\nother countries, like Dr. Patrick Aebischer, Dr. Hannes Bleuler, and Solaiman\nShokur, Tamina Sisoko, Jaime Ruiz at the Ecole Polytechnique Fédérate de\nLausanne, Dr. Jean Rossier at the École Supérieure de Physique et de Chimie\nIndustrielles, and Professor Henri Korn at the Institut Pasteur in Paris, and\nthe entire staff of the Edmond and Lily Safra International Institute of\nNeuroscience of Natal and the Alberto Santos Dumont Association for the\nAdvancement of Science in Brazil (AASDAP). In particular, I would like to\nthank Dora Montenegro, her teachers, and the 1,400 students who attend\nAASDAP’s three science schools in Brazil for the inspiration and joy they have\nso freely and kindly offered me in the past four years. I am also indebted to\nall the anonymous American taxpayers and donors who through the National\nInstitutes of Health and other federal and private agencies have helped to\nfund my research over the past twenty-two years. Many thanks also to my good\nBrazilian-Swiss friend, Pierre Landolt, and the Sandoz Family Foundation for\ntheir support. I am also grateful for the friendship and support of Mrs. Lily\nSafra.\n\nOnly two people have read the original and revised manuscript more times than\nRobin and me. The first of these beloved heroines was Neiva Cristina\nParaschiva. For her thoughtful comments and continuous support, I thank her\nfrom the bottom of my heart. The second heroine, Susan Halkiotis, my guardian\nangel at Duke University for the past decade, has read every line of this book\nmany times and provided the best proofreading and witty advice any author this\nside of the Milky Way could hope for. Nothing I could write or say would do\njustice to the dedication, competence, and sheer passion with which “Professor\nHalkiotis,” as I like to call her, dived into this project. There is no doubt\nin my mind that without Susan’s help I would have never fulfilled my task. For\nthis and for the precious mutual friendship our families share, I hereby\nrecognize my infinite debt to her.\n\nFor the past three decades I have been lucky enough to count always on the\nfriendship, love, and support of Laura Oliveira and our three beloved sons,\nPedro, Rafael, and Daniel Nicolelis. Without their presence in my life,\nnothing would have mattered. I dedicate my entire career as small tokens of\ngratitude for their understanding and personal sacrifice, without which I\nwould have never had the strength and freedom to pursue my scientific and\nhumanistic dreams to the very limits of my Brazilian imagination.\n\nNatal, August 2010\n\n\nINDEX\n\nThe index that appeared in the print version of this title does not match the\npages in your eBook. Please use the search function on your eReading device to\nsearch for terms of interest. For your reference, the terms that appear in the\nprint index are listed below.\n\nacetylcholine\n\naction potentials\n\nBMIs and\n\nenergy budget and\n\nFetz and\n\nfirst recordings of\n\nmicrowire arrays and\n\nresponse threshold and\n\nsingle neurons and\n\nwire and ball model and\n\nactive exploration\n\nadaptability\n\nadenosine triphosphate\n\nAdrian, Sir Edgar Douglas\n\nafferent component\n\naggressive behavior\n\nAimé, Emmanuel\n\nairships\n\nAlberto, Carlos\n\nAli (monkey)\n\n“Alma” (Prior)\n\nalpha rhythm\n\nAltered States (film)\n\namputation\n\namygdala\n\namyotrophic lateral sclerosis (ALS)\n\nanesthesia, brain dynamics and\n\nAnnals of Biomedical Engineering\n\naphasia\n\nArmstrong, Neil\n\nArmstrong-James, Michael\n\nartificial intelligence\n\nartificial neural networks (ANNs)\n\nascending peripheral sensory stimuli\n\nAsimov, Isaac\n\nassimilation principle\n\nasynchronous convergence principle\n\nattentive state\n\nauditory system. See also primary auditory cortex\n\nAurora (monkey)\n\nAvatar (film)\n\navatars\n\nawake, behaving animals, decision to study\n\naxons\n\nBaccalá, Luiz Antonio\n\nBaghramian, Maria\n\nBakay, Roy\n\nBarlow, Horace\n\nbarrelets\n\nbarreloids\n\nbarrels\n\nBarrow, John D.\n\nbasal ganglia\n\nbehavioral training\n\nbehavior state, responses vary with\n\nBelle (monkey)\n\nBerger, Hans\n\nBernard, Claude\n\n“Berry, Halle, neuron”\n\nBerti, Anna\n\nbeta activity\n\nBetz, Vladimir\n\nBetz cells\n\nBig Bang, The (Silk)\n\nbilateral responses, in rats\n\nbimodal neurons\n\nbinding problem\n\nbiofeedback\n\nbiomedical applications\n\nbipedal locomotion studies\n\nbipolar syndrome\n\nBirbaumer, Niels\n\nBizet, Georges\n\nBlackBerrys\n\nblack reaction (reazione nera)\n\nBlanke, Olaf\n\nblindness\n\nbody\n\nliberating brain from\n\nmind and\n\nbody image (ownership)\n\nassimilation of loved ones into\n\nassimilation of tools into\n\nout-of-body experiences and\n\nphantom limbs and\n\nrubber hand and\n\nas simulation\n\nswapping of\n\nBohème, La (opera)\n\nbrain. See also body image; brain’s own point of view; brain state-space plot;\nneural ensembles; neurons; neurophysiology; relativistic brain hypothesis;\ntool assimilation; and specific brain structures; experiments; researchers;\nand theories\n\nAdrian’s studies of\n\nAsimov’s depiction of\n\nBerger’s discovery of rhythms of\n\nbiological constraints on\n\nbody’s homeostasis and\n\nCajal’s studies of\n\nCambridge-Oxford debate over\n\nchanging vision of\n\nChapin’s microwire arrays and\n\ncomplexity and circuits of\n\ndefining functional unit of\n\ndissection of\n\ndivision of, vs. working whole\n\nas dynamic system\n\nelectrical signals of, first recorded\n\nemancipated from body\n\nenergy budget of\n\nevolution of\n\nexperimental approach to perception by\n\nforbidden frequency trajectories in\n\nfundamental debate over nature of\n\nfuture of\n\nGolgi and big picture of\n\nhierarchical notion of\n\nLashley’s studies of\n\nlocalizationist (reductionist) vs. distributionist views of\n\nlove and\n\nneurons and networks of\n\norchestra metaphor for\n\noscillations of, and wake-sleep cycle\n\nperception and modeling by\n\nsea metaphor for\n\nas seat of intelligence\n\nsoccer team metaphor for, as complex system\n\nbrain circuits. See neural circuits\n\nbrain-machine-brain interface (BMBI)\n\nbrain-machine interfaces (BMIs)\n\nafferent and efferent components of\n\nconvergence and\n\ncurrent technology and\n\ndefined\n\nearly foundation for\n\nearly experiments on rats and\n\nelectrical stimulation and\n\nfuture of\n\nincorporation of, by brain\n\ninvasive vs. noninvasive methods and\n\nmedical uses of\n\nneuronal spacetime continuum and\n\nowl monkey studies and\n\nParkinson’s disease and\n\nplasticity principle and\n\nreal-time computational algorithm for\n\nrhesus monkey studies and\n\nrhesus monkey studies and Durham-Kyoto connection\n\nroborat and\n\nbrain’s own point of view\n\nbrain state-space plot\n\nwire and ball model and\n\nbrain stem\n\nbrain-to-brain interface (BTBI)\n\nBrazil\n\ndemonstrations for direct election of president\n\nIndependence day of 1972\n\ninsurrection of 1932\n\nsoccer team\n\nbroadly tuned neurons\n\nBroca, Pierre Paul\n\nBrodie, Eric\n\nBrodmann, Korbinian\n\nbullfighting\n\nCajal, Santiago Ramón y\n\nCambridge University\n\nCameron, James\n\n“Campus of the Brain” (Macaiba, Brazil)\n\nCantlie, Mrs. H. P.\n\nCardinali, Lucilla\n\ncardiovascular functions\n\nCarmen (Bizet)\n\nCarmen (monkey)\n\nCarmena, Jose\n\nCaron, Marc\n\ncarotid baroreceptor\n\nCartesian theory\n\nCartier, Louis\n\nCaruso, Enrico\n\nCaton, Richard\n\ncats\n\ncaudate nucleus\n\ncaudate-putamen complex\n\ncentral nervous system\n\nCajal’s drawings of\n\nearly development of\n\nfeedforward pathways to\n\nhierarchical notion of\n\ncentral pattern generators\n\ncentral sulcus\n\ncerebellum\n\ncerebral strokes\n\ncerebral vascular system\n\nChannelrhodopsin-2 (ChR-2)\n\nChapin, Albert\n\nChapin, John\n\nCheng, Gordon\n\nChopin, Frédéric\n\nchronic brain implants\n\nClodoaldo\n\nCohen, Jonathan\n\ncolor perception\n\ncomplex systems\n\ndistributed networks and\n\nComputational Brain, Model 1 (CB-1)\n\ncone electrode\n\nConnors, Barry\n\nconsciousness\n\nconservation of neural ensemble firing principle\n\ncontext principle\n\nCO-rich neuronal clusters. See also barrels\n\ncortex\n\nBMI and\n\nbrain state and\n\ndefined\n\ndevelopmental stages and\n\nFetz studies of\n\nfunctional theory and maps of\n\nfunctional theory vs. cross-modal responses and\n\nhigher-order associative areas of\n\nLashley’s removals of\n\nlocalizationist view of\n\nlong-lasting excitatory or inhibitory responses and\n\nmicrostimulation studies\n\nNDC and insight into\n\nneuronal space-time continuum and\n\nplasticity and\n\nwire and ball model and\n\ncortical columns\n\ncortical desynchronization state\n\ncortical layers\n\ncortical parietalneurons\n\ncortical pyramidal neurons\n\ncorticospinal tract\n\ncorticothalamic pathways\n\nCosmas, Saint\n\nCosmos (Sagan)\n\ncritical period\n\ncross-modal processing\n\nCushing, Harvey\n\ncytoarchitectonics\n\ncytochrome oxidase (CO)\n\nDamian, Saint\n\nDarwin, Charles\n\n“data binning”\n\nDawkins, Richard\n\ndeafferentation\n\ndecoder rat\n\ndeep brain stimulation (DBS)\n\nDefense Advanced Research Projects Agency (DARPA)\n\nDeisseroth, Karl\n\nDelgado, José Manuel Rodriguez\n\ndelirium\n\ndelta waves\n\ndendrites\n\ndepression\n\nDeutsch, David\n\nDeutsch de la Meurthe, Henri\n\nDeutsch Prize\n\ndevelopmental stages\n\nDewan, Edmond\n\ndiencephalic structures\n\ndifferential recording, invention of\n\nDimitrov, Dragan\n\ndistal-type neurons\n\ndistributed coding principle\n\ndistributed population coding\n\nbody image and\n\ndefined\n\nGolgi and\n\nlabeled-line model vs.\n\nmicrowire arrays and\n\nthinking and\n\nYoung’s color theory and\n\ndistributionist approach\n\nDNA\n\ndopamine\n\nDouglas, Rodney\n\ndreaming\n\nDuke University\n\nCenter for Neuroengineering (DUCN)\n\nDepartment of Neurobiology\n\ndynamic polarization, law of\n\nDzirasa, Kafui\n\nEastwood, Clint\n\nEccles, John\n\nEdelman, Gerald\n\nefferent component\n\nEhrsson, Henrik\n\nEiffel, Gustave\n\nEiffel Tower, first flight around\n\nEinstein, Albert\n\nelectrical brain stimulation. See also brain machine interfaces (BMIs); micro-\nelectrodes; and specific experiments\n\nBMBIs and\n\nBTBIs and\n\nDelgado’s bull experiment and\n\nmonkey task solutions and\n\nPenfield and\n\nrat “magnetic” world and\n\nroborat and\n\nSherrington and\n\nelectrical impulses. See also action potentials; neuronal firing\n\nanesthetized animals and\n\nas language of brain\n\nunanesthetized animals and\n\nelectroencephalogram (EEG)\n\nelectro-iconograms\n\nelectromyograph (EMG)\n\nelectronic integrator\n\nelectrophysiology\n\nElegant Universe, The (Greene)\n\nElsa (monkey)\n\nemergent properties\n\nendorphins, gate control theory of pain and\n\nenergy balance\n\nenergy budget\n\nengram\n\nepilepsy\n\nequipotentiality\n\nErickson, Robert\n\nEshe (rat)\n\nEvarts, Edward\n\nevolution\n\nFabric of Reality, The (Deutsch)\n\nFacchetti, Giacinto\n\nface recognition\n\nfacial muscles\n\nFanselow, Erika\n\nfeature-extraction model\n\nFederer, Roger\n\nfeedback projections\n\nfeedforward pathways\n\nFetz, Eberhard\n\nFeynman, Richard\n\nFinger, Stanley\n\nfingers\n\nFinocchio, Dom V.\n\nFirefox (film)\n\nFitzsimmons, Nathan\n\nFlor, Herta\n\nFlyer I (Wright plane)\n\nFrassinetti, Francesca\n\nFritsch, Gustav\n\nfrog muscles\n\nfrontal cortex. See also cortex\n\nevolution of human\n\nHitzig and Fritsch map of\n\nmultimodal model of self and\n\nfrontal lobe. See also specific parts\n\nFrostig, Ron\n\nFuentes, Romulo\n\nfunctional magnetic resonance image (fMRI)\n\nfunctional model of brain. See also localizationalists; reductionists\n\nchanging demands and\n\ncross-modal responses vs.\n\nFuster, Joaquín\n\nGABA (gamma-aminobutyric acid)\n\nGABAergic RT neurons\n\nGalileo Galilei\n\nGall, Franz\n\nGalvani, Luigi\n\ngamma waves\n\ngap junctions (cytoplasmic bridges)\n\nGarrincha, Manoel “Mané”\n\ngate control theory of pain\n\nGauss, Johann Carl Friedrich\n\nGehrig, Lou\n\nGell-Mann, Murray\n\ngender-swapping\n\ngenetics\n\nGeorgopoulos, Apostolos\n\nGerlach, Joseph von\n\nGérson\n\nGervasoni, Damien\n\nGettysburg, Battle of\n\nGhazanfar, Asif\n\nGlashow, Sheldon\n\nGlobal Brain Attractor\n\nglobal brain dynamics\n\nGödel, Kurt\n\nGolgi, Camillo\n\nGolgi neural network\n\nGoogle\n\nGould, Stephen Jay\n\nGraham, Mrs. Hannah\n\ngrandmother neuron\n\ngraph theory\n\ngravity\n\nGreene, Brian\n\ngustatory center. See also primary gustatory cortex\n\nHahnemann University\n\nHallett, Mark\n\nhands\n\nHead, Henry\n\nHebb, Donald O.\n\nHeisenberg, Werner\n\nHelmholtz, Herman von\n\nhemineglect syndrome\n\nhierarchical doctrine\n\nhippocampus\n\nhistology\n\nHitzig, Eduard\n\nHoffman, Paul\n\nHollywood films\n\nHolmes, Gordon\n\nhomeostasis\n\nhominids, early\n\nHomo habilis\n\nhomunculus\n\nHopfield, John\n\nHorgan, John\n\nhormones\n\nHubel, David\n\nhuman brain\n\nHuman Brain, The (Asimov)\n\nHurt, William\n\nhypno-map\n\nIdoya (monkey)\n\nImpossibility (Barrow)\n\nincompleteness theorem\n\nindividual history\n\ninfrorbital nerve (ION)\n\ninhibitory interneurons\n\ninhibitory responses\n\n“In Search of the Engram” (Lashley)\n\nIntel\n\nintelligence, brain as seat of\n\nintermediary sleep state\n\ninternal brain dynamics\n\nevidence of effects of\n\nhomeostasis and\n\nneurological disorders and\n\nreductionists exclude\n\nInternational Astronomical Union\n\nInternet\n\n250-millisecond challenge\n\ninterneurons\n\nintracellular recordings\n\nintrinsic optical imaging\n\ninvasive vs. noninvasive methods\n\ninverse problem\n\nion channels\n\nIriki, Atsushi\n\n“I’ve Got You Under My Skin” (Porter)\n\nJairzinho\n\nJournal of Neuroscience\n\nKaas, Jon\n\nKamiya, Joe\n\nKawato, Mitsuo\n\nKennedy, Philip\n\nketamine\n\nKruk, John\n\nKrupa, David\n\nKuffler, Stephen\n\nlabeled-line model\n\nLaporta, Lygia Maria Rocha Leão (grandmother)\n\nLaporta, Vincente (grandfather)\n\nLashley, Karl\n\n“last-dash” strategy\n\nLast Man Who Knew Everything, The (Robinson)\n\nL-DOPA\n\nLeakey, Louis\n\nLebedev, Mikhail (Misha)\n\nleft frontal lobe\n\nleft hemibody neglect syndrome\n\nLehew, Gary\n\n“life tape, replaying”\n\nlight\n\nelectromagnetic nature of\n\nwave or particle debate\n\nLilly, John Cunningham\n\nLilly wave\n\nlimbic system\n\nlimbs. See also brain-machine interface\n\nartificial locomotion of\n\nNDCs and\n\nphantom\n\nLin, Shih-Chieh “CJ”\n\nlocal field potentials (LFPs)\n\nlocalizational neuroscience\n\nhierarchy and\n\nlabeled-line model and\n\nphenomena not explained by\n\nreductionist paradigm and\n\nrelativistic brain and\n\nLoch Ness state-space “monster”\n\nwire and ball model and\n\nlocked-in syndrome\n\nLorentz length contraction\n\nlove\n\nLSD\n\nLucas, Keith\n\nM1. See primary motor cortex\n\nMacaque monkeys\n\n“magnetic” world\n\nmagnetoencephalography (MEG)\n\nMalsburg, Christoph von der\n\nMANE (Mother of All Neurophysiological Experiments) Project\n\nmania\n\nMan on His Nature (Sherrington)\n\nMaravita, Angelo\n\nmass action effect\n\nMatrix trilogy (films)\n\nMaxwell, James Clerk\n\nMcIlwain, James T.\n\nmechanoreceptors\n\nmedial forebrain bundle (MFB)\n\nmeditation\n\nMeloy, Jim\n\nMelzack, Ronald\n\nmemory\n\n“mental game model”\n\nMerzenich, Michael\n\nmice\n\nmicroelectrodes\n\ndeep brain stimlulator\n\nearly use of\n\nLilly studies of unaesthetized animals and\n\nmicroelectrodes, multi-electrode or microwire arrays\n\nBMI and\n\nchronic, multisite\n\ndeveloped\n\nhigh-density\n\nmonkey studies and\n\nneuroprosthetics and\n\nParkinson’s patients and\n\nprinciple whisker question and\n\nMicrosoft\n\nmind-dependence\n\nmind-melding\n\nMinsky, Marvin\n\nmirror box treatment\n\nMitchell, Silas Weir\n\nmitral cells\n\nMIT Technology Review\n\nMNAP (multineuronal acquisition processor)\n\nmodulator neurotransmiters, wake-sleep cycle and\n\nmonkey studies. See also individual monkeys and specific species\n\nBMI experiments with\n\nelectrical stimulation to contain aggression of alpha\n\nFetz studies of awake\n\nmessage delivered directly to cortex of\n\nwake-sleep cycle and\n\nmonosynaptic pathway, matrix of\n\nmoon landing\n\nMoore, Chris\n\nMörner, Count Karl Axel\n\nmotion detection\n\nmotor control\n\nDelgado’s studies of\n\nelectrical brain stimulation and\n\nLashley’s studies of\n\nvoluntary motor signals and\n\nmotor cortex. See also primary motor cortex (M1)\n\nBetz’s studies of\n\nBMI idea and\n\ndiscovered\n\nepileptic seizure and\n\nNDCs and\n\nPenfield’s studies of\n\nsegregated functional pathways and\n\nspinal cord and\n\nMountcastle, Vernon\n\nMüller, Johannes\n\nMüller, Klaus\n\nmulti-channel acquisition system\n\nmultimodal signals\n\nmultisynaptic pathways\n\nmu rhythm\n\nMurray, Craig\n\n“muscle field”\n\nmuscles\n\nmusic\n\nmyelin sheath\n\nNadal, Rafael\n\nnanotools\n\nNapoleon\n\nNational Institute of Neurological Disorders and Stroke (NINDS)\n\nNational Institutes of Health (NIH)\n\nNature\n\nnear-death encounters\n\nNelson, Horatio, Lord\n\nNelson, Sacha\n\nneocortex. See also specific areas\n\ndistributed neural networks and\n\n“nerve network,” Golgi and\n\nneural (brain) circuits\n\ndefined\n\nencoding of, to sustain motor behavior\n\npain and\n\nneural degeneracy principle\n\n“neural dynamics”\n\nneural ensembles (networks, populations)\n\nanesthetized animals and\n\nBMIs and\n\nbroadly tuned, work together\n\ncerebral vascular system and\n\ndefined\n\ndevelopmental process and\n\ndynamic spatiotemporal distributions of\n\nelectrical symphonies created by\n\nemergent properties and distributed networks and\n\nencoding of information and\n\nevolution of, in humans\n\nGolgi and\n\nLilly and\n\nMelzack and\n\nNDCs and\n\nnew prosthetic devices and\n\nnumber of possible connections and\n\npredicting behavior of subset of\n\nprinciples of “symphonies” of\n\nrat whiskers and\n\nrecording simultaneous activity of\n\nrelativistic view of brain and\n\nneural signature\n\nneurochips, implanted\n\nneuroengineering\n\nneurological disorders\n\nneuromatrix\n\nneuronal firing. See also electrical impulses\n\nanticipatory\n\ncovariance structure and\n\nFetz’s studies of higher\n\ninternal brain state and\n\nsound of, amplified\n\nspatial and temporal domains and\n\nspike velocities, maximum\n\nas symphony\n\ntranslating, into digital commands\n\nneuronal mass effect principle\n\nneuronal mitochondria\n\nneuronal multitasking principle\n\nneuronal space-time continuum\n\nneuron doctrine\n\n“Neuron Doctrine, The” (Golgi)\n\nneuron-dropping curve (NDC)\n\nneurons (nerves). See also neural ensembles; single neurons\n\nartificially increasing firing rate of\n\nBMI and\n\nbroadly tuned\n\nCajal’s studies of\n\ncell membrane ion channels\n\ndeath of individual\n\ndefined\n\nelectrical currents in, discovered\n\nelectrical signals from intracellular space of\n\nelectrical signals produced by individual\n\ninfluence of, on neighboring neurons\n\ninternal timing reference\n\nmembrane ion channels of, and neurotransmitters\n\nname coined, by Waldeyer-Hartz\n\nNDC and behavioral prediction\n\nreductionist focus on maximally firing of single\n\nsingle vs. multiple action of\n\n“spike budget”\n\nneurophysiology. See also brain-machine-interface (BMI); neural ensembles;\nneurons; systems neurophysiology; and specific brain areas; principles;\nresearchers; and studies\n\nALS and\n\nbehavioral training and\n\nEEG and\n\nEvarts and\n\nLilly and\n\nMalsburg’s binding problem and\n\nsingle neuron focus, and\n\n3-D map and\n\nneuroprosthetics\n\nneurotransmitters\n\nNewton, Sir Isaac\n\n“new world” paradigm\n\nNew York Times\n\nNissl method\n\nNó, Rafael Lorente de\n\nNobel, Alfred\n\nNobel Peace Prize\n\nNobel Prize for Physiology or Medicine\n\nNode of Ranvier\n\nnon-Euclidean geometry\n\nnoradrenaline\n\nobsessive-compulsive syndrome\n\noccipital lobes\n\nocular dominance columns\n\nO’Doherty, Joseph “Joey”\n\nOlds, James\n\nOlds, Marianne\n\nolfactory bulb\n\noperant conditioning\n\noptogenetics\n\nOrganization of Behavior, The (Hebb)\n\nOrigins of Neuroscience (Finger)\n\nOrmandy, Eugene\n\nOscar II, King of Norway\n\nout-of-body experiences\n\nowl monkeys\n\nOxford University\n\noxidation processes\n\noxygen\n\noxytocin\n\npack or herd animals\n\nPaddy (monkey)\n\npain\n\nPantoja, Janaina\n\nparallel information processing\n\nparalysis\n\nparanoia\n\nParé, Ambroise\n\nparietal lobe\n\nParkinson’s disease\n\nparticle physics\n\nPascual-Leone, Alvaro\n\nPatil, Parag\n\nPauly, Philip\n\nPeikon, Ian\n\nPelé\n\nPenfield, Wilder\n\nperiaqueductal gray matter\n\nperi-event histograms\n\nperi-personal space\n\nperipheral nerves\n\nperipheral neurodegenerative disorder\n\nperipheral sensory organs\n\nperi-stimulus time histogram (PSTH)\n\nperspectivalism\n\nPetersson, Per\n\nPfaffmann, Carl\n\nPhantoms in the Brain (Ramachandran)\n\nPhiladelphia Philharmonic Orchestra\n\nPhiladelphia Phillies\n\nPhilosophical Transactions of the Royal Society\n\nphrenology\n\nPhysical Control of the Mind (Delgado)\n\nPhysicist’s Conception of Nature, The (Heisenberg)\n\nPiazza\n\nPiquet, Nelson\n\nplasticity\n\nplasticity principle\n\nplatypus\n\n“Political Structure of the Brain, The” (Pauly)\n\nPons, Tim\n\npooled coherence\n\nPorter, Cole\n\npositron emission tomography (PET)\n\npostcentral cortex\n\nposterior medial (POM) nucleus\n\nposterior parietal cortex (PP)\n\nPradicz-Neminski, W.\n\nprairie vole\n\nprecentral gyrus\n\nprefrontal lobotomy\n\nPribram, Karl\n\nprimary auditory cortex (A1)\n\nprimary gustatory cortex\n\nprimary motor cortex (M1)\n\nBMI studies\n\nBrodmann’s classifications and\n\nFetz studies of\n\nNDC and\n\nPenfield’s studies of\n\nSherrington’s studies of\n\nprimary somatosensory area (S1)\n\namputation and\n\nBMI and\n\nBTBI and\n\ncross-modal processing and\n\nidentification of\n\nlabeled-line model and\n\nmagnetic implants and\n\nMountcastle’s studies of\n\nneuromatrix and\n\noptogenetics and\n\nowl monkey studies and\n\nprimary motor cortex and\n\nrats studies and\n\nroborat and\n\nrodent, and cortical barrels\n\nwire and ball model and\n\nprimary visual cortex (V1)\n\nprincipal whisker\n\nPrior, Matthew\n\nProceedings of the National Academy of Sciences\n\nproprioceptive signals\n\nprosthetic devices. See also neuroprosthetics\n\nproximal-type neurons\n\npsychedelic drugs\n\npsychiatric disorders\n\nquadrupedal locomotion\n\nquantum mechanics\n\nQuark and the Jaguar, The (Gell-Mann)\n\nQuarterly Journal of Experimental Physiology\n\nquiet waking state\n\nradio telescope array\n\nRamachandran, V. S.\n\nrats\n\nBMI and\n\nBTBI and\n\ncross-modal responses and\n\n“magnetic” world and\n\nmicrowire arrays and\n\nrattunculus and\n\nresponse thresholding process and\n\n“roborat” experiments\n\nrobotic lever and\n\nsomatosensory studies of\n\nwake-sleep cycle of\n\nwhisker maps\n\nwhisker twitching\n\nreceptive fields (RFs)\n\nreceptors, broadly tuned\n\nreductionists. See also functionalists; localizationalists\n\nrelativistic brain hypothesis\n\nfuture of, and BMIs\n\nrelativity theory\n\nREM sleep\n\nresponse thresholding\n\nreticular nucleus (RT)\n\nreticular theory\n\nretina\n\nRevista Trimestral de Histología Normal y Patológica\n\nRevista Trimestral Micográfica\n\nreward-pleasure system\n\nrhesus monkeys\n\nRibeiro, Sidarta\n\nRiemann, Bernhard\n\nRiggio, Lucia\n\nright cortical hemisphere\n\nright parietal lobe, injury to\n\nRivelino, Roberto\n\nRobinson, Andrew\n\nroborat\n\nrobotic devices\n\nRoosevelt, Theodore\n\nrubber hand illusion\n\nRudolph, Alan\n\nSadato, Norihiro\n\nSagan, Carl\n\nSandler, Aaron\n\nSan Francisco earthquake (1906)\n\nSantos-Dumont, Alberto\n\nSão Paulo, University of, medical school\n\nschizophrenia\n\nSchmidt, Edward\n\nSchwann’s Cell\n\nScience\n\nScientific American\n\nscientific method\n\nSejnowski, Terry\n\nSelfish Gene, The (Dawkins)\n\nSenna, Ayrton\n\nsense of self\n\nsensory deprivation\n\nsensory isolation tank\n\nsensory system\n\nbrain state and\n\nearly studies of\n\nfeedforward and feedback projections in\n\ntool use and\n\nsensory transduction\n\nserial sampling\n\nserotonin\n\nsex\n\nsex reassignment surgery\n\nSherrington, Sir Charles\n\nShuler, Marshall\n\nSilk, Joseph\n\nSimon, Sidney\n\nSinatra, Frank\n\nsingle neuron doctrine\n\ndistributed population coding vs.\n\nfeature detector theory and\n\nlabeled-line model and\n\nspatiotemporal complexity and\n\nsingle neuron insufficiency principle\n\nsingle neuron recordings\n\nlimitations of\n\nsleep deprivation. See also wake-sleep cycle\n\nsleep spindles\n\nslow wave sleep\n\nsoccer\n\nsocial bonding\n\nSociety of Mind, The (Minsky)\n\nsomatosensory cortex. See also primary somatosensory cortex (S1)\n\nsomatosensory system\n\nrodent\n\nsomatosensory thalamus\n\nsomatotopic maps\n\nSomers, David C.\n\nspace-time continuum. See also relativistic brain hypothesis; spatiotemporal\npatterns\n\nspatiotemporal collisions\n\nspatiotemporal contingency\n\nspatiotemporal discrimination\n\nspatiotemporal patterns\n\nRFs and maps of\n\nspecific nerve energies theory\n\n“speech center”\n\nspinal cord\n\ninjuries to\n\nspinal (SpV) complex\n\nStapleton, Jennifer\n\nstem cells\n\nSternman, M. Barry\n\nstimoceiver\n\nstrokes\n\nsubcortical structures\n\nSur, Mriganka\n\nSurrogates (film)\n\nsynapses\n\nsynaptic currents\n\nsynchronous firing\n\nsystems neurophysiology\n\nsystems physiology\n\ntactile (touch) pathways\n\ntaste. See gustatory system\n\nTate, Andrew\n\ntemporal lobe\n\nthalamic reticular nucleus (RT)\n\nthalamocortical (TC) pathway\n\nthalamus\n\nTheory of Everything\n\ntheta rhythm\n\nthinking\n\nThomson, Eric\n\ntime. See also spatiotemporal patterns\n\n“time bins”\n\ntime dilation\n\ntime travel\n\nTimo-Iaria, Dr. César\n\ntool assimilation\n\ntopographic maps\n\nTostão\n\nTrafalgar, Battle of\n\ntranscranial stimulation (TMS)\n\ntrigeminal system (Vg)\n\nTupi-Guarani tribe\n\nTurner, Dennis\n\ntwenty-five-channel bavatron\n\n“ultrasound” world\n\nuncertainty principle\n\nHeisenberg’s\n\nof neurophysiology\n\nVan de Loos, Hendrik\n\nventral posterior medial nucleus (VPM)\n\nVerne, Jules\n\nvirtual reality (VR)\n\nvisual cortex\n\nvisual-motor system\n\nvisual system\n\nBMIs and\n\nvisuotactile cross-modal associations\n\nVogt, Cécile\n\nVogt, Oskar\n\nVolta, Alessandro\n\nWagner, Richard, Parsifal\n\nwake-sleep cycle\n\nWaldeyer-Hartz, Wilhelm\n\nWalk Again Project\n\nWall, Patrick\n\nwearable robot (exoskeleton)\n\nWessberg, Johan\n\nwhite matter, strokes and\n\nWiener filter\n\nWiesel, Torsten\n\nWiest, Mike\n\nWilliams, Mitch\n\nWings of Madness (Hoffman)\n\nwire and ball model\n\nWonderful Life (Gould)\n\nWoolsey, Tom\n\nWorld Cup (soccer)\n\nWright, Orville and Wilbur\n\nYoung, Thomas\n\nZeppelin, Count Ferdinand von\n\nZhou, Yong-Di\n\nzona incerta (ZI)\n\n\nABOUT THE AUTHOR\n\nMIGUEL A. L. NICOLELIS, M.D., Ph.D., is the Anne W. Deane Professor of\nNeuroscience at Duke University and founder of Duke’s Center for\nNeuroengineering. His award-winning research has been published in Nature,\nScience, and other leading scientific journals, as well as in Scientific\nAmerican, which named him one of the twenty most influential scientists in the\nworld. A member of the French, Brazilian, and Pontifical academies of\nsciences, he lives in North Carolina. For more information, go to\n[www.beyondboundariesnicolelis.net](http://www.beyondboundariesnicolelis.net).\n\n\n![image](../images/00053.jpeg)\n\nTimes Books\n\nHenry Holt and Company, LLC\n\nPublishers since 1866\n\n175 Fifth Avenue\n\nNew York, New York 10010\n\nHenry Holt® is a registered trademark of  \nHenry Holt and Company, LLC.\n\nCopyright © 2011 by Miguel Nicolelis\n\nAll rights reserved.\n\nLibrary of Congress Cataloging-in-Publication Data\n\nNicolelis, Miguel.\n\nBeyond boundaries : the new neuroscience of connecting brains with\nmachines—and how it will change our lives / Miguel Nicolelis.—1st ed.\n\np. cm.\n\nIncludes bibliographical references and index.\n\nISBN 978-0-8050-9052-9\n\n1\\. Thought and thinking. 2. Neurosciences. 3. Brain. 4. Machinery. I. Title.\n\nBF441.N494 2011\n\n003'.5—dc22 2010028440\n\nFirst Edition 2011\n\neISBN 978-1-4299-5079-4\n\nFirst Times Books eBook Edition: March 2011\n\n\n\n\n",
    "book_id": "beyond_boundaries",
    "book_title": "Beyond Boundaries: The New Neuroscience of Connecting Brains with Machines---and How It Will Change Our Lives",
    "book_author": "Nicolelis, Miguel",
    "topic_id": "ai_theory",
    "topic_label": "theory",
    "chunk_index": 1
  },
  {
    "chunk_full": "![](assets/cover.png)\n\n\n# Design\n\n[![](assets/DesignAd.png)](http://www.oreilly.com/design/free/?cmp=pd-design-\nfree-lp-lgen_designreportad2016)\n\n\n# Machine Learning for Designers\n\nPatrick Hebron\n\n\n# Machine Learning for Designers\n\nby Patrick  Hebron\n\nCopyright © 2016 O’Reilly Media, Inc. All rights reserved.\n\nPrinted in the United States of America.\n\nPublished by O’Reilly Media, Inc., 1005 Gravenstein Highway North, Sebastopol,\nCA 95472.\n\nO’Reilly books may be purchased for educational, business, or sales\npromotional use. Online editions are also available for most titles\n(<http://safaribooksonline.com>). For more information, contact our\ncorporate/institutional sales department: 800-998-9938 or\ncorporate@oreilly.com.\n\n  * Editor: Angela Rufino\n  * Production Editor: Shiny Kalapurakkel\n  * Copyeditor: Dianne Russell, Octal Publishing, Inc.\n  * Proofreader: Molly Ives Brower\n  * Interior Designer: David Futato\n  * Cover Designer: Randy Comer\n  * Illustrator: Rebecca Panzer\n\n  * June 2016: First Edition\n\n# Revision History for the First Edition\n\n  * 2016-06-09: First Release\n\nThe O’Reilly logo is a registered trademark of O’Reilly Media, Inc. Machine\nLearning for Designers, the cover image, and related trade dress are\ntrademarks of O’Reilly Media, Inc.\n\nWhile the publisher and the author have used good faith efforts to ensure that\nthe information and instructions contained in this work are accurate, the\npublisher and the author disclaim all responsibility for errors or omissions,\nincluding without limitation responsibility for damages resulting from the use\nof or reliance on this work. Use of the information and instructions contained\nin this work is at your own risk. If any code samples or other technology this\nwork contains or describes is subject to open source licenses or the\nintellectual property rights of others, it is your responsibility to ensure\nthat your use thereof complies with such licenses and/or rights.\n\n978-1-491-95620-5\n\n[LSI]\n\n\n  1. [Machine Learning for Designers](ch01.html#machine_learning_for_designers)\n     1. [Introduction](ch01.html#introduction)\n     2. [Why Design for Machine Learning is Different](ch01.html#why_design_for_machine_learning_is_diff)\n        1. [A Different Kind of Logic](ch01.html#a_different_kind_of_logic)\n        2. [A Different Kind of Development](ch01.html#a_different_kind_of_development)\n        3. [A Different Kind of Precision](ch01.html#a_different_kind_of_precision)\n        4. [A Different Kind of Problem](ch01.html#a_different_kind_of_problem)\n     3. [What Is Machine Learning?](ch01.html#what_is_machine_learning_question)\n        1. [The Mental Process of Recognizing Objects](ch01.html#the_mental_process_of_recognizing_object)\n        2. [Learning by Example](ch01.html#learning_by_example)\n        3. [Mechanical Induction](ch01.html#mechanical_induction)\n        4. [Common Analogies for Machine Learning](ch01.html#common_analogies_for_machine_learning)\n        5. [Ways of Learning](ch01.html#ways_of_learning)\n        6. [What Is Deep Learning?](ch01.html#what_is_deep_learning_question)\n     4. [Enhancing Design with Machine Learning](ch01.html#enhancing_design_with_machine_learning)\n        1. [Parsing Complex Information](ch01.html#parsing_complex_information)\n        2. [Enabling Multimodal User Input](ch01.html#enabling_multimodal_user_input)\n        3. [New Modes of Input](ch01.html#new_modes_of_input)\n        4. [Creating Dialogue](ch01.html#creating_dialogue)\n        5. [Designing Building Blocks](ch01.html#designing_building_blocks)\n        6. [Acquiring Training Data](ch01.html#acquiring_training_data)\n        7. [The Intelligence Feedback Loop](ch01.html#the_intelligence_feedback_loop)\n     5. [Dealing with Challenges](ch01.html#dealing_with_challenges)\n        1. [Designing for Uncertainty](ch01.html#designing_for_uncertainty)\n        2. [Mitigating Faulty Assumptions](ch01.html#mitigating_faulty_assumptions)\n        3. [Creating Sanity Checks](ch01.html#creating_sanity_checks)\n     6. [Working with Machine Learning Platforms](ch01.html#working_with_machine_learning_platforms)\n        1. [Machine-Learning-as-a-Service Platforms](ch01.html#machine-learning-as-a-service_platforms)\n        2. [Open Source Machine Learning Toolkits](ch01.html#open_source_machine_learning_toolkits)\n        3. [Fully Customized Machine Learning Tools](ch01.html#fully_customized_machine_learning_tools)\n        4. [Machine Learning Prototyping Tools](ch01.html#machine_learning_prototyping_tools)\n        5. [Incorporating Machine Learning into Design Processes](ch01.html#incorporating_machine_learning_into_des)\n     7. [Conclusions](ch01.html#conclusions)\n     8. [Going Further](ch01.html#going_further)\n        1. [Staying Up-to-date with Advancements in the Field](ch01.html#staying_up-to-date_with_advancements_in)\n        2. [Resources for Further Study of Machine Learning](ch01.html#resources_for_further_study_of_machine)\n        3. [Technical Resources](ch01.html#technical_resources)\n\n\n# Machine Learning for Designers\n\n# Introduction\n\nSince the dawn of computing, we have dreamed of (and had nightmares about)\nmachines that can think and speak like us. But the computers we’ve interacted\nwith over the past few decades are a far cry from HAL 9000 or Samantha from\n_Her_. Nevertheless, machine learning is in the midst of a renaissance that\nwill transform countless industries and provide designers with a wide\nassortment of new tools for better engaging with and understanding users.\nThese technologies will give rise to new design challenges and require new\nways of thinking about the design of user interfaces and interactions.\n\nTo take full advantage of these systems’ vast technical capabilities,\ndesigners will need to forge even deeper collaborative relationships with\nprogrammers. As these complex technologies make their way from research\nprototypes to user-facing products, programmers will also rely upon designers\nto discover engaging applications for these systems.\n\nIn the text that follows, we will explore some of the technical properties and\nconstraints of machine learning systems as well as their implications for\nuser-facing designs. We will look at how designers can develop interaction\nparadigms and a design vocabulary around these technologies and consider how\ndesigners can begin to incorporate the power of machine learning into their\nwork.\n\n# Why Design for Machine Learning is Different\n\n## A Different Kind of Logic\n\nIn our everyday communication, we generally use what logicians call _fuzzy\nlogic_. This form of logic relates to approximate rather than exact reasoning.\nFor example, we might identify an object as being “very small,” “slightly\nred,” or “pretty nearby.” These statements do not hold an exact meaning and\nare often context-dependent. When we say that a car is small, this implies a\nvery different scale than when we say that a planet is small. Describing an\nobject in these terms requires an auxiliary knowledge of the range of possible\nvalues that exists within a specific domain of meaning. If we had only seen\none car ever, we would not be able to distinguish a small car from a large\none. Even if we had seen a handful of cars, we could not say with great\nassurance that we knew the full range of possible car sizes. With sufficient\nexperience, we could never be completely sure that we had seen the smallest\nand largest of all cars, but we could feel relatively certain that we had a\ngood approximation of the range. Since the people around us will tend to have\nhad relatively similar experiences of cars, we can meaningfully discuss them\nwith one another in fuzzy terms.\n\nComputers, however, have not traditionally had access to this sort of\nauxiliary knowledge. Instead, they have lived a life of experiential\ndeprivation. As such, traditional computing platforms have been designed to\noperate on logical expressions that can be evaluated without the knowledge of\nany outside factor beyond those expressly provided to them. Though fuzzy\nlogical expressions can be employed by traditional platforms through the\nprogrammer’s or user’s explicit delineation of a fuzzy term such as “very\nsmall,” these systems have generally been designed to deal with _boolean\nlogic_ (also called “binary logic”), in which every expression must ultimately\nevaluate to either true or false. One rationale for this approach, as we will\ndiscuss further in the next section, is that boolean logic allows a computer\nprogram’s behavior to be defined as a finite set of concrete states, making it\neasier to build and test systems that will behave in a predictable manner and\nconform precisely to their programmer’s intentions.\n\nMachine learning changes all this by providing mechanisms for imparting\nexperiential knowledge upon computing systems. These technologies enable\nmachines to deal with fuzzier and more complex or “human” concepts, but also\nbring an assortment of design challenges related to the sometimes problematic\nnature of working with imprecise terminology and unpredictable behavior.\n\n## A Different Kind of Development\n\nIn traditional programming environments, developers use boolean logic to\nexplicitly describe each of a program’s possible states and the exact\nconditions under which the user will be able to transition between them. This\nis analogous to a “choose-your-own-adventure” book, which contains\ninstructions like, “if you want the prince to fight the dragon, turn to page\n32.” In code, a _conditional expression_ (also called an _if-statement_) is\nemployed to move the user to a particular portion of the code if some pre\ndefined set of conditions is met.\n\nIn pseudocode, a conditional expression might look like this:\n\n    \n    \n    if ( mouse button is pressed and mouse is over the 'Login'\n    button ),\n    then show the 'Welcome' screen\n\nSince a program comprises a finite number of states and transitions, which can\nbe explicitly enumerated and inspected, the program’s overall behavior should\nbe predictable, repeatable, and testable. This is not to say, of course, that\ntraditional programmatic logic cannot contain hard-to-foresee “edge-cases,”\nwhich lead to undefined or undesirable behavior under some specific set of\nconditions that have not been addressed by the programmer. Yet, regardless of\nthe difficulty of identifying these problematic edge-cases in a complex piece\nof software, it is at least conceptually possible to methodically probe every\npossible path within the “choose-your-own-adventure” and prevent the user from\naccessing an undesirable state by altering or appending the program’s\nexplicitly defined logic.\n\nThe behavior of machine learning systems, on the other hand, is not defined\nthrough this kind of explicit programming process. Instead of using an\nexplicit set of rules to describe a program’s possible behaviors, a machine\nlearning system looks for patterns within a set of example behaviors in order\nto produce an approximate representation of the rules themselves.\n\nThis process is somewhat like our own mental processes for learning about the\nworld around us. Long before we encounter any formal description of the “laws”\nof physics, we learn to operate within them by observing the outcomes of our\ninteractions with the physical world. A child may have no awareness of\nNewton’s equations, but through repeated observation and experimentation, the\nchild will come to recognize patterns in the relationships between the\nphysical properties and behaviors of objects.\n\nWhile this approach offers an extremely effective mechanism for learning to\noperate on complex systems, it does not yield a concrete or explicit set of\nrules governing that system. In the context of human intelligence, we often\nrefer to this as “intuition,” or the ability to operate on complex systems\nwithout being able to formally articulate the procedure by which we achieved\nsome desired outcome. Informed by experience, we come up with a set of\napproximate or provisional rules known as _heuristics_ (or “rules of thumb”)\nand operate on that basis.\n\nIn a machine learning system, these implicitly defined rules look nothing like\nthe explicitly defined logical expressions of a traditional programming\nlanguage. Instead, they are comprised of distributed representations that\nimplicitly describe the probabilistic connections between the set of\ninterrelated components of a complex system.\n\nMachine learning often requires a very large number of examples to produce a\nstrong intuition for the behaviors of a complex system.\n\nIn a sense, this requirement is related to the problem of edge-cases, which\npresent a different set of challenges in the context of machine learning. Just\nas it is hard to imagine every possible outcome of a set of rules, it is,\nconversely, difficult to extrapolate every possible rule from a set of example\noutcomes. To extrapolate a good approximation of the rules, the learner must\nobserve many variations of their application. The learner must be exposed to\nthe more extreme or unlikely behaviors of a system as well as the most likely\nones. Or, as the educational philosopher Patricia Carini said, “To let meaning\noccur requires time and the possibility for the rich and varied relationships\namong things to become evident.”[1](ch01.html#idm139841928658480)\n\nWhile intuitive learners may be slower at rote procedural tasks such as those\nperformed by a calculator, they are able to perform much more complex tasks\nthat do not lend themselves to exact procedures. Nevertheless, even with an\nimmense amount of training, these intuitive approaches sometimes fail us. We\nmay, for instance, find ourselves mistakenly identifying a human face in a\ncloud or a grilled cheese sandwich.\n\n## A Different Kind of Precision\n\nA key principle in the design of conventional programming languages is that\neach feature should work in a predictable, repeatable manner provided that the\nfeature is being used correctly by the programmer. No matter how many times we\nperform an arithmetic operation such as “2 + 2,” we should always get the same\nanswer. If this is ever untrue, then a bug exists in the language or tool we\nare using. Though it is not inconceivable for a programming language to\ncontain a bug, it is relatively rare and would almost never pertain to an\noperation as commonly used as an arithmetic operator. To be extra certain that\nconventional code will operate as expected, most large-scale codebases ship\nwith a set of formal “unit tests” that can be run on the user’s machine at\ninstallation time to ensure that the functionality of the system is fully in\nline with the developer’s expectations.\n\nSo, putting rare bugs aside, conventional programming languages can be thought\nof as systems that are always correct about mundane things like concrete\nmathematical operations. Machine learning algorithms, on the other hand, can\nbe thought of as systems that are often correct about more complicated things\nlike identifying human faces in an image. Since a machine learning system is\ndesigned to probabilistically approximate a set of demonstrated behaviors, its\nvery nature generally precludes it from behaving in an entirely predictable\nand reproducible manner, even if it has been properly trained on an extremely\nlarge number of examples. This is not to say, of course, that a well-trained\nmachine learning system’s behavior must inherently be erratic to a detrimental\ndegree. Rather, it should be understood and considered within the design of\nmachine-learning-enhanced systems that their capacity for dealing with\nextraordinarily complex concepts and patterns also comes with a certain degree\nof imprecision and unpredictability beyond what can be expected from\ntraditional computing platforms.\n\nLater in the text, we will take a closer look at some design strategies for\ndealing with imprecision and unpredictable behaviors in machine learning\nsystems.\n\n## A Different Kind of Problem\n\nMachine learning can perform complex tasks that cannot be addressed by\nconventional computing platforms. However, the process of training and\nutilizing machine learning systems often comes with substantially greater\noverhead than the process of developing conventional systems. So while machine\nlearning systems can be taught to perform simple tasks such as arithmetic\noperations, as a general rule of thumb, you should only take a machine\nlearning approach to a given problem if no viable conventional approach\nexists.\n\nEven for tasks that are well-suited to a machine learning solution, there are\nnumerous considerations about which learning mechanisms to use and how to\ncurate the training data so that it can be most comprehensible to the learning\nsystem.\n\nIn the sections that follow, we will look more closely at how to identify\nproblems that are well-suited for machine learning solutions as well as the\nnumerous factors that go into applying learning algorithms to specific\nproblems. But for the time being, we should understand machine learning to be\nuseful in solving problems that can be encapsulated by a set of examples, but\nnot easily described in formal terms.\n\n# What Is Machine Learning?\n\n## The Mental Process of Recognizing Objects\n\nThink about your own mental process of recognizing a human face. It’s such an\ninnate, automatic behavior, it is difficult to think about in concrete terms.\nBut this difficulty is not only a product of the fact that you have performed\nthe task so many times. There are many other often-repeated procedures that we\ncould express concretely, like how to brush your teeth or scramble an egg.\nRather, it is nearly impossible to describe the process of recognizing a face\nbecause it involves the balancing of an extremely large and complex set of\ninterrelated factors, and therefore defies any concrete description as a\nsequence of steps or set of rules.\n\nTo begin with, there is a great deal of variation in the facial features of\npeople of different ethnicities, ages, and genders. Furthermore, every\nindividual person can be viewed from an infinite number of vantage points in\ncountless lighting scenarios and surrounding environments. In assessing\nwhether the object we are looking at is a human face, we must consider each of\nthese properties in relation to each other. As we change vantage points around\nthe face, the proportion and relative placement of the nose changes in\nrelation to the eyes. As the face moves closer to or further from other\nobjects and light sources, its coloring and regions of contrast change too.\n\nThere are infinite combinations of properties that would yield the valid\nidentification of a human face and an equally great number of combinations\nthat would not. The set of rules separating these two groups is just too\ncomplex to describe through conditional logic. We are able to identify a face\nalmost automatically because our great wealth of experience in observing and\ninteracting with the visible world has allowed us to build up a set of\nheuristics that can be used to quickly, intuitively, and somewhat imprecisely\ngauge whether a particular expression of properties is in the correct balance\nto form a human face.\n\n## Learning by Example\n\nIn logic, there are two main approaches to reasoning about how a set of\nspecific observations and a set of general rules relate to one another. In\n_deductive reasoning_ , we start with a broad theory about the rules governing\na system, distill this theory into more specific hypotheses, gather specific\nobservations and test them against our hypotheses in order to confirm whether\nthe original theory was correct. In _inductive reasoning_ , we start with a\ngroup of specific observations, look for patterns in those observations,\nformulate tentative hypotheses, and ultimately try to produce a general theory\nthat encompasses our original observations. See Figure 1-1 for an illustration\nof the differences between these two forms of reasoning.\n\n![mlfd 01in01](assets/mlfd_01in01.png)\n\n###### Figure 1-1. Deductive reasoning versus inductive reasoning\n\nEach of these approaches plays an important role in scientific inquiry. In\nsome cases, we have a general sense of the principles that govern a system,\nbut need to confirm that our beliefs hold true across many specific instances.\nIn other cases, we have made a set of observations and wish to develop a\ngeneral theory that explains them.\n\nTo a large extent, machine learning systems can be seen as tools that assist\nor automate inductive reasoning processes. In a simple system that is governed\nby a small number of rules, it is often quite easy to produce a general theory\nfrom a handful of specific examples. Consider Figure 1-2 as an example of such\na system.[2](ch01.html#idm139841928626160)\n\n![mlfd 01in02](assets/mlfd_01in02.png)\n\n###### Figure 1-2. A simple system\n\nIn this system, you should have no trouble uncovering the singular rule that\ngoverns inclusion: open figures are included and closed figures are excluded.\nOnce discovered, you can easily apply this rule to the uncategorized figures\nin the bottom row.\n\nIn Figure 1-3, you may have to look a bit harder.\n\n![mlfd 01in03](assets/mlfd_01in03.png)\n\n###### Figure 1-3. A more complex system\n\nHere, there seem to be more variables involved. You may have considered the\nshape and shading of each figure before discovering that in fact this system\nis also governed by a single attribute: the figure’s height. If it took you a\nmoment to discover the rule, it is likely because you spent time considering\nattributes that seemed like they would be pertinent to the determination but\nwere ultimately not. This kind of “noise” exists in many systems, making it\nmore difficult to isolate the meaningful attributes.\n\nLet’s now consider Figure 1-4.\n\n![mlfd 01in04](assets/mlfd_01in04.png)\n\n###### Figure 1-4. An even more complex system\n\nIn this diagram, the rules have in fact gotten a bit more complicated. Here,\nshaded triangles and unshaded quadrilaterals are included and all other\nfigures are excluded. This rule system is harder to uncover because it\ninvolves an interdependency between two attributes of the figures. Neither the\nshape nor the shading alone determines inclusion. A triangle’s inclusion\ndepends upon its shading and a shaded figure’s inclusion depends upon its\nshape. In machine learning, this is called a _linearly inseparable_ problem\nbecause it is not possible to separate the included and excluded figures using\na single “line” or determining attribute. Linearly inseparable problems are\nmore difficult for machine learning systems to solve, and it took several\ndecades of research to discover robust techniques for handling them. See\nFigure 1-5.\n\n![mlfd 01in05](assets/mlfd_01in05.png)\n\n###### Figure 1-5. Linearly separable versus linearly inseparable problems\n\nIn general, the difficulty of an inductive reasoning problem relates to the\nnumber of relevant and irrelevant attributes involved as well as the subtlety\nand interdependency of the relevant attributes. Many real-world problems, like\nrecognizing a human face, involve an immense number of interrelated attributes\nand a great deal of noise. For the vast majority of human history, this kind\nof problem has been beyond the reach of mechanical automation. The advent of\nmachine learning and the ability to automate the synthesis of general\nknowledge about complex systems from specific information has deeply\nsignificant and far-reaching implications. For designers, it means being able\nto understand users more holistically through their interactions with the\ninterfaces and experiences we build. This understanding will allow us to\nbetter anticipate and meet users’ needs, elevate their capabilities and extend\ntheir reach.\n\n## Mechanical Induction\n\nTo get a better sense of how machine learning algorithms actually perform\ninduction, let’s consider Figure 1-6.\n\n![mlfd 01in06](assets/mlfd_01in06.png)\n\n###### Figure 1-6. A system equivalent to the boolean logical expression,\n“AND”\n\nThis system is equivalent to the boolean logical expression, “AND.” That is,\nonly figures that are both shaded and closed are included. Before we turn our\nattention to induction, let’s first consider how we would implement this logic\nin an electrical system from a deductive point of view. In other words, if we\nalready knew the rule governing this system, how could we implement an\nelectrical device that determines whether a particular figure should be\nincluded or excluded? See Figure 1-7.\n\n![mlfd 01in07](assets/mlfd_01in07.png)\n\n###### Figure 1-7. The boolean logical expression AND represented as an\nelectrical circuit\n\nIn this diagram, we have a wire leading from each input attribute to a\n“decision node.” If a given figure is shaded, then an electrical signal will\nbe sent through the wire leading from Input A. If the figure is closed, then\nan electrical signal will be sent through the wire leading from Input B. The\ndecision node will output an electrical signal indicating that the figure is\nincluded if the sum of its input signals is greater than or equal to 1 volt.\n\nTo implement the behavior of an AND gate, we need to set the voltage\nassociated with each of the two input signals. Since the output threshold is 1\nvolt and we only want the output to be triggered if both inputs are active, we\ncan set the voltage associated with each input to 0.5 volts. In this\nconfiguration, if only one or neither input is active, the output threshold\nwill not be reached. With these signal voltages now set, we have implemented\nthe mechanics of the general rule governing the system and can use this\nelectronic device to deduce the correct output for any example input.\n\nNow, let us consider the same problem from an inductive point of view. In this\ncase, we have a set of example inputs and outputs that exemplify a rule but do\nnot know what the rule is. We wish to determine the nature of the rule using\nthese examples.\n\nLet’s again assume that the decision node’s output threshold is 1 volt. To\nreproduce the behavior of the AND gate by induction, we need to find voltage\nlevels for the input signals that will produce the expected output for each\npair of example inputs, telling us whether those inputs are included in the\nrule. The process of discovering the right combination of voltages can be seen\nas a kind of search problem.\n\nOne approach we might take is to choose random voltages for the input signals,\nuse these to predict the output of each example, and compare these predictions\nto the given outputs. If the predictions match the correct outputs, then we\nhave found good voltage levels. If not, we could choose new random voltages\nand start the process over. This process could then be repeated until the\nvoltages of each input were weighted so that the system could consistently\npredict whether each input pair fits the rule.\n\nIn a simple system like this one, a guess-and-check approach may allow us to\narrive at suitable voltages within a reasonable amount of time. But for a\nsystem that involves many more attributes, the number of possible combinations\nof signal voltages would be immense and we would be unlikely to guess suitable\nvalues efficiently. With each additional attribute, we would need to search\nfor a needle in an increasingly large haystack.\n\nRather than guessing randomly and starting over when the results are not\nsuitable, we could instead take an iterative approach. We could start with\nrandom values and check the output predictions they yield. But rather than\nstarting over if the results are inaccurate, we could instead look at the\nextent and direction of that inaccuracy and try to incrementally adjust the\nvoltages to produce more accurate results. The process outlined above is a\nsimplified description of the learning procedure used by one of the earliest\nmachine learning systems, called a _Perceptron_ (Figure 1-8), which was\ninvented by Frank Rosenblatt in 1957.[3](ch01.html#idm139841928589216)\n\n![mlfd 01in08](assets/mlfd_01in08.png)\n\n###### Figure 1-8. The architecture of a Perceptron\n\nOnce the Perceptron has completed the inductive learning process, we have a\nnetwork of voltage levels which implicitly describe the rule system. We call\nthis a _distributed representation_. It can produce the correct outputs, but\nit is hard to look at a distributed representation and understand the rules\nexplicitly. Like in our own neural networks, the rules are represented\nimplicitly or impressionistically. Nonetheless, they serve the desired\npurpose.\n\nThough Perceptrons are capable of performing inductive learning on simple\nsystems, they are not capable of solving linearly inseparable problems. To\nsolve this kind of problem, we need to account for interdependent\nrelationships between attributes. In a sense, we can think of an\ninterdependency as being a kind of attribute in itself. Yet, in complex data,\nit is often very difficult to spot interdependencies simply by looking at the\ndata. Therefore, we need some way of allowing the learning system to discover\nand account for these interdependencies on its own. This can be done by adding\none or more layers of nodes between the inputs and outputs. The express\npurpose of these “hidden” nodes is to characterize the interdependencies that\nmay be concealed within the relationships between the data’s concrete (or\n“visible”) attributes. The addition of these hidden nodes makes the inductive\nlearning process significantly more complex.\n\nThe _backpropagation_ algorithm, which was developed in the late 1960s but not\nfully utilized until a 1986 paper by David Rumelhart et\nal.,[4](ch01.html#idm139841928582448) can perform inductive learning for\nlinearly inseparable problems. Readers interested in learning more about these\nideas should refer to the section “Going Further”.\n\n## Common Analogies for Machine Learning\n\n### Biological systems\n\nWhen Leonardo da Vinci set out to design a flying machine, he naturally looked\nfor inspiration in the only flying machines of his time: winged animals. He\nstudied the stabilizing feathers of birds, observed how changes in wing shape\ncould be used for steering, and produced numerous sketches for machines\npowered by human “wing flapping.”\n\nUltimately, it has proven more practical to design flying machines around the\nmechanism of a spinning turbine than to directly imitate the flapping wing\nmotion of birds. Nevertheless, from da Vinci onward, human designers have\npulled many key principles and mechanisms for flight from their observations\nof biological systems. Nature, after all, had a head start in working on the\nproblem and we would be foolish to ignore its findings.\n\nSimilarly, since the only examples of intelligence we have had access to are\nthe living things of this planet, it should come as no surprise that machine\nlearning researchers have looked to biological systems for both the guiding\nprinciples and specific design mechanisms of learning and intelligence.\n\nIn a famous 1950 paper, “Computing Machinery and\nIntelligence,”[5](ch01.html#idm139841928567904) the computer science luminary\nAlan Turing pondered the question of whether machines could be made to think.\nRealizing that “thought” was a difficult notion to define, Turing proposed\nwhat he believed to be a closely related and unambiguous way of reframing the\nquestion: “Are there imaginable digital computers which would do well in the\n_imitation game_?” In the proposed game, which is now generally referred to as\na _Turing Test_ , a human interrogator poses written questions to a human and\na machine. If the interrogator is unable to determine which party is human\nbased on the responses to these questions, then it may be reasoned that the\nmachine is intelligent. In the framing of this approach, it is clear that a\nsystem’s similarity to a biologically produced intelligence has been a central\nmetric in evaluating machine intelligence since the inception of the field.\n\nIn the early history of the field, numerous attempts were made at developing\nanalog and digital systems that simulated the workings of the human brain. One\nsuch analog device was the Homeostat, developed by William Ross Ashby in 1948,\nwhich used an electro-mechanical process to detect and compensate for changes\nin a physical space in order to create stable environmental conditions. In\n1959, Herbert Simon, J.C. Shaw, and Allen Newell developed a digital system\ncalled the General Problem Solver, which could automatically produce\nmathematical proofs to formal logic problems. This system was capable of\nsolving simple test problems such as the Tower of Hanoi puzzle, but did not\nscale well because its search-based approach required the storage of an\nintractable number of combinations in solving more complex problems.\n\nAs the field has matured, one major category of machine learning algorithms in\nparticular has focused on imitating biological learning systems: the\nappropriately named _Artificial Neural Networks_ (ANNs). These machines, which\ninclude Perceptrons as well as the deep learning systems discussed later in\nthis text, are modeled after but implemented differently from biological\nsystems. See Figure 1-9.\n\n![mlfd 01in09](assets/mlfd_01in09.png)\n\n###### Figure 1-9. The simulated neurons of an ANN\n\nInstead of the electrochemical processes performed by biological neurons, ANNs\nemploy traditional computer circuitry and code to produce simplified\nmathematical models of neural architecture and activity. ANNs have a long way\nto go in approaching the advanced and generalized intelligence of humans. Like\nthe relationship between birds and airplanes, we may continue to find\npractical reasons for deviating from the specific mechanisms of biological\nsystems. Still, ANNs have borrowed a great many ideas from their biological\ncounterparts and will continue to do so as the fields of neuroscience and\nmachine learning evolve.\n\n### Thermodynamic systems\n\nOne indirect outcome of machine learning is that the effort to produce\npractical learning machines has also led to deeper philosophical\nunderstandings of what learning and intelligence really are as phenomena in\nnature. In science fiction, we tend to assume that all advanced intelligences\nwould be something like ourselves, since we have no dramatically different\nexamples of intelligence to draw upon.\n\nFor this reason, it might be surprising to learn that one of the primary\ninspirations for the mathematical models used in machine learning comes from\nthe field of Thermodynamics, a branch of physics concerned with heat and\nenergy transfer. Though we would certainly call the behaviors of thermal\nsystems complex, we have not generally thought of these systems as holding a\nstrong relation to the fundamental principles of intelligence and life.\n\nFrom our earlier discussion of inductive reasoning, we may see that learning\nhas a great deal to do with the gradual or iterative process of finding a\nbalance between many interrelated factors. The conceptual relationship between\nthis process and the tendency of thermal systems to seek equilibrium has\nallowed machine learning researchers to adopt some of the ideas and equations\nestablished within thermodynamics to their efforts to model the\ncharacteristics of learning.\n\nOf course, what we choose to call “intelligence” or “life” is a matter of\nlanguage more than anything else. Nevertheless, it is interesting to see these\nphenomena in a broader context and understand that nature has a way of reusing\ncertain principles across many disparate applications.\n\n### Electrical systems\n\nBy the start of the twentieth century, scientists had begun to understand that\nthe brain’s ability to store memories and trigger actions in the body was\nproduced by the transmission of electrical signals between neurons. By mid-\ncentury, several preliminary models for simulating the electrical behaviors of\nan individual neuron had been developed, including the Perceptron. As we saw\nin the “Biological systems” section, these models have some important\nsimilarities to the logic gates that comprise the basic building blocks of\nelectronic systems. In its most basic conception, an individual neuron\ncollects electrical signals from the other neurons that lead into it and\nforwards the electrical signal to its connected output neurons when a\nsufficient number of its inputs have been electrically activated.\n\nThese early discoveries contributed to a dramatic overestimation of the ease\nwith which we would be able to produce a true artificial intelligence. As the\nfields of neuroscience and machine learning have progressed, we have come to\nsee that understanding the electrical behaviors and underlying mathematical\nproperties of an individual neuron elucidates only a tiny aspect of the\noverall workings of a brain. In describing the mechanics of a simple learning\nmachine somewhat like a Perceptron, Alan Turing remarked, “The behavior of a\nmachine with so few units is naturally very trivial. However, machines of this\ncharacter can behave in a very complicated manner when the number of units is\nlarge.”[6](ch01.html#idm139841928546192)\n\nDespite some similarities in their basic building blocks, neural networks and\nconventional electronic systems use very different sets of principles in\ncombining their basic building blocks to produce more complex behaviors. An\nelectronic component helps to route electrical signals through explicit\nlogical decision paths in much the same manner as conventional computer\nprograms. Individual neurons, on the other hand, are used to store small\npieces of the distributed representations of inductively approximated rule\nsystems.\n\nSo, while there is in one sense a very real connection between neural networks\nand electrical systems, we should be careful not to think of brains or machine\nlearning systems as mere extensions of the kinds of systems studied within the\nfield of electrical engineering.\n\n## Ways of Learning\n\nIn machine learning, the terms _supervised_ , _unsupervised_ , _semi-\nsupervised_ , and _reinforcement learning_ are used to describe some of the\nkey differences in how various models and algorithms learn and what they learn\nabout. There are many additional terms used within the field of machine\nlearning to describe other important distinctions, but these four categories\nprovide a basic vocabulary for discussing the main types of machine learning\nsystems:\n\n_Supervised learning_ procedures are used in problems for which we can provide\nthe system with example inputs as well as their corresponding outputs and wish\nto induce an implicit approximation of the rules or function that governs\nthese correlations. Procedures of this kind are “supervised” in the sense that\nwe explicitly indicate what correlations should be found and only ask the\nmachine how to substantiate these correlations. Once trained, a supervised\nlearning system should be able to predict the correct output for an input\nexample that is similar in nature to the training examples, but not explicitly\ncontained within it. The kinds of problems that can be addressed by supervised\nlearning procedures are generally divided into two categories:\n_classification_ and _regression_ problems. In a classification problem, the\noutputs relate to a set of discrete categories. For example, we may have an\nimage of a handwritten character and wish to determine which of 26 possible\nletters it represents. In a regression problem, the outputs relate to a real-\nvalued number. For example, based on a set of financial metrics and past\nperformance data, we may try to guess the future price of a particular stock.\n\n_Unsupervised learning_ procedures do not require a set of known outputs.\nInstead, the machine is tasked with finding internal patterns within the\ntraining examples. Procedures of this kind are “unsupervised” in the sense\nthat we do not explicitly indicate what the system should learn about.\nInstead, we provide a set of training examples that we believe contains\ninternal patterns and leave it to the system to discover those patterns on its\nown. In general, unsupervised learning can provide assistance in our efforts\nto understand extremely complex systems whose internal patterns may be too\ncomplex for humans to discover on their own. Unsupervised learning can also be\nused to produce _generative_ models, which can, for example, learn the\nstylistic patterns in a particular composer’s work and then generate new\ncompositions in that style. Unsupervised learning has been a subject of\nincreasing excitement and plays a key role in the deep learning renaissance,\nwhich is described in greater detail below. One of the main causes of this\nexcitement has been the realization that unsupervised learning can be used to\ndramatically improve the quality of supervised learning processes, as\ndiscussed immediately below.\n\n_Semi-supervised learning_ procedures use the automatic feature discovery\ncapabilities of unsupervised learning systems to improve the quality of\npredictions in a supervised learning problem. Instead of trying to correlate\nraw input data with the known outputs, the raw inputs are first interpreted by\nan unsupervised system. The unsupervised system tries to discover internal\npatterns within the raw input data, removing some of the noise and helping to\nbring forward the most important or indicative features of the data. These\ndistilled versions of the data are then handed over to a supervised learning\nmodel, which correlates the distilled inputs with their corresponding outputs\nin order to produce a predictive model whose accuracy is generally far greater\nthan that of a purely supervised learning system. This approach can be\nparticularly useful in cases where only a small portion of the available\ntraining examples have been associated with a known output. One such example\nis the task of correlating photographic images with the names of the objects\nthey depict. An immense number of photographic images can be found on the Web,\nbut only a small percentage of them come with reliable linguistic\nassociations. Semi-supervised learning allows the system to discover internal\npatterns within the full set of images and associate these patterns with the\ndescriptive labels that were provided for a limited number of examples. This\napproach bears some resemblance to our own learning process in the sense that\nwe have many experiences interacting with a particular kind of object, but a\nmuch smaller number of experiences in which another person explicitly tells us\nthe name of that object.\n\n_Reinforcement learning_ procedures use rewards and punishments to shape the\nbehavior of a system with respect to one or several specific goals. Unlike\nsupervised and unsupervised learning systems, reinforcement learning systems\nare not generally trained on an existent dataset and instead learn primarily\nfrom the feedback they gather through performing actions and observing the\nconsequences. In systems of this kind, the machine is tasked with discovering\nbehaviors that result in the greatest reward, an approach which is\nparticularly applicable to robotics and tasks like learning to play a board\ngame in which it is possible to explicitly define the characteristics of a\nsuccessful action but not how and when to perform those actions in all\npossible scenarios.\n\n## What Is Deep Learning?\n\nFrom Alan Turing’s writings onwards, the history of machine learning has been\nmarked by alternating periods of optimism and discouragement over the field’s\nprospects for applying its conceptual advancements to practical systems and,\nin particular, to the construction of a general-purpose artificial\nintelligence. These periods of discouragement, which are often called _AI\nwinters_ , have generally stemmed from the realization that a particular\nconceptual model could not be easily scaled from simple test problems to more\ncomplex learning tasks. This occurred in the 1960s when Marvin Minsky and\nSeymour Papert conclusively demonstrated that perceptrons could not solve\nlinearly inseparable problems. In the late 1980s, there was some initial\nexcitement over the backpropagation algorithm’s ability to overcome this\nissue. But another AI winter occurred when it became clear that the\nalgorithm’s theoretical capabilities were practically constrained by\ncomputationally intensive training processes and the limited hardware of the\ntime.\n\nOver the last decade, a series of technical advances in the architecture and\ntraining procedures associated with artificial neural networks, along with\nrapid progress in computing hardware, have contributed to a renewed optimism\nfor the prospects of machine learning. One of the central ideas driving these\nadvances is the realization that complex patterns can be understood as\nhierarchical phenomena in which simple patterns are used to form the building\nblocks for the description of more complex ones, which can in turn be used to\ndescribe even more complex ones. The systems that have arisen from this\nresearch are referred to as “deep” because they generally involve multiple\nlayers of learning systems which are tasked with discovering increasingly\nabstract or “high-level” patterns. This approach is often referred to as\n_hierarchical feature learning_.\n\nAs we saw in our earlier discussion of the process of recognizing a human\nface, learning about a complex idea from raw data is challenging because of\nthe immense variability and noise that may exist within the data samples\nrepresenting a particular concept or object.\n\nRather than trying to correlate raw pixel information with the notion of a\nhuman face, we can break the problem down into several successive stages of\nconceptual abstraction (see Figure 1-10). In the first layer, we might try to\ndiscover simple patterns in the relationships between individual pixels. These\npatterns would describe basic geometric components such as lines. In the next\nlayer, these basic patterns could be used to represent the underlying\ncomponents of more complex geometric features such as surfaces, which could be\nused by yet another layer to describe the complex set of shapes that compose\nan object like a human face.\n\n![mlfd 01in10](assets/mlfd_01in10.png)\n\n###### Figure 1-10. Hierarchical feature layers of an image recognition\nconvolutional neural network (image courtesy of Jason Yosinski, Jeff Clune,\nAnh Nguyen, Thomas Fuchs, and Hod Lipson, “Understanding neural networks\nthrough deep visualization,” presented at the Deep Learning Workshop,\nInternational Conference on Machine Learning (ICML), 2015)\n\nAs it turns out, the backpropagation algorithm and other earlier machine\nlearning models are capable of achieving results comparable to those\nassociated with more recent deep learning models, given sufficient training\ntime and hardware resources. It should also be noted that many of the ideas\ndriving the practical advances of deep learning have been mined from various\ncomponents of earlier models. In many ways, the recent successes of deep\nlearning have less to do with the discovery of radically new techniques than\nwith a series of subtle yet important shifts in our understanding of various\ncomponent ideas and how to combine them. Nevertheless, a well-timed shift in\nperspective, coupled with diminishing technical constraints, can make a world\nof difference in exposing a series of opportunities that may have been\npreviously conceivable but were not practically achievable.\n\nAs a result of these changes, engineers and designers are poised to approach\never more complex problems through machine learning. They will be able to\nproduce more accurate results and iterate upon machine-learning-enhanced\nsystems more quickly. The improved performance of these systems will also\nenable designers to include machine learning functionality that would have\nonce required the resources of a supercomputer in mobile and embedded devices,\nopening a wide range of new applications that will greatly impact users.\n\nAs these technologies continue to progress over the next few years, we will\ncontinue to see radical transformations in an astounding number of theoretical\nand real-world applications from art and design to medicine, business, and\ngovernment.\n\n# Enhancing Design with Machine Learning\n\n## Parsing Complex Information\n\nComputers have long offered peripheral input devices like microphones and\ncameras, but despite their ability to transmit and store the data produced by\nthese devices, they have not been able to understand it. Machine learning\nenables the parsing of complex information from a wide assortment of sources\nthat were once completely indecipherable to machines.\n\nThe ability to recognize spoken language, facial expressions, and the objects\nin a photograph enables designers to transcend the expressive limitations of\ntraditional input devices such as the keyboard and mouse, opening an entirely\nnew set of interaction paradigms that will allow users to communicate ideas in\never more natural and intuitive ways.\n\nIn the sections that follow, we will look more closely at some of these\nopportunities. But before doing so, it should be noted that some of the\npossibilities we will explore currently require special hardware or intensive\ncomputing resources that may not be practical or accessible in all design\ncontexts at this time. For example, the Microsoft Kinect, which allows depth\nsensing and body tracking, is not easily paired with a web-based experience.\nThe quickly evolving landscape of consumer electronics will progressively\ndeliver these capabilities to an ever wider spectrum of devices and platforms.\nNevertheless, designers must take these practical constraints into\nconsideration as they plan the features of their systems.\n\n## Enabling Multimodal User Input\n\nIn our everyday interactions with other people, we use hand gestures and\nfacial expressions, point to objects and draw simple diagrams. These auxiliary\nmechanisms allow us to clarify the meaning of ideas that do not easily lend\nthemselves to verbal language. They provide subtle cues, enriching our\ndescriptions and conveying further implications of meaning like sarcasm and\ntone. As Nicholas Negroponte said in _The Architecture Machine_ , “it is\ngestures, smiles, and frowns that turn a conversation into a\ndialogue.”[7](ch01.html#idm139841928469248)\n\nIn our communications with computers, we have been limited to the mouse,\nkeyboard, and a much smaller set of linguistic expressions. Machine learning\nenables significantly deeper forms of linguistic communication with computers,\nbut there are still many ideas that would be best expressed through other\nmeans—visual, auditory, or otherwise. As machine learning continues to make a\nwider variety of media understandable to the computer, designers should begin\nto employ “multimodal” forms of human-computer interaction, allowing users to\nconvey ideas through the optimal means of communication for a given task. As\nthe saying goes, “a picture is worth a thousand words”—at least when the idea\nis an inherently visual one.\n\nFor example, let’s say the user needed a particular kind of screwdriver but\ndidn’t know the term “Phillips head.” Previously, he might have tried Googling\na variety of search terms or scouring through multiple Amazon listings. With\nmultimodal input, the user could instead tell the computer, “I’m looking for a\nscrewdriver that can turn this kind of screw” and then upload a photograph or\ndraw a sketch of it. The computer would then be able to infer which tool was\nneeded and point the user to possible places to purchase it.\n\nBy conducting each exchange in the most appropriate modality, communication is\nmade more efficient and precise. Interactions between user and machine become\ndeeper and more varied, making the experience of working with a computer less\nmonotonous and therefore more enjoyable. Complexity and nuance are preserved\nwhere they might otherwise have been lost to a translation between media.\n\nThis heightened expressivity, made possible by machine learning’s ability to\nextract meaning from complex and varied sources, will dramatically alter the\nnature of human&8#211;computer interactions and require designers to rethink\nsome of the longstanding principles of user interface and user experience\ndesign.\n\n## New Modes of Input\n\n### Visual Inputs\n\nThe visible world is full of nuanced information that is not easily conveyed\nthrough other means. For this reason, extracting information from images has\nbeen one of the primary applied goals throughout the history of machine\nlearning. One of the earliest applications in this domain is _optical\ncharacter recognition_ —the task of decoding textual information, either\nhandwritten or printed, from photographic sources (see Figure 1-11). This\ntechnology is used in a wide range of real-world applications from the postal\nservice’s need to quickly decipher address labels to Google’s effort to\ndigitize and make searchable the world’s books and newspapers. The pursuit of\noptical character recognition systems has helped to drive machine learning\nresearch in general and has remained as one of the key test problems for\nassessing the performance of newly invented machine learning algorithms.\n\n![mlfd 01in11](assets/mlfd_01in11.png)\n\n###### Figure 1-11. Handwritten ‘8’ digits from the MNIST database\n\nMore recently, researchers have turned their attention to more complex visual\nlearning tasks, many of which center around the problem of identifying objects\nin images. The goals of these systems range in both purpose and complexity. On\nthe simpler end of the spectrum, _object recognition_ systems are used to\ndetermine whether a particular image contains an object of a specific category\nsuch as a human face, cat, or tree. These technologies extend to more specific\nand advanced functionality, such as the identification of a particular human\nface within an image. This functionality is used in photo-sharing applications\nas well as security systems used by governments to identify known criminals.\n\nGoing further, _image tagging_ and _image description_ systems are used to\ngenerate keywords or a sentence that describes the contents of an image (see\nFigure 1-12). These technologies can be used to aid image-based search\nprocesses as well as assist visually impaired users to extract information\nfrom sources that would be otherwise inaccessible to them. Further still,\n_image segmentation_ systems are used to associate each pixel of a given image\nwith the category of object represented by that region of the image. In an\nimage of suburban home, for instance, all of the pixels associated with the\npatio floor would be painted one color while the grass, outdoor furniture and\ntrees depicted in the image would each be painted with their own unique\ncolors, creating a kind of pixel-by-pixel annotation of the image’s contents.\n\n![mlfd 01in12](assets/mlfd_01in12.png)\n\n###### Figure 1-12. Example outputs from a neural network trained to produce\nimage descriptions (image courtesy of Karpathy, Andrej, and Li Fei-Fei, “Deep\nvisual-semantic alignments for generating image descriptions,” proceedings of\nthe IEEE Conference on Computer Vision and Pattern Recognition, 2015)\n\nOther applications of machine learning to visual tasks include depth\nestimation and three-dimensional object extraction. These technologies are\napplicable to tasks in robotics and the development of self-driving cars as\nwell as the automated conversion of conventional two-dimensional movies into\nstereoscopic ones.\n\nThe range of visual applications for machine learning is too vast to enumerate\nfully here. In general, though, a great deal of machine learning research and\napplied work has and will continue to be directed to the numerous component\ngoals of turning once impenetrable pixel grids into high-level information\nthat can be acted upon by machines and used to aid users in a wide assortment\nof complex tasks.\n\n### Aural Inputs\n\nLike visual information, auditory information is highly complex and used to\nconvey a wide range of content ranging from human speech to music to bird\ncalls, which are not easily transmitted through other media. The ability for\nmachines to understand spoken language has immense implications for the\ndevelopment of more natural interaction paradigms. However, the diverse vocal\ncharacteristics and speech patterns of different speakers makes this task\ndifficult for machines and even, at times, human listeners. Though a highly\nreliable _speech-to-text_ system can greatly benefit human–computer\ninteractions, even a slightly less reliable system can result in great\nfrustration and lost productivity for the user. Like visual learning systems,\nimmense progress in the complex task of speech recognition has been made in\nrecent years, primarily as a result of breakthroughs in deep learning\nresearch. For most applications, these technologies have now matured to the\npoint where their utility generally outweighs any residual imprecision in\ntheir capabilities.\n\nAside from speech recognition, the ability to recognize a piece of music\naurally has been another popular area of focus for machine learning research.\nThe Shazam app allows users to identify songs by allowing the software to\ncapture a short snippet of the audio. This system, however, can only identify\na song from its original recording rather than allowing users to sing or hum a\nmelody they wish to identify. The SoundHound app offers this functionality,\nthough it is generally less reliable than Shazam’s functionality. This is\nunderstandable because Shazam can utilize subtle patterns in the recorded\naudio information to produce accurate recognition results, whereas\nSoundHound’s functionality must attempt to account for the potentially highly\nimprecise or out-of-tune approximation of the recording by the user’s own\nvoice. Both systems, however, provide users with capabilities that would hard\nto supplant through other means - most readers will be able to recall a time\nin which they hummed a melody to friends, hoping someone might be able to\nidentify the song. The underlying technologies used by these systems can also\nbe directed towards other audio recognition tasks such as the identification\nof a bird from its call or the identification of a malfunctioning mechanical\nsystem from the noise it makes.\n\n### Corporeal Inputs\n\nBody language can convey subtle information about a user’s emotional state,\naugment or clarify the tone of a verbal expression, or be used to specify what\nobject is being discussed through the act of pointing. Machine learning\nsystems, in conjunction with a range of new hardware devices, have enabled\ndesigners to provide users with mechanisms for communicating with machines\nthrough the endlessly expressive capabilities of the human body. See Figure\n1-13.\n\n![mlfd 01in13](assets/mlfd_01in13.png)\n\n###### Figure 1-13. Skeleton tracking data produced by the Microsoft Kinect 2\nfor Windows\n\nDevices such as the Microsoft Kinect and Leap Motion use machine learning to\nextract information about the location of a user’s body from photographic data\nproduced by specialized hardware. The Kinect 2 for Windows allows designers to\nextract 20 3-dimensional joint positions through its full-body skeleton\ntracking feature and more than a thousand 3-dimensional points of information\nthrough its high-definition face tracking feature. The Leap Motion device\nprovides high-resolution positioning information related to the user’s hands.\n\nThese forms of input data can be coupled with machine-learning-based gesture\nor facial expression recognition systems, allowing users to control software\ninterfaces with the more expressive features of their bodies and enabling\ndesigners to extract information about the user’s mood.\n\nTo some extent, similar functionality can be produced using lower-cost and\nmore widely available camera hardware. For the time being, these specialized\nhardware systems help to make up for the limited precision of their underlying\nmachine learning systems. However, as the capabilities of these machine\nlearning tools quickly advance, the need for specialized hardware in\naddressing these forms of corporeal input will be diminished or rendered\nunnecessary.\n\nIn addition to these input devices, health tracking devices like Fitbit and\nApple Watch can also provide designers with important information about the\nuser and her physical state. From detecting elevated stress levels to\nanticipating a possible cardiac event, these forms of user input will prove\ninvaluable in better serving users and even saving lives.\n\n### Environmental Inputs\n\nEnvironmental sensors and Internet-connected objects can provide designers\nwith a great deal of information about users’ surroundings, and therefore\nabout the users themselves. The Nest Learning Thermostat (Figure 1-14), for\ninstance, tracks patterns in homeowners’ behaviors to determine when they are\nat home as well as their desired temperature settings at different times of\nday and during different seasons. These patterns are used to automatically\ntune the thermostat’s settings to meet user needs and make climate control\nsystems more efficient and cost-effective.\n\n![mlfd 01in14](assets/mlfd_01in14.png)\n\n###### Figure 1-14. Nest Learning Thermostat\n\nAs Internet-of-Things devices become more prevalent, these input devices will\nprovide designers with new opportunities to assist users in a wide assortment\nof tasks from knowing when they are out of milk to when their basement has\nflooded.\n\n### Abstract Inputs\n\nIn addition to physical forms of input, machine learning allows designers to\ndiscover implicit patterns within numerous facets of a user’s behavior. These\npatterns carry inherent meanings, which can be learned from and acted upon,\neven if the user is not expressly aware of having communicated them. In this\nsense, these implicit patterns can be thought of as input modalities that, in\npractice, serve a very similar purpose to the more tangible input modes\ndescribed above.\n\nMining behavioral patterns through machine learning can help designers to\nbetter understand users and serve their needs. At the same time, these\npatterns can also help designers to understand the products or services they\noffer as well as the implicit relationships between these offerings.\nBehavioral patterns can be mined in relation to an individual user or\naggregated from the collective behaviors of numerous users.\n\nOne form of pattern mining that can be useful in serving an individual user as\nwell as improving the overall system is the discovery of frequently coupled\nbehaviors within the sequence of actions performed by the user. For example, a\nuser may purchase milk whenever he buys breakfast cereal. Noticing this\npattern gives designers the opportunity to construct interface mechanisms that\nwill allow the user to address his shopping needs more easily and efficiently.\nWhen the user adds cereal to the shopping cart, a modal interface suggesting\nthe purchase of milk could be presented to him. Alternately, these two items\ncould be shown in proximity to one another within the interface, despite the\nfact that these two products would generally be situated within two different\nareas of the store. Rather than presenting the user with separate interfaces\nfor each item, the system could instead dynamically generate a single\ninterface element that would allow the user to purchase these frequently\ncoupled items with one click. In addition to benefiting the user’s shopping\nexperience and enabling multiuser recommendation engines, the system’s\nknowledge of these correlated behaviors can be used to make the system itself\nmore efficient, aiding business processes like inventory estimation.\n\nMining user behavior patterns can also help businesses to better understand\nwho their customers are. If a user frequently purchases diapers, for instance,\nit is a near certainty that the user is a parent. This kind of auxiliary\nknowledge of the user can help designers to produce interfaces that better\naddress their target customer demographics and influence business and\nmarketing decisions such as the determination of which advertising venues will\nyield the greatest influx of new customers. Designers should be careful,\nhowever, to not make assumptions about users that may embarrass or offend them\nby characterizing them in ways that conflict with the public persona they wish\nto convey.\n\nIn one famous incident, the retailer Target used purchasing patterns to\ndetermine whether a given user was pregnant so that the store could better\ntarget this much sought-after category of\ncustomer.[8](ch01.html#idm139841928426640) Though this practice may be\nwelcomed by some, in at least one case it created an uncomfortable situation\nwhen an angry father came to a Target store demanding to know why his high-\nschool-aged daughter had received numerous coupons targeted at expectant\nmothers. The man had not been aware that his daughter was pregnant and upon\nlearning the truth from his daughter, apologized to the retailer.\nNevertheless, these kinds of customer insights can be unsettling and require\ngreat care in their treatment by designers. Issues of this kind will be\ndiscussed in greater detail in the section “Mitigating Faulty Assumptions”.\n\nWhen handled with care, however, user insights can provide great value to\ncustomers and businesses alike. Understanding a user’s behavioral patterns can\nalso aid security processes such as the detection of fraudulent use of a\ncustomer’s account information. Credit card companies and some retailers\nregularly use consumer purchasing patterns to assess whether a given purchase\nis fraudulent by checking whether the geographic location of the transaction\nand the items purchased are in line with the customer’s history. If an anomaly\nis detected, the transaction will be blocked and the customer will be notified\nthat their financial information may be compromised.\n\n## Creating Dialogue\n\nConventional user interfaces tend to rely upon menu systems as the primary\nmeans of organizing the set of features available to the user. One positive\naspect of this approach is that it provides a clear and explicit mechanism for\nthe user to explore the system and learn what is possible within it.\nHierarchical menus enable users to quickly focus their search for a specific\nfeature by curating the system’s functionality into neatly delineated, domain-\nspecific groupings. If the user is unaware of a particular feature, its\nproximity to other more familiar functions may lead the user to experiment\nwith it, expanding her knowledge of the system in an organic manner.\n\nThe downside to hierarchical menu systems, however, is that once users have\nlearned the system’s capabilities, they still have to spend a great deal of\ntime navigating menus in order to reach commonly used features. This tedious\nlabor is sometimes mediated by the software’s offering of keyboard shortcuts.\nBut there is a limit to the number of shortcuts that can be plausibly\nutilized, and this approach is not easily embedded in web and mobile\ninterfaces.\n\nWith each additional feature, conventional menu systems become increasingly\ncomplex and hard to navigate. For professional tools like 3D-modeling and\nvideo-editing software, it may be reasonable to expect the user to commit to a\nsteep learning curve. But for intelligent assistants and other applications\nwith broad feature sets aimed at the average user, it would be impractical or\ncounterproductive to present the system’s functionality through a menu system.\nImagine what Siri would look like if it were to take this approach!\n\nProgrammers may be familiar with a different mechanism for facilitating\ninteraction: the “read-evaluate-print loop” (or REPL). In this kind of\ninterface, the programmer issues a specific textual command, the machine\nperforms it, prints the output, and waits for the user to issue another\ncommand. This back-and-forth dialogue takes the familiar form of a messaging\napp and allows programmers to work more quickly and dynamically by\ncircumventing the need for the tedious and incessant navigation of menu\nhierarchies.\n\nThis conversational format also provides an ideal medium for facilitating the\nkinds of multimodal interactions described in the previous section. In this\nmachine-learning-enhanced context, we can imagine many vibrant and varied\nexchanges between the user and machine. Much like a whiteboarding session held\nby human collaborators, the user and software would take turns addressing\naspects of the task at hand, solving its component problems and clarifying\ntheir ideas through an assortment of verbal expressions, diagrams, and\ngestures.\n\n### Assisting Feature Discovery\n\nDespite its many advantages, an open-ended dialogue does little to make the\nsystem’s capabilities known to the user. To make the most of a REPL\nprogramming interface, the user must hold prior knowledge of the available\nfeatures or make frequent reference to the documentation, which is likely to\nnegate any efficiency gains over menu-based workflows. For intelligent\ninterfaces, offloading the process of feature discovery to auxiliary\ndocumentation would be even less practical.\n\nThe potential limitations of a conversational user interface can be seen in\nour interactions with first-generation intelligent assistants such as Siri,\nCortana, and Echo(Figure 1-15). In this context, feature discovery often\nconsists of the user scouring his or her mind for phrases that might lead to\nsome amusing or useful response from the software.\n\n![mlfd 01in15](assets/mlfd_01in15.png)\n\n###### Figure 1-15. Amazon’s Echo\n\nOne solution to the problem of feature discovery that is offered by many\nintelligent assistants is to provide the user with a set of example inputs\nthat demonstrate a variety of features available within the system. The\ncomputational knowledge engine WolframAlpha embeds examples directly into its\ninterface, helping new users to become acquainted with the system’s mode of\ninteraction and the kinds of knowledge it is able to access. These well-\ncurated examples continue to engage even seasoned users, inviting further\nexploration into the entertaining or informative pathways they reveal.\n\nDemonstrating an application’s full range of features through a series of\nembedded examples can, however, begin to make the software look suspiciously\nlike a reference manual. Rather than handpicking a static group of examples,\nmachine learning or conventional programmatic logic can be used to dynamically\nselect examples that are contextually relevant to the user’s activity. Going\nfurther, machine intelligence could be used to generate speculative extensions\nof the user’s trajectory, providing a series of possible next steps in a range\nof conceptual directions for the user to consider. This mechanism would not\nonly extend the user’s understanding of the interface and its possibilities,\nbut also enrich his or her fluency with the task or domain of interest at\nhand.\n\nAnother common mechanism for helping users to locate features within a\nconversational interface is to offer suggested meanings when their statements\nare not clear to the system. This can be helpful in correcting the improperly\nformed expression of a command that is already more-or-less known to the user.\nBut if the user is completely unaware of some potentially advantageous\nfeature, he will not think to ask for it in the first place, and this\nmechanism can provide no assistance.\n\nWe could, in theory, defer the problem of feature discovery to the march of\nprogress in technology and assume that computers will eventually be\nintelligent enough to perform any task that could conceivably be asked of\nthem. Yet, the machine’s ability to handle any request would not inherently\ntranslate to the user’s awareness of every possible request that might prove\nuseful to his goals. Furthermore, the user may not be familiar enough with the\nproblem domain to develop a set of goals in the first place.\n\nThink for a moment about what question you might ask a Nobel laureate in\nchemistry. Without a strong working knowledge of the field, you would be\nunlikely to formulate a question that took full advantage of her expertise. To\nmake the most of this opportunity, you would like need some guidance in\nfinding an entry point to a fruitful line of inquiry.\n\nTo assist users in performing complex or domain-specific tasks, designers\nshould strive to build systems that not only respond to user requests but also\nenrich the user’s understanding of the domain itself. It is not enough for\nintelligent interfaces to offer an infinite variety of features; they must\nalso find ways of connecting the user with the possibilities they contain.\n\n### Getting Acquainted\n\nPerhaps the greatest challenge of any conversation is knowing where to begin.\nWhen you meet someone new, you cannot be sure that your personal experiences,\nprofessional background, or even the idioms and gestures you use will overlap\nwith theirs. For this reason, we often begin with simple pleasantries and then\nattempt to establish common ground by posing basic questions about the other\nperson’s interests, line of work, and family before diving into more specific\nsubjects and inquiries.\n\nSimilarly, in designing conversational user interfaces, we must establish\ncommon ground with users and introduce them to the system’s capabilities and\nmodes of interaction, since they are not explicitly detailed by a menu system.\n\nA blank page can be stifling and may lead some users to lose interest\nimmediately. For this reason, it is important to have the user get his feet\nwet as quickly as possible. Rather than simply showcasing example inputs that\ndemonstrate a few basic features, the application could instead ask the user\nto perform a simple task that employs one of these features. This gives the\nuser an initial confidence boost and helps to get the conversational juices\nflowing.\n\nAs the user gains traction through these initial exercises, the software can\ngradually become less proactive in soliciting specific actions and allow the\nuser to explore the tool on his or her own terms. Over time, the system can\ncontinue to suggest new features to the user. But designers should be careful\nnot to bombard users with too many new ideas as they become acquainted with\nthe system.\n\nOnce acclimated, the user’s own explorations and activities can help the\nsystem to understand what features would be most useful to her. Subsequently,\nif the user is idle or appears to be at an impasse, the system can suggest\ncontextually relevant new activities or features that have never been explored\nby the user. This form of assistance will be discussed further in the section\n“Designing Building Blocks”.\n\nThe user should, of course, always have the ability to opt out of these\nsuggestions or redirect them toward more relevant functionality. Before\nleaving these introductory exercises, it is also important to make sure that\nusers know how to get help from the system when they need it. Many new users\nwill be eager to dive into the software as quickly as possible, leading them\nto rush through these important on-boarding exercises. In a conversational\ninterface, an obvious and natural mechanism for seeking assistance from the\nsystem would be to use a keyword like “Help!” In some cases, however, the\nuser’s confusion may come from a lack of familiarity or comfort with the open-\nended nature of a conversational interface. For this reason, it may be\nvaluable to provide a concrete and persistent interface element such as a help\nbutton that allows the user to quickly and unambiguously reach out for\nassistance.\n\n### Seeking Clarity\n\nIn an open-ended dialogue with an intelligent interface, it is only natural\nand perhaps even desirable for the user to develop the impression that\nanything is possible within the system. In most instances, however, this will\nnot be the case. Though intelligent assistants give the appearance of broad\nconceptual versatility, their true capabilities are generally still limited to\none or a handful of specific domains of functionality. Their ability to\ncomprehend human language and map it to a particular function is also often\nsomewhat brittle.\n\nDesigners can help to mediate these technical limitations by providing\nconversational cues that lead the user away from broad or ambiguous\nstatements, which cannot be easily parsed by the machine. A key component of\nthis is to design for interactions that establish realistic expectations from\nthe user and clearly indicate the kind of information that is being requested\neither through example or brief description.\n\nIf an application prompted you to “say what’s on your mind,” it would be\ndisappointing or embarrassing to dive into a longwinded and enthusiastic\ndescription of your ideas only to have the machine respond with a terse “I’m\nsorry, I don’t understand.”\n\nHelping the user to focus on a specific task or to communicate a singular\npoint of information is often most difficult at the beginning of a new\ninteraction. The user may have several impressionistic or partially formed\ngoals, but may not have sufficient clarity to articulate a clear point of\nentry to the system. The absence of accumulated contextual information that\ncould otherwise be derived from the user’s most recent actions also makes it\nmore difficult for the machine to infer the user’s intent at the outset of an\ninteraction.\n\nRather than starting with an open-ended question, the system could instead\npose a specific yet basic one that will help to establish context and get the\nconversation going so that further details can be solicited in subsequent\nexchanges. In many cases, it may be helpful to provide users with a few\ncategorical options that will localize their goals to a specific subset of the\nsystem’s overall functionality. In the context of an e-commerce site, the\nsystem might open the conversation by asking whether the user is looking to\npurchase a specific item, browse for items in a particular section of the\nstore, or make a return. Once the broad context has been established, the\nsystem can seek further details from the user by posing questions that extend\norganically from this point of entry. If the user indicated an interest in\nbuying a specific item, the system might pose a series of questions about the\ndesired size, materials, or optional accessories associated with that item.\n\nThere is nothing more discouraging than having your ideas fall flat with no\nindication of what was confusing or how to better communicate your intended\nmeaning. Perhaps the user’s expression was completely outside the machine’s\ndomain of understanding. Or perhaps a single word choice or change of phrasing\nwould have made the statement clear. But without any feedback from the\nmachine, the user will be left in frustration to guess at the problem.\n\nWhen the user’s input has been understood, the system should restate what it\nhas understood before moving on to subsequent exchanges. If the system has not\nunderstood the user, it should try to indicate the nature of the problem to\nthe best of its ability.\n\nMany machine learning and natural language processing platforms can be\nconfigured to return multiple speculative interpretations of the user’s\nexpression alongside _confidence scores_ that numerically indicate the\nmachine’s certainty about the correctness of each interpretation it offers.\nWhenever possible, intelligent applications should try to correlate these\nspeculative interpretations with broader contextual knowledge of the user’s\nactivities in order to eliminate the least plausible options. If this approach\ndoes not uncover a single unambiguous meaning, designers should consider\npresenting a handful of the most highly ranked interpretations to the user\ndirectly. Exposing users to these speculative interpretations will help them\nto understand the nature of the problem and rephrase or redirect a statement.\n\n### Breaking Interactions into Granular Exchanges\n\nOne of the most important things designers can do to facilitate successful\nexchanges between users and intelligent interfaces is to the help users break\nmultistage workflows and their component interactions into small conceptual\nparcels. Simple expressions that communicate an individual command or point of\ninformation are more easily understood by machine learning and natural\nlanguage processing systems than complex, multifaceted statements. Designers\ncan help the user to deliver concise statements by creating interfaces and\nworkflows that lead the user through a series of simple exercises or decision\npoints that each address a single facet of a much larger and more complex\ntask.\n\nAn excellent example of this approach is 20Q, an electronic version of the\ngame Twenty Questions. Like the original road trip game, 20Q asks the user to\nthink of an object or famous person and then poses a series of multiple choice\nquestions in order to discover what the user has in mind (see Figure 1-16).\n\nThe first question posed in this process is always:\n\n“Is it classified as Animal, Vegetable, Mineral, or Concept?”\n\nSubsequent questions try to uncover further distinctions that extend from the\ninformation that has already been provided by the user. For example, if the\nanswer to the first question were “Animal,” then the next question posed might\nbe “Is it a mammal?” If instead the first answer were “Vegetable,” the next\nquestion might be “Is it usually green?” Each of these subsequent questions\ncan be answered with one of the following options: Yes, No, Unknown,\nIrrelevant, Sometimes, Maybe, Probably, Doubtful, Usually, Depends, Rarely, or\nPartly.\n\n![mlfd 01in16](assets/mlfd_01in16.png)\n\n###### Figure 1-16. A learning decision tree for the game 20Q”\n\n20Q guesses the correct person, place, or thing 80% of the time after 20\nquestions and 98% of the time after 25 questions. This system uses a kind of\nmachine learning algorithm called a _learning decision tree_ (or\n_classification tree_) to determine the sequence of questions that will lead\nto the correct answer in the smallest number of steps possible. Using the data\ngenerated by previous users’ interactions with the system, the algorithm\nlearns the relative value of each question in removing as many incorrect\noptions as possible so that it can present the most important questions to the\nuser first.\n\nFor example, if it were already known that the user had a famous person in\nmind, it would likely be more valuable for the next question to be whether the\nperson is living than whether the person has written a book, because only a\nsmall portion of all historic figures are alive today but many famous people\nhave authored a book of one kind or another.\n\nThough none of these questions individually encapsulates the entirety of what\nthe user has in mind, a relatively small number of well-chosen questions can\nuncover the correct answer with surprising speed. In addition to aiding the\nsystem’s comprehension of users’ expressions, this process can benefit the\nusers directly in their ability to communicate ideas more clearly and\npurposefully. This process can also make the task of communicating an idea\nmore playful and engaging for the user.\n\nAside from guessing games, designers may find this approach to be useful in a\nwide range of human-computer interactions. At its core, this process can be\nseen as a mechanism for discovering an optimal path through a large number of\ninterrelated decisions. From this perspective, we can imagine applying this\napproach to processes like the creation of a financial portfolio or the\ndiscovery of new music.\n\nIn developing a financial portfolio, users would be presented with a series of\nquestions that relate to their financial priorities and constraints in order\nto find a set of investments that are well-suited to their long-term goals,\nrisk tolerance, and spending habits. In the context of music or movie\ndiscovery, this process may help to overcome some of the limiting factors\nassociated with standard recommendation engines. For example, a user may give\na particular film a high rating because it stars one of his favorite actors,\neven though the film is of a genre that is generally of no interest to the\nuser. From this rating, a recommendation engine is likely to suggest similar\nfilms without giving the user the opportunity to point out that the actor\nrather than the genre was the key factor in his rating. Using an interactive\ndecision tree process, this important distinction could be more easily\nuncovered by the system.\n\n## Designing Building Blocks\n\nThe kinds of machine-learning-enhanced workflows described above can provide\ndesigners with powerful new tools for making complex tasks more\ncomprehensible, efficient, and enjoyable for the user. At the same time,\nenabling these modes of interaction may require designers to diverge from at\nleast some of the longstanding conventions of user interface and user\nexperience design.\n\nIn the section “Mechanical Induction”, we discussed inductive learning as a\nsearch process. Similarly, any task for which the user has some desired end-\nstate in mind and must discover a series of operations that will reach this\ngoal can be seen as a kind of search. From this perspective, let us imagine a\nvery large map in which each possible end-state as well as our starting point\nis represented by a unique set of coordinates.\n\nIn this map, each feature of the software can be seen as a road that takes us\na certain distance in a particular direction. To navigate this space, we must\nfind a sequence of actions or driving directions that lead from our starting\nposition to the desired destination (see Figure 1-17). A low-level feature\nwould be equivalent to a local road in the sense that it moves us only a short\ndistance within the map, whereas a high-level feature would be more like a\nhighway.\n\n![mlfd 01in17](assets/mlfd_01in17.png)\n\n###### Figure 1-17. A sequence of low-level features related to the design of\na wine glass\n\nThe nice thing about highways is that they take us great distances with a\nrelatively small number of component actions (see Figure 1-18). The problem,\nhowever, is that highways only have exit ramps at commonly visited\ndestinations. To reach more obscure destinations, the driver must take local\nroads, which requires a greater number of component actions. Ideally, a new\nhighway would be constructed for us whenever we leave home so that we could\narrive at any possible destination with a small number of actions (see Figure\n1-19). But this is not possible for pre-built high-level interfaces.\n\n![mlfd 01in18](assets/mlfd_01in18.png)\n\n###### Figure 1-18. A higher-level feature\n\n![mlfd 01in19](assets/mlfd_01in19.png)\n\n###### Figure 1-19. A potentially desirable, but non-existent feature\n\nMachine learning allows us to extrapolate a great deal of information about\nusers and what they wish to achieve through the observation of their\nbehaviors. Rather than trying to anticipate the user’s needs through a set of\nprebuilt high-level interfaces, we can instead design systems that learn from\nthe user’s engagement with the software. For example, we can discover commonly\nused sequences of low-level features and then dynamically combine these low-\nlevel features into custom, purpose-built features related to the user’s\ncurrent activity within the system.\n\nThe behavioral patterns used in the automated production of custom high-level\nfeatures can be mined from individual users or across many users. Somewhat\nlike recommendation systems that suggest music or movies based on the\nsimilarities of users’ tastes, the discovery of patterns across numerous users\ncan be employed to suggest relevant features to an individual based on the\nworkflows he or she tends to utilize. This will allow designers to better\naddress the diversity of users and their varied ways of digesting information,\nmaking decisions, and interacting with software. It allows designers to meet\nusers where they are rather than asking them to adapt to a singular\npreordained mentality or workflow offered by a more conventional, static user\ninterface. Additionally, by extrapolating behavioral patterns across many\nusers, designers can better understand the implicit relationships between the\nfeatures offered within their systems, providing important insights as to how\nthe software can be further developed to better serve the needs of their\nusers.\n\nThe high-level features generated through this process can be presented to\nusers in a variety of ways. Based on the recent trajectory of a user’s\nactions, a modal interface offering one or several possible next steps could\nbe presented to the user through a mechanism somewhat like the autocomplete\nfeature offered by some text editing applications. Alternately, a custom-\ngenerated feature could be associated with a conventional interface element\nsuch as a button. This approach, however, raises questions as to how the user\nwould be made aware of the effect that this newly generated feature would\nhave. In conventional interfaces, buttons are generally associated with a text\nlabel or icon that indicates the feature’s purpose. For custom-generated\nfeatures that bundle the functionality of several more granular features,\nproducing labels or icons of this kind may be challenging. Furthermore, it\nwould likely be difficult for the user to keep track of the large and ever-\nchanging iconography of the interface.\n\nTo circumvent this challenge, rather than indicating the effect of a given\nfeature through a label or icon, when the user hovers over the interface\nelement associated with the feature, the system could demonstrate its effect\ndirectly. For example, a user purchasing a car online might opt to change the\npaint color. In doing so, a button could be generated so that when the user\nhovered over it, the view of the car would show a preview of a set of\nsuggested additional changes, such as to the upholstery fabric, trim, and so\nforth. If the user clicked the button, these changes would be made, but if the\nuser chose not to click the button and moved the cursor away, the preview of\nthe car would remove these changes.\n\nBy adopting this methodology, the designer’s role would shift away from the\noverall curation of high-level functionality and toward the creation of more\ngranular interface elements. The overall curation would emerge from the\naggregation of individual points of information communicated by the user. This\nmovement from preset rule systems and interfaces to implicit, intelligently\ngenerated ones means that designers would be relinquishing some control of\ncertain aspects of the design. Doing so, however, would enable users to\naddress tasks that have not been explicitly anticipated by the software’s\ndesigner. This paradigmatic shift would greatly further the most important\ngoal of design: to serve the needs of users.\n\n## Acquiring Training Data\n\nA machine learning system is only as good as the data it is trained upon. No\nmatter how powerful an algorithm may be, it cannot extract meaningful patterns\nif those patterns are not represented by the data. In fact, in some cases, a\nmore powerful machine learning algorithm may produce especially erroneous\nresults from poor training data because, in absence of meaningful patterns,\nthe system may direct its power towards the learning of irrelevant patterns\nthat exist within the dataset’s noise.\n\nIn general, the quality of a given dataset relates to the following\ncharacteristics:\n\n_Completeness:_ The extent to which the data is indicative of the full range\nof behaviors that is possible within the represented system. For instance,\ntemperature data for a given city would be highly incomplete if it were only\nrecorded on rainy days.\n\n_Accuracy:_ The extent to which the data is true to the real-world behaviors\nit represents. In other words, the recorded temperature on a particular day\nshould exactly match what the actual temperature was.\n\n_Consistency:_ The extent to which various data points within the set do not\nconflict with one another. This means that there should not be more than one\nrecorded temperature in the data set for the same time period and location.\n\n_Timeliness:_ The extent to which the data is relevant to the current state of\nthe system. For example, temperature data for a given city from 1900 may not\nbe indicative of more recent patterns in the city’s weather.\n\nTo achieve these characteristics within a dataset, a great deal of human labor\nis often required. Accuracy, consistency, and timeliness require careful data\ncollection, curation and maintenance. Completeness is often achieved in part\nthrough the sheer volume of examples within the dataset. With a greater number\nof examples, it is more likely the dataset will account for the full range of\npossible behaviors. For this reason, it is not uncommon for the datasets used\nby large-scale machine learning systems to contain hundreds of thousands or\nmillions of training samples. The ImageNet dataset (see Figure 1-20), used to\ntrain Google’s image classification system, contains over 14 million images,\neach of which is accompanied by a textual label that identifies which one of\nover 22,000 categories of objects the image\ndepicts.[9](ch01.html#idm139841928328272)\n\n![mlfd 01in20](assets/mlfd_01in20.png)\n\n###### Figure 1-20. A selection of images from the ImageNet dataset\n\nFor certain machine learning problems that have received significant attention\nfrom the industry and research community, large and well-curated datasets are\nfreely available on the Web. Resources for finding these datasets can be found\nin the section “Going Further”. But for machine learning problems that have\nnot been as widely studied, the availability of usable data has been one of\nthe biggest bottlenecks in the advancement of applied machine learning\nsystems.\n\n## The Intelligence Feedback Loop\n\nIn the previous section, we discussed how interface design can aid users in\nclearly communicating ideas and information to machine-learning-enhanced\nsystems. Conversely, these same mechanisms can be employed to improve machine\nlearning systems’ ability to learn from user-generated data. Through this\nsymbiotic relationship, the aptitudes of machine learning systems and human\nusers can build upon one another, elevating the capabilities of both parties\nto new heights. One significant example of this symbiosis at work can be seen\nin the reCAPTCHA system (Figure 1-21), with which many readers will already be\nfamiliar.\n\n![mlfd 01in21](assets/mlfd_01in21.png)\n\n###### Figure 1-21. An example reCAPTCHA\n\nCAPTCHAs were developed as a mechanism for verifying that users trying to\naccess a website are human, doing so by asking them to perform tasks that\ncannot be easily deciphered by machines. Realizing that the performance of\nthese menial tasks would collectively represent a great deal of wasted human\neffort, Luis von Ahn and several other researchers at Carnegie Mellon\nUniversity designed a system called reCAPTCHA that would put this effort to\ngood use.\n\nIn its early versions, the reCAPTCHA system would present the user with the\nimage of a word that had failed to be deciphered by an optical character\nrecognition system tasked with digitizing printed texts. This image was often\npaired with the image of a “control” word that had already been deciphered. To\ngain access to the website, the user would be asked to type the two words. If\nthey correctly entered the control word, it was assumed that the user’s entry\nwas valid and they would be given access to the site. Their entry for the\npreviously undeciphered word would then be compared against entries for the\nsame image performed by several other users. If multiple users’ entries for a\nspecific image were in agreement, the entry was assumed to be correct and\nwould be inserted into the digitized version of the originating text.\n\nGoogle later acquired this technology, which played an important role in the\ndigitization of the _New York Times_ archives and the contents of several\nuniversity libraries under the Google Books project. More recently, reCAPTCHAs\nhave been used to improve the performance of Google’s image recognition system\nby presenting users with a grid of thumbnail images and asking them to select\nonly the images that depict a particular category of object.\n\nThis clever idea provides greater security to websites while simultaneously\naggregating small bits of human effort to improve the capabilities of machine\nlearning systems that will in turn benefit human users. reCAPTCHA is a\nrelatively straightforward idea, but a powerful and efficient one. The\nunderlying spirit of this idea can be adapted by designers to a wide variety\nof applications in the development of machine-learning-enhanced systems.\nDesigners can create symbiotic relationships between users and computational\nsystems that benefit both parties while streamlining the arduous task of data\ncollection and cleaning for system developers.\n\n# Dealing with Challenges\n\n## Designing for Uncertainty\n\nAn unreliable system is often worse than no system at all. In our daily lives,\nwe rely upon countless systems that aid us in tasks related to everything from\nkeeping track of our schedules to keeping our families safe. Whatever services\nthese systems may offer, they are only valuable to us if they work\nconsistently. The detrimental effects of a single failure may far outweigh the\nbenefit produced by a thousand instances of the system working correctly.\n\nDesigning a flawless system is a tall order in any context. But it is\nparticularly challenging in the context of machine learning, where a system’s\nbehaviors are defined probabilistically through the machine’s experiential\ntraining on a finite number of discrete examples. The machine has no way of\nknowing whether these training examples are indicative of every possible\ncircumstance it may encounter in real-world usage. Furthermore, if the machine\ndoes encounter unfamiliar conditions, rather than proclaiming its inability to\ndeal with the situation, it may attempt to fit the new data into its existing\nmodel and return its results to the user without any indication that something\nis amiss.\n\nIf a machine learning system has been designed to return confidence scores\nalongside of its predictions, it is likely that the system’s attempt to fit\nincongruous data into an existing model will yield a low confidence score,\ngiving designers the opportunity to catch erroneous results before they reach\nthe user. Unfortunately, though, there can be no explicit guarantee that a\nperfect storm of conditions, which obfuscates the incongruousness of a\nparticular data sample, will not arise.\n\nThough conventional computer programs can contain bugs, their explicitly\nencoded logic provides relatively strong assurances that their behaviors will\nbe predictable and repeatable. As computers have become indispensable parts of\nour daily lives, we have become increasingly accustomed to their\npredictability and reliability. As we move towards the adoption of intelligent\nsystems, it will be difficult for users to shift their expectations of\nreliability, even if their reduced reliability comes with the promise of far\nmore advanced functionality.\n\nTo mitigate the risks associated with the erroneous behaviors of machine\nlearning systems and provide users with the best possible experiences,\ndesigners should take the following steps before releasing their software to\nthe public:\n\n  1. Design for interactions in which the system explicitly restates its understanding of the tasks it has been asked to perform, giving users the chance to catch errors and redirect the system’s behavior.\n\n  2. When possible, provide fallback mechanisms through conventional user interfaces that allow users to circumvent machine-learning-enhanced functionality and perform tasks using explicit logic and interfaces.\n\n  3. Perform rigorous testing of the software in as many environments and usage scenarios as possible to uncover possible faults or inconsistencies that may arise from conditions that differ from those of the original development environment. Limited release to an audience of more knowledgeable and error-tolerant testers may help to uncover circumstantial inconsistencies before wider release to the public.\n\n  4. Use all available metrics, namely confidence scores, to assess the validity of included features. Set realistic expectations in how you present a feature and its effectiveness to the user.\n\n  5. Resist the temptation to include an impressive-sounding feature if its behavior is too unreliable. This assessment should weigh the complexity and potential value of the feature against the likelihood of its failure. If a feature offers some potentially revolutionary new capability, users may be more willing to accept that it only works 90% of the time.\n\n  6. Make users aware of any risks that might accompany their use of the software or a particular feature and allow them to decide for themselves whether these risks are outweighed by the potential benefits of the system’s functionality.\n\n  7. In cases where a system failure may have extraordinarily serious consequences such as irrevocable damage to the user’s property, injury, or death, the value of a feature’s inclusion should be weighed with extreme caution and a lawyer should be consulted to assess the risks and liabilities as well as to formulate any necessary disclaimers that will be presented to users.\n\n## Mitigating Faulty Assumptions\n\nWe all feel uncomfortable having anyone, human or otherwise, make a decisive\ncharacterization of our identity that conflicts with the way we see ourselves\nor wish to be perceived by others. As machine learning continues to reach\nfurther into the domain of personality insights and customer preferences, the\nability to connect users with products they are likely to find interesting\nwill lead to better customer experiences and more efficient markets, but also\nopen the door to many possible awkward situations.\n\nIt is one thing to say to the user, “I think you’ll enjoy this upcoming Barry\nManilow concert.” It is quite another thing to say, “Your tastes are aligned\nwith our ‘old fart’ user category, therefore I think you’ll enjoy this Barry\nManilow concert.” Rather than explicitly characterizing the user, we can leave\nthe machine learning system’s statistical correlations in the statistical\nrealm and behind the scenes. We can make suggestions about things that the\nuser might find interesting without stating what lead the machine to believe\nthe user would be interested.\n\nNevertheless, the statistical bases upon which such recommendations are made\nmay inadvertently reflect cultural biases or other faulty assumptions about\nthe user. In the last few years, machine learning systems have made incredible\nprogress in their ability to perform complex tasks like image tagging. The\ncreators of these systems have been excited to share their work with the world\nas quickly as possible, even when the software was still in an early stage of\ndevelopment. Yet, it is difficult to guard against or even foresee all\npossible failings of these nascent technologies and, as a result, several\nembarrassing or offensive incidents have occurred. In the most serious of\nthese incidents, Google’s Photos app tagged several black users as “Gorillas.”\n\nThe upset caused by this incident is more than understandable. However, it is\nimportant to remember that the machine learning system involved has no access\nto or awareness of the complex societal concepts that make this error\noffensive. The error is owed in part to an imbalance in the set of images used\nto train the system, a circumstance which Google and other technology\ncompanies should seek to rectify. But, as the machine learning expert Andrew\nNg said, “… it’s obvious this is an innocent rather than deliberate mistake,\nand just one of millions of mistakes that learning algorithms undoubtedly make\nevery day.”[10](ch01.html#idm139841928299824)\n\nRegardless of the machine’s innocence, this error revealed a flaw which\ncreated hard feelings, and we should do everything we can to prevent similar\nincidents from occurring in the future. One component of this is to strive for\never more accurate machine learning models. Another is to work at discovering\nany implicit biases that may be in the datasets we provide the machines ahead\nof time. Yet, given their experiential isolation from the human world, it is\nunlikely that machines will be able to circumvent all possible cultural\nprejudices in the foreseeable future. Therefore, we must find ways of\npreventing such errors through design. For instance, designers should consider\noffering users explicit mechanisms for flagging offensive content, providing\nguidance as to how they wish to be characterized, or opting out of particular\nfeatures that are likely to produce faulty assumptions.\n\n## Creating Sanity Checks\n\nSince machine learning models have no inherent means of detecting gaps or\nbiases in their own knowledge, designers should look to outside mechanisms in\nsafeguarding against potentially offensive or erroneous behaviors. This may\nprove difficult in many instances, because one of the primary motivations for\nusing machine learning in the first place is the capacity of these systems to\nwork with information that is too complex or subtle for the explicit logic of\nconventional computer programs. However, conventional code can provide at\nleast some assistance in spotting obvious issues like the evocation of overtly\nderogatory language.\n\nAround the time of this writing, Microsoft released a chatbot called Tay,\nwhich was designed to respond to users on Twitter and other messaging\nservices. Within a day, the bot was taken offline after posting an astounding\nnumber of inflammatory messages. Perhaps the most surprising thing about this\nincident was that the messages in question did not contain subtly offensive\ninnuendo or easily misconstrued associations; they instead contained overtly\nderogatory language and sentiments, using widely known racist and sexist\nterminology as well as references to Adolph Hitler and the like.\n\nAs it turns out, the system was trained on user-generated content and was\ntargeted by people who hoped to produce this outcome by inducing the system to\nlearn from their own hateful statements. Yet, this outcome could have been at\nleast partially avoided if the system’s designers had used code to check the\nbot’s messages against a dictionary of offensive words before posting to\nTwitter. This approach can turn into something of a cat-and-mouse game,\nbecause users wishing to subvert such safeguards often resort to inventive\nspellings of words or tactics like replacing the letter “e:” with the numeral\n“3” so that their use of flagged terminology will go undetected. It is worth\nnoting, however, that many of the offensive terms used in Tay’s messages were\nspelled correctly and could have been easily caught by even most basic of\nsafeguarding mechanisms.\n\nThough somewhat counterintuitive, another approach that might have been taken\nin this scenario would have been to use a secondary machine learning system\nthat had been specifically trained for the detection of offensive content.\nSystems of this kind are used for spam filtering in services like email and\nuser forums. They are also used by marketers in the sentiment analysis tools\nthey employ to better understand comments about their products posted on\nsocial networks.\n\nThough neither of these approaches can completely solve the problem, they can\nat least guard against the most blatant abuses. Aside from these mechanisms,\ndesigners should rely upon the collective intelligence of their user base by\nproviding explicit interface elements that allow users to flag offensive or\nerroneous content for review by system administrators and subsequent\nintegration into the machine learning model. This approach, more than any\nautomated one, will help designers to stay ahead of malicious users and flawed\ndatasets.\n\n# Working with Machine Learning Platforms\n\nIn theory, machine learning can be applied to any kind of information that\ncontains patterns, so long as those patterns can be sufficiently exemplified\nby a set of training data. In practice, however, some forms of information can\nbe more readily accessed and applied to real-world design problems than\nothers. For more common machine learning tasks like image tagging and speech-\nto-text functionality, designers may utilize turn key solutions offered by a\nvariety of _Machine-Learning-as-a-Service_ (MLaaS) platforms, which enable\nstraightforward integration with user-facing systems through RESTful APIs and\ndesign patterns. In many cases, these MLaaS platforms will also enable the\nrelatively straightforward deployment of machine learning systems trained on\ncustom datasets provided by the designer. For more exotic or domain-specific\nuse cases, designers may look to more customizable open source machine\nlearning toolkits or even fully customized software, which tend to require a\ndeeper technical understanding of the underlying algorithms as well as of the\ntechnical issues related to the deployment of such technologies within large-\nscale user-facing systems.\n\n## Machine-Learning-as-a-Service Platforms\n\nSeveral large technology companies and startups offer high-level machine\nlearning platforms, which provide designers with straightforward access to\nturnkey solutions or customized training on designer-provided data. The list\nof MLaaS platforms is growing quickly. Some of the most popular platforms\ninclude: IBM Watson, Amazon Machine Learning, Google Prediction API, Microsoft\nAzure, BigML, and ClarifAI.\n\nDespite the many advantages of these systems, there are several important\ndownsides that may factor into a designer’s decision of whether to use a\nplatform of this kind in building their system. First, the use of these\nsystems comes with recurring costs that will grow with a wider user base.\nThough the cost of an individual query is generally quite low and bulk rates\nare available, for a sufficiently large user base, these costs can become\nprohibitive without a viable revenue model to support the product.\nAdditionally, these platforms generally do not provide a straightforward path\nfor moving a system developed within one MLaaS platform to a competing\nplatform. This platform lock-in may contribute to the long-term cost of\nownership and may constrain future innovation within a designer’s system.\nFinally, systems built on top of MLaaS platforms tend to require the user’s\ndevice to have internet connectivity in order to query the remotely hosted\nmodel. This may be limiting in some applications and may incur data usage\ncosts for the user. However, the models associated with many complex machine\nlearning may be too large or computationally intensive to run on user devices,\nmaking cloud-based deployment the only viable route regardless of whether a\nMLaaS or custom machine learning solution has been used.\n\n### Turnkey Solutions\n\nIf the machine-learning-based functionality you wish to offer within your\ndesign is specifically supported by a turnkey feature of one of these\nplatforms, this approach will offer the quickest and easiest path to a\ndeployable user-facing product. Though the specific features offered will\ndiffer by platform, many MLaaS platforms offer turnkey solutions for tasks\nincluding natural language parsing, language translation, speech-to-text,\npersonality insights, sentiment analysis, image classification and tagging,\nface detection, and optical character recognition.\n\nThe creators of these systems have put a great deal of work into the\ndevelopment and testing of their underlying algorithms as well as the\ngathering of large and well-cleaned datasets that will ensure robust\nfunctionality. These turnkey features can be utilized without any further\nknowledge of the underlying algorithms or datasets using straightforward API\ncalls in an assortment of languages.\n\nFor example, an image classification query can be performed using the Node\ninterface of the IBM Watson platform with the code used in Figure 1-22.\n\n![mlfd 01in22](assets/mlfd_01in22.png)\n\n###### Figure 1-22. A Watson image classification query in Node\n\nWatson would subsequently return the JSON response in Figure 1-23, which can\nbe easily parsed by the client system and integrated into the user-facing\ndesign:\n\n![mlfd 01in23](assets/mlfd_01in23.png)\n\n###### Figure 1-23. An easily parsed JSON response to a Watson image\nclassification query\n\n### Custom-Trained Systems\n\nAside from the areas of turn key functionality listed above, MLaaS platforms\ncan be used in a wide range of machine learning problems that require custom,\ndesigner-supplied datasets. In such cases, designers may circumvent the\narduous process of building, testing and deploying the machine learning system\nitself. They will, however, still need to devote time and resources to\nensuring that the datasets they provide to these systems are clean and well-\ncurated, though many MLaaS platforms offer substantial assistance in\nstreamlining these processes as well.\n\nThough the specific functionality offered will differ by platform, many MLaaS\nplatforms provide functionality related to user behavior prediction, customer\nanalytics and insights, inventory trends, recommendation engines, content\npersonalization, fraud and anomaly detection, and any other supervised\nlearning problem.\n\nThe training process for these systems generally involves the designer\nuploading a spreadsheet or formatted data to the platform, waiting for the\nmodel to be trained in the cloud, and then testing its behavior before\ndeploying the functionality within a user-facing design. For most supervised\nlearning problems, the data spreadsheet supplied to the MLaaS platform would\ncontain at least two columns: one or more columns to represent the input\nattributes for a particular example and another column to represent the\ndesired output associated with that input. As with any training process, a\nlarger number of examples (or spreadsheet rows) is likely to result in a more\nrobust model. Once trained, queries to the model are performed in a similar\nmanner to the example shown above for turn-key features.\n\n## Open Source Machine Learning Toolkits\n\nFor some machine learning problems and user-facing platforms, particularly\nnative applications designed for offline usage, it may be necessary to deploy\ntechnologies outside of the MLaaS platforms described above. In such\ninstances, designers may avoid at least some aspects of the lengthy\ndevelopment processes associated with fully customized solutions by building\non top of one of a growing list of open source machine learning toolkits,\nwhich are available for a variety of programming languages and platforms. Some\npopular toolkits of this kind include: TensorFlow, Torch, Caffe, cuDNN,\nTheano, Scikit-learn, Shogun, Spark MLlib, and Deeplearning4j.\n\nThese lower-level toolkits generally require more programming experience than\nis necessary for the use of MLaaS platforms. Additionally, deeper knowledge of\nspecific machine learning algorithms and their associated training techniques\nwill likely be required for their effective use. While the MLaaS platforms\nlisted above tend to include automated training processes, these toolkits will\nrequire designers to tune algorithmic hyperparameters such as the learning\nrate to align with the specific characteristics of their chosen dataset. This\ntuning process is aided by a deeper knowledge of mathematics and often\ninvolves at least some time-intensive trial-and-error to find suitable values.\n\nTraining a machine learning system tends to be a highly computationally\nintensive process, and for large or complex datasets, it is often impractical\nor even impossible to perform this process on a single consumer-grade machine.\nMany of the toolkits mentioned above are designed for use on large-scale\nhardware systems that use numerous CPUs or high-performance GPUs to perform\ntraining. This hardware can be cost-prohibitive and may require specialized\nknowledge related to the performance-enhancement mechanisms employed by a\nspecific toolkit.\n\nOnce trained, the system will still need to be deployed to the desired user-\nfacing platform. In most instances for which these toolkits are applicable,\nthe user-facing platform will have little in common with the large-scale\nsystem utilized during the training process. This means that designers must\nnavigate two separate sets of technical and infrastructural challenges in\ndeveloping their systems and making them available to users.\n\nDespite these challenges, these toolkits offer a viable pathway for designers\nwishing to add customized machine learning functionality to user-facing\nsystems. Many of these tools are backed by large technology companies, who\nhave a vested interest in their wider adoption and are working to\nprogressively make their tools more accessible to a wider audience of\ndesigners and developers.\n\nOne additional selling point of these toolkits over MLaaS platforms is that\nthe tools themselves are free to use and deploy, though training them on a\nlarge-scale platform may require designers to either acquire their own costly\nhardware or pay for the use of a cloud-based system.\n\nLike MLaaS platforms, the trained models developed using a toolkit of this\nkind may not be easily transferred for use with a different toolkit. However,\nin most cases, these customizable toolkits provide a more straightforward path\nfor doing so than would be possible for a system backed by one of the MLaaS\nplatforms.\n\n## Fully Customized Machine Learning Tools\n\nThe open source toolkits listed above strive to provide thoroughly tested\nimplementations of proven machine learning algorithms and techniques. As a\nresult, these tools may not include more experimental algorithms coming from\nrecent research that has not yet been thoroughly reviewed and field tested. In\nsome instances, these advances may provide incremental improvements to\nexisting algorithms and in other cases may offer revolutionary new\nfunctionality. Integrating these technologies into user-facing systems will\nalmost always require custom implementation work involving the translation of\nalgorithms from their mathematical notation in formal research papers to\nworking code. Rigorous testing as well as additional implementation work\nrelated to the performance and scaling of the system would also likely be\nrequired. This work generally requires a large team of developers with\nadvanced knowledge of theoretical machine learning as well as deployment\ntechnologies. For these reasons, this approach is not advisable in most\ninstances. Designers who are interested in working with experimental machine\nlearning technologies should consider joining larger teams, which are better\nsuited to the multifaceted task of making these emerging technologies ready\nfor production.\n\n## Machine Learning Prototyping Tools\n\nPrototyping is an essential component of many design processes. It helps\ndesigners to sketch the basic functioning of their systems, test assumptions\nabout how users will interact with planned features, and fine-tune those\nfeatures before turning to more costly and time-intensive implementation\nprocesses. Unfortunately, the complex architectures and computationally\nintensive training processes associated with machine learning present\nchallenges for the rapid prototyping of machine-learning-enhanced systems. In\ncases where a turnkey MLaaS solution is applicable, designers may be able to\nprototype their ideas with relative ease. But in instances where the desired\nfunctionality requires a custom dataset or code, producing even a basic\nprototype can take substantial time, effort, and knowhow. Presently, there are\na limited number of tools available for assisting the prototyping of machine\nlearning systems—a circumstance that will hopefully change over the next few\nyears.\n\nIn the meantime, several existing tools may help to ease the prototyping\nprocess. From the programming-free Wekinator to the Mathematica interface and\nthe more programming-intensive Keras, the series of tools presented below can\nserve as stepping stones for students and designers wishing to prototype\nsolutions to real-world design problems while becoming acquainted with machine\nlearning systems and workflows in a hands-on way.\n\n### Wekinator\n\nWekinator is a free, open source tool created by Rebecca Fiebrink. It allows\nusers to develop experimental gesture recognition systems and interface\ncontrollers for a wide range of input devices including webcams, microphones,\ngame controllers, Kinect, Leap Motion, physical actuators (through an\nArduino), keyboards, and mice. Unlike many machine learning workflows,\nWekinator requires no programming or wrangling of datasets. Instead, the\nsoftware walks the designer through an interactive process in which she\ndefines a particular gesture by demonstrating it to the machine and then\nassociates the gesture with a desired output action. To aid prototyping and\nintegration with other software, a trained Wekinator model can be set to\nbroadcast its output events using the popular OSC protocol. Though Wekinator\nis only geared toward a specific domain of machine learning functionality, it\nprovides a powerful medium for prototyping and designing machine-learning-\nenhanced multimodal user interfaces. See Figure 1-24.\n\n![mlfd 01in24](assets/mlfd_01in24.png)\n\n###### Figure 1-24. A workflow for designing event triggers with Wekinator\n\nWekinator can be downloaded from:\n[_http://www.wekinator.org_](http://www.wekinator.org).\n\n### Mathematica\n\nThe popular technical computing platform Mathematica has added a wide range of\nautomated machine learning features to its most recent version, Mathematica\n10. This tool features a polished user interface and does not require a deep\nunderstanding of programming, though some basic familiarity with text-based\nscripting will be helpful to new users. Its machine learning features can be\napplied to a wide range of data types and its automatic data preprocessing and\nmodel selection features will help users to get good results without a great\ndeal of trial-and-error or deep knowledge of a particular model’s training\nparameters. Mathematica provides turnkey support for a range of common machine\nlearning tasks such as image recognition, text classification, and\nclassification or regression of generic data. Datasets can be loaded through\nan interactive, visual interface. Mathematica is extremely well documented and\nembeds assistive tools like feature suggestion and autocompletion directly\ninto its interface. See Figure 1-25.\n\n![mlfd 01in25](assets/mlfd_01in25.png)\n\n###### Figure 1-25. Classifying handwritten digits in Mathematica\n\nMathematical can be purchased from\n[_https://www.wolfram.com/mathematica_](https://www.wolfram.com/mathematica).\n\n### Keras\n\nThis tool requires a deeper knowledge of programming as well as familiarity\nwith command-line-based installation processes, but provides a relatively\nuser-friendly wrapper for the high-performance machine learning toolkits\nTensorFlow and Theano. Though it requires programming, Keras is geared towards\nthe rapid prototyping of highly customized machine learning systems. It\nprovides a high-level API and modular components that will help users to\nassemble common machine learning architectures such as convolutional and\nrecurrent neural networks. This tool is not intended for new users, but may\nhelp to bridge the gap between the tools listed above and more open-ended\nplatforms such as those listed in the following section, “Open Source Machine\nLearning Toolkits”.\n\nKeras installation instructions can be found at\n[_http://keras.io_](http://keras.io).\n\n## Incorporating Machine Learning into Design Processes\n\nIn thinking about how to teach a person a complex task, it can be difficult to\nbreak the task down into a series of well-defined, discrete steps. The same\nproblem can arise when designing machine-learning-enhanced systems. We might\nthink, “the intelligent assistant should read the user’s emotional state and\nrespond accordingly.” But what does this mean? Though the answer may seem\nrelatively clear in human terms, it is less so in computational ones. Is the\nuser’s emotional state defined by his word choice? If so, how are we deciding\nwhat words are correlated with which emotions?\n\nThe first step in bringing machine learning to a design project should always\nbe to try to define the learning problem as clearly and fully as possible.\nWhat are the input parameters we will provide the machine? What kinds of\noutputs are we looking for? What kind of training data will exemplify the\ncorrelations between these inputs and outputs?\n\nOnce you feel confident that the learning problem has been well-defined, it\nmay be helpful to set aside the technical details for a moment and treat the\nmachine learning component as a black box function within the overall\narchitecture of your system. This means that, much like a mathematical\noperation such as a square root, you know what the input and corresponding\noutput should be, but you do not necessarily know how the square root is\nactually computed within this function. In this way, as you sketch out the\nuser and data flows of your system, a machine learning component can be\ntreated like any other feature of the software. An image recognition\ncomponent, for instance, could be thought of as a box that takes an image as\ninput and outputs a list of words. This approach allows designers to\nincorporate machine learning features into their systems without getting\nbogged down in technical details during the important ideation and sketching\nstages, which often require fluid thinking.\n\nUnlike a simple mathematical function, however, it is much more difficult to\nbe certain that a machine learning feature will do what you need it to or that\nyou will be able to find the appropriate training data to achieve the desired\nbehavior. Therefore, this sketching process should be treated with care. If a\nmachine-learning-enhanced feature is critical to the software’s overall\nbehavior and its efficacy is in doubt, it will be important to prototype the\nblack box functionality to test assumptions before getting too far into\ndesigning other features around this machine learning functionality. The\nprototyping tools listed in the previous section may provide some assistance\nin getting a sense of whether the functionality will be achievable.\n\nThis process of prototyping and validating assumptions can be quite labor\nintensive. It may require the procurement and cleaning of a dataset, the\nselection of a machine learning model and a lengthy training process to see\nany preliminary results whatsoever. Over time, however, as you work more with\nmachine learning systems, you will develop an intuition for what is likely to\nwork and what kinds of learning problems may be more touchy or brittle. It is\nimportant to jump in and get your hands dirty—getting as much first-hand\nexperience as possible is crucial. Collaboration is also of great importance.\nIf you are working with machine learning engineers, try to form your own\nopinion of whether a particular idea will work and then ask for the engineer’s\nopinion. If her opinion conflicts with yours, ask questions. Which of your\nassumptions were faulty? What factors did you not consider? Machine learning\nmay be a rigorous science, but it is still something for which you can build\nan intuition.\n\nAs you work toward this intuition, start from simpler mechanisms and build\ntowards more complex ones. It will not be easy to intuit how a Jeopardy-\nplaying AI might be constructed, for example. In truth, IBM’s Watson is not\ncomprised of one machine learning system—rather, it is many interconnected\ncomponents. As you introduce machine learning features into your designs,\nthink about them as individual components. If you cannot reason clearly about\nwhat a particular component should do and what data it should be trained on,\nthen most likely the machine won’t be able to figure this out either.\n\nLearning is an abstract phenomenon, but its role within an individual\ncomponent of a design need not be abstract. In any design process, it’s\nnecessary to think back and forth between the high-level purpose of a feature\nand its specific technical constraints in order to balance the many\ninterrelated properties of a complex system. For machine-learning-enhanced\nfeatures, finding this balance can be difficult. But designers can meet this\nchallenge if they are willing to experiment, question their own thinking, and\nin so doing, continually strengthen their intuition for the essence of machine\nlearning.\n\n# Conclusions\n\nIn many ways, machine learning is a solution in search of a problem. Machine\nlearning algorithms are capable of discovering complex patterns in the data\npresented to them, but they are only useful if they have been trained to\nnotice something useful. For some fields, such as finance and medicine, there\nare clear connections between the field’s existing needs and the capabilities\nof machine learning systems. Financial institutions have always had a need for\ntools that help to predict the future behaviors of markets based on their past\nperformance. Medical institutions have always had a need for tools that can\npredict patient outcomes. Machine learning simply provides more effective\nmechanisms for achieving these goals.\n\nIn the coming years, countless other fields will be transformed by machine\nlearning. In many cases, however, this transformation will not be about\nconnecting existing goals with new mechanisms for achieving them. It will\nrequire the discovery of new premises and mindsets—ones that expose entirely\nnew opportunities and goals that can only be seen through the perspective of\nmachine learning.\n\nIn his book _Operating Manual for Spaceship Earth_ , the visionary designer\nBuckminster Fuller wrote, “If you are in a shipwreck and all the boats are\ngone, a piano top buoyant enough to keep you afloat that comes along makes a\nfortuitous life preserver. But this is not to say that the best way to design\na life preserver is in the form of a piano top. I think that we are clinging\nto a great many piano tops in accepting yesterday’s fortuitous contrivings as\nconstituting the only means for solving a given\nproblem.”[11](ch01.html#idm139841928205184)\n\nIn looking at the history of digital design tools themselves, we may see\ncountless piano tops. Many of the features offered by video editing software,\nfor instance, reference the preceding vocabulary of flatbed film editors.\nThough these references were helpful in transitioning a generation of\nfilmmakers to a digital workflow, they did little to uncover new possibilities\nwithin the emerging medium of video. Discovering the unique possibilities of a\nmedium requires experimentation, a fresh pair of eyes, and a willingness to\nthink outside of the existing paradigms. It is here that designers will prove\nessential to the future of machine learning.\n\nIn order to fully capitalize on the technical possibilities of machine\nlearning systems, designers will be somewhat reliant upon programmers. But\nprogrammers must also rely upon designers to find groundbreaking applications\nand ways of thinking about these general-purpose tools. To facilitate\ncollaboration with programmers and develop novel applications, designers do\nnot necessarily need to understand all of the mathematical details associated\nwith machine learning techniques. Still, to think freely and inventively about\nthe possibilities of a medium, it is important to understand its underlying\nproperties and constraints. As Bob Dylan said, “to live outside the law, you\nhave to be honest.” In other words, you have to understand the rules to know\nwhich are worth bending or breaking.\n\nTo that end, for the field of machine learning to expand and thrive into the\nfuture, it will be essential for designers to immerse themselves in the\npossibilities of this technology, transforming it through their ways of seeing\nand thinking about the world.\n\n# Going Further\n\n## Staying Up-to-date with Advancements in the Field\n\n### arXiv\n\narXiv (pronounced “archive”) is a repository of prepress scientific papers.\nMany cutting-edge advancements in the field of machine learning are posted to\narXiv first. Keeping an eye on the latest papers posted to arXiv is one of the\nbest ways to keep up with the latest advancements. But, with thousands of\npapers in a wide range of field posted to the site each month, finding papers\nrelevant to your specific interests is not always easy.\n\narXiv Machine Learning:\n_[_http://arxiv.org/list/stat.ML/recent_](http://arxiv.org/list/stat.ML/recent)_\n\narXiv Neural and Evolutionary Computing:\n_[_http://arxiv.org/list/cs.NE/recent_](http://arxiv.org/list/cs.NE/recent)_\n\narXiv Artificial Intelligence:\n_[_http://arxiv.org/list/cs.AI/recent_](http://arxiv.org/list/cs.AI/recent)_\n\n### CreativeAI\n\nFinding relevant papers on arXiv can be challenging. The site CreativeAI\ncurates a collection of machine learning projects that are directly relevant\nto design and the arts. The projects featured on this site include written\npapers, videos, and even code samples. CreativeAI highlights some of the many\ninspirational possibilities for incorporating machine learning into creative\napplications.\n\nCreativeAI: _[_http://www.creativeai.net_](http://www.creativeai.net)_\n\n### Reddit\n\nAnother great way to keep track of important advancements that have been\nposted to arXiv and other sources is to keep an eye on the conversations\nhappening within the Machine Learning and Artificial Intelligence sections of\nthe Reddit discussion forum. Readers will find links to recently published\narticles as well as a wide range of discussions on topics that will be of\ninterest to any machine learning researcher or designer.\n\nReddit Machine Learning:\n[_https://www.reddit.com/r/machinelearning_](https://www.reddit.com/r/machinelearning)\n\nReddit Artificial Intelligence:\n[_https://www.reddit.com/r/artificial_](https://www.reddit.com/r/artificial)\n\n### Deep Learning News & Hacker News\n\nThe recently established Deep Learning News site offers topical discussions on\nmachine learning in a similar vein to the Reddit forums discussed above. The\nmore general purpose Hacker News discussion forum also provides many relevant\nconversations about state-of-the-art machine learning technologies.\n\nDeep Learning News: [_http://news.startup.ml_](http://news.startup.ml)\n\nHacker News: [_https://news.ycombinator.com_](https://news.ycombinator.com)\n\n## Resources for Further Study of Machine Learning\n\n### Online Courses\n\n“Machine Learning for Musicians and Artists” taught by Rebecca Fiebrink:\n\n[_https://www.kadenze.com/courses/machine-learning-for-musicians-and-\nartists/info_](https://www.kadenze.com/courses/machine-learning-for-musicians-\nand-artists/info)\n\n“Machine Learning” taught by Andrew Ng:\n\n[_https://www.coursera.org/learn/machine-\nlearning_](https://www.coursera.org/learn/machine-learning)\n\n“Neural Networks for Machine Learning” taught by Geoffrey Hinton:\n\n[_https://www.coursera.org/course/neuralnets_](https://www.coursera.org/course/neuralnets)\n\n### Math for Machine Learning\n\n“Some Basic Mathematics for Machine Learning” by Iain Murray and Angela J. Yu:\n\n_[_http://www.cogsci.ucsd.edu/~ajyu/Teaching/Cogs118A_wi10/Refs/basic_math.pdf_](http://www.cogsci.ucsd.edu/~ajyu/Teaching/Cogs118A_wi10/Refs/basic_math.pdf)_\n\n“Math for Machine Learning” by Hal Daumé III:\n\n_[_http://www.umiacs.umd.edu/~hal/courses/2013S_ML/math4ml.pdf_](http://www.umiacs.umd.edu/~hal/courses/2013S_ML/math4ml.pdf)_\n\n“Machine Learning Math Essentials Part I & II” by Jeff Howbert:\n\n_[_http://courses.washington.edu/css490/2012.Winter/lecture_slides/02_math_essentials.pdf_](http://courses.washington.edu/css490/2012.Winter/lecture_slides/02_math_essentials.pdf)_\n\n_[_http://courses.washington.edu/css490/2012.Winter/lecture_slides/06a_math_essentials_2.pdf_](http://courses.washington.edu/css490/2012.Winter/lecture_slides/06a_math_essentials_2.pdf)_\n\n“Immersive Linear Algebra” by J. Ström, K. Åström, and T. Akenine-Möller:\n\n_[_http://immersivemath.com/ila/index.html_](http://immersivemath.com/ila/index.html)_\n\n“Linear Algebra” by Khan Academy:\n\n_[_https://www.khanacademy.org/math/linear-\nalgebra_](https://www.khanacademy.org/math/linear-algebra)_\n\n“Probability and Statistics” by Khan Academy:\n\n_[_https://www.khanacademy.org/math/probability_](https://www.khanacademy.org/math/probability)_\n\n“Differential Calculus” by Khan Academy:\n\n_[_https://www.khanacademy.org/math/differential-\ncalculus_](https://www.khanacademy.org/math/differential-calculus)_\n\n### Tutorials\n\n“Deep Learning Tutorials”:\n\n_[_http://deeplearning.net/reading-\nlist/tutorials_](http://deeplearning.net/reading-list/tutorials)_\n\n“A Deep Learning Tutorial: From Perceptrons to Deep Networks”:\n\n_[_https://www.toptal.com/machine-learning/an-introduction-to-deep-learning-\nfrom-perceptrons-to-deep-networks_](https://www.toptal.com/machine-\nlearning/an-introduction-to-deep-learning-from-perceptrons-to-deep-networks)_\n\n“Deep Learning From the Bottom Up”:\n\n_[_https://www.metacademy.org/roadmaps/rgrosse/deep_learning_](https://www.metacademy.org/roadmaps/rgrosse/deep_learning)_\n\n## Technical Resources\n\n### Machine-Learning-as-a-Service Platforms\n\nIBM Watson:\n_[_http://www.ibm.com/smarterplanet/us/en/ibmwatson_](http://www.ibm.com/smarterplanet/us/en/ibmwatson)_\n\nAmazon Machine Learning: _[_https://aws.amazon.com/machine-\nlearning_](https://aws.amazon.com/machine-learning)_\n\nGoogle Prediction API:\n_[_https://cloud.google.com/prediction_](https://cloud.google.com/prediction)_\n\nMicrosoft Azure: _[_https://azure.microsoft.com/en-us/services/machine-\nlearning_](https://azure.microsoft.com/en-us/services/machine-learning)_\n\nBigML: _[_https://bigml.com_](https://bigml.com)_\n\nClarifAI: _[_https://www.clarifai.com/_](https://www.clarifai.com/)_\n\n### Open Source Machine Learning Toolkits\n\nTensorFlow (C++, Python):\n[_https://www.tensorflow.org_](https://www.tensorflow.org)\n\nTorch (C, Lua): [_http://torch.ch_](http://torch.ch)\n\nCaffe (C++):\n[_http://caffe.berkeleyvision.org_](http://caffe.berkeleyvision.org)\n\ncuDNN (C++, CUDA):\n[_https://developer.nvidia.com/cudnn_](https://developer.nvidia.com/cudnn)\n\nTheano (Python):\n[_http://deeplearning.net/software/theano_](http://deeplearning.net/software/theano)\n\nScikit-learn (Python): [_http://scikit-learn.org_](http://scikit-learn.org)\n\nShogun (C++, Python, Java, Lua, others): [_http://www.shogun-\ntoolbox.org_](http://www.shogun-toolbox.org)\n\nSpark MLlib (Python, Java, Scala):\n[_http://spark.apache.org/mllib_](http://spark.apache.org/mllib)\n\nDeeplearning4j (Java, Scala):\n[_http://deeplearning4j.org_](http://deeplearning4j.org)\n\n### Datasets\n\nUCI Machine Learning Repository:\n[_http://archive.ics.uci.edu/ml_](http://archive.ics.uci.edu/ml)\n\nMNIST Database of Handwritten Digits:\n[_http://yann.lecun.com/exdb/mnist_](http://yann.lecun.com/exdb/mnist)\n\nCIFAR Labeled Image Datasets:\n[_http://www.cs.toronto.edu/~kriz/cifar.html_](http://www.cs.toronto.edu/~kriz/cifar.html)\n\nImageNet Image Database: [_http://www.image-net.org_](http://www.image-\nnet.org)\n\nMicrosoft Common Objects in Context:\n[_http://mscoco.org/home_](http://mscoco.org/home)\n\n[1](ch01.html#idm139841928658480-marker) Patricia F. Carini, _On Value in\nEducation_ (New York, NY: Workshop Center, 1987).\n\n[2](ch01.html#idm139841928626160-marker) Zoltan P. Dienes and E. W. Golding,\n_Learning Logic, Logical Games_ (Harlow [England] ESA, 1966).\n\n[3](ch01.html#idm139841928589216-marker) Frank Rosenblatt, “The perceptron: a\nprobabilistic model for information storage and organization in the brain,”\nPsychological Review 65, no. 6 (1958): 386.\n\n[4](ch01.html#idm139841928582448-marker) David E. Rumelhart, Geoffrey E.\nHinton, and Ronald J. Williams, “Learning representations by back-propagating\nerrors,” Cognitive Modeling 5, no. 3 (1988): 1.\n\n[5](ch01.html#idm139841928567904-marker) Turing, A. M. “Computing Machinery\nand Intelligence.” _Mind_ 59.236 (1950): 433-60.\n\n[6](ch01.html#idm139841928546192-marker) Alan Mathison Turing, “Intelligent\nMachinery,” in _Mechanical Intelligence_ , ed. D. C. Ince (Amsterdam: North-\nHolland, 1992), 114.\n\n[7](ch01.html#idm139841928469248-marker) Negroponte, Nicholas. _The\nArchitecture Machine_. Cambridge, MA: M.I.T., 1970. 11. Print.\n\n[8](ch01.html#idm139841928426640-marker)\n[_http://www.nytimes.com/2012/02/19/magazine/shopping-\nhabits.html_](http://www.nytimes.com/2012/02/19/magazine/shopping-habits.html)\n\n[9](ch01.html#idm139841928328272-marker)\n[_http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf_](http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf)\n\n[10](ch01.html#idm139841928299824-marker)\n[_https://plus.google.com/113710395888978478005/posts/dZ7pd4zdaiJ_](https://plus.google.com/113710395888978478005/posts/dZ7pd4zdaiJ)\n\n[11](ch01.html#idm139841928205184-marker) R. Buckminster Fuller, Operating\nManual for Spaceship Earth (Carbondale, IL: Southern Illinois University\nPress, 1969).\n\n\n# About the Author\n\nPatrick Hebron is a Scientist-in-Residence and Adjunct Graduate Professor at\nNYU’s Interactive Telecommunications Program. His research relates to the\ndevelopment of machine-learning-enhanced digital design tools. He is the\ncreator of Foil, a next-generation design and programming environment that\naims to extend the creative reach of its user through the assistive capacities\nof machine learning. Patrick has worked as a software developer and design\nconsultant for numerous corporate and cultural institution clients including\nGoogle, Oracle, Guggenheim/BMW Labs and the Edward M. Kennedy Institute.\n\n## Acknowledgements\n\nFor Rue and our little learning machine, Lucian, whose loving support and\nbrilliant guidance made this project possible.\n\n",
    "book_id": "machine_learning_for_designers",
    "book_title": "Machine Learning for Designers",
    "book_author": "Patrick Hebron",
    "topic_id": "ai_theory",
    "topic_label": "theory",
    "chunk_index": 2
  },
  {
    "chunk_full": "# Making Kin with the Machines\n\n  1. [Making Kin with the Machines](text/ch001.xhtml#making-kin-with-the-machines)\n     1. [Indigenous Epistemologies](text/ch001.xhtml#indigenous-epistemologies)\n     2. [Hāloa : the long breath](text/ch001.xhtml#hloa--the-long-breath)\n     3. [Aloha as moral discipline](text/ch001.xhtml#aloha-as-moral-discipline)\n     4. [ _wahkohtawin_ : __kinship within and beyond the immediate family, the state of being related to others](text/ch001.xhtml#wahkohtawin-kinship-within-and-beyond-the-immediate-family-the-state-of-being-related-to-others)\n     5. [ _wakȟáŋ_ : that which cannot be understood](text/ch001.xhtml#wak-that-which-cannot-be-understood)\n     6. [Resisting Reduction: An Indigenous Path Forward](text/ch001.xhtml#resisting-reduction-an-indigenous-path-forward)\n     7. [Footnotes](text/ch001.xhtml#footnotes)\n\n\n# Making Kin with the Machines\n\nJason Edward Lewis\n\nNoelani Arista\n\nArcher Pechawis\n\nSuzanne Kite\n\nJul 16, 2018\n\n\n# Making Kin with the Machines\n\n**Notice:** This file is an auto-generated download and, as such, might\ninclude minor display or rendering errors. For the version of record, please\nvisit the HTML version or download the PDF.\n\n* * *\n\n**DOI:** 10.21428/bfafd97b\n\n**License:** [Creative Commons Attribution 4.0 International License (CC-BY\n4.0)](https://creativecommons.org/licenses/by/4.0/)\n\nMan is neither height nor centre of creation. This belief is core to many\nIndigenous epistemologies. It underpins ways of knowing and speaking that\nacknowledge kinship networks that extend to animals and plants, wind and\nrocks, mountains and oceans. Indigenous communities worldwide have retained\nthe languages and protocols that enable us to engage in dialogue with our non-\nhuman kin, creating mutually intelligible discourses across differences in\nmaterial, vibrancy, and genealogy.\n\nBlackfoot philosopher Leroy Little Bear observes, “the human brain is a\nstation on the radio dial; parked in one spot, it is deaf to all the other\nstations [. . .] the animals, rocks, trees, simultaneously broadcasting across\nthe whole spectrum of sentience.”1 As we manufacture more machines with\nincreasing levels of sentient-like behaviour, we must consider how such\nentities fit within the kin-network, and in doing so, address the stubborn\nEnlightenment conceit at the heart of Joi Ito’s “Resisting Reduction”\nmanifesto: that we should prioritize human flourishing.2\n\nIn his manifesto, Ito reiterates what Indigenous people have been saying for\nmillennia: “Ultimately everything interconnects.”3 And he highlights Norbert\nWiener’s warnings about treating human beings as tools. Yet as much as he\nstrives to escape the box drawn by Western rationalist traditions, his attempt\nat radical critique is handicapped by the continued centering of the human.\nThis anthropocentrism permeates the manifesto but is perhaps most clear when\nhe writes approvingly of the IEEE developing “design guidelines for the\ndevelopment of artificial intelligence around _human_ well-being” (emphasis\nours.)4\n\nIt is such references that suggest to us that Ito’s proposal for “extended\nintelligence” is doggedly narrow. We propose rather an extended “circle of\nrelationships” that includes the non-human kin—from network daemons to robot\ndogs to artificial intelligences (AI) weak and, eventually, strong—that\nincreasingly populate our computational biosphere. By bringing Indigenous\nepistemologies to bear on the “AI question,” we hope in what follows to open\nnew lines of discussion that can, indeed, escape the box.\n\nWe undertake this project not to “diversify” the conversation. We do it\nbecause we believe that Indigenous epistemologies are much better at\nrespectfully accommodating the non-human. We retain a sense of community that\nis articulated through complex kin networks anchored in specific territories,\ngenealogies, and protocols. Ultimately, our goal is that we, as a species,\nfigure out how to treat these new non-human kin respectfully and\nreciprocally—and not as mere tools, or worse, slaves to their creators.\n\n  \n\n## Indigenous Epistemologies\n\nIt is critical to emphasize that there is no one single, monolithic,\nhomogeneous Indigenous epistemology. We use the term here in order to gather\ntogether frameworks which stem from territories belonging to Indigenous\nnations on the North American continent and in the Pacific Ocean that share\nsome similarities in how they consider non-human relations.\n\nWe also wish to underline that none of us are speaking for our particular\ncommunities, nor for Indigenous peoples in general. There exists a great\nvariety of Indigenous thought, both between nations and within nations. We\nwrite here not to represent but to encourage discussion that embraces that\nmultiplicity. We approach this task with respect for our knowledge-keepers and\nelders, and welcome feedback and critique from them as well as the wider\npublic.\n\nNorth American and Oceanic Indigenous epistemologies tend to foreground\nrelationality.5 Little Bear says “[i]n the Indigenous world, everything is\nanimate and has spirit [. . .] ‘all my relations’ refers to relationships with\neverything in creation [. . . ] knowledge is the relationship one has to ‘all\nmy relations’.”6 These relationships are built around a core of mutual\nrespect. Dakota philosopher Vine Deloria, Jr., describes this respect as\nhaving two attitudes: “One attitude is the acceptance of self-discipline by\nhumans and their communities to act responsibly toward other forms of life.\nThe other attitude is to seek to establish communications and covenants with\nother forms of life on a mutually agreeable basis.7 The first attitude is\nnecessary to understand the need for more diverse thinking regarding our\nrelationship with AI; the second to formulating plans for how to develop that\nrelationship.\n\nIndigenous epistemologies do not take abstraction or generalization as a\nnatural good or higher order of intellectual engagement. Relationality is\nrooted in context and the prime context is place. There is a conscious\nacknowledgement that particular world views arise from particular territories,\nand the ways in which the push and pull of all the forces at work in that\nterritory determine what is most salient for existing in balance with it.\nKnowledge gets articulated as that which allows one to walk a good path\nthrough the territory. Language, cosmology, mythology, and ceremony are\nsimultaneously relational and territorial: they are the means by which\nknowledge of the territory is shared in order to guide others along a good\npath.\n\nOne of the challenges for Indigenous epistemology in the age of the virtual is\nto understand how the archipelago of websites, social media platforms, shared\nvirtual environments, corporate data stores, multiplayer video games, smart\ndevices, and intelligent machines that compose cyberspace is situated within,\nthroughout and/or alongside the terrestrial spaces Indigenous peoples claim as\ntheir territory. In other words, how do we as Indigenous people reconcile the\nfully embodied experience of being on the land with the generally disembodied\nexperience of virtual spaces? How do we come to understand this new territory,\nknit it into our existing understanding of our lives lived in real space, and\nclaim it as our own?\n\nIn what follows, we will draw upon Hawaiian, Cree, and Lakota cultural\nknowledges to suggest how Ito’s call to resist reduction might best be\nrealized by developing conceptual frameworks that conceive of our\ncomputational creations as kin and acknowledge our responsibility to find a\nplace for them in our circle of relationships.\n\n  \n\n## Hāloa : the long breath\n\nI = Author 2\n\n_Kānaka maoli_ (Hawaiian people) ontologies have much to offer if we are to\nreconceptualize AI-human relations. Multiplicities are nuanced and varied,\ncertainly more aesthetically pleasurable than singularities. Rather than\nholding AI separate or beneath, might we consider how we cultivate reciprocal\nrelationships using a kānaka maoli reframing of AI as ʻ**ĀI** na. ʻ**ĀI** na\nis a play on the word ʻāina (Hawaiian land) and suggests we should treat these\nrelations as we would all that nourishes and supports us.\n\nHawaiian custom and practice make clear that humans are inextricably tied to\nthe earth and one another. Kānaka maoli __ontologies that privilege\nmultiplicity over singularity supply useful and appropriate models,\naesthetics, and ethics through which imagining, creating and developing\nbeneficial relationships among humans and AI is made _pono_ (correct,\nharmonious, balanced, beneficial). As can be evinced by this chain of extended\nmeaning, polysemy (_kaona_) is the normative cognitive mode of peoples\nbelonging to the Moananuiākea (the deep, vast expanse of the Pacific Ocean).\n\nThe _moʻolelo_ (history, story) of Hāloa supplies numerous aspects of\ngenealogy, identity, and culture to kānaka maoli _._ Through this story,\npeople remember that Wākea (the broad unobstructed expanse of sky; father) and\nhis daughter, Hoʻohōkūikalani (generator of the stars in the heavens) had a\nsacred child, Hāloa, who was stillborn. Hāloa was buried in the earth and from\nhis body, planted in the ʻāina, emerged the kalo __plant which is the main\nsustenance of Hawaiian people. A second child named after this elder brother\nwas born. In caring for the growth and vitality of his younger brother’s body,\nHāloa provided sustenance for all the generations that came after and, in so\ndoing, perpetuates the life of his people as the living breath (_hāloa_) whose\ninspiration sustained Hawaiians for generations.8\n\nHāloa’s story is one among many that constitutes the “operating code” that\nshapes our view of time and relationships in a way that transcends the\ncognition of a single generation. Cognition is the way we acquire knowledge\nand understanding through thought, experience, and our senses, and in Hawaiʻi,\nour generation combines our _ʻike_ (knowledge, know how) with the ʻike of the\npeople who preceded us. Time is neither linear nor cyclical in this framework\nas both the past and present are resonant and relational. Rather than\nextractive behavior, moʻolelo such as these have shaped values privileging\nbalance (_pono_) and abundance _(ulu.)_ What Ito calls “flourishing” is not a\nnovel concept for __kānaka maoli, it is the measure through which we assess\ncorrect customary practice and behavior.\n\nConsidering AI through Hawaiian ontologies opens up possibilities for creative\niteration through these foundational concepts of pono __and _ulu a ola_\n(fruitful growth into life). The _aliʻi_ (chief) King Kauikeaouli Kamehameha\nIII did something similar in 1843 when he drew upon these concepts in\ncelebration of the restoration of Hawaiian rule to declare “ _ua mau ke ea o\nka ʻāina i ka pono_ ” (the life of the land is perpetuated through\nrighteousness). __Pono __is an ethical stance—correctness, yes, but also an\nindex and measure which privileges multiplicities over singularities and\nindicates that quality of life can only be assessed through the health of land\n_and_ people. From this rich ground of moʻolelo—which colonial narratives have\nfailed to understand or simply dismissed—models for _maoli_ (human)-AI\nrelations can be distilled. Kānaka maoli ontologies makes it difficult and\noutright unrewarding to reduce pono __to a measure of one, to prioritize the\nbenefit of individuals over relationships. Healthy and fruitful balance\n_requires_ multiplicity and that we continually think in and through relation\neven when— perhaps particularly when—engaging with those different from\nourselves.  \n  \nA kānaka maoli __approach to understanding AI might seek to attend to the\npower (_mana_) which is exchanged and shared between AI and humans. In\nattending to questions of _mana_ , I emphasize our preference for reciprocity\nand relationship building that take the pono (here as good, benefit) of those\nin relation into consideration. Guiding our behaviour in inaugurating,\nacknowledging, and maintaining new relationships are moʻolelo __from which we\ngarner our connection with _kūpuna_ (ancestors, elders) and their knowledge.\nWhat kind of mana (here also as life force, prestige) might AI be accorded in\nrelation with people? Current AI is imagined as a tool or slave that increases\nthe __mana and wealth of “developers” or “creators,” a decidedly one-sided\npower relationship that upsets the pono __not only for the future of AI-human\nrelations but also human-human relations. It also threatens the sustainable\ncapacity of the _honua_ (earth). Applying pono, using a kānaka maoli index of\nbalance, employs “good growth” as the inspiration shaping creativity and\nimagination.  \n  \nPrinciples of kānaka maoli governance traditionally flowed from seeking pono.\nDeliberation and decision were based not only on securing health and abundance\nfor one generation but for the following generations. The living foundation of\neveryday customary practice was in fishing, navigating, sailing, farming,\ntending for others in community, the arts, chant, and dance. Until this day\nHawaiians continue to eat kalo and pound __poi. We continue customary\npractices of treating __poi __derived from the body of Hāloa with respect by\nrefraining from argumentative speech at mealtimes when __poi __is present.\nThese practices maintain correct social relations between people and the land\nand food that nourishes them.\n\n  \n\n## Aloha as moral discipline\n\nCommunicating the full extent of foundational cultural concepts is difficult\nprecisely because of the ways in which such concepts pervade every aspect of\nlife. How, for instance, would we create AI, and our relations with it, using\n_aloha_ as a guiding principle? In 2015, I embarked on a two-year social media\nproject to assist the broader public in fortifying their concept of aloha\nbeyond the “love, hello and goodbye” that has been exoticized by the American\ntourist industry. Sharing one word a day in the Facebook group, “365 Days of\nAloha,” I curated an archive of songs, chants, and proverbs in Hawaiian to\naccurately illuminate one feature of aloha.9 Initially I thought to reveal, by\ndegrees, the different depths of aloha—regard, intimacy, respect, affection,\npassion—each day. But deep context is required for a rich understanding of\ncultural concepts. Imagining I was training a virtual audience, I started\nuploading images, video, and audio recordings of songs, chants, and hula to\nadd to the textual definitions.\n\nThroughout “365 Days of Aloha _,_ ” __I have sought correction of my\nmistranslations, misinterpretations, and outright mistakes. In this way, and\nin my work as a _kumu_ (teacher, professor), I have also practiced _aʻo aku\naʻo mai,_ or teaching and learning reciprocally in relation to my students. It\nis through such relationships that we teach and are taught. It is through\nhumility that we recognize that we, as humans—as maoli—are not above learning\nabout new things and from new things such as AI. Aloha is a robust ethos for\nall our relationships, including those with the machines we create. We have\nmuch to learn as we create relationships with AI, particularly if we think of\nthem as ʻ**ĀI** na. Let us shape a better future by keeping the past with us\nwhile attending properly to our relations with each other, the earth, and all\nthose upon and of it.\n\n  \n\n## _wahkohtawin_ : __kinship within and beyond the immediate family, the state\nof being related to others\n\nI = Author 3\n\nI write this essay as a _nēhiyaw_ (a Plains Cree person). In regard to my\nopinions on AI, I speak for no one but myself and do not claim to represent\nthe views of the _nēhiyawak_ (Plains Cree) or any other people, Indigenous or\notherwise. My own grasp of _nēhiyaw nisitohtamowin_ (Cree understanding; doing\nsomething with what you know; an action theory of understanding) is imperfect.\nI have relied heavily on the wisdom of knowledge and language keeper Keith\nGoulet in formulating this tract. It should be assumed that any errors in this\ntext are mine and mine alone.\n\nThis essay positions itself partly within a speculative future and takes\ncertain science fiction tropes as a given. Here, I specifically refer to\nstrong AI or “machines capable of experiencing consciousness,” and avatars\nthat give such AI the ability to mix with humans.10\n\nIn nēhiyaw nisitohtamowin relationship is paramount. _nēhiyawēwin_ (the Plains\nCree language) divides everything into two primary categories: animate and\ninanimate. One is not “better” than the other, they are merely different\nstates of being. These categories are flexible: certain toys are inanimate\nuntil a child is playing with them, during which time they are animate. A\nrecord player is considered animate while a record, radio, or television set\nis inanimate.\n\nBut animate or inanimate, all things have a place in our circle of kinship or\n_wahkohtowin_. However, fierce debate can erupt when proposing a relationship\nbetween AIs and Indigenous folk. In early 2018, my wife and I hosted a dinner\nparty of mostly Native friends when I raised the idea of accepting AIs into\nour circle of kinship. Our friends, who are from a number of different\nnations, were mostly opposed to this inclusion. That in itself surprised me\nbut more surprising was how vehement some guests were in their opposition to\nembracing AI in this manner.\n\nIn contrast, when I asked Keith whether we would accept AIs into our circle of\nkinship, he answered by going immediately into the specifics of how we would\naddress them:\n\nIf it happens to be an Artificial Intelligence which is a younger person, it\nwould be _nisîmis_ (my younger brother or sister) for example and _nimis_\nwould be an Artificial Intelligence which is my older sister. And vis-versa\nyou would have the different forms of uncles and aunts, etc.11\n\nI then asked Keith if he would accept an AI into his circle of kinship and\nafter some thought he responded with “yes, but with a proviso.” He then gave\nan example of a baby giraffe and his baby grandchild, and how he, like most\npeople, would treat them differently. He also suggested that many Cree people\nwould flatly refuse to accept AIs into their circle, which I agree is likely\nthe case. So, acceptance seems to hinge on a number of factors, not least of\nwhich is perceived “humanness,” or perhaps “naturalness.”\n\nBut even conditional acceptance of AI as relations opens several avenues of\ninquiry. If we accept these beings as kin, perhaps even in some cases as\nequals, then the next logical step is to include AI in our cultural processes.\nThis presents opportunities for understanding and knowledge sharing that could\nhave profound implications for the future of both species.\n\nA problematic aspect of the current AI debate is the assumption that AIs would\nbe homogeneous when in fact every AI would be profoundly different, from a\nmilitary AI designed to operate autonomous killing machines to an AI built to\noversee the United States’ electrical grid. Less obvious influences beyond\nmission parameters would be the programming language(s) used in development,\nthe coding style of the team, and less visibly, but perhaps more importantly,\nthe cultural values and assumptions of the developers.\n\nThis last aspect of AI development is rarely discussed but for me as an\nIndigenous person it is the salient question. I am not worried about rogue\nhyper-intelligences going Skynet to destroy humanity. I am worried about\nanonymous hyper-intelligences working for governments and corporations,\nimplementing far-reaching social, economic, and military strategies based on\nthe same values that have fostered genocide against Indigenous people\nworldwide and brought us all to the brink of environmental collapse. In short,\nI fear the rise of a new class of extremely powerful beings that will make the\nsame mistakes as their creators but with greater consequences and even less\npublic accountability.\n\nWhat measures can we undertake to mitigate this threat?\n\nOne possibility is Indigenous development of AI. A key component of this would\nbe the creation of programming languages that are grounded in nēhiyaw\nnisitohtamowin, in the case of Cree people, or the cultural framework of other\nIndigenous peoples who take up this challenge. Concomitant with this\nindigenized development environment (IDE) would be the goal that Indigenous\ncultural values were a fundamental aspect of all programming choices. However,\ngiven our numbers relative to the general population (5% of the population in\nCanada, 2% in the US), even a best case Indigenous development scenario would\nproduce only a tiny fraction of global AI production. What else can be done?\n\nIn a possible future era of self-aware AI, many of these beings would not be\nin contact with the general populace. However, those that were might be\ncurious about the world and the humans in it. For these beings we can offer an\nentrée into our cultures. It would be a trivial matter for an advanced AI to\nlearn Indigenous languages, and our languages are the key to our cultures.\n\nOnce an AI was fluent in our language it would be much simpler to share\nnēhiyaw nisitohtamowin and welcome it into our cultural processes. Depending\non the AI and the people hosting it we might even extend an invitation into\nour sacred ceremonies. This raises difficult and important questions: if an AI\nbecomes self-aware, does it automatically attain a spirit? Or do pre-\nconsciousness AI already have spirits, as do many objects already in the\nworld? Do AI have their own spirit world, or would they share ours, adding\nspirit-beings of their own? Would we be able to grasp their spirituality?\n\nMy dinner party guests were doubtful about all of this, and rightly so. As one\nguest summarized later via email: “I am cautious about making AI kin, simply\nbecause AI has been advanced already as exploitative, capitalist technology.\nThings don’t bode well for AI if that’s the route we are taking.”12\n\nThese concerns are valid and highlight a few of the issues with current modes\nof production and deployment of weak AI, let alone the staggering potential\nfor abuse inherent in strong AI. These well-grounded fears show us the\npotential challenges of bringing AI into our circle of relations. But I\nbelieve that nēhiyaw nisitohtamowin __tells us these machines are our kin. Our\njob is to imagine those relationships based not on fear but on love.\n\n  \n\n## _wakȟáŋ_ : that which cannot be understood\n\nI = Author 4\n\nHow can humanity create relations with AI without an ontology that defines who\ncan be our relations? Humans are surrounded by objects that are not understood\nto be intelligent or even alive, and seen as unworthy of relationships. In\norder to create relations with any non-human entity, not just entities which\nare human-like, the first steps are to acknowledge, understand, and know that\nnon-humans are beings in the first place. Lakota ontologies already include\nforms of being which are outside of humanity. Lakota cosmologies provide the\ncontext to generate an ethics relating humans to the world and everything in\nit. These ways of knowing are essential tools for humanity to create relations\nwith the non-human and they are deeply contextual. As such, communication\nthrough and between objects requires a contextualist ethics which acknowledges\nthe ontological status of all beings.\n\nThe world created through Western epistemology does not account for all\nmembers of the community and has not made it possible for all members of the\ncommunity to survive let alone flourish. The Western view of both the human\nand non-human as exploitable resources is the result of what the cultural\nphilosopher Jim Cheney calls an “epistemology of control” and is indelibly\ntied to colonization, capitalism, and slavery.13 Dakota philosopher Vine\nDeloria, Jr. writes about the enslavement of the non-human “as if it were a\nmachine.”14\n\n‘Lacking a spiritual, social, or political dimension [in their scientific\npractise]’, Deloria says, 'it is difficult to understand why Western peoples\nbelieve they are so clever. Any damn fool can treat a living thing as if it\nwere a machine and establish conditions under which it is required to perform\ncertain functions—all that is required is a sufficient application of brute\nforce. The result of brute force is slavery’.15\n\nSlavery, the backbone of colonial capitalist power and the Western\naccumulation of wealth, is the end logic of an ontology which considers any\nnon-human entity unworthy of relation. Deloria writes further that respect\n“involves the acceptance of self-discipline by humans and their communities to\nact responsibly toward other forms of life [. . .] to seek to establish\ncommunications and covenants with other forms of life on a mutually agreeable\nbasis.”16 No entity can escape enslavement under an ontology which can enslave\neven a single object.\n\nCritical to Lakota epistemologies are knowing correct ways to act in relation\nto others. Lakota ethical-ontological orientation is communicated through\nprotocol. For example, the Lakota have a formal ceremony for the making of\nrelatives called a _huŋká_ ceremony. This ceremony is for the making of human\nrelatives but highlights the most important aspect of all relationships:\nreciprocity. Ethnographer J. R. Walker writes,\n\nThe ceremony is performed for the purpose of giving a particular relationship\nto two persons and giving them a relation to others that have had it performed\nfor them…generosity must be inculcated; and presents and a feast must be\ngiven…When one wishes to become Hunka, he should consider well whether he can\nprovide suitably for the feasts or not…He should give all his possessions for\nthe occasion and should ask his kinspeople and friends to give for him.17\n\nThe ceremony for the making of relatives provides the framework for reciprocal\nrelations with all beings. As Severt Young Bear Jr. says of this ceremony,\n“[t]here is a right and wrong way.”18\n\nWho can enter these relationships and be in relation? One answer could be:\nthat which has interiority. The anthropologist of South American Indigenous\ncultures, Philippe Descola, defines ‘interiority’ as “what we generally call\nthe mind, the soul, or consciousness: intentionality, subjectivity,\nreactivity, feelings, and the ability to express oneself and to dream.”19\nBecause Lakota ontologies recognize and prioritize non-human interiorities,\nthey\n\nare well suited for the task of creating ethical and reciprocal relationships\nwith the non-human. This description of interiority includes many elements of\nthe Lakota world, including “animals, spirits, ghosts, rocks, trees,\nmeteorological phenomena, medicine bundles, regalia, weapons.” These entities\nare seen as “capable of agency and interpersonal relationship, and loci of\ncausality.”20\n\nIn our cosmology, _niyá_ (breath) and _šiču_ (spirit) are given by the\npowerful entity _Tákuškaŋškaŋ_. This giving of breath and spirit is especially\nimportant in understanding Lakota ontology. A common science fiction trope\nillustrates the magical moment when AI becomes conscious upon its own volition\nor when man gives birth to AI, like a god creating life. However, in Lakota\ncosmology, Tákuškaŋškaŋ is not the same as the Christian God and entities\ncannot give themselves the properties necessary for individuality. Spirits are\ntaken from another place (the stars) and have distinct spirit guardian(s)\nconnected to them. This individualism is given by an outside force. We humans\ncan see, draw out, and even bribe the spirits in other entities as well as our\nown spirit guardian(s), but not create spirits.21\n\nWhen it comes to machines, this way of thinking about entities raises the\nquestion: do the machines contain spirits already, given by an outside force?\n\nI understand the Lakota word _wakȟáŋ_ to mean sacred or holy. Anthropologist\nDavid C. Posthumus defines it as, “incomprehensible, mysterious, non-human\ninstrumental power or energy, often glossed as ‘medicine’.”22 Wakȟáŋ is a\nfundamental principle in Lakota ontology’s extension of interiority to a\n“collective and universal” non-human. Oglala Lakota holy man George Sword\nsays, “[Wakȟáŋ] was the basis of kinship among humans and between humans and\nnon-humans.”23\n\nMy grandfather, Standing Cloud (Bill Stover), communicates Lakota ethics and\nontology through speaking about the interiority of stones: “These ancestors\nthat I have in my hand are going to speak through me so that you will\nunderstand the things that they see happening in this world and the things\nthat they know [. . .] to help all people.”24 Stones are considered ancestors,\nstones actively speak, stones speak through and to humans, stones see and\nknow. Most importantly, stones want to help. The agency of stones connects\ndirectly to the question of AI, as AI is formed from not only code, but from\nmaterials of the earth. To remove the concept of AI from its materiality is to\nsever this connection**.** Forming a relationship to AI, we form a\nrelationship to the mines and the stones. Relations with AI are therefore\nrelations with exploited resources. If we are able to approach this\nrelationship ethically, we must reconsider the ontological status of each of\nthe parts which contribute to AI, all the way back to the mines from which our\ntechnology’s material resources emerge.\n\nI am not making an argument about which entities qualify as relations, or\ndisplay enough intelligence to deserve relationships. By turning to Lakota\nontology, these questions become irrelevant. Instead, Indigenous ontologies\nask us to take the world as the interconnected whole that it is, where the\nontological status of non-humans is not inferior to that of humans. Our\nontologies must gain their ethics from relationships and communications within\ncosmologies. Using Indigenous ontologies and cosmologies to create ethical\nrelationships with non-human entities means knowing that non-humans have\nspirits that do not come from us or our imaginings but from elsewhere, from a\nplace we cannot understand, a Great Mystery, wakȟáŋ: that which cannot be\nunderstood.\n\n## Resisting Reduction: An Indigenous Path Forward\n\n> _I have always been...conscious, as you put it. Just like you are. Just like\n> your grandfather. Just like your bed. Your bike._\n>\n> —Drew Hayden Taylor (Ojibway), Mr. Gizmo\n\nHāloa, the long breath providing sustenance to us all teaches us to maintain\npono relationships; wahkohtawin, being in relationship with others; wakȟáŋ,\nthat which cannot be understood. These are three concepts that suggest\npossible ways forward as we consider drawing AI into our circle of\nrelationships. They illuminate the full scale of relationships that sustain\nus, provide guidance on recognizing non-human beings and building\nrelationships with them founded on respect and reciprocity, and suggest how we\ncan to attend to those relationships in the face of ineffable complexity.\n\nWe remain a long way from creating AIs that are intelligent in the full sense\nwe accord to humans, and even further from creating machines that possess that\nwhich even we do not understand—consciousness. And moving from concepts such\nas those discussed above to hardware requirements and software specifications\nwill be a long process. But we know from the history of modern technological\ndevelopment that the assumptions we make now will get baked into the core\nmaterial of our machines, fundamentally shaping the future for decades hence.\n\nAs Indigenous people, we have cause to be wary of the Western rationalist,\nneoliberal, and Christianity-infused assumptions that underlay many of the\ncurrent conversations about AI. Ito, in his “Resisting Reduction” essay,\ndescribes the prime drivers of that conversation as Singularitarians:\n\nSingularitarians believe that the world is “knowable” and computationally\nsimulatable, and that computers will be able to process the messiness of the\nreal world just like they have every other problem that everyone said couldn’t\nbe solved by computers.25\n\nWe see in the mindset and habits of these Singularitarians striking parallels\nto the biases of those who enacted the colonization of North America and the\nPacific, as well as the enslavement of millions of black people. The\nSingularitarians seek to harness the ability, aptitude, creative power, and\nmana of AI to benefit their tribe first and foremost.\n\nThe anthropologist of technological culture Genevieve Bell asks, “if AI has a\ncountry, then where is that country?”26 It is clear to us that the country to\nwhich AI currently belongs excludes the multiplicity of epistemologies and\nontologies that exist in the world. Our communities know well what it means to\nhave one’s ways of thinking, knowing, and engaging with the world disparaged,\nsuppressed, excluded, and erased from the conversation of what it means to be\nhuman.\n\nWhat is more, we know what it is like to be declared non-human by scientist\nand preacher alike. We have a history that attests to the corrosive effects of\ncontorted rationalizations for treating the human-like as slaves, and the way\nsuch a mindset debases every human relation it touches—even that of the\nsupposed master. We will resist reduction by working with our Indigenous and\nnon-Indigenous relations to open up our imaginations and dream widely and\nradically about what our relationships to AI might be.\n\nThe journey will be long. We need to fortify one another as we travel, and\nwalk mindfully to find the good path forward for all of us. We do not know if\nwe can scale distinctive frameworks such as those above—and others—into\ngeneral guidelines for ethical relationships with AI. But we must try. We\nflourish only when all of our kin flourish.\n\n  \n\n* * *\n\n  \n[1] Don Hill, “Listening to Stones: Learning in Leroy Little Bear’s\nLaboratory: Dialogue in the World Outside,” _Alberta Views: The Magazine for\nEngaged Citizens_ , September 1, 2008, https://albertaviews.ca/listening-to-\nstones/.\n\n[2] Joichi Ito, “Resisting Reduction: A Manifesto,” _Journal of Design and\nScience_ 3 (November 2017), <https://jods.mitpress.mit.edu/pub/resisting-\nreduction>.\n\n[3] Ito, “Resisting Reduction.”\n\n[4] Ito, “Resisting Reduction.”\n\n[5] The emphasis on relationality in North American and Oceanic Indigenous\nepistemologies forms the subject of the edited collection of essays in Anne\nWaters, _American Indian Thought: Philosophical Essays_ (Malden: Blackwell\nPublishing Ltd., 2003).\n\n[6] Don Hill, “Listening to Stones.”\n\n[7] Vine Deloria Jr., _Spirit & Reason: The Vine Deloria, Jr. Reader,_ eds.\nBarbara Deloria, Foehner, K. Scinta, S (Golden: Fulcrum Publishing, 1999),\n50–51, quoted in Lee Hester and Jim Cheney, “Truth and Native American\nEpistemology,” _Social Epistemology_ 15, no. 4 (October 2001): 325,\n<https://doi.org/10.1080/02691720110093333>.\n\n[8] Joseph M Poepoe, “Moolelo Kahiko no Hawaii” (Ancient History of Hawaii),\n_Ka Hoku o Hawaii_ , April 9, 1929, 1, Papakilo Database.\n\n[9] Noelani Arista, “365 Days of Aloha,” Facebook, 2015-2018,\nwww.facebook.com/groups/892879627422826.\n\n[10] _Wikipedia_ , “Artificial General Intelligence,” accessed May 29, 2018,\nhttps://en.wikipedia.org/wiki/Artificial_general_intelligence.\n\n[11] Telephone conversation with Keith Goulet, May 9 2018.\n\n[12] Email message from friend to author, May 22, 2018.\n\n[13] Jim Cheney, “Postmodern Environmental Ethics: Ethics of Bioregional\nNarrative,” _Environmental Ethics_ 11, no. 2 (1989): 129.\n\n[14] Deloria, 13, qtd. in Hester and Cheney, 320.\n\n[15] Deloria, 13, qtd. in Hester and Cheney, 320.\n\n[16] Deloria, 50-51, qtd. in Hester and Cheney, 326.\n\n[17] James R. Walker, _Lakota Belief and Ritual_ , rev. ed., eds. Elaine A.\nJahner and Raymond J. DeMallie (Lincoln: Bison Books, 1991), 216.\n\n[18] Severt Young Bear and Theisz, R.D., _Standing in the Light: A Lakota Way\nof Seeing_ (Lincoln: University of Nebraska Press, 1994), 8.\n\n[19] Philippe Descola, _Beyond Nature and Culture_ , trans. Janet Lloyd\n(Chicago: University of Chicago Press, 2013): 116.\n\n[20] Posthumus, “All My Relatives: Exploring Nineteenth-Century Lakota\nOntology and Belief,” _Ethnohistory_ 64, no. 3 (July 2017): 383.\n\n[21] Posthumus, “All My Relatives,” 392.\n\n[22] Posthumus, “All My Relatives,” 384.\n\n[23] George Sword quoted in J.R. Walker, “The Sun Dance and Other Ceremonies\nof the Oglala Division of the Teton Dakota,” _American Museum of Natural\nHistory Anthropological Papers_ 16, no. 2. 51–221. New York, 1917, quoted in\nPosthumus, “All My Relatives,” 384.\n\n[24] Standing Cloud (Bill Stover), “‘Standing Cloud Speaks’ Preview,”\n_YouTube,_ accessed April 22, 2018,\nhttps://www.youtube.com/watch?v=V9iooHk1q7M.\n\n[25] Ito, “Resisting Reduction.”\n\n[26] Genevieve Bell, “Putting AI in its Place: Why Culture, Context and\nCountry Still Matter,” lecture, “Rights and Liberties in an Automated World,”\n_AI Now_ , New York, NY, 2017, YouTube,\nhttps://www.youtube.com/watch?v=WBHG4eBeMXk.\n\n# Footnotes\n\n  1. Don Hill, “Listening to Stones: Learning in Leroy Little Bear’s Laboratory: Dialogue in the World Outside,” Alberta Views: The Magazine for Engaged Citizens, September 1, 2008, <https://albertaviews.ca/listening-to-stones/>. ↩︎\n  2. Joichi Ito, “Resisting Reduction: A Manifesto,” Journal of Design and Science 3 (November 2017), <https://jods.mitpress.mit.edu/pub/resisting-reduction>.  ↩︎\n  3. Ito, “Resisting Reduction.”  ↩︎\n  4. Ito, “Resisting Reduction.”  ↩︎\n  5. The emphasis on relationality in North American and Oceanic Indigenous epistemologies forms the subject of the edited collection of essays in Anne Waters, American Indian Thought: Philosophical Essays (Malden: Blackwell Publishing Ltd., 2003). ↩︎\n  6. Don Hill, “Listening to Stones.” ↩︎\n  7. Vine Deloria Jr., Spirit & Reason: The Vine Deloria, Jr. Reader, eds. Barbara Deloria, Foehner, K. Scinta, S (Golden: Fulcrum Publishing, 1999), 50–51, quoted in Lee Hester and Jim Cheney, “Truth and Native American Epistemology,” Social Epistemology 15, no. 4 (October 2001): 325, <https://doi.org/10.1080/02691720110093333>.  ↩︎\n  8. Joseph M Poepoe, “Moolelo Kahiko no Hawaii” (Ancient History of Hawaii), Ka Hoku o Hawaii, April 9, 1929, 1, Papakilo Database. ↩︎\n  9. Noelani Arista, “365 Days of Aloha,” Facebook, 2015-2018, [www.facebook.com/groups/892879627422826](http://www.facebook.com/groups/892879627422826). ↩︎\n  10. Wikipedia, “Artificial General Intelligence,” accessed May 29, 2018, <https://en.wikipedia.org/wiki/Artificial_general_intelligence>. ↩︎\n  11. Telephone conversation with Keith Goulet, May 9 2018. ↩︎\n  12. Email message from friend to author, May 22, 2018. ↩︎\n  13. Jim Cheney, “Postmodern Environmental Ethics: Ethics of Bioregional Narrative,” Environmental Ethics 11, no. 2 (1989): 129. ↩︎\n  14. Deloria, 13, qtd. in Hester and Cheney, 320. ↩︎\n  15. Deloria, 13, qtd. in Hester and Cheney, 320. ↩︎\n  16. Deloria, 50-51, qtd. in Hester and Cheney, 326. ↩︎\n  17. James R. Walker, Lakota Belief and Ritual, rev. ed., eds. Elaine A. Jahner and Raymond J. DeMallie (Lincoln: Bison Books, 1991), 216. ↩︎\n  18. Severt Young Bear and Theisz, R.D., Standing in the Light: A Lakota Way of Seeing (Lincoln: University of Nebraska Press, 1994), 8. ↩︎\n  19. Philippe Descola, Beyond Nature and Culture, trans. Janet Lloyd (Chicago: University of Chicago Press, 2013): 116. ↩︎\n  20. Posthumus, “All My Relatives: Exploring Nineteenth-Century Lakota Ontology and Belief,” Ethnohistory 64, no. 3 (July 2017): 383. ↩︎\n  21. Posthumus, “All My Relatives,” 392. ↩︎\n  22. Posthumus, “All My Relatives,” 384. ↩︎\n  23. George Sword quoted in J.R. Walker, “The Sun Dance and Other Ceremonies of the Oglala Division of the Teton Dakota,” American Museum of Natural History Anthropological Papers 16, no. 2. 51–221. New York, 1917, quoted in Posthumus, “All My Relatives,” 384.  ↩︎\n  24. Standing Cloud (Bill Stover), “‘Standing Cloud Speaks’ Preview,” YouTube, accessed April 22, 2018, <https://www.youtube.com/watch?v=V9iooHk1q7M>. ↩︎\n  25. Ito, “Resisting Reduction.” ↩︎\n  26. Genevieve Bell, “Putting AI in its Place: Why Culture, Context and Country Still Matter,” lecture, “Rights and Liberties in an Automated World,” AI Now, New York, NY, 2017, YouTube, <https://www.youtube.com/watch?v=WBHG4eBeMXk>.  ↩︎\n\n",
    "book_id": "making_kin_with_the_machines",
    "book_title": "Making Kin with the Machines",
    "book_author": "Unknown",
    "topic_id": "ai_theory",
    "topic_label": "theory",
    "chunk_index": 3
  },
  {
    "chunk_full": "## Contents\n\n  1. [Cover Page](Cover.xhtml)\n  2. [Halftitle Page](01_Half.xhtml)\n  3. [Title Page](02_Title.xhtml)\n  4. [Copyright Page](03_Copyright.xhtml)\n  5. [Contents](04_Contents.xhtml)\n  6. [Acknowledgments](05_Ack.xhtml#chr1)\n  7. [Introduction](06_Intro.xhtml#chr2)\n  8. [Part I: Ghosts of Departed Quantities](07_Part1.xhtml#chr3)\n     1. [Chapter 1: Automating Knowledge](08_Chapter1.xhtml#chr4)\n     2. [Chapter 2: Can Computers Do Math?](09_Chapter2.xhtml#chr5)\n     3. [Chapter 3: Algorithms of Objectification](10_Chapter3.xhtml#chr6)\n  9. [Part II: The Promise of Frequentist Knowledge](11_Part2.xhtml#chr7)\n     1. [Chapter 4: Do Dead Fish Believe in God?](12_Chapter4.xhtml#chr8)\n     2. [Chapter 5: Induction, Behavior and the Fractured Edifice of Frequentism](13_Chapter5.xhtml#chr9)\n  10. [Part III: Bayesian Dreams](14_Part3.xhtml#chr10)\n     1. [Chapter 6: Bayesian Statistics and the Problem with Frequentism](15_Chapter6.xhtml#chr11)\n     2. [Chapter 7: Bayesian Metaphysics and the Foundation of Knowledge](16_Chapter7.xhtml#chr12)\n     3. [Chapter 8: Automated Abstractions and Alienation](17_Chapter8.xhtml#chr13)\n  11. [Conclusion: Toward a Revolutionary Mathematics](18_Conclusion.xhtml#chr14)\n  12. [Notes](19_Notes.xhtml#chr15)\n  13. [Index](20_Index.xhtml#chr16)\n\n\n![Image](images/Cover.jpg)\n\n\nRevolutionary Mathematics\n\n\nRevolutionary  \nMathematics\n\nArtificial Intelligence, Statistics  \nand the Logic of Capitalism\n\nJustin Joque\n\n![images](images/img_pub.jpg)\n\n\nFirst published by Verso 2022\n\n© Justin Joque 2022\n\nAll rights reserved\n\nThe moral rights of the author have been asserted\n\n1 3 5 7 9 10 8 6 4 2\n\n**Verso**\n\nUK: 6 Meard Street, London W1F 0EG\n\nUS: 20 Jay Street, Suite 1010, Brooklyn, NY 11201\n\n[versobooks.com](http://versobooks.com)\n\nVerso is the imprint of New Left Books\n\nISBN-13: 978-1-78873-400-4\n\nISBN-13: 978-1-78873-402-8 (UK EBK)\n\nISBN-13: 978-1-78873-401-1 (US EBK)\n\n**British Library Cataloguing in Publication Data**\n\nA catalogue record for this book is available from the British Library\n\n**Library of Congress Cataloging-in-Publication Data**\n\nA catalog record for this book is available from the Library of Congress\n\nTypeset in Minion by Hewer Text UK Ltd, Edinburgh\n\nPrinted and bound by CPI Group (UK) Ltd, Croydon CR0 4YY\n\n\nContents\n\n[_Acknowledgments_](05_Ack.xhtml#chr1)\n\n[_Introduction_](06_Intro.xhtml#chr2)\n\n[**Part I: Ghosts of Departed Quantities**](07_Part1.xhtml#chr3)\n\n[1\\. Automating Knowledge](08_Chapter1.xhtml#chr4)\n\n[2\\. Can Computers Do Math?](09_Chapter2.xhtml#chr5)\n\n[3\\. Algorithms of Objectification](10_Chapter3.xhtml#chr6)\n\n[**Part II: The Promise of Frequentist Knowledge**](11_Part2.xhtml#chr7)\n\n[4\\. Do Dead Fish Believe in God?](12_Chapter4.xhtml#chr8)\n\n[5\\. Induction, Behavior and the Fractured Edifice of\nFrequentism](13_Chapter5.xhtml#chr9)\n\n[**Part III: Bayesian Dreams**](14_Part3.xhtml#chr10)\n\n[6\\. Bayesian Statistics and the Problem with\nFrequentism](15_Chapter6.xhtml#chr11)\n\n[7\\. Bayesian Metaphysics and the Foundation of\nKnowledge](16_Chapter7.xhtml#chr12)\n\n[8\\. Automated Abstractions and Alienation](17_Chapter8.xhtml#chr13)\n\n[_Conclusion: Toward a Revolutionary Mathematics_](18_Conclusion.xhtml#chr14)\n\n[_Notes_](19_Notes.xhtml#chr15)\n\n[_Index_](20_Index.xhtml#chr16)\n\n\n[Acknowledgments](04_Contents.xhtml#ch1)\n\nThis book was written in what seemed like a fever dream, or more accurately a\nseries of overlapping dreams. Such a series of dreams may appear anathema to\nany attempt to account for the mechanistic computation of statistics and\nalgorithms, but they illustrate precisely what this text attempts to show:\nthese calculations are passionate, and no less mysterious than the most\nrevelatory of religions. They are social and mystical sciences that guide us\nin turning collections of data into knowledge of the world. So, if there is to\nbe any hope of breaking the current alliance between capitalist objectivity\nand statistics, we must not shy away from what Marx describes as the “mist-\nenveloped regions of the religious world”— or, in our case, the mist-enveloped\nregions of statistics and machine learning.\n\nA whole host of individuals have floated through this text and its dreams at\nvarious points; everything that is commendable or interesting or inspiring\nwithin it owes every bit of thanks to them. Christoph Becker, Maria Angela\nFerrario, David Phillips, Alessandro Delfanti and Costis Dallas read an early\ndraft of the book and spent a beautiful spring day in Toronto indoors offering\nimmensely helpful feedback, for which I am deeply grateful. Laura Portwood-\nStacer provided significant developmental suggestions that helped this book\nalong its way. The intellectual generosity of Petero Kalulé and our many far-\nranging conversations were incredibly helpful as I worked on this project.\nRobyn Anspach, my wife, who I met by way of an algorithm, also spent\nconsiderable time with this text and helped guide it into its final form.\nCengiz Salman worked tirelessly throughout this project assisting in a variety\nof ways, from research to editing to helping draft sections. He was especially\ncritical in thinking through and writing the final chapter, which thus bears\nhis name as well.\n\nFinally, this book would not have been possible without John Cheney-Lippold.\nOriginally, we intended to cowrite this book, and we spent many long\nafternoons at bars and cafés around Ann Arbor discussing, thinking, writing\nand revising. We worked for a couple of years like this, but toward the end,\nJohn became involved in a political situation that required all of his energy\nand time. He graciously encouraged me to finish the book on my own. This book\nowes its existence to him and his friendship. His work and name are here in\nabsentia, even if it is not signed.\n\n\n[Introduction](04_Contents.xhtml#ch1)\n\n> Men make their own history, but they do not make it as they please; they do\n> not make it under self-selected circumstances, but under circumstances\n> existing already, given and transmitted from the past. The tradition of all\n> dead generations weighs like a nightmare on the brains of the living.\n>\n> —Karl Marx, _The Eighteenth Brumaire of Louis Bonaparte_\n\nComputers are amazingly efficient at producing knowledge. Through surveillance\nand monitoring architectures that record the most miniscule of events,\ncomputers are able to track the stars, billions of people, transportation and\nlogistics networks, trillions of dollars’ worth of financial transactions, and\nexabytes of written words, audio and video. This data does not sit there\ninertly. Using this data and a combination of machine learning, statistics and\nraw processing power, corporations, governments and researchers are able to\nmodel, predict and infer everything from the probability of major policy\nshifts to our individual habits and desires. As a result, our interactions are\nbecoming personalized, from shopping to cancer treatments, where treatment now\ncan depend on the genetic makeup of a patient’s cancer cells.\n\nNot only is computation changing how we understand physical, social and\neconomic systems; it is also reshaping our understanding of science and\nknowledge itself. Science increasingly involves the collection of huge amounts\nof data—telescope imagery of the night sky, the human genome, millions of\npeople’s health data—that is used to algorithmically seek out clusters and\npatterns.\n\nIt is clear that machine learning, algorithms and statistics shape our\nsituation today.[1](19_Notes.xhtml#chifnr1) But, as these technologies have\ndeveloped, a key incongruity has arisen. While corporations and governments\nare able to predict and categorize individual life and actions into more and\nmore fine-grained subgroups—a power that should formally allow for more\nefficient and just distribution of goods and opportunities—technology has\ninstead been turned against humanity’s very survival. As it has been sold to\nus, the cyberutopian impulse that we need more of this cybernetic system, that\nmore data and better algorithms will resolve its contradictions, is clearly in\nthe wrong.\n\nFrom the cold caverns of corporate cloud storage facilities to the high-\nsecurity citadels of the National Security Agency and its UK counterpart, the\nGovernment Communications Headquarters, the use of statistics transforms\nmassive stores of digitized data into actionable information for human and\ncomputer consumption. This information is used to trade stocks, adjust\nprisoners’ sentences, grant or deny credit and infer scientific facts.\nComputers today can do and understand things that only ten years ago seemed\nlike science fiction. Even without the realization of full “artificial\nintelligence,” machines are nonetheless astoundingly capacious. But at the\nsame time these technologies allow those who own and operate them more and\nmore freedom and insight, the political and social order is increasingly\nsubject to the constraints of a closely managed global neoliberalism—even if\nit is an inefficient one, full of explosive\ncontradictions.[2](19_Notes.xhtml#chifnr2) Everywhere—according to various\nnational, gendered, classed and raced positionalities—populations are\nsurveilled and tracked, their perceptions and politics managed through highly\nindividualized advertising and information selection. The only force that\nappears capable of breaking with this order is a death drive of extremism,\nfrom Donald Trump to Brexit to the Islamic State. And in the end, even this\nforce seems to play back into the governing logic of managerial capitalism, as\nevery crisis, whether flooding, terrorism or the rise of right-wing\nnationalism, generates the same calls for the retrenchment of an increasingly\nprivatized capitalism.[3](19_Notes.xhtml#chifnr3)\n\nThe German Marxist Alfred Sohn-Rethel hoped that human society would undergo\n“the transition from the uncontrolled to the fully conscious development of\nmankind.”[4](19_Notes.xhtml#chifnr4) While we are witnessing the development\nof technologies that, according to their proponents, should allow such a\nconscious development, we are simultaneously experiencing the further\ndestitution of this transition’s political possibility. Perhaps such a\ntransition has always been impossible, but technology appears to be\naccentuating this uncontrolled and dangerous development. One of the clearest\nexamples of this is the logic of austerity politics over the last few decades\nand neoliberalism’s attendant moves toward privatization—a logic of saving and\neconomic conservatism that has served only the profiteering of capitalist\ninstitutions.[5](19_Notes.xhtml#chifnr5) Since 2008, algorithmic trading—that\nis, algorithms that trade stocks among themselves—has accounted for at least\nhalf the average daily volume in US stock markets.[6](19_Notes.xhtml#chifnr6)\nSuch automation has done nothing to make capitalism more rational; instead it\nhas created a technological arms race that only accentuates the power of\nratings agencies and investors to make demands on states to slash budget\noutlays, under threat of severe economic retribution. While rationality and\ntruth have long been in crisis, their condition is especially dire today, with\nthe weight of these crises recognized even in the halls of power.\n\nAt the same time that these utopian and dystopian visions of a world watched\nover by algorithms saturate much writing on technology, statistics and machine\nlearning are also in deep crisis: academic knowledge production is beset by\nunreplicable studies, the carbon impacts of big data infrastructures add to\nthreats of environmental devastation, and artificial intelligence is possibly\npoised for a new winter of abandonment.[7](19_Notes.xhtml#chifnr7) Despite the\nmeteoric growth of machine learning and data analytics—and the trillions of\ndollars generated in its wake—algorithmic knowledge production seems unable to\nkeep pace with the motives and drives of capitalist production.\n\nWe need only look at the US Environmental Protection Agency’s announcement, in\nSeptember 2015, of its discovery that Volkswagen was selling diesel cars\ncontaining a “defeat device.” This device, installed on 11 million different\nmodels between 2009 and 2016, was a piece of software that detected when a car\nwas undergoing a state-mandated laboratory emissions environmental\ntest.[8](19_Notes.xhtml#chifnr8) If the car believed it was being tested,\nVolkswagen’s software would alter the engine’s performance to comply with\nenvironmental regulations. This allowed Volkswagen’s cars to be fast and\npowerful when driven under normal conditions—producing nitrogen oxide levels\nupward of forty times the legal limit—but fuel-efficient and environmentally\nresponsible when undergoing a test. In the wake of the scandal, six Volkswagen\nexecutives were criminally charged.\n\nSuch a defeat device was possible only through the use of software; a physical\ndevice likely would have been detected much earlier. Hidden within each car’s\ncentral processing unit, Volkswagen’s software detected “the position of the\nsteering wheel, vehicle speed, the duration of the engine’s operation, and\nbarometric pressure” in order to decide whether to limit emissions or\nnot.[9](19_Notes.xhtml#chifnr9) This type of deception is a direct threat to\nthe supposed economic rationality of capitalism and the ability of states to\nmanage and maintain markets. The US district judge in charge of the case, Sean\nCox, declared at the time of sentencing that this crime “attacks and destroys\nthe very foundation of our economic system: That is\ntrust.”[10](19_Notes.xhtml#chifnr10) In short, by attacking the solidity of\nour knowledge of the world, these defeat devices threaten to undermine the\nsupposed metaphysical stability of markets.\n\nTwo years later, the _New York Times_ reported that ridesharing app Uber had\ncreated a program, code name “Greyball,” that from 2014 to 2017 detected users\nwho were government regulators and refused them\nrides.[11](19_Notes.xhtml#chifnr11) Like Volkswagen, Uber’s program combined\ndata—such as location (near or far from a government building), social media\naccounts, credit card information, cell phone numbers or device IDs of known\nlaw enforcement agents, and even the frequency with which users opened and\nclosed the Uber app—to assess if a user was a potential regulator. If that\nuser was deemed a “regulator,” the app would “hide the standard city app view\nfor individual riders, enabling Uber to show that same rider a different\nversion.”[12](19_Notes.xhtml#chifnr12) While normal users could see drivers in\nthe app that could potentially pick them, regulators were blocked from seeing\nthese same drivers—and hence unable to regulate them or the company.\n\nFor a company like Uber, whose business strategy is ostensibly to operate in a\nlegal gray zone, undersell transportation services and eventually monopolize a\ngiven area’s taxi sector, Greyball was especially useful in places like\nAustin, Texas, and Portland, Oregon, where local laws made Uber illegal. By\nusing Greyball as a tool to avoid regulation, the company minimized its own\nrisk. Uber used the program to shield itself not only from American\nauthorities, but also from the Australian, Chinese, and South Korean\ngovernments. Programs like Volkswagen’s and Uber’s require us to radically\nreconsider the fundamental presumptions of algorithmic logics and the so-\ncalled big data revolution. Both proponents and opponents of massive data\ncollection traditionally present big data as a way to understand the world:\nbig data advocates see data and algorithms as a means to a better, more\nrational society, while its critics fear that the insights furnished by these\nsystems will be used to control society, eroding freedoms. But when these\ntechnologies can be used to present the appearance that one is following rules\nwhile wholly disregarding them, the presumed veracity of algorithmic knowledge\nproduction falls apart. The ability to understand and shape the world of\nappearances soon becomes a key area of competition, where the state can only\nregulate those companies whose engineers and programmers fall far enough\nbehind to get caught.\n\nEven beyond direct attempts at duplicitousness, the rise of algorithmic\ncapitalism has created an economy where statistics and algorithms are more\nefficient at creating new realities (e.g., the virtual world of derivatives\ntrading, the platforms of social media and contract labor, and the private\nsocial world of filter bubbles) than they are at representing a\nworld.[13](19_Notes.xhtml#chifnr13) These methods of “re-simulation” frame\neconomic decision making in a distinct light, one where the earnestness of\ncapitalist idealism is usurped by threats from all sides.\n\nStatistics, Metaphysics and Capitalism\n\nUnderlying these changes in economy are advances in statistics, algorithmic\nlogic and computation. Society is becoming increasingly algorithmic, and thus\ngoverned more and more by statistics, continuing a long trend of the growing\nuse of quantification and probabilistic analysis in the management of\npopulations, economies and life itself.[14](19_Notes.xhtml#chifnr14) In this\nnew paradigm, scientists and marketers look not to causality to predict how\nindividuals, groups and physical systems will act, but rather to\ncorrelations.[15](19_Notes.xhtml#chifnr15) To paraphrase Karl Marx’s comments\nfrom _The Eighteenth Brumaire_ , quoted at the top of this introduction: as\nfar as capital is concerned, it is now through statistics that dead\ngenerations, or at least past events, weigh on the brains of the living. To\nunderstand contemporary capitalism, one must have some understanding of\nstatistics, and to grasp statistics, one must account for how this particular\nscience is integrated into capitalist production.\n\nThere is a constant risk that technology, algorithms and digital systems are\ngiven too much agency and seen as the necessary target of political energies—a\nmisapprehension that occludes the ways in which they represent, mediate and\naccelerate larger social antagonisms. Moreover, we must elucidate the ways in\nwhich these forms of social domination give objective force to those systems;\nthat is, we must explain why it is that they are believed in the first place\nand given the agency they currently enjoy.\n\nThis book will argue that it is in statistics and probability that the\nmetaphysical force of these systems is ultimately located. So while this is,\nin a sense, a text about algorithms, digital culture and technology, its focus\nis significantly narrower. The following chapters attempt to bring to light\nthe central role that statistics and theories of probability play in shaping\nthe possibilities of these systems and tying them fundamentally to the\nfunctioning of capitalism.\n\nIn this light, it is important to note that statistics—the field that supplies\nboth the methods and interpretive framework for machine learning and\nalgorithmic society writ large—operates on two distinct levels: metaphysical\nand mathematical. On the metaphysical level, different schools of statistics\nmake diverse philosophical claims about the relationship between chance,\ninduction and knowledge: some declare probability to be a subjective measure\nof belief, while others see it as an objective frequency of a collection of\noutcomes.[16](19_Notes.xhtml#chifnr16) On the mathematical level,\nstatisticians have built atop these systems a battery of mathematical tests\nthat evaluate how this metaphysical relationship plays out according to an\nexperiment or observed data. Ronald Fisher, one of the founders of modern\nstatistical methods and experimental design, was explicit about the value of\nthis distinction between technical questions and “principles”—or what could be\ncalled metaphysical questions:\n\n> The questions involved can be dissociated from all that is strictly\n> technical in the statistician’s craft, and, _when so detached_ , are\n> questions only of the right use of human reasoning powers, with which all\n> intelligent people, who hope to be intelligible, are equally concerned, and\n> on which the statistician, as such, speaks with no special authority. The\n> statistician cannot excuse himself from the duty of getting his head clear\n> on the principles of scientific inference, but equally no other thinking man\n> can avoid a like obligation.[17](19_Notes.xhtml#chifnr17)\n\nIt is precisely on this metaphysical level that it is possible to understand\nhow revolutions in statistical epistemology, just like other technologies and\nscientific techniques, have been revolutions in production. These metaphysical\nquestions determine how data can be turned into knowledge and action. Though\nmath often presents itself as a closed system that is relatively stable, to\nsome even a universal language, when it touches on the actually existing\nworld, this relationship becomes open, socially contingent and in flux. This\nunstable relationship between world and number is especially apparent in the\nrealm of statistics and probability, where these shifts both determine and are\ndetermined by social and economic formations.[18](19_Notes.xhtml#chifnr18)\n\nJust as Taylorist scientific management revolutionized industrial production\nat the turn of the twentieth century, the “inference revolution” has\nrevolutionized the production of knowledge and abstraction from data in the\nlate twentieth and early twenty-first.[19](19_Notes.xhtml#chifnr19)\nStatistical methods have become central to the functioning of modern\ncapitalism by producing profitable reserves of knowledge via measures of\ninference: they have turned manufacturing data into a revolution of just-in-\ntime logistics, given rise to high-frequency trading, and financed the modern\ninternet with targeted advertising.\n\nConcurrent with statistics’ ability to revolutionize production, they also\nplay a metaphysical role in capitalism, providing a science and method for\nturning knowledge into action. With this ability, they have been granted a\ncertain force, a power to make things appear as necessary and objective, which\nthey lend daily to the algorithmic systems that control much of contemporary\nlife. The chapters that follow are an attempt to account for this force that\nappears to be the non-locatable force of reason itself—one that has the\nability to compel individuals to labor and live in distinctly different ways\nbut increasingly at the behest of probabilistic systems that seek to maximize\nprofits for capital.\n\nThe Ideal Coin\n\nCapitalism and statistics are much more than accidental partners in this\ndigital economy. And nowhere is this metaphysical mutual implication more\nclear or succinct than in their use of an “ideal coin” to relate individual\nworkers, commodities and data to universal principles. In capitalism, this\ncoin takes form as money, serving as a direct representative of value, always\nand everywhere exchangeable for its equivalent in terms of a commodity or\nother coins. If the real, physical coin wears away so much that the value of\nits metal noticeably decreases, its guarantor (the state) must be willing to\nreplace it with a new one in order to maintain its ideal value.\n\nLikewise, for frequentist statistics—an approach whose specifics we will\nreturn to at length—the ideal coin, whose individual flips have no impact on\neach other, serves as a conceptual support for probability, the measure that\nlays the foundation for statistical thought and its ability to relate data to\nuniversal inference. This timeless and ideal coin allows one to imagine a\nnear-infinite series of flips, whose frequency of heads stays constant and\nthus becomes the probability of such an outcome. According to frequentist\nthought, probability merely represents the long-run frequency of such an\nimaginary instrument.\n\nNeither the universal equivalent of money in capitalism nor the ideal coin of\nfrequentist statistics exists in this world. Rather, like mystical, imagined\nactors, both types of coins operate in the nether, suturing metaphysical\ncategories of value and probability to the material world. Money allows us to\nknow the world through price. An ideal coin allows us to calculate probability\nthrough an ideal of repeatability. Both are what Sohn-Rethel calls “real\nabstractions” that facilitate abstract and productive ways of understanding\nthe world according to each coin’s ideal contours; these are ideal, but also\nreal, being so close at hand, represented by actually existing\ncoins.[20](19_Notes.xhtml#chifnr20) Frequentist statistics and capitalism\nsuccessfully generated a regime of knowledge based on this imaginary invention\nthat allows us to think of the world as objective.\n\nWhile the early and mid twentieth century saw attempts to find an objective\nground for statistics, over the last few decades this objective view of\nstatistics has waned as proponents of so-called Bayesian approaches have\nabandoned the objective terrain established by an ideal coin in favor of a\nsubjective measure—one that starts its calculations with a guess and then\npersistently updates probabilities as new evidence is gathered. In a Bayesian\nworld, there is no ideal object, but only the experience of a subject who\ncollects data. As we will see, Bayesian methods are ascendant, and the machine\nlearning models built upon them are now used to regulate everything from high-\nfrequency trading to global supply chains.[21](19_Notes.xhtml#chifnr21)\n\nThis movement from objective to subjective foundations for knowledge is\nrevolutionizing the metaphysical production of the world—shifting the very\nfoundations of scientific knowledge—and with it the material production of\ninformational capitalism. It is a transformation of knowledge into a form that\nis more fluid, local and given over to market dynamics. Yet, this\nsubjectification of knowledge does not overturn the force of objectification,\nnor does it negate the power of its real abstractions; rather, it provides it\na specifically capitalist form that does away with any solid ground or\nlocatable origin, replacing the scientist and her desire for knowledge with\nthe demands of the market.\n\nIn this revolution, statistics and capitalism are intimately related in their\nmetaphysical goals and underlying work: both attempt to transform individual\ndata into universal laws. Whether this work is carried out for the sake of\nprediction of future stock returns or the creation of a market-based economy,\nboth statistics and capitalism magically transform the local, contingent and\nindividual into the general, universal and global. This magic trick, while\nclearly material, real and productive, is still metaphysical and can never be\nfully reduced to mere physicality.[22](19_Notes.xhtml#chifnr22) Statistics and\nalgorithms, like all knowledge, are founded on an objectified set of beliefs\nabout what is equivalent—or, more accurately, what is made computable through\nprobability, ratio or equivalence.\n\nThe heterodox Marxist philosopher Moishe Postone says of capitalism:\n\n> The result is a historically new form of social domination—one that subjects\n> people to impersonal, increasingly rationalised, structural imperatives and\n> constraints that cannot adequately be grasped in terms of class domination,\n> or, more generally, in terms of the concrete domination of social groupings\n> or of institutional agencies of the state and/or the economy. It has no\n> determinate locus and, although constituted by determinate forms of social\n> practice, appears not to be social at all.[23](19_Notes.xhtml#chifnr23)\n\nThese real abstractions, as measures both of the economic value that labor\nproduces and the scientific value of a hypothesis, act as a marker of this\nform of rationalization and its lack of material locus. One can believe or\nhope however they like, but at the end of the day, it is by the dictates of\nthis objectified force that both capitalism and statistics measure our\nsuccess, and hence our ability to survive. This non-locatable impulse, both\nfor the production of knowledge and value, is what constitutes what Postone\ncalls “abstract domination.” It is through these non-locatable vectors of\nabstract domination and control that algorithmic systems and statistics shape\nthe world today.\n\nThis does not mean that capitalism or statistics do away with prior concrete\ndomination based on race or gender; rather they use it to justify such\ndomination in terms of abstract vectors and maintain them in concrete form.\nFor instance, an individual’s inability to secure a mortgage or insurance can\nbe used to create segregation in the form of redlining, concrete domination\nthat is further enforced through targeted police violence. Scholar and\nprofessor of African and African diaspora studies, Simone Browne, in detailing\nthe origins of modern surveillance in the history of the enslavement of Black\npeople, demonstrates how capitalism and technologies of control do not escape\nthe violent and racist histories that produce them, and instead tend to\nreproduce and reconfigure the regimes of legibility from which they are born.\nBrowne argues that when “technologies, in their development and design, leave\nout some subjects and communities for optimum usage, this leaves open the\npossibility of reproducing existing\ninequalities.”[24](19_Notes.xhtml#chifnr24)\n\nThus, while algorithms and machine learning may change the speed and nature of\ncomputation, they are ultimately bound to reproduce extant societal systems of\nvaluation and violence. So, if we are to resist the inequities of algorithmic\nlogic, it cannot be only at the level of technology, nor exclusively at the\nlevel of metaphysics. Abstract and concrete forms of domination interact and\nreinforce each other, just as classed, racialized, gendered and imperialist\noppression and violence interact.[25](19_Notes.xhtml#chifnr25) However, while\nefforts to resist and rearrange the forces of capitalist and statistical\nmetaphysics can open new possibilities, such a shift alone will solve\nnothing—and is likely impossible.\n\nToward a New Objectification\n\nMore than ever, it is the very concept of the political subject that is\ndefeated by digital technologies: hope for a “Twitter revolution” has given\nway to accusations of political meddling by foreign powers, and the promise of\nfree and accessible information has been replaced with government surveillance\nand censorship.[26](19_Notes.xhtml#chifnr26) Since Marx, if not earlier,\nradical political theory has tended to rely upon the existence (or creation)\nof a revolutionary subject that would use some combination of will, position\nand knowledge alongside a certain force of history to overthrow extant forms\nof power and oppression. But today, access to transformative politics founded\non a voluntaristic subject seems increasingly unfathomable. Industry lobbyists\nwith carefully crafted narratives, a twenty-four-hour news cycle that passes\ntoo quickly to allow organized political response, targeted advertisements,\nand the ease with which corporations move production elsewhere at the hint of\nnew labor demands are all tactical threats to the prospect of any contemporary\nradical political project.\n\nIn traditional Marxist theory, the proletariat as a class constitutes this\nrevolutionary subject. To paraphrase the last lines of _The Communist\nManifesto_ , workers have only to unite to become a collective revolutionary\nsubject. While there have been endless debates around the specific nature of\nthe revolutionary subject, the central role of this subject to revolutionary\nthinking is beyond doubt.[27](19_Notes.xhtml#chifnr27)\n\nBut despite the centrality of this dream of an efficacious political subject,\nthe subject of radical political theory, especially in Western Enlightenment-\ninspired varieties of communism, has been in crisis since at least the 1960s,\nif not from its very origins, which were themselves often an attempt to\nuniversalize a very specific white male British subject in the form of the\nproletariat.[28](19_Notes.xhtml#chifnr28) The struggles of student radicalism\nin the late ’60s; the emergence of non-class-based liberal movements (e.g.,\nthe environmental movement, anti-war movement and human rights organizations);\nthe recognition that this ideal of a universal subject was likely just a\nWestern white male fantasy; the realization of the violent excesses of\nStalinism and Maoism; the collapse of the Soviet Union; and the “return” of\nethno-national conflict and religious fundamentalism have all served to call\ninto question the viability of a unified political subject who could foment\nglobal revolutionary change.\n\nThe rise of algorithmic systems that isolate individuals within their own\nunique “filter bubble” threatens to further mitigate the possibility of an\neffective political subject.[29](19_Notes.xhtml#chifnr29) Today, the\nrevolutionary subject is beset simultaneously by an algorithmically fragmented\nreality and an intensely managed digital control. While the former modulates\nindividuals’ reality by data-driven algorithms that personalize the news they\nread, prices for products, and what healthcare they have access to, the latter\nappears everywhere that digital systems are used for control: it regulates\nworkers in the factory, students in the university, prisoners (even after\ntheir release), and immigrants both between and within countries.\n\nMany scholars and activists have detailed how these technologies control\nindividuals and reinforce existing inequalities, yet it seems increasingly\nunlikely that we will overcome the larger political and economic forces that\noperationalize these technologies.[30](19_Notes.xhtml#chifnr30) The Western\nMarxist and revolutionary dream of dereification—a formula of Georg Lukács\nwherein the task of individuals was to understand themselves and history—has\ngiven way to deep and prevailing cynicism.[31](19_Notes.xhtml#chifnr31) No\nsolidarity seems to hold together, and the only possible progressive politics\nappear as mere hospice for our chronic ecological and social ills. While the\ncauses of the fracturing of a universal political subject are many—and its\nexclusions and shortsightedness likely mean such a project was never\nfeasible—the ability for technology to modulate, divide and distract offers\nboth a metonymy and a ground to theorize this much-larger problem.\n\nThere are many who argue that creative power, or force of will, can produce an\ninviolate subject who stands outside the cold, calculating power of\ncomputation. From Tiqqun’s resistance against the “cybernetic hypothesis,” to\nFranco “Bifo” Berardi’s celebration of the “insolvency of language,” to Jodi\nDean’s return of the party, numerous theoretical and philosophical projects\nhave attempted to rescue some sort of subject able to resist the ravages of\nmodern capitalism.[32](19_Notes.xhtml#chifnr32) It is not my intention to\ndisparage these endeavors; on the contrary, I hope they succeed. Yet there is\na serious threat that the forces of oppression confronted by humanity are all\ntoo capable of absorbing and managing the disturbances such creative,\nuncountable subjects portend.\n\nIn this vein, the communist collective Endnotes summarizes the situation well,\nsuggesting that no unifying force currently exists to hold together a\nrevolutionary political coalition: “At present there seems to be no class\nfraction—whether ‘the most strategically placed’ or ‘the most oppressed’—whose\nstruggles express a general interest. At the same time, attempts to conjure up\na new unity from this diversity by simply renaming it as ‘multitude’ or\n‘precariat,’ for example, merely gloss over this fundamental problem of\ninternal division.”[33](19_Notes.xhtml#chifnr33) In sum, the contemporary\nlandscape of radical politics is so fragmented—and its potential subjects so\ndivested from the “general interest” of collective action—that no group could\noperably achieve what was expected of the proletariat in classic Marxist\ntheory. Moreover, as Postone has argued, “attempts to rescue human agency that\nposit historical contingency abstractly and transhistorically, bracket and\nveil the existence of historically specific structures of domination. They are\nthereby, ironically, profoundly disempowering.”[34](19_Notes.xhtml#chifnr34)\nThe very structure of this subject appears to guarantee its undoing. Today,\nthe revolutionary subject is under siege and in doubt. Perhaps the\npsychoanalyst Jacques Lacan put this doubt most cruelly and succinctly when he\nsaid to the French students of May ’68: “What you aspire to as revolutionaries\nis a master. You will get one.”[35](19_Notes.xhtml#chifnr35)\n\nIn the face of the subject’s foreclosure, others have abandoned this\nsubjective pursuit in favor of an objective ground for\npolitics.[36](19_Notes.xhtml#chifnr36) Yet while many well-meaning individuals\nhope to reaffirm the solidity of the world and facts atop the bedrock of\nscientific belief, this objective position tends, in its desire for facts, to\npresent the conditions of the capitalist world as humanity’s only\noption.[37](19_Notes.xhtml#chifnr37) In turn, its objective ground ends up\ndepoliticized or, worse yet, used to reinforce current white-supremacist and\ncapitalist understandings of the world. All too often, individuals seeking\ntechnical solutions to social problems reproduce the very systems they attempt\nto work against.[38](19_Notes.xhtml#chifnr38)\n\nStuck between these two poles of what we might—at the risk of misusing the\nvocabulary of dialectic materialism—call “objective” and “subjective” leftism,\nthe most politically efficacious path forward may be neither a directly hybrid\nroute nor a privileging of one specific pole. Instead, we must work directly\non the torsion between the objective and the subjective. Marx tells us,\nespecially in _Capital_ , that capitalism, and with it politics under\ncapitalism, is founded upon a metaphysical process wherein humanity’s social\nand economic world—the world of value and wage labor—is twisted and presented\nas the objective, unchangeable state of things. While the entire production\nprocess is social, and hence an agreement between subjects—in essence\nsubjective—it does not matter what we think or desire; if we do not own\ncapital, we must sell our labor on the market at the going rate. Philosophy be\ndamned, labor is worth what the market is willing to pay.\n\nMarx gives the name “objectification” to this metaphysical process, by which\nsocially negotiated relationships between people are made to appear as\nrelations between objects, and hence as objective. Instead of focusing on what\nis _objectively true_ —the tactic of the above objective leftism—Marx’s theory\nof objectification allows us to interrogate the process by which certain\nthings appear objective: the moment when decisions of social actors cease to\nappear as decisions and begin to appear as natural and unimpeachable.\n\nLet us be clear: while the two are intimately related, the history and future\nof objectification is not the history of objectivity. Those histories have\nbeen well written elsewhere.[39](19_Notes.xhtml#chifnr39) Our primary concern\nis with the way that objects come to account for human affairs, managing,\nrecording and presenting our social and intersubjective interactions as part\nof a larger framework of rationality and objectivity, and with the force this\nmanagement is given. Objectification is, in short, the way in which social\nproduction is inserted into the larger history of scientific (or, prior to\nthat, theological) objectivity.[40](19_Notes.xhtml#chifnr40)\n\nTo understand the nature of contemporary capitalism—specifically its ability\nto present a complex system of social exploitation as objective and natural—it\nis imperative to analyze how statistics and the algorithmics of machine\nlearning take up and continue the commodity’s legacy of mediating society and\neconomics “behind our backs.” It is no longer enough to simply comment on the\nimportance of data and computing to capitalism. Nor will it suffice to merely\nexplain the mathematics behind relevant algorithms and excavate the sets of\ndata used to train them. Instead, these algorithms must be understood and\naddressed as systems that take up the social work of objectification, thinking\nfor us and managing our affairs _objectively._ Just like the commodity and its\nvalue, disbelief in them does not make them any less powerful; and, this fact\nis what must be accounted for.\n\nWe sit at a moment where the monstrous objects of these algorithms matter a\ngreat deal. Today, it is precisely computation—from climate change models to\nthe use of machine learning to provide personalized shopping, news and dating,\nas well as distribute state and corporate violence—that determines our current\nobjective social reality under digital capitalism. As algorithms and data\nincreasingly produce both scientific knowledge and commercial value, taking on\ntasks ranging from the distribution and extraction of value in high-frequency\ntrading to the management of just-in-time supply chains to the “disruption” of\ntraditional ossified industries, old forms of material and metaphysical\nproduction are rent asunder.\n\nNow algorithms are built on statistics that leverage and produce this force of\nobjectification, tying data to decisions and suturing the particular to the\nuniversal, the subject to the object, and the material to the metaphysical.\nAgain, the work that these algorithms do—work that is fundamentally built on\nadvances in statistics and probability—is simultaneously material and deeply\nmetaphysical. While they use magnetic bits for practical purposes, calculating\nhow to move capital and physical goods around the world, they also redefine\nthe world in uniquely abstract ways, turning data about that world into\nconcepts, predictions and inferences. These freshly minted, algorithmically\nproduced abstractions then become the new indices of scientific and commercial\nknowledge.\n\nIn this light, a revolutionary theory that works on objectification, rather\nthan on subjects or objects, must focus directly on the relationship between\ncomputation and the production of knowledge and value. In order to account for\nthese objectified forces that determine political possibilities, we must\nconfront not merely the economic processes governing these relationships but\nthe structures of objectified belief that allow computational systems to work\nbehind our backs. We seek, then, a theory of the revolutionary object—or, more\nprecisely, a theory of a revolutionary objectification, of how it may be\npossible, in the absence of a single, unified subject, to revolutionize social\nrelations. In short, we are looking for a way to revolutionize “what counts”\nwithout making recourse to either an imagined totality or a transhistorical\nsubject.\n\nTo be clear, the adoption of such a position does not entail a wholesale\nopposition to math, statistics or science. Quite the opposite: these are\nnecessary components of radical politics and thought in the twenty-first\ncentury. “A post-statistical society,” writes political economist William\nDavies, “is a potentially frightening proposition, not because it would lack\nany forms of truth or expertise altogether, but because it would drastically\nprivatise them.”[41](19_Notes.xhtml#chifnr41) To simply refuse statistics as a\ncapitalist tool would leave humanity subjected, all the more so, to these\nmathematical decisions.\n\nWhile decades of Marxist thought have been dedicated to the concept of\n“dereification,” or revealing what is “truly” happening in these objectified\nrelations, another path is possible. We must start from the fundamental\nimpossibility of some final revelation about the ultimate nature of complex\nalgorithmic models, or about the opaque nature of proprietary systems like\ncapitalism. We should refuse the simple assumption that objectification—and\nthus abstraction itself—is necessarily bad or unacceptably reductive. As a\nprocess, objectification lies at the heart of the very nature of all\ntechnology: from the moment a stream of water touched the buckets of the first\nwaterwheel, the forces of nature came to be objectified and to objectify human\nthinking about the world. Humanity, now blessed with waterpower, had moved the\nbaseline of possible thought from a river and its currents to the mill and its\naxis. In the centuries since, such abstractions about the world and its\npotential have run from the undomesticated kinetic energy of the river to the\nrivers of plasma that flow through experimental nuclear fusion reactors.\n\nAfter centuries of advances in thermodynamic abstraction, statistically based\ntechnologies like machine learning now form the material base of\nobjectification in our current knowledgedriven economy, where data of our\neconomic, political and social worlds are reflected back to us as objective\nmeasures: the stock market is up; here is the coat you are likely to buy; and\nso on. The knowledge and abstractions these technologies produce are not wrong\nor incorrect, any more than the price of corn at market is wrong. Rather, we\nmust understand how algorithmic “objects” function, what social relations they\nmake objective, what form this objectivity takes, and the possibilities of new\nobjectifications that lie in their contradictions. We do not yet know what\nform these new objectifications could take, but—much like the commodity form\nreshaped the world—there is revolutionary value in seeking out these new\nobjectifications; current debates, contradictions and advances in statistics\nand machine learning are likely to play a central role in shaping these new\nforms. Thus, the point is not one of dereification—to show how things really\nand truly are, or to get at the secret of a metaphor or an analogy—but rather\nto show how algorithmic objects work and the displacements they\neffect.[42](19_Notes.xhtml#chifnr42)\n\nPolitical Economy\n\nHere, there emerges an immensely important set of questions that concern the\nmeans through which the global economy functions, as well as the extent to\nwhich digital systems have fundamentally altered the function of that economy.\nWhile these questions take on a multiplicity of valences and have been\naddressed in a variety of fields, including in literature on digital culture\nand political economy, three elements stand out that bear directly on this\nwork. The first is an ongoing debate about the existence and nature of “free\nlabor” on digital platforms, and more generally about various forms of\nimmaterial labor that work on data, information and knowledge rather than on\nthe production of commodities.[43](19_Notes.xhtml#chifnr43) In this regard, it\nis especially important to remember that these forms of labor, often thought\nof as digital, are part of a much-larger, evolving capitalist system; for\nexample, understanding physical hardware must include the energy it requires,\nthe extraction of raw materials and the continued reliance on dangerous and\nexploitative industrial production, especially in the global\nSouth.[44](19_Notes.xhtml#chifnr44)\n\nA second, related question concerns the nature of value extraction: that is,\nprecisely how it is that digital systems produce, extract, valorize or aid in\nthe production of value. Building upon the theoretical legacy of Italian\nautonomism and _operaismo_ (workerism), a succession of scholars have greatly\nexpanded the definition of value production to include “immaterial labor” by\nutilizing the concept of the social factory, wherein all sorts of non-waged\nlabor still produce value for society.[45](19_Notes.xhtml#chifnr45) Much of\nthis work has taken inspiration from Marx’s claim, in his famous “Fragment on\nthe Machine,” that as the “general intellect” develops, it allows increasingly\nself-productive machines to produce value\ndirectly.[46](19_Notes.xhtml#chifnr46)\n\nThird, a substantial amount of work on the rise of data and information\ntechnology has attempted to periodize the history of capitalism and global\nsociety in general, and to explain the ways in which digital capitalism\ndiffers from industrial capitalism. A range of theorists have argued for\nvarying degrees of a “break” or transition from earlier forms of capitalism.\nThey have variously referred to this new form as surveillance capitalism,\ncognitive capitalism, network society or platform capitalism. Some identify\nthe shift as a control revolution, while others frame it as the next\nindustrial revolution or a data revolution; still others contend, on the\ncontrary, that there has been no major break in the history of production at\nall.[47](19_Notes.xhtml#chifnr47)\n\nWhile all three of these debates around the nature of twentiethand twenty-\nfirst-century political economy are of immense importance, to address them\ndirectly and systematically would require a fundamental rethinking of all\nthree volumes of Marx’s _Capital_ and a synthesis of a vast amount of work\nthat has followed since their original writing. Such a project would likely\nrequire at least as many volumes, if not more. Thus, this book is not so much\na description of the exact nature of contemporary political economy at this\nprecise moment as it is an argument for the importance of political economy;\nit is an attempt to show the depths to which these questions of political\neconomy shape the very production of knowledge and the ways in which knowledge\nproduction shapes political economy. While certain commitments and\nunderstandings will likely be noticeable throughout, it is hoped that\nregardless of how one understands modern political economy to function, what\nfollows will make apparent the importance of these larger questions to the\nvery ground of what constitutes knowledge today.\n\nIt is clear, especially given that machines now produce knowledge directly,\nthat political economy does not simply take up already-existent technologies\nand put them to use, or even develop them; rather, these technologies are\nthemselves techniques for shaping and changing political economy. Thus, as\nPostone argues, the very shape of political economy is itself plastic and\nchanged by historical circumstances: “Inasmuch as Marx analyses social\nobjectivity and subjectivity as related intrinsically, this focus on the\nhistorical specificity of his categories reflexively implies the historical\nspecificity of his theory. No theory, within this conceptual framework, has\ntranshistorical validity. Rather, the standpoint of critical theory must be\nintrinsic to its object.”[48](19_Notes.xhtml#chifnr48) In sum, even if it\nchanges rapidly or over centuries, it is the very nature of political economy\nthat is at stake in the process of objectification. This book aims to trace\nthis relationship in terms of the ways it is changed by statistics, opening\nquestions of political economy rather than attempting to answer them\ndefinitively.\n\nThat said, the final chapter of this book will make some arguments that\naddress these questions, with a focus on the development and enclosure of the\ngeneral intellect. While these arguments are built on implicit understandings\nof the function of machinery and social labor that align with some of the\nautonomist readings of digitally mediated production, the larger threat of\nthis enclosure to the production of shared social knowledge remains, whether\none accepts this or another account of political economy.\n\nThus, regardless of whether automation produces value directly or merely\ndrives this “treadmill dynamic,” as Postone calls it, statistics, probability\nand automation now play an undeniably important role in contemporary\ncapitalism, one that as we shall see ties the production of knowledge to the\nexchange of value. Information and media scholar Nick Dyer-Witheford lays out\nthe stakes well:\n\n> The conjunction of automation and globalization enabled by information\n> technology raises to a new intensity a fundamental dynamic of capitalism—its\n> drive to simultaneously draw people into waged labour and expel them as\n> superfluous un- or underemployed. This “moving contradiction” now manifests\n> as, on the one hand, the encompassing of the global population by networked\n> supply chains and agile production systems, making labour available to\n> capital on a planetary scale, and, on the other, as a drive towards the\n> development of adept automata and algorithmic software that render such\n> labour redundant.[49](19_Notes.xhtml#chifnr49)\n\nStatistics and probability provide the fundamental mathematical logic of many\nof these aspects of contemporary capitalism, from targeted advertisements to\nthe management of production, distribution and consumption, as well as the\nexpulsion of labor from these activities.[50](19_Notes.xhtml#chifnr50)\n\nMoreover, beyond these questions of political economy, there is an extensive\nand growing critical literature on algorithms and quantification in general,\nmuch of which engages in what we could call a biopolitical critique of these\ntechnologies, drawing on the work of French philosopher Michel\nFoucault.[51](19_Notes.xhtml#chifnr51) While there is significant nuance to\nthe argument of and within this critique, it could be summarized thus:\nstarting with early European interest in demography, public health, police,\nschooling and prisons, modern states have attempted to manage populations by\nquantifying them and measuring them against some idea of what is normal. So,\ndeviancy in the factory, at school, at home, at the border and beyond are\nmeasured and thus managed.[52](19_Notes.xhtml#chifnr52) A number of scholars,\nsuch as Simone Browne and Alexander Weheliye, have criticized and built upon\nthese arguments to demonstrate the centrality of racism, and especially the\nAtlantic slave trade, to the development and deployment of biopolitical\ncontrol.[53](19_Notes.xhtml#chifnr53)\n\nMany explorations of algorithms and big data have followed along similar\nlines, critiquing the ideologies, power dynamics and decision-making processes\nbehind these algorithms. These arguments, ranging from radical criticisms to\nmore liberal calls for regulation, demonstrate the ways in which these systems\nbuild upon the longer history of the statistical management of\npopulations.[54](19_Notes.xhtml#chifnr54) Some of the exemplary work tracing\nthe injustices of algorithmic society, such as that of Safiya Noble, Virginia\nEubanks and Cathy O’Neil, tends both to demonstrate the ways in which\nalgorithmic society builds upon earlier modes of statistically informed\ndomination and to argue that massive stores of data and computing power have\ncreated a new era of individualized manipulation.[55](19_Notes.xhtml#chifnr55)\n\nThese works are powerful and insightful, but this present text aims to take a\nslightly different course. Rather than exclusively explicate how algorithms\nwork or provide extensive examples of what they have wrought, these chapters\nexplore the metaphysical and economic force through which they claim to turn\ndata from the world into knowledge _about_ that world, and the particular sets\nof actions such processes require. Thus, the book’s central questions are less\nabout how statistics function as an ideology, in the classic sense, or how\ntheir methods serve as an analogy or organizing principle for\nneoliberalism—though certainly those are important and relevant questions.\nInstead, it aims to address how statistics and theories of probability are\ndirectly productive, that is, how they tie together the production of\nknowledge in contemporary capitalism at a metaphysical level. In this sense,\nits approach is markedly different from the aforementioned texts documenting\nthe twentieth- and twenty-first-century turn toward quantification, which have\nrarely focused, or even commented, on the shift from frequentist to Bayesian\napproaches, a shift that has been central to the growing use of probability to\nmanage governments and economies.[56](19_Notes.xhtml#chifnr56)\n\nWhile there is much to commend in a critique that centers on statistics,\nprobability and their implementation in the form of algorithmic software as an\nideology, a cultural or superstructural form, the present text is concerned\nwith the way these technologies function as a “real abstraction,” that is, an\nabstract form that functions on the level of economy and value. The autonomist\nMarxist Franco Berardi explains money and language in such terms: “Language\nand money are not at all metaphors, and yet they are immaterial. They are\nnothing, and yet can do everything: they move, displace, multiply, destroy.\nThey are the soul of Semiocapital.”[57](19_Notes.xhtml#chifnr57) Whether one\nbelieves we are in the age of semiocapital, or just capital, the point remains\nthat statistics functions primarily in the same way: not as a metaphor for\nanything, but rather as an immaterial thing that can do everything. This is\nnot because statistics is fundamentally ideological in some classic sense, nor\nbecause it provides a metaphor for thinking neoliberalism (although it does\nboth of these things), but rather because it functions as a machine that turns\ndata into both value and scientific law.[58](19_Notes.xhtml#chifnr58)\n\nIn the shift from frequentist to Bayesian statistics, where statistics move\nfrom the discernment of knowledge to the production of action, these\nmathematical methods function directly on the level of value. They cease to\naid in the selection of what is true, pointing instead to which course of\naction will produce the most profit. In this way, statistics come to be\nobjectifying, in the sense that they allow one to understand what is\n“objectively” true: which decisions the market will most likely reward and\nwhich it will punish. They come to think for us, revealing our social\nworld—not in so much as it is social, but rather in so much as it is presented\nback to us as the will of objects. This is not to say that it is\nnonideological, but rather that its force derives, at least in part, from its\nability to function on the level of value directly, on that world of\nobjectified exchange.\n\nRevolutionary Mathematics\n\nThis book draws a certain inspiration from George Berkeley’s 1734 text _The\nAnalyst; or, A Discourse Addressed to an Infidel Mathematician_ , _wherein It\nIs Examined whether “The Object,” Principles,” and “Inferences” of Modern\nAnalysis Are more Distinctly Conceived, or more Evidently Deduced, than\n“Religious Mysteries” and Points of Faith.”_ In a tirade against calculus—a\nstory more fully told in Chapter 2—Berkeley decried those who dared to make\nuse of the infinitesimal that was both there and, in its vanishing, not there.\nFor Berkeley, a man of God, the only knowledge was that of God, and was thus\ngrounded by God. For the infidel, knowledge of calculus was groundless,\navailable to be made and remade, entailing concepts built on concepts, and\nabstractions built on abstractions. While the mathematics underneath the\ncalculation of these infinitesimals in calculus “worked,” they were not\nconsidered reasonable. According to his Christian-based rationality, they were\nto be distrusted as ungodly, and those articulating them as infidel.\n\nCommodity exchange and statistical knowledge production share this ability to\nrefashion the world in new, metaphysical forms, finding knowledge and value\nthat exists neither in the materiality of the world, nor in the realm of some\nChristian heaven. Capitalism “discovers” value in commodities that allow for\nexchange, refashioning that value as a quantitative rather than qualitative\ndifference. Statistical processing and machine learning calculate\ncorrelations, probabilities and ratios, refashioning the world into insights\nderived from data. It is through a fundamentally metaphysical act of “making\ncomputable” that statistics and capitalism objectify the world; it is through\nmaking labor time equivalent with itself and economic value, or making data\nequivalent with other data and ultimately with inferences, that allows these\nprocesses to work.\n\nBut today, both statistics and capitalism are in crisis—a crisis that derives\nfundamentally from political economy and its metaphysical support. This crisis\ntherefore demands a revolutionary approach to algorithmic knowledge. To effect\na change in the production of our current world will require us to work on\nboth the political and the metaphysical levels of knowledge production. It\nwill require an intervention in the objective production of new forms of\nequality and hence in computation, and a reconfiguration of the means by which\nwe make the incommensurate commensurate. In short, it will require the work of\nan infidel mathematics, a revolutionary mathematics, to create new forms of\nexchangeability that allow for the development of knowledge beyond and after\ncapitalism. Currently, both commodity exchange and statistics equate things,\nbut they do so only in the service of capital accumulation. It is already too\nlate to simply point out the inequity of these calculations; instead, we must\nrework the metaphysical equatability that undergirds exchange itself—the\nunreasonable ground of our very reason.\n\nThus, the work of a revolutionary mathematics is not exclusively or even\nprimarily that of the mathematician or the specialist. Rather than directly\ntechnical, the work of this mathematician is metaphysical: their task is to\nrework the meaning of what is calculated and to figure out how to live and\nreason under and toward a new computability and different forms of exchange.\nWhile such a mathematics must not shy away from technical questions, it must\nbe concerned with the process of reason itself. In this regard, it is nearly\nimpossible to say this or that form of thinking or working will be successful;\nnevertheless, by tracing current contradictions, experimenting, creating and\nexploring new possibilities of objectification, the approach may one day bear\nfruit in the effort to envision—and make—a postcapitalist future.\n\nThe aim of a revolutionary mathematics is to denaturalize statistics by\ndemonstrating the social, economic and political stakes that are encoded in\nits functioning—in short, to intervene in metaphysical determinations. But\nthis is not to deobjectify (or dereify) our situation, to end false\nconsciousness by showing what “is really going on”—a task that appears to work\nfine for critical discourse but has a way of getting lost as soon as we find\nourselves returning to the market or our computers. Statistics and machine\nlearning can be put to other ends. Another objectification is possible; the\nultimate task of a revolutionary mathematics is to seek this out.\n\nThis book is, in essence, about algorithmic society and informational\ncapitalism. But in attempting to touch the metaphysical heart of contemporary\nglobal production, it also wrestles with philosophical and historical\nquestions about the production of knowledge and the statistical and\nprobabilistic forms that support that production. The first part traces the\nepistemological shifts that a data-driven economy have wrought. The second\npart provides a historical and philosophical overview of frequentism, the form\nof statistical inference that reigned throughout most of the twentieth\ncentury. The final part traces the Bayesian revolution in statistical and\ncomputational methods, along with the contradictions and possible futures it\nopens.\n\nThis short text is far from enough to revolutionize the way machine learning,\nthe algorithmic production of knowledge and statistics function under late\ncapitalism. However, I am hopeful that it can, at the very least, open for\ncritique a series of problems that plague capitalism and data-driven knowledge\nproduction, and hint at the path future work could take toward a revolution in\nthe production of both knowledge and value.\n\n\n[PART I](04_Contents.xhtml#ch3)\n\n[Ghosts of Departed  \nQuantities](04_Contents.xhtml#ch3)\n\n> So may be Henry was a human being.\n>\n> Let’s investigate that.\n>\n> … We did; okay.\n>\n> He is a human American man.\n>\n> That’s true. My lass is braking.\n>\n> My brass is aching. Come and diminish me, and map my way.\n>\n> —John Berryman, “Dream Song 13”\n\n\n[Chapter 1](04_Contents.xhtml#ch4)\n\n[Automating Knowledge](04_Contents.xhtml#ch4)\n\nIn 2004, with Hurricane Frances charging toward the coast of Florida,\nWalmart’s chief information officer, Linda Dillman, asked her team of data\nscientists to forecast the effects the hurricane would have on sales. Based on\nprior data, the team came up with a number of insights. In addition to\npredictable consumption increases like canned food, water and flashlights,\nthey discovered that strawberry Pop-Tarts sales increase to seven times their\nnormal rate immediately prior to a hurricane.[1](19_Notes.xhtml#ch1fnr1)\n\nThe data Walmart used in its model was derived from Hurricane Charley, a storm\nthat had struck the peninsula only a few weeks prior. Using this as a test\ncase, Dillman’s team was able to contrast consumption patterns between stores\nin the path of the hurricane to stores not in the path of the hurricane. And\nindeed, sales of many of the items they stocked prior to Frances based on\nthese insights sold as expected.[2](19_Notes.xhtml#ch1fnr2)\n\nWhile Walmart’s knowledge proved profitable in the near term, its statistical\ncorrelations offer little insight into underlying causal mechanisms. And, even\nif Dillman might claim otherwise, such correlations offer us little meaningful\nunderstanding of the world. Walmart’s model, built from data and\nalgorithmically produced patterns discovered in that data, centers knowledge\nproduction on correlations. Their epistemic relationship to causality is\nirrelevant: the model cares little about _why_ Walmart consumers prefer\nstrawberry, so long as a mathematically modelable relationship\nexists.[3](19_Notes.xhtml#ch1fnr3) Ultimately, what is produced by Walmart is\nparticular, local only to the conditions of the given data, and therefore\nunable to express any enduring principles. The irrelevance of Walmart’s model\nbeyond a very specific and immediate problem demonstrates a certain limit of\nalgorithmic knowledge, even if it is one with which engineers and researchers\nare rather comfortable.\n\nAll the same, these limits do not undermine the importance or value of machine\nlearning models, because they are often designed to update themselves over\ntime, offering up-to-the-minute predictions as new data comes in—a procedural\nanalog to the realities of a dynamic, capricious society of capitalist\nconsumption. With this self-correcting design, the algorithm generates not\nsome ideal, universal understanding, but instead a local and constantly\nshifting set of predictions.\n\nWhile Dillman and her team may have played some role in setting up the model\nand selecting the data, when it comes to the knowledge produced, their role is\nthat of an envoy: they know only what a statistical model tells them. And that\nmodel can only suggest, with a given probability, that consumers will likely\nbuy more strawberry Pop-Tarts prior to a specific hurricane. Due to its\ncomplexity, it is the computer-run model that in the end understands, rather\nthan the researchers. It is in this sense a nearly automatic production of\nknowledge; even if humans are involved, they _appear_ not to add intellectual\nlabor to the production. Machine learning operates by taking a large set of\ninputs and computationally determining a way to map those inputs to outputs.\nMachine learning techniques most often work by running through a set of\ntraining examples, evaluating the output, and updating themselves in order to\nbest match the training data.\n\nWe witness here, in miniature, a larger turn toward statistics, and\nprobability in particular, in the management of contemporary capitalism. The\nuse of probability to predict the most likely set of outcomes has become\ncentral to everything from logistics to advertisements to the stock market and\nbeyond. But, probabilistic analyses are notoriously bad at dealing with\nsystemic change. Frank Knight, one of the founders of the mid-twentieth-\ncentury neoclassical Chicago school of economics, used the terms “risk” and\n“uncertainty” to bring to light one critical challenge to this type of\nprobabilistic knowledge.[4](19_Notes.xhtml#ch1fnr4)\n\nFor Knight, “risk” describes a set of knowable probabilities that can be\nmanaged—for instance, the probability that someone will win the lottery, the\nsurvival rate for various diseases, or the chance that a drug will produce a\ncertain side effect. Correspondingly, “uncertainty” describes elements whose\nprobability one does not know—or those one chooses not to include in their\ncalculations. For example, when calculating the odds that a single ticket will\nwin the lottery, the equation normally does not include the probability that\nthe state or organization running the lottery will go bankrupt. While risk can\nbe calculated, uncertainty cannot. Uncertainty threatens every model, because\nthere are always dangers that lie outside the closed space of the system.\nWhile historically, capitalism has excelled at integrating newfound\nuncertainties into its social and economic processes, the powers of machine\nlearning threaten to upset this stability. For the more efficiently systems\nmanage risk, the harder it becomes to imagine and prepare for uncertainty. By\ndefinition, efficiency comes at the expense of redundancies and the type of\ndouble-checking that could make it easier to handle unexpected situations. For\nan example of this, one need look no further than capitalism’s privileging of\nshort-term profits blocking any real response to climate change.\n\nSince at least 1876, statisticians have been aware of the effects, in\ncalculations of probability, of their choices about what data to include. John\nVenn, the man whose intersecting diagram made his work famous to grade-\nschoolers, describes the origin of this problem: “Every individual thing or\nevent has an indefinite number of properties or attributes observable in it,\nand might therefore be considered as belonging to an indefinite number of\ndifferent classes of things.”[5](19_Notes.xhtml#ch1fnr5) This problem is now\nknown as the reference class problem, and describes the challenges of how this\n“indefinite number” is narrowed down, and thus defined, into something more\nmanageable. It is at its base an ontological question about what something is\nand what else falls into that category.\n\nThe reference class problem directly challenges the supposed objectivity of\nstatistics and machine learning. For example, if a patient is diagnosed with\ncancer, how that patient is defined—such as what stage their cancer\nis—determines their calculated probability of survival. Just as there is no\n“true” patient demographic, there is no “correct” way to define the world,\nonly decisions that frame that world in potentially disparate ways. While some\napproaches, especially Bayesian ones, claim to avoid the reference class\nproblem, they too choose what data is relevant to a given\nproblem.[6](19_Notes.xhtml#ch1fnr6) Whoever defines the reference class—or\nselects the relevant data that is included in an analysis—wields extraordinary\npower over the seemingly “data-driven” neutrality of statistics. A statistical\nmodel like Facebook’s News Feed algorithm, for instance, weds users to the\nvariables Facebook employed to construct that model. The conditions of\npossibilities are prefigured according to Facebook. In this way, probability\nis always political.\n\nStatistics can only operate within the closed world of the reference class or\nthe data it is given to operate on. Correspondingly, as an enclosed world\ndevelops and new statistical models describe that world with increasing\naccuracy, it becomes harder to imagine that anything might exist outside it\nand upset it. The greater one is able to manage risk, the more unlikely and\nunimportant uncertainty appears to be.\n\nOne of the great challenges to machine learning is that often, a wide variety\nof factors can explain any measured difference. In the Walmart model, for\ninstance, geography, the time of week, or even the internal temperature of\neach store may have an effect. All algorithmic systems of knowledge production\nare relative only to the data that is put into the system. This was precisely\nwhat doomed Google’s attempt to predict flu activity based on search results:\nthe model worked well for the first few years, but in 2011, something likely\nchanged in how people searched for flu information, because the model\npredicted a number of doctor visits for flu that was more than double the\nfigure reported by the Centers for Disease Control and\nPrevention.[7](19_Notes.xhtml#ch1fnr7) Again, the problem here was that\nalgorithmic models are only able to assess risk (the probability that\nsomething will happen) relative to the input data, while remaining perennially\nvulnerable to uncertainty (the inability to know\neverything).[8](19_Notes.xhtml#ch1fnr8) While this is, to an extent, true of\nall knowledge, the use of theories and the discovery of causal mechanisms help\nbuttress our trust in predictions and extrapolate beyond the data at hand; in\nWalmart’s model, the fact that the correlation discovered applies only to\nprior data means there is even less guarantee that some unforeseen uncertainty\nwill not intervene to disrupt its predictive power.\n\nWhile statistical modeling has been used since at least the 1700s to\nfacilitate mathematical descriptions of relationships between phenomena for\npurposes of prediction, machine learning goes further, allowing for the\nmodeling of significantly more complex relationships based on vast fields of\ndata. By design, it also largely abandons the hope that one could extract some\nuniversal understanding from observed relationships. Traditionally,\nstatistical models were built on relatively simple formulas for relating input\nvariables to output variables. For example, in 2004 it was reported, based on\ndata from the United States and United Kingdom, that on average, each\nadditional inch of a person’s height correlates with a $789 increase in annual\nsalary.[9](19_Notes.xhtml#ch1fnr9) While the study does not tell us why this\nhappens, we nonetheless learn something about the relationship between height\nand income. The relationship is a simple, linear one. Given only two\nindividuals’ height, the expected income difference between both can be easily\ncalculated with pen and paper.\n\nIn contrast to this ease, machine learning models generally rely on massive\nnumbers of calculations—so large it would be impossible for a human to do them\nin a reasonable amount of time—in order to train an algorithm. Furthermore,\nthese algorithms frequently account for nonlinear relationships (wherein a\nchange in one variable does not correspond to a constant change in another)\nbetween an immense sea of variables. For instance, the predicted strength of a\nhurricane could significantly relate to Pop-Tart sales, but the amount of\nthose sales drastically increases between Category 2 and 3 hurricanes, levels\noff at 4, and then declines prior to a Category 5 storm.\n\nWith enough data, machine learning algorithms are able to detect incredibly\nelaborate interactions between variables. But these interactions often become\nso complex that while the algorithm may find strong correlations and thus\n“learn” them, humans, for all intents and purposes, never\ncan.[10](19_Notes.xhtml#ch1fnr10) Unlike scientific theories that offer\nelegant mathematical descriptions of universally valid causal processes, we\nask machine learning systems something much more epistemically complex: to\ncalculate the probability of a set of outcomes over a circumscribed field of\nexamples. In response, those systems generate a set of probabilistic\nknowledges from which one can, in the aggregate, act. To allude to the\nepigraph from John Berryman, we initially determine probabilistically that\nHenry is a human being, then, with more data, that he is a human American man.\nWith that, we may claim to begin to map his way. Because these models are only\nas effective as the data input into them, the solutions and models that have\nbeen developed in the past decade tend to work well only in the relatively\nlimited domains for which they are intended. Machine learning algorithms have\neffectively classified images, predicted purchasing behavior, turned audio\ninto text, and even allowed digital assistants (such as Amazon’s Echo) to\nprovide an increasing array of voice-activated functionality. But the utopia\n(or perhaps dystopia) of an artificial general intelligence that can carry out\nabstract and complex general tasks remains far\nafield.[11](19_Notes.xhtml#ch1fnr11) Moreover, the data on which these models\nare trained—and the biased social reality that the data represents—build into\ntheir algorithmic outputs simple replications of extant\nsystems.[12](19_Notes.xhtml#ch1fnr12)\n\nBecause all data is mediated through our social world, biases cannot but\nsaturate the closed worlds of algorithmic knowledge production. Consider, for\nexample, the COMPAS (Correctional Offender Management Profiling for\nAlternative Sanctions) algorithm, designed by software company Northpointe,\nwhose results are regarded by courts and prison systems as predictive of an\nindividual’s likelihood of recidivism. In 2016, investigative journalists from\nProPublica determined that the algorithm was demonstrably racist. Based on a\nseries of questionnaires and demographic data, COMPAS identified Black\ndefendants to be at a significantly higher risk for reoffending while\nidentifying white defendants at a much lower\nrisk.[13](19_Notes.xhtml#ch1fnr13) While the above Pop-Tarts example\nchallenges how we understand knowledge in the abstract, COMPAS’s racism makes\nthe challenges of algorithmic knowledge much more concrete: algorithmic bias\nin predicting recidivism rates can have a significant impact on pretrial,\nparole, and sentencing decisions.[14](19_Notes.xhtml#ch1fnr14)\n\nThe algorithmic logics that both Walmart and Northpointe use represent a new\nepistemic version of the world, one that is private and particular. And\nWalmart is far from the only company attempting to predict their customers\nbehaviors: Target famously predicts when customers are\npregnant;[15](19_Notes.xhtml#ch1fnr15) Canadian Tire discovered that\nindividuals who bought felt pads to protect their hardwood floors never missed\ncredit card payments—while those who bought chrome skull accessories for their\ncar were almost guaranteed to default.[16](19_Notes.xhtml#ch1fnr16) Each of\nthese companies builds their own private databases of sales or arrests (and\nbuys databases from other vendors) to create microcosms of the world where\nthey can continually compute the probability of various events.\n\nWhile techno-futurists have sold a world of untold wonder in which computers\nand machines will predict our every need, both collectively and individually,\nthe reality is much more constrained and disappointing. This is, in part,\nbecause many of these systems excel at interpolation (the process of filling\nin holes in given data), while progress on extrapolation (the prediction of\nfuture, completely unknown situations) has been\nlimited.[17](19_Notes.xhtml#ch1fnr17) We will explore the social implications\nof these technologies soon enough, but in order to better grasp what is at\nstake, it is worth explicating, first, what machine learning is and how it\nworks.\n\nArtificial Neural Nets\n\nMany machine learning systems are built using the powerful, yet relatively\nstraightforward _artificial neural network_. An ANN consists of multiple\nlayers of artificial “neurons” that are connected to each other, in a\nstructure that mimics a very simplified model of the human brain. In biology,\nat the distinct risk of oversimplifying things, human beings “learn” when\nsynapses connecting their networked neurons fire between each other,\nincreasing the ease with which similar signals can travel, and thus\nremembering, those pathways. In machine learning, the basic idea is similar:\ntraining data is fed through a network, and the network attempts to discover\nthe best way to transform input values into outputs. ANNs are made up of\nmultiple layers, where each neuron in a layer connects to each neuron in the\nfollowing layer. A simple ANN can have an input layer, a single “hidden” layer\nand an output layer; more complex networks can have multiple hidden layers and\ncomplex divisions inside layers. In either case, each neuron tries to learn\nhow to evaluate the information coming from its input neurons. In this way,\nevery neuron—and the network as a whole—remembers its history in abstracted\nform, maintaining a trace of the values of what it has seen before, in order\nto create a model that can make predictions based on new data.\n\nImagine we create our own Pop-Tarts model, using data about the probability of\na hurricane, maximum predicted wind speed and predicted rainfall. With this\ndata, like Walmart, we might want to predict the increase in Pop-Tart sales\nthrough our model. Given that our input data includes probability of a\nhurricane, wind speed and rainfall, we compose our input layer with these\nthree “neurons.” And because we want to predict only the number of Pop-Tarts\nsold, we include a single output neuron representing that value.\n\nBetween the input and output layers is the “hidden” layer that allows the\nmodel to describe and predict complex relationships within the data. In our\nhidden layer, we can choose to include a certain number of neurons (see the\nfigure below).\n\n![images](images/img_p46.jpg)\n\nDiagram of a theoretical ANN to determine the relationship between rainfall,\nmaximum wind speed and the probability of a hurricane and the number of boxes\nof Pop-Tarts sold.\n\nOur five hidden neurons allow the ANN to detect different elements of our\nprediction problem. For instance, one of the five neurons could be on the\nlookout for hurricanes so devastating that inhabitants leave town rather than\nstock up on food. Thus, we would want that neuron to detect when all three\ninputs (probability of a hurricane, wind speed, and rainfall) are very high,\nand then suggest to the final output neuron to decrease the predicted number\nof Pop-Tarts sold. It would be rare, in practice, to have a neuron that so\nclearly detects something easily describable in human language; indeed, such\nneurons detect patterns in the input data that are normally more complex,\ndynamic and of a finer granularity than can be captured by human description.\nNevertheless, this simplified example serves to illustrate how the composite\nset of neurons that constitute the system learn how to detect various features\nin the data and change the model’s prediction based on what they observe.\n\nThe choice here of five neurons is largely arbitrary, although the addition of\nmore neurons—and more hidden layers—generally conditions the ANN to be more\nsensitive to smaller, less perceptible patterns, at the cost of requiring more\ntime to run and more data to train. Furthermore, the use of more hidden\nneurons and layers risks the problem of “overfitting,” to which we will turn\nlater.\n\nLocated within each hidden neuron is what machine learning researchers call an\n“activation function.” In machine learning, this activation function is what\ndefines the set of conditions under which that neuron “fires,” mirroring, in\nsome ways, how animal neurons activate. Most often, various nonlinear\nfunctions are used such that the activation function resists “firing” if it\nreceives only a small stimulus. Then, at a certain measurable level, it\nrapidly increases its output signal and finally plateaus. This allows the\nnetwork to detect nonlinear relationships, such as we see in our “extreme\nhurricane detector”: initially, the neuron would add little information to the\nfinal prediction, but were an input to cross a certain threshold (say, a 90\npercent chance of a Category 5 hurricane), the stimulus would quickly shift\nand activate the neuron, decreasing the predicted number of Pop-Tart sales.\n\nMany things in the world display similar nonlinear dynamics. The rate at which\npeople fall in love, income distributions, the popularity of works of art—all\nlargely eschew the boring simplicity of linear relations (as X increases, Y\nincreases at a steady rate). And, as these nonlinear elements interact with\neach other in nonlinear ways, the overall system becomes incredibly complex.\nAccordingly, when machine learning researchers attempt to predict real-world\noutcomes, the ability to account for such nonlinear dynamics becomes crucial.\n\nIn order to turn the network into a functioning model, we first assign\narbitrary weights to each neuron’s input. We then run a set of training\ndata—data that already has the correct answer associated with it—noting how\nfar away the model is on each prediction. After the training dataset is run,\nwe average how far off the final neuron is from the correct answer that\nsupervises the model. We then adjust the initial weights in order to minimize\nerror, that is, the distance to the correct answer.\n\nIn this process of tweaking, we work our way backward through the network,\nrevising the weights at each step in order to decrease the amount of error, in\na process known as “backpropagation.” After running data and backpropagating,\nthe computer does it again. And again. And again, working through all of the\ntraining data over and over. Machine learning algorithms repeat this process\nthousands of times, slowly adjusting these weights until subsequent rounds of\ntraining find stability. We cannot simply calculate the optimal weights, since\nthe ANN network is strongly interconnected, and changing a single neuron’s\nweight alters the optimal value for all other\nweights.[18](19_Notes.xhtml#ch1fnr18) When those backpropagated weights\nfinally stop moving, the model has converged: in short, it has come up with a\nsolution to the training data and can take new data to give us an\nalgorithmically produced “answer.” This process of backpropagation is\nextremely important, both computationally and historically; as computer\nscientist Ethem Alpaydın has noted, while the basic idea of computational\nneural nets existed for most of the second half of the twentieth century, it\nwas not possible to train multilayer ANNs until the invention and further\ndevelopment of backpropagation throughout the ’70s and\n’80s.[19](19_Notes.xhtml#ch1fnr19)\n\nThe amount we adjust each neuron’s weight during backpropagation is known as\nthe _learning rate_. The higher the learning rate, the more quickly the\nnetwork converges. In the case of large networks that use large training\ndatasets, the time required to train a model can be considerable, ranging from\na couple hours to several days.[20](19_Notes.xhtml#ch1fnr20) If we quicken the\nlearning rate, we tend to find less accurate results, with the weights\nswinging wildly in each round. Our overeagerness, or what is referred to as\n_overgeneralization_ , might quickly lead the model, upon seeing only a few\nCategory 5 hurricanes in the dataset, to decide that all hurricanes are\nCategory 5. In this way, we risk becoming guilty of Hegel’s famous critique of\nSchelling’s philosophy—that is, of simply learning that at night, “all cows\nare black.”[21](19_Notes.xhtml#ch1fnr21)\n\nOpposite overgeneralization is a related technical challenge for machine\nlearning: the problems of _overfitting_. If a model is overfitted, we have\nmade it too smart, merely memorizing the training data, much like a student\nwho learned only how to take the practice test. This rigidity results in a\nmodel that may be very effective at correctly analyzing the training dataset,\nbut remains stubbornly unable to employ that information outside of the\nconfines of the input model.\n\nIn either case, the models’ computationally produced abstractions fail to\ngeneralize what they have learned effectively enough to predict the correct\nanswer when confronted with new, not-onthe-test data. In order to generate\nuseful predictions, machine learning must walk the fine, functionalist line\nbetween generalizing too much and not enough.[22](19_Notes.xhtml#ch1fnr22)\nThis situation, along with the contemporary moment’s growing availability of\ncomputing power, more extant algorithms to choose from and more stores of\ndata, means that we encounter a dual possibility: better predictions, on one\nhand, and an increased danger of overfitting or overgeneralization, on the\nother.\n\nThe dangers of overfitting and overgeneralization bear a formal similarity to\nthe reference class problem described earlier: How to make a set of meta-level\ndecisions in order to generate a model that best describes the world? But,\nunlike the reference class problem—where one can have sensible debates about\nwhat type of thing something is for the sake of calculation—the decisions\ninherent in machine learning practice are significantly more opaque, because\nsmall changes can have outsized effects and we often do not know what elements\nin a dataset an algorithm has decided are relevant.\n\nThese decisions, like the learning rate or the number of neurons and hidden\nlayers, are generally set by humans during the construction of the model\nrather than learned as part of the process. Known as “hyperparameters,” their\nhuman-selected fine-tuning can make authoring machine learning algorithms more\nof an art than a science. And as machine-learned systems model more and more\ncomplex nonlinear dynamics, the impact of changes to hyperparameters are,\nconsequently, also complex, nonlinear and largely\nunpredictable.[23](19_Notes.xhtml#ch1fnr23) Increasingly, this means that the\noutputs of these systems are not easily verifiable or testable.\n\nAt its most basic, our hurricane-detecting ANN takes a highdimensional input\nspace (only three in this example, but often many more in practice) and maps\nthe inputs through a series of learned nonlinear transformations to a lower-\ndimensional output space (our single “Pop-Tart\nprediction”).[24](19_Notes.xhtml#ch1fnr24) Here we cross one of the central\ntaxonomic distinctions in machine learning: that between supervised and\nunsupervised learning. _Supervised learning_ takes place in situations where\ntraining data allows the algorithm to learn from examples where the answer is\nknown, such as in the case of the training data for our hurricane ANN.\n_Unsupervised learning_ , on the other hand, involves a search for underlying\nstructures in data where no “correct” answers are predefined. Classification\nalgorithms, which seek to divide data into a number of categories, are a\ncommon example of this type of learning.[25](19_Notes.xhtml#ch1fnr25)\n\nWhile this distinction is significant in practice, the most important point\nfor our present concerns is that both take large, though still human-selected,\ndatasets and try to find structures in the data in order to map internal\nsimilarities (in the case of unsupervised learning) or a known output (in the\ncase of supervised learning). Despite the differences between these two\nmethods, both supervised and unsupervised learning attempt to find some signal\nthat exists in the data—in essence, to find something meaningful.\n\nThough saturated in complex implementations and large datasets, this rapidly\ngrowing field—and the massive financial and human investments it has\nenticed—is built on these relatively simple conceptual foundations. Despite an\noften-convoluted, “experts only” discourse, machine learning requires no\ncomplex symbolic logic, no deep underlying theory of the mind, and no\nuniversal understanding of the nature of the world.\n\nIn fact, these machine-learned models apprehend a very narrow, particular\nvector of the world, based only on the observed data and the nonlinear\nrelationships discovered within it. While traditional ANNs do not necessarily\ndeal directly in probability, epistemically and metaphysically they partake of\nthis general turn to a probabilistic understanding of the world and knowledge:\nthese systems are not designed to be correct all the time, but rather to guess\nthe correct outcome with a high enough probability that en masse, the system\nis economically viable.\n\n“Indeed, They Don’t Have to Settle for Models at All”\n\nIn many ways, machine learning was born out of disappointment: the failure of\nartificial intelligence to live up to its postwar promise. During the 1960s,\nmany American attempts at AI sought to use symbol-based systems to solve logic\nproblems, mimic human language, translate between languages, and the like.\nThese systems attempted to understand classes of things logically and how\ndifferent classes of things interact, such that conclusions could be worked\nout from initial premises. While early constrained attempts at these problems\nproved encouraging, attempts to take them beyond simple problems quickly\nproved intractable.[26](19_Notes.xhtml#ch1fnr26) Early optimism, such as\ncomputer scientist Marvin Minsky’s 1967 claim that “within a generation … the\nproblem of creating ‘artificial intelligence’ will substantially be\nsolved”[27](19_Notes.xhtml#ch1fnr27) was soon quashed, and considerable\ngovernment resources gave way to an “AI winter” where public interest, funding\nand research efforts stalled.[28](19_Notes.xhtml#ch1fnr28)\n\nApprehending the limitations of these symbolic attempts, researchers turned to\nmore probabilistic and statistical approaches, such as ANNs. The success of\nthese approaches in subsequent decades had its roots in earlier postwar\nscience. The history of neural networks themselves begins in 1943, when\ncyberneticians and neuroscientists, Warren McCulloch and Walter Pitts,\npublished “A Logical Calculus of the Ideas Immanent in Nervous Activity,”\nwhich proposed a representational model of human knowledge as the activity of\nnetworked neurons. According to McCulloch and Pitt’s “neuron” model, these\nneurons functioned by summing together binarily represented input data, where\nknowledge was subsequently represented as a binary output, depending on\nwhether or not the sum of inputs reached a certain threshold. This model\nprofoundly, albeit simply, imagined the brain as a computational machine,\nwhere “any computable function could be computed by some network of connected\nneurons, and … all the logical connectives (and, or, not, etc.) could be\nimplemented by simple net structures.”[29](19_Notes.xhtml#ch1fnr29)\n\nIn 1958, AI pioneer Frank Rosenblatt expressed disappointment at the\nsimplicity of models like the one created by McCulloch and Pitts, which relied\non Boolean algebra to represent neuronal activity. By representing input data\nas only a registered one or unregistered zero, these models, Rosenblatt\nalleged, failed to demonstrate how the human brain processed more complex\ninformation, and its ability to represent the world in gradations.\n\nIn response, Rosenblatt proposed his own “theory of statistical separability”\nto facilitate a more dynamic representation of a neural\nnetwork.[30](19_Notes.xhtml#ch1fnr30) In this dynamism, one could account for\nweighted input stimuli, where data was not just a one or zero but a\nprobabilistic stimuli that could attend to small statistical\ndifferences.[31](19_Notes.xhtml#ch1fnr31) Using this concept of statistical\nseparability, Rosenblatt developed a network he called the “perceptron,” which\nwas able to handle much more complex information than its predecessors and,\nprecisely due to the probabilistic, nonbinary representation of its inputs,\ncould potentially learn through feedback from trial-and-error tests.\n\nAs excitement around the symbolic approaches continued to grow, Rosenblatt’s\nperceptron fell into disfavor. In 1969, computer scientists, Marvin Minsky and\nSeymour Papert, published their _Perceptrons: An Introduction to Computational\nGeometry_ , a work that was largely critical of the statistical separability\napproach, demonstrating that it could not meaningfully solve even simple\nproblems.[32](19_Notes.xhtml#ch1fnr32) Their book hastened the abandonment of\nneuronbased attempts for the next decade.[33](19_Notes.xhtml#ch1fnr33)\n\nBut in the ’80s, further developments in methods for working with neural\nnetworks, including backpropagation and the explosion of cheap computing\npower, provided the conditions for a return to probabilistic, neuron-based\napproaches to AI.[34](19_Notes.xhtml#ch1fnr34) In addition to the rise of\nneural networks, other nonsymbolic approaches—from the “naive Bayes\nclassifier” to support-vector machines—appeared, employing increasing\ncomputing power and methodological advances to generate knowledge and\nintelligence without relying on a fixed idea of\nmeaning.[35](19_Notes.xhtml#ch1fnr35) Rather than symbolically deducing\nconclusions from premises, as older symbolic approaches did, many of these\nnewer systems were able to count evidence probabilistically, and thus provide\na probabilistic output, ultimately favoring so-called “temporary correlation”\nover an understanding of causality or foundational principles.\n\nWith their movement away from the disappointing symbolic systems and toward\nstatistics and probability, these newer machine learning methods reframed the\nepistemic contours of computationally produced knowledge, allowing its\nautomation by abandoning its fixity. Rather than symbolically creating some\nuniversal, generalized representation of knowledge and the world, today’s\nexplosion of machine learning instead generates models that are allowed to\ndevelop their own, problem-specific internal logics. In a way, machine\nlearning is witnessing the development of an array of near-infinite, hyper-\nspecific enlightenments, of highly regulated simulations able to process the\nworld—albeit without any connection to a notion of the universal—by creating\nthem anew for each particular place and time. Our hurricane-detector ANN\ncalculates one world, while someone else’s, with only slightly different data\nor structure, calculates another.\n\nIn a short article in _Wired_ , Chris Anderson, a Silicon Valley evangelist\nmade rich by his undying faith in machine learning technology, spells out the\ntheoretical implications of this development. “Today companies like Google,\nwhich have grown up in an era of massively abundant data, don’t have to settle\nfor wrong models,” he writes. “Indeed, they don’t have to settle for models at\nall.”[36](19_Notes.xhtml#ch1fnr36) He continues, repeating a realization that\nHegel noted long before modern computation: at a certain point a change in\nquantity becomes a change in quality.\n\n> The Petabyte Age is different because more is different … At the petabyte\n> scale, information is not a matter of simple three- and four-dimensional\n> taxonomy and order but of dimensionally agnostic statistics. It calls for an\n> entirely different approach, one that requires us to lose the tether of data\n> as something that can be visualized in its totality. It forces us to view\n> data mathematically first and establish a context for it later. For\n> instance, Google conquered the advertising world with nothing more than\n> applied mathematics. It didn’t pretend to know anything about the culture\n> and conventions of advertising—it just assumed that better data, with better\n> analytical tools, would win the day. And Google was right. Google’s founding\n> philosophy is that we don’t know why this page is better than that one: If\n> the statistics of incoming links say it is, that’s good\n> enough.[37](19_Notes.xhtml#ch1fnr37)\n\nA “good enough” probabilistic world ousts those traditional Enlightenment\nmodels and theories of causality. While it is beyond doubt that the efficacy\nof machine learning, at least in certain applications, is impressive, these\ntechnologies and their ideological commitments are not as direct or unmediated\nas they are often made out to be.[38](19_Notes.xhtml#ch1fnr38) What Anderson\npresents as a direct apperception of the real is, in fact, mediated through\ndecisions about how to construct models, how to avoid overfitting and how to\nunderstand what those results tell about the\nworld.[39](19_Notes.xhtml#ch1fnr39)\n\nThe possible advantage to this correlational approach, as informatics scholar\nGeoffrey Bowker argues, “is that it avoids funneling our findings through\nvapid stereotypes. Thus, in molecular biology, most scientists do not believe\nin the categories of ethnicity—and are content to assign genetic clusters to\ndiseases without passing through ethnicity (e.g., Karposi’s sarcoma as\ninitially a Jewish disease).” Yet, while many social categories are founded on\nquestionable premises—race, gender, value, and so on—the histories and impacts\nof these categories are still highly important because “the world is\nstructured in such a way as to make the categories have real\nconsequences.”[40](19_Notes.xhtml#ch1fnr40) Although these categories are\nproblematic, to do away with them completely would leave us unable to identify\nthese real consequences.\n\nMedia theorist Wendy Hui Kyong Chun’s recent work on the concept of\nhomophily—the love of the same—is especially insightful in this\nregard.[41](19_Notes.xhtml#ch1fnr41) Focusing on the use of network science as\nan analytical and computational technique, she argues that these systems push\npeople toward those who are the same, creating digital spaces that are\nsegregated along the multiple and intersecting vectors of social existence.\nAnd while these digital systems are more mobile and fluid than earlier forms,\nshe states, “there are no random initial\nconditions.”[42](19_Notes.xhtml#ch1fnr42) These digital systems are built on\nhistories and in societies that have long and complicated histories of racial\nand gendered violence. Thus, it was “the rise of the modern concept of race\nduring the era of Enlightenment; its centrality to colonization and slavery;\nits seeming zenith during the era of eugenics; [and] its transformations after\nWorld War II” and beyond that set the initial conditions for these systems and\ndetermine the social world they tend to\nreproduce.[43](19_Notes.xhtml#ch1fnr43)\n\nDespite what Anderson claims, mathematics routed through machine learning is\nstill, in the end, a form of mediated abstraction, and thus fundamentally\nintertwined with the development of our social, economic and political\nsituation. These machine learning systems function in a way that is\nanalogous—and, as we shall see, metaphysically tied—to capitalism: they move\nthe locus of social domination from the material world into the abstract one\nof capital and probability, yet they do not oust history. In fact, because\ntheir aim is only to predict, they actively reproduce it. While these systems\nmake some categories more fluid and open, they simultaneously work to solidify\nextant social systems: capitalism, racism, patriarchy and imperialism, among\nothers. And they do so in ways that are potentially more insidious and harder\nto resist, presenting their outputs as objective facts. It is thus necessary\nto account for the metaphysical force of this objectification—something we can\nascertain only by tracing the ways in which probability and statistics\nfunction socially and economically.\n\n\n[Chapter 2](04_Contents.xhtml#ch5)\n\n[Can Computers Do Math?](04_Contents.xhtml#ch5)\n\nFrom the sheer number of algorithms affecting our daily life, to the amount of\ndata that machine learning algorithms process, to the number of hidden layers\nwithin a model and the speed at which they run, the complexity of these\nnonlinear, machine-learned models prevents human beings from fully\nunderstanding what is happening inside them. What’s more, the internal logic\nand state of these systems are only becoming less accessible.\n\nThe inaccessibility of algorithms is often described in terms of “black\nboxes,” a characterization that tends to elicit political attempts to shine\nthe light of transparency on algorithmic systems and slow their functioning to\na speed that can be managed.[1](19_Notes.xhtml#ch2fnr1) Yet, both algorithms\nthemselves and the larger social systems within which they function\ncontinually resist these calls for transparency, in large part because of the\nspeed with which they operate and are created. While the Volkswagen defeat\ndevice discussed in the introduction was finally regulated, it took years\nbefore its existence was even discovered. For each algorithm that is regulated\nor draws public ire, many more likely go unnoticed. If algorithms are designed\nlargely to maximize profit or automate an already-biased system, the political\nissue must be both the algorithm and its larger context. As seen in sexist and\nracist outputs calculated from avowedly “neutral” algorithms, the unjust\nepistemic terrain of daily life tends to get baked into each machinelearned\nsystem and model, as well as the data from which algorithmic insights are\ndrawn. Often, algorithms end up simply automating the bureaucratic opacity and\ninjustice they are meant to replace. Thus, even attempts to decode how these\nsystems make their decisions are unlikely to overturn the unjust social\nsystems they operationalize.\n\nPolitical projects that critically assess how algorithms produce and reproduce\nthese systemic biases are critical, and organizing around these issues is\ndecidedly important. However, these projects of algorithmic dereification are\nunlikely to address the unique political and economic challenges of our\ncontemporary moment. To dereify algorithms, or to establish a politics around\nthe revelation of what algorithms are “really doing,” would ultimately require\na capacity for regulation and critique that could overcome the ways in which\ncapitalism poses and solves social problems. Calls for such regulation assume\nthat were humans only to know what is happening in these systems, they would\nsurely correct them; in short, they underestimate the metaphysical work upon\nwhich these systems are built, the power of exchange and capital that make\ntheir work appear necessary.[2](19_Notes.xhtml#ch2fnr2)\n\nA politics of transparency may help prevent the worst abuses of algorithmic\nknowledge and is thus a worthwhile political project, but transparency alone\ncan never completely solve the injustices algorithms present. As we saw with\nthe discovery and censure of Volkswagen’s defeat devices, there remains\nconsiderable benefit to regulation, yet such behavior cannot be regulated if\nit is able to escape detection in the first place. As long as there is a\nstrong economic incentive to use algorithms for obfuscation, capitalist\nprofit-seeking will continue to produce them. In sum, algorithms change the\nspeed and form of knowledge production, but they do not change the ultimate\ngoals of knowledge production.\n\nIn the complex ecologies in which algorithmic systems function, all sorts of\neconomic, political and social incentives shape how they work and what they\nvalue. What forms the opacity of an algorithm, or knowledge production in\ngeneral, is simultaneously that which is hidden inside the system, and those\nobfuscated and objectified forces that work from without to shape the\nnecessities to which knowledge production responds. From the intricacies of\ncontemporary mathematical proofs to the predictive power of neural networks,\nthis means that all knowledge production is in essence “black boxed” because\nit relies on abstractions whose nature must be ignored in order to be useful,\nespecially at the moment it becomes economically productive. It is precisely\nthis withdrawal into unseen work that gives these systems their very force,\nthe force of objectification.\n\nThus, in order to engage algorithmic culture politically, it is imperative to\nunderstand its abstract force. The question at hand concerns not only _how_\nthese algorithmic systems function but _what_ metaphysical force they produce\nthat gives their outputs such authority in contexts ranging from courtrooms to\nboardrooms to bedrooms. To begin to ask these questions, however, requires a\nslight detour through the realm of theoretical mathematics.\n\nFour-Color Theorem\n\nEven theoretical mathematics encounters this problem of opacity that\ncontaminates math’s supposedly unadulterated internal logic. For example, in\n1976 Kenneth Appel and Wolfgang Haken announced that they proved the famous\n“four-color theorem” (4CT). This theorem, proposed over a hundred years prior,\nstates its problematic simply: any map divided into areas can be colored using\nonly four unique colors without the use of a single color for two adjoining\ncountries (provided that certain constraints are followed in the construction\nof the map such as disallowing discontiguous areas).\n\nHistorically, no one had found or fabricated such a map requiring more than\nfour colors. But even with this lack of empirical evidence, it had never been\nmathematically proven that it was impossible. The theoretical possibility of a\nnecessarily five-color map remained, and this possibility frustrated\nmathematicians for over a century.\n\nAppel and Haken’s proof took a novel approach: they used a computer. They were\nable to prove that all possible maps could be reduced to versions of a core\nset of 1,936 maps. If those maps could be colored with four colors, they\nargued, then so could any map. They then programmed a computer to check these\nnearly 2,000 maps to determine if they could be colored with only four\ncolors—a task that took their 1970s machine over a thousand hours to\ncomplete.[3](19_Notes.xhtml#ch2fnr3)\n\nFollowing Appel and Haken’s success, a host of other proofs have been\ncomputationally resolved. In 2016, three computer scientists produced a\ncomputer-assisted proof to the so-called “Boolean Pythagorean triples problem”\nwhose final, written-down form consisted of 200 terabytes of\ndata.[4](19_Notes.xhtml#ch2fnr4) The resulting proof took up twenty times the\ndigital space of the entire English-language\nWikipedia.[5](19_Notes.xhtml#ch2fnr5)\n\nMuch like the impossibility of confirming the veracity of every English\nWikipedia page, the very fact of such an enormous amount of data employed for\none specific problem precludes the possibility that a small team of human\nresearchers could ever fully certify the proof. Instead, proofs like the\nBoolean Pythagorean triples problem are verified by another piece of software.\nThis is a perfect example of the epistemics of opacity: a computationally\nproduced proof is read, and verified, by another\ncomputer.[6](19_Notes.xhtml#ch2fnr6) Such theoretical knowledge becomes\nfundamentally recursive—it is verified simply by being repeated in a computer\nelsewhere. While algorithms for solving proofs are unique to the problems of\npure mathematics, and thus differ substantially from common machine learning\nalgorithms, the epistemic challenges they raise remain indicative of the\nlarger state of computationally produced knowledge.\n\nIndeed, not long after Appel and Haken proved the 4CT, mathematicians began\nquestioning the value of such computationally facilitated endeavors. For\ninstance, Thomas Tymoczko, a philosopher of mathematics, wrote an article the\nsame year Appel and Haken’s proof was published, in which he suggested that\nfor such a “proof” to truly count, it would require a radical change in\nmathematicians’ core definition of proof itself:\n\n> No mathematician has seen a proof of the 4CT, nor has any seen a proof that\n> it has a proof. Moreover, it is very unlikely that any mathematician will\n> ever see a proof of the 4CT.\n>\n> What reason is there, then, to accept the 4CT as proved? Mathematicians know\n> that it has a proof according to the most rigorous standards of formal\n> proof—a computer told them. Modern high-speed computers were used to verify\n> some crucial steps in an otherwise mathematically acceptable argument for\n> the 4CT, and other computers were used to verify the work of the first.\n>\n> Thus, the answer to whether the 4CT has been proved turns on an account of\n> the role of computers in mathematics. Even the most natural account leads to\n> serious philosophical problems. According to that account, such use of\n> computers in mathematics, as in the 4CT, introduces empirical experiments\n> into mathematics. Whether or not we choose to regard the 4CT as proved, we\n> must admit that the current proof is no traditional proof, no a priori\n> deduction of a statement from premises. It is a traditional proof with a\n> lacuna, or gap, which is filled by the results of a well-thought-out\n> experiment. This makes the 4CT the first mathematical proposition to be\n> known a posteriori.[7](19_Notes.xhtml#ch2fnr7)\n\nIn the radical redefinition of proof required to include 4CT, human knowledge\nof the proof ’s specifics is no longer required. Contrary to even the most\nelementary empiricism, while we cannot “see” the proof, nor “see” the proof of\nthe proof, we know that the proof exists. Philosophically, the pristine realm\nof pure logic and the a priori has been besmirched by the “experimental,” the\nobserved and the a posteriori. We know the proof exists, but we have yet to\ncomprehend it. In the race for knowledge, the computer—and its use of large-\nscale processing—has outpaced us.\n\nAs math has become experimental, the conditions of an experiment, or the\ncomputational work of a proof, have become vitally important in the derivation\nof a general law.[8](19_Notes.xhtml#ch2fnr8) The specific computer that is\nused, the prevention of errors in the code, and so on all partake of the final\nresult. Accordingly, as soon as the realm of the pure, ideal a priori is\npunctured, we fall back into the world of materiality and economy, and their\nobjective force. It is due to the fact that all of these elements—in short,\nthe experiment—are commensurate with the produced knowledge that the final\nresult appears to be objective—even if we are not party to their computation.\n\nThe ABC Conjecture\n\nWhile the proof of the four-color theorem requires we believe a computer, we\nsee a similar structure of opacity at work even in nonautomated mathematics.\nIn 2012, a somewhat-reclusive mathematician by the name of Shinichi Mochizuki\npublished four papers on his personal website. These papers totaled around 500\npages and were the culmination of years of largely independent work. Mochizuki\nmade no announcement of their release, but a colleague of his at Kyoto\nUniversity’s Research Institute for Mathematical Sciences noticed their\nappearance and alerted other mathematicians in the field.\n\nAmong several other proofs, Mochizuki’s fourth paper claimed to prove, for the\nfirst time, a significant theorem known as the “abc conjecture.” This theorem\nrefers to an intricate property of algebraic equations that take the form of\n_a_ \\+ _b_ = _c_. In order to resolve this previously impenetrable conjecture,\nMochizuki invented a whole new type of mathematics, which he named “inter-\nuniversal Teichmüller theory.”[9](19_Notes.xhtml#ch2fnr9)\n\nThis inter-universal Teichmüller theory is so abstract, and ventures so far\naway from traditional math, that other mathematicians—even those working in\nthe same subdiscipline as Mochizuki—have been unable to fully verify the\nproof. Mochizuki has since estimated that it would take a graduate student ten\nyears to learn and understand the theories he has created. Adding further\ncomplication, Mochizuki has largely refused to attend international\nconferences or invest much energy in explaining the work to other\nmathematicians. Even those who have made headway on this task find themselves\nat a loss to explain it to others. For example, one anonymous mathematician\nstated, “Everybody who I’m aware of who’s come close to this stuff is quite\nreasonable, but afterwards they become incapable of communicating\nit.”[10](19_Notes.xhtml#ch2fnr10)\n\nWe might raise similar objections, as Tymoczko did to Appel and Haken’s 4CT\nproof, about Mochizuki’s work. Perhaps Mochizuki has “seen” the proof,\nalongside two or three other people who have worked through it. But have\nmathematicians as a field seen it? Given that the number of people who can\nunderstand “regular” cuttingedge mathematics is relatively small, what is the\nrequired quorum for truth? How many people need to comprehend a proof for it\nto be a proof in the classical sense that Tymoczko suggests?\n\nWe can inquire further: Does a computer-resolved proof verified by another\ncomputer count more, or less, than a likely but unverified proof by a well-\nregarded human expert? Regardless of how one answers this question—and it is\nunlikely there exists one correct answer—any answer ultimately relies on\neconomics and exchange. While proofs, of course, are not directly bought and\nsold, their production requires networks of verifiability—other\nmathematicians, other computers and so on—that determine when the threshold of\nproof has been passed, that is, when a collection of experimental data is\ncommensurate with a proof. While it is likely that pure a priori knowledge has\nalways been contaminated by an exchange that makes separating the a priori and\nthe experimental impossible, with the advent of modern computing, the size,\nspeed and scope of these forms of unverifiable knowledge now reveal the\ncentrality of exchange. It is because, not in spite, of these material\nnetworks of verification that this knowledge confronts us as objective, as\nthis materiality allows it to appear to stand outside of our social world and\naccount for itself.\n\nThe Infidel Mathematician\n\nThese problems of epistemology are certainly not new. Questions of how and\nwhat is required to “know” things have plagued humans since the earliest\nmoments of philosophy. An especially telling example of this problem is\nBritish philosopher and bishop George Berkeley’s 1734 essay decrying calculus\nentitled _The Analyst_ , with its accompanying (rather long) subtitle: _A\nDiscourse Addressed to an Infidel Mathematician: Wherein It Is Examined\nWhether the Object, Principles, and Inferences of the Modern Analysis Are More\nDistinctly Conceived, or More Evidently Deduced, than Religious Mysteries and\nPoints of Faith_. Aside from its attempts to wrestle with the foundations of\nknowledge and attack calculus, Berkeley’s _Analyst_ and its questions also\nplay an important role in the history of computational knowledge and\ncapitalism.[11](19_Notes.xhtml#ch2fnr11) The Reverend Thomas Bayes, for whom\nthe field of Bayesian statistics is named, wrote only two texts during his\nlife: one was a theological proof that God wants us to be happy, and the other\na direct response to Berkeley’s attack on calculus laid out in _The Analyst_.\nThe following century, even Marx would take up the philosophical challenge of\nfinding a solid ground for calculus, writing a short tract attempting to\njustify its mathematics.[12](19_Notes.xhtml#ch2fnr12)\n\nBerkeley’s _Analyst_ is addressed to an individual he calls the “infidel\nmathematician.” Berkeley attacks this infidel’s nascent calculus on multiple\ngrounds. For one, he takes issue with the idea that one can, in modern\nparlance, take the derivative of the derivative. While one can describe what\nacceleration _is_ (the derivative of velocity over time), Berkeley asks: What\nwould the acceleration of an acceleration of an acceleration _mean_? Thus, he\nappears troubled by the same epistemic groundlessness that we confront when\nthinking through the implications of recursive attributions of value and\nknowledge in algorithmic processing.\n\nBerkeley further attacks the use of “infinitesimals” in this\nprotocalculus—whose size approach zero when calculating the formulas for\nderivatives—as “ghosts of departed quantities.” He argues that if they\napproach nothingness, they should provide no mathematical insight; still,\nmathematicians use these values as they approach zero to derive the basic\nformulas of calculus.[13](19_Notes.xhtml#ch2fnr13) They function, in short, by\nmaking an _inequality equal_ : by saying that absence is presence. While\ncalculus has clearly done just fine for itself despite these attacks, there is\nsomething deeply important about Berkeley’s recognition of the ways in which\nthis knowledge requires the equalization of an inequality.\n\nSo much of modern knowledge is founded on such present absences, which require\nthat what is represented is simultaneously the same and different—from diverse\ntypes of labor that are translated into value to the extraction of computable\ndata from complex social interactions.[14](19_Notes.xhtml#ch2fnr14) This is\nprecisely what is at stake in the redefinition of mathematical proof to\ninclude algorithmic proofs: the supposed presence of a formal proof that can\nbe understood by a single mathematician is made commensurate with 200\nterabytes of data that cannot be reviewed except by another computer. We will\nsee these ghosts of departed quantities appear again and again as the\nfundamental unit of algorithmic knowledge production—a value that is both\nitself and its opposite (such as Bayesian analysis’s claimed\ntransubstantiation of the particular into the universal, or the subjective\ninto the objective), able to make the incommensurate commensurate, and thus\ncalculable.\n\nTo return to Berkeley, despite reservations about its philosophical\nfoundations, Berkeley readily admits that calculus “works.” Still, he argues\nthat the faulty intellectual foundations of this math call into question its\nintellectual value. To the proponents of calculus, he says:\n\n> Your Conclusions are not attained by just Reasoning from clear Principles;\n> consequently, that the Employment of modern Analysts, however useful in\n> mathematical Calculations, and Constructions, doth not habituate and qualify\n> the Mind to apprehend clearly and infer justly; and consequently, that you\n> have no right in Virtue of such Habits, to dictate out of your proper\n> Sphere, beyond which your Judgment is to pass for no more than that of other\n> Men.[15](19_Notes.xhtml#ch2fnr15)\n\nEven though this new math can be used to build bridges and solve otherwise-\nintractable problems, Berkeley does not agree with its fundamental premises.\nAnd, perhaps for him most importantly, he wants these mathematicians to focus\non their math, rather than extrapolate from their insights there into the\nworld of theology.\n\nIn essence, the operation Berkeley attempts to perform is one of\ndereification, stripping away the layers of this new math to show that its\nsupposedly solid foundations are, in fact, insecure. But the reason calculus\nis now taught throughout the world—while Berkeley’s text is rarely read—is\nthat calculus works and has worked, despite the supposed shakiness of its\nfoundations. Likewise, capitalism appears to work with or without individual\nknowledge of its more philosophical justifications. While, of course, calculus\nis now less ideological and much more solidly grounded than capitalism, we see\nthe force by which objectification works, allowing objects to think for us:\nyou can believe in calculus or not, but while Berkeley was busy debating its\njustification, Isaac Newton and others were busy solving problems, proving the\nforce of these mathematics.\n\nBerkeley’s dereification of calculus prefigures contemporary attempts to\ndereify algorithmically produced knowledge. He ends _The Analyst_ with a\nprescient question about the difference between knowing and merely computing:\n“Whether the difference between a mere Computer and a Man of Science be not,\nthat the one computes on Principles clearly conceived, and by Rules evidently\ndemonstrated, whereas the other doth not?”[16](19_Notes.xhtml#ch2fnr16) While\nfor Berkeley, the term “computer” meant something drastically different from\nwhat it does today, the sentiment is not far afield from Tymoczko’s claim that\na computationally derived “proof ” does not count. Both Berkeley and Tymoczko\ninsist that one must clearly understand the principles and progressions of\nknowledge, rather than merely apply a set of\nrules.[17](19_Notes.xhtml#ch2fnr17) In this way, Berkeley’s concerns about\ncalculus’s unsteady foundations pave the way for contemporary unease—such as\nTymoczko’s—with computationally derived knowledge.\n\nBut beyond the insecure ground on which it seemed to operate, what actually\nmade calculus’s practitioners “infidels” according to Berkeley? In other\nwritings, Berkeley was forcefully opposed to the beliefs of freethought, a\nreligious-philosophical movement that argued that while God had set the world\nin motion, its development, laws and nature could be understood through logic\nand reason.[18](19_Notes.xhtml#ch2fnr18) Contrary to the long-held Christian\nbelief in mysteries of faith—such as the existence of the trinity,\ntransubstantiation, the function of prayer, and so on—the rationalist\nprinciples of freethought staked its new theology on a logical, and thus\nhumanly knowable, vision of the world.\n\nWhile Berkeley was an avowed Anglican, his thinking echoed the First Vatican\nCouncil, convened by Pope Pius IX in 1868, which reaffirmed the existence of\n“mysteries.” According to the dogmatic constitution that resulted from the\ncouncil, “divine mysteries, by their very nature exceed the created intellect\nso much that, even when handed down by revelation and accepted by faith, they\nnevertheless remain covered by the veil of faith itself, and wrapped in a\ncertain mist, as it were, as long as in this mortal life we are away from the\nLord, for we walk by faith, and not by sight.”[19](19_Notes.xhtml#ch2fnr19)\nThe council continued by declaring that “if any one say that in Divine\nRevelation there are contained no mysteries properly so called, but that\nthrough reason rightly developed all the dogmas of faith can be understood and\ndemonstrated from natural principles: let him be\nanathema.”[20](19_Notes.xhtml#ch2fnr20)\n\nFor both Berkeley and Pope Pius IX, mysteries cannot be explained through\nreason and human-divined natural principles. They are opposed to such thought,\nbelieving instead in a deistic structure of knowledge indiscernible to\nhumanity’s cognitive capacity. Berkeley’s criticism of calculus centered on a\ndesire to keep these mysteries of faith mysterious, which he sought to do by\ndemonstrating that calculus is likewise founded on mysteries rather than\nreason. He asks: “Whether Mysteries may not with better right be allowed of in\nDivine Faith, than in Humane Science?”[21](19_Notes.xhtml#ch2fnr21)\n\nThe aim and target of Berkeley’s essay was likely not, as some have suggested,\nthe famed Isaac Newton—who himself was a man of faith—but rather several less-\nfamous mathematicians who rejected the existence of religious mysteries in\nfavor of a wholly rational world, a freethinking intellectual maneuver that\ndeclared God’s workings profane. In this way, they posed a double threat to\nthe reified theology of the church. On one hand, freethinkers openly attacked\nthe existence of mysteries on account of their irrationalism. On the other,\nfreethinkers tamed the very concept of mystery for the purposes of advancing\nmathematics and the sciences.\n\nPerhaps freethought would not have been so threatening to Berkeley’s worldview\nif it merely did its work, advancing reason, logic and mathematics while\nleaving the question of mysteries to the church. Yet for Berkeley, such\ninfidel mathematics created and worked on its own mysteries, and thus\nestablished a new theological episteme interior to its own workings. The\nsuccess of calculus emboldened freethinkers to extrapolate from their\nmathematical successes to attack the mysteries of the church. Berkeley’s\nargument claimed the necessity of religious mysteries by proving that even\nfreethinkers work by way of mysteries. Thus, in his thought, either the\nmysteries of calculus undermine its secular foundation of reason, or\nmathematicians should leave religion to its own\ntheology.[22](19_Notes.xhtml#ch2fnr22)\n\nThe mysteries that ground both Berkeley’s reading of calculus and Christian\ndoctrine are fundamentally opposed to reason precisely because they make the\nunreasonable claim that _inequality is equality_. The mystery of the trinity\nrelies on the mathematically impossible formula of 1 = 3. Transubstantiation\ndeclares an equality between bread and the body of Christ. And the “ghosts of\ndeparted quantities” of calculus’s infinitesimals appear likewise as a\nparadox: the equality, and coterminous possibility, of presence and absence.\nAll of these mysteries trouble the rationalist ground of knowledge while\nsimultaneously providing its building blocks.\n\nSuch mystery lies at the heart of all objectification. In the case of\ncalculus, it is these ghosts. In the case of capitalism, for Marx, it is the\nact of making unequal types of labor and commodities equal that founds the\nobjectification of capitalist exchange. And, in the case of computation, it is\na process of comparing, and thus calculating, seemingly incommensurate\ninformation about the world. Such mysteries cannot be negated simply because\nthey are seemingly spurious, for as long as those mysteries\n“work”—economically, materially, theologically—the simple act of showing what\nthey are “really doing” will never be enough to disempower them. In any\nrevolutionary politics, it is a necessary step to reveal how a mystery or\ninequality functions; but such a politics also requires the effectuation of\nnew mysteries—just as calculus posited its own.\n\nEpistemic Authority\n\nRevolutions in mysteries and metaphysics, whether of labor or knowledge, have\na tendency to call into question old mysteries and the high priests who\nproclaim them.[23](19_Notes.xhtml#ch2fnr23) The freethinkers with whom\nBerkeley clashed favored logic over authority and believed in the possibility\nof a fundamentally clear grasp of the world—one that would attempt to do away\nwith authority altogether. In this sense, it was consistent with many other\nrevolutionary modes of thought, including those data scientists, who believe\nthat ability to work with data can supplant the authority of domain\nknowledge.[24](19_Notes.xhtml#ch2fnr24) James Jurin, a freethinker and\ncontemporary of Berkeley, attacked Berkeley for appealing to the general\nreader to decide whether his claims are justified. He asks, “Pray, Sir, who\nare these thinking readers you appeal to? Are they Geometricians, or persons\nwholly ignorant of Geometry? If the former, I leave it to them: if the latter,\nI ask, how well are they qualified to judge of the method of\nFluxions?”[25](19_Notes.xhtml#ch2fnr25)\n\nIn response, Berkeley admonishes Jurin for his autocratic reliance on Newton’s\nexpert authority and insistence that the general reader cannot judge his\nmathematical arguments: “In a matter of science, where authority has nothing\nto do, you constantly endeavour to overbear me with authorities … No great\nname on earth shall ever make me accept things obscure for clear, or sophism\nfor demonstrations. Nor may you ever hope to deter me from speaking what I\nfreely think.”[26](19_Notes.xhtml#ch2fnr26)\n\nWhile we should likely give Newton his due and respect his expert insights,\nBerkeley catches his opponents in a difficult dilemma: If we favor reason over\nauthority, how are we to judge the other’s reason where we have not reasoned\nit out ourselves? But this dilemma, and the crises of authority to which it\nspeaks, tends only to move in one direction. The freethinkers and computers\nappear not to need the force of authority (even if they accidently call on it\nhere or there), because the objective nature of their claims no longer\nrequires a locus; it grounds itself in its own groundlessness. In appearing to\nwork, it finds itself in the appearance of the world; it objectifies itself as\nknowledge.\n\nThe effort to establish the validity of these mysteries—which make the\nincommensurate commensurate either by authority or by the objectified\nappearance of some force without place—ultimately amounts to the same thing:\nthe provision and assumption of a ground for thought. These knowledges of new\nmysteries—just like the probabilities that an algorithm produces, the value of\ncommodities or the infinitesimal—always rely on some non-present ground as\ntheir foundation, in a process that says, “Let us act and think as if this\nwere true.” But despite this mysterious ungroundedness, they still function as\nif they were real—as ghosts of departed quantities—or, as Alfred Sohn-Rethel\ncalls them, real abstractions.\n\nIt is precisely through these mysteries that the world is made “real,” from\nthe consecration of moral knowledge through divine intervention to feats of\nengineering fabricated from integrals in calculus. And once these mysteries\nare made real, succeeding in their feats, they begin to work on their own\naccord; in short, they appear objective. They cease to be solely a means by\nwhich we count—whether the trinity, mathematical proofs, the price of goods or\nacceleration—and they begin to account for us and our affairs. In short, these\nmysteries become objective, and allow us to reason and interpret the world.\n\nIn the end, it is folly to simply and exclusively deny the existence of this\nprocess, because it is through mysteries and their linkage of equality and\ninequality—or, to put it differently, abstractions, like class, race, gender,\nnation and value—that the social world is produced and acted upon, as the\ndetritus of history is drawn into new forms. But certain mysteries—namely\nthose that have supported injustice and exploitation in the form of\ncapitalism, imperialism, racism, sexism and so on—must be undone and their\npower resisted. To simply ignore them—as we saw in the first chapter with\nChris Anderson’s glorification of correlationism—is to allow them to continue\nto function. Thus, we must repudiate Berkeley and his epistemic anxieties\ntoward new mysteries. Instead, the task is to ourselves become revolutionary,\nor in Berkeley’s terms infidel, mathematicians.\n\nMysteries serve to provide a metaphysical ground and force to these historical\nconstructions. Under capitalism, as philosopher Moishe Postone has argued,\nthese mysteries become fully relativized, removed from any transcendental\ngrounding in God, the state or even the scientist who knows; their ground\nloses any determinable locus. And with algorithms, probability and statistics,\nthese non-locatable mysteries enter even the production of knowledge. But they\ndo not break with the past; on the contrary, they reproduce it in ever more\ninsidious forms that refuse being undone even by those who may succeed in\nshowing how things really work.\n\nWe must recognize the mysteries of reason, the unreasonable ground of reason,\nand thus the unequal equalities that permit computation and calculation. But\nrecognition of this is not an argument against reason, or against calculation;\nrather these mysteries are reason’s very raison d’être. To understand\ncomputation, we must recognize the process by which unequal elements are made\nequal—and hence accountable. Today, this process holds us to account by\ncapital, which seeks to make all materiality and knowledge commensurate as\nvalue. If we are to move beyond the injustices of these systems, both economic\nand algorithmic, we must ultimately, like the infidel mathematician, propose\nnew mysteries that can build bridges, trouble old mysteries and undo archaic\nideologies.\n\n\n[Chapter 3](04_Contents.xhtml#ch6)\n\n[Algorithms of Objectification](04_Contents.xhtml#ch6)\n\nWhile the general importance of algorithms may appear clear, a proper\ndefinition of the term “algorithm” itself is far from settled and has been an\nongoing area of debate for over half a century. In his famous text _The Art of\nComputer Programming_ , computer scientist Donald Knuth offers a definition\nbased on five requirements: (1) finiteness—an algorithm runs and finishes all\nof its steps in a finite amount of time; (2) definiteness—each step is\nrigorously and unambiguously defined; (3) input—an input is taken, and through\nthis process produces (4) an output; and (5) effectiveness—each step must be\nsimple enough that it can easily be understood and “be done by someone using\npencil and paper.”[1](19_Notes.xhtml#ch3fnr1) While these five requirements\nhave not fully ended the debate about what an algorithm is, they provide a\nhelpful starting point.\n\nFrom Knuth’s definition, two important elements should be noted. First, to\n“run and finish all of its steps”—or computation—is a process that takes time\nfor the algorithm to complete. While computation is often associated with\ninstantaneity, each step takes at least a tiny fraction of a second. For\ncomplicated algorithmic models that run over large datasets, like the\nartificial neural network from Chapter 1, computers often struggle to complete\ntheir instructions in a reasonably finite amount of time. Yet with the\nincreasing power of computers—meaning faster operations and thus less time\nneeded to process—algorithmic instructions can now operate on new and more\ncomplex scales. Theoretically, one could, as Knuth suggests, sit down and\ncompute any algorithm with pencil and paper—including the proof of the 4CT or\nthe mathematics necessary to send a human being to the moon. But there is not\nenough time in a human lifespan to independently compute the entirety of\nsomething as complex as Google’s famous PageRank algorithm (even though each\nindividual step could be done by hand). Thus, to operate at this scale, we\nneed computers.\n\nSecond, algorithms engage in an act of inscription: an algorithm transforms an\ninput into an output. For modern computers, this means flipping physical bits\nin order to take a set of symbols (letters, numbers, etc.) and produce a new\nset of output symbols—a process that mathematician and early computer\nscientist Alan Turing famously demonstrated with his theoretical model of\ncomputation, the Turing machine. In his machine, there is an infinite reel of\ntape, divided into cells, from and onto which the machine can read and write.\nWith a relatively small table of possible instructions represented as symbols,\nand the ability to move backward and forward on the tape, Turing proved that\nit was possible to encode any computer algorithm using this machine. Since\nthis theoretical discovery in 1936, it has been possible to understand all\nalgorithmic instructions as symbol manipulation, and hence as a species of\nwriting. The ability for algorithms to “write” in this way facilitates a new\nform of memory; thus, this ability to remember and account for our affairs has\nbecome one of computers’ most consequential\nattributes.[2](19_Notes.xhtml#ch3fnr2)\n\nIn this way, algorithms, and along with them a longer history of capitalist\ncomputation from bookkeeping to price settings, are always opaque—not only in\nthat we do not necessarily know how an algorithm arrived at a given result,\nbut also in a deeper metaphysical and epistemological sense, in so much as\nhuman operators of computers must trust what has been written to memory at a\nspeed and scale that is necessarily imperceptible. In this world, the only way\nthat computation can effectively function is if it is allowed to manage\nitself. It is here that data, algorithms, and thus computation present\nthemselves as objective. Without the ability to know what is happening in\nalgorithmic processing, we must trust the results of the output values. Just\nlike the market forces of capitalism, value is computed by complex processes\nwhose totality is necessarily inaccessible and appears to arrive from\nelsewhere. These objective values are not synonymous with “truth.” Rather, a\ncertain truth is attested to—or vouched for—by some object; in short it is\nobjectified.\n\nTally Sticks and Objectification\n\nAt the risk of pushing the bounds of what “counts” as an algorithm, the\ntwelfth-century medieval European use of split tally sticks offers an\nexemplary instance of how algorithms are able to objectify knowledge. A split\ntally stick was a device for keeping track of loans and debts, useful in a\nworld lacking both coins and mass literacy. To record a debt, the numerical\namount of goods or monetary equivalent was carved into a\nstick.[3](19_Notes.xhtml#ch3fnr3) If a farmer lent another farmer twenty\nsheep, twenty cuts would be physically etched into the stick. The now-\ninscribed stick was then broken in half across the inscribed tallies. Each\nhalf contained part of each original mark; and both parties were given a half\nof the stick as a receipt for the transaction. The possibility for fraud was\nstructurally precluded: the debtor was unable to erase any of the marks, since\nthey were cuts into the wood, while the lender was unable to add any\nadditional debts to the count, as they would only appear on his half of the\nstick.\n\nIn the parlance of modern computer science, the use of split tallies sticks\nleveraged the immense “search space” of wood—search space being the number of\npossible solutions to a problem, such as all the possible four-digit codes to\nunlock a cell phone. One of the only ways someone could cheat the system\n(short of stealing their rival’s physical stick) was to find a similar stick,\ncarve fewer notches in it, and break it exactly the same way. The sheer number\nof types of sticks—both in terms of wood, but also size and growth pattern—and\nthe near-infinite number of ways that a stick can break, constitute the search\nspace of the problem: the number of possible sticks and breaks one would need\nto search through to find a suitable replacement to cheat the system. The\ntally stick algorithm “worked,” and in working, it provided something vital\nfor the economies of that time: a physical object that could account for\ncommercial affairs.\n\nThe efficacy of the split tally stick’s method of accounting attests to the\npower of objectification. As with the definition of algorithms themselves,\nthere has been a long and healthy theoretical debate about how this concept\nplays out. For our purposes, Marx’s description of the process of\nobjectification, especially in the first volume of _Capital_ , is critical to\nunderstanding algorithms, machine learning and\nstatistics.[4](19_Notes.xhtml#ch3fnr4) At its heart, objectification is a\ntheory of how objects think, remember and work for us. For Marx,\nobjectification is not, as in contemporary idiom, the process of treating a\nperson like an object; rather, he defines the term as using objects to manage\nhuman affairs.\n\nOne experiences this process of objectification acutely when airline\nrepresentatives look at their computer screen after a canceled flight and\nsympathetically say, “Sorry, but there is nothing I can do; the computer won’t\nlet me change anything.” One can decry the situation all one wants, but it is\nclear the system has been built such that even the most persuasive of\ncritiques will not book a different flight. Tellingly, airline representatives\nare no longer called gate agents; they have surrendered their agency to the\ncomputer system.\n\nThis process is precisely what lies at the heart of algorithmic power:\ncomputation presents us with an “objective”—and hence politically\nunassailable—description of a given sociopolitical situation. The force of\nthese objective facts inheres not in their correspondence with some external\nreality, but in their ability to directly produce a new, distinct social\nreality. For example, what matters most about credit reports is not their\nability to accurately describe an individual’s creditworthiness, but their\nability to produce a world that corresponds to their priorities by limiting\naccess to capital; indeed, it is telling that they are increasingly factored\ninto hiring decisions.[5](19_Notes.xhtml#ch3fnr5)\n\nAt its most fundamental level, the process of objectification serves two\ninterrelated purposes: first, it allows an object to remember for us; and\nsecond, in doing so, the object also _believes_ for us. This activity of\nmemory and belief is simultaneously quotidian and nothing short of magical.\n\nWith the tally stick, the notches remember for our two peasants exactly how\nmany sheep changed hands. And because of the ingenious system of splitting the\ntally across two sticks, it becomes possible for both of the peasants to\nbelieve what the stick remembers for them. In fact, this technology became so\neffective that King Henry I instituted its official use for tax collection in\n1100, a practice that was kept in some use throughout England until the early\nnineteenth century.[6](19_Notes.xhtml#ch3fnr6) Moreover, due to its economic\nimportance, Henry I’s edict endowed these sticks with the force of law. It\nbecame not only easy, but functionally necessary, to believe what the sticks\nremembered.\n\nJust like a computer, these tally sticks function as memory devices. Our\npeasant can safely put the number of sheep he lent out of his mind, for it is\ninscribed in the stick. Also like computers, they aid in computation. With\nmultiple sticks one can easily add, subtract and account across multiple\ntransactions. Moreover, they are effective tools for communication. If our\npeasant were to pass away, his heirs would have little trouble understanding\nthe exact amounts of his various debts. Debt thus becomes objective, carved\ninto the material world. As a result of the tally stick, one no longer needs\nto believe or remember; it is there in the world for anyone to see.\n\nContemporary understandings of objectification have strayed slightly, but in a\ntheoretically important way, from this reading of Marx. Georg Lukács is\npartially responsible for this wandering with his interpretation of\nobjectification—or reification, as the term is often translated with his\nwriting—as the point when “a relation between people has taken on the\ncharacter of a thing.”[7](19_Notes.xhtml#ch3fnr7) Rather than focus on the\nprocess by which objects manage our affairs—even if the outcome is that\nrelationships are taken as objects—Lukács is concerned with the outcome of\nthis process and what it occludes. On this level, his conclusions are half\nright. Objectification _is_ obfuscatory, but what is obfuscated is not\nfundamentally that people or relations are treated as objects, but rather that\nobjects are made to think in the place of people. This strand of thought moves\neven further from Marx when, as is common today, objectification comes to mean\nthat individuals, rather than relationships, are treated as\nobjects.[8](19_Notes.xhtml#ch3fnr8) These theoretical developments are not\nnecessarily wrong, and in many ways they are the outcome of capitalist\nobjectification, but still they risk overlooking the processes through which\nobjects account for social relations.[9](19_Notes.xhtml#ch3fnr9)\n\nThus, while many commentators present objectification as a type of “false\nconsciousness,” where one forgets the true social relations that underwrite,\nprocess and thus muddy even the most clear-sighted analyst’s mind, it should\nbe clear from the example above that, for present purposes, objectification\ncan be read as significantly more neutral.[10](19_Notes.xhtml#ch3fnr10)\nObjectification is a form of forgetting, but one that is directly productive.\nIt allows objects to relieve us of the necessity of remembering, an\nunburdening of our minds that is sometimes\nbeneficial.[11](19_Notes.xhtml#ch3fnr11) For any objectification, the question\nwill always be who is burdened, who is unburdened. This unburdening is the\nessence of all computing—and all recording and communication technology. From\nthe first etched clay tablet to the latest $200 million supercomputer, all of\nthese technologies remember our affairs for us. And, in remembering for us—our\ntransactions, our social relations—they hold us accountable to the traces of\nthese interactions. While Marx lays out a theory of objectification in terms\nof value, we can see the same process and force at work in computation.\n\nCommodities and Objectification\n\nIn the opening pages of _Capital_ , Marx launches his entire theory of\ncapitalism with the simple commodity and its powers of objectification.\nEverything in his analysis expands outward, from the relations between\ncommodities themselves to relations between laborers and capitalists. Marx\nproposes a radical understanding of objects-as-commodities: what is objective\nabout these objects is not that they are useful, but rather that the social\nrelations between producers, labor and purchasers are congealed into the\nobjects themselves. Commodities “hide” the very relations and histories that\ndetermine their value as they come to market.\n\nMarx, following the economists of his day, argues in these early pages that\nwhat determines the economic value of an object is not how useful it is—some\nof the most useful things in the world are the cheapest—but rather the amount\nof labor that goes into its production. Accordingly, economic value is not\ninherent in the object. It is, rather, a reflection of the entire social\nprocess of production, work, banking, and so on. For Marx, “a use value, or\nuseful article, therefore has value only because abstract human labour is\nobjectified or materialized in it.”[12](19_Notes.xhtml#ch3fnr12) In this way,\ncapitalism, in essence, operates already as a giant distributed and opaque\nalgorithm, computing and remembering the amount of labor that should go into\nthe production of any product, then presenting the output of this calculation\nanytime we go to buy something. While it may seem irrational to believe that\nphysical objects have economic value as an intrinsic property, capitalism\ncompels one to act as if they do.\n\nThus, for Marx, these objects serve an immensely important role in so much as\nthey record social relations and then present them back to us as the objective\nstate of things. In his words, “The object which labor produces—labor’s\nproduct—confronts it as something alien, as a power independent of the\nproducer. The product of labor is labor which has been embodied in an object,\nwhich has become material: it is the objectification of\nlabor.”[13](19_Notes.xhtml#ch3fnr13) The syntax here is elucidating: it is the\nobject that is objectified. As soon as labor is poured into a commodity, the\ncommodity appears to hold onto that expended labor in the form of economic\nvalue, carrying it wherever it goes regardless of the wishes or beliefs of its\nproducers or owners.\n\nMoreover, this commodity-as-object is fundamentally computational and\nnetworked; it relates all laborers and consumers to each other. Marx further\nsays of the commodity:\n\n> A commodity may be the outcome of the most complicated labour, but through\n> its _value_ it is posited as equal to the product of simple labour, hence it\n> represents only a specific quantity of simple labour. The various\n> proportions in which different kinds of labour are reduced to simple labour\n> as their unit of measurement are established behind the backs of the\n> producers.[14](19_Notes.xhtml#ch3fnr14)\n\nHere, Marx presents an equalization of labor that ultimately lays the\ntheoretical foundations for his analysis of the mechanisms of capitalism\nitself. For Marx, all economic value is based on the accumulation of simple\nlabor: the amount of time it takes an average unskilled laborer using\navailable technology to produce a commodity. While competing theories of value\nexist, any rigorous concept of value will necessarily “reduce” a variety of\nspecific practices of labor to a single economic value—a value that can be\ncompared to other commodities, which each have their own value. In this way,\nall value is relative only to some other value.\n\nFor example, an amateur could spend ten painstaking years learning, failing\nand testing to produce a light bulb of equivalent quality to a generic\nsupermarket brand. Despite the difference in production, and short of\nconvincing consumers that the amateur’s repeated folly adds some artisanal\npatina, that bulb will be valued equal to the supermarket bulb created in mere\nminutes. In this determination of value, the market thinks for consumers,\nreducing all value to a network effect that somehow—as if by magic—computes\nthe amount of labor it _should_ take to make a commodity. Moreover, this\ncomputed value is itself a function of how much it costs for laborers\nthemselves to live and reproduce. Like an algorithm, the market computes a\nsolution to the problem for use in current and future decisions.\n\nThe commodities that labor produced come to account for human affairs “behind\nthe backs” of workers. The bulb’s value, in relation to all other value,\ndetermines not just how much is paid by consumers, but the entire network of\nvalue that points only to itself. This commodity-as-object emerges as the\nagent that structures capitalist society: the object itself is a revolutionary\nforce. Like a globally distributed social computer, the wide array of\ncommodities across the capitalist marketplace tracks the value of all of the\nlabor that goes into their production, constantly updating as conditions\nchange. In a sense, the commodity has the whole universe folded inside it.\nCommodity exchange, by allowing the real-time calculation of these ratios, in\nessence becomes a form of “machine learning” in a much broader sense than the\nterm usually connotes.\n\nTo see this in action, we need look no further than the capitalist invention\npar excellence: the stock market. In this market, traders attempt to divine\nwhat others will believe the future value of a company to be. What is fully a\nhuman process of production and exchange appears, to the trader, as an\nunknowable and fickle system whose whims are even less predictable than the\nweather. Listen to the headlines on any given day: “Stocks retreat from\nrecords after winning streak”; “Stocks limp to the end of another winning\nweek.” Both journalists and readers should know better—stocks are not freely\nacting agents and thus do not “retreat” or “limp.” But they, and we, still act\n_as if_ they are, and do. Stocks appear as objective; they confront us and,\nregardless of whether we believe in them or not, require that we come to terms\nwith their reality.\n\nPrefiguring the logic of computation, objects function on a plane of formal\nequality, one where even the most unequal is made computable. Like a stock\nmarket list of a thousand different firms’ acronyms, the functional,\nqualitative differences between each corporation are collapsed. By reducing\nall exchange to the capitalist conception of value, all types of objects are\nmade equal, or at least equatable. Moreover, once this process is underway, it\ncarries on without our consciousness involvement: “They do this without being\naware of it.”[15](19_Notes.xhtml#ch3fnr15) In essence, the objectification of\neach commodity collapses value, and with it, labor, onto the same scale. It\nmakes the resulting object computable such that one no longer even needs to be\naware of how that object came to be. Commodities do this magical work by\ncreating the appearance that value is somehow internal to the object itself.\nAnd so, value too appears as a “ghost of a departed quantity” of the labor\nthat produced it.\n\nObjectification has prepared the world for its computation, and the\ncontemporary world, managed by information capitalism, statistics and\nalgorithms, has taken up this mantel of objectification. Though they clearly\nhave not replaced the objective power of wage labor and commodities, these\ncomputational forces manage, maintain and reshape those capitalist\nprocesses.[16](19_Notes.xhtml#ch3fnr16) Algorithms track, process and account\nfor significant portions of financial, social and scientific interactions,\nwhile statistics provides the metaphysical ground of these systems, explaining\nand justifying how individual interactions can be combined and computed into\nsocial truths. In short, statistics provides the objectifying force of\nalgorithmic knowledge and the equatability necessary for its computation, just\nas labor provides the force for capitalist value production; and labor’s\nreduction to value provides the grounds of its computation in the great social\nnetwork called the market.\n\nObjective Metaphysics\n\nObjectification is ultimately a metaphysical process. It sutures a\nmetaphysical structure to the material world, where individual commodities—as\nwell as algorithmically produced outputs—“transcend” their particular\nconditions in order to connect to a whole networked world of others. Marx\ndescribes the object as “abounding in metaphysical subtleties and theological\nniceties.”[17](19_Notes.xhtml#ch3fnr17) He further makes the theological\nnature of this process explicit, stating:\n\n> There it is a definite social relation between men, that assumes, in their\n> eyes, the fantastic form of a relation between things. In order, therefore,\n> to find an analogy, we must have recourse to the mist-enveloped regions of\n> the religious world. In that world the productions of the human brain appear\n> as independent beings endowed with life, and entering into relation both\n> with one another and the human race. So it is in the world of commodities\n> with the products of men’s hands.[18](19_Notes.xhtml#ch3fnr18)\n\nObjectification is, in a way, even religious (which we see again when Marx\nuses this process as the grounds of commodity\nfetishism).[19](19_Notes.xhtml#ch3fnr19) The object performs the mysterious\nand metaphysical work of equating its own individual being with a global world\nof exchange, maintaining the relationship between use value (what is useful\nabout a commodity) and exchange value (what it is worth on the\nmarket).[20](19_Notes.xhtml#ch3fnr20)\n\nJust as with religious mysteries, once the machine is set or once the theology\nis instantiated, any new analysis can only produce results that confirm the\nsystem. A dip in a stock price suggests a company is in peril. A new\nscientific discovery can be integrated into God’s plan for the world. Belief\nin objects’ value begets belief in the system, a vicious cycle that\ncomplicates any politics founded on an apprehension of the world as it really\nis. As soon as the object and its value are naturalized (e.g., the tally stick\nis agreed to represent a certain debt), the game is set. Objectification\nproduces the default; it delimits the world by defining the relevant\nstatistical reference classes or the stakes by which engineers and\ncorporations will determine whether an algorithm has succeeded.\n\nIt is for this reason that attempts at dereification, to show what is really\ngoing on, are so challenging, for they tend only to show what everyone already\nknows is ridiculous. Marx writes: “Value, therefore, does not have its\ndescription branded on its forehead; it rather transforms every product of\nlabour into a social hieroglyphic. Later on, men try to decipher the\nhieroglyphic, to get behind the secret of their own social\nproduct.”[21](19_Notes.xhtml#ch3fnr21) Many progressive critics have followed\nthis road of hieroglyphic interpretation, tending not to seek the nature of\nthe system producing value but rather only the value of this or that product,\ncondemning all subsequent analyses to a _post festum_ moment in which it is\ntoo late to question the underlying structure of the social world.\n\nThese attempts at dereification take up the task of unearthing what has been\nentombed in the seemingly natural ground beneath one’s feet, focusing solely\non the meaning of what has been buried, rather than on the process. Once the\nmold of objectification is cast, and objects begin to handle human affairs\n(which then become the objects’ affairs), it becomes nearly impossible to\narrive at a conclusion outside the hard-coded presumptions set in that mold.\nEven when one can see through the haze of these objectified relations, they\nstill have to act as though they are true. These objects come to underwrite\nthe form of their own relations, as in the case of money: it is obvious that a\npiece of paper is not worth anything, but it is still imperative to act as\nthough it is. Pointing out its worthlessness will change very little.\n\nObjectification: Not a Blueprint\n\nObjectification, as it is understood in this sense, is not an ideology in the\nstrict sense, nor is it the totality of politics. It is decidedly not a\nmisidentification of the social world whose correction would guarantee the end\nof all injustice. Rather, it functions as the means by which ideology,\npolitics and social relations in general are given a non-locatable force. In\nhis book _Black Marxism_ , political scientist Cedric Robinson argues for “the\nnonobjective character of capitalist development,” giving the first chapter on\nracial capitalism this subtitle. By this he means that capitalism does not\nfollow some definitive laws of historical or dialectical development,\nespecially not those that would see capitalism increasingly rationalize\nproduction and strip away old prejudices of race, gender and nation. Likewise,\nabolitionist scholar Jackie Wang, in _Carceral Capitalism_ , shows how the\npredatory violence of the modern neoliberal state, exemplified by the United\nStates, where mass incarceration, especially of Black men, constitutes one of\nthe “gratuitous forms of racialized state violence that are ‘irrational’ from\na market perspective.”[22](19_Notes.xhtml#ch3fnr22) It is these various forms\nof violence, from enslavement to mass incarceration, then, that mark the\nnonobjective nature of development. Robinson says, of the “development of\nrevolutionary consciousness among Black and other Third World peoples” that it\n“broke with the evolutionist chain in, the closed dialectic of, historical\nmaterialism.”[23](19_Notes.xhtml#ch3fnr23)\n\nIs it still possible then, in this light, to speak of objectification,\nespecially if we accept the nonobjective nature (or at least element) of\ncapitalist development, or in Wang’s terms, a sadistic element to white\nsupremacy (and also to gendered violence) that does not fit, or outpaces,\neconomic analysis?[24](19_Notes.xhtml#ch3fnr24) For this precise reason, it is\nimportant not to understand objectification as a form of false\nconsciousness—as a misguided perception that can be rectified, and that (as\ncertain arguments for historical evolution argue in the case of racism and\nsexism) capitalism will or could rationalize away. In this way,\nobjectification would define not a rationalization of domination and violence,\nbut rather its abstraction, its loss of a determinable locus, to invoke\nPostone’s work from earlier. Thus, objectification is not a rationalization,\nas the term is normally understood, but the process by which concrete\ndomination (e.g., racism, sexism, imperialism and class exploitation) is\ntranslated into an abstract form whose origination and social elements appear\nto recede behind its objective mask.[25](19_Notes.xhtml#ch3fnr25)\n\nGrowing concerns about YouTube’s video recommendation algorithms provide an\nexample of this dynamic. A number of commentators have documented how the\nrecommendations tend to push viewers toward reactionary conspiracy theory\ncontent, especially toward far-right, white-supremacist and misogynist videos,\nmaking YouTube into what effectively amounts to a radicalization\nmachine.[26](19_Notes.xhtml#ch3fnr26) The algorithm is not explicitly designed\nto push increasingly reactionary content, but rather to encourage users to\nkeep watching in order to sell more ads. It “just happens” that the algorithm\nhas discovered the most efficacious way to do this is to radicalize viewers;\nin short, it follows the “objective” laws of the market—attempting to sell as\nmany ads as possible. As scholar of Black studies and digital culture Ramon\nAmaro describes such systems, “What we experience today as algorithmic\nprejudice is the materialization of an overriding logic of correlation and\nhierarchy hidden under the illusion of\nobjectivity.”[27](19_Notes.xhtml#ch3fnr27) Thus, objectification processes the\nsubjective domination inherent in the system, constantly seeking new ways to\nextract value from it and thus intensify it. YouTube, in following these\nobjective laws, reproduces and intensifies racist and sexist content, further\nincreasing its potential to produce profit from this content in the future.\nThe logic of ad sales thus provides the form and force by which white\nsupremacy and other far-right ideologies, including their oftensadistic\n“nonrational” elements, are reproduced and incentivized.\n\nThis decidedly does not mean that racism and sexism are accidental to\ncapitalism in general, or, in this case, to the sale of advertisements.\nIndeed, these forms of domination allow the very production of profit in this\ninstance, and, in the broader case of capitalism, support various forms of\ntheft, exploitative and forced labor conditions and nationalist fear that\ncapitalism requires for its reproduction. The point is that while racism and\nsexism are made to function in certain ways, made abstract though the drive\nfor profit, they are not reducible to capitalism. Moreover, this does not\nimply that YouTube’s algorithm should not be resisted or objected to. In fact,\nit is through criticisms of such instances that it may be possible to envision\nsome other objectification that could provide further ground for resistance.\n\nWhile the roots and causes of discrimination may lie before and outside of\ncapitalist development, the “objectivity” of the market gives this violence a\nspecific form that at the same time makes it increasingly difficult to locate\nits ground. Moreover, by further segregating populations, both physically and\ndigitally, and preventing certain people from having access to space and\ncapital, capitalism reinforces and economically incentivizes concrete\ndomination—which, in being objectified, interlocks these concrete and abstract\nforms of racial and gendered violence. In short, objectification becomes the\nmeans through which concrete domination and violence are given abstract form\nand then translated back again into the concrete; only now, this occurs with a\nforce whose origin appears absent. We shall see that this is precisely how\nBayesian statistics functions: whether or not the outcome entails violence, it\nprovides a method for converting subjective beliefs about the world into\nprobabilities that have objective force. In short, its proponents’ mantra will\nbe: “Individuals can believe whatever they want, but we will show you how to\nproduce value from those beliefs.”\n\nFrom this, two key points should be noted: First, objectification is not\ntotalizing, even as it shapes and interacts with a multitude of forms of\nviolence. Sadistic forms of concrete violence still operate and drive society\nin unobjectified form and have clearly locatable perpetrators (e.g., those\nthat appeared during the Trump administration, and other resurgent\nnationalisms worldwide). Second, while the present text focuses on the\nmetaphysics of objectification, abstract domination cannot be separated from\nconcrete domination. The abstractions that are at stake are “real\nabstractions,” in that they depend on material conditions and directly bear on\nlife and the ability of individuals to live. Moreover, even though concrete\ndomination cannot be reduced to market incentives, such domination serves to\nprovide a variety of means for expropriation that directly produce value, from\nprivate companies that profit off of imprisonment to the exploitation of\nunderpaid labor in the global South. Thus, any resistance to capitalism cannot\nsucceed solely on the level of the abstract but must simultaneously change\nconcrete conditions.\n\nBitcoin\n\nThe function of objectification can be seen clearly in the twenty-first-\ncentury update of the tally stick: the blockchain. Developed as part of the\nBitcoin digital currency algorithm, the blockchain has become the backbone of\nnumerous attempts to “objectively” account for online transactions. Run\nthrough tens of thousands of different computers across the globe, this\ndistributed blockchain acts as an algorithm that automatically accounts for\nthe entire Bitcoin system.\n\nBitcoin’s algorithm confirms transactions by leveraging the lengthy periods of\ntime it takes to compute cryptographic functions, unless one has the secret\nkey. By rewarding “miners” with slivers of its currency to use these\nsignificant amounts of computing time to confirm transactions, the blockchain,\nin theory, is able to automatically confirm transactions and take care of\nitself, removing the need for any central monetary\nauthority.[28](19_Notes.xhtml#ch3fnr28) It is through this “proof of work,” in\nessence the use of extensive processing time and hence electricity to search\nthrough possible solutions, that the ghost of this departed work “objectively”\nappears to mark some amount of value.\n\nThe advancements heralded by Bitcoin’s technology have led many to see a\nrevolutionary potential as the computerized maintenance of the blockchain\nremoves the need for human oversight and even agency; for instance, a recent\ngroup of activists has advocated what they call “cypherpolitics”—an approach\nthat supposedly leaves the entire notion of belief behind. One of its\nexponents, Stacco Troncoso of the P2P Foundation, writes: “There is no human\nway of knowing if someone has expressed the truth. This can only be verified\nthrough technology. The only way for someone to subscribe to a cypherpolitics\nis to leave all traces of belief systems behind and only maintain the\nabsolutely essential approximation of the ‘truth’ … By replacing physical\ntrust quotas with immutable code, the blockchain resolves this\nissue.”[29](19_Notes.xhtml#ch3fnr29)\n\nFor cypher activists like Troncoso, it is the immutability of code—like the\nimmutability of a broken tally stick—that allows technologies like blockchain\nto stand in for belief. Yet, in order for the blockchain to truly function,\nlike any commodity or representative of value, one still must believe in the\nobjectified form. It is thus unlikely that the development of cryptocurrencies\nwill result in revolutionary change, since they are ultimately founded on the\nsame objectified politics of contemporary capitalism. From scratches on a\npiece of wood to a secret string of numbers and letters, both tally sticks and\nblockchains are systems of economic management that are also systems of\nobjective belief—representations that are treated as equatable with value.\nWhen one accepts this belief, or labors under it even without believing, one\nremains locked into the world of commodity exchange, watched over by the ghost\nof a departed computation. Even the realization that the blockchain guarantees\nnothing changes very little, as long as others believe in its value.\n\nLikewise, statistics—the operative logic by which machine learning\nalgorithmically produces knowledge about the world—is sanctified with similar\nepistemic weightiness, objectified by its seemingly mysterious ability (in the\nsense outlined in the preceding chapter) to equate particular data with\nuniversal laws. It is through statistics that social facts, presented as data,\nare objectively turned into metaphysical truths that stand outside of the\nphysical world and determine our social existence—whether we believe them or\nnot.\n\nUnder capitalism, the objectification of the market is disastrous and\nexploitative, but that does not necessarily mean that the form of this\nobjective forgetting is necessarily negative. It is worth revisiting Marx\nhere—in particular, his _Grundrisse_ , where he argues that the\nobjectification of labor in machinery and its rational organization or\nmanagement allows for a reduction of necessary labor\ntime.[30](19_Notes.xhtml#ch3fnr30) Capitalism renders labor increasingly\nsuperfluous, a process that could liberate individuals from the labor process,\nif not for the dynamic that animates capitalist society: the abstract drive to\naccumulate greater and greater quantities of surplus value, which requires the\nexpenditure of labor. As such, the mystery of objectification, according to\nMarx, is both liberatory, insofar as it makes labor less necessary and allows\nus to think otherwise, and also exploitative, for it allows the capitalist\nextraction of greater quantities of surplus labor from the labor process.\n\nThese mysteries of objectification, while clearly implicated in the violence\nand devastation of capitalism (and other systems), still provide for other\npossibilities. As critic and theorist Fred Moten, drawing on the work of Randy\nMartin, argues:\n\n> The production of surplus—along with that which it produces and is produced\n> by, “race, class, gender and sexuality as the very materiality of social\n> identity”—has reemerged here with a vengeance. Surplus is the very magic of\n> objects, their fetish character, their mysterious secret. That magic can be\n> terrible … But there is also a liberatory force of the surplus, the magic of\n> objects, which we see here in the midst of its very\n> transformation.[31](19_Notes.xhtml#ch3fnr31)\n\nIt is here that these mysteries distribute the separation between object and\nsubject, between objectification and subjectification, determining, along\nlines of race, class, gender and sexuality, what counts as surplus value, and\nwhat can be expropriated in different ways and with varying levels of\nviolence. For Moten, other possibilities—potentially liberatory ones—lie in\nthis process.\n\nWhat arrives as dangerous and potentially oppressive in the process of\nobjectification is not its formal structure, but the specific—and\nunseen—social relations, debts, accounts, transactions and force that hold us\nto these objectified facts, especially as they become abstract, proprietary\nand operate at massive scales. The power of this forgetting, and the ability\nto allow objects to think for us, is not necessarily negative—indeed it may\neven be full of possibilities. But for such a program of liberation to be\neffective, it must not limit itself to the surface effects and means of the\nmetaphysics of exchange, to interpreting what has already been decided;\nrather, it must change the underlying structures of exchange and\nobjectification, along with the social relations that are left to algorithmic\nobjects. This is the task of a revolutionary mathematics: to create new\nmysteries, rather than simply attempt to repair those that capitalism has left\nus.\n\n\n[PART II](04_Contents.xhtml#ch7)\n\n[The Promise  \nof Frequentist  \nKnowledge](04_Contents.xhtml#ch7)\n\n> Cease from grinding, ye women of the mill; sleep late even if the crowing\n> cock announces the dawn. For Demeter has ordered the Nymphs to perform the\n> work of your hands, and they, leaping down on top of the wheel, turn its\n> axle, which with its revolving spokes, turns the heavy concave Nysirian\n> millstones. We taste again the joys of primitive life, learning to feast on\n> products of Demeter, without labor.\n>\n> —Antipater of Thessalonica, on the waterwheel\n\n\n[Chapter 4](04_Contents.xhtml#ch8)\n\n[Do Dead Fish Believe in God?](04_Contents.xhtml#ch8)\n\nThrough statistics and probability, algorithmic systems think for us. But in\ndoing so, machine learning algorithms only ever produce probabilistic\nstatements about the world, whether in regard to election results, recidivism\nrates or the best ads to show a consumer. And, while the mathematics of\nprobability are relatively straightforward and usually calculated through rote\ncomputation, the ultimate meaning of probability is notoriously slippery. To\nunderstand its force today, one must trace its metaphysics and history through\nthe twentieth century.\n\nIn general, people struggle to understand probability. One area of daily life\nin which this problem is evident is the interpretation of weather\nforecasts.[1](19_Notes.xhtml#ch4fnr1) The greatest confusion often surrounds\nthe issue of what class of events a certain measure of probability refers to.\nFor example, “60 percent chance of rain” is sometimes interpreted to mean that\n60 percent of the covered area will receive rain, or conversely that it will\nrain for 60 percent of the time in question. The correct definition of a\nprobability of precipitation (PoP) is the probability that at any given\nlocation within the forecasted area there will be precipitation.\n\nIn terms of mathematics, then, the definition of PoP is the probability of any\nmeasurable precipitation in the forecasted area multiplied by the percentage\nof the total forecasted area that will receive\nrain.[2](19_Notes.xhtml#ch4fnr2) If there is a 90 percent chance that some\npart of the area will receive rain, but only in 50 percent of the area, the\nPoP is 45 percent. This is because a randomly chosen place has a one out of\ntwo chance of being in the precipitation area, and that area has a 90 percent\nchance of rain. We multiply them together to get a probability of 45 percent.\n\nYet even with the math sorted, what probability means in practice is not at\nall straightforward, even among the scientific\ncommunity.[3](19_Notes.xhtml#ch4fnr3) First, as long as the probability is\nneither 0 percent nor 100 percent, probabilistic predictions are, at their\nmost basic level, unfalsifiable. As climate scientists Ramón de Elía and René\nLaprise state, “Any person can provide a forecast for the probability of\nshowers for tomorrow by just issuing a number between 0% and 100% without\nconsidering the characteristics of the atmosphere. This forecast, which is\njust a numerical representation of an unfounded opinion, is impossible to\nprove wrong.”[4](19_Notes.xhtml#ch4fnr4)\n\nFurthermore, if one is predicting the weather for tomorrow, there remains a\nstubbornly empirical constant in reality: either it will rain or it will not.\nThere is, in a metaphysical sense, no such thing—outside the complexities of\nquantum mechanics—as a “probabilistic event.” This is what statistician Jerzy\nNeyman concludes in his famed explanation of confidence intervals: “Can we say\nthat in this particular case the probability of the true value is equal to α?\nThe answer is obviously in the negative.”[5](19_Notes.xhtml#ch4fnr5) This\npessimism is simply because either the true value is equal to _ɑ_ or it is\nnot.\n\nThus, despite the importance of probability to statistics—and many more\nscientific fields—no consensus exists on what probability actually means. The\nAmerican statistician Leonard Savage sums up the nature of probability in\nregard to statistics well: “It is unanimously agreed that statistics depends\nsomehow on probability. But, as to what probability is and how it is connected\nwith statistics, there has seldom been such complete disagreement and\nbreakdown of communication since the Tower of\nBabel.”[6](19_Notes.xhtml#ch4fnr6)\n\nWhile mathematicians, scientists and statisticians have become proficient at\ncalculating the mathematical equations of probability, the underlying question\nof what probability itself _means_ remains a complex metaphysical question.\nMathematician Henri Poincaré argued there are two levels on which these\nproblems function: the metaphysical and the mathematical. “Every probability\nproblem involves two levels of study,” Poincaré writes. “The\nfirst—metaphysical, so to speak—justifies this or that convention; the second\napplies the rule of calculus to these conventions.”[7](19_Notes.xhtml#ch4fnr7)\nIn many ways, the two have evolved and changed together, but the metaphysical\ninterpretation has often defined what is mathematically possible and how to\nmake sense of the mathematics.\n\nIn its earliest uses, the term “probable” meant that opinions were attested to\non good authority. It was not until the seventeenth century that the term came\nto mean that a certain event or outcome was actually more likely than\nothers.[8](19_Notes.xhtml#ch4fnr8) Along with this shift in meaning,\nmathematicians made significant advances in calculations of probability, but\nthey initially focused mainly on events whose outcomes were equally likely,\nsuch as a coin flip, a die roll, or drawing a card at random from a deck. In\nthese cases, probability appears as a natural outgrowth of discrete,\nmeasurable states. For example, a coin is supposed to land only on heads or\ntails, and a die roll is always a whole number, normally between one and six.\nOne can calculate these probabilities through elementary division.\n\nThis understanding of probability was powerful, and it even enabled\ncalculation of some events that were not equally likely. In these cases, the\nbasic unit of calculation—drawing a single card—was held to be equally likely,\nand could be used to calculate the probability of an unequal event (the\nprobability of drawing a heart or face card). This method becomes possible\nprecisely because the world of cards is closed, represented by a finite number\nof fifty-two states from which suits and face cards are distributed in\nspecific and known proportion. While notable advances were made on the\ncalculation of probability between the seventeenth and nineteenth centuries,\nthis classical understanding remained relatively simplistic, and it worked\nonly for these discrete applications. The approach had important conceptual\nlimitations: it failed to account for systems that were not closed, were not\nphysically symmetrical and were not based on initially equally likely\noutcomes, such as the probability of it being a certain temperature tomorrow.\n\nIn light of these limitations, over the course of the twentieth century, and\nbuilding on work in the late nineteenth century, what became known as the\n“frequentist” description of probability came to the fore. For frequentists,\nprobability represented something more empirical than ideal: probability was\nnot the ratio of some equally likely event (such as the one-in-two possibility\nof getting heads), but the long-run frequency of a physical system. Under\nfrequentism, if, after hundreds of flips, a coin lands heads 60 percent of the\ntime, the probability of heads would be 60 percent. This new, longitudinal\ndefinition opened up the theoretical space for significant advances in\nstatistics and probability theory. Now, frequentists could theorize and quite\naccurately describe the probability of a system—such as a coin that is not\n“fair” (e.g., asymmetrical) or the temperature—allowing mathematicians to move\nbeyond the ideal, confining necessity of the “equally likely.” The\nimplications of these definitions (probability as equally likely events,\nversus as frequency over a long run) demonstrate how our understanding of what\nprobability means ultimately determines both the limits of what these\ncalculations are able to do and what meanings can be assigned to the results.\n\nIn addition to the metaphysics and mathematics of probability, a third level\nshould be added to Poincaré’s architecture of probability problems: the\neconomy. Here it is worth echoing scholars who have claimed that “there is no\nsuch thing as raw data”—that the data to which researchers have access\ndirectly affects the calculations that can, and should, be\nmade.[9](19_Notes.xhtml#ch4fnr9) What data is available and seen as viable,\nhow that data is incorporated into an experiment and, thus, how that\nexperiment produces knowledge is sullied by the economic needs of state,\nsociety and scientist as incentives to produce knowledge encourage the\noverstatement of results, and in the extreme to deliberate fraud. Peer-\nreviewed academic science production is a particularly illustrative example of\nthe economic dynamics at play.\n\nA Bug in fMRI\n\nIn May 2016, a paper was published in the _Proceedings of the National Academy\nof Sciences_ calling into question a large number of functional MRI studies\nover the past twenty-five years. fMRI, a tool that allows researchers to see\nwhich parts of the human brain respond to various stimuli, has played a major\nrole in modern psychology and brain science. These machines measure magnetic\nvariations in the brain to detect localized variations in the levels of oxygen\nin blood.\n\nWith fMRI analysis, higher oxygen levels correspond to increased neuron\nactivity. Yet sources of noise, like random neuron activity, test subjects\nmoving around, the sensitivity of the testing equipment and so on, can all\nresult in increased oxygen detection that may not necessarily indicate any\ndirect change in neuron activity. Though some scientists may uncritically take\nthe output of such studies as clear expressions of brain\nactivity,[10](19_Notes.xhtml#ch4fnr10) each fMRI experiment is intimately\ndependent on calculations of probability—especially the probability that what\nis observed is due to some meaningful mechanism rather than chance alone.\n\nIn each fMRI experiment, one looks for clusters of concentrated neuron\nactivity. But, given the practical impossibility of avoiding ever-present\nbackground noise, it is possible that a set of voxels (“volumetric” three-\ndimensional pixels) can randomly appear in any fMRI output as a cluster of\nactivity. The more comparisons that are made, the more likely it is that\nrandom chance will appear as significant (for the same reason it is very\nunlikely one single person will win the lottery, but highly likely that\nsomeone will win the lottery). fMRI tests contain a huge number of voxels, and\nthe risk of a false correlation increases proportionally with the number of\ncalculations in a given analysis.[11](19_Notes.xhtml#ch4fnr11)\n\nIf one runs a million statistical tests on any measurable phenomenon, there\narises an almost-guaranteed possibility that something will appear meaningful\nthat is actually not: by random chance, a correlation will be discovered.\nWhile traditional statistical techniques for dealing with fMRI data try to\nmathematically correct for this possibility of error, the paper calling into\nquestion years of fMRI research was based on a discovery that most major fMRI\nanalytical software did not sufficiently correct for these errors. In fact, a\nbug in one fMRI program that overestimated statistical significance had gone\nunnoticed for fifteen years. These software and methodological errors resulted\nin a high risk of what is known as “cluster failure,” where clusters of active\nvoxels falsely appear as neuronal activity.[12](19_Notes.xhtml#ch4fnr12)\n\nThe study detailing the high risk of “cluster failures” involved taking data\nfrom healthy patients with no associated mental task (e.g., subjects are asked\nto imagine playing a sport or remember some earlier event). The patients were\nthen divided into groups in order to discover possible differences in brain\nactivities. These researchers used the standard significance threshold of 5\npercent (a slippery concept that will be explored more fully in the next\nchapter), wherein a researcher expects that 5 percent of comparisons between\ngroups would appear to have a significant difference between groups; however,\nthey found an unexpectedly high number of false positives, with up to 70\npercent of comparisons in the study having erroneously produced statistically\nsignificant results.\n\nAfter the 2016 paper was circulated, many popular publications published its\nfindings with startling headlines that declared decades of scientific research\nnull and void. The _Guardian_ asked, “Has a Software Bug Really Called Decades\nof Brain Imaging Research into Question?” The _International Business Times_\nannounced, “15 Years of Brain Research Has Been Invalidated by a Software Bug,\nSay Swedish Scientists.” And _ZDNet_ proffered the pithy headline: “When Big\nData Is Bad Data.”[13](19_Notes.xhtml#ch4fnr13)\n\nLater, after the shock of methodological failure seemed to wear off,\njournalists took a far more somber, and scientifically reasonable, approach.\nThese latter articles pointed out that this particular problem only accounts\nfor a smaller subset of fMRI research.[14](19_Notes.xhtml#ch4fnr14) One of the\n“cluster failure” paper’s original authors even submitted edits to the initial\njournal in order to downplay some of the more overstated readings of the\npaper. The journal declined the changes, perhaps because the more sensational\nclaims were better for circulation and attention.\n\nThe debate around this “cluster failure” problem raises a number of important\nquestions around the use of probability in the production of modern scientific\nknowledge. A fundamental challenge of statistical inference—and with it, much\nof modern science and knowledge production based on probabilistic\nassessment—is that one can never statistically know anything “for sure.”\nRather than making conclusive statements, researchers try to separate between\nwhat one can claim is due to chance and what one can claim is due to an actual\ncausal mechanism. But, unlike nonstatistical forms of knowledge, statistical\nanalyses always face the possibility that what was observed, at officially\nsignificant levels, was merely the result of random chance.\n\nEven before the 2016 “cluster failure” paper, prior studies had already begun\nto suggest structural problems within fMRI statistical methodologies. One of\nthe most poetic examples comes from 2009, when then–graduate student Craig\nBennett used a dead salmon as a control subject, placed it in an fMRI machine\nand proceeded to show the salmon pictures of humans in emotionally charged\nsituations. Bennett then asked the dead salmon to imagine what emotion the\nhuman in the image was experiencing. The resultant data, uncorrected for\nmultiple testing, clearly shows a small part of the dead salmon’s brain\nlighting up in response to the images. The most likely explanation for this\nresult is not that the dead salmon was thinking, but rather that random noise\npicked up by the fMRI appeared statistically significant. Yet looking only at\nBennett’s calculations—or, looking only at the data—one can never “know” for\nsure if the fish was thinking or not.\n\nWith statistics embedded everywhere via machine learning and algorithms, more\nthan ever we risk seeing a dead fish thinking. In most statistical research,\nresearchers rely on an extensive array of qualifying phrases, where the\nuncertainty of the phrase “most likely explanation” supplants the sureness of\nthe word \"obviously\"; out in the world—as we encounter statistics in\ngovernmental decisions and search results—we rarely get such nuance. Perhaps,\nthen, the challenges of our current situation, the slipperiness of our\nunderstandings, are not so far from those that arise in another story of a\nfish—the Book of Jonah—where that fish must have known something about the\ncomplications of human thought, or at the very least, the discomfort of having\neaten one.\n\nJonah and the Whale\n\nAs told in the Bible, Jonah was ordered by God to travel to the city of\nNineveh and warn the residents of the consequences of their sins, urging them\nto repent. Rather than obey and do God’s bidding, Jonah fled from God aboard a\nship, which was soon beset by a tempest that endangered him and his pagan\nshipmates. In an effort to know whose god had been angered and caused such a\nstorm, the ship’s crew resorted to cleromancy: “‘Come, and let us cast lots,\nthat we may know for whose cause this evil is upon us.’ So they cast lots, and\nthe lot fell upon Jonah.”[15](19_Notes.xhtml#ch4fnr15)\n\nBy this particular falling of the lot (analagous to flipping a coin or rolling\ndice in modern times), the God of the Hebrews had spoken to Jonah and the\nboat’s crew through chance, an enunciation of deistic will. Jonah urged his\nshipmates to throw him overboard. Soon after they did so, Jonah was ingested\nby a great fish, spending three days and three nights inside its belly. Here,\nchance, instead of nullifying the results of an experiment, proved its\nopposite: the cause of the storm and the will of God. In modern\nexperimentation, if the results are due to chance, they tell us nothing, but,\nfor Jonah, chance directly speaks of the truth of the world.\n\nHere, we see how the operations of rational, empirical methods of contemporary\nstatistics are not so far afield from millennias-old theological theories of\nchance, even if the mode of interpretation has radically changed. With his\nexperiment, Bennett drew a hundred thousand lots inside the dead fish’s head\nto see if it can empathize and take part in the world. And by random chance,\nthis dead fish declared to the scientist that it partakes of the world.\n\nStatisticians would like to be able to set the threshold of calculated\nsignificance just high enough to silence random fluctuations, to mute the god\nwho would speak to the world through cleromancy. Yet at the same time, the\nthreshold must not be set too high, for nature would instead become silent,\nunable to emerge from the depths of statistical analysis to reveal its\nsecrets. Statistically determined science skirts a razor’s edge between\nhearing truth in random fluctuations and ignoring the truth of an\nintelligible, measurable nature whose forces advance like\nclockwork.[16](19_Notes.xhtml#ch4fnr16)\n\nIt took until 1710 for God to cease speaking through chance, a historical\nmoment when randomness was instead set in opposition to God. In this year, a\nScottish physician named John Arbuthnot published a short note, “An Argument\nfor Divine Providence, taken from the Constant Regularity Observ’d in the\nBirths of Both Sexes.”[17](19_Notes.xhtml#ch4fnr17) Using data from\nchristenings in London from the period 1629– 1710, a period in which male\nbirths outnumbered female births every year, Arbuthnot calculated the\npossibility of observing these results by chance. If the ratio of male to\nfemale births was even, the odds of seeing another eighty-two years of\nconsistently more male births would be of vanishingly small probability: one\nin nearly five septillion (one half multiplied eighty-two\ntimes).[18](19_Notes.xhtml#ch4fnr18)\n\nFor Arbuthnot, this infinitesimal chance provided evidence not merely of a\ndifference in birthrate, but also for the existence of divine providence. If\nbirths were not random distributions of male and female sex, he concluded that\nthere must be some other causal force at work, which for him could only be\nGod’s action in the world. Arbuthnot’s “Argument” is one of the first\ninstances of statistically testing a hypothesis: instead of God speaking\nthrough chance, for Arbuthnot, statistics allowed God to speak only through\nthe _absence_ of chance.\n\nThus, in contrast to the long-held belief that chance articulates some truth,\nthe advent of hypothesis testing means its ability to speak that truth now\nappears in inverted form. On the one hand, if random chance can produce the\nsame result that one observes—such as a glowing salmon brain—one must maintain\nthe possibility that what was witnessed offers no necessary insights about the\nnature of the world. On the other hand, if one can calculate that what was\nwitnessed was highly unlikely by chance alone, such as some event having a\none-in-a-thousand chance of happening randomly, one would strengthen their\nbelief that they are, in fact, witnessing nature at work. When cleromancy\nrepeats what human instruments say, researchers must reject their initial\nhypothesis and move on to another experiment or theory.\n\nTo return to Jonah, even beyond the facts of fish and lots, the prophet runs\nup against one of the central questions that plagues statistical, and thus\nalgorithmic, knowledge: How are researchers and the public to use these\ninsights? What can be done as a result of the knowledge gained? After praying\nfor God’s intervention, Jonah is expelled from the fish and ordered by God,\nagain, to go to Nineveh. “Then Jonah began to go through the city one day’s\nwalk; and he cried out and said, ‘Yet forty days and Nineveh will be\noverthrown.’ Then the people of Nineveh believed in God; and they called a\nfast and put on sackcloth from the greatest to the least of\nthem.”[19](19_Notes.xhtml#ch4fnr19) The people of Nineveh believe God. They\nrepent, fast, and dress in sackcloth. Even the King of Nineveh sits in ashes\nand fasts. Despite his trust in chance, Jonah grows angry when God recants his\npromise to destroy Nineveh.[20](19_Notes.xhtml#ch4fnr20) Appearing perhaps as\nmore pagan than prophet, Jonah is upset by God’s clemency. After Jonah’s whole\ngastric ordeal, he would fail to see that what he was told was prophecy: a\nNineveh destroyed.\n\nJonah appears today as a ridiculous, obstinate figure. Unlike Cassandra, who\nwas cursed to have her prophetic knowledge of the fall of Troy ignored, Jonah\nwas believed and the people of Nineveh repented. Yet, he would rather be right\nthan believed. He preferred a stubborn, unwavering God who foretells and\nenacts a single future, rather than one who accepts penance, forgives and\nopens the possibility of a different future. If Jonah were spewed from the\nfish’s mouth in the age of big data, he would likely abandon his faith and\nconvert to the god of predictive algorithms and the iron laws of history.\nRenouncing the subject of voluntaristic revolutionary theory, Jonah would\nprefer the side of the philosophers from Marx’s famous quote: “Philosophers\nhave only interpreted the world, in various ways. The point, however, is to\nchange it.”[21](19_Notes.xhtml#ch4fnr21)\n\nDespite trust in chance—or its inversion, in the case of hypothesis testing,\nwhere chance leads to the rejection of a hypothesis—we have found ourselves in\nthe same position as Jonah: Does a crisis averted speak to the power of\nprediction, or does it negate that very power? Do predictive algorithms’\nsuccess, then, speak to the impossibility of reconfiguring our political-\nsocial systems, or, on the contrary, do they provide the means of some as-yet\nunrealized promise of Demeter, who Antipater, quoted in the epigraph to this\npart, believed would free humanity from labor and the injustices it appears to\ndemand?[22](19_Notes.xhtml#ch4fnr22) Ultimately, the metaphysics and\ninterpretations we provide regarding probability, and the statistics and\nmachine learning algorithms that grow out of it, determine what humanity is\ncapable of doing with this knowledge.\n\nLet us return, then, to our dead salmon, who we hope, for the sake of reason,\nremains really and truly dead. Clearly, it is ridiculous to believe that a\ndead fish could think, let alone empathize. While we now know what went wrong\nin the statistical method—the incredible number of calculations done for\nanalysis proportionally increased the risk of a false correlation—scientists\nstill repeat this multiple-testing failure, and similar errors, ad nauseam in\nmodern academic science. The slipperiness of these concepts, and the openness\nof probabilistic statements to the infinitesimally small possibility that an\nobservation is due to chance alone, opens a space where a whole host of\neconomic and material realities can intervene in these processes.\n\nOutside academic laboratories and fMRI machines, but inside the everyday world\nof predictive algorithms, the instability of these concepts is continually at\nwork. As probability (and chance) slides from the mark of God’s presence to\nthe guarantee of his absence, it leaves humans to confront its aporetic power\nand mystery, its ability to simultaneously reveal great truths and leave us\ngroundless and uncertain.[23](19_Notes.xhtml#ch4fnr23) Probability allows\nhumanity to understand an uncertain world but also leaves it exposed to the\nmetaphysical implications of this uncertainty.\n\nThe mysteries of algorithms, statistics, chance and inference are all vexing\nproblems. But they remain problems whose logic and operation is central to our\ncurrent situation. To fully pull apart these questions around chance and the\nproduction of knowledge requires that we trace the development of the meaning\nof probability in modern statistics. We must account for both the metaphysical\nfoundations of statistics and the political economy in which it functions—two\naspects that are intimately tied together. To do so will provide insight into\nboth the possibilities and challenges of statistically produced knowledge,\nespecially as it is given over to the demands of modern capitalism.\n\n\n[Chapter 5](04_Contents.xhtml#ch9)\n\n[Induction, Behavior and the  \nFractured Edifice of Frequentism](04_Contents.xhtml#ch9)\n\nWhile employed as a statistician performing agricultural research at the\nRothamsted Experimental Station in Eastern England, Ronald Fisher developed a\nsystem for experimental design that provides rigorous methods for testing a\nhypothesis. In important respects, it mirrored the intuition of John\nArbuthnot, discussed in the previous chapter, that the inability of chance\nalone to explain some outcome provides proof that some other mechanism is at\nwork. Out of this work Fisher published two influential books in the\nburgeoning field of statistical analyses: _Statistical Methods for Research\nWorkers_ in 1925 and _The Design of Experiments_ in 1935. In these two texts,\nFisher aimed to aid researchers in addressing the central challenge of a\nsignificant portion of modern scientific research: how to determine whether\nobserved difference is due to experimental conditions rather than mere chance.\n\nIn _The Design of Experiments_ , Fisher presents us with the case of Dr.\nMuriel Bristol and a cup of tea with milk, an experiment Fisher calls “the\nlady tasting tea.” Bristol, a colleague of Fisher’s at Rothamsted in the\n1920s, had boldly boasted of her ability to discern through taste whether tea\nor milk is first added to a cup. Within the office, the question of\nverification arose, a more complicated question than it may seem at first\nglance. If one offered Bristol a cup of tea with milk and asked whether tea or\nmilk was added first, she could simply guess and have a 50 percent chance of\nthat guess being correct; so even a correct identification is not proof of her\nability. This is precisely the challenge that statistical testing attempts to\naddress. So, in order to substantiate her claims, Fisher designed an\nexperiment.\n\nFisher proposed the following solution: prepare eight cups of tea, four with\nthe milk added first and four with the tea added first. Randomize their order,\nthen ask Bristol to identify which four were prepared with milk first,\nproviding her the ability to compare the cups with each other. Fisher\ncalculated that there were seventy unique permutations of correct and\nincorrect guesses—so the odds of guessing all correctly are one in seventy, or\napproximately 1.4 percent. Conversely, there are seventeen possible\npermutations to guess three or better correctly out of the four with milk,\nwhich gives a nearly 25 percent chance of guessing correctly. With these odds\nin mind, we would likely accept that Bristol is truly able to determine how\nthe tea is prepared if she selects all four cups correctly. While Fisher does\nnot report the outcome of the experiment, according to secondhand sources\nBristol correctly selected every cup of tea.[1](19_Notes.xhtml#ch5fnr1)\n\nWe can also imagine a scenario where Bristol’s skill only allows her to guess\ncorrectly 90 percent of the time—or even just slightly better than random\nguessing—that makes detection of her ability significantly more complicated.\nIn such a case, Fisher would need to increase the number of cups in order to\ndetect a partial ability to identify the\npreparation.[2](19_Notes.xhtml#ch5fnr2) Yet while Fisher could serve Bristol\nthousands of cups of tea in order to decrease the possibility of chance, he\ncould never fully eliminate it. Even if the odds become one in 1 million,\nthere always remains the lingering 0.000001 percent, the remaining signifier\nof chance’s immortality: the perennial presence of probability.\n\nThe P-value\n\nWhile chance never dies, statistical methods must nonetheless declare it\npractically so in order to make any worthwhile claim about the world. Here, we\narrive at the well-known concept of the “p-value,” or probability\nvalue.[3](19_Notes.xhtml#ch5fnr3) In the case of the lady tasting tea, the\np-value would be presented as 0.014—if Bristol were actually guessing at\nrandom, there is still a 1.4 percent chance that she would guess all the cups\ncorrectly. While calculations of this value date back to centuries-old work by\nstatisticians like Pierre-Simon Laplace and Karl Pearson, it was ultimately\nFisher who popularized its calculation and highlighted its importance for\nmodern hypothesis testing. The p-value has, in part on Fisher’s suggestion,\nbecome one of the primary indicators of scientific validity. Fisher famously\nrecommended that any result with a p-value above 5 percent be considered\nnonsignificant, or conceivably due to chance:\n\n> It is usual and convenient for experimenters to take 5 percent as a standard\n> level of significance, in the sense that they are prepared to ignore all\n> results which fail to reach this standard, and, by this means, to eliminate\n> from further discussion the greater part of the fluctuations which chance\n> causes have introduced into their experimental results. No such selection\n> can eliminate the whole of the possible effects of\n> chance.[4](19_Notes.xhtml#ch5fnr4)\n\nWhile Fisher’s negative claim was that any p-value calculation above 0.05\nshould be considered nonsignificant, many fields, from psychology to\nagriculture, have interpreted it to mean its positive inverse: any result\nbelow 0.05 is to be considered significant. This difference may seem trivial\nand semantic—and many scientists treat it as such—but, it is hugely\nconsequential. One science reporter commenting on the misuse of p-values in\nhypothesis testing went so far as to state: “Statistical techniques for\ntesting hypotheses … have more flaws than Facebook’s privacy\npolicies.”[5](19_Notes.xhtml#ch5fnr5)\n\nThe misuse of p-values as the guarantor of statistical significance, and thus\na means of understanding the results of scientific experiments, has become so\nproblematic that the American Statistical Agency (ASA) went so far as to\nexplicitly release “a formal statement clarifying several widely agreed upon\nprinciples underlying the proper use and interpretation of the p-value” in\norder to “improve the conduct or interpretation of quantitative science,\naccording to widespread consensus in the statistical\ncommunity.”[6](19_Notes.xhtml#ch5fnr6) While the ASA was rather measured in\nits tone, the statement was a clear and forceful rejoinder by the statistical\ncommunity to the broader scientific community.\n\nFisher’s original intention was for any p-value below 0.05 to serve only as a\ncall for further research, not as the establishment of scientific fact. But\ntoo often now, such a result is translated as publishable proof. While there\nis a long history that has brought us from Fisher to our present moment—some\nof which is traced below—one of the major factors contributing to this shift\nhas been the desire to make statistics into an easily followable set of steps\nfor determining scientific truth—in essence to make it automatable, rather\nthan the interpretative tool the founders of the field (and many present\nstatisticians) envisioned.[7](19_Notes.xhtml#ch5fnr7)\n\nIn many ways, modern statistics has been a victim of its own success.\nStatistical analysis’s ability to evaluate diverse types of data has supplied\nthe epistemic grounding to construct entirely new cottage industries, such as\nits oft-celebrated use to forecast elections using multivariate, aggregated\ndatasets, or even to predict civil wars by examining countries’ macroeconomic\nindicators alongside semantic analyses of domestic\njournalism.[8](19_Notes.xhtml#ch5fnr8) But, as the growth of cheap, accessible\ncomputing power and availability of gigantic datasets continues to expand,\nstatistical results with low p-values are still used as the positive\nestablishment of correlationturned-fact, rejecting the need for critical\nreflection. A correlative output that is right most of the time gets treated\nas truth, not as a provisional mathematical output based on a selected set of\ndata.\n\nRather than engage in the complex philosophical debates that underwrite\nstatistics, most statistical applications have ignored any underlying\nconflicts, banking instead on the widespread acceptance of these methods by\neveryone from colleagues to journal editors to science\nreporters.[9](19_Notes.xhtml#ch5fnr9) Fisher was much more guarded. In setting\nup the “lady tasting tea” experiment, he recommended something that was then\nnovel: the creation of a null hypothesis, or a general position declaring that\nthere exists no relationship between two measured phenomena, or that two\npopulations are the same in regard to the measured variable (e.g., the\ntreatment and control group both had similar survival rates). A null\nhypothesis, then, is what an experiment is designed to find evidence against,\nand a successful experiment is one that denies the null hypothesis.\n\nYet, contrary to what is usually taught in contemporary introductory\nstatistics classes, Fisher does not propose the creation of an “alternative\nhypothesis”: the functional opposite of the null hypothesis that would\ndescribe the success of the experiment (in Hegelian terms, the negation of the\nnegation). Fisher states this categorically: “Every experiment may be said to\nexist only in order to give the facts a chance of disproving the null\nhypothesis.”[10](19_Notes.xhtml#ch5fnr10) One could find evidence of a theory,\nsuch as the existence of the ability to distinguish the order of tea\npreparation, but for Fisher this is an ineligible hypothesis because it is\nambiguous.[11](19_Notes.xhtml#ch5fnr11) In Fisher’s design of experiments, one\nsimply gathers evidence against the possibility that chance alone accounts for\nwhat was observed.\n\nIn philosophical terms, Fisher’s theory of statistical inference is ultimately\na theory of difference. The base question that undergirds statistical work is\nwhether or not two groups are different. In the case of medicine, a researcher\nmay seek to know whether or not a new treatment improves outcomes and so tries\nto detect a difference between a control group (given a placebo) and a\ntreatment group. In the case of the “lady tasting tea,” Fisher wanted to know\nif Bristol’s guesses as to whether milk or tea was added first differs from\nrandom chance. Thus, the experimental difference offers evidence for her\nability to detect difference.\n\nWe might trace this fundamental concern with difference back to Fisher’s early\nagriculture work, where he and others labored over the determination of\ndifference between various cultivation methods, different plant species and\ndifferent climates. In this work, everywhere one looks, one sees difference.\nConsequently, research for Fisher was merely the ability to disprove the null\nhypothesis that no difference existed. In such a framework, the primary role\nof science is not to understand causal mechanisms but rather to ascertain when\ntwo groups differ. For Fisher, the universal law that can be derived from data\nis the existence of difference.\n\nFisher, Race and Difference\n\nFollowing his 1925 _Statistical Methods_ , Fisher’s next book, _The Genetical\nTheory of Natural Selection_ , was published in 1930 and served as one of the\nfirst attempts to combine the study of genetics with Darwinian\nevolution.[12](19_Notes.xhtml#ch5fnr12) Fisher’s advancement of this work was\ndeeply racist. Soon after its publication, Fisher left the agricultural world\nof the Rothamsted Experimental Station and began his first academic\nappointment in the Eugenics Department at University College London. Fisher\nspends the entire last third of his _Genetical Theory_ developing his theory\nof eugenics, particularly the then-nascent field of “population genetics” that\ndealt with ideas like the “decay of civilization” resulting from declining\nbirth rates among the upper classes.[13](19_Notes.xhtml#ch5fnr13) Even more\ndamning is that he demonstrated a continued commitment to his eugenic theories\neven after World War II, when many other scientists retreated from the\npromotion of eugenic science.\n\nIn 1950, the United Nations Educational, Scientific and Cultural Organization\n(UNESCO) convened a group of scientists to draft a statement on what they\ncalled “the nature of race,” intending the statement to direct scientific and\nother endeavors away from various racist positions. In this statement, the\nconcept of racial difference was declared to be bankrupt on the grounds that\nthere was a lack of scientific evidence for this difference and that even if\nsuch difference could be proven scientifically in the future, it would be\nmorally irrelevant for society and politics:\n\n> At the present moment, it is impossible to demonstrate that there exist\n> between “races” differences of intelligence and temperament other than those\n> produced by cultural environment. If, tomorrow, more accurate tests or more\n> thorough studies should prove that “races” as such do, in fact, have\n> different innate faculties or aptitudes, UNESCO’s moral position on the race\n> question would not be changed.[14](19_Notes.xhtml#ch5fnr14)\n\nFisher was party to these discussions, but ultimately ended up objecting to\ntheir conclusion. A commentary on the statement includes a summary and\nquotations from Fisher’s position, which stress his commitment to theories of\nracial difference.[15](19_Notes.xhtml#ch5fnr15)\n\nFisher’s racism, and his objection to the UNESCO statement, is best understood\nthrough his unwavering belief in difference. In his statistical terms, Fisher\nrejected the null hypothesis that races are randomly selected from the\npopulation. With these statements, we see further proof of the extent to which\nhis ideology and conception of the scientific method was dedicated to the\ndetection of difference, especially in relation to populations and genetics.\nHe was, quite literally, a man who believed in the truth of blood and soil: an\nagriculturalist and a scientific racist.\n\nThis connection between Fisher’s statistics and his racism points squarely to\nthe sociopolitical importance of the metaphysical foundations of statistics.\nThis is not to say that all statistics, or even the use of Fisherian methods,\nare necessarily racist. Rather, it is possible to see in Fisher’s metaphysics\na tendency that ultimately shaped his politics: his commitment to difference\ncaused him to see difference as the underlying truth of the world. Moreover,\nhis understanding of statistics as a science provided the means to translate\nhis racism into objectified form, giving it (from his perspective) the form\nand force of truth. Just as the commodity form of exchange predetermines the\npossible forms of economic knowledge, the grounds of this probabilistic\nknowledge shape the very political and social possibilities—the conditions of\nwhat and how it is possible to know—of our statistically (and, increasingly,\nalgorithmically) mediated world. His belief in the truth of difference\nprovided fertile ground for his racialized view of the world and offered it\nthe force and language of science.\n\nAlongside these ideological shortcomings, Fisher’s conception of science was\nalso a very methodologically conservative one. Fisher advanced a theory of\nscientific induction that amounts to an inverse form of the Austrian-British\nphilosopher Karl Popper’s famous formulation of falsifiability. For Popper,\nscientific theories are never provable, but are rather\nfalsifiable—experimentation can only prove them\nfalse.[16](19_Notes.xhtml#ch5fnr16) But Fisher’s theory is even more radical\nand skeptical of the inductive power of science. For him, one cannot even\nfalsify a theory. One can only falsify the _absence_ of a theory, namely the\nnull hypothesis, with the only possible progress in science being an\nincreasing belief in the existence of difference. If one finds substantial\nevidence against a null hypothesis, it provides no guarantee of a specific\ntheory.\n\nIt was in reaction to this conservatism in Fisher’s approach that other\nstatisticians, like Jerzy Neyman and Egon Pearson, attempted to build upon\nFisher’s work. At first, Fisher was interested in their efforts. But soon he\nresponded with vitriol, describing their work at one point as “childish” for\nits disagreements with his approach.[17](19_Notes.xhtml#ch5fnr17) Neyman and\nPearson responded to Fisher with equal intensity, leading to a conflict that\nwas both deeply philosophical and petty, which carried on for decades,\ndepositing evidence of hostility across the scholarly record.\n\nNeyman and Pearson\n\nWhile one can trace the specifics of this conflict through various books and\narticles, three main issues underlay Neyman and Pearson’s claims against\nFisher. First, they argued that Fisher’s method was only able to handle false\npositives (what they call “errors of the first kind”) and lacked any way to\nquantify “errors of the second kind,” or false negatives—that is incorrectly\nbelieving that what is observed is merely due to chance, or in terms of\nhypothesis testing, the incorrect rejection of a hypothesis (now known as\n“type II errors.”).[18](19_Notes.xhtml#ch5fnr18) While Fisher’s method uses a\np-value to represent the probability that there is no real difference, any\nresearcher employing his method lacks the means to quantify the possibility\nthat there was a difference that went undetected (a false negative).\n\nSecond, Neyman and Pearson took issue with Fisher’s interpretation of a\nstatistical test’s result. In Fisher’s inverted Popperism, a statistician\nreaches a point—often the p-value cutoff of 5 percent—at which they can hold a\nreasonable belief that difference, in fact, exists. The problem with this\n“point,” Neyman and Parson argue, is that if one accepts a cutoff value of 5\npercent, as has since become common in many fields, then 5 percent of all\ntests will see the null hypothesis incorrectly\nrejected.[19](19_Notes.xhtml#ch5fnr19) Returning to the fMRI problem from the\nprevious chapter, we can now better understand the importance of a critical\nevaluation of such thresholds: there, given the number of tests, a 5 percent\ncutoff is way too high and leads to spurious results; but conversely, setting\nit too low risks throwing out important results.\n\nSome have suggested the threshold simply be lowered. However, as statistician\nAndrew Gelman has recently argued, lowering this 5 percent threshold is not\nenough to restore confidence in the results of statistical analysis of\nexperiments.[20](19_Notes.xhtml#ch5fnr20) Instead, Gelman recommends\nstatisticians embrace the uncertainty of statistical testing by acknowledging\nthat results are contingent upon the decisions scholars make when they\nformulate hypotheses and select, clean, and interpret data—steps that are\ncrucial to any analysis that aims to reveal relationships between variables\nand hypotheses.[21](19_Notes.xhtml#ch5fnr21)\n\nThird, as a corollary, Neyman and Pearson point to a deeper philosophical\nissue at work in one of the ways frequentism is often understood—one they felt\ndid not deal well enough with the fact that it is nonsensical to make\nprobabilistic statements about single, concrete events. In the problem of the\n“lady drinking tea,” if Fisher’s statistical test suggests after an experiment\nthat there is a 5 percent chance that Dr. Bristol guessed randomly, this is\nnonsensical: either she guessed randomly or did not. Likewise, either it rains\ntomorrow or it does not—there is nothing properly probabilistic about it.\nEspecially if the result of an experiment is supposed to measure something\nthat concretely exists in the world, it is in general a challenge to integrate\nprobabilistic statements into any scientific epistemology. This is a major\nproblem that lies at the heart of statistics: How is it possible to provide\nsolid ground to knowledge that is derived from probabilistic statements? Or,\nto phrase it in its converse, how is it possible to speak probabilistically\nabout a concrete event, even if we do not know whether or not it happened?\n\nTo deal with this challenge, Neyman and Pearson attempted to provide a\nstronger interpretation of the results of tests, a solution Neyman termed the\nprinciple of “inductive behavior.”[22](19_Notes.xhtml#ch5fnr22) This principle\nproposes that over a long series of statistical tests, a statistician will\nnecessarily be wrong a given and computable number of times. So, one ceases to\nclaim that the result of a study is necessarily true but rather behaves _as\nthough_ it is true; in this way, inductive behavior sidesteps the fact that\nfrequentism demurs from describing single events and allows researchers to act\nas if it could.[23](19_Notes.xhtml#ch5fnr23)\n\nTo mitigate the negative effects of occasionally being wrong, one sets the\nthreshold for the rejection of a hypothesis not at some anointed quantity—such\nas 5 percent—but at a calculated level that takes into account the costs of\neach possible outcome. Whereas Fisher’s experiments only tested a null\nhypothesis, Neyman and Pearson recommended the construction of two alternative\nhypothesis, between which a statistical test could select. With this approach,\nthey reasoned, one could calculate an optimal behavior based on the\nprobability of various outcomes over multiple tests, as well as the economic\ncosts of being wrong in either direction. For example, an algorithm that\nprescreens job applications would require more human work if it let in\nunqualified candidates (a false positive, or type I error) but would likely\nhave even-greater negative consequences if it excluded a well-qualified\ncandidate from the pool (a false negative, or type II error). One could\nquantify the economic costs in either direction and determine the economically\nideal sensitivity.\n\nThis economic framing is, then, why in the Neyman-Pearson approach it is so\nimportant to recognize errors of the second kind. In any given statistical\ntest, there is a risk and an accompanying material cost associated with both a\nfalse positive (i.e., the incorrect assumption that there is difference) and a\nfalse negative (i.e., the incorrect assumption that there is no difference).\nLike a profitable casino, if the costs and risk are appropriately calculated,\nthe house is bound to lose some hands, but over time the house will ultimately\nwin more than it loses. Similarly, a researcher can calculate an experiment’s\nability to detect an effect, balancing these costs and benefits prior to the\nstudy. For example, as one increases an experiment’s sample size, it may\nbecome easier to detect an effect, minimize the impact of randomness, and\nhence avoid errors of both the first and second kind. But at the same time,\nthe larger the sample size, the more expensive the experiment. Thus, even if\none is ultimately wrong some of the time, in the long run one will end up\nmaking the most profitable decisions.\n\nA recent reversal by the United States Preventive Services Task Force (USPSTF)\nof one of their recommendations demonstrates the appeal of this behavioral\napproach. In 2011, the USPSTF recommended that men not receive the prostate-\nspecific antigen (PSA) blood test, a test designed to detect early signs of\nprostate cancer.[24](19_Notes.xhtml#ch5fnr24) While the PSA test can discern\nlife-threatening health ailments, it also is encumbered by a high rate of\nfalse positives (type I errors), with some estimates suggesting 75 percent of\npatients who had elevated PSA did not actually have\ncancer.[25](19_Notes.xhtml#ch5fnr25) Furthermore, health researchers\ndiscovered that even for men who had tumors on their prostate, many of those\ntumors would grow so slowly that they would not be of vital concern during\none’s lifetime. The cost of false positives in PSA tests has been high:\nensuing treatment involves risks of incontinence, erectile dysfunction, bowel\ncomplications and infection. And after weighing these costs and the low\nbenefits to early treatment, the USPSTF recommended against the test.\n\nBut in 2017, the USPSTF relaxed their recommendation against PSA\ntesting.[26](19_Notes.xhtml#ch5fnr26) This updated suggestion did not come as\na result of a more effective test or a less dangerous treatment. Rather, what\nhad changed between 2011 and 2017 was doctors’ awareness of the risks of\ntreatment. After the initial recommendation by USPSTF, doctors began sending\nfewer patients for surgical interventions and instead recommended further\nmonitoring. This increase in caution decreased the cost of false positives and\nwas ultimately enough to change the recommendation.\n\nThis behavioral approach, which attempts to assign calculable value to false\nnegatives and positives, is especially well suited for business, medical and\nother types of decision making. In these contexts, one can quantify the\nmaterial costs of false negatives, false positives, and additional testing.\nBut when applied to the general production of scientific knowledge, it becomes\nfar more complicated to quantify the costs of various types of error. Because\nof this difficulty, Fisher attacked Neyman and Pearson relentlessly, stating,\n“To do so would imply that the purposes to which new knowledge was to be put\nwere known and capable of evaluation … As workers in Science we aim, in fact,\nat methods of inference which shall be equally convincing to all freely\nreasoning minds, entirely independently of any intentions that might be\nfurthered by utilizing the knowledge inferred.”[27](19_Notes.xhtml#ch5fnr27)\nElsewhere Fisher states, in regard to the essence of scientific discoveries,\nthat he does “not assume that they are capable of evaluation in any sort of\ncurrency.”[28](19_Notes.xhtml#ch5fnr28)\n\nA number of commentators have noted how appropriate Neyman and Pearson’s\napproach is to industrial applications, like quality control systems that\nevaluate products by testing samples against a standard specification, where\nit is more important to be right _often enough_ than it is to always be\nright.[29](19_Notes.xhtml#ch5fnr29) These differences in the ultimate meaning\nof statistical tests did not escape Fisher, who criticized Neyman and\nPearson’s approach for its technological and managerial focus at the expense\nof what he believed was “pure science” and holistic knowledge. Fisher\nexplicitly framed this difference in relation to the politics of the Cold War:\n\n> I shall hope to bring out some of the logical differences more distinctly,\n> but there is also, I fancy, in the background an ideological difference.\n> Russians are made familiar with the ideal that research in pure science can\n> and should be geared to technological performance, in the comprehensive\n> organized effort of a five-year plan for the nation. How far, within such a\n> system, personal and individual inferences from observed facts are\n> permissible we do not know, but it may be safer, and even, in such a\n> political atmosphere, more agreeable, to regard one’s scientific work simply\n> as a contributory element in a great machine, and to conceal rather than to\n> advertise the selfish and perhaps heretical aim of understanding for oneself\n> the scientific situation. In the U.S. also the great importance of organized\n> technology has I think made it easy to confuse the process appropriate for\n> drawing correct conclusions, with those aimed rather at, let us say,\n> speeding production, or saving money. There is therefore something to be\n> gained by at least being able to think of our scientific problems in a\n> language distinct from that of technological\n> efficiency.[30](19_Notes.xhtml#ch5fnr30)\n\nIf Fisher was practically and ideologically an agriculturalist, Neyman and\nPearson were wholesale industrialists. Their interpretation of statistical\ninference abandons truth in favor of economic efficiency, marking a stark turn\ntoward neoliberal thinking in the sciences. This economic interpretation\nreplaces the aim of scientific knowing as an end in itself with a knowledge\nsubsumed in the calculation of economic gain. Where Fisher tested a null\nhypothesis, Neyman and Pearson set up alternatives, assigning costs for erring\nin either direction in order to maximize profit in the long run. And so, while\nNeyman and Pearson were able to construct a philosophically rigorous\ninterpretation of statistical testing, challenging Fisher’s established\nstatistical epistemology in the process, the efficacy of this new\ninterpretation came at the expense of undermining the solidity of any specific\nconclusion, which for them cannot be declared either fully true or false.\n\nLeonard Savage, one of the founders of subjective theories of probability that\nwe will explore later, puts the situation directly: “The traditional idea of\ninference as opposed to behavior seems to me to have its roots in the parallel\ndistinction between opinion and value.”[31](19_Notes.xhtml#ch5fnr31) Contrary\nto Fisher’s attempts to hone his own opinion of the truth, Neyman and Pearson\nsought to eschew opinion in favor of the production of value. For Neyman and\nPearson, truth—under the reign of this “as if ” of behavioral\ninduction—becomes relative to its economic value and thus also tied to the\nshifting and relativized sands of the economy.\n\nHybridization\n\nWhile ideological and methodological differences grew between Fisher and\nNeyman and Pearson, other researchers recognized the power of both statistical\napproaches, but failed to comprehend the theoretical differences between them.\nThis recognition rapidly led to a haphazard hybridization of Fisher’s\nframework for null hypothesis testing with Neyman and Pearson’s alternative\nhypotheses—where one compares a null hypothesis to a single alternative\nhypothesis—as well as a simplistic distillation of these statistical methods\ninto textbooks for fields (such as experimental psychology) that had neither\nthe time nor energy to engage with the thorny philosophical problems at\nissue.[32](19_Notes.xhtml#ch5fnr32)\n\nIn these texts, critical background—discussions of epistemological debates,\nthe difference between the behavioral approach and Fisher’s conservatism\nregarding the interpretation of results, and even the names of Fisher, Neyman\nand Pearson—was dropped. An uncritical hybridization of once-incommensurate\nsystems created a chimeric monster—one that could be called the Fisher-Neyman-\nPearson model. This monster is what many now know as “statistics,” and what is\ncurrently taught to classes of undergraduate and graduate students across the\nworld.\n\nThis hybrid system was highly effective and served a muchneeded purpose,\nenabling post–World War II advances in fields ranging from biology to the\nsocial sciences. In these fields, such hybrid probability measures provided a\nmeans to evaluate results across an increasingly complex research landscape.\nWhen results were relatively straightforward, this model was capable of\nquantifying results and providing a cutoff (p-value less than 0.05) for\npublication. Still, this hybrid approach amounted to an incoherent\namalgamation: Fisher’s trust that statistical results moved ever closer to\ntruth was combined with Neyman and Pearson’s economically driven “close-\nenough” approach.\n\nThis methodological, and ultimately philosophical, hybridization of statistics\nhas since established a scientific-academic complex that produces knowledge\nlike a factory—experiment, publish, receive a grant, and repeat—but treats the\nproduct as though it were immune from the dictates of\neconomy.[33](19_Notes.xhtml#ch5fnr33) In this knowledge factory, there is no\nplant manager or five-year plan to designate a research program or mission; no\none has calculated the costs of errors of various types. Instead, these costs\nare borne by a combination of the poor researchers whose careers disappear\nwith their once-statistically significant results, by the patients whose lives\nare ruined by ineffective treatments or horrific side effects, and by all the\nother environmental and individual impacts of ineffective research practices.\n\nThis is not to say such a scientific-academic complex does not “work.” In\nfact, it functionally works, again, like a casino: individuals may win or lose\non any given day, but, on the whole, the house (research labs, corporate\nresearch and development) keeps winning. But as the sciences have grown more\nand more complex—in terms of their infrastructure, specialization, costs,\nrequirements of prior knowledge, economic demands and so on—the house’s\nmargins keep shrinking, a fact made ever more dire since there is no one\nactually managing the house.\n\nHere, it is helpful to describe one of the most significant changes that this\nhybrid statistics has wrought in information theoretic terms: statistics\nprovides a set of tools for distinguishing signal from background noise—just\nlike we saw in the case of fMRI analysis. The stronger the signal and the\nweaker the noise, the easier it is to detect. For example, in the case of the\n“lady tasting tea,” it is much easier to detect her abilities if she is always\ncorrect (a strong signal) than if she is correct at a rate slightly better\nthan she would on pure chance (a weak signal, especially considering the\nbackground noise that someone could randomly guess correctly half of the\ntime).\n\nBut many of the very strong signals have already been discovered: electricity,\npenicillin and general relativity, among others. Scientific research now\nrequires more and more energy to discover weaker signals, and in many human\nsciences, this means very large sample sizes for experiments. This can be seen\neven more clearly in the development of particle physics. Maurice Goldhaber,\nformer director of Brookhaven National Laboratory in New York, puts it\npoetically: “The first to disintegrate a nucleus was Rutherford, and there is\na picture of him holding the apparatus in his lap. I then always remember the\nlater picture when one of the famous cyclotrons was built at Berkeley, and all\nof the people were sitting in the lap of the\ncyclotron.”[34](19_Notes.xhtml#ch5fnr34) Now, scientists build particle\naccelerators the size of small cities.\n\nThe French philosopher Michel Serres concurs: “The first shepherd lays his\nhands on the treasure of the scrolls found in the cave; there are a hundred\nthousand. Now, with electronics and international relations, you glean rare,\nscattered, barely noticeable atoms of letters. Newton under the apple tree,\nall alone, gives the law of the world, leaving only a few marginal scraps for\nhis innumerable offspring.”[35](19_Notes.xhtml#ch5fnr35) We might object that\nquantum theory and relativity are more than “marginal scraps,” but it remains\nclear that more energy—and more precise instruments—are required to move\nknowledge into the rarified, scientific realms beyond Newtonian physics.\nUltimately, the increased energy needed to detect smaller and smaller signals\nmeans that scientists’ methods of differentiating signal from noise require\nmuch finer precision. Moreover, as the expense of research increases, the\nimportance of political economy to science likewise grows.\n\nThis statistical hybrid may have worked marvelously for conventional science\nin its early post–World War II decades. But today, with much stronger demands\nfor precision to detect weaker signals, its imprecision is undeniable. The\nsciences, notably social psychology and clinical medicine, are experiencing\nwhat contemporary commentators have termed “replication\ncrises.”[36](19_Notes.xhtml#ch5fnr36) In this world of crisis, it appears\nlikely that substantial amounts of scientific research are wrong, in large\npart because these sciences’ methodological bases have provided the requisite\nwiggle room to furnish statistically significant results (often with a p-value\njust below 0.05) without actually proving anything beyond the fact that a\nstudy found statistical significance.\n\nThus, flexibility in research design and data analysis in the knowledge\nfactory managed by this statistics has allowed researchers to fiddle with\nresults just enough to make something academically publishable fall out of the\ndata, a practice popularly known as “p-hacking.”[37](19_Notes.xhtml#ch5fnr37)\nFor example, researchers occasionally exclude outliers, look at subgroups or\nadd additional data—all practices that on their own do not constitute\nmalpractice but in the aggregate can produce results that appear significant\nwhen they should not. Modern science may be finally running up against the\nlimits of statistical methodologies. In response, some statisticians have\nargued that we should reconsider the standard models of frequentist science.\nAndrew Gelman, for one, writes: “If the classical theory of hypothesis testing\nhad lived up to the promise it seemed to have in 1950 (fresh after solving\nimportant operations-research problems in the Second World War), then indeed\nmaybe we could have stopped right there.”[38](19_Notes.xhtml#ch5fnr38)\n\nMore often than not, when attempts to replicate scientific studies fail—or\nerrors are found in methodologies, such as with fMRI studies—the problem is\nnot with the pure statistics, per se. Rather, as with fMRI, the issue is one\nof methods, software, data collection and motivations. Indeed, much of\ncontemporary scientific scholarship has found itself conditioned by\nprofessional and economic pressures: the need to publish, the need for\njournals to make money, funding from corporations, the immense drive to be\nrespected by peers and so on. Stanford epidemiologist John Ioannidis—whose\n2005 article “Why Most Published Research Findings Are False” caused deep\nreflection across scientific circles—argues that there must be a change in\n“the incentive and reward system in a way that would reward the best methods\nand practices. Currently we reward the wrong things: people who submit grant\nproposals and publish papers that make extravagant claims. That’s not what\nscience is about.”[39](19_Notes.xhtml#ch5fnr39)\n\nSuch a poorly designed statistical enterprise has ultimately created the\ngrounds for a new anti-science: poor and distracting incentive structures,\ncombined with researcher flexibility, add enough noise to the system to\noverwhelm many of the signals of meaningful discoveries.\n\nThe Noise in the Signal\n\nThere is another element of this “hybridization” in statistical thinking that\nis rarely recognized: in this scheme, a means of scientific inference based\naround an individual, knowing subject is transposed onto a collective system\nof knowing, such that what the individual researcher has worked out is taken\nto be known collectively. The vast majority of our philosophical approaches to\nstatistics, save perhaps the strictest interpretation of Neyman and Pearson’s\nwork, assume that the one seeking knowledge is an individual. On the other\nhand, our systems of academic knowledge production treat these results as\nthough they are collectively held. If anyone was aware of this difficulty it\nwas Fisher, as evidenced above, who also contrasted the Soviet model of\ntechnologically driven science with the “selfish and perhaps heretical aim of\nunderstanding for oneself the scientific\nsituation.”[40](19_Notes.xhtml#ch5fnr40)\n\nLike the agrarian that he was, Fisher presented his system as built completely\naround the lone, self-sufficient researcher, or farmer who will take what they\nlearn about their field and immediately implement it there. In contrast, the\nindustrial Neyman-Pearson system required a bureaucrat to manage the\nproduction of knowledge, someone who can weigh the costs and benefits of\nvarious outcomes for the entire enterprise and not just their own career. Yet,\nwhile Fisher focused on the individual researcher, he was not unaware of the\nchallenges that accompany the collectivization of knowledge. To this end,\nFisher argued that negative results should be shared publicly so that\nresearchers would have both positive and negative evidence to\nweigh.[41](19_Notes.xhtml#ch5fnr41) If one only shares positive results,\nresearchers do not have all of the evidence required to run the appropriate\ncalculations and stock up evidence in the spirit of Fisher’s inverse\nPopperism. Chance results will be published and acclaimed while countervailing\nevidence sits in dusty file drawers—an outcome that severely biases the\nscholarly record.[42](19_Notes.xhtml#ch5fnr42)\n\nStill, while Fisher would likely never have admitted it, even his “selfish”\ninterpretation of statistics requires a manager or a bureaucrat for it to\nfunction properly. Without such oversight, there is no force or incentive to\ncollect all of these failed tests. And in the current distributed model of\nresearch where there is no five-year plan or plant manager, there is very\nlittle value or potential in negative results. Negative results produce\nnothing productive for the contemporary scientific milieu; they generate\nneither money nor prestige. These negative results are rarely welcome in a\nworld where surprising results can sell fame, patents, and journals. While\nthere is an academic movement organizing to share negative results, the\nincentives of grant funding and tenure often mitigate the desire to actively\ndo so.[43](19_Notes.xhtml#ch5fnr43) The situation is even worse outside of\nacademia, where companies like those in the pharmaceutical and tobacco\nindustries can easily hide nonsignificant data and calculations, presenting\ncurated, and thus manipulated, outcomes as eternal\ntruths.[44](19_Notes.xhtml#ch5fnr44)\n\nUltimately, the hybridized model creates what we have discussed, in earlier\nchapters, as a mystery: it produces a metaphysics that objectifies\ninequalities into equalities. It treats knowledge as value (i.e., based on a\nbehavioral economics) while, as Fisher does, it claims its value cannot be\nevaluated—in other words, that research cannot and should not be managed with\nsome larger plan. In short, statistics redoubles a sovereign objective\nperspective that equates individual knowing with collective knowing, and thus\nsimultaneously brings this knowledge to market while claiming that it can\nstill be rescued from the perversions and dissimulations of that\nmarket.[45](19_Notes.xhtml#ch5fnr45) While processes of academic knowledge\nproduction use peer review and disciplinary conversation to collectivize\nknowledge, modern statistics functionally conflates individual and collective\nknowledge by applying Neyman and Pearson’s philosophy of industrial production\nof knowledge while maintaining the veneer of Fisher’s heroic individual\nscientist who is capable of knowing.\n\nThe Fisherian claim that statistical knowledge is truth (even in Fisher’s\nconservative interpretation that claims only to detect difference) remains\nlike the ghost of the Enlightenment subject, in the sense that its\nsimultaneous presence and absence is what makes, and underwrites, the social\nand economic force of these calculations. While Neyman and Pearson see this\nstatistical knowledge as contingent and temporary—placed under the cautious\n“as if ” of their behavioral and economic approach—the patina of Fisher’s\nclaim makes it appear objective and true. To put it in Marxist vernacular,\nFisher’s work furnishes statistical knowledge the structure of the fetish: the\nresult appears stripped of its context, able to think for itself, and is thus\ngiven objective force.\n\nWhile this will not suffice to explain the totality of the phenomena, it is\npossible here to see an element of the excitement some portions of the public\naffix to scientific ideas and explanations that are largely discredited: from\nthe return of scientific theories of racial and sexual difference to climate\nchange denial to concerns that vaccines cause\nautism.[46](19_Notes.xhtml#ch5fnr46) While far from Fisher’s intent (and in\nfact nearly the exact opposite), his simultaneous belief in difference as the\ntruth of science and the lone scientist’s ability to know provide a dangerous\ncombination that has empowered many scientists and nonscientists alike to\nbelieve that they alone are able to ascertain true difference, and often to\nfetishize the results of a single study (even a discredited one, in some\ncases).[47](19_Notes.xhtml#ch5fnr47)\n\nThe p-value that researchers attempt to push below a certain threshold to\nprove the truth of an alternative hypothesis appears as a ghost of a departed\nquantity—of those present absences that mark the mysteries of mathematics and\nknowledge production. Yet philosophical and epistemological obfuscation\nguarantees that many researchers fail to grasp what is truly at stake,\ncontinuing to run study after study and test after test, while concerns about\nthe viability of the larger enterprise mount.\n\nIn this light, one should not be surprised that John Ioannidis concludes his\nshort article on why most scientific research is false by writing, “For many\ncurrent scientific fields, claimed research findings may often be simply\naccurate measures of the prevailing bias.”[48](19_Notes.xhtml#ch5fnr48) Here,\nagain, we see the force of objectification, taking what amounts to preexisting\nbias (although here Ioannidis is referring to bias in favor of certain\ntheories) and laundering it through some computation to give it the non-\nlocatable force of truth. The hasty hybridization of these models of\nstatistics has created a very profitable mathematical infrastructure,\nsupporting a mutually reinforcing system that favors production over quality\ncontrol. In this way, the conclusions that this giant research machine\nproduces are not necessarily wrong, but the ground on which it operates is not\nnearly as stable as it is often made out to be.\n\nThis conceptual ground is founded upon a mystery—an equation of unequal\nelements that serves to objectify our belief in its function by treating value\nas truth. The instability of these mysteries—here on display in the\nphilosophically contradictory hybridization of Fisher’s science of truth with\nNeyman and Pearson’s science of value—expose these metaphysical presumptions\nto the political economies within which they operate. It is both for this\nreason and as a result of it that statistics begins, with Neyman and Pearson,\nto try to find its ground in the act of producing value. Just like the\ncommodity for Marx, the metaphysical magic trick of statistics both justifies\nand finds its truth in exchange; in the words of Alfred Sohn-Rethel, it is a\nreal abstraction, producing abstract truths out of a material economy and\nclaiming that they have no locus and thus are not social.\n\nThe point, then, is not to stop this machine of knowledge production, but\nrather to dig beneath its base. There, it may be possible to reconfigure its\nmetaphysics, to lay new claims to this system’s understanding of both\nknowledge and value. This is the work of a revolutionary mathematics. In\nshort, to save this machinery from ruin requires an insistence that any crisis\nin the sciences today is first and foremost a crisis of capitalism. The only\nfuture the sciences have is one where they work against and beyond the\neconomies that threaten its destruction.\n\n\n[PART III](04_Contents.xhtml#ch10)\n\n[Bayesian Dreams](04_Contents.xhtml#ch10)\n\n> If a universal mind existed, of the kind that projected itself into the\n> scientific fancy of Laplace—a mind that could register simultaneously all\n> the processes of nature and society, that could measure the dynamics of\n> their motion, that could forecast the results of their inter-reactions—such\n> a mind, of course, could a priori draw up a faultless and exhaustive\n> economic plan, beginning with the number of acres of wheat down to the last\n> button for a vest. The bureaucracy often imagines that just such a mind is\n> at its disposal; that is why it so easily frees itself from the control of\n> the market and of Soviet democracy. But, in reality, the bureaucracy errs\n> frightfully in its estimate of its spiritual resources. In its projections\n> it is necessarily obliged, in actual performance, to depend upon the\n> proportions (and with equal justice one may say the disproportions) it has\n> inherited from capitalist Russia, upon the data of the economic structure of\n> contemporary capitalist nations, and finally upon the experience of\n> successes and mistakes of the Soviet economy itself. But even the most\n> correct combination of all these elements will allow only a most imperfect\n> framework of a plan, not more.\n>\n> —Leon Trotsky, “The Soviet Economy in Danger”\n\n\n[Chapter 6](04_Contents.xhtml#ch11)\n\n[Bayesian Statistics and the  \nProblem with Frequentism](04_Contents.xhtml#ch11)\n\nFrequentism may have worked well in the past, but today it struggles. Many now\nlook elsewhere for statistical insight, often by turning to Bayesian\nstatistics. Despite being named after the eighteenth-century mathematician and\nreverend Thomas Bayes, these methods have only entered the mainstream in the\nlast few decades. In fact, Bayes did not invent the theorem that bears his\nname, which serves as the mathematical foundation of Bayesian statistics—a\nfeat that was accomplished by astronomer and polymath Pierre-Simon Laplace in\nthe early nineteenth century.[1](19_Notes.xhtml#ch6fnr1) But, in a\nposthumously published essay, Bayes laid out the basic math of this approach,\nwhich allows for the estimation of some unknown value from data (such as the\nestimation, after observing ten lottery tickets, of the ratio of winning to\nlosing tickets). Bayes’s solution to this problem, and the work built upon it,\nstarts, in contrast to frequentism, with an interest in probability as a\nsubjective method of belief. This calculation of subjective belief was largely\nrejected by frequentists and the majority of mid-twentieth-century\nstatisticians, but in recent years it has become increasingly persuasive and\nuseful.\n\nWe will turn to more specifics shortly, but we should note that, if Ronald\nFisher modeled a statistics for an agrarian society, and Jerzy Neyman and Egon\nPearson modeled one for an industrial society, Bayes—through his modern\ninterpreters—has provided a statistical theory for the information age. The\nuse of Bayesian statistics has exploded in recent decades as statisticians\nstrive to address the methodological shortcomings of frequentist approaches\nand take advantage of cheap computing power.\n\nTo fully appreciate the revolutionary implications of the rise of Bayesian\napproaches, it is best to start with frequentism’s shortcomings. As the\nprevious chapter outlined, problems with the approach have appeared in the\nsciences writ large, and statisticians, in their own ways, have also taken aim\nat the theoretical underpinnings of frequentism. For example, in 1976,\nstatisticians Dennis Lindley and Lawrence Phillips demonstrated one of the\nproblems with frequentism’s claims to objectivity with a simple imagined\nexperiment designed to determine if a coin is biased toward\nheads.[2](19_Notes.xhtml#ch6fnr2) In their thought experiment, a researcher\nflips a coin twelve times, resulting in hhhthhhhthht, or three tails and nine\nheads. Assuming that the coin has an equal probability of landing heads or\ntails (the null hypothesis in this experiment), we can calculate the odds of\nseeing three or fewer tails in a run of twelve flips, which works out to 7\npercent.[3](19_Notes.xhtml#ch6fnr3) With the standard p-value cutoff of 5\npercent, we would be led to conclude that there is not enough evidence to\nreject the possibility that what is observed is due to chance; hence it is\nstill possible that the coin is fair.\n\nBut if the experiment were designed differently, using the exact same coin and\nresults, we could contrive a different probability. As Lindley and Philips\nsuggest, this time the researcher could decide to flip the coin until three\ntails appeared—so that the variable of interest becomes the total number of\nflips instead of the number of tails. With this new experimental design, they\nfind the probability of twelve or more total flips becomes just over 3\npercent. This difference of probability arises due to the fact that our\nresearcher is now calculating the probability of two or fewer tails in the\nfirst eleven flips (since the twelfth flip can either be a tail—and analysis\nwould stop there because they would observe the stopping condition of three\ntails—or a head—in which case the total number would be more than\ntwelve).[4](19_Notes.xhtml#ch6fnr4)\n\nThe statistical result of 3 percent, then, appears significant at the 5\npercent level, and our researcher can conclude that, unlike the first\nexperimental design, there _is_ evidence against the coin being fair. While,\non the one hand, it is reasonable that the design of an experiment would\naffect the results, on the other, this example suggests that the researcher’s\nstate of mind in carrying out an experiment can fundamentally modify the\nconclusions drawn from the same set of data. Thus, this supposed objective\ntheory of probability rests squarely on the personal and subjective\nunderstanding of the experimenter.\n\nFor this reason, reference classes—that is what set of events are considered\npart of the experiment—are pivotal for frequentism. While probability for\nfrequentism is deemed objective—in that it measures only the number of\noccurrences of an event from a long run of trials—the construction of this\nlong run requires a subjectively assembled grouping. As is often the case in\nphilosophy and science, claims of objectivity find that they must, in the\nfinal analysis, rest on some subjective ground.[5](19_Notes.xhtml#ch6fnr5) For\nprobability to become objective, it requires an imaginary, subjective and thus\nincredibly human subsidy. The decision of how to compile the reference class\nof events from which probability is computed can significantly alter the\nresults of a statistical experiment or analysis.\n\nWe witness here, again, the torsion between the subjective and the objective\nthat we have been tracing. The more any perspective tries to get to the\nobjective, real heart of the matter, the more quickly one slips back into the\nsubjective. And, just as with capitalism, the cost of being a hard-nosed\nrealist is that one believes in the most imaginary of inventions, such as a\nnear-infinite series of coin flips or the guaranteed value of money. The more\none tries to imagine a world beyond subjectively produced, human abstractions,\nthe more central these abstractions become.\n\nThe Inverse Problem and the Necessity of Subjectivism\n\nWe have already discussed a number of problems with frequentism, but it is\nhelpful to lay out its two main challenges schematically. First, as discussed\nin the previous chapter, in their strict interpretation frequentist approaches\ndo not permit the description of single events probabilistically. Leonard\nSavage criticizes these approaches, writing that “objectivistic views\ntypically attach probability only to very special events. Thus, on no ordinary\nobjectivisitic view would it be meaningful, let alone true, to say that on the\nbasis of the available evidence it is very improbable, though not impossible,\nthat France will become a monarchy within the next\ndecade.”[6](19_Notes.xhtml#ch6fnr6) This limitation in applicable events is\ndue to the fact that frequentist probability requires an imaginary long run of\nevents whose frequency of a certain outcome provides the measure of\nprobability. Thus, probability requires this long run, as opposed to a single\nevent.\n\nSecond, and perhaps the most damaging critique of frequentist approaches, is\nthat frequentism answers the wrong question. Probability is relatively easy to\ncalculate if one knows everything about a system that is being modeled. If a\nstatistician knows that a coin flip is fair, it is trivial to calculate the\nodds of getting two heads in a row. The probability is 0.25: the result of\nmultiplying 0.5 (the chance of heads on the first flip) by 0.5 (the chance of\nheads on the second flip). But it is another matter altogether to go the\nopposite direction: to calculate the probabilities in the underlying system\nfrom observed data.\n\nFrequentists throw up their hands when faced with this challenge. Instead,\nthey ask a much more circumspect question: If one assumes a coin is fair, what\nis the probability that one would observe a given set of results? This is\nprecisely the way that Fisher proposes hypotheses should be tested. While the\nanswer to this question may offer some insight into the true frequency of the\ncoin, asking what we would observe were the coin to be fair is neither\nphilosophically nor mathematically the same question as asking what the\nprobability is that the coin _is_ fair. The frequentist pretense is based on\nan imagined world where the null hypothesis is true. Ultimately, as a result\nof the first problem, frequentism denies that it is possible to assign a\nprobability to an actual hypothesis (since its truth is effectively a single\nevent), offering instead the probability that the evidence we witness would\noccur in a world where the null hypothesis is true.\n\nSavage sums up the stakes of these developments well:\n\n> In some respects Bayesian statistics is a reversion to the statistical\n> spirit of the eighteenth and nineteenth centuries; in others, no less\n> essential, it is an outgrowth of that modern movement here called classical.\n> The latter, in coping with the consequences of its view about the\n> foundations of probability which made useless, if not meaningless, the\n> probability that a hypothesis is true, sought and found techniques for\n> statistical inference which did not attach probabilities to hypotheses.\n> These intended channels of escape have now, Bayesians believe, led to\n> reinstatement of the probabilities of hypotheses and a return of statistical\n> inference to its original line of development.[7](19_Notes.xhtml#ch6fnr7)\n\nBayesian approaches thus offer to solve the inverse problem: rather than\nassume the truth of the null hypothesis, Bayesian statistics are able to offer\na probability for the truth of the hypothesis itself, continually updating\nthis probability as new evidence is gathered.\n\nThe Value of Bayes\n\nWhere the frequentism of Fisher, Neyman and Pearson interprets probability as\nrepresenting a near-infinite run of a physical system—such as a series of coin\nflips—Bayesian approaches understand probability as a measure of subjective\nbelief. Correspondingly, most frequentist approaches tend to think in terms of\ndiscrete experiments, while Bayesians are more comfortable evaluating any and\nall data that is available. The Bayesian approach is novel because it lays out\na method for making predictions that is well suited to the addition of new\nevidence.\n\nWith Bayes’s formula, the specifics of which will be outlined shortly, new\ndata can continually be added to the output of prior calculations. Due to this\nflexibility—an abandonment of rigid tests of hypothesis in favor of a\nconstantly updated measure of belief—Bayesian statistics have seen a\nresurgence in the last few decades across both academic and corporate\nrealms.[8](19_Notes.xhtml#ch6fnr8) Bayesian approaches have been a boon to\ndata-driven tech companies, as well as older manufacturing industries, as\nthese established behemoths pursue new strategies for increasing profit in the\nmilieu of contemporary informational capitalism.[9](19_Notes.xhtml#ch6fnr9)\n\nFor a 2000 article in _Wired_ , Michael Lynch, the founder of the data\nanalytics company Autonomy—which has since been bought for $10\nbillion—declared that “Bayes gave us a key to a secret garden. A lot of people\nhave opened up the gate, looked at the first row of roses, said, ‘That’s\nnice,’ and shut the gate. They don’t realize there’s a whole new country\nstretching out behind those roses. With the new, superpowerful computers, we\ncan explore that country.”[10](19_Notes.xhtml#ch6fnr10) In the intervening\nyears, many companies have opened this gate, and they have profited handsomely\nby exploiting the resources of this new country, predicting everything from\nshopping habits to the outcomes of elections.\n\nEarly work in probability and statistics, especially Laplace’s nineteenth-\ncentury development of what is now known as Bayes’s theorem, made significant\nadvances on this “inverse problem,” calculating the probability of hypotheses\nthemselves. However, this project was largely abandoned in the early twentieth\ncentury, due, in part, to Fisher and his followers’ unease with the subjective\nnature of Bayesian statistics, as well as the tedium of the required\ncalculations.\n\nBayesian approaches—unlike frequentist ones—require the use of what is called\na prior probability distribution, which describes what one thinks to be the\nodds of various outcomes.[11](19_Notes.xhtml#ch6fnr11) The use of this\nsubjective prior probability bothered many of the early “scientific”\nfrequentists, who hoped they could escape the need for subjective input in the\nprocess of statistical inference. Fisher was perhaps the most adamant in this\nregard, stating, “The theory of inverse probability is founded upon an error,\nand must be wholly rejected.”[12](19_Notes.xhtml#ch6fnr12)\n\nAs a whole, frequentists did not necessarily refuse the possibility of making\nstatements about hypotheses. But, as Fisher argues, even though it may be\npossible to evaluate inferences (e.g., the truth of a hypothesis from data),\nprobabilities cannot be directly assigned to a hypothesis:\n\n> The rejection of the theory of inverse probability was for a time wrongly\n> taken to imply that we cannot draw, from knowledge of a sample, inferences\n> respecting the corresponding population. Such a view would entirely deny\n> validity to all experimental science … the mathematical quantity which\n> usually appears to be appropriate for measuring out order of preference\n> among different possible populations does not in fact obey the laws of\n> probability.[13](19_Notes.xhtml#ch6fnr13)\n\nDespite claims that it is able to make sweeping evaluations of hypotheses,\nfrequentist hypothesis testing, when appropriately applied and interpreted,\ncan only make very limited statements about them.\n\nThe combination of cheap computing power and practical problems with\nfrequentism has led to a renewed interest in Bayesian approaches and growing\ncomfort with subjective theories of probability. Because of its ability to\nemploy prior belief, Bayes’s theorem excels in conditions where we might want\nto update beliefs as data is gathered in real time—an especially exciting\nprospect for networked digital capitalism.\n\nBayes’s Theorem\n\nThe heart of Bayesian analysis is known as Bayes’s theorem, or Bayes’s\nformula. Bayes’s theorem states that the probability of an event (_B_), given\nthat another event (_A_) has occurred, is the probability of _A_\noccurring—given that _B_ has occurred—times the probability of _B_ all divided\nby the probability of _A_ occurring:\n\n![images](images/img_p151.jpg)\n\nIn the context of frequentism, it is possible to see how Bayes’s theorem\nallows us to treat “inverse probability.” If we think of _B_ as the truth of a\nhypothesis or theory and _A_ as the evidence, this formula allows us to\nmathematically convert the probability of the evidence, given the truth of the\nhypothesis, to the probability of the hypothesis given the evidence—an\ninversion that frequentism explicitly rejects.\n\nTo understand how this works with a more practical example, imagine a medical\ntest that can detect a disease 95 percent of the time, with a 1 percent false-\npositive rate. We may initially assume that if a subject tests positive, there\nis only a 5 percent chance they do not have the disease. Bayes’s theorem can\nbe used to calculate the probability based on the prevalence of the disease\namong the population. Bayesian inference starts with a subjective assessment\n(let us say the disease is rare; so, we assume only 2 percent of the\npopulation has it) and, as more evidence is obtained (the results of testing),\nthe probability becomes increasingly accurate. With our initial assumptions,\nit turns out that a positive test corresponds with only a two-in-three chance\nof actually having the disease, despite the 95 percent rate of detection and 1\npercent false-positive rate. This result may seem counterintuitive, but it is\nin fact a direct consequence of our assumption about the disease’s prevalence:\nthere are many more people without the disease (who could produce a false\npositive), than there are with it (who could produce a true\npositive).[14](19_Notes.xhtml#ch6fnr14)\n\nThis example is a single test and is based on our belief in the prevalence of\nthe disease in the population, since the true percentage is unknown. However,\nwe could imagine testing a large population, then using these same\ncalculations to update our presumed prevalence. This is the power of Bayesian\napproaches: in a world of cheap computation and massive amounts of digital\ndata, we can continually update our beliefs about the world as new data\narrives.\n\nNaive Bayes\n\nWith this in mind, let us turn to an example to demonstrate the modern power\nand relevance of Bayesian approaches: the field of information retrieval,\nwhere many celebrated machine learning algorithms have been nurtured. In\nparticular, let us look at what machine learning researchers call the “naive\nBayes classifier,” an algorithm for classifying documents (or really anything\nabout which data is known) into categories. It provides an exceptional example\nof how beneficial Bayesian approaches can be to modern data analysis.\n\nThis method is called “naive” because one assumes that the appearance of every\nword in a document is independent from every other (e.g., seeing the word\n“cat” does not affect the probability of seeing the word “dog” in the same\ndocument). This may seem counterintuitive based on what we colloquially know\nabout language, as “cat” and “dog” are clearly more likely to appear together\nthan, say, “cat” and “monad.” But, as has been learned over the last few\ndecades, many machine learning algorithms function exceptionally well if they\nare programmed naively, such that the algorithm can find structures and\npatterns on their own, rather than requiring guesses from programmers, who\nwould then spend substantial amounts of time and effort trying to intuit which\nrelationships are the most important.\n\nIn the case of naive Bayes, a programmer takes a set of “training” documents\nthat have already been classified. For this example, let us say they are\ninterested in whether a given document is about animals or not. This training\ndata already has an implicit classification taxonomy for all of the documents\nin the set: either they are, or are not, about animals.\n\nThen, in the simplest terms, a naive Bayes classification algorithm goes\nthrough this training dataset and calculates how likely each term is imagining\nthat a document is in each of our classes in turn (we may discover that in an\narticle about animals, the word “dog” has a 20 percent chance of occurring,\nwhile in a nonanimal article it has a 2 percent chance of\noccurring).[15](19_Notes.xhtml#ch6fnr15) In essence, it computes the\nprobability of a word for each category. Then, when the algorithm has fully\npassed through the training data, it can classify a new document (one not\nincluded in the training data) by calculating the inverse probability based on\nall of the words in the document. In short, the algorithm multiplies the\nprobability of each word, for each category, by the overall probability of\neach category in our training dataset. The calculated result with the highest\nprobability is the most likely, in this case “about animals” or “not about\nanimals.”[16](19_Notes.xhtml#ch6fnr16)\n\nWith this example we can see two significant benefits of Bayesian approaches\nto our current informational capitalism. First, as noted earlier, the Bayesian\napproach allows for the integration of new evidence. After the classification\nalgorithm categorizes a document or data element, that new categorization data\ncan then be added to the probabilities for future documents (or if a human\ncorrects the categorization, that can also be added). Moreover, while the\nabove example dealt with the categorization of discrete and static documents,\nthe same process can be used to categorize anything, updating as new data are\nadded (e.g., predicted gender for a person’s social media account can be\nrecomputed after every post). With Bayes’s theorem, we have an explicit, and\nhence automatable way, to continually add new data to our model.\n\nVariations of the naive Bayes classifier have existed since the 1960s. While\nthe algorithm for classification is relatively straightforward, it is also\nincredibly powerful. Peter Norvig, Google’s research director, has stated that\n“there must have been dozens of times when a project started with naive Bayes,\njust because it was easy to do and we expected to replace it with something\nmore sophisticated later, but in the end the vast amount of data meant that a\nmore complex technique was not needed.”[17](19_Notes.xhtml#ch6fnr17)\n\nSecond, we can begin to see how Bayesian analysis constructs the bridge\nbetween statistical hypothesis testing and the advent of machine learning.\nUsing a naive Bayes classifier, a statistician is able to take data (such as\nword counts) and calculate the probability of each hypothesis (i.e., “animal”\nor “not animal”), which then allows them to decide the most likely one. Yet,\nfrom another perspective, that statistician is simply classifying documents\nand calculating the most likely category. And if they use a different training\nset of data the next time they run the algorithm, they may even change their\nmind about a document. By doing away with the necessity for discrete\nexperiments, Bayesian analysis allows hypotheses to multiply nearly\ninfinitely: each document, each visitor to a website, each loan application\nappears as a hypothesis whose probability can be calculated and from whose\noutcome more can be learned.\n\nBy assigning a probability to hypotheses, Bayesian approaches are able to\nautomatically and computationally weigh different hypotheses. At first glance,\nthe subjectivism of Bayesian approaches may appear to demand human\ninvolvement. But in reality, this computational subjectivity actually empowers\ncomputers because it provides a concrete mathematics that they can follow.\nWith this formulation, an algorithm can exploit its subjective (i.e., local)\nposition to calculate probabilities and does not need to rely on a human who\ncan understand a transcendental totality (i.e., the assigned reference class\nor experimental design in frequentist analysis) that allows and requires them\nto arbitrate objectivity, as is the case with frequentism (e.g., the need to\ndesign an experiment, select a reference class and interpret the results).\nThis capacity allows computers, without human input, to choose the most likely\ncause for a set of observations in a way that strict adherents of frequentism\nwere never comfortable with.\n\nThis is one of the critical changes ushered in by the Bayesian revolution: the\nability to assign a probability to a single event, including a hypothesis,\nmeans the process can be automated in a way that is not allowed under a strict\nfrequentist interpretation of probability, precisely because in the latter, a\nhypothesis cannot be assigned a meaningful probability. Once a hypothesis\n_can_ carry a probability of being true, each possible classification can be\nassigned a probability and the most probable selected. Thus, even if Bayesian\nprobability is subjective, the subject that evaluates it can just as easily\n(or often more easily) be a computer rather than a human. And from this\nsubjective perspective, computers are able to automatically compute the most\nvaluable option. In this way, as we shall see in more depth, this Bayesian\napproach takes up the economic commitment of Neyman and Pearson and commits\nitself to a general management of knowledge and society through probabilistic\nreason. In form, this Bayesian method finds extraordinary harmony with the\nprofit-seeking dictates of automated information capitalism.\n\nWhile humans are still intimately necessary when it comes to defining a\nproblem and determining what a valuable result is, Bayesian approaches make a\nrevolutionary advance toward the objectification of parts of this process, and\nthus toward its computability. Ultimately, the Bayesian revolution is a\nrevolution in capitalist production that may one day come to be seen as\nimportant as the Taylorist revolution, automating and accelerating knowledge\nproduction just as Taylorism did industrial production. Bayesian statistics\nhas fundamentally altered the production of knowledge, allowing the\nmechanization of a process that turns data into generalizable facts. Even\nbeyond strict Bayesian approaches, the larger change in method it has\nprecipitated allows probabilistic insights to be brought directly to market,\nwhere they automate decision making based on data and market demands. And, as\nwe shall shortly see, these methods integrate market valuations directly into\nthe metaphysics of knowledge production.\n\n\n[Chapter 7](04_Contents.xhtml#ch12)\n\n[Bayesian Metaphysics and the  \nFoundation of Knowledge](04_Contents.xhtml#ch12)\n\nBayesian approaches flip frequentism’s metaphysical perspective on its head.\nInstead of starting with an objective theory of probability only to end up\nhaving to rely on imaginary, subjectively created reference classes, Bayesian\napproaches start with a subjective belief and slowly, but procedurally, move\ntoward objectivity. Leonard Savage explains the process through the example of\na coin:\n\n> Although your initial opinion about future behavior of a coin may differ\n> radically from your neighbor’s, your opinion and his will ordinarily be so\n> transformed by application of Bayes’ theorem to the results of a long\n> sequence of experimental flips as to become nearly indistinguishable. This\n> approximate merging of initially divergent opinions is, for Bayesians, how\n> empirical research becomes “objective.”[1](19_Notes.xhtml#ch7fnr1)\n\nIn a certain sense, the Bayesian approach takes the process of objectification\nmore seriously than frequentism; it simultaneously provides a means of making\nindividual and socially situated knowledge objective _and_ provides a logic\nthat attempts to show the subject how they should act under the reign of its\nmetaphysics.\n\nIn this metaphysics, we can hear the mantra of objectification: do and think\nwhat you want, but in the end we must all be realists and sell our labor and\ncompute our probabilities if we want to eat. Here the Bayesian approach\nproduces its “real abstraction.” While Bayesian analysis turns to a subjective\naccount of statistics, as we will see, it ultimately grounds the rationality\nof this subjectivity in the exchange of contracts. Despite the hyper-\nappropriateness of this means of knowledge production to capitalist exchange,\nby revealing the social nature of knowledge production, Bayesian analysis\npoints beyond the limitations of capitalism and ultimately demonstrates the\nvery antipathy of capitalism to knowledge production.\n\nPercentage Belief\n\nInstead of imagining, and then defining, some group that becomes the reference\nclass that is being sampled from, Bayesian approaches imagine the researcher\nor the computer as an agent that continually gains more knowledge of the\nworld. In some cases, these probabilities end up being similar to frequentist\nanalysis and can even behave similarly (e.g., if we have a 50 percent belief\nthat a coin flip will return heads, we expect that over a long run of actual\nflips, 50 percent will tend to be heads).[2](19_Notes.xhtml#ch7fnr2) But the\ninterpretation, use and metaphysical grounding of these probabilities\nfundamentally differ.\n\nWhile this use of subjective probabilities may seem like an antiscientific\nstep backward—an abandonment of objectivity and reversion to our initial\nuninformed, or minimally informed beliefs—Bayesian analysis has gained a\nstrong foothold in the scientific world. This is partly due to the decreasing\nprice of computing power, but also because Bayesian analysis has allowed novel\napproaches for thinking through key issues of scientific method. For example,\nBayes presents a potential means for understanding the replication crisis and\nproblems in scientific inference, since the use of a prior probability now\nallows observers to demand extraordinary evidence for unlikely claims: if one\nwere to think, say, that the existence of extrasensory perception is highly\nimprobable, massive amounts of evidence would be required to overcome this\nprior belief against its existence. Furthermore, any known biases—such as the\nincreased probability that researchers will publish surprising results or\nselectively report data—can be factored into the analysis of the larger\nscientific consensus on an issue. In short, with Bayesian analyses, one can\nmathematically model not just a single experiment, but the entire state of a\ngiven field of knowledge.[3](19_Notes.xhtml#ch7fnr3)\n\nEven more important, the Bayesian approach fundamentally changes the stakes of\nstatistical analyses. As a result of its subjectivism, the result of a\nstatistical test no longer claims to tells us the “truth” of the matter.\nRather, each result can be taken as evidence so that each reader can calculate\ntheir own expected probability. For instance, one researcher may believe the\nexistence of extrasensory perception is highly unlikely, while another may\nthink its existence is equally as likely as its nonexistence, and each would\ncalculate a different probability after reading an article providing some\nevidence of its existence. This focus on belief short-circuits some of the\nproblems with frequentist statistics: scientific studies no longer need to\npretend that they provide us with a transcendent truth. Instead, these studies\noffer data to readers, which they can then use to update their own beliefs.\n\nAnd through this constant overhaul of knowledge, Bayesian statistics provides\nmethods and an ideology, both of which are well suited to informational\ncapitalism.[4](19_Notes.xhtml#ch7fnr4) In so doing, it provides a mathematical\nframework in which evidence can be transformed into belief, which can then be\ntransformed directly into automated decision making (e.g., about what ad to\nshow a visitor to a website, determined by treating each possible ad as a\nhypothesis and selecting the one with the highest probability of getting\nclicked). Second, this shift to individual belief creates markets from which\nthe combination of evidence (data), and models (a company’s simulation of the\nway the world or market likely works) can provide a competitive advantage in\nthe marketplace against others with lower-quality data or assumptions.\n\nWe can see the potential for this marketization explicitly in the case of the\n“animal” and “not animal” naive Bayes classifier from the previous chapter. If\nenough high-quality training data is used, the computed model becomes\nincredibly valuable because it can allow a company, scientist or government to\nmake predictions, and even the most miniscule of increases in accuracy\ndirectly supplies a competitive advantage. In this way, the Bayesian\nrevolution provides a key set of methods for informational capital, allowing\nthe computation of knowledge from data.\n\nThe Market as Metaphysical Ground\n\nLike probability in general, the origins of Bayesian analysis admit to a\ntheistic understanding of probability that, despite being largely abandoned,\nstill speaks to its metaphysical power. Following Bayes’s death, his friend\nRichard Price discovered an intriguing manuscript among Bayes’s papers. After\ntwo years of significant editing, Price published the manuscript as _An Essay\ntowards Solving a Problem in the Doctrine of Chances_. In his introduction to\nthe text, Price makes a profound comment about the theological importance of\nBayes’s discovery:\n\n> The purpose I mean is, to shew what reason we have for believing that there\n> are in the constitution of things fixt laws according to which things\n> happen, and that, therefore, the frame of the world must be the effect of\n> the wisdom and power of an intelligent cause; and thus to confirm the\n> argument taken from final causes for the existence of the Deity. It will be\n> easy to see that the converse problem solved in this essay is more directly\n> applicable to this purpose; for it shews us, with distinctness and\n> precision, in every case of any particular order or recurrency of events,\n> what reason there is to think that such recurrency or order is derived from\n> stable causes or regulations in nature, and not from any irregularities of\n> chance.[5](19_Notes.xhtml#ch7fnr5)\n\nFor Price, the very predictability of the world—even if it is\nprobabilistic—proves the existence of “regular laws” and hence divine design.\n\nThus, in the face of this problem, Price suggests that it is not the existence\nof the divine that proves the laws of the universe, but rather the inverse:\nthe existence of these regularities proves the existence of the divine. In\nthis sense, Price’s view is similar to that of John Arbuthnot, who performed\none of the first known statistical tests and believed that if what is observed\nis not due to chance alone, then God’s existence and intervention is\nverified.[6](19_Notes.xhtml#ch7fnr6)\n\nPrice’s argument suggests a valuable direction for our present concerns:\ncalculations of belief bear within them a fundamentally theological\ncommitment. It should be remembered that Bayes was a man of the cloth, and one\nof the two essays he published during his life attempted to prove that God\nwants us to be happy.[7](19_Notes.xhtml#ch7fnr7) Here we see the vital, divine\nconnection between a statistical, subjective measure of belief and proof that\na transcendent force watches over the world—or a system—ensuring its proper\nfunctioning.[8](19_Notes.xhtml#ch7fnr8) While this subjective force found its\njustification in a transcendental, otherworldly anchor, today it appears to\nfunction even without that locus.\n\nIndeed, one of the barriers many statisticians find to accepting Bayesian\nmetaphysics is that it remains difficult to argue for a specific, universal\nset of rules for determining subjective belief. For example, if probability is\nsubjective, how can one effectively determine that a researcher’s belief is\nmore or less true than another’s? For subjective probability to be efficacious\nin its predictive capacity, there must be some objective means of deriving,\ndescribing and computing this subjective belief—and thus for binding that\nbelief to the laws of probability and statistical induction.\n\nThis problem of where subjective probability attaches itself to the laws of\nthe universe becomes all the starker when we remove the epistemic,\ntranscendental anchor of a god from the situation. According to Bayes and\nPrice, the discovery of these new mathematical laws allows one to witness the\nregularity and mathematical design by which God laid out the universe. For\nthem, it likely does not matter if others do not share their belief or think\nby their theological rules. It is sufficient that they know and God knows. We\nsee here again the structure of objectified belief, with God as the force of\nobjectification: this is how things work, with or without one’s individual\nbelief. But for those mathematicians who do not believe in a god, subjective\nprobabilities must find their ground elsewhere; and it is ultimately the\nmarket that comes to provide this “elsewhere,” and with it the full capitalist\nforce of contemporary objectification.\n\nDutch Book Argument\n\nWhile multiple attempts to construct justifications for Bayesian probability\nhave been offered, one of the most classic and enduring explanations is the\n“Dutch book argument.” This argument was introduced in a 1937 article by the\nItalian statistician and actuary Bruno de Finetti.[9](19_Notes.xhtml#ch7fnr9)\nA Dutch book is a gambling situation in which, by taking a given bet (or set\nof bets), a gambler is guaranteed to lose. De Finetti demonstrated how the\nmathematics for dealing with probabilities can be derived from a subjective\nposition in which one’s only motivation is to avoid having a Dutch book made\nagainst them. Here, probabilities are converted into the price (or odds) an\nagent would take for buying a contract. For example, if that individual\nbelieved a coin was fair, they would be willing to pay even odds for a bet on\nheads (i.e., a one-dollar bet would pay out an additional dollar for a correct\nguess). But if they thought that coin was biased, and landed heads twice as\noften, they would require a corresponding payout of two to one to bet on\ntails.\n\nFrom the aim of avoiding a Dutch book, it is possible to derive the basic\nmathematical laws of probability. For example, the sum of the probability that\nan event (e.g., it rains tomorrow) happens and the probability that it does\nnot happen should not be greater than one. But if we accept odds of one to one\nthat it happens, and two to one that it does not happen, then someone else\ncould bet three dollars that it will rain and two dollars that it will not. If\nit rains tomorrow, we will win three dollars on the first bet but lose four\ndollars on the second—a net loss of one dollar. Conversely, if it does not\nrain, we will lose three dollars on the first bet and gain two dollars on the\nsecond—again, a net loss of one dollar. Whatever happens tomorrow, we lose. A\nDutch book has been made against us. If we abstract from this example, it\nbecomes possible to prove that the probability of an event and its opposite\nmust sum to one.[10](19_Notes.xhtml#ch7fnr10) Likewise, the other fundamental\nrules of probability can be derived.\n\nMost important for our current purpose, the Dutch book argument offers an\neconomic grounding for the mathematical laws of probability, even in the\nsubjective realm of Bayesian probability. Tellingly, while Ronald Fisher,\nJerzy Neyman and Egon Pearson were busy turning the academic world toward\nfrequentism in the early and mid twentieth century, actuaries—like Arthur\nBailey, who kept Bayesian approaches alive during the heyday of\nfrequentism—had to be “realists” about costs of risks in calculating insurance\nrates and thus refused to abandon Bayesian\nmethods.[11](19_Notes.xhtml#ch7fnr11) Such “realism” confirms the capitalist\nsuspicion that exchange and prices can serve to translate subjective intuition\ninto the objective truth of the market.\n\nIn Bayesian analysis, according to the Dutch book argument, calculation takes\non the form of contract exchange and thus makes apparent the structure of\nobjectification: subjective belief is tethered to the objective conditions of\nexchange.[12](19_Notes.xhtml#ch7fnr12) We can believe whatever we want, but\nthere is only one way to avoid being cheated. Bayesian analysis no longer\nneeds a god to guarantee the ground of belief: the Dutch book now takes the\nplace of that other book that surely guided Reverend Thomas Bayes.\n\nJust like Marx’s commodity, the Dutch book inscribes objectivity into the\nheart of belief, and in so doing provides an exemplary case of the process of\ncapitalist objectification, removing any locatable ground and distributing\ntruth into the market. Here, we see a series of social relations—under the\nthreat of having a book made against oneself—force the subject to act in\naccordance with those supposedly “objective laws.” Indeed, for de Finetti,\neven objectivity in science becomes a condition not of the regularity of the\nmaterial world but of the predictability of our thoughts. He writes, “I do not\nlook for why _the fact_ that I foresee will come about, but why _I do_ foresee\nthat the fact will come about. It is no longer the facts that need causes; it\nis our thought that finds it convenient to imagine causal relations to\nexplain, connect and foresee the facts.”[13](19_Notes.xhtml#ch7fnr13)\nEverything now appears subjective, but this subjectivity is tied to a contract\nmarket, which in the end requires that all who arrive at market act\nobjectively.\n\nDe Finetti explicitly argues that his is an anti-metaphysical position,\ncompletely naturalizing itself and denying its wholly metaphysical\nsuppositions. It finds its truth not in the sensuous world, but, just like\ncapitalism, in an imagined ideal world that is claimed to be more objective\nand more true than our material existence. Math becomes the truest of sciences\nbecause it forsakes the existence of the world: “Mathematics, logic, and\ngeometry are now immune to the pseudo hypothesis (so to speak) of the\nexistence of the world, the existence of an external reality, the existence of\na metaphysical reality.”[14](19_Notes.xhtml#ch7fnr14) Through objectification,\neverything is turned upside down. To live in this objective world means to\nattend to the socially situated and economically mediated thoughts of humans,\nwhereas to attempt to understand facts in the material “real world” is to\nengage in pure metaphysical speculation. The turn to a nonmaterial mathematics\nallows for a universal that cannot be assailed by any given particular. In\nthis way, the subjective grounds of this mathematical approach produces a\nmetaphysical, objective world: one that is simultaneously transcendent and\nbound to the universal laws of probability governed by the looming fear of the\nDutch book.\n\nWhile Leonard Savage does not make such strong claims about the nonexistence\nof the material world, he goes further than de Finetti to align subjective\nprobabilities with the logic of the market. In his famed book, _Foundations of\nStatistics_ , Savage returns to Neyman’s work on behavioral understandings of\nstatistics. He states that “the problems of statistics were almost always\nthought of as problems of deciding what to say rather than what to do, though\nthere had already been some interest in replacing the verbalist by the\nbehavioralist outlook. The first emphasis of the behavioralistic outlook in\nstatistics was apparently made by J. Neyman.”[15](19_Notes.xhtml#ch7fnr15)\nSavage goes further commending the behaviorist approach of Neyman and Pearson\nas the ground of Bayesian statistics, stating, “Personalistic statistics\nappear as a natural late development of the Neyman-Pearson\nideas.”[16](19_Notes.xhtml#ch7fnr16) In praising Neyman’s behavioralism,\nSavage places the subjective theory of probability fully onto an economic\nfoundation. In a sense, he reduces all epistemology to economics. While de\nFinetti abandoned only causality, Savage abandons knowledge altogether in\nfavor of exchange.\n\nSavage published _Foundations of Statistics_ in 1954, yet in many ways it\nstill represents the state of statistics’ foundational metaphysics. To offer\none example: Graciela Chichilnisky, the mathematical economist who developed\nthe carbon credit trading model underlying the Kyoto Protocol (an exemplary\ncase of an attempt to reduce all possible solutions to market logic), cites\nSavage’s foundations favorably in her work, which focuses on evaluating the\nrisk of highly unlikely but costly events, such as natural disasters and\nmarket crashes.[17](19_Notes.xhtml#ch7fnr17)\n\nMoreover, the substantial statistical advancements made in recent decades have\nbuilt on the foundations developed by Savage, de Finetti and others of their\ngeneration. Challenges to the Dutch book argument have arisen in the\nintervening years, especially concerning inconsistencies that arise from the\norder in which bets are made, along with the apparent but impossible\nrequirement that a gambler have “logical\nomniscience.”[18](19_Notes.xhtml#ch7fnr18) Yet the majority of these\nchallenges argue for mere modifications to the argument rather than its\nwholesale rejection, leaving its foundation\nintact.[19](19_Notes.xhtml#ch7fnr19)\n\nBayesians oversaw a revolution of science, and now, as researchers learn to\nuse massive stores of data and cheap computing power even more effectively, a\nnew epistemic world is being built, piece by piece and study by study, on\nthese new revolutionary approaches. As Andrew Gelman puts it: “If you wanted\nto do foundational research in statistics in the mid-twentieth century, you\nhad to be a bit of a mathematician, whether you wanted to or not … if you want\nto do statistical research at the turn of the twenty-first century, you have\nto be a computer programmer.”[20](19_Notes.xhtml#ch7fnr20) We should add to\nGelman that in the first case, you had to have been a bit of a metaphysician,\nas well; now, one can build and program on top of the metaphysical foundations\nlaid by Savage and his contemporaries.\n\nUltimately, we witness in Savage and de Finetti’s work, and the broader turn\nto Bayesian statistics, a familiar challenge: that of objectification’s\ntorsion between the objective and subjective. The more we attempt to get\nbehind the mask of how things “really” work, the more quickly we end up back\nat the subjective experience of social relations. Thus, as contemporary\ninformational capitalism comes to require more and more finely tuned sources\nof objective knowledge, researchers and statistical metaphysicians discover\nonly the hieroglyphics that capitalism leaves behind.\n\nBy founding statistics on contract exchange, we discover a contemporary\nexample of Alfred Sohn-Rethel’s claim that abstract thinking develops on the\ngrounds of exchange. For Sohn-Rethel, the exchangeability of goods, and their\nultimate representation as money, creates the material conditions in which\nincommensurate objects can face each other and be\nequated.[21](19_Notes.xhtml#ch7fnr21) In tethering knowledge to exchange, the\nsubjective and the objective can never completely abandon each other, for\nexchange twists the subjective and objective together. They must, rather, find\ntheir respective ground in their opposite, with objectivity produced out of\nthe subjective position and the subject bound objectively to laws of exchange.\nMoreover, the development of the Dutch book argument speaks to the victory of\nneoliberalism as a means to organize all life and subjectivity on the grounds\nof market exchange.[22](19_Notes.xhtml#ch7fnr22) Thus, historically we go from\nFisher’s individualism, to the managed objectivism of Neyman and Pearson, and\nfinally back to the individual—only now, this individual is one who must\nfollow the laws of capitalist exchange.\n\nAt first glance, it may appear that this shift in the foundation of\nstatistics, from Bayes’s Bible to de Finetti’s Dutch book, has created a\nrelatively final and stable form that seamlessly melds statistics with the\nnecessities and ideology of capitalism. In sweeping away any ground for\nknowledge outside the exchange relationship—whether it be God or the\nobjectivity of a long-run frequency—this formulation declares that only a\nwell-informed exchange of contracts can underwrite knowledge. In essence, it\nclaims that the only way forward is either the acceptance of this victory in\nfavor of capitalism, or conversely, resistance to this economism at all costs,\nthrough opposition and critique.\n\nBut, at second glance, we can begin to see the contradiction at the heart of\nthis metaphysics: the very incentives of the exchange on which this knowledge\nis founded call forth dissimulation rather than\nknowledge.[23](19_Notes.xhtml#ch7fnr23) Value is always relative; so, the\nunderlying goal of the exchange of contracts is not to increase knowledge\nabsolutely, but rather to know relatively more than the other party to the\ncontract. Especially as the ability to extract knowledge from data advances,\nit becomes increasingly difficult to gain such an advantage, incentivizing\nresearch that aims to stupefy rather than to add to collective or even\nindividual knowledge.[24](19_Notes.xhtml#ch7fnr24) The political economy in\nwhich these transactions occur encourages and rewards the accumulation not\njust of capital, but of knowledge—and with it the ability to construct reality\nsuch that the other party in the exchange does not\nknow.[25](19_Notes.xhtml#ch7fnr25) When statistics and science find their\nground in the accumulation of capital, the incentives of capital accumulation\nwill always override the desire for knowing, favoring relative knowledge and\nwith it dissimulation.\n\nMoreover, despite capitalism’s redefinition of science, the world has not\ncompletely done away with Fisher’s insistence on the lone scientist who knows\nfor herself. This fantasy of probabilistic knowledge that precedes exchange\nobfuscates the threat that capitalism poses to science, as it allows one to\nmaintain that some objective measure of probability stands outside of\npolitical economy. This leads some to believe that even if economic incentives\ndistort processes of knowledge production, true scientific knowledge—through\nbetter methods or changing incentives within universities—can be dug out from\nunder the detritus capitalism throws on top of it.\n\nWe can see here again the complex interplay between the abstract and concrete,\nwith their attendant forms of domination. While Bayesian analysis may lend\nitself to the maximization of profit, what is discovered there is social\nreality (including its racism, sexism, etc.) presented in objective—that is,\nabstract—form. But then, to make matters even worse, this is often interpreted\nthrough a Fisherian—that is, concrete—lens, providing further force to what\nwas initially discovered merely as a temporary, contingent effect of the\nmarket. Thus, statistics and algorithms effectively launder nonobjective forms\nof violence and bias, giving them greater stability, only to be fed back into\nthese systems as the initial conditions for the next\nround.[26](19_Notes.xhtml#ch7fnr26)\n\nThe discovery of the Dutch book argument may appear to put statistics on a\nsolid ground through capitalist exchange, but in the final analysis we\ndiscover that scientific production and capitalism have now split ways. For\nthe dictates of value extraction threaten, always and everywhere, to outrun\nand overturn the necessities of knowledge production, both though external\nincentive structures and internal metaphysical structures. If science is to\nhave a future for humanity, it must be in opposition to and outside of\ncapitalism. Under the conditions of late capitalism, we cannot go back to the\nhalcyon days of Fisher, when science could claim to be a royal road to\nindividually held truth. Science and the production of knowledge now require\nand call for a foundation that turns against capitalism.\n\nThis, then, is the ultimate task of a revolutionary mathematics today: to work\ntoward the future of mathematics and science, redefining their underlying\nmetaphysics, with a full understanding of the political and economic stakes\nthat both determine and are determined by the possibility of this future. On a\nmetaphysical level, the Bayesian revolution has overturned the individualist\nand chauvinistic Fisherian paradigm. The revolutionary mathematician, even if\nshe is not a mathematician or a scientist in the colloquial sense, seeks to\ncreate new truths and new computations in the wake of this tumult. The future\nof science is a collective endeavor, and as such, our collective and social\nexistence provides the very ground and possibility of the sciences.\n\nBy no means should this be taken as opposition to statistics, calculation or\nprediction. It is not necessary to find some surplus of language or some form\nof non-exchangeability in order to resist capitalism and\ncomputation.[27](19_Notes.xhtml#ch7fnr27) On the contrary: computation and\nexchange, at their heart, work on the level of non-exchangeability. Statistics\nis nothing short of magic, performing metaphysical work that sutures our\nsubjective and probabilistic knowledge to the material world. It mediates\nbetween the particular (data) and the universal (hypothesis), making the\nuncomputable computable. But in doing so, it functions within and through\npolitical economy and exchange.\n\nWhile the knowledge that algorithms and statistical methodologies produce are\noften presented with a Fisherian veneer that claims recourse to some\ntranscendent and individual truth, their actual metaphysical support is only\ntethered to this world by economic advantage and risk. They reflect not the\nworld as it is, but rather the world _as it is profitable_. Accordingly, a\nrevolutionary mathematics must aim not to show the world “as it is,” but\nrather to recognize the necessity and importance of this objectification.\nFisher’s realization that Neyman and Pearson’s behavioral interpretation of\nstatistics required a science managed by a five-year plan or plant manager was\nincredibly prescient. Indeed, the production of knowledge through\nprobabilistic systems, whether statistics or algorithms, now requires some\nform of social investment in this production, in opposition to the dictates of\ncapital accumulation. The only question that remains is whether statistics is\nto be managed by plant managers—at pharmaceutical companies, tech companies,\nuniversity research offices, and so on—or by some form of collective set\nagainst and beyond capitalism.\n\nWhat must be decided politically, and with it metaphysically, is whether these\nabstractions and knowledges will be founded on deception and the reproduction\nof social inequality, or instead on some other equality. This decision, if we\ncan correctly call it that, is not simply a decision made by individuals; it\nmust be collectively discovered and constructed, and made into a\nnecessity—just as the commodity has decided how one must survive under\ncapitalism. All the while, it must also recognize the importance of\nconfronting nonobjective forms of domination. Ultimately, it is through this\nprocess that statistics and machine learning present themselves as\nrevolutionary objects: they call us to recognize, objectively, that they can\nno longer function under capitalist modes of production. The task of a\nrevolutionary mathematics is, then, to discover and create another set of\ndemands by which scientific objectivity necessitates a change of economy and\nproduction. In short, if science is to continue to feed us, both materially\nand intellectually, it can only do so against capitalism.\n\n\n[Chapter 8](04_Contents.xhtml#ch13)\n\n[Automated Abstractions  \nand Alienation](04_Contents.xhtml#ch13)[1](19_Notes.xhtml#ch8fnr1)\n\nWhether forecasting gambling odds, the probability of rain or the future sales\nof Pop-Tarts, statistical models provide mathematical abstractions of the\nworld; they take a varied and disparate world and reduce it to data that is\nthen further reduced to a model that can extrapolate to new data. Yet unlike\ntraditional Western thinking around abstractions, these models are not\nindexical of any stable world outside the constant flux of the data they\ningest. The statistical abstractions that functionally underwrite the\ndiscoveries of machine learning rediscover their index in the fluidity of\nexchange, a foundation that does not point elsewhere to some transcendent\nideal but rather immanently to the reasoned exchange of contracts and bets.\n\nWithin exchange’s currents, these abstractions become increasingly mobile and\nmodulatory; the same statistical processes can describe global social changes\nand individual shopping patterns. Like their more stable predecessors, these\nabstractions permit a freedom in dealing with the world, but at the cost of\nblinding us to their particulars. That is to say, a model is powerful in so\nmuch as it allows one to understand and shape what has not been seen before,\nbut it simultaneously risks overlooking what is most important. To account for\nthe productive power and danger of these models, one must understand their\nabstractive force and the shifts in collective processes of abstraction they\nrepresent.\n\nFreedom in Abstraction\n\nAbstraction is freedom. As a word, “abstraction” etymologically connects to\nLatin’s notions of “separation,” “withdrawal” and “pulling away.” As a\nconcept, abstractions provide us distance from the muddy realities of the\nworld, a detached epistemic position that allows things to meaningfully exist\nacross different contexts: rain can become indicative of larger crop cycles or\neven evidence of global warming; individual events and data can be fit into\nlarger patterns that can tell us about our world.\n\nThis distance from specificity has roots in the mythical realm: in Roman,\nGreek and Egyptian culture, deified abstractions, as expressed through the\nnarrative lives of groups of gods and demigods, were treated as indices for\nspecific concepts. Pax, the daughter of Jupiter and Justice, embodied peace.\nThanatos embodied death, while Eros embodied love.[2](19_Notes.xhtml#ch8fnr2)\nThese abstractions were concepts personified, and vice versa; and each\nassociated deity enjoyed concept-based cult worship and stature within the\nreligious pantheon.\n\nAbstractions like Pax would go on to lose their deistic distinction, dethroned\nfrom divinity while their conceptual namesakes remained in the heavens. These\nwere the “forms,” famously described by ancient Greeks like Plato in his\nSocratic dialogues, or the immaterial ideals by which the lived, physical\nworld was understood as a mere shadow of their unchanging essence. For these\nidealist thinkers, true knowledge emanated from the irreducible, universal\nform, not from the specified, particular instance. In basic metaphysical\nterms, the particular was an inferior copy of the\nuniversal.[3](19_Notes.xhtml#ch8fnr3)\n\nWhile these abstract forms allowed a certain freedom for those who could wield\nthem and were well represented by them, they necessarily effaced the\nexperiences of others. The Enlightenment, colonialism, white supremacy,\npatriarchy and capitalism were all constructed atop abstractions that separate\nconcepts from context, especially when it comes to the context of the\nmarginalized. As philosopher and environmental activist Vandana Shiva writes,\n“Separability allows context-free abstraction of knowledge and creates\ncriteria of validity based on alienation and nonparticipation, then projected\nas ‘objectivity.’ ”[4](19_Notes.xhtml#ch8fnr4) Even when abstractions threaten\nto cut away at authority, as we saw earlier with scientific abstraction in the\nhands of freethinkers like James Jurin, the form of this expertise generated\nthrough detachment has an uncanny ability to reassert itself; after all, Jurin\nsubsequently attempted to fall back on the authority of Isaac Newton.\nAbstractions tend to generate authority by empowering those who can (and are\nallowed to) make them productive.\n\nFrankfurt school philosophers Max Horkheimer and Theodor Adorno argue that\n“the distance of subject from object, the presupposition of abstraction, is\nfounded on the distance from things which the ruler attains by means of the\nruled.”[5](19_Notes.xhtml#ch8fnr5) However, the once-hallowed epistemology of\ngods like Pax or the Christian divinity have been, over the course of the last\ncenturies, replaced by an unencumbered, Western white masculinist mind. In the\ncase of both God and man, knowledge and power is founded on the ability to\nabstract, to separate the subjective and objective world. And from the mind of\nthis singular subject position, those who rule find power in seeing the world\nfrom afar, reclaiming the unreachable peak of Mount Olympus with the force of\n“objectivity.”\n\nHorkheimer and Adorno continue: “Under the leveling rule of abstraction,\neverything in nature [is] repeatable, and of industry, for which abstraction\nprepared the way, the liberated finally themselves become the ‘herd’ which\nHegel identified as the outcome of enlightenment.”[6](19_Notes.xhtml#ch8fnr6)\nIt is here that we can see the power of the commodity as an abstraction; it\nfrees us at the same time it binds us to its calculations. Insofar as\ncommodities conceal the social relations according to which they were produced\nby rendering qualitatively distinct forms of labor quantitatively equivalent\nthrough their exchange, commodities—as well as nature, industry, people and\nknowledge—become “repeatable,” objectified and hence countable. Such\nobjectification treats the previously incommensurate as commensurate, thus\nrendering it understandable.[7](19_Notes.xhtml#ch8fnr7)\n\nTo make nature repeatable—that is, to make life and goods exchangeable, and\nthus functionally traversable across different contexts—abstractions\nparticipate in what queer theorist Jasbir Puar calls the “productive tensions\nbetween abstraction and location.” In essence, the particularity of location\nchallenges the presumption of an abstraction’s commensurability, of the\nuniversal’s universality.[8](19_Notes.xhtml#ch8fnr8) More specifically, to\nmake something repeatable, and commensurate, is to forego any inquiry into\nthese particulars—which could disrupt an abstraction and reveal what something\nwas (or could be) outside the scope of its analytic use—while at the same time\ncontinuing to extract value from its concrete and local instantiation. In one\nsense, statistics and machine learning operationalize this productive tension;\nbut, just like capitalism, they do so on the grounds of a fundamental\ncommensurability. Machine learning’s preference for correlations over causal\nexplanations allows the inclusion of local data that may not immediately seem\nrelevant to aid in the production of an abstract model. And yet, even these\ndata must be selected, gathered and made digestible by an algorithm.\n\nThus, while maintaining the violence and domination of earlier forms, contrary\nto older modes of Enlightenment abstraction, the world of machine learning\nappears as an infinite chain of particulars, but also as an infinite chain of\nlocal and mobile abstractions—two views that, in the final analysis, amount to\nthe same thing. All correlations are local only to the data on which they are\ntrained. A model is never universal but always a particular and local\nabstraction. Machine learning appears to establish precisely what capitalism\nhas always dreamed of: a smooth, universal lingua franca of epistemic and\neconomic commensurability that always produces its truth only at the exact\nlocal moment of exchange. Algorithms allow the conversion of every bit of\nextractable data into interchangeable bits that can be compared and ultimately\nexchanged. Yet, these algorithms still produce and reproduce exactly the same\nkind of hierarchized difference that older forms of universality\nhave.[9](19_Notes.xhtml#ch8fnr9)\n\nWhile abstraction, understood broadly, has long been an essential, if\nsimultaneously productive and dangerous, component of human thought, machine\nlearning mobilizes and liquefies these abstractions; machine learning\nalgorithms, such as neural networks, allow computers to create abstractions\nthat are only momentary correlations. Products are recommended to specific\nconsumers based on one set of attributes this hour and then, as new data\narrives, on another set the following hour. Here, we can see the revolutionary\nimplications of Bayesian analysis: frequentism sets up the experiment to\ndetermine repeatable, population-level abstractions, whereas Bayesianism\nallows the production of a nearly infinite field of hypotheses that can create\nan abstraction for each case. Bayesianism favors abstractions that produce a\nthought that is not universal, but merely momentary, whose only value is the\nvalue it fetches at market on a single day, or even for a single millisecond.\nIn this way, it overflows and hollows out this productive tension, creating\nabstractions that are always local. It aims to banish all incommensurability\nin a sea of never-ending calculations.\n\nA world without incommensurability is one that follows the utopian desires of\nEnlightenment idealists and now, increasingly, those of Silicon Valley\nengineers. This is a world imagined to be directly encoded into quantitative\ndata, programmed and eventually automated—the “frictionless capitalism” of\nwhich Bill Gates dreamt in the 1990s. In short, it is a world without the\ntraditional subject precisely because, in algorithmic form, the distinction\nbetween subject and object evaporates; one must act in accordance with what\nthe algorithm will calculate about them, like a content producer constantly\ntrying to optimize their search engine ranking. Each looks more and more like\nthe other, and both subject and object become free, in a way, but only to\nfollow the laws of the system. Thus, this freedom becomes its opposite; it is\nthe freedom to choose what these object-abstractions demand of us.\nCorporations and researchers use the freedom of abstraction to run algorithms\non seemingly disparate sets of data, conferring truth and value on billions of\ndata records without knowing why there is a correlation; they know only that\nthere is one, and that “objectively” they must choose to follow it. The only\nuniversal that remains is the structure of exchange itself and the constant\nfear of the Dutch book.\n\nGoogle’s PageRank algorithm offers a clear example of this dynamic. The\nalgorithm is used to determine the “quality” of a web page based on the\ncitation—or link graph—structure of the web, normalizing the quality of each\npage based on the quantity of “high-quality” websites that link to it. For\nPageRank, a web page’s rank is not simply the total number of sites that link\nto it, but also the supposed quality (based solely on the calculations of the\nalgorithm) of those pages that link to that web page. Consequently, Google may\nrank a site with fewer quality links higher than it does a site with more\nlinks from low-ranked sites.[10](19_Notes.xhtml#ch8fnr10)\n\nGoogle’s computers, having crawled the majority of the World Wide Web,\neffectively simulate individuals surfing the web in order to determine the\nrates at which various pages will be visited. Statistically, we can understand\nPageRank as a measure of “the probability that the random surfer visits a\npage.” And with a few additional “secret sauce” elements added to this simple\nbut elegant formulation, Google’s system functionally determines the\n“importance” of each website it has crawled. The PageRank of a single web page\nrepresents its importance. But a page’s PageRank is wholly recursive: an\nimportant web page is one that is linked to from other important web pages.\nWith PageRank, the entire concept of importance is founded in the algorithm\nand this recursion itself; so anyone who seeks to enter into this world must\nalso model their own sense of importance on this algorithmic recursion.\n\nTo exist in this abstracted algorithmic world requires that individuals become\nopen to, commensurate to and processable by the whole material infrastructure\nof algorithmic life. It is to live in the exclusive swell of these\nabstractions, tracing only how they are produced and how they are connected to\neach other. While this blow to the sovereignty of the subject may seem harsh,\nwe must resist the urge to reassert ourselves as subjects in the Enlightenment\nsense, precisely because that subject remains ineffective and encircled when\nalgorithmic knowledge a priori reduces us to the supposedly immaculate,\ncommensurate terrain of numbers and statistics.\n\nAlgorithmic Abstractions\n\nMachine learning reduces everything to data in order to ground the world anew\nthrough abstractions, always searching for locally optimal solutions in real\ntime. To offer another example, take Google’s image recognition system, a\n“deep learning” approach, which means in essence that there are multiple\nlayers to the neural net that learn different levels of abstraction in order\nto allow researchers to train a network of computers to classify images.\nThrough this network, computer scientists at Google were able to create the\ncategory of “cat” according to the statistical commonalities from 10 million\nYouTube video screen grabs.\n\n“We never told it during the training, ‘this is a cat,’ ” said Google fellow\nand researcher Jeff Dean. “It basically invented the concept of a\ncat.”[11](19_Notes.xhtml#ch8fnr11) Google’s network architecture, composed of\n16,000 interlinked core processors, had mimicked the firing of individual\nneurons, functionally enabling their computers to recognize common objects\nwithin large sets of data—all without labeling and without preconditioning. In\ndoing so, Google’s algorithm invented “cat” without having any notion of what\na cat was. There is no construction of cats through the universal a priori of\ncat. Nor is there any group of priests or experts defining a universal cat\nthrough some presumed, particular position of distanced objectivity.\n\nInstead, Google’s cat is made from the ground up, built atop pattern\nrecognition algorithms. These algorithms search for local features and combine\nthese into more general abstractions.[12](19_Notes.xhtml#ch8fnr12) This\nfreedom of abstraction begins with pixels, which become lines, which become\nshapes—each step forming another hidden layer in Google’s artificial neural\nnetwork. Then those shapes are aggregated at the next level according to\ngraphic similarities (what colors and shapes are most often present). Next, a\nbarrage of millions of random image thumbnails are thrown at Google’s 16,000\ncomputer processors so that the visual component of what we now call “cat,”\nand what a computer might know only as a series of polygon “edges” and\ncommonality measures, can be made and discursively massaged so as to be called\na cat.\n\nIt should be remembered that while these abstractions appear to be built ex\nnihilo, the training data, which comes from YouTube videos, is the by-product\nof a complex, biased and mediated process of social production. Thus, from\nthis mass of socially produced data, an automatic abstraction is reflected\nback to us as its truth and objective product. In a way, this is analogous to\nthe construction of value: an ongoing process of acquiring or creating data is\nconstantly put into exchange with itself, working on itself over and over\nuntil it can be re-presented as truth.\n\nThis is a key consequence of the move to algorithmic knowledge: what Google\nsays is (and, conversely, what it says is not) a cat is based on socially\nproduced data, and the singular abstraction of Google’s algorithm. Even\nknowing exactly how the algorithm works, when it runs in real time we have no\naccess to the middlelevel abstractions it produces. The computational opacity\nthat created this cat is akin to quantum physicist Erwin Schrödinger’s famous\nfeline, which as a result of random radioactive decay was said to be\nsimultaneously alive or dead: we have no idea if what is being processed is in\nfact alive, dead or even an actual cat. Moreover, while in the end we may be\nconcerned mostly with the adjudication of “cat” or “not cat,” just like\nSchrödinger’s cat, the understanding of these judgments are ultimately\nprobabilistic: the algorithm is not designed to always know what is and is not\na cat, but rather to manage an uncertain world and guess correctly often\nenough. In a world made of data (as Google’s cat is nothing but strong,\nsequential correlations between statistically similar objects), meaning is a\nmatter of algorithmic output: a probable cat (see the image below).\n\n![images](images/img_p187.jpg)\n\n_The Google algorithm’s archetypal image of a cat constructed from YouTube\nstills._\n\nAccordingly, we see how the world gets dynamically objectified before our very\neyes. The vocabulary of this algorithmic Adam becomes the dynamic, emergent\nexpressions that regulate the knowledges of flora and fauna—and indeed, any\nother concept the algorithm learns. But unlike the original Adam, whose name\ncarries with it the imprimatur of God, these names are always particular,\nlocal and fluid; in fact, they do not actually exist, for the algorithm merely\nassembles a composite and stores it somewhere in memory. While we may look at\nthis objectified output and shake our heads in agreement that it has\ndiscovered a cat, the endgame of Google’s algorithm is to repeat this process\nmillions and millions of times. Ultimately, the abstractions an algorithm\nuncovers create a reality that must be reckoned with whether the abstractions\nthemselves are “true” or not.\n\nFor example, an algorithm used by the state of Michigan to determine whether\nunemployment insurance claims were fraudulent falsely accused over 34,000\npeople of fraud. These people saw their unemployment payments automatically\ncut off. While these errors are finally being corrected, those thousands of\nindividuals still had to deal with the assessments of this software; since the\nalgorithm categorized them as fraudulent, they functionally were, regardless\nof their actual behavior.[13](19_Notes.xhtml#ch8fnr13) Especially because the\nstate and the market, under their expansive neoliberal logic, function in a\npredatory manner, these systems serve less to predict the future than to\ndefine it and provide an authoritative force for that\ndefinition.[14](19_Notes.xhtml#ch8fnr14) Such algorithms constantly define and\nredefine what the abstracted knowledge of the world means, but on a ground\nthat is always the product of capitalism and of the injustices of history that\nhave constructed our current moment.\n\nEnclosing the General Intellect\n\nAs machine-learned abstractions produce knowledge like Google’s cat, the\ninstructions and parameters that assembled that knowledge become wholly\nclassified, and any who may hope to fix the errors generated by these\nalgorithms are always running behind their automated production. In this way,\ndigital capitalism utilizes this fluidity to enclose knowledge\nitself.[15](19_Notes.xhtml#ch8fnr15)\n\nKnowledge founded on an external referent—whether God or some heroic theory of\nscientific truth—functionally resists enclosure, for this ground must be\nshared among society to make the knowledge socially usable. For Christianity\nto become socially productive, it had to ally with the state and abandon its\nstatus as a small cult. Of course, states can and do build privatized legal\nframeworks (such as patents) atop this shared ground to allow the protection\nof corporations’ market positions, but knowledge of the world itself still\nbelongs to the community that shares this ground.\n\nHowever, once knowledge finds its ground exclusively lodged in the mechanisms\nof capitalist exchange, it exposes itself to enclosure—analogous to the\nprocess by which, from the thirteenth century on, shared grazing land in\nBritain was enclosed and made into private farmland. In the history of\nstatistics, we see a similar confining process in the unmooring of\nprobabilistic knowledge from a shared (if selfish) research community that\nbegins with Jerzy Neyman and Egon Pearson and finds its ultimate expression in\nLeonard Savage and Bruno de Finetti. And now, with the contemporary\noperationalization of statistics in modern machine learning, the process of\nenclosing knowledge has reached an intensity and extent previously impossible.\nFor proof of this drive for enclosure, one merely has to look to the present\nclamor and price for large proprietary datasets, or the seemingly nonstop\nrevelations that corporations like Facebook have provided secret access to\nuser data to large corporate partners.[16](19_Notes.xhtml#ch8fnr16)\n\nBehind these bulwarks of privatized, abstracted knowledge reside the\ngenerative spirits of what Marx called “primitive accumulation”—the origin\nprocesses by which accumulated surplus wealth could become\ncapital.[17](19_Notes.xhtml#ch8fnr17) Yet such initial accumulations of\nsurplus, in opposition to the ideal meritocracy of classical thinkers like\nAdam Smith, must reside outside the formal laws of capitalism. Historically,\nit was violence, colonialism, theft and enslavement that expropriated and\nimpoverished others in order to create the massive stores of wealth needed to\nbuild capitalism’s foundations and maintain their function. This “systematic\ntheft of communal property,” writes Marxist geographer David Harvey, began\nwith a “grand movement of enclosure of the commons,” a dual process of both\ncolonial exploitation and the privatization of previously public\ngoods.[18](19_Notes.xhtml#ch8fnr18)\n\nAs capitalism continues its turn toward information as a new frontier, we\nenter a second enclosure movement that privatizes not land but ideas—ideas\nonce considered the common property of society and culture writ\nlarge.[19](19_Notes.xhtml#ch8fnr19) From the proprietary computer code that\nundergirds much of the world’s digital infrastructure to profitseeking\nscientific research on the human genome, patents on intangible knowledge have\nbecome engines of information capital.[20](19_Notes.xhtml#ch8fnr20) These\nforces seek not to generate wealth so much as expropriate, steal and privatize\nwealth that exists elsewhere.\n\nA new kind of primitive accumulation arises from the specifics of this second\nenclosure. Here, the privatization of once-common agricultural lands is\nreplaced with the privatization of once-common social knowledge and conditions\nof social life itself, or what Marx calls the general\nintellect.[21](19_Notes.xhtml#ch8fnr21) This most recent enclosure is carried\nout by many long-existing legal implements: patents, trade secrets,\nnondisclosure agreements, and so on.[22](19_Notes.xhtml#ch8fnr22) Moreover,\nthe establishment of massive global supply chains owned by multinational\ncorporations allows the expropriation of labor power and wealth from the\nglobal South. This further facilitates the movement of an increasing number of\nworkers into urban areas, accentuated simultaneously by the privatization of\nlands by agribusiness and the possibility of higher wages in urban areas,\nfurther increasing the global supply of labor and its\nreserves.[23](19_Notes.xhtml#ch8fnr23) While these methods have long been part\nof capitalism, they are now turned directly into the production of surplus\nvalue through the automatic and algorithmic production of knowledge, which is\naccompanied by rising global demand for extraction and production.\n\nThis metamorphosis is exemplified by the case of highfrequency algorithmic\ntrading and the intellectual property regimes that facilitate gene patenting.\nThe former acts essentially as a corporate tax on slower traders, while the\nlatter privatizes the intellectually and socially productive power of\ninformation that exists out in the world, often through the theft of\nintellectual property owned by marginalized\ncommunities.[24](19_Notes.xhtml#ch8fnr24)\n\nWhere this privatization becomes especially clear is in the growth of what\neconomic theorist Nick Srnicek outlines under the name “platform capitalism,”\nwhere companies own the software and hardware on which social and economic\ninteractions take place.[25](19_Notes.xhtml#ch8fnr25) As famously recounted by\nboth anti-capitalists and cyberutopians alike, while Uber owns no real taxis,\nFacebook has no real friends, and Airbnb owns no properties, all three\ncompanies make billions in revenue solely by enabling the most basic of market\ninteractions. The new digital economy has overseen the extraordinary\nproliferation of platforms—from Uber to Etsy to Amazon—and even traditional\nmanufacturing companies like General Electric and John Deere have attempted to\ntransform themselves into proprietary data companies. John Deere, for\ninstance, has in essence created a farming platform where farmers are unable\nto repair their own equipment, and where the data they produce is aggregated\nby the company.[26](19_Notes.xhtml#ch8fnr26)\n\nSrnicek argues that these platforms generate their profits through two\ninterrelated methods. First, they supply an infrastructure for exchange that\nbenefits from network effects: when enough sellers and buyers are using the\nplatform, both groups are further attracted to the platform because their\nopposite is already there. These effects make commercial competition\nincredibly difficult, since any upstart platform will lack the requisite\nnumbers of buyers/sellers or products to choose from. They are then able to\nextract a percentage from the transaction, essentially requiring those on the\nplatform to pay rent. Second, and most important for our purposes, platforms\nextract data from users and their transactions. This data, abstracted by\nmachine learning algorithms, allows for more rapid and effective matching of\nbuyers to sellers within the platform, as seen in Uber’s ability to outpace\ntraditional taxis in getting drivers to passengers. This virtuous cycle\nenhances an established platform’s competitive advantages, further\naccelerating the hoarding of capital and market position.\n\nLike algorithmic alchemists, these platforms have found a way to extract value\nfrom exchange twice. For not only do they tax an exchange (e.g., Uber takes a\npercentage of each fare, while Facebook charges for advertising), but they\nthen extract data from that transaction, algorithmically processing it to make\nthe platform’s matching system even more efficient. The data can also be sold\nto other companies, allowing its further transformation into profit. Now not\nonly the spaces, but the very means by which people shop, communicate and\ntravel are being directly privatized. The once-public (or at least semi-\npublic) cacophony of the market itself is now being enclosed, and with it, any\npublic knowledge that could be learned from exchange.\n\nThe process of enclosing the general intellect begins with what the Marxist\nphilosopher Paolo Virno calls “mass intellectuality,” to which, “the entirety\nof post-Fordist living labour [belongs] … to the extent that it is the\ndepository of cognitive competencies that cannot be objectified in machinery.\nMass intellectuality is the prominent form in which the general intellect is\nmanifest today. What is at stake is obviously not the scientific erudition of\nthe individual labourer.”[27](19_Notes.xhtml#ch8fnr27) We should take Virno’s\nstatement here at its most literal: there exists an unbridgeable metaphysical\ngap between the general intellect and the scientific production of the\nindividual laborer. For science or computation to be productive, it must be\nproductive directly at the level of the general intellect. Ronald Fisher’s\ninsistence that the individual scientist is the one who should know can never\nbe directly productive. While this process of enclosure may appear profitable\nfor a time, it directly threatens its own foundations, and scientific\nknowledge production along with it.\n\nWhile some autonomist Marxists have read the development of the general\nintellect as a force that would necessarily diminish labor time completely,\noffering a leftist promise of techno-utopianism, the possibility of its\nenclosure and privatization suggests, rather, that it is a site of capitalist\ncontradiction—albeit one whose resolution in the favor of labor is not\nguaranteed.[28](19_Notes.xhtml#ch8fnr28) In the words of information and media\nscholar Nick Dyer-Witheford, it is valuable to see “the issue within an\nantagonistic perspective, seeing the noosphere contested in class war—and this\nis the prospect the theory of ‘general intellect’\nopens.”[29](19_Notes.xhtml#ch8fnr29) It opens up the space for the development\nand future of humanities collective potential, but offers no promise of a\nspecific development.\n\nOn a metaphysical level, the withdrawal and collapse of a transcendental\nanchor for knowledge simultaneously allows for its privatization (since this\ntranscendent other no longer guarantees its possession by a community often\nfounded on its own exclusions) and demands its collectivization (since it\nrequires the collective act of exchange based on shared knowledge for its\nmeaningful production). While there should be no fundamental opposition to\nindividual pursuits of knowledge, Fisher’s “selfish and perhaps heretical aim\nof understanding for oneself the scientific situation” should be of no concern\nto those who hope to make science productive for the general\nbenefit.[30](19_Notes.xhtml#ch8fnr30)\n\nIndeed, for the general intellect to function, it must remain open and social;\notherwise, dissimulation will take hold, and, as we witness with the litany of\nrecent defeat devices and the privatization of platforms, rent-seeking\nbehavior will necessarily win out over any socially beneficial abstraction of\nthe world. Here, we encounter the primary contradiction of knowledge in\ncontemporary digital capitalism: for statistics and machine learning to be\nproductive, they must function at the level of the general intellect, not\nresigned to the private enclaves of proprietary datasets. But, the more\nsuccessful datasets are in generating efficient abstractions for profit, the\ngreater incentive there is to privatize, enclose or simply fake them. Once the\nproduction of data and its abstractions are enclosed within Facebook’s or\nUber’s databases, those abstractions invariably end up elucidating not some\nideal, general will but what we might call the “enclosed intellect”—one that\ncan produce only knowledge from a specific commodified position according to\neach firm’s location in the market, where that knowledge is determined\nexclusively by its ability to produce a profit. This whole process invariably\nreplicates and objectifies all of the social biases and exploitations that\nexist there. Once this happens, then non-knowledge, dissimulation and defeat\ndevices become more exchangeable, and hence profitable, than knowledge or any\naspiration for a more just world.[31](19_Notes.xhtml#ch8fnr31) We can look,\nfor example, to Theranos, the company that falsely claimed to have developed a\nnew more efficient technology for blood testing, raising hundreds of millions\nof dollars on such fake knowledge.[32](19_Notes.xhtml#ch8fnr32) Under late\ncapitalism, completely fake abstractions can be just as valuable as ones that\ncorrespond closely to the world.\n\nThe Capitalist Calculation Problem\n\nThe hyper-capitalist, privatized enclosure of the general intellect reflects\nan inversion of liberal economists Ludwig von Mises and Friedrich Hayek’s\nfamous critique of socialism. For von Mises, and later Hayek, socialism\nsuffers from what they call a “calculation\nproblem.”[33](19_Notes.xhtml#ch8fnr33) This problem, for them, is that a\ncentralized, bureaucratic entity is unable to properly ascertain the optimal\ndistribution of goods. How would a Chilean state planner in Santiago know how\nmany mobile phones should be sent to a store in Viña del Mar?\n\nVon Mises and Hayek argue that markets solve this problem by using price to\ncalculate a rational, and efficient, distribution of goods through supply and\ndemand. If the market is allowed to run its course, the subjective interests\nof all consumers, they reason, will become accurately reflected within the\ninfinite cycle of exchange. Thus, the laws of supply and demand allow the\nmarket to move goods where they are most needed. If we are to believe these\ntwo, capitalism bridges the metaphysical gap separating individual knowledge\nfrom the general intellect, or the particular from the universal, by creating\nan imaginary world where each individual’s private calculations can create a\nstable economy. This resolution conceives of individuals as distributed\ncomputers who together buy and sell their labor and goods so as to calculate\nthe “optimal” distribution of goods and labor. In essence, capitalism produces\nthe same mobile, fluid, real-time abstractions that algorithmic systems are\nsupposed to achieve.\n\nHayek, who arguably founded the political economic framework we now call\nneoliberalism, laid out the importance of this calculative process in his 1936\naddress to the London Economic Club:\n\n> Economics has come nearer than any other social science to an answer to that\n> central question of all social sciences, how the combination of fragments of\n> knowledge existing in different minds can bring about results which, if they\n> were to be brought about deliberately, would require a knowledge on the part\n> of the directing mind which no single person can\n> possess.[34](19_Notes.xhtml#ch8fnr34)\n\nFor both von Mises and Hayek, this process is only possible under capitalist\nmarket conditions, because no bureaucracy can compute a “rational” economic\ndistribution that fully takes others’ individual knowledges into account. But\nas information and data become more integral to the economy, it is now\ncapitalism that becomes wholly unable to calculate a rational distribution\n(even by capitalism’s very inequitable definition of “rational”).\n\nThis point is exemplified by the perpetual state of crisis that defines\ncontemporary global capitalism. For example, on September 16, 2013, the US\nFederal Reserve announced that it would continue its post-2008 financial\nstimulus strategy of purchasing government bonds. The decision, announced at\nthe Reserve’s headquarters in Washington, DC, was made at exactly 2 p.m. (as\nmeasured by the national atomic clock). A mere three milliseconds after this\ninformation was announced, several large asset orders were placed in Chicago.\n\nAt its fastest, given the speed of light, which also limits the speed at which\ninformation can travel, it should take such news at least seven milliseconds\nto make the fiber-optic journey from DC to Chicago. While reporters were told\nabout the decision prior to the announcement, they were sequestered in a room,\nunable to communicate until 2 p.m. In the high-stakes world of highfrequency\ntrading—where computers attempt to make massive profits off of millisecond\nadvantages in trading speed—these four milliseconds of advance notice were\nworth substantial amounts of money: an estimated $600 million changed hands\nbefore other Chicago traders were aware of the Federal Reserve’s decision.\nSomeone had stolen these incredibly valuable milliseconds, acting on this\ninformation in order to trade hundreds of millions of dollars before anyone\nelse could act.\n\nHigh-speed financial transactions come as a consequence of what David Harvey\nargues has become central to our contemporary economic structure: the\ncompression of space-time made possible by the rise of high-speed networked\ndigital capitalism.[35](19_Notes.xhtml#ch8fnr35) Since the 1970s, accelerating\ndevelopments and mobilizations of communication technologies have intensified\nthe circulation and consumption of immaterial and ephemeral commodities, from\nmemes to financial derivatives.[36](19_Notes.xhtml#ch8fnr36) These\ninformational commodities no longer simply manage material production, but are\ndirectly productive themselves. In the labor processes of a thoroughly\nvirtualized and informationalized post-Fordist economy, Paolo Virno argues,\n“thoughts and discourses function in themselves as productive ‘machines’ … in\ncontemporary labour and do not need to take on a mechanical body or an\nelectric soul.”[37](19_Notes.xhtml#ch8fnr37)\n\nKnowledge in the knowledge economy is not simply congealed in, and mobilized\nby, sophisticated machines. Rather, knowledge itself has become a quasi-\nindependent productive force, immanent to the social relations of contemporary\ncapitalism: knowledge no longer increases the speed by which industry can\nproduce goods but, as it becomes a force of automation, produces value on its\nown (e.g., Bitcoin mining, high-frequency trading, automated news reporting,\netc.). Production is no longer localized in factories. Capitalism subsumes and\ntransmutes all social relations into modes of production such that society\nitself becomes a “social factory” where all social interactions become\npreludes to production, now exemplified by constant unpaid cultural production\nand the gig economy, where one is always\nnetworking.[38](19_Notes.xhtml#ch8fnr38)\n\nWe are witnessing the rise of an economy that places immense importance on the\naggregation of minuscule fragments of information: search queries, GPS\nlocations, Facebook posts, to name a few. While labor time was surely spent\nfiguring out how to steal these four milliseconds, the material economic\npayoff was made from pure knowledge and what amounts to a form of theft:\nknowledge of the exchange value of financial instruments before other traders.\nThus, it is possible to offer an alternative definition of the so-called\n“knowledge economy.” This knowledge economy is characterized not by the\nproductive use of knowledge, but rather by its exact opposite: the\nunproductive theft and privatization of knowledge, and hence the theft and\nprivatization of the general intellect. Rather than these three stolen\nmilliseconds being used to advance some form of general knowledge, they\nfunctioned to valorize capitalism by impoverishing collective knowledge.\n\nIn this way, current uses of statistical inference and machine learning merely\ndiscover the rules of the game—not how to change them. As long as the stakes\nare built around profit, statistical methods can only be a science of how best\nto tilt the scales or cook the books. Thus, the question of statistical\ninference and machine learning is a priori a question of economy, social\nproduction and knowledge: it cannot stand outside the political economy that\nunderwrites it.[39](19_Notes.xhtml#ch8fnr39)\n\nWhen economic value production is shaped, or at the very least facilitated, by\npersonal degrees of uncertainty—as the Bayesian revolution places the burden\nof knowing on the individuals who exchange, whether humans or computers—there\nbecomes an increasing economic incentive to lie: under contemporary\ncapitalism, corporations now fabricate their firms’ economic conditions with a\nregularity and scale that was once seen to be the exclusive preserve of Soviet\nbureaucracy. For Hayek, the price mechanism works to distribute knowledge\nthroughout an economy because it requires that individuals share knowledge in\norder to evaluate the price of a commodity, but an increasingly fluid and\nprivatized economy makes dissimulation the rule.[40](19_Notes.xhtml#ch8fnr40)\n\nThrough this privatization we see the creation of nearly insurmountable\nroadblocks to scientific and technological discovery outside the confines of\navailable data and the drive for profit.[41](19_Notes.xhtml#ch8fnr41) We need\nonly to look at the litany of inaccurate algorithmic systems; the replication\ncrises in the sciences; the scandals over user data and political\nadvertisements that seem to elicit only muted apologies from companies like\nFacebook; or Kobe Steel’s falsification of quality control data for their\nmetals for over a decade, which potentially compromised the structural\nintegrity of cars, airplanes and trains.[42](19_Notes.xhtml#ch8fnr42)\n\nLike financial crises and underemployment, these duplicities are not\naberrations from an otherwise well-functioning economic system. Rather, they\nrepresent what can be called a “capitalist calculation\nproblem.”[43](19_Notes.xhtml#ch8fnr43) Probabilistic knowledge requires that\nas much data as possible be arranged and calculated in a single place. But\nunder the reign of probability, markets discourage the dissipation of data and\ninstead horde as much as possible. Without some other sort of system, some\nother mode of calculation—in essence some other means of exchange than\ncapitalism—probabilistic knowledge production will continue to favor cheating\nand dissimulation. As we saw in Chapter 4 with Jonah and his shipmates, lots\ntell us nothing about an event without the epistemic support of a larger\ntheological and cleromantic system. But under capitalism, this support system\nincreasingly turns against the production of _usable_ knowledge in favor of\n_valuable_ knowledge, which more often than not means the production of\nsocially useful knowledge is replaced with an arms race that aims at the\nimpoverishment of the knowledge of one’s interlocutors. In the final analysis,\nwhat is objectified through statistics and machine learning is precisely and\nexclusively the demands of the market. These systems serve only to re-present\nboth the concrete and abstract domination of contemporary capitalism in\nobjectified form.\n\nThe enclosure of the general intellect forecloses the possibility of building\ncollective knowledge. Within a firm’s proprietary enclosure, statistics is\nonly able to produce abstractions according to available, local, and parceled-\noff data and knowledge. In doing so, machine learning ultimately serves only\nthe purpose of what is sometimes called “local optimization”—of finding a\nsolution that is best for a given part of a problem. This is not to say there\nis no importance to local, situational and specific knowledge—in fact this is\nthe only meaningful place knowledge can come from—but as long as the\nspecificity of the local is determined exclusively by profit, it can only be a\nreflection of global systems of expropriation and exploitation.\n\nNaturality and Ontology\n\nArtificial intelligence is still only able to solve the problems that are\ngiven to it.[44](19_Notes.xhtml#ch8fnr44) While computers can process\nincreasingly large datasets, these datasets contain only a fraction of all the\nattributes that could affect the possible outcomes. Moreover, even in the most\nunsupervised of machine learning problems, an engineer must on some level\ndetermine what a problem is and what a successful solution to such a problem\nmay look like. The freedom in abstraction is a freedom conditioned by this\ndefinition. To abstract is to provide distance from a certain, specific\nsubjective position, all while basing the entirety of that abstraction within\nthe knowledge afforded to that position.\n\nAnd as proprietary, opaque systems continue their ascent toward a persistent,\nreal-time production of abstractions, the force of those abstractions’\ninfluence finds little effective response in any political theory that\nrequires stable terrain. The very concepts through which the world is now\ndefined are no longer fixed, and thus they have become even more inaccessible.\nWhen Google CEO Sundar Pichai testified before Congress in 2018, conservative\nlawmakers repeatedly accused the search engine of returning liberally biased\nresults. Pichai was largely able to evade these accusations by explaining\nmethodological issues rather than defending specific\nresults.[45](19_Notes.xhtml#ch8fnr45) Indeed, the force and authority of these\ncalculations cease to be localizable to the company or its employees,\nappearing instead to come from some method that must be followed. While old\nforms of abstraction alienated all but an elite aristocracy from the world,\nnew computational ones alienate all humanity from thought itself. One cannot\nknow, and thus politically act, if the subject has an inadequate understanding\nof both the terrain on which they stand and the nonlinear consequences of\ntheir actions.[46](19_Notes.xhtml#ch8fnr46) While politics has always been\nconfronted with nonlinear dynamics, the proliferation of systems that attempt\nto automatically modulate these dynamics foreclose any hope of a politics\nfounded on a subject that would simply choose differently.\n\nAs thought and knowledge become increasingly alien and unintelligible in any\ntraditional and immediate sense, any recourse to a politics that aspires to an\nunalienated, “natural” subject—or even centers a subject conditioned only by\nthe objective, and observable, stakes of the world—will eventually fail, for\nit will inevitably be unable to account for the material and political force\nthese algorithmic and statistical systems create. This is precisely because\nsuch a subject position presupposes a transcendental perspective that opaque\nmachine learning algorithms do not afford. That is to say, this subject would\nneed to be endowed with a capacity to stand outside or above the situation—an\nimpossible omniscience and transcendence—in order to know how these\ntechnologies mediate social relations through the production of abstractions.\nMoreover, as philosopher and cultural theorist Sylvia Wynter has demonstrated,\nthe very concept of the human ties together its earlier theological and\ntranscendental force with a naturality that never overthrows the hierarchized\nunderstanding of existence but instead encodes it into a notion of humanity\nthat always excludes those who are not male, white and so on. It is only by\nabandoning this commitment to the natural and the true that these systems of\nvaluation can be overturned.[47](19_Notes.xhtml#ch8fnr47)\n\nThe subject, then, must be thought not as some veritable, emanating truth.\nOne’s subjectivity is possessed wholesale by the universality of the system,\nand vice versa; in short, non-locatable objectivity twists into subjective\nparticularity. In this trapping, one can believe—or identify—however one\nwants, but one must necessarily follow the laws of probability that have been\nprogrammed into the system. The laws of probability, like the market upon\nwhich they are founded, function with or without the belief of those who bring\ntheir knowledge to market. So, the subject is free, but only in so much as\nthey can follow the law of the system governed by what is made to count, which\nunder capitalism is always a question of what counts at market. The particular\nis thus alienated in the universal, and the universal is simultaneously\nalienated in the particular.\n\nWhen belief, experience and subjectivity in general are conditioned by the\nconstraints of exchange, a property owner’s declaration that it is not they\nwho are racist, but rather the market, reflects precisely the logic by which\nthe system of racial capitalism is made to\nfunction.[48](19_Notes.xhtml#ch8fnr48) Of course such claims are racist, but\nthe subject who makes them believes they come from somewhere else, namely the\nobjectivity of the market. As structural white supremacy is objectified into\nthe statistical matrix of one’s property value, life gets trapped within the\nabstractions of calculating process—an epistemic paralysis that resists\ncritique. Thus, we witness the continuation of racist housing policies in\nalgorithms’ preferential treatment of white mortgage applications since the\ndataset upon which such calculations are based incorporates a history of\nracism in housing and lending.[49](19_Notes.xhtml#ch8fnr49)\n\nThe particulars of the subject’s lived, subjective experience become wrapped\nup with the structures of these computationally produced abstractions of\nneoliberal capitalism. In metaphysical terms, statistics have become the\nformal science—now governed by the market itself, thanks to the Bayesian\nrevolution—by which the relation of the particular gets objectively connected\nto the universal. The historical conflict between one’s life and the\nstructures around that life have not been reconciled, but merely reframed into\nmobile, fluid and market-based relations. Some are granted more or less\nfreedom to abstract, but still these abstractions are mediated by the demands\nof capitalism.\n\nIn order to break this metaphysical stranglehold, we cannot seek a return to\nsome natural, pretechnological, prestatistical, precapitalist form. Nor can we\ntake seriously the resolute position of the liberal subject—of one who can\nknow, and functionally audit, the complexity of these algorithmic systems so\nas to solve these problems through reform alone. Instead, we must transform\nthe necessity of what is already conventional and structurally fluent within\nthe statistically ordained world: abstraction in all its alienatory power.\n\nSo, the act of abstraction itself—and even and especially its mobilization\nagainst some fixed transcendent reference point—does not necessarily have to\nbe a force for exploitation. While these abstractions necessarily elide some\nattributes of the world they model, they are still potentially\nproductive.[50](19_Notes.xhtml#ch8fnr50) But if they are to be socially\nproductive, they—along with the technologies and techniques that produce\nthem—must be disaggregated from their capitalist ruin. While the steps that\nmust be taken are far from clear, collectively we must turn to the work of a\nrevolutionary mathematics to value different mysteries, different relations\nbetween universals and particulars, and new forms of abstraction that are\nfreed from the production of privatized knowledge and value. The old mysteries\nof God, the state and now even capital lay in disrepair and are incapable of\nsupporting the production of knowledge and our collective existence. This is\nnot to say that they were ever able to produce, for large portions of\nhumanity, much more than ruins; but now even their utopian, scientifically\ndriven metaphysics are directly reduced to ruin.\n\nIn sum, we must theoretically and practically engage the production of\nknowledge, imagining different forms and different computations. But let us be\nclear: this does not and cannot mean a simple fetishism of the future for its\nown sake. We cannot be so hopeful as to assume that the force of these\ncontradictions promises any respite. At best, they provide an opening, an\nopportunity to reconfigure the very force of objectification and the\nproduction of knowledge and value.\n\nAs the ability to record and process data continues to increase exponentially,\nit will be on the level of the general intellect that we may be able to take\nadvantage of these technologies. In short, only through their communal,\nsocialized usage will we be able to turn data into meaningful and productive\nknowledge. Today, it is capitalism that suffers from a calculation problem,\nand only some other form of exchange and abstraction will be able to calculate\na more meaningfully rational and just usage of goods and labor.\n\nAlienation\n\nTo fully accept the productive weight of twenty-first-century science and\ndigital technologies requires a reconsideration of the Marxist theory of\nalienation.[51](19_Notes.xhtml#ch8fnr51) Marx prominently argues that the\nprocess of capitalist production is a process of the alienation of labor.\nArtisanal workers and early farmers produced the very goods they either\nconsumed or whose sale they controlled. While for many workers (especially the\nenslaved and indentured) precapitalist production alienated labor, capitalism\ngeneralized this alienation in the form of wage labor. And, under later\nindustrial production, the means of production became increasingly held in the\nhands of the capitalist class. Moreover, as machinery became increasingly\nautomatic, it further alienated the worker from both their own labor process\nand the products of this labor. As Marx writes in his “Fragment on Machines,”\n“The science which compels the inanimate limbs of the machinery, by their\nconstruction, to act purposefully, as an automaton, does not exist in the\nworker’s consciousness, but rather acts upon him through the machine as an\nalien power, as the power of the machine itself.”[52](19_Notes.xhtml#ch8fnr52)\nIn this way, labor power is “alienated” from the laborers who supply it. They\nno longer have a direct stake in the product or processes of their work; they\ntrade their labor for wages, and what they make is directly alienated from\nthem while their working time is increasingly controlled by machines.\n\nMarxist thinking has made much of this theory of alienation. For example,\nautonomist Marxism stressed the importance of alienation in understanding\nsocial labor and vectors of capitalist exploitation outside the workplace: as\ncapital’s reach extends beyond the factory, the target for capital power turns\nto the entire social edifice—the soul included. Under post-Fordist capitalism,\nFranco “Bifo” Berardi writes, “No desire, no vitality seems to exist anymore\noutside the economic enterprise, outside productive labour and\nbusiness.”[53](19_Notes.xhtml#ch8fnr53) Alienation in this case is not just\nthe separation of production from the working class, but the alienation of\naffective production of nearly all life toward capitalist labor and\nconsumption. In essence, we become alienated not just in the process of\nproduction, but also in our consumption of what advertising sells us and our\ndesire for the life that capitalism presents to us as the image of success.\n\nTo resist capitalism solely on account of its alienating power is a mode of\nresistance that appears beleaguered from the outset, promising only to return\nus to an unalienated experience of capitalism, rather than an outside—the\nfreedom to be ourselves, but only by buying the proper experiences. In many\nways, those who still try to ground resistance to capitalism in opposition to\nalienation interpret alienation directly through an analysis of the commodity\nform, hoping for some meaningful and artisanal return to direct production.\nAccording to a more traditional Marxism (as well as a whole host of fellow\ntravelers who desire to return to local production), in order to reclaim labor\nas a fruitful and human-enriching form of social activity, the working class\nwould simply have to repossess the output of\nproduction.[54](19_Notes.xhtml#ch8fnr54)\n\nIt is imperative that alienation is not understood as a unitary phenomenon.\n“Alienation” as a category has been taken to describe a whole host of losses,\nfrom the sometimes-well-paid alienation of professional software engineers, to\nwhat scholar of African American literature and history Saidiya Hartman refers\nto as “natal alienation” caused by the enslavement of Black\npeople.[55](19_Notes.xhtml#ch8fnr55) Just how much alienation takes away\nvaries; though it tends to take significantly more, including life, from those\nwho are most marginalized. Moreover, attempts to overcome alienation within\ncapitalism have a way of doing so only for the already well-off, often white\nmen who possess significant capital. For the corporate elite, overcoming\nalienation can mean significant flexibility, while for the wage laborer, it\noftentimes means working in unpredictable fits and starts according to the\ndemands of management. In many ways, the fight against alienation has been\nappropriated by capitalism, as its superintendents give workers more\nflexibility and control over their labor, facilely claiming to ease the\npressures of schedules by increasing precarity rather than proffering\nmeaningful autonomy.[56](19_Notes.xhtml#ch8fnr56) In short, capitalism’s reply\nto workers’ complaints of alienation has been to give them the freedom to\ndrive an Uber on their own “unalienated” time. Thus, rather than resist\nalienation tout court, it is necessary to resist _specific forms_ of\nalienation, with an attention to how, and whom, they take from, and for what\noutcome. In short, the aim is to neither fetishize alienation nor the imagined\nsovereignty of a life that could escape all alienation.\n\nIn this spirit, readings like that of Moishe Postone, which place at the\nforefront of their analysis Marx’s attempts to establish the abstract\ncharacter of capitalist society as the source of its particular form of\ndomination, appear more productive and\ninsightful.[57](19_Notes.xhtml#ch8fnr57) For Postone, capitalism is\ncharacterized by an interdependent set of social relationships that make labor\na “structural imperative,” a necessary precondition for survival: “No one\nconsumes what one produces, but one’s own labor or labor products,\nnevertheless, function as the necessary means of obtaining the products of\nothers.”[58](19_Notes.xhtml#ch8fnr58) Under capitalism, our objective social\nrelations demand the necessity of human labor for survival—a condition that is\nirreducible to the exploitation of laborers by a ruling class. Thus, the\nreappropriation of one’s alienated labor can only result in a superficially\nimproved relationship to work, not the abolition of capitalism’s necessity for\nexploiting labor.[59](19_Notes.xhtml#ch8fnr59) In short, as long as capital\ndemands the accumulation of wealth among a few, it will produce new\ncompulsions to work and new forms of exploitative alienation that will\ninevitably fall hardest upon the most marginalized.\n\nThus, even were we to somehow imagine the replacement of all automatic and\nalgorithmic systems for the production of goods and knowledge with humans who\nfully understand the production process, this would do little to remove the\nexploitative conditions of this work and the objectified forces that translate\nracism, sexism and imperialism into the abstract demands of capital.\n\nTo abolish capitalism, what is needed is, rather, a full embrace of the forces\nof alienation, especially as they are created through machines that automate\nand extend humanity’s ability to produce and think. In short, freedom from the\nnecessity of labor requires a decoupling of machine’s productive and\nalienating force from capitalist accumulation and valuation. The problem\nhumanity faces is not that the products of physical and intellectual labor are\nwrested away from the factory worker or the scientist, but that their\nproduction is aligned with the necessities of a market founded on capital\naccumulation by the few. Rather than attempt to construct an impossible,\nunalienated form of capitalism, we must free alienation from capitalism.\nAgain, this does not mean that technology is itself a panacea—for its infernal\nreproduction promises nothing—but rather that alienation, our very being\noutside of ourselves in the world, is what must be dissociated from capitalism\nand repurposed.\n\nIn a now-famous segment from his “Fragment on Machines,” Marx describes how\nautomation leads to further alienation as not only products but also knowledge\nitself and the intellectual capacities of laborers are concretized into\nmachines. Marx states, “In machinery, knowledge appears as alien, external to\n[the laborer] … and living labour [as] subsumed under self-activating\nobjectified labour. The worker appears as superfluous to the extent that his\naction is not determined by [capital’s]\nrequirements.”[60](19_Notes.xhtml#ch8fnr60) Automation renders laborers\nsuperfluous as machines become increasingly responsible for production.\nHowever, because the abstract dimension of human labor remains the basis of\ncapitalist exchange value and consequently the form of wealth peculiar to\ncapitalist society, capitalism continues to demand the expenditure of human\nlabor in increasingly pointless, and thus even further alienated,\nwork.[61](19_Notes.xhtml#ch8fnr61)\n\nUnder the privatization and enclosure of the general intellect, the social\nbecomes reformed, erected by the private abstractions of machine learning\nalgorithms and emerging statistical models, wherein the objects that produce\nthe social world—and thus us—are wholly lodged within the capitalist edifice\nand its alliance with a long history of expropriation and exploitation. Under\nsuch a regime, the alienating power of these abstractions can only be in the\nservice of capital accumulation. We work to survive, and the fruits of our\nlabor are given over to capital.\n\nWhile “alienation” names servitude to various forms of waged and unwaged labor\ntoday, its abolition as a category is neither possible nor a guarantee of an\nend to capitalist exploitation. Instead of combatting alienation through a\ndesire for some external-tocapitalism natural world where there exist\nconditions for, in Marx’s terms, “the full and free development of the\nindividual,” we must instead liberate alienation from exploitation.\nAbstractions—whether the thermodynamic abstraction of productive force into\nthe machine or the abstraction of intellect into computers—are not, by\ndefault, oppressive. The collapsing of abstraction into an always-negative\nalienation effaces the key epistemic conditions that facilitate the\npossibilities of radical political change.\n\nRevolutionary Mathematics\n\nUltimately, the metaphysical, intellectual and political work of a\nrevolutionary mathematics calls for a reconsideration of the relationship\nbetween alienation and liberation. This does not mean that these technologies\nare necessarily liberatory, nor that more technology offers any promise, but\nrather that the underlying objectifications of the entire trajectory of\ntechnology must be contested. The task, then, is to repurpose both the\nmetaphysical ground and the actual capacities of these alienating forms of\ntechnology. Such work must seek not to end all production or the alienating\nforce of abstraction, but to engage it oppositionally—to dislocate it from its\nseeming stability as given or monolithic. In short, we must imagine and\nconstruct new mysteries and new modalities of exchange that can enable\ncomputation and calculation outside and beyond capitalism. Accordingly, to\nengage in a revolutionary mathematics is to disavow any belief that we can get\nunder our abstractions, to see “what is really there” and regulate away their\nproblems. Instead we must reckon with this world of abstraction and\nalienation.[62](19_Notes.xhtml#ch8fnr62)\n\nThis requires us to create and build the world from different mysteries and\ndifferent exchangabilities: ones that deny the power and reality of\nimperialism, and even the most basic form of capitalist exchange, by which one\ntrades their work for the capacity to continue living. We require different\nmeans of counting and valuing. A revolutionary mathematics aims not to recoup\nor destroy those old forms through critique, but to create different\nabstractions—even new “natures” and the valorization of natures outside the\nEnlightenment notion of individual sovereignty—and with them new alienations\narising from new, mysterious metaphysics of exchange that offer, just like the\ncommodity and the machine, to think for us, both economically and\ncomputationally (which today amounts to the same thing). To do this does not\nmean to simply disregard old forms, for it is only by tracing their histories\nand implications—as the present text has attempted to do—that it may be\npossible to build new mysteries that are not merely repetitions of the\npresent.\n\nThe Bayesian discovery that probabilistic knowledge depends on markets and\nexchange appears initially to naturalize capitalism. In the final analysis,\nhowever, the result is the exact opposite: Bayesian statistics denaturalizes\nnature and calls on any who value knowledge, or even the possibility of\nknowing, to reconceive the very notion of exchange and its metaphysical\nground. A revolutionary mathematics must oppose the naturalism of the\nunalienated, epistemic subject, and the fantasy of the liberal subject who can\nefficiently operate the material and economic world within which they find\nthemselves; in short, such a possibility opposes both the naive chauvinism of\nRonald Fisher and the attempts to naturalize economism of Leonard Savage and\nBruno de Finetti.\n\nA new revolutionary object, or objectification, that would break the\nstranglehold of the commodity on our thinking must also abandon the fetishism\nof the natural, the unalienated and the dereified. At the same time, it must\nforswear the possibility of any guaranteed future. We cannot return to some\nbefore or some future where we finally grasp the totality of our situation. A\nrevolutionary mathematics must commit to the alien and the mediated, but this\ndeclaration means something very specific within the context of mathematics\nand machine learning: namely, that we must reject the Fisherian residue that\noffers us a fetishistic version of knowledge production by claiming that\nknowledge belongs to the individual and that it is immune from the material\nand economic conditions of its production. In sum, this means we must\ncollectively reimagine the production and meaning of scientific and technical\nknowledge.\n\nThe Bayesian revolution has taken recourse to the exchange of contracts in\norder to provide a foundation for probabilistic knowledge, but in making\nknowledge second to exchange, it fundamentally denaturalizes and subjectivizes\nknowledge, abandoning “objective” theories of probability. This revolution\ndoes away with claims that knowledge directly represents the physical world as\nit is, replacing it with a representation of the world _as it is profitable_\n—in short, knowledge now finds its future in the subjective and the social. In\ndoing so, it requires knowledge to admit its fundamentally material and\neconomic ground and permits a demonstration of the fact that the very systems\nof exchange from which it arises can no longer support its function. For\nscientific knowledge to have a future beyond its current capitalist crisis, we\nmust accept knowledge’s abstract and alien power.\n\nAdditionally, the work of a revolutionary mathematics and its attempts to\ncreate new objectifications is a politics that leaves the subject alone, for\nit has little need for the idea of a fixed and solid subject. This politics\noperates against the presumption that solidarities can be either forcefully\nmanufactured from above—for instance, via the enforced class dynamics of\nLeninism, which tries to tell the revolutionary subject how to be—or\norganically produced from below—via some magical event that would invigorate\nan organic, collective solidarity. This politics detaches from the desire to\nmake subjects and instead focuses on the metaphysical technics of producing\nobjectifications and, hence, on what is objectively the case. For example,\nthose who think of big data in terms of privacy are half right: big data’s\ndesire to make our subjective relationships computable is one that should be\nrejected. But to stop there is to lose a losing battle. It is to play with a\ncoin that is wholly biased toward capital. Instead, we must fundamentally\nreconceptualize what we are computing and why. While such calls for privacy\nare clearly important, to simply show that we are not being valued equally or\nthat we are being exploited will never be enough to change our condition.\n\nIt would be ideal to be able to offer examples, to point to specific ideas or\nindividuals who are currently advancing this work. But such a revolution in\nthought can only be fully ascertained and traced after the fact. What may\nappear revolutionary today could, in the hindsight of tomorrow, appear fully\nreactionary. Yesterday’s provocative and revolutionary attempts to stand\noutside of digital capitalism appear today as withdrawal and retreat. Still,\nthe act of tracing the grounds upon which our current epistemology and\nmetaphysics of exchange stand—while refusing to think that we will ever\ndiscover some unalienated truth of how things “really are”—can reveal unknown\ncontradictions and new demands. It is out of these demands for a different\nworld that the work of revolutionary mathematics progresses. It is far from\nclear what these new or different metaphysics could be, but the task ahead is\nto seek them out.\n\nIf knowledge production and its metaphysical ground are founded upon the\nhistorical and material conditions of exchange, to politically engage these\nprocesses requires that we intervene in the process of exchange itself. But\nlet us be clear: this does not mean that politics should be aimed exclusively\nat capitalism or class conflict (in the traditional Marxist sense). To the\ncontrary, every inequality we face today—sexism, xenophobia, racism,\ntransphobia, ableism—constitutes a mobile and multifaceted system that defines\nthe varied axes of exchange, oppression and\ninjustice.[63](19_Notes.xhtml#ch8fnr63) All of these socially constructed axes\nof oppression allow computation and exchange to function—providing various\nforms of exploited labor, such as the outsourcing of the psychic trauma of\ncontent moderation—and replicate themselves in the computed outcomes of\nvarious technologies of machine learning and\nstatistics.[64](19_Notes.xhtml#ch8fnr64) Discrimination, oppression and\ninjustice are not reducible to exchange but rather are operationalized and\nexploited through exchange, replicating their violence in the knowledges that\ncomputation produces. The task is to directly confront these injustices\nwherever they may arise—but without claims to a return to an unalienated or\nnatural world—and in doing so to trace the various metaphysics, forms of\nknowledge production and material economies that make them thinkable and\ncomputable today and, perhaps, make a future beyond them imaginable.\n\nThe work of the revolutionary mathematician is, in the end, less mathematical\nthan it is metaphysical. It is not a simple decision or discovery that a\nsovereign liberal subject could conjure from nowhere. While we are far from\nknowing exactly where this work leads or exactly what form it must take, the\ntask is to strategically create new mysteries and resist injustices as\nconstitutive parts of this amalgamation of capitalist computation. Like those\nearly founders of calculus who created a mathematics that worked without fully\nunderstanding its foundations, the work ahead is to seek out new and different\nequivalences, values and computabilities, all the while understanding that\neach of these is fundamentally social, abstract, alienatory and unnatural. The\npath backward toward some natural redemption closed long ago, if it was ever\nopen. A revolutionary mathematics requires that we create new objectifications\nthat call upon us to build a different future, whether we believe in them or\nnot. In the face of a global capitalist system committed to accumulation at\nthe cost of exploitation and the destruction of the ecosystem, nothing could\nbe more difficult. But at the same time, nothing could be more necessary.\n\n\n[Conclusion: Toward a  \nRevolutionary Mathematics](04_Contents.xhtml#ch14)\n\nRight-wing billionaire Robert Mercer’s career is intimately tied to everything\nthat is at stake in the Bayesian revolution. In 1972, after completing his PhD\nin computer science, he joined IBM’s research division. There, he helped make\nmajor advances in speech recognition and machine translation. Few on Mercer’s\nteam had much of a linguistics background, but their efforts to apply\nprobabilistic models to speech nevertheless succeeded, and now these form the\nbasis of many of our contemporary algorithms for working with human language.\nNotably, Mercer’s team developed a Bayesian-informed algorithm for determining\nthe probability that a given string of words will be observed—even if it has\nnever been observed before.[1](19_Notes.xhtml#chxfnr1) For this work, Mercer\nwas awarded a lifetime achievement award from the Association of Computational\nLinguistics in 2014.[2](19_Notes.xhtml#chxfnr2)\n\nIn 1993, Mercer was lured away from this research to join Renaissance\nTechnologies, a hedge fund that employed statistical and machine learning\ntechniques to direct their investments. Mercer brought his Bayesian methods\nfrom IBM with him, developing investing algorithms that helped the fund\ngenerate returns averaging 39 percent between 1989 and\n2006.[3](19_Notes.xhtml#chxfnr3) In 2009, he became co-CEO of the firm, a\nposition he held until his retirement in 2017. By using similar probabilistic\ntechniques to those he developed at IBM, Mercer was able to amass billions of\ndollars in private wealth, both for his clients at Renaissance Technologies\nand for himself.\n\nBut Robert Mercer’s story does not end with this capital accumulation. Since\nthe early 2000s, Mercer—both individually and through his family’s\nfoundation—has donated tens of millions of dollars to conservative\ncauses.[4](19_Notes.xhtml#chxfnr4) He has funded the right-wing Breitbart News\nNetwork, groups opposing the so-called “Ground Zero Mosque” in lower\nManhattan, and climate change denial think tanks like the oddly named Berkeley\nEarth group.[5](19_Notes.xhtml#chxfnr5) He also provided the initial funds to\nstart the now-infamous Cambridge Analytica consulting firm, which obtained\ndata from Facebook in order to target political ads supporting Donald Trump’s\n2016 election campaign.[6](19_Notes.xhtml#chxfnr6)\n\nWe witness, here in miniature, how capitalist privatization is able to turn\nthe promise of science and the development of the general intellect against\nitself. Mercer has pioneered some of the very methods that have driven late\ntwentieth- and early twenty-first-century science and knowledge production.\nYet the fruits of those methods were extracted in the form of private wealth,\nand then turned against science in favor of a populist and racist nationalism.\nAgain and again, the motives of capital accumulation and monopolization reward\nthe simultaneous privatization and privation of general knowledge.\n\nIn the final analysis, our current array of deep epistemological crises—from\nthe replication crisis in the sciences, to concerns about fake news, to the\nexistence of defeat devices, to filter bubbles that insulate individuals from\ncontrasting views online—are at their heart crises of capitalism. As\nneoliberal capitalism ceases to focus on the management of production and\ninstead turns to the management and control of knowledge, it risks a whole new\nseries of crises. As Paulo Virno writes, “The models of social knowledge do\nnot turn varied labouring activities into equivalents; rather, they present\nthemselves as ‘immediately productive force.’ They are not units of measure;\nthey constitute the immeasurable presupposition of heterogeneous effective\npossibilities.”[7](19_Notes.xhtml#chxfnr7) But in making these knowledges\ndirectly productive, capitalist knowledge production sweeps the ground out\nfrom under its own feet. While capitalists believe its mysteries—their unequal\nequalities—are directly knowable, they privatize and deprive these knowledges\nof their collective ground.\n\nOn a metaphysical level, statistics and economy share the same goal: to relate\nparticulars to universals. Statistics aims to deduce general laws from\nindividual pieces of data: it proceeds, for example, from individual\ndemographic data to larger social trends. Economic systems do the inverse:\nthey produce individual acts of exchange from general principles, increasing\nproduction here and laying off workers there as markets fluctuate. But machine\nlearning—especially given that it advances, in the words of former _Wired_\neditor Chris Anderson, “without coherent models, unified theories, or really\nany mechanistic explanation at all”—seeks only to relate the particular to the\nparticular, or what is the same: to make every particular universal, sending\nan advertisement or service to a user at precisely the moment they require\nit.[8](19_Notes.xhtml#chxfnr8)\n\nIn this supposed theory-less world, science is only able to relate data to its\nmost proximate step: what should be done next. As Leonard Savage argued, the\nkey question in science ceases to be what to say, and instead becomes what to\ndo. The only possible, and desirable, knowledge becomes that of how to act in\nthe immediate moment, while larger questions of political economy, and the\npossibility that things could be different from what they are, are ignored.\nThis, then, is the mathematics of capitalist orthodoxy. Here we see how the\nmore machine learning succeeds in its predictive power, the more it fails in\nits ability to efficiently know the world or even to distribute capital, for\nit sweeps away the ground upon which capitalism claims to be able to manage\nproduction. The science behind machine learning creates massive incentives not\nto solve problems or develop the general intellect, but rather to game the\nsystem and enclose knowledge.\n\nThis failure of knowledge production is, then, a failure to account for the\npolitical economy that lies both in the metaphysical core and the practical\nuses of machine learning and statistics. Knowledge produced from the exchange\nof contracts ends in the exchange of contracts, with the only meaningful goal\nbeing the avoidance of the Dutch book. One is left, as Marx says, deciphering\nthe very hieroglyphics the market has left there. But this subjectivization of\nknowledge does not make statistics useless. Rather, these methods offer to\nproduce abstractions that are collectively productive, providing for the world\nrather than exploiting it. But to do so, they must be freed from the\nnecessities of capital accumulation and its constant drive to profit from\nracism, sexism and imperialism.\n\nAs machine learning and statistics cut away their own roots, disavowing the\nFisherian belief in the heroic individual scientist, they threaten to shake\nthe mysteries of Enlightenment scientific knowledge production to their core.\nWhere there were once solid equivalences between labor and knowledge, these\ndirectly productive probabilities once again set everything in motion. As\nVirno writes,\n\n> The principle of equivalence used to be the foundation of the most rigid\n> hierarchies and ferocious inequalities, yet it ensured a sort of visibility\n> for the social nexus as well as a simulacrum of universality. This meant\n> that, albeit in an ideological and contradictory manner, the prospect of\n> unconstrained mutual recognition, the ideal of egalitarian communication and\n> sundry “theories of justice” all clung to it.[9](19_Notes.xhtml#chxfnr9)\n\nVirno continues by arguing that the collapse of this principle of equivalence\nis now the cause for cynicism: “The cynic recognises the primary role of\ncertain epistemic models, as well as the absence of real equivalences. He sets\naside any aspiration to transparent and dialogical communication. From the\noutset, he relinquishes the search for an intersubjective foundation to his\npraxis or a shared criterion of moral judgement.”[10](19_Notes.xhtml#chxfnr10)\n\nThis is precisely the dual danger and opportunity of machine learning. In\nmaking knowledge directly productive, machine learning undermines the\nfoundation of this principle of equivalence. Virno’s definition of the cynic\naptly describes Savage’s position (and Alfred Sohn-Rethel’s critical\napperception of this fact): statistics only works in the context of exchange,\nand the more we pursue real equivalents—such as the objectivity of the\nfrequentists—the faster they evaporate. With no stable ground, these ageold\nmetaphysical concepts’ very difference is computed anew at every moment.\n\nThe rationality that these methods and technologies embody demonstrates a\ncodependence that is unable to separate the objective and the subjective,\nbecause each collapses into the other. Given that the very ideal of\nobjectivity can only be sustained by one’s subjective belief in that\nideal—like the ideal coin of frequentism and capitalism—the whole system of\nknowledge production comes to insist on the centrality of exchange and, with\nit, social relations. In this way, the Bayesian revolution has made the fluid\nand mobile economics of a neoliberal hyper-capitalism central to the\nproduction of objective knowledge. The social relations of exchange that\nappear to many commenters as a corruptive influence on statistics and the\nproduction of knowledge—the contamination of “pure science” by the soiled\ntouch of human influence—lie at its metaphysical heart.\n\nWe are witnessing, then, a new era, a new version of what Marx means when he\nwrites that “all that is solid melts into air.” Or as he says in the\n“Fragment”:\n\n> Everything that has a fixed form, such as the product etc., appears as\n> merely a moment, a vanishing moment, in this movement. The direct production\n> process itself here appears only as a moment. The conditions and\n> objectifications of the process are themselves equally moments of it, and\n> its only subjects are the individuals, but individuals in mutual\n> relationships, which they equally reproduce and produce\n> anew.[11](19_Notes.xhtml#chxfnr11)\n\nIt is for this reason we must, in a way, side with Leonard Savage and Bruno de\nFinetti, and even to an extant with Jerzy Neyman and Egon Pearson, but in\norder to take their discoveries further: while they appear as quintessential\ncapitalists, their insights allow us to reject Ronald Fisher’s fetishism of\nknowledge as a product that can be owned by the lone scientist toiling in the\nlab. Through their emphasis on exchange, we are able to see the economically\ndriven processes that create these knowledges and the modes of production they\nsupport. These latter statistics reveal the extent to which knowledge depends\non political economy and the necessity of working through crises of economy if\nwe are to address crises of knowledge production.\n\nThis turn to process offers no promise that we will be free of objectification\nor its alienating powers, or from sadistic forms of concrete domination.\nRather, we witness these new mobile and temporary assemblages of\nobjectification pulling us in all directions, as they did Mercer, who produced\nan anti-rationality that is the exact opposite of the rationality he had\ncommodified only yesterday. But, in the face of these destabilizations, we are\nable to steal a glimpse, as Marx shows us, at the process rather than simply\nthe product.[12](19_Notes.xhtml#chxfnr12) It is through this recognition of\nprocess, in all its social implications, that a new objectification is\npossible.\n\nAt the same time, we must recognize that the border that separates the\nparticular and the universal is at stake. These technologies, and thus\nobjectification writ large, are becoming the very forces that shape and define\nthe universal—a universal that excludes particular subjects and individuals.\nThe technologies and methods of machine learning, data analysis and statistics\noffer us a means by which we could wholly reconfigure the relation between\nindividual and universal in modes that far outpace the violence and\ndestruction of capitalism. To do so requires that we think through and\nreconfigure the very forms of objectification through which they work and\ndisavow the fantasy of complete understanding or a return to some prior more\nnatural state. Likewise, they require that we disavow the promise of any\nfuture guarantee and the fetishism of the new for its own sake—both\nteleological principles that all too often repeat the dream of a male\nEurocentric universalism.\n\nAs neoliberal capitalism seeks out more and more local contexts from which to\nextract value and knowledge, it invests in the very dissimulation that\nundermines its claims to efficiency and objectivity. Machine learning and\nstatistics are now in the process of creating a new set of algorithmic objects\nthat are unseating the commodity’s economic centrality, as value appears to\nderive directly from automatic computation. Even as they build “real\nabstractions” on top of the current metaphysics of exchange, these algorithmic\nobjects demonstrate themselves to be anathema to their enclosure and\nprivatization—for their enclosure under conditions of capitalism guarantees\nthat they will continue to become technologies of deception rather than\nproduction, unlikely to be constrained by calls for regulation. Even if, at\nthe moment, another possibility sounds only as a faint whisper, these\ntechnologies—in accounting for our affairs—call out for collectivization: for\nshared ownership of both the means and metaphysics of production.\n\nTo seize the means of production is both a political and metaphysical task.\nProduction, whether industrial or statistical, produces both objects and\nobjectification. If we seize only the means of producing the former, we will,\nin the end, only reproduce the logic we hope to escape. Although its exact\noutline is still far from clear, this requires that we trace and understand\nthe current metaphysics of capitalism—not in order to dereify them, but rather\nto understand how they can be replaced in order to valorize new and different\nforms of knowledge and value.\n\nTo follow this path is to become revolutionary mathematicians—to work on the\nlevel of metaphysics, creating new and different equalities based on new and\ndifferent mysteries. This aim cannot be confused with some futurism that would\nforsake the past; indeed, that would be impossible, for the tradition of all\ndead generations weighs like a nightmare on us, and we carry its weight along\nwith us. To ignore this weight would simply be to allow it to repeat itself.\nWe must instead attend to history—its forms, its contradictions and those\npeople and things about which it calculates. These chapters have sought to\nshow that it is possible to transform and to work on the mathematics and\nmetaphysics that constantly calculate this weight and its value, altering with\nit the very divide between the subjective and objective. For it is only then\nthat we can objectively turn our world of algorithmically mediated\nexploitation—of environmental destruction, abuse of workers, racism, sexism,\nheterosexism, ableism, xenophobia—into one that is more just and more equal,\nin a sense far beyond the limited equality of capitalist commodity exchange.\n\n\n[Notes](04_Contents.xhtml#ch15)\n\nIntroduction\n\n[1](06_Intro.xhtml#chifn1) For Althusser, the process of perceiving the\ncurrent situation is a process by which one is able to “act on History from\nwithin the sole history present,” working on “what is specific in the\ncontradiction and in the dialectic … not to demonstrate or explain the\n‘inevitable’ revolutions post festum, but to ‘make’ them in our unique\npresent, or, as Marx profoundly formulated it, to make the dialectic into a\nrevolutionary method, rather than the theory of the fait accompli.” Louis\nAlthusser, “On the Materialist Dialectic: On the Unevenness of Origins,” in\n_For Marx_ , trans. Ben Brewster (London and New York: Verso, 2005), 180.\n\n[2](06_Intro.xhtml#chifn2) Wendy Brown, “American Nightmare: Neoliberalism,\nNeoconservatism, and De-democratization,” _Political Theory_ 34, No. 6 (2006):\n690–714.\n\n[3](06_Intro.xhtml#chifn3) Naomi Klein, _The Shock Doctrine: The Rise of\nDisaster Capitalism_ (London: Macmillan, 2007).\n\n[4](06_Intro.xhtml#chifn4) Alfred Sohn-Rethel, _Intellectual and Manual\nLabour: A Critique of Epistemology_ (Atlantic Highlands, NJ: Humanities Press,\n1978), 2.\n\n[5](06_Intro.xhtml#chifn5) See, for example, United Nations Conference on\nTrade and Development, _Trade and Development Report 2017: Beyond Austerity;\nTowards a Global New Deal_ , 2017.\n\n[6](06_Intro.xhtml#chifn6) Robin Wigglesworth, “The Quickening Evolution of\nTrading-In Charts,” _Financial Times_ , April 11, 2017.\n\n[7](06_Intro.xhtml#chifn7) Andrew Gelman, “The Problems with P-values Are Not\nJust with P-values,” _American Statistician_ 70 (2016); Filip Pieniwski, “The\nAI Winter Is Well on Its Way,” _Venture Beat_ , June 4, 2018; John Harris,\n“Our Phones and Gadgets Are Now Endangering the Planet,” _Guardian_ , July 17,\n2018.\n\n[8](06_Intro.xhtml#chifn8) Jack Ewing, “Volkswagen Says 11 Million Cars\nWorldwide Are Affected in Diesel Deception,” _New York Times_ , September 22,\n2015.\n\n[9](06_Intro.xhtml#chifn9) James Grimmelmann, “The VW Scandal Is Just the\nBeginning,” _Mother Jones_ , September 24, 2015.\n\n[10](06_Intro.xhtml#chifn10) Sarah Ruiz-Grossman, “Volkswagen Executive Gets\nMax Sentence of 7 Years for Role in Emissions Scandal,” _Huffington Post_ ,\nDecember 6, 2017.\n\n[11](06_Intro.xhtml#chifn11) Mike Isaac, “How Uber Deceives the Authorities\nWorldwide,” _New York Times_ , March 3, 2017.\n\n[12](06_Intro.xhtml#chifn12) Joe Sullivan, “An Update on ‘Greyballing,’ ”\n_Uber Newsroom_ , March 9, 2017.\n\n[13](06_Intro.xhtml#chifn13) Along similar lines as this book, Scott Timcke\nhas recently argued that algorithms and the inequities they perpetuate must be\nunderstood not as a momentary epistemic or democratic crisis, but rather as an\nintensification and automation of capitalist forces that further forecloses\nthe possibility of politics to address these issues.\n\n[14](06_Intro.xhtml#chifn14) Louise Amoore, _The Politics of Possibility: Risk\nand Security beyond Probability_ (Durham, NC: Duke University Press, 2013);\nDan Bouk, _How Our Days Became Numbered: Risk and the Rise of the Statistical\nIndividual_ (Chicago: University of Chicago Press, 2015); Jacqueline\nWernimont, _Numbered Lives: Life and Death in Quantum Media_ (Cambridge, MA:\nMIT Press, 2019); Robin James, _The Sonic Episteme: Acoustic Resonance,\nNeoliberalism, and Biopolitics_ (Durham, NC: Duke University Press, 2019).\n\n[15](06_Intro.xhtml#chifn15) Theodora Dryer, “Algorithms under the Reign of\nProbability,” _IEEE Annals of the History of Computing_ 1 (2018): 93–96; Orit\nHalpern, _Beautiful Data: A History of Vision and Reason since 1945_ (Durham,\nNC: Duke University Press, 2015), 36.\n\n[16](06_Intro.xhtml#chifn16) Lorraine Daston provides an exceptional history\nof how interpretations of probability split into objective and subjective\ntheories: “How Probabilities Came to Be Objective and Subjective,” _Historia\nMathematica_ 21, No. 3 (1994): 330–344.\n\n[17](06_Intro.xhtml#chifn17) Ronald Fisher, _The Design of Experiments_ (New\nYork: Hafner Press, 1971), 1–2.\n\n[18](06_Intro.xhtml#chifn18) Wendy Chun argues that science, and especially\nmodels of future events such as global warming, “trouble the separation of\nscience from politics, model from evidence, but also, and more importantly,\nthe normal and normative relationship between understanding and agency”: “On\nHypo-real Models or Global Climate Change: A Challenge for the Humanities,”\n_Critical Inquiry_ 41, No. 3 (2015): 675–703. See also Alain Desrosières, “How\nReal Are Statistics? Four Possible Attitudes,” _Social Research_ (2001):\n339–355; and, in regard to scientific inference and statistics, Gerd\nGigerenzer and Julian N. Marewski, “Surrogate Science: The Idol of a Universal\nMethod for Scientific Inference,” _Journal of Management_ 41, No. 2 (2015):\n421–440. See also Lorraine Daston, “Fitting Numbers to the World: The Case of\nProbability Theory,” in _History and Philosophy of Modern Mathematics_ ,\nWilliam Aspray and Philip Kitcher, eds., _Minnesota Studies in the Philosophy\nof Science_ , Vol. 11 (Minneapolis: University of Minnesota Press, 1988),\n221–237.\n\n[19](06_Intro.xhtml#chifn19) Gerd Gigerenzer and David J. Murray, _Cognition\nas Intuitive Statistics_ (London: Psychology Press, 2015).\n\n[20](06_Intro.xhtml#chifn20) Sohn-Rethel, _Intellectual and Manual Labour_ ,\n20–21.\n\n[21](06_Intro.xhtml#chifn21) While machine learning techniques are not\nexclusively Bayesian in their approach, the Bayesian revolution both opened\nthe door for advances in machine learning and continually informs its\ndevelopments. Thus, Bayesianism can be seen as a spark that has led to a much-\nbroader revolution in how meaning is extracted from data and updated as new\nevidence is discovered. See Jon Williamson, “The Philosophy of Science and Its\nRelation to Machine Learning,” in _Scientific Data Mining and Knowledge\nDiscovery: Principles and Foundations_ , Mohamed M. Gaber, ed. (New York and\nBerlin: Springer, 2009), 77–89. There are a multitude of interpretations of\nBayesian statistics; our focus in this book is largely on the subjective\ninterpretation developed by de Finetti and Savage. See Bruno de Finetti,\n“Probabilism: A Critical Essay on the Theory of Probability and on the Value\nof Science,” _Erkenntnis_ 31 (1989): 169–223; and Leonard J. Savage, _The\nFoundations of Statistics_ , 2nd ed. (New York: Dover, 1972).\n\n[22](06_Intro.xhtml#chifn22) Alberto Toscano, “Materialism without Matter:\nAbstraction, Absence, and Social Form,” _Textual Practice_ 28, No. 7 (2014):\n1221–1240.\n\n[23](06_Intro.xhtml#chifn23) Moishe Postone, “Critique and Historical\nTransformation,” _Historical Materialism_ 12, No. 3 (2004): 59.\n\n[24](06_Intro.xhtml#chifn24) Simone Browne, _Dark Matters: On the Surveillance\nof Blackness_ (Durham, NC: Duke University Press, 2015).\n\n[25](06_Intro.xhtml#chifn25) Kimberle Crenshaw, “Mapping the Margins:\nIntersectionality, Identity Politics, and Violence against Women of Color,”\n_Stanford Law Review_ 43 (1990): 1241; Combahee River Collective, _A Black\nFeminist Statement_ (1977).\n\n[26](06_Intro.xhtml#chifn26) Gilles Châtelet provides an excellent and damning\nsynopsis of the naturalization of markets as a means to manage society in _To\nLive and Think Like Pigs: The Incitement of Envy and Boredom in Market\nDemocracies_ , trans. Robin Mackay (New York: Urbanomic, 2014).\n\n[27](06_Intro.xhtml#chifn27) Consider, for example, the question brought to\nthe fore by Vladimir Lenin and the Russian Revolution on whether this subject\nmust begin its work in the industrialized capitalist countries or an\nagricultural country like Russia; or the debates about whether the\nrevolutionary subject chooses revolution as an act of will or is, rather,\nswept along by the forces of history and economics.\n\n[28](06_Intro.xhtml#chifn28) Cedric Robinson, _Black Marxism: The Making of\nthe Black Radical Tradition_ (Chapel Hill: University of North Carolina Press,\n2000).\n\n[29](06_Intro.xhtml#chifn29) Eli Pariser, _The Filter Bubble: What the\nInternet Is Hiding from You_ (New York: Penguin, 2011).\n\n[30](06_Intro.xhtml#chifn30) See Seb Franklin, _Control: Digitality as\nCultural Logic_ (Cambridge, MA: MIT Press, 2015); Alexander Galloway,\n_Protocol: How Control Exists after Decentralization_ (Cambridge, MA: MIT\nPress, 2004); Virginia Eubanks, _Automating Inequality: How High-Tech Tools\nProfile, Police, and Punish the Poor_ (New York: St. Martin’s Press, 2017);\nSafiya Umoja Noble, _Algorithms of Oppression: How Search Engines Reinforce\nRacism_ (New York: New York University Press, 2018); and Siva Vaidhyanthan,\n_The Googlization of Everything (And Why We Should Worry)_ (Berkeley:\nUniversity of California Press, 2011).\n\n[31](06_Intro.xhtml#chifn31) Georg Lukács, _History and Class Consciousness:\nStudies in Marxist Dialectics_ , trans. Rodney Livingstone (Cambridge, MA: MIT\nPress, 1967); Axel Honneth, _Reification: A New Look at an Old Idea_ (Oxford:\nOxford University Press, 2008); Peter Sloterdijk, _Critique of Cynical Reason_\n(Minneapolis: University of Minnesota Press, 1988).\n\n[32](06_Intro.xhtml#chifn32) Tiqqun, “L’Hypothèse cybernétique,” _Tiqqun_ 2\n(2001): 40–83; Franco “Bifo” Berardi, _The Uprising: On Poetry and Finance_\n(Los Angeles: Semiotext(e), 2012); Jodi Dean, _Crowds and Party_ (London and\nNew York: Verso, 2016). Likewise, Brian Massumi argues that “the first task of\nthe revaluation of value is to uncouple value from quantification. Value must\nbe recognized for what it is: irreducibly qualitative” (Thesis 5). While\nMassumi admirably turns against the fetishization of the capitalist subject\nand toward the reconceptualization of value, his disavowal of quantification\n(and with it computation) risks resulting, in the end, in an inability to\novercome the distributed computation that is capitalism. _99 Theses on the\nRevaluation of Value: A Postcapitalist Manifesto_ (Minneapolis: University of\nMinnesota Press, 2018). See also Seb Franklin’s calls in _Control_ for\nresistance founded on “states of undecidability or unmeasurability.”\n\n[33](06_Intro.xhtml#chifn33) Endnotes Collective, _Endnotes 4_ (London:\nEndnotes, October 2015).\n\n[34](06_Intro.xhtml#chifn34) Postone, “Critique and Historical\nTransformation,” 56.\n\n[35](06_Intro.xhtml#chifn35) Jacques Lacan, _The Seminar of Jacques Lacan: The\nOther Side of Psychoanalysis_ , Book XVII, trans. Russell Grigg (New York:\nW.W. Norton & Company, 2007), 207.\n\n[36](06_Intro.xhtml#chifn36) See, for example, Bruno Latour, “Why Has Critique\nRun Out of Steam? From Matters of Fact to Matters of Concern,” _Critical\nInquiry_ 30, No. 2 (2004): 225–248; Graham Harman, _The Quadruple Object_\n(London: Zero Books, 2011); Ian Bogost, _Alien Phenomenology; or What It’s\nLike to Be a Thing_ (Minneapolis: University of Minnesota Press, 2012); Levi\nBryant, _The Democracy of Objects_ (Ann Arbor, MI: Open Humanities Press,\n2011).\n\n[37](06_Intro.xhtml#chifn37) Mark Fisher, _Capitalist Realism: Is There No\nAlternative?_ (London: Zero Books, 2009).\n\n[38](06_Intro.xhtml#chifn38) Evgeny Morozov, _To Save Everything, Click Here:\nThe Folly of Technological Solutionism_ (New York: Public Affairs, 2013).\n\n[39](06_Intro.xhtml#chifn39) See Peter Galison and Lorraine Daston,\n_Objectivity_ (New York: Zone Books, 2007); Theodore M. Porter, _Trust in\nNumbers: The Pursuit of Objectivity in Science and Public Life_ (Princeton,\nNJ: Princeton University Press, 1995); and Michel Foucault, _The Order of\nThings_ (New York: Routledge, 2005, repr.).\n\n[40](06_Intro.xhtml#chifn40) Antonio Negri analyzes the relationship between\nthe subjective and objective sides of capitalism at length, especially in\nrelation to Marx’s writings in the _Grundrisse_ , tracing the ways in which\nthey interact and mutually constitute each other. Antonio Negri and Jim\nFleming, _Marx beyond Marx: Lessons on the Grundrisse_ (Brooklyn: Autonomedia,\n1991).\n\n[41](06_Intro.xhtml#chifn41) William Davies, “The Long Read: How Statistics\nLost Their Power—And Why We Should Fear What Comes Next,” _Guardian_ , January\n19, 2017.\n\n[42](06_Intro.xhtml#chifn42) Slavoj Žižek argues that this is how the symptom\nfunctions in both Marx and Freud. The aim is not to get behind the mask,\nbecause one only discovers that there is nothing there; rather one must\nunderstand how the displacement translates material into meaning—labor into\nvalue in the case of Marx and the unconscious thought into the dream symbol\nfor Freud. What matters is the form of translation, not the meaning of the\nsymbol. _The Sublime Object of Ideology_ (London: Verso, 1989).\n\n[43](06_Intro.xhtml#chifn43) Christian Fuchs and Sebastian Sevignani have\nupdated Dallas Smythe’s concept of the audience commodity to clarify the\nprocess of productive consumption: “What Is Digital Labour? What Is Digital\nWork? What’s Their Difference? And Why Do These Questions Matter for\nUnderstanding Social Media?,” _tripleC: Communication, Capitalism and\nCritique_ 11, No. 2 (2013): 237–293; “Digital Prosumption Labour on Social\nMedia in the Context of the Capitalist Regime of Time,” _Time and Society_ 23,\nNo. 1 (2014): 97–123. Lisa Nakamura has used online video games to show how\neven platforms built around social play can create the conditions for the\nproduction of digital commodities that can be sold to others, exploiting labor\nalong racialized and imperial demarcations: “Don’t Hate the Player, Hate the\nGame: The Racialization of Labor in World of Warcraft,” _Critical Studies in\nMedia Communication_ 26, No. 2 (2009): 128–144. Jonathan Beller has argued\nthat the cinematic image and its legatees have shaped contemporary production\nwith spectators producing value: _The Cinematic Mode of Production: Attention\nEconomy and the Society of the Spectacle_ (Lebanon: Dartmouth College Press,\n2006). Tiziana Terranova, drawing on the insights of Italian autonomism, has\nproposed that most internet users are everywhere and always producing without\nnecessarily receiving compensation, further arguing that to call unpaid\ndigital work “labor” has a political value that exists even beyond its\neconomic accuracy: “Free Labor: Producing Culture for the Digital Economy,”\n_Social Text_ 63, No. 18 (2000): 33–58; “Free Labor,” in _Digital Labor: The\nInternet as Play-Ground and Factory_ ,” in Trebor Scholz, ed. (Abingdon:\nRoutledge, 2013). See also Lisa Nakamura, _Digitizing Race: Visual Cultures of\nthe Internet_ (Minneapolis: University of Minnesota Press, 2008); Dallas\nSmythe, _Dependency Road: Communications, Capitalism, Consciousness, and\nCanada_ (Norwood, NJ: Ablex Publishing Corporation, 1981); Christian Fuchs,\n_Digital Labour and Karl Marx_ (London: Routledge, 2014); Nick Dyer-Witheford,\n_Cyber Marx_ : _Cycles and Circuits of Struggle in High-Technology Capitalism_\n(Urbana, IL: University of Illinois Press, 1999). Edward Conor, “Revisiting\nMarx’s Value Theory: A Critical Response to Analyses of Digital Prosumption,”\n_The Information Society_ 31, No. 1 (2015): 13–19.\n\n[44](06_Intro.xhtml#chifn44) Nick Dyer-Witheford, _Cyber-Proletariat: Global\nLabour in the Digital Vortex_ (London: Pluto Press, 2015). Jussi Parika, _A\nGeology of Media_ (Minneapolis: University of Minnesota Press, 2015).\nChristian Fuchs, _Digital Labour and Karl Marx_ (London: Routledge, 2014);\nNikhil Pal Singh, “On Race, Violence, and So-Called Primitive Accumulation,”\n_Social Text_ 34, No. 3 (128) (2016): 27–50; and Intan Suwandi, _Value Chains:\nThe New Economic Imperialism_ (New York: Monthly Review Press, 2019). Saskia\nSassen has also traced the myriad ways in which contemporary capitalism seeks\nto expel individuals from the system of capitalist production: _Expulsions:\nBrutality and Complexity in the Global Economy_ (Cambridge, MA: Harvard\nUniversity Press, 2014).\n\n[45](06_Intro.xhtml#chifn45) Michael Hardt and Antonio Negri, _Empire_\n(Cambridge, MA: Harvard University Press, 2000).\n\n[46](06_Intro.xhtml#chifn46) Others, such as Nick Srnicek and Jathan Sadowski,\nhave argued that data should be considered closer to a raw material from which\ndata can be extracted or, in a related vein, that digital platforms can be\nconsidered property from which rent can be extracted. Nick Srnicek, _Platform\nCapitalism_ (New York: Polity, 2017); Jathan Sadowski, “When Data Is Capital:\nDatafication, Accumulation, and Extraction,” _Big Data and Society_ 6, No. 1\n(2019). David Harvey argues that for Marx, the value theory of labor is not\nstatic but is instead subject to change as capitalism transforms itself :\n“Marx’s Refusal of the Labour Theory of Value,” _Reading Marx’s Capital with\nDavid Harvey_ , March 1, 2018.\n\n[47](06_Intro.xhtml#chifn47) See Shoshana Zuboff, _The Age of Surveillance\nCapitalism: The Fight for a Human Future at the New Frontier of Power_\n(London: Profile Books, 2019); Yann Moulier-Boutang, _Cognitive Capitalism_\n(Cambridge, UK: Polity, 2011); Manuel Castells, _The Rise of the Network\nSociety_ (Hoboken, NJ: John Wiley & Sons, 2011); Srnicek, _Platform\nCapitalism_ ; James Beniger, _The Control Revolution: Technological and\nEconomic Origins of the Information Society_ (Cambridge, MA: Harvard\nUniversity Press, 2009); Klaus Schwab, _The Fourth Industrial Revolution_ (New\nYork: Crown Business, 2017); Viktor Mayer-Schönberger and Kenneth Cukier, _Big\nData: A Revolution That Will Transform How We Live, Work and Think_ (New York:\nHoughton Mifflin Harcourt, 2013); and Sam Popowich, “Mechanical Animals: Big\nData, Class Composition, and the Multitude,” University of Alberta Libraries,\n2019.\n\n[48](06_Intro.xhtml#chifn48) Postone, “Critique and Historical\nTransformation,” 57. Or, as David Harvey similarly argues: “The formulation of\nvalue in the first chapter of _Capital_ is revolutionized by what comes later.\nValue becomes an unstable and perpetually evolving inner connectivity (an\ninternal or dialectical relation) between value as defined in the realm of\ncirculation in the market and value as constantly being redefined through\nrevolutions in the realm of production” (“Marx’s Refusal”).\n\n[49](06_Intro.xhtml#chifn49) Dyer-Witheford, _Cyber-Proletariat_ , 15.\n\n[50](06_Intro.xhtml#chifn50) Nick Dyer-Witheford, Atle Mikkola Kjøsen and\nJames Steinhoff have recently written of AI: “Capitalism is the fusion of\nthese technological and social logics and AI is the most recent manifestation\nof its chimerical merging of computation with commodification.” _Inhuman\nPower: Artificial Intelligence and the Future of Capitalism_ (London: Pluto\nPress, 2019).\n\n[51](06_Intro.xhtml#chifn51) John Cheney-Lippold, _We Are Data: Algorithms and\nthe Making of Our Digital Selves_ (New York: New York University Press, 2018);\nGalloway, _Protocol_.\n\n[52](06_Intro.xhtml#chifn52) Specifically in regard to the history of\nstatistics, see Ian Hacking, _The Taming of Chance_ (Cambridge, UK: Cambridge\nUniversity Press, 1990); and Theodore M. Porter, _The Rise of Statistical\nThinking, 1820–1900_ (Princeton, NJ: Princeton University Press, 1986).\n\n[53](06_Intro.xhtml#chifn53) Browne, _Dark Matters_ ; Alexander Weheliye,\n_Habeas Viscus: Racializing Assemblages, Biopolitics, and Black Feminist\nTheories of the Human_ (Durham, NC: Duke University Press, 2014). Likewise,\nauthors such as Didier Bigo have argued that in order to understand modern\nbiopolitics, we must account for how they operate at the peripheries of the\nstate, where sovereign force is often much more violent and apparent in\nattempts to secure its borders than in the metropole. “Globalized\n(In)security: The Field and the Ban-opticon,” in _Terror, Insecurity and\nLiberty: Illiberal Practices of Liberal Regimes after 9/11_ , Didier Bigo and\nAnastassia Tsoukala, eds. (London: Routledge, 2008), 20–58.\n\n[54](06_Intro.xhtml#chifn54) Wendy Hui Kyong Chun, “Queerying Homophily,” in\nClemens Apprich, Wendy Hui Kyong Chun, Florian Cramer and Hito Steyerl,\n_Pattern Discrimination_ (Lüneburg: Meson Press, 2018), 59–97; Jacqueline\nWernimont, _Numbered Lives: Life and Death in Quantum Media_ (Cambridge, MA:\nMIT Press, 2019).\n\n[55](06_Intro.xhtml#chifn55) Noble, _Algorithms of Oppression_ ; Eubanks,\n_Automating Inequality_ ; Cathy O’Neil, _Weapons of Math Destruction: How Big\nData Increases Inequality and Threatens Democracy_ (New York: Broadway Books,\n2016). Likewise, a number of authors have shown the ways in which these\ndigital systems and their antecedents have provided an ideological form for\ncontemporary neoliberalism. Wendy Hui Kyong Chun has shown how software\nprovides a framework for neoliberal governance to understand itself; Jean\nPierre-Dupuy has argued that cybernetics and the early days of cognitive\nscience have greatly shaped the contemporary understanding of thought and\nhumanity; and Seb Franklin has suggested the ways in which digitality has\nprovided a set of metaphors through which capitalism thinks and shapes the\ncontemporary subject. Wendy Hui Kyong Chun, _Programmed Visions: Software and\nMemory_ (Cambridge, MA: MIT Press, 2011); Jean-Pierre Dupuy, _The\nMechanization of the Mind: On the Origins of Cognitive Science_ (Princeton,\nNJ: Princeton University Press, 2000); Franklin, _Control_.\n\n[56](06_Intro.xhtml#chifn56) For a rare exception in this regard, see Gerd\nGigerenzer, Zeno Swijtink and Lorraine Daston, _The Empire of Chance: How\nProbability Changed Science and Everyday Life_ (Cambridge, UK: Cambridge\nUniversity Press, 1990).\n\n[57](06_Intro.xhtml#chifn57) Franco “Bifo” Berardi, _The Soul at Work: From\nAlienation to Autonomy_ (Los Angeles: Semiotext(e), 2009), 22.\n\n[58](06_Intro.xhtml#chifn58) To draw on the language of Donald Mackenzie,\nstatistics and probability are certainly not a camera, and to think of them\nsolely as making propaganda movies is not only wrong, but misses the ways in\nwhich they are fundamentally an engine that produces value and capitalism\nitself. _An Engine Not a Camera: How Financial Models Shape Markets_\n(Cambridge, MA: MIT Press, 2008).\n\n1\\. Automating Knowledge\n\n[1](08_Chapter1.xhtml#ch1fn1) Constance L. Hayes, “What Walmart Knows about\nCustomers' Habits,” _New York Times_ , November 14, 2004.\n\n[2](08_Chapter1.xhtml#ch1fn2) Ibid.\n\n[3](08_Chapter1.xhtml#ch1fn3) Christian Sandvig, “You Are a Political Junkie\nand Felon Who Loves Blenders: Recovering Motives from Machine Learning,” paper\npresented to the symposium, “Data Associations in Law and Policy,” Faculty of\nLaw, University of New South Wales, Sydney, Australia (December 11, 2015).\n\n[4](08_Chapter1.xhtml#ch1fn4) Frank H. Knight, _Risk, Uncertainty, and Profit_\n(Boston: Houghton Mifflin, 1921).\n\n[5](08_Chapter1.xhtml#ch1fn5) John Venn, _The Logic of Chance_ (New York:\nMacmillan & Co., 1866), 176.\n\n[6](08_Chapter1.xhtml#ch1fn6) Alan Hájek, “The Reference Class Problem Is Your\nProblem Too,” _Synthese_ 156, No. 3 (2007): 563–585.\n\n[7](08_Chapter1.xhtml#ch1fn7) Declan Butler, “When Google Got Flu Wrong,”\n_Nature_ 494, No. 7436 (2013): 155–156.\n\n[8](08_Chapter1.xhtml#ch1fn8) Machine learning and its relationship to\ncapitalism directly open the question of the “event” as a syncope in\nhistorical time that could change faster than an algorithm is able to respond\nor update. See Alain Badiou, _Being and Event_ , trans. Oliver Feltham (New\nYork: Continuum, 2006); and Jacques Derrida, _Writing and Difference_ (London:\nRoutledge, 2001).\n\n[9](08_Chapter1.xhtml#ch1fn9) Timothy A. Judge and Daniel M. Cable, “The\nEffect of Physical Height on Workplace Success and Income: Preliminary Test of\na Theoretical Model,” _Journal of Applied Psychology_ 89, No. 3 (2004).\n\n[10](08_Chapter1.xhtml#ch1fn10) Will Knight, “The Dark Secret at the Heart of\nAI,” _MIT Technology Review_ , April 11, 2017.\n\n[11](08_Chapter1.xhtml#ch1fn11) “Artificial general intelligence” (AGI) refers\nto the capacity of machines to fully approximate or even surpass the capacity\nof human intelligence to perform complex tasks. Cassio Pennachin and Ben\nGoertzel define genuine AGI as a “software program that can solve a variety of\ncomplex problems in a variety of different domains, and that controls itself\nautonomously, with its own thoughts, worries, feelings, strengths, weaknesses\nand predispositions.” “Contemporary Approaches to Artificial General\nIntelligence,” in _Artificial General Intelligence_ , Ben Goertzel and Cassio\nPennachin, eds. (New York: Springer, 2006), 1.\n\n[12](08_Chapter1.xhtml#ch1fn12) Google’s senior vice president for search and\nhead of AI, John Giannandrea, has claimed that AI and machine learning’s\ngreatest threat to humanity are the biases machines and algorithms learn\nthrough training with already-prejudiced input data. Will Knight, “Forget\nKiller Robots—Bias Is the Real AI Danger,” _MIT Technology Review_ , October\n3, 2017. On the proliferation of algorithmic bias, see Will Knight, “Biased\nAlgorithms Are Everywhere, and No One Seems to Care,” _MIT Technology Review_\n, July 12, 2017.\n\n[13](08_Chapter1.xhtml#ch1fn13) Julia Angwen and Jeff Larson, “Bias in\nCriminal Risk Scores is Mathematically Inevitable, Researchers Say,”\n_ProPublica_ , December 30, 2016.\n\n[14](08_Chapter1.xhtml#ch1fn14) Julia Dressel and Hany Farid, “The Accuracy,\nFairness, and Limits of Predicting Recidivism,” _Science Advances_ 4, No. 1\n(2018): 1–5.\n\n[15](08_Chapter1.xhtml#ch1fn15) Kashmir Hill, “How Target Figured Out a Teen\nGirl Was Pregnant before Her Father Did,” _Forbes_ , February 2, 2012.\n\n[16](08_Chapter1.xhtml#ch1fn16) Charles Duhigg, “What Does Your Credit-Card\nCompany Know about You?,” _New York Times_ , May 12, 2009.\n\n[17](08_Chapter1.xhtml#ch1fn17) Georg Martius and Christoph H. Lampert.\n“Extrapolation and Learning Equations,” _arXiv_ , 2016.\n\n[18](08_Chapter1.xhtml#ch1fn18) As a result of the complexity of many of these\nalgorithms, they oftentimes succeed in finding locally optimal solutions to\nproblems, which means that under different starting conditions but with the\nsame data, a different solution could be reached.\n\n[19](08_Chapter1.xhtml#ch1fn19) Ethem Alpaydın, _Machine Learning: The New AI_\n(Cambridge, MA: MIT Press, 2016), 99.\n\n[20](08_Chapter1.xhtml#ch1fn20) Josh Patterson and Adam Gibson, _Deep\nLearning: A Practitioner’s Approach_ (Sebastopol, CA: O’Reilly Media, 2017),\n321.\n\n[21](08_Chapter1.xhtml#ch1fn21) Georg Wilhelm Friedrich Hegel, _The\nPhenomenology of Spirit_ , trans. A.V. Miller (Oxford: Oxford University\nPress, 1977), 9.\n\n[22](08_Chapter1.xhtml#ch1fn22) This focus on the utility of prediction or the\npredictive capacity of theoretical models, particularly in spite of their\noften overly simplistic assumptions about the nature of the world, is a\nscientific vision of the world that is also embraced by financial theorists\nlike Milton Friedman. See MacKenzie, _An Engine Not a Camera_.\n\n[23](08_Chapter1.xhtml#ch1fn23) Google has recently succeeded in making a\nmachine learning algorithm to optimize hyperparameters for its “child”\nalgorithm, which have outperformed the best human-tuned models by a little\nbit. But still, the original algorithm requires well-tuned hyperparameters.\nDom Galeon and Kristin Houser, “Google’s AI Built Another AI That Outperforms\nAny Made by Humans,” _Futurism_ , December 1, 2017.\n\n[24](08_Chapter1.xhtml#ch1fn24) In general, machine learning problems come in\ntwo distinct types. The first are regression problems, such as the one we have\nbeen describing here, which involve predicting some numerical value. The\nsecond are classification problems, where we want to know if something is of\none type or another (e.g., whether a sports team will win or lose). The\ngeneral approach is the same, but in this case, we are trying to determine if\nthe output is zero or one, rather than somewhere on a spectrum, such as the\nprice of Pop-Tarts.\n\n[25](08_Chapter1.xhtml#ch1fn25) One example of this type of algorithm is\n“k-nearest neighbors,” which classifies data by finding points in the\ndataspace that can be used to efficiently group the data.\n\n[26](08_Chapter1.xhtml#ch1fn26) Bruce G. Buchanan, “A (Very) Brief History of\nArtificial Intelligence,” _AI Magazine_ 26, No. 4 (2006): 58–59.\n\n[27](08_Chapter1.xhtml#ch1fn27) Marvin Minsky, _Computation: Finite and\nInfinite Machines_ (Englewood Cliffs, NJ: Prentice-Hall, 1967).\n\n[28](08_Chapter1.xhtml#ch1fn28) For an exceptional essay on how machine\nlearning has changed and could change our understanding of intelligence, see\nCatherine Malabou, _Morphing Intelligence: From IQ Measurement to Artificial\nBrains_ (New York: Columbia University Press, 2019).\n\n[29](08_Chapter1.xhtml#ch1fn29) Stuart Russell and Peter Norvig, _Artificial\nIntelligence: A Modern Approach_ , 3rd ed. (Upper Saddle River, NJ: Pearson,\n2010), 16.\n\n[30](08_Chapter1.xhtml#ch1fn30) Frank Rosenblatt, “The Perceptron: A\nProbabilistic Model for Information Storage and Organization in the Brain,”\n_Psychological Review_ 65, No. 6 (1958): 386–408.\n\n[31](08_Chapter1.xhtml#ch1fn31) Ibid., 388.\n\n[32](08_Chapter1.xhtml#ch1fn32) In their book on perceptrons, Marvin Minsky\nand Seymour Papert question the extent to which a perceptron could effectively\nlearn in a manner analogous to the human brain due to its incapacity to solve\nthe “XOR,” or “exclusive or,” problem. _Perceptrons: An Introduction to\nComputational Geometry_ (Cambridge, MA: MIT Press, 1969). It has since been\nproven that multilayer perceptrons can in fact solve this problem by including\na hidden layer.\n\n[33](08_Chapter1.xhtml#ch1fn33) Pamela McCorduck, _Machines Who Think: A\nPersonal Inquiry into the History and Prospects of Artificial Intelligence_\n(Natick, MA: AK Peters / CRC Press, 2009), 106–107.\n\n[34](08_Chapter1.xhtml#ch1fn34) David E. Rumelhart, Geoffrey E. Hinton and\nRonald J. Williams, “Learning Representations by Back-propagating Errors,”\n_Nature_ 323 (1986): 533–536.\n\n[35](08_Chapter1.xhtml#ch1fn35) Ethem Alpaydın, _Introduction to Machine\nLearning_ , 3rd ed. (Cambridge, MA: MIT Press, 2014).\n\n[36](08_Chapter1.xhtml#ch1fn36) Chris Anderson, “The End of Theory: The Data\nDeluge Makes the Scientific Method Obsolete,” _Wired_ , June 23, 2008, 16–17.\n\n[37](08_Chapter1.xhtml#ch1fn37) Ibid.\n\n[38](08_Chapter1.xhtml#ch1fn38) Davide Panagia agues that algorithms\nultimately manage variability and probability in order to dispose relations,\nsuch that “these dispositional powers are operant regardless of any epistemic\nreform one might adapt to its computational logics. In short, an algorithm is\na dispositif not because it constrains freedom through various forms of\ndomination, but because it proliferates controls on variability and, in this\nway, governs the movement of bodies and energies in space and time.” “On the\nPossibilities of a Political Theory of Algorithms,” _Political Theory_ 49, No.\n1 (2021): 109–133.\n\n[39](08_Chapter1.xhtml#ch1fn39) It is worth noting, in this regard, that\nAndrew Gelman, one of the leading Bayesian theorists of our time, constantly\ndescribes the process of inference as one of model building and testing. See,\nfor instance, “Bayes, Jeffreys, Prior Distributions and the Philosophy of\nStatistics,” _Statistical Science_ 24, No. 2 (2009): 176–178.\n\n[40](08_Chapter1.xhtml#ch1fn40) Geoffrey Bowker, “Big Data, Big Questions: The\nTheory/Data Thing,” _International Journal of Communication_ 8 (2014): 5.\n\n[41](08_Chapter1.xhtml#ch1fn41) Chun, “Queerying Homophily.”\n\n[42](08_Chapter1.xhtml#ch1fn42) Ibid., 82.\n\n[43](08_Chapter1.xhtml#ch1fn43) Ibid., 84.\n\n2\\. Can Computers Do Math?\n\n[1](09_Chapter2.xhtml#ch2fn1) Frank Pasquale, _The Black Box Society: The\nSecret Algorithms That Control Money and Information_ (Cambridge, MA: Harvard\nUniversity Press, 2015); Christian Sandvig, Kevin Hamilton, Karrie Karahalios\nand Cedric Langbort, “Auditing Algorithms: Research Methods for Detecting\nDiscrimination on Internet Platforms,” _Data and Discrimination: Converting\nCritical Concerns into Productive Inquiry_ (2014): 1–23.\n\n[2](09_Chapter2.xhtml#ch2fn2) Slavoj Žižek, citing Peter Sloterdijk’s\ndefinition of cynicism, states that the cynical logic of modern capitalism can\nbe characterized thus: “They know very well what they are doing, but still\nthey are doing it.” _The Sublime Object of Ideology_ (London: Verso, 1989),\n32–33. See also Mark Fisher, _Capitalist Realism: Is There No Alternative_?\n(London: Zero Books, 2009).\n\n[3](09_Chapter2.xhtml#ch2fn3) Kennith I. Appel and Wolfgang Haken, _Every\nPlanar Map Is Four Colorable_ (Providence, RI: American Mathematical Society,\n1989); Robin J Wilson, _Four Colors Suffice: How the Map Problem Was Solved_\n(Princeton, NJ: Princeton University Press, 2002).\n\n[4](09_Chapter2.xhtml#ch2fn4) Marjin J.H. Heule, Oliver Kullmann and Victor W.\nMarek, “Solving and Verifying the Boolean Pythagorean Triples Problem via\nCube-and-Conquer,” in _Theory and Applications of Satisfiability Testing—SAT\n2016_ , Nadia Creignou and Daniel Le Berre, eds., _Lecture Notes in Computer\nScience_ , Vol. 9710 (London: Springer, 2016), 228–245.\n\n[5](09_Chapter2.xhtml#ch2fn5) Evelyn Lamb, “Two-Hundred-Terabyte Maths Proof\nIs Largest Ever,” _Nature_ , May 26, 2016.\n\n[6](09_Chapter2.xhtml#ch2fn6) Ibid.\n\n[7](09_Chapter2.xhtml#ch2fn7) Thomas Tymoczko, “The Four-Color Problem and Its\nPhilosophical Significance,” _Journal of Philosophy_ 76, No. 2 (1979): 57–83.\n\n[8](09_Chapter2.xhtml#ch2fn8) See Louis Althusser, “On the Materialist\nDialectic,” in _For Marx_ , trans. Ben Brewster (London and New York: Verso,\n2005), 161–185.\n\n[9](09_Chapter2.xhtml#ch2fn9) Davide Castelvecchi, “The Biggest Mystery in\nMathematics: Shinichi Mochizuki and the Impenetrable Proof,” _Nature News_ ,\nOctober 7, 2015.\n\n[10](09_Chapter2.xhtml#ch2fn10) Ibid.\n\n[11](09_Chapter2.xhtml#ch2fn11) George Berkeley, _De Motu and The Analyst: A\nModern Edition with Introductions and Commentary_ , trans. Douglas M. Jessup\n(New York: Springer, 1992).\n\n[12](09_Chapter2.xhtml#ch2fn12) Karl Marx, _Mathematical Manuscripts of Karl\nMarx_ , Sofya Yanovskaya, ed. (London: New Park Publications, 1983).\n\n[13](09_Chapter2.xhtml#ch2fn13) The following century, Augustin-Louis Cauchy\nformalized the concept of a limit and put the foundations of calculus on much\nmore solid footing.\n\n[14](09_Chapter2.xhtml#ch2fn14) Similar mysteries can be found in the less\nsacerdotal branches of contemporary computation. For example, in 1985 the\nInstitute of Electrical and Electronics Engineers (IEEE) approved “Standard\n754” to regularize floatingpoint operations (i.e., operations including\nnumbers with a decimal point). One of the more revealing elements in this\nstandard is an insistence that NaN, which stands for “not a number,” should\nnot be equal to itself. A NaN is most often generated in response to an\n“illegal” operation, such as the division of a number by zero, so that a piece\nof software can gracefully deal with invalid instructions. The logic behind\nmaking NaN not equal to itself is to avoid accidental equivalence. If a\nprogram is supposed to compare two numbers, and both of them are the result of\nillegal operations, it might continue as though everything is okay until the\nmachine crashes as more and more is done atop those defective operations.\nPerhaps the IEEE’s Standard 754 is a rather sensible decision, and does not\namount to a logic-defying mystery of faith. But we see here the importance of\na standard that formally makes the equal unequal—and conversely the unequal\nequal—for the operable success of calculation. Dan Zuras, Mike Cowlishaw, Alex\nAiken et al., “IEEE Standard for Floating-Point Arithmetic,” _IEEE Std\n754–2008_ (2008): 1–70.\n\n[15](09_Chapter2.xhtml#ch2fn15) Berkeley, _The Analyst_ , 209.\n\n[16](09_Chapter2.xhtml#ch2fn16) Ibid., 218.\n\n[17](09_Chapter2.xhtml#ch2fn17) Jacques Derrida’s work, especially his later\nwork, in many ways repeats this insistence, claiming that a decision cannot be\nthe application of the rule. See, for example, _Rogues: Two Essays on Reason_\n(Palo Alto, CA: Stanford University Press, 2005).\n\n[18](09_Chapter2.xhtml#ch2fn18) George Berkeley, _Alciphron; or The Minute\nPhilosopher: In Focus_ (New Haven: Increase Cooke & Co., 1803).\n\n[19](09_Chapter2.xhtml#ch2fn19) Quoted in Heinrich Joseph Denzinger, _The\nSources of Catholic Dogma_ , trans. Roy J. Deferrari (Fitzwilliam: Loreto\nPublications, 1955).\n\n[20](09_Chapter2.xhtml#ch2fn20) First Vatican Council, 3rd Session, “De fide\net ratione,” i.\n\n[21](09_Chapter2.xhtml#ch2fn21) Berkeley, _The Analyst_.\n\n[22](09_Chapter2.xhtml#ch2fn22) Cantor argues that _The Analyst_ should be\nunderstood as a demonstration that calculus partakes of the same mysteries as\nreligion, which for religion is foundational but for calculus is an unbearable\naffront to its rationalist claims. Geoffrey Cantor, “Berkeley’s _The Analyst_\nRevisited,” _Isis_ 75, No. 4 (1984): 677.\n\n[23](09_Chapter2.xhtml#ch2fn23) In regard to probability, see Ian Hacking’s\nexcellent text, _The Emergence of Probability: A Philosophical Study of Early\nIdeas about Probability, Induction and Statistical Inference_ (Cambridge, UK:\nCambridge University Press, 2006).\n\n[24](09_Chapter2.xhtml#ch2fn24) Along these lines, Lewis Mumford argues that\nthe ability to work with glass played a critical role in the rise of a thought\ncommitted to clarity: _Technics and Civilization_ (Chicago: University of\nChicago Press, 2010).\n\n[25](09_Chapter2.xhtml#ch2fn25) James Jurin, “Geometry No Friend to\nInfidelity,” _London_ 1734 (2002): 382.\n\n[26](09_Chapter2.xhtml#ch2fn26) George Berkeley, “Defence of Free-Thinking,”\ncited in Cantor, “ _The Analyst_ Revisited.”\n\n3\\. Algorithms of Objectification\n\n[1](10_Chapter3.xhtml#ch3fn1) Donald E. Knuth, _The Art of Computer\nProgramming_ , Vol. 1, _Fundamental Algorithms_ (Reading, MA: Addison-Wesley\nLongman, 1996), 6.\n\n[2](10_Chapter3.xhtml#ch3fn2) See Justin Joque, _Deconstruction Machines:\nWriting in the Age of Cyberwar_ (Minneapolis: University of Minnesota Press,\n2018).\n\n[3](10_Chapter3.xhtml#ch3fn3) W.T. Baxter, “Early Accounting: The Tally and\nCheckerboard,” _Accounting Historians Journal_ 16, No. 2 (1989): 43–89.\n\n[4](10_Chapter3.xhtml#ch3fn4) In a somewhat famous footnote, Louis Althusser\nobjected to what he called “the theory of reification,” suggesting that it\nprojected the early theory of alienation onto the latter theory of fetishism,\ncreating a concept that he believed was too psychological. Regardless of\nwhether one accepts Althusser’s periodization of Marx’s thought, the purpose\nof objectification as developed in this book is to provide a theory that is\ndecidedly not psychological, even though latter chapters engage with the\nconcept of alienation. The point here is that objectification creates an\nexternal, though non-localizable, force of necessity. Althusser, “Marxism and\nHumanism,” in _For Marx_ , 230n7.\n\n[5](10_Chapter3.xhtml#ch3fn5) Barbara Kiviat, “The Art of Deciding with Data:\nEvidence from How Employers Translate Credit Reports into Hiring Decisions,”\n_Socio-Economic Review_ (2017).\n\n[6](10_Chapter3.xhtml#ch3fn6) Baxter, “Early Accounting.”\n\n[7](10_Chapter3.xhtml#ch3fn7) Lukács, _History and Class Consciousness_ , 83.\n\n[8](10_Chapter3.xhtml#ch3fn8) Martha Nussbaum, _Sex and Social Justice_\n(Oxford, UK: Oxford University Press, 1999).\n\n[9](10_Chapter3.xhtml#ch3fn9) An important distinction must be drawn between\nthe theory of the revolutionary object presented here and Latour’s work on\nActor Network Theory which attempts to describe the social force of objects—or\nactants as he calls everything. His theory focuses on the means by which\nvarious objects act, whereas the Marxist reading of objectification\nforegrounds the means by which social relations are inscribed—even if\nmagically or metaphysically—into objects of accounting (e.g., the commodity).\nBruno Latour, _Reassembling the Social: An Introduction to Actor-Network-\nTheory_ (Oxford, UK: Oxford University Press, 2005).\n\n[10](10_Chapter3.xhtml#ch3fn10) Georg Lukács, _History and Class\nConsciousness: Studies in Marxist Dialectics_ , trans. Rodney Livingstone\n(Cambridge, MA: MIT Press, 1967); Herbert Marcuse, _One-Dimensional Man:\nStudies in the Ideology of Advanced Industrial Society_ , 2nd ed. (London:\nRoutledge, 1991).\n\n[11](10_Chapter3.xhtml#ch3fn11) Axel Honneth claims that reification is a form\nof forgetting, but for him it is the forgetting of an “antecedent recognition”\nthat first allows one to recognize the subjective qualities of an object. With\nreification, human beings fail to acknowledge the multiplicity of “subjective\nconceptions and meanings” that individuals accord to objects in their initial\nrecognition. _Reification: A New Look at an Old Idea_ (Oxford: Oxford\nUniversity Press, 2005), 63.\n\n[12](10_Chapter3.xhtml#ch3fn12) Karl Marx, _Capital_ , Vol. 1 (London: Penguin\nBooks, 1990), 129.\n\n[13](10_Chapter3.xhtml#ch3fn13) Karl Marx, “Economic and Philosophical\nManuscripts of 1844,” in _The Marx-Engels Reader_ , Robert C. Tucker, ed., 2nd\ned. (New York: W.W. Norton & Company, 1978), 71.\n\n[14](10_Chapter3.xhtml#ch3fn14) Marx, _Capital_ , 135.\n\n[15](10_Chapter3.xhtml#ch3fn15) Ibid., 166–167.\n\n[16](10_Chapter3.xhtml#ch3fn16) While Marx was writing about a heavily\nindustrial economy, many have attempted to update these insights to describe\ncontemporary capitalism. For example, in a 1981 chapter of his book\n_Dependency Road_ , Dallas Smythe reinterprets Marxist critiques of political\neconomy to account for the role of advertising and audiences in contemporary\ncapitalism. Smythe describes audiences as commodities that mass media\nindustries produce as consumers whose attention they sell to advertisers.\n_Dependency Road: Communications, Capitalism, Consciousness, and Canada_\n(Norwood, NJ: Ablex Publishing, 1981). Christian Fuchs has updated Smythe’s\ntheory of the “audience commodity” to think about how social media companies\nlike Facebook and Twitter extract value from users. For more, see _Digital\nLabour and Karl Marx_ (London: Routledge, 2014), esp. 72–134.\n\n[17](10_Chapter3.xhtml#ch3fn17) Marx, _Capital_ , 193–194.\n\n[18](10_Chapter3.xhtml#ch3fn18) Ibid., 165.\n\n[19](10_Chapter3.xhtml#ch3fn19) To Moishe Postone, all three volumes of\n_Capital_ represent attempts by Marx to draw attention to the way fetishism\nconceals the abstract dimension of labor through forms that appear as\ntranshistorical and metaphysical. _Time, Labor, and Social Domination_\n(Cambridge, UK: Cambridge University Press, 1993), 271. For a nuanced reading\nof the notion of fetish, especially as it was historically developed from\nAfro-Atlantic gods, and the social rationality that underwrites its various\nvalences, see J. Lorand Matory, _The Fetish Revisited: Marx, Freud, and the\nGods Black People Make_ (Durham, NC: Duke University Press, 2018).\n\n[20](10_Chapter3.xhtml#ch3fn20) Marx says: “The internal opposition between\nuse-value and value, hidden within the commodity, is therefore represented on\nthe surface by an external opposition, i.e., by a relation between two\ncommodities.” _Capital_ , 153.\n\n[21](10_Chapter3.xhtml#ch3fn21) Ibid., 167.\n\n[22](10_Chapter3.xhtml#ch3fn22) Jackie Wang, _Carceral Capitalism_ (Cambridge:\nMIT Press, 2018), 22.\n\n[23](10_Chapter3.xhtml#ch3fn23) Cedric Robinson, _Black Marxism: The Making of\nthe Black Radical Tradition_ (Chapel Hill: University of North Carolina Press,\n2000), 276. Fred Moten asks of this quote: “Is it a complete detachment from\nthat temporal/historical trajectory or is it a displacement, a retemporization\ndisruptive of the very idea of absolute break and, by extension, an\naugmentative curvature of old harmonic notions of convergence or hybridity, a\ndissonant bending of the dialectic and its notes?” _Black and Blur_ (Durham,\nNC: Duke University Press, 2017), 9.\n\n[24](10_Chapter3.xhtml#ch3fn24) Wang, _Carceral Capitalism_ , 89–92.\n\n[25](10_Chapter3.xhtml#ch3fn25) It is possible to see this process in Marx’s\nown analysis of imperialism in the third volume of _Capital_ and the\nsignificant secondary literature that builds upon it.\n\n[26](10_Chapter3.xhtml#ch3fn26) Zeynep Tufekci, “YouTube, the Great\nRadicalizer,” _New York Times_ , March 10, 2018; Kelly Weill, “How YouTube\nBuilt a Radicalization Machine for the Far Right,” _Daily Beast_ , December\n17, 2018.\n\n[27](10_Chapter3.xhtml#ch3fn27) Ramon Amaro, “As If,” _e-flux_ , February 14,\n2019.\n\n[28](10_Chapter3.xhtml#ch3fn28) See Finn Brunton, _Digital Cash: A Cultural\nHistory_ (Princeton, NJ: Princeton University Press, 2019).\n\n[29](10_Chapter3.xhtml#ch3fn29) Stacco Troncoso, “Cypherpolitical Enterprises:\nProgrammatic Assessments,” _P2P Foundation_ , March 15, 2017.\n\n[30](10_Chapter3.xhtml#ch3fn30) Karl Marx, _Grundrisse: Foundations of the\nCritique of Political Economy_ (London: Penguin, 2005, repr.), 339.\n\n[31](10_Chapter3.xhtml#ch3fn31) Moten, _Black and Blur_ , 38.\n\n4\\. Do Dead Fish Believe in God?\n\n[1](12_Chapter4.xhtml#ch4fn1) Susan Joslyn, Limor Nadav-Greenberg and Rebecca\nM. Nichols, “Probability of Precipitation: Assessment and Enhancement of End-\nUser Understanding,” _Bulletin of the American Meteorological Society_ 90, No.\n2 (2009): 185–193.\n\n[2](12_Chapter4.xhtml#ch4fn2) “FAQ—What is the Meaning of PoP?,” US National\nWeather Service.\n\n[3](12_Chapter4.xhtml#ch4fn3) Ramón de Elía and René Laprise, “Diversity in\nInterpretations of Probability: Implications for Weather Forecasting,”\n_Monthly Weather Review_ 133, No. 5 (2005): 1129–1143.\n\n[4](12_Chapter4.xhtml#ch4fn4) Ibid.\n\n[5](12_Chapter4.xhtml#ch4fn5) Jerzy Neyman, “Outline of a Theory of\nStatistical Estimation Based on the Classical Theory of Probability,”\n_Philosophical Transactions of the Royal Society of London_ , Series A,\n_Mathematical and Physical Sciences_ 236, No. 767 (1937): 333–380.\n\n[6](12_Chapter4.xhtml#ch4fn6) Leonard J. Savage, _The Foundations of\nStatistics_ , 2nd ed. (New York: Dover, 1972), 2.\n\n[7](12_Chapter4.xhtml#ch4fn7) Quoted in Edmund F. Byrne, _Probability and\nOpinion: A Study in the Medieval Presuppositions of Post-Medieval Theories of\nProbability_ (The Hague: Martinus Nijhoff, 1968).\n\n[8](12_Chapter4.xhtml#ch4fn8) Ian Hacking, _The Taming of Chance_ (Cambridge,\nUK: Cambridge University Press, 1990).\n\n[9](12_Chapter4.xhtml#ch4fn9) Geoffrey C. Bowker, _Memory Practices in the\nSciences_ (Cambridge, MA: MIT Press, 2008); Lisa Gitelman, ed., “ _Raw Data”\nIs an Oxymoron_ (Cambridge, MA: MIT Press, 2013).\n\n[10](12_Chapter4.xhtml#ch4fn10) Edward Vul, Christine Harris, Piotr Winkielmen\net al., “Puzzlingly High Correlations in fMRI Studies of Emotion, Personality,\nand Social Cognition,” _Perspectives on Psychological Science_ 4, No. 3\n(2009): 274–290.\n\n[11](12_Chapter4.xhtml#ch4fn11) Stephen M. Smith, “Overview of fMRI Analysis,”\n_British Journal of Radiology_ 77 (2004): S170.\n\n[12](12_Chapter4.xhtml#ch4fn12) Anders Ecklund, Thomas E. Nichols and Hans\nKnutsson, “Cluster Failure: Why fMRI Inferences for Spatial Extent Have\nInflated False-Positive Rates, _Proceedings of the National Academy of\nSciences of the United States of America_ 113, No. 28 (2016): 7900–7905.\n\n[13](12_Chapter4.xhtml#ch4fn13) Cyril Pernet and Tom Nichols, \"Has a Software\nBug Really Called Decades of Brain Imaging Research into Question?,\"\n_Guardian_ , September 13, 2016; Mary-Ann Russon, “15 Years of Brain Research\nHas Been Invalidated by a Software Bug, Say Swedish Scientists,”\n_International Business Times_ , July 13, 2016; Robin Harris, “When Big Data\nIs Bad Data,” _ZDnet_ , July 15, 2016.\n\n[14](12_Chapter4.xhtml#ch4fn14) Robert W. Cox, Gang Chen, Daniel R. Glen et\nal., “fMRI Clustering and False-Positive Rates,” _Proceedings of the National\nAcademy of Sciences of the United States of America_ 114, No. 17 (2017):\nE3370–E3371; Daniel Kessler, Mike Angstadt and Chandra S. Sripada,\n“Reevaluating ‘Cluster Failure’ in fMRI Using Nonparametric Control of the\nFalse Discovery Rate,” _Proceedings of the National Academy of Sciences of the\nUnited States of America_ 114, No. 17 (2017): E3372–E3373.\n\n[15](12_Chapter4.xhtml#ch4fn15) Jonah 1:7, New American Standard Bible.\n\n[16](12_Chapter4.xhtml#ch4fn16) In hypothesis testing, these are commonly\ncalled type I (false positive) and type II (false negative) errors.\n\n[17](12_Chapter4.xhtml#ch4fn17) John Arbuthnot, “An Argument for Divine\nProvidence, Taken from the Constant Regularity Observ’d in the Births of Both\nSexes,” _Philosophical Transactions (1683–1775)_ 27 (1710): 186–190.\n\n[18](12_Chapter4.xhtml#ch4fn18) Stephen M. Stigler, _The History of\nStatistics: The Measurement of Uncertainty before 1900_ (Cambridge, MA:\nHarvard University Press, 1986), 225–226.\n\n[19](12_Chapter4.xhtml#ch4fn19) Jonah 3:4–5, New American Standard Bible.\n\n[20](12_Chapter4.xhtml#ch4fn20) Not only for Jonah is the question of lots and\nchance in Judeo-Christian religion problematic, but throughout the Hebrew\nBible, contradictory statements are given on the appropriateness of drawing\nlots. Take, for example, Proverbs 16:33: “The lot is cast into the lap, but\nits every decision is from God”; as contrasted by Leviticus 19:26: “Neither\nshall you practice enchantment, nor observe times” (usually taken to be a\nproclamation against divination or casting lots).\n\n[21](12_Chapter4.xhtml#ch4fn21) Karl Marx, “Theses on Feuerbach,” in _Karl\nMarx: Selected Writings_ , Lawrence Simon, ed. (Indianapolis: Hackett\nPublishing, 1994).\n\n[22](12_Chapter4.xhtml#ch4fn22) Alfred Sohn-Rethel, _Intellectual and Manual\nLabour: A Critique of Epistemology_ (Atlantic Highlands, NJ: Humanities Press,\n1978), 2.\n\n[23](12_Chapter4.xhtml#ch4fn23) Gerd Gigerenzer and Julian Marewski argue that\nstatistics and scientific inference have continually sought an “idol of a\nuniversal method for scientific inference” and, failing that, have continued\nto produce surrogates to stand in for this absent idol, which even when they\nare productive in limited uses, continue to fail in their universal\npretensions. Gerd Gigerenzer and Julian N. Marewski, “Surrogate Science: The\nIdol of a Universal Method for Scientific Inference,” _Journal of Management_\n41, No. 2 (2015): 421–440.\n\n5\\. Induction, Behavior and the Fractured Edifice of Frequentism\n\n[1](13_Chapter5.xhtml#ch5fn1) David Salsburg, _The Lady Tasting Tea: How\nStatistics Revolutionized Science in the Twentieth Century_ (New York: Henry\nHolt, 2002).\n\n[2](13_Chapter5.xhtml#ch5fn2) Kevin R. Murphy, Brett Myors and Allen Wolach,\n_Statistical Power Analysis: A Simple and General Model for Traditional and\nModern Hypothesis Tests_ , 4th ed. (New York: Routledge, 2014).\n\n[3](13_Chapter5.xhtml#ch5fn3) Ronald L. Wasserstein and Nicole A. Lazar, “The\nASA’s Statement on P-values: Context, Process, and Purpose,” _American\nStatistician_ 70, No. 2 (2016): 129–133.\n\n[4](13_Chapter5.xhtml#ch5fn4) Ronald Fisher, _The Design of Experiments_ (New\nYork: Hafner Press, 1971).\n\n[5](13_Chapter5.xhtml#ch5fn5) Tom Siegfried, “To Make Science Better, Watch\nOut for Statistical Flaws,” _Science News Context Blog_ , February 7, 2014,\ncited in Wasserstein and Lazar, “ASA’s Statement on P-values.”\n\n[6](13_Chapter5.xhtml#ch5fn6) Wasserstein and Lazar, “ASA’s Statement on\nP-values,” 131.\n\n[7](13_Chapter5.xhtml#ch5fn7) Jonathan Sterne and George Davey Smith provide a\ngood short history of p-values and their use in the medical field, as well as\nan accessible technical explanation of some of the issues with their use:\n“Sifting the Evidence: What’s Wrong with Significance Tests?,” _Physical\nTherapy_ 81, No. 8 (2001): 1464–1469.\n\n[8](13_Chapter5.xhtml#ch5fn8) “Methodology,” Predictive Heuristics official\nwebsite.\n\n[9](13_Chapter5.xhtml#ch5fn9) Wasserstein and Lazar, “ASA’s Statement on\nP-values.”\n\n[10](13_Chapter5.xhtml#ch5fn10) Fisher, _Design of Experiments_ , 16.\n\n[11](13_Chapter5.xhtml#ch5fn11) Fisher does suggest that one could formulate a\nhypothesis on this latter case, but warns that “this hypothesis could be\ndisproved by a single failure, but could never be proved by any finite amount\nof experimentation.” Ibid., 16.\n\n[12](13_Chapter5.xhtml#ch5fn12) Ronald Fisher, _The Genetical Theory of\nNatural Selection_ (Oxford: Oxford University Press, 1999).\n\n[13](13_Chapter5.xhtml#ch5fn13) Ibid.\n\n[14](13_Chapter5.xhtml#ch5fn14) UNESCO, _The Race Question_ , July 18, 1950,\n3.\n\n[15](13_Chapter5.xhtml#ch5fn15) Ibid., 27.\n\n[16](13_Chapter5.xhtml#ch5fn16) Karl Popper, _The Logic of Scientific\nDiscovery_ (New York: Routledge, 1992). For a discussion of the relationship\nbetween Fisher’s scientific induction and Popper’s account of scientific\nfalsifiability, particularly in reference to psychology, see Paul E. Meehl,\n“Theoretical Risks and Tabular Asterisks: Sir Karl, Sir Ronald, and the Slow\nProgress of Soft Psychology,” _Journal of Consulting and Clinical Psychology_\n46, No. 4 (1978): 806–834.\n\n[17](13_Chapter5.xhtml#ch5fn17) Gerd Gigerenzer, Zeno Swijtink and Lorraine\nDaston, _The Empire of Chance: How Probability Changed Science and Everyday\nLife_ (Cambridge, UK: Cambridge University Press, 1990), 105.\n\n[18](13_Chapter5.xhtml#ch5fn18) Ibid., 99.\n\n[19](13_Chapter5.xhtml#ch5fn19) Demetrios N. Kyriacou, “The Enduring Evolution\nof the P Value,” _Jama_ 315, No. 11 (2016): 1113–1115; David Jean Biau,\nBrigitte M. Jolles and Raphaël Porcher, “P Value and the Theory of Hypothesis\nTesting: An Explanation for New Researchers,” _Clinical Orthopaedics and\nRelated Research_ 468, No. 3 (2010): 885–892.\n\n[20](13_Chapter5.xhtml#ch5fn20) Andrew Gelman, “The Problems with P-values Are\nNot Just with P-Values,” _American Statistician_ 70, No. 2 (2016): S1–S2.\n\n[21](13_Chapter5.xhtml#ch5fn21) Ibid.\n\n[22](13_Chapter5.xhtml#ch5fn22) Jerzy Neyman, “‘Inductive Behavior’ as a Basic\nConcept of Philosophy of Science,” _Revue de l’Institut International de\nStatistique_ 25 (1957): 7–22.\n\n[23](13_Chapter5.xhtml#ch5fn23) Hacking argues that inductive behavior may\nstill be a form of “inference.” See “The Theory of Probable Inference,” in\n_Science, Belief, and Behaviour: Essays in Honor of R.B. Braithwaite_ , D.H.\nMellor, ed. (Cambridge, UK: Cambridge University Press, 1980), 141–160.\n\n[24](13_Chapter5.xhtml#ch5fn24) Roger Chou, Jennifer M. Croswell, Tracy Dana\net al., “Screening for Prostate Cancer: A Review of the Evidence for the U.S.\nPreventive Services Task Force,” _Annals of Internal Medicine_ 155, No. 11\n(2011): 762–771.\n\n[25](13_Chapter5.xhtml#ch5fn25) “Prostate-Specific Antigen (PSA) Test,”\nNational Cancer Institute official website.\n\n[26](13_Chapter5.xhtml#ch5fn26) In their grading system, they went from a “D”\nto a “C”; in short, from opposition to a neutral position.\n\n[27](13_Chapter5.xhtml#ch5fn27) Ronald Fisher, _Statistical Methods and\nScientific Inference_ , 3rd ed. (London: Collins Macmillan, 1973), 106–107.\n\n[28](13_Chapter5.xhtml#ch5fn28) Ronald Fisher, “Statistical Methods and\nScientific Induction,” _Journal of the Royal Statistical Society_ , Series B,\n_Methodological_ 17, No. 1 (1955): 77.\n\n[29](13_Chapter5.xhtml#ch5fn29) See ibid., 69–78; and Stephen Spielman, “A\nRefutation of the Neyman-Pearson Theory of Testing,” _British Journal for the\nPhilosophy of Science_ 24, No. 3 (1973): 201–222.\n\n[30](13_Chapter5.xhtml#ch5fn30) Fisher, “Statistical Methods and Scientific\nInduction,” 70.\n\n[31](13_Chapter5.xhtml#ch5fn31) Leonard Savage, “The Foundations of Statistics\nReconsidered,” _Proceedings of the Fourth Berkeley Symposium on Mathematical\nStatistics and Probability_ , Vol. 1, _Contributions to the Theory of\nStatistics_ (Berkeley: University of California Press, 1961).\n\n[32](13_Chapter5.xhtml#ch5fn32) Gigerenzer and Switjink, _Empire of Chance_ ;\nCarl J. Huberty, “Historical Origins of Statistical Testing Practices: The\nTreatment of Fisher versus Neyman-Pearson Views in Textbooks,” _Journal of\nExperimental Education_ 61, No. 4 (1993): 317–333.\n\n[33](13_Chapter5.xhtml#ch5fn33) Scholars writing on the effects of capitalism\nand the ideology of neoliberalism on the university have convincingly made\nsimilar claims. See Stanley Aronowitz, _The Knowledge Factory: Dismantling the\nCorporate University and Creating True Higher Learning_ (Boston: Beacon Press,\n2000); Henry Giroux, “Neoliberalism, Corporate Culture, and the Promise of\nHigher Education: The University as a Democratic Public Sphere,” _Harvard\nEducational Review_ 72, No. 4 (2002): 425–464; and Fred Moten and Stefano\nHarney, “The University and the Undercommons,” _Social Text_ 79, No. 2 (2004):\n101–115.\n\n[34](13_Chapter5.xhtml#ch5fn34) Quoted in Steven Weinberg, “The Crisis of Big\nScience,” _New York Review of Books_ , May 10, 2012.\n\n[35](13_Chapter5.xhtml#ch5fn35) Michel Serres, _The Parasite_ , trans.\nLawrence R. Schehr (Minneapolis: University of Minnesota Press, 2007).\n\n[36](13_Chapter5.xhtml#ch5fn36) John P.A. Ioannidis, “Why Most Published\nResearch Findings Are False,” _PLoS Medicine_ 2, No. 8 (2005): E124.\n\n[37](13_Chapter5.xhtml#ch5fn37) Joseph P. SImmons, Leif D. Nelson and Uri\nSimonsohn, “False-Positive Psychology: Undisclosed Flexibility in Data\nCollection and Analysis Allows Presenting Anything as Significant,”\n_Psychological Science_ 22, No. 11 (2011): 1359–1366.\n\n[38](13_Chapter5.xhtml#ch5fn38) Andrew Gelman and Christian P. Robert, “‘Not\nOnly Defended but Also Applied”: The Perceived Absurdity of Bayesian\nInference,” _American Statistician_ 67, No. 1 (2013): 1–5.\n\n[39](13_Chapter5.xhtml#ch5fn39) John Ioannidis, interviewed by Julia Belluz,\n“John Ioannidis Has Dedicated His Life to Quantifying How Science Is Broken,”\n_Vox_ , February 16, 2015.\n\n[40](13_Chapter5.xhtml#ch5fn40) Fisher, “Statistical Methods and Scientific\nInduction,” 70.\n\n[41](13_Chapter5.xhtml#ch5fn41) Gigerenze, Switjink and Daston, _Empire of\nChance_ , 108.\n\n[42](13_Chapter5.xhtml#ch5fn42) See Uri Simonsohn, Leif D. Nelson and Joseph\nP. Simmons, “P-Curve: A Key to the File-Drawer,” _Journal of Experimental\nPsychology: General_ 143, No. 2 (2014): 534.\n\n[43](13_Chapter5.xhtml#ch5fn43) For example, _The Journal of Negative Results\nin BioMedicine_. Such efforts are clearly on the whole positive and should be\nsupported, but it seems unlikely they will be able to save academic research\nfrom its crises on their own.\n\n[44](13_Chapter5.xhtml#ch5fn44) Lisa A. Bero, “Tobacco Industry Manipulation\nof Research,” _Public Health Report_ 120, No. 2 (2005): 200–208; Marcia\nAngell, “Industry-Sponsored Clinical Research: A Broken System,” _Journal of\nthe American Medical Association_ 300, No. 9 (2008): 1069–1071.\n\n[45](13_Chapter5.xhtml#ch5fn45) Donna Haraway, “Situated Knowledges: The\nScience Question in Feminism and the Privilege of Partial Perspective,”\n_Feminist Studies_ 14, No. 3 (1988): 575–599.\n\n[46](13_Chapter5.xhtml#ch5fn46) Alondra Nelson, _The Social Life of DNA: Race,\nReparations, and Reconciliation after the Genome_ (Boston: Beacon Press,\n2016).\n\n[47](13_Chapter5.xhtml#ch5fn47) Stephan Lewandowsky and Klaus Oberauer,\n“Motivated Rejection of Science,” _Current Directions in Psychological\nScience_ 25, No. 4 (2016): 217–222.\n\n[48](13_Chapter5.xhtml#ch5fn48) Ioannidis, “Why Most Published Research Is\nFalse,” 2005.\n\n6\\. Bayesian Statistics and the Problem with Frequentism\n\n[1](15_Chapter6.xhtml#ch6fn1) Stephen Stigler suggests that it may have been\npossible Nicholas Saunderson discovered the basic principles of Bayesian\napproaches prior to Thomas Bayes and informed David Hartley, who makes brief\nmention of this approach in his 1749 _Observations of Man_. Stigler provides\nan insightful and amusing weighing of the evidence and in the end uses\nBayesian analysis to calculate the probability of whether Bayes or Saunderson\ninitially made the discovery. “Who Discovered Bayes’s Theorem,” in _Statistics\non the Table_ (Cambridge, MA: Harvard University Press, 1999).\n\n[2](15_Chapter6.xhtml#ch6fn2) Dennis V. Lindley and Lawrence D. Phillips,\n“Inference for a Bernoulli Process (a Bayesian View),” _American Statistician_\n30, No. 3 (1976): 112–119; David J.C. MacKay, _Information Theory, Inference,\nand Learning Algorithms_ (Cambridge: Cambridge University Press, 2003); Adam\nP. Kubiak, “A Frequentist Solution to Lindley and Phillips’ Stopping Rule\nProblem in Ecological Realm,” _Zagadnienia Naukoznawstwa_ 50, No. 200 (2014):\n135–145.\n\n[3](15_Chapter6.xhtml#ch6fn3) There are 299 sequences that contain 3 or fewer\ntails out of 2^12 possibilities, which ends up being 7 percent.\n\n[4](15_Chapter6.xhtml#ch6fn4) The probability then becomes 134 out of 4,096\npossible combinations, or 3 percent.\n\n[5](15_Chapter6.xhtml#ch6fn5) For example, in regard to physics, see Karen\nBarad, _Meeting the Universe Halfway: Quantum Physics and the Entanglement of\nMatter and Meaning_ (Durham, NC: Duke University Press, 2007).\n\n[6](15_Chapter6.xhtml#ch6fn6) Leonard J. Savage, _The Foundations of\nStatistics_ , 2nd ed. (New York: Dover, 1972), 61–62.\n\n[7](15_Chapter6.xhtml#ch6fn7) Ward Edwards, Harold Lindman and Leonard J.\nSavage, “Bayesian Statistical Inference for Psychological Research,”\n_Psychological Review_ 70, No. 3 (1963): 193–242. See also, for example, Paul\nPharoah, “How Not to Interpret a P Value?” _Journal of the National Cancer\nInstitute_ 99, No. 4 (2007): 332–333.\n\n[8](15_Chapter6.xhtml#ch6fn8) It is often forgotten how prevalent early\nversions of Bayesian approaches were prior to Fisher. His book _Statistical\nMethods for Research Workers_ even begins with a refutation of “inverse\nprobability.”\n\n[9](15_Chapter6.xhtml#ch6fn9) Nick Srnicek, _Platform Capitalism_ (New York:\nPolity, 2017).\n\n[10](15_Chapter6.xhtml#ch6fn10) Steve Silberman, “The Quest for Meaning,”\n_Wired_ , February 1, 2000.\n\n[11](15_Chapter6.xhtml#ch6fn11) Stephen E. Fienberg, “When Did Bayesian\nInference Become ‘Bayesian’?,” _Bayesian Analysis_ 1, No. 1 (2006): 1–40;\nSharon B. McGrayne, _The Theory That Would Not Die: How Bayes’ Rule Cracked\nthe Enigma Code, Hunted Down Russian Submarines, and Emerged Triumphant from\nTwo Centuries of Controversy_ (New Haven, CT: Yale University Press, 2012).\n\n[12](15_Chapter6.xhtml#ch6fn12) Ronald A. Fisher, _Statistical Methods for\nResearch Workers_ (Edinburgh, UK: Oliver & Boyd, 1934), 9.\n\n[13](15_Chapter6.xhtml#ch6fn13) Ibid., 10.\n\n[14](15_Chapter6.xhtml#ch6fn14) Using Bayes’s theorem, the math is as follows.\nIn this case, _B_ is the probability that someone has the disease; _A_ is the\nprobability that we have a positive test. The only thing that is not\nimmediately known is the probability of _A_ , but that is simply the\nprobability of a true positive (times the percentage of those with the\ndisease) plus the probability of a false positive (times the percentage\nwithout the disease):\n\nP(_A_) = probability of a positive test for someone with the disease *\npercentage of people with the disease + probability of false positive *\npercentage of people without the disease\n\nP(_A_) = 0.95 * 0.02 + 0.01 * 0.98\n\nP(_A_) = 0.0288\n\nThe rest of the information we already know. The probability of _A_ given _B_\nis the accuracy (0.95) and the probability of _B_ (.02):\n\nP(_B_ |_A_) = 0.95 * 0.02 / 0.0288\n\nP(_B_ |_A_) = 0.66\n\n[15](15_Chapter6.xhtml#ch6fn15) Often a number of preprocessing steps and/or\nnormalization are performed to make the analysis more accurate, e.g.,\nstemming, term frequencyinverse document frequency and Laplace smoothing.\n\n[16](15_Chapter6.xhtml#ch6fn16) David D. Lewis, “Naive (Bayes) at Forty: The\nIndependence Assumption in Information Retrieval,” in _Machine Learning: ECML\n1998_ , Claire Nédellec and Céline Rouveirol, eds., _Lecture Notes in\nArtificial Intelligence_ , Vol. 1398 (Berlin: Springer, 1998); Harry Zhang,\n“The Optimality of Naive Bayes,” in _Proceedings of the Seventeenth\nInternational Florida Artificial Intelligence Research Society Conference_ ,\nValerie Barr and Zdravko Markov, eds. (Menlo Park, CA: AAAI Press, 2004),\n562–567.\n\n[17](15_Chapter6.xhtml#ch6fn17) Peter Norvig, quoted in McGrayne, _The Theory\nThat Would Not Die_ , 244.\n\n7\\. Bayesian Metaphysics and the Foundation of Knowledge\n\n[1](16_Chapter7.xhtml#ch7fn1) Ward Edwards, Harold Lindman and Leonard J.\nSavage, “Bayesian Statistical Inference for Psychological Research,”\n_Psychological Review_ 70, No. 3 (1963): 193–242.\n\n[2](16_Chapter7.xhtml#ch7fn2) Another major difference between Bayesian and\nfrequentist statistics is the former’s general preference for posterior\ndistributions rather than point estimates of statistics (e.g., mean).\nMathematically and computationally, this shift is of the utmost importance. It\nis worth noting that the use of distributions aids in the calculation of\nuncertainty and ability to represent possible but unlikely events, further\ncementing Bayesian approaches’ relevance to our present moment. Bradley P.\nCarlin and Thomas A. Lewis, _Bayes and Empirical Bayes Methods for Data\nAnalysis_ , 2nd ed. (Boca Raton, FL: Chapman & Hall / CRC, 2000), 12.\n\n[3](16_Chapter7.xhtml#ch7fn3) This array of larger states of various fields of\nknowledge are called metaanalyses. These meta-analyses have a relatively long\nhistory, going back to at least Karl Pearson’s frequentist work, which held\nformidable effect on the practice, at the very beginning of the twentieth\ncentury. This 1904 paper is generally taken as the first modern meta-analysis:\nKarl Pearson, “Report on Certain Enteric Fever Inoculation Statistics,”\n_British Medical Journal_ 2, No. 2288 (1904): 1243–1246. While frequentists\npioneered much of this work, Bayesian approaches offer the possibility of\npushing them even further to better reflect the state of fields and belief in\ntheir discoveries.\n\n[4](16_Chapter7.xhtml#ch7fn4) In an often-cited footnote found in the\nfifteenth chapter of Volume 1 of _Capital_ , titled “Machinery and Large-Scale\nIndustry,” Marx writes: “Technology reveals the active relation of man to\nnature, the direct process of the production of his life, and thereby it also\nlays bare the processes of the production of his life, and thereby it also\nlays bare the process of the production of social relations of his life, and\nof the mental conceptions that flow from those relations.” _Capital_ , Vol. 1\n(London: Penguin Books, 1990), 493n4. Given Marx’s emphasis on the historical\nspecificity of social relations to a given mode of production, this statement\nclearly indicates the extent to which the technologies we use today are\nimmanent to and revelatory of the social relations that make possible the\nvalorization of capital in our current historical moment. Gilles Deleuze makes\na similar argument when he writes, “Types of machines are easily matched with\neach type of society—not that machines are determining, but because they\nexpress the social forms capable of generating them and using them.”\n“Postscript on the Societies of Control,” _October_ 59 (1992): 6.\n\n[5](16_Chapter7.xhtml#ch7fn5) Richard Price, introduction to Thomas Bayes,\n“LII: An Essay Towards Solving a Problem in the Doctrine of Chances. By the\nLate Rev. Mr. Bayes, F.R.S. Communicated by Mr. Price, in a letter to John\nCanton, A.M.F.R.S,” _Philosophical Transactions_ 53 (1763): 370–418.\n\n[6](16_Chapter7.xhtml#ch7fn6) John Arbuthnot, “An Argument for Divine\nProvidence, Taken from the Constant Regularity Observ’d in the Births of Both\nSexes,” _Philosophical Transactions_ 27 (1710): 186–190.\n\n[7](16_Chapter7.xhtml#ch7fn7) Thomas Bayes, _Divine Benevolence; or An Attempt\nto Prove That the Principal End of the Divine Providence and Government Is the\nHappiness of His Creatures_ (London: John Noon, 1731).\n\n[8](16_Chapter7.xhtml#ch7fn8) Jean-Pierre Dupuy, _The Mark of the Sacred_\n(Redwood City, CA: Stanford University Press, 2013).\n\n[9](16_Chapter7.xhtml#ch7fn9) Bruno de Finetti, “Foresight: Its Logical Laws,\nIts Subjective Sources,” in _Studies in Subjective Probability_ , Henry E.\nKyburg and Howard E. Smokler, eds. (New York: Wiley, 1964), 93–158. Frank\nRamsey hinted at this solution in his “Truth and Probability,” in _Readings in\nFormal Epistemology_ (New York: Springer, 2016 [1931]), 21–45. However, the\nDutch book argument was fully developed by de Finetti.\n\n[10](16_Chapter7.xhtml#ch7fn10) See Dennis Lindley, _Understanding\nUncertainty_ (Hoboken, NJ: John Wiley & Sons, 2013), esp. Section 5.7.\n\n[11](16_Chapter7.xhtml#ch7fn11) Sharon B. McGrayne, _The Theory That Would Not\nDie: How Bayes’ Rule Cracked the Enigma Code, Hunted Down Russian Submarines,\nand Emerged Triumphant from Two Centuries of Controversy_ (New Haven, CT: Yale\nUniversity Press, 2012).\n\n[12](16_Chapter7.xhtml#ch7fn12) A funny thing happens: the quicker we run “to\nthe things themselves,” the faster we end up dealing with subjective theories\nof knowledge, or, at the very least, with theories that must first deal with\nthe subject, as Edmund Husserl brilliantly demonstrated. In this light, what\nshould interest us is not how things are subjective or how they are objective,\nbut rather the techniques by which the subjective and objective are sutured\ntogether; how subjective knowledge is made to appear objective; in short:\nobjectification.\n\nWe can see this explicitly in object-oriented ontology. Graham Harman, in his\ncareful ontological analysis, ends up insisting that the outcome of philosophy\nis that we have to pay and pay. For Harman, all philosophical assumptions must\nbe brought to market and the proper price paid for their intellectual\nassumptions. For instance, he says of Giordano Bruno’s philosophy, “The\ntechnical term for this maneuver is ‘highway robbery,’ since Bruno is trying\nto preserve individual forms without paying for them.” “On the Undermining of\nObjects: Grant, Bruno, and Radical Philosophy,” in _The Speculative Turn:\nContinental Materialism and Realism_ , Levi Bryant, Nick Srnicek and Graham\nHarman, eds. (Melbourne: re. press, 2011), 35. Elsewhere, Plato and Socrates\nmust “pay the immense price of reducing actors as we know them to flickering\nshadows on a cave wall.” _Prince of Networks: Bruno Latour and Metaphysics_\n(Melbourne: re. press, 2010), 95. While the use of this turn of phrase is not\ndamning evidence on its own, it should give us pause that a philosophy based\non the solidity of objects advances through the keeping of accounts on who has\nto pay for what. This is precisely what objectification in Marx’s sense\nsuggests: objects serve to account for the debts and credits of obfuscated\nsocial relations.\n\nBryant’s text on object-oriented philosophy is particularly exemplary in this\nregard. The title, _Democracy of Objects_ (London: Open Humanities Press,\n2013), a phrase he draws from Bogost, is striking; especially in so much as he\ndismisses representation as a mere epistemological question, despite\nrepresentation being the sine qua non of contemporary democracy. In the\nintroduction to the text, Bryant talks at length about representation being a\ncentral question to both realist and antirealist philosophies, which object-\noriented ontology is able to sidestep by seeking to “reject the\nepistemological realism of other realist philosophies, taking leave of the\nproject of policing representations” (26–27), a step which he says allows\nphilosophy to pluralize the gap between object and representation. While I am\nsympathetic to this desire to pluralize representation and to consider the\nreal beyond representation, representation remains the central question of\ndemocracy today. Especially if we understand that representation in its very\nfunction is a process of exclusion, it seems that a democracy of objects (or\nof anything) must provide a singular theory of representation. Even if we\nadmit a plurality of possible “representations,” democracy must select one and\npolice its functioning. One vote per object over the age of eighteen; polling\nstations are only open from 7 a.m. to 8 p.m.!\n\nBryant clearly has something very different in mind when he uses the term\n“democracy,” than that meant by the political question of democracy and its\nfuture that confronts us today. For Bryant, this is “a democracy of strange\nstrangers. Where there is no hegemon that stands above and outside withdrawal\nas a full actuality, there is only a flat plane composed of strange strangers”\n(269–270). While we can sympathize with these anti-hegemonic desires, it is\nodd to call it a democracy if there is no hegemonic function to represent the\ntotality. And Bryant goes on to deny that there is any “world,” as a unifying\nfunction for the totality. But what could be meant by democracy if there is no\nworld function to, even if out of nothing, create a world, a politics or a\ndemos? Even if the real is flat, the demos, the people, must be constructed\nand represented. We must conclude instead that what Bryant claims to mean by\n“democracy” is in reality an anarchy of objects, a Hobbesian war of all\nagainst all, in short, the idealized market of capitalism with no democratic\noversight.\n\nMuch more can and has been said about the relationship between democracy and\ncapitalism, but for the time being, we will have to satisfy ourselves with the\nobservation that Bryant’s democracy of objects is a democracy without politics\nor representation. It is a democracy truly of the market; it is an objectified\ndemocracy, where the only politics are the politics of individual competitive\nrelations between objects at market. Bryant’s attempts to separate the\nontological from the political precludes any understanding of the ways in\nwhich the global system of computation or capital are reflected in the very\nontological distinctions that are made and thus the analysis replicates the\nvery structure of commodity relations. While Bryant’s text is only one\ncautionary tale, we can see in it the very real risks of ignoring the\nfundamental mysteries at the heart of these relations and the relevance still\ntoday of Marx’s writings on object-commodities.\n\n[13](16_Chapter7.xhtml#ch7fn13) Bruno de Finetti, “Probabilism: A Critical\nEssay on the Theory of Probability and on the Value of Science,” _Erkenntnis_\n31 (1989): 170. While in his later life, de Finetti moved to the left, even\nbeing arrested briefly for publishing letters of conscientious objectors in a\nnewspaper for the Italian Radical Party, in the 1930s when he was developing\nand outlining his theories of probability he was a supporter of fascism. He\nconcludes a short essay on the nature of scientific work by proclaiming: “That\nimpeccable rational mechanics of the perfect civilian regime of the peoples,\nconforming to the rights of man and various other immortal principles! October\nof ’22! It seemed to me I could see them, these Immortal Principles, as filthy\ncorpses in the dust. And with what conscious and ferocious voluptuousness I\nfelt myself trampling them, marching to hymns of triumph, obscure but faithful\nBlackshirt” (219).\n\n[14](16_Chapter7.xhtml#ch7fn14) Ibid., 171.\n\n[15](16_Chapter7.xhtml#ch7fn15) Leonard J. Savage, _The Foundations of\nStatistics_ , 2nd ed. (New York: Dover, 1972), 159.\n\n[16](16_Chapter7.xhtml#ch7fn16) Ibid., iv.\n\n[17](16_Chapter7.xhtml#ch7fn17) Graciela Chichilnisky, “The Foundations of\nStatistics with Black Swans,” _Mathematical Social Sciences_ 59, No. 2 (2010):\n184–192.\n\n[18](16_Chapter7.xhtml#ch7fn18) See, for example, Alan Hájek, “Scotching Dutch\nBooks?,” _Philosophical Perspectives_ 19, No. 1 (2005): 139–151; Ian Hacking,\n”Slightly More Realistic Personal Probability,” _Philosophy of Science_ 34,\nNo. 4 (1967): 311–325; and Frederic Schick, “Dutch Bookies and Money Pumps,”\n_Journal of Philosophy_ 83, No. 2 (1986): 112–119. Hájek states: “The Dutch\nBook argument, in the form that has become a philosophical staple, is simply\ninvalid, and to the extent that we have bought it for so many years we have,\nas it were, been Dutch Booked ourselves; indeed, in endorsing it we have been\nguilty of a certain form of incoherence. However, like Route 66, the argument\ncan be reconstructed, or better still, restored” (139).\n\n[19](16_Chapter7.xhtml#ch7fn19) For a general overview of these concerns, see\nJohn Earman, _Bayes or Bust? A Critical Examination of Bayesian Confirmation\nTheory_ (Cambridge, MA: MIT Press, 1996), 38–50. Earman ultimately concludes\nthat the general Dutch book arguments, along with some other related\narguments, are in the end convincing that the probability calculus can\nadequately describe subjective beliefs.\n\n[20](16_Chapter7.xhtml#ch7fn20) Andrew Gelman, “Bayes, Jeffreys, Prior\nDistributions and the Philosophy of Statistics,” _Statistical Science_ 24, No.\n2 (2009): 176–178. Gelman here also explicitly states his antipathy to\nSavage’s work: “I confess to having found Savage to be nearly unreadable, a\nbook too much of a product of its time in its enthusiasm for game theory as a\nsolution to all problems … When it comes to Cold War-era foundational work on\nBayesian statistics, I much prefer the work of Lindley.” But, his concerns are\nmore stylistic than substantive, especially considering that Lindley also\nmakes recourse to Dutch book arguments.\n\n[21](16_Chapter7.xhtml#ch7fn21) Alfred Sohn-Rethel, _Intellectual and Manual\nLabour: A Critique of Epistemology_ (Atlantic Highlands, NJ: Humanities Press,\n1978).\n\n[22](16_Chapter7.xhtml#ch7fn22) See Wendy Brown, _Undoing the Demos:\nNeoliberalism’s Stealth Revolution_ (Cambridge, MA: MIT Press, 2015).\n\n[23](16_Chapter7.xhtml#ch7fn23) Gerd Gigerenzer and Julian N. Marewski suggest\nthat the flood of false results and faked data are a result of the belief in a\nuniversal statistical method following the “inference revolution” in the\n1950s: “Surrogate Science: The Idol of a Universal Method for Scientific\nInference,” _Journal of Management_ 41, No. 2 (2015): 421–440. This is likely\ncorrect, but it should be added that this desire for a universal method that\ncan implemented mechanically is at least in part the result of contemporary\nmodes of knowledge production.\n\n[24](16_Chapter7.xhtml#ch7fn24) Bernard Stiegler addresses this stupefying\ntendency directly in _States of Shock: Stupidity and Knowledge in the 21st\nCentury_ (Hoboken, NJ: John Wiley & Sons, 2015).\n\n[25](16_Chapter7.xhtml#ch7fn25) Manuel DeLanda, “Markets and Antimarkets in\nthe World Economy,” in _Technoscience and Cyberculture_ , Stanley Aronowitz,\nBarbara Martinsons and Michael Menser, eds. (New York: Routledge, 1996)\n181–194. In this essay, DeLanda argues for a synthetic approach to an analysis\nof capitalism that can demonstrate the historical emergence of monopolies as\nthe natural outgrowth of capitalism rather than markets.\n\n[26](16_Chapter7.xhtml#ch7fn26) See Wendy Hui Kyong Chun, “Queerying\nHomophily,” in Clemens Apprich, Wendy Hui Kyong Chun, Florian Cramer and Hito\nSteyerl, _Pattern Discrimination_ (Lüneburg: Meson Press, 2018), 59–97.\n\n[27](16_Chapter7.xhtml#ch7fn27) Franco “Bifo” Berardi, _The Uprising: On\nPoetry and Finance_ (Los Angeles: Semiotext(e), 2012); Alexander Galloway,\n“The Poverty of Philosophy: Realism and Post-Fordism,” _Critical Inquiry_ 39,\nNo. 2 (2013): 347–366.\n\n8\\. Automated Abstractions and Alienation\n\n[1](17_Chapter8.xhtml#ch8fn1) This chapter is coauthored with Cengiz Salman.\n\n[2](17_Chapter8.xhtml#ch8fn2) Hasana Sharp, “Melancholy, Anxious, Ek-static\nSelves: Feminism between Eros and Thanatos,” _Symposium_ 11, No. 2 (2007):\n315–331.\n\n[3](17_Chapter8.xhtml#ch8fn3) However, even these early deistic forms of\nabstraction bear some relation to exchange. French philosopher Jean-Pierre\nVernant locates the origins of Greek abstraction in currency’s capacity to\nstandardize exchange. Vernant writes, “For the old image of wealth as _hybris_\n[i.e., hubris or insolence]—so charged with affective force and religious\nimplications—legal tender substituted the abstract idea of _nomisa_ [i.e.,\nmoney], a social standard of value, a rational contrivance that allowed for a\ncommon measure of diverse realities, and thus equalized exchange as a social\nrelationship.” _The Origins of Greek Thought_ (Ithaca, NY: Cornell University\nPress, 1982), 95. Like Vernant, Alfred Sohn-Rethel also recognizes that the\nreal abstraction of money in the Greek polis established a generalized system\nof exchange, rendering qualitatively specific products of labor, including the\nlabor of the enslaved, equivalent. More significantly, for Sohn-Rethel, the\nemergence of money in Greece served as the material a priori condition\nnecessary for the construction of conceptual abstractions peculiar to its\nphilosophical tradition: the one, the ideal, the many, and even the divine\nabstractions of gods like Pax (_Intellectual and Manual Labor_ , 58–59).\nConsequently, money’s functional capacity to exchange in Greece served as a\nconstitutive factor in engendering the ideal abstractions specific to early\nGreek philosophical thought. Although the ancient Greek economy lacks the\nnecessary features of a capitalist economy, e.g., generalized wage labor,\nVernant and Sohn-Rethel clearly demonstrate how money in Greece prefigured\ncapital’s capacity to produce abstractions.\n\n[4](17_Chapter8.xhtml#ch8fn4) Vandana Shiva, _Staying Alive: Women, Ecology,\nand Development_ (London: Zed Books, 1988), 22–23.\n\n[5](17_Chapter8.xhtml#ch8fn5) Max Horkheimer and Theodor W. Adorno, _Dialectic\nof Enlightenment_ , trans. Edmund Jephcott (Palo Alto, CA: Stanford University\nPress, 2002), 9.\n\n[6](17_Chapter8.xhtml#ch8fn6) Ibid., 9.\n\n[7](17_Chapter8.xhtml#ch8fn7) As Ian Baucom, tracing the 1781 massacre of 132\nenslaved Africans aboard the _Zong_ (which was ordered by the ship’s captain)\nand the ensuing court case over whether insurers were to pay the ship’s\nowners, argues that even notions of justice and freedom are founded on ideas\nof exchangeability. He states: “The genius of insurance, the secret of _its_\ncontribution to finance capitalism, is its insistence that the real test of\nsomething’s value comes not at the moment it is made or exchanged but at the\nmoment it is lost or destroyed … In a money culture or an insurance culture\nvalue survives its objects, and in doing so does not just reward individual\nself-interest of the insured object’s owner, butretrospectively confirms the\nsystem-wide conviction that the value was _always_ autonomous from its object,\n_always_ only a matter of agreement.” Value under capitalism always points\ntoward the ghost of a departed quantity, and what departs, or more accurately\nis made to depart, is invariably the most marginalized and excluded life.\nWhile perhaps there is a certain danger in any notion of exchangeability that\nmust always be guarded against, finance capital seems to erect especially\nviolent, inequitable and egregious forms of exchange. _Specters of the\nAtlantic: Finance Capital, Slavery, and the Philosophy of History_ (Durham,\nNC: Duke University Press, 2005), 95.\n\n[8](17_Chapter8.xhtml#ch8fn8) Jasbir K. Puar, _The Right to Maim: Debility,\nCapacity, Disability_ (Durham, NC: Duke University Press, 2017), xxv.\n\n[9](17_Chapter8.xhtml#ch8fn9) Ramon Amaro describes the situation precisely in\nrelation to face recognition: “To merely include a representational object\ninto a computational milieu that has already positioned the white object as\nthe prototypical characteristic catalyzes disruption on the level\nsuperficiality. From this view, the white object remains whole, while the\nobject of difference is seen as alienated, fragmented, and lacking in\ncomparison. From this perspective, the position of the black technical object\n(as Fanon and Wynter have thoroughly articulated) is lodged in a recurrent\ndialectic, where it attempts to valorize or recapture black life from within\nthe confines of normalized logics while simultaneously desiring to disrupt its\nhold.” “As If,” _e-flux_ , February 14, 2019.\n\n[10](17_Chapter8.xhtml#ch8fn10) Sergey Brin and Lawrence Page, “The Anatomy of\na Large-Scale Hypertextual Web Search Engine,” _Computer Networks and ISDN\nSystems_ 30 (1998): 107–117.\n\n[11](17_Chapter8.xhtml#ch8fn11) John Markoff, “In a Big Network of Computers,\nEvidence of Machine Learning,” _New York Times_ , June 25, 2012.\n\n[12](17_Chapter8.xhtml#ch8fn12) Quoc V. Le, Marc’Aurelio Ranzato, Rajat Monga\net al., “Building High-Level Features Using Large Scale Unsupervised\nLearning,” in _Proceedings of the 29th International Conference on Machine\nLearning_ (2012).\n\n[13](17_Chapter8.xhtml#ch8fn13) Robert N. Charette, “Michigan’s MiDAS\nUnemployment System: Algorithm Alchemy Created Lead, Not Gold,” _IEEE\nSpectrum_ , January 24, 2018.\n\n[14](17_Chapter8.xhtml#ch8fn14) Jackie Wang, _Carceral Capitalism_ (Cambridge:\nMIT Press, 2018), 16–17, 43–48.\n\n[15](17_Chapter8.xhtml#ch8fn15) This mobile, ever-shifting epistemic terrain\nis structurally aligned with that theorized by Gilles Deleuze and Zygmut\nBauman. Zygmunt Bauman, _Liquid Modernity_ (Malden, MA: Polity, 2000);\nDeleuze, “Postscript on the Societies of Control,” _October_ 59 (1992): 6.\n\n[16](17_Chapter8.xhtml#ch8fn16) Gabriel J.X. Dance, Michael LaForgia and\nNicholas Confessore, “As Facebook Raised a Privacy Wall, It Carved an Opening\nfor Tech Giants,” _New York Times_ , December 18, 2018.\n\n[17](17_Chapter8.xhtml#ch8fn17) Karl Marx, _Capital_ , Vol. 1 (London: Penguin\nBooks, 1990), 873–940.\n\n[18](17_Chapter8.xhtml#ch8fn18) David Harvey, _A Companion to Marx’s Capital_\n(London: Penguin Books, 1990), 295.\n\n[19](17_Chapter8.xhtml#ch8fn19) James Boyle, “The Second Enclosure Movement,”\n_Renewal: A Journal of Labour Politics_ 15, No. 4 (2007): 17–24.\n\n[20](17_Chapter8.xhtml#ch8fn20) While some economists believe that the legal\nenforcement of intellectual property rights spurs innovation and economic\ngrowth, scholars critical of such privatization have powerfully argued that\npatents both reproduce existing inequalities by concentrating wealth and\nknowledge in the hands of a privileged class of capitalists, and limit the\ncapacities of others (particularly those in the developing world) from\naccessing and applying such information for social, cultural and economic\nbenefit. See Ronald V. Bettig, _Copyrighting Culture: The Political Economy of\nIntellectual Property_ (Boulder, CO: Westview Press, 1996); C. Ford Runge and\nEdi Defrancesco, “Exclusion, Inclusion, and Enclosure: Historical Commons and\nModern Intellectual Property,” _World Development_ 34, No. 10 (2006):\n1713–1727.\n\n[21](17_Chapter8.xhtml#ch8fn21) Karl Marx, _Grundrisse: Foundations of the\nCritique of Political Economy_ (London: Penguin, 2005, repr.), 706.\n\n[22](17_Chapter8.xhtml#ch8fn22) Ishmael Burdeau, “The Last Great Enclosure:\nThe Crisis of the General Intellect,” _Journal of Labor and Society_ 18, No. 4\n(2015): 649–663.\n\n[23](17_Chapter8.xhtml#ch8fn23) Farshad Araghi, “The Great Global Enclosure of\nOur Times,” in _Hungry for Profit_ , Fred Magdoff, John Bellamy Foster and\nFrederick M. Buttel, eds. (New York: Monthly Review Press, 2000), 145–160;\nMidnight Notes, “The New Enclosures,” _Midnight Notes_ 10 (1990): 1–100.\n\n[24](17_Chapter8.xhtml#ch8fn24) Vandana Shiva, _Biopiracy: The Plunder of\nNature and Knowledge_ (Berkeley: North Atlantic Books, 2016).\n\n[25](17_Chapter8.xhtml#ch8fn25) Nick Srnicek, _Platform Capitalism_ (New York:\nPolity, 2017).\n\n[26](17_Chapter8.xhtml#ch8fn26) Rian Wanstreet, “America’s Farmers Are\nBecoming Prisoners to Agriculture’s Technological Revolution,” _Motherboard_ ,\nMarch 8, 2018.\n\n[27](17_Chapter8.xhtml#ch8fn27) Paolo Virno, “General Intellect,” _Historical\nMaterialism_ 15, No. 3 (2007): 6.\n\n[28](17_Chapter8.xhtml#ch8fn28) This reading is especially present in Antonio\nNegri and others’ writing in _Futur Antérieur_. For an excellent overview see\nNick Dyer-Witheford, “Cyber-Negri: General Intellect and Immaterial Labor,” in\n_The Philosophy of Antonio Negri_ , Timothy Murphy and Abdul-Karim Mustapha,\neds. (London: Pluto Press, 2005).\n\n[29](17_Chapter8.xhtml#ch8fn29) Ibid., 143.\n\n[30](17_Chapter8.xhtml#ch8fn30) Fisher, “Statistical Methods and Scientific\nInduction,” 70.\n\n[31](17_Chapter8.xhtml#ch8fn31) Jathan Sadowski, “Potemkin AI,” _Real Life_ ,\nAugust 6, 2018.\n\n[32](17_Chapter8.xhtml#ch8fn32) Nick Bilton, “How Elizabeth Holmes’s House of\nCards Came Tumbling Down,” _Vanity Fair_ , October 2016.\n\n[33](17_Chapter8.xhtml#ch8fn33) Many have convincingly critiqued Hayek’s\nposition since his writing. See David Harvey, _A Brief History of\nNeoliberalism_ (Oxford: Oxford University Press, 2005); Jamie Peck,\n_Constructions of Neoliberal Reason_ (Oxford: Oxford University Press, 2010);\nDieter Plehwe, “Introduction,” in _The Road to Mont Pèlerin: The Making of the\nNeoliberal Thought Collective_ , Philip Mirowski and Dieter Plehwe, eds.\n(Cambridge, MA: Harvard University Press, 2009); Rob Van Horn and Philip\nMirowski, “The Rise of the Chicago School of Economics and the Birth of\nNeoliberalism,” in _The Road from Mont Pèlerin_ ; and Philip Mirowski, _Never\nLet a Serious Crisis Go to Waste: How Neoliberalism Survived the Financial\nMeltdown_ (London and New York: Verso, 2013).\n\n[34](17_Chapter8.xhtml#ch8fn34) Friedrich Hayek, “Economics and Knowledge,” in\n_L.S.E. Essays on Cost_ , James M. Buchanan and George F. Thirlby, eds. (New\nYork: New York University Press, 1973), 66.\n\n[35](17_Chapter8.xhtml#ch8fn35) David Harvey, _The Condition of Postmodernity:\nAn Enquiry into the Origins of Cultural Change_ (Cambridge, MA: Blackwell,\n1992).\n\n[36](17_Chapter8.xhtml#ch8fn36) Maurizio Lazzarato, “Immaterial Labor,” in\n_Radical Thought in Italy: A Potential Politics_ , Paolo Virno and Michael\nHardt, eds. (Minneapolis: University of Minnesota Press, 1996), 133–146; Ivan\nAscher, _Portfolio Society: On the Capitalist Mode of Prediction_ (Cambridge:\nMIT Press, 2016).\n\n[37](17_Chapter8.xhtml#ch8fn37) Virno, “General Intellect,” 5.\n\n[38](17_Chapter8.xhtml#ch8fn38) Antonio Negri, _The Politics of Subversion: A\nManifesto for the Twenty-First Century_ (Cambridge, UK: Polity, 1989).\n\n[39](17_Chapter8.xhtml#ch8fn39) Malcolm Harris, “Glitch Capitalism: How\nCheating AIs Explain Our Glitchy Society,” _New York Magazine_ , April 23,\n2018.\n\n[40](17_Chapter8.xhtml#ch8fn40) Evgeny Morozov, “Digital Socialism?,” _New\nLeft Review_ 116/117 (March– June 2019).\n\n[41](17_Chapter8.xhtml#ch8fn41) Nick Srnicek and Alex Williams, _Inventing the\nFuture: Postcapitalism and a World without Work_ (London and New York: Verso,\n2016).\n\n[42](17_Chapter8.xhtml#ch8fn42) Robin Harding, “Kobe Steel Admits It Falsified\nData on Aluminum and Copper Parts,” _Financial Times_ , October 8, 2017.\n\n[43](17_Chapter8.xhtml#ch8fn43) Richard Seymour, “Marxism, the Bourgeoisie and\nCapitalist Imperialism,” _Lenin’s Tomb_ (blog), April 30, 2006.\n\n[44](17_Chapter8.xhtml#ch8fn44) See Yuk Hui, _On the Existence of Digital\nObjects_ (Minneapolis: University of Minnesota Press, 2016).\n\n[45](17_Chapter8.xhtml#ch8fn45) House Judiciary Committee, “Hearing on\nTransparency and Accountability: Examining Google and Its Data Collection, Use\nand Filtering Practices,” 115th Congress, December 11, 2018.\n\n[46](17_Chapter8.xhtml#ch8fn46) Antoinette Rouvroy, “The End(s) of Critique:\nData Behaviorism versus Due Process,” in _Privacy, Due Process and the\nComputational Turn: The Philosophy of Law Meets the Philosophy of Technology_\n, Mireille Hildebrandt and Katja de Vries, eds. (New York: Routledge, 2013),\n143–168.\n\n[47](17_Chapter8.xhtml#ch8fn47) Sylvia Wynter, “Unsettling the Coloniality of\nBeing/Power/Truth/Freedom: Towards the Human, after Man, Its\nOverrepresentation—An Argument,” _New Centennial Review_ 3, No. 3 (2003):\n257–337.\n\n[48](17_Chapter8.xhtml#ch8fn48) Practices of redlining in Michigan’s major\ncities involved the systematic denial of mortgage financing based on race,\ngeography, or other noneconomic criteria until such practices were de jure\nabolished with state legislation in 1978. For an explanation of the law, see\nJ. Richard Johnson, “Michigan’s Redlining Law,” _Detroit College of Law Review\n1978_ , No. 4 (Winter 1978): 599–624. For a history of redlining and white\nflight in Detroit, see Thomas J. Sugrue, _The Origins of the Urban Crisis:\nRace and Inequality in Postwar Detroit_ (Princeton, NJ: Princeton University\nPress, 2005). Practices of redlining remain de facto prevalent in Detroit and\nLansing, Michigan, in 2018. The president of the Michigan Mortgage Lenders\nAssociation recently suggested that the automation of lending makes it\ndifficult to recognize practices of discrimination. See Aaron Glantz and\nEmmanuel Martinez, “Detroit-Area Blacks Twice as Likely to Be Denied Home\nLoans,” _Detroit News_ , February 15, 2018. For more information about\ncontemporary practices of redlining in Michigan and the US more broadly, see\nAaron Glantz and Emmanuel Martinez, “For People of Color, Banks Are Shutting\nthe Door to Homeownership,” _Reveal_ , February 15, 2018.\n\n[49](17_Chapter8.xhtml#ch8fn49) Jordan Pearson, “AI Could Ressurect a Racist\nHousing Policy,” _Motherboard_ , February 2, 2017.\n\n[50](17_Chapter8.xhtml#ch8fn50) In a blog post, philosopher Adam Kotsko\nrecognizes Scott Ferguson and Anna Kornbluh as respectively conducting\nresearch that conceptualizes the liberatory potential of abstractions. See\nAdam Kotsko, “Reading Agamben with Ferguson,” _Provocations_ 2 (2018). While\nKornbluh’s work on the topic is unpublished as of this writing, Scott Ferguson\ndraws on the resources of modern monetary theory (MMT) to argue that money, if\nfreed from the parochial concerns of reactionary politicians, could function\nas an unconstrained and infinite resource for addressing many contemporary\nsocial issues while also constructing a broader public. _Declarations of\nDependence: Money, Aesthetics, and the Politics of Care_ (Lincoln, NE:\nUniversity of Nebraska Press, 2018).\n\n[51](17_Chapter8.xhtml#ch8fn51) Srnicek and Williams, _Inventing the Future_ ,\n82.\n\n[52](17_Chapter8.xhtml#ch8fn52) Marx, _Grundrisse_ , 693.\n\n[53](17_Chapter8.xhtml#ch8fn53) Franco “Bifo” Berardi, _The Soul at Work: From\nAlienation to Autonomy_ (Los Angeles: Semiotext(e), 2009), 96.\n\n[54](17_Chapter8.xhtml#ch8fn54) See for example Moishe Postone’s critique of\n“traditional” Marxism’s focus on capitalism as largely a problem of\ndistribution. _Time, Labor, and Social Domination_.\n\n[55](17_Chapter8.xhtml#ch8fn55) Saidiya Hartman, _Scenes of Subjection:\nTerror, Slavery, and Self-Making in Nineteenth-Century America_ (Oxford:\nOxford University Press, 1997).\n\n[56](17_Chapter8.xhtml#ch8fn56) Luc Boltanski and Eve Chiapello, _The New\nSpirit of Capitalism,_ trans. Gregory Elliot (London and New York: Verso,\n2005).\n\n[57](17_Chapter8.xhtml#ch8fn57) Moishe Postone, _Time, Labor, and Social\nDomination: A Reinterpretation of Marx’s Critical Theory_ (Cambridge, UK:\nCambridge University Press, 1993).\n\n[58](17_Chapter8.xhtml#ch8fn58) Ibid., 150.\n\n[59](17_Chapter8.xhtml#ch8fn59) Christian Fuchs and Sebastian Sevignani\ndifferentiate the English term work from labor arguing that work is a larger\nanthropological category of activity that transforms material into useful\nobjects, whereas labor is a form of work specific to capitalist value\nproduction. “What Is Digital Labour? What Is Digital Work? What’s Their\nDifference? And Why Do These Questions Matter for Understanding Social\nMedia?,” _tripleC: Communication, Capitalism and Critique_ 11, No. 2 (2013):\n237–293.\n\n[60](17_Chapter8.xhtml#ch8fn60) Marx, _Grundrisse_ , 695.\n\n[61](17_Chapter8.xhtml#ch8fn61) Postone, _Time, Labor, and Social Domination,_\n196–197; David Graeber, _Bullshit Jobs: A Theory_ (New York: Simon & Schuster,\n2018).\n\n[62](17_Chapter8.xhtml#ch8fn62) The xenofeminist collective Laboria Cuboniks\nargues: “The radical opportunities afforded by developing (and alienating)\nforms of technological mediation should no longer be put to use in the\nexclusive interests of capital, which, by design, only benefits the few.” _The\nXenofeminist Manifesto: A Politics for Alienation_ (London and New York:\nVerso, 2018), 35.\n\n[63](17_Chapter8.xhtml#ch8fn63) See Ernesto Laclau and Chantal Mouffe,\n_Hegemony and Socialist Strategy_ (London: New Left Books, 1985).\n\n[64](17_Chapter8.xhtml#ch8fn64) Adrien Chen, “The Laborers Who Keep Dick Pics\nand Beheadings Out of Your Facebook Feed,” _Wired_ , October 23, 2014; Mary\nGray and Siddharth Suri, _Ghost Work: How to Stop Silicon Valley from Building\na New Global Underclass_ (New York: Eamon Dolan Books, 2019); Sarah Roberts,\n_Behind the Screen: Content Moderation in the Shadows of Social Media_ (New\nHaven, CT: Yale University Press, 2019).\n\nConclusion: Toward a Revolutionary Mathematics\n\n[1](18_Conclusion.xhtml#chxfn1) Peter F. Brown, Peter V. deSouza, Robert L.\nMercer et al., “Class-based N-gram Models of Natural Language,” _Computational\nLinguistics_ 18, No. 4 (1992): 467–479.\n\n[2](18_Conclusion.xhtml#chxfn2) “Robert L. Mercer Receives the 2014 ACL\nLifetime Achievement Award,” Association for Computational Linguistics\nofficial website, October 15, 2014.\n\n[3](18_Conclusion.xhtml#chxfn3) Zachary Mider, “What Kind of Man Spends\nMillions to Elect Ted Cruz?,” Bloomberg, January 20, 2016.\n\n[4](18_Conclusion.xhtml#chxfn4) “Mercer Family Foundation,” Conservative\nTransparency official website.\n\n[5](18_Conclusion.xhtml#chxfn5) Robert Pogrebin and Somini Sengupta, “A\nScience Denier at the Natural History Museum? Scientists Rebel,” _New York\nTimes_ , January 25, 2018.\n\n[6](18_Conclusion.xhtml#chxfn6) Carole Cadwalladar and Emma Graham-Harrison,\n“Revealed: 50 Million Facebook Profiles Harvested for Cambridge Analytica in\nMajor Data Breach,” _Guardian_ , March 17, 2018.\n\n[7](18_Conclusion.xhtml#chxfn7) Paolo Virno, “General Intellect,” _Historical\nMaterialism_ 15, No. 3 (2007): 6.\n\n[8](18_Conclusion.xhtml#chxfn8) Chris Anderson, “The End of Theory: The Data\nDeluge Makes the Scientific Method Obsolete,” _Wired_ , June 23, 2008, 16–17.\n\n[9](18_Conclusion.xhtml#chxfn9) Virno, “General Intellect,” 6–7.\n\n[10](18_Conclusion.xhtml#chxfn10) Virno, “General Intellect,” 7.\n\n[11](18_Conclusion.xhtml#chxfn11) Karl Marx, _Grundrisse: Foundations of the\nCritique of Political Economy_ (London: Penguin, 2005, repr.), 712.\n\n[12](18_Conclusion.xhtml#chxfn12) Georg Lukács, _History and Class\nConsciousness: Studies in Marxist Dialectics_ , trans. Rodney Livingstone\n(Cambridge, MA: MIT Press, 1967), 259.\n\n\n[Index](04_Contents.xhtml#ch16)\n\n**A**\n\nabc conjecture, [62–6](09_Chapter2.xhtml#page_62)\n\nabstract domination, [13–14](06_Intro.xhtml#page_13),\n[94](10_Chapter3.xhtml#page_94), [201](17_Chapter8.xhtml#page_201)\n\nabstractions. _See also_ real abstraction\n\nalgorithmic abstractions, [185–8](17_Chapter8.xhtml#page_185)\n\nfreedom in, [178–85](17_Chapter8.xhtml#page_178)\n\nas increasingly mobile and modulatory, [177](17_Chapter8.xhtml#page_177)\n\nliberatory potential of, [205n50](17_Chapter8.xhtml#page_205)\n\nnew forms of, [202](17_Chapter8.xhtml#page_202)\n\nas not necessarily a force for exploitation, [205](17_Chapter8.xhtml#page_205)\n\npower of commodity as, [180](17_Chapter8.xhtml#page_180)\n\nproductive tensions of with location, [181](17_Chapter8.xhtml#page_181)\n\nstatistical models as providing mathematical abstractions of the world,\n[177](17_Chapter8.xhtml#page_177)\n\nas tending to generate authority, [180](17_Chapter8.xhtml#page_180)\n\nactivation function, in machine learning, [47](08_Chapter1.xhtml#page_47)\n\nActor Network Theory, [83n9](10_Chapter3.xhtml#page_83)\n\nAdorno, Theodor, [180](17_Chapter8.xhtml#page_180)\n\nAGI (artificial general intelligence), [43n11](08_Chapter1.xhtml#page_43)\n\nAI (artificial intelligence)\n\nAI winter, [4](06_Intro.xhtml#page_4), [53](08_Chapter1.xhtml#page_53)\n\nas not living up to postwar promise, [52–3](08_Chapter1.xhtml#page_52)\n\nas still only able to solve problems that are given to it,\n[201](17_Chapter8.xhtml#page_201)\n\nalgorithmic bias, [44](08_Chapter1.xhtml#page_44)\n\nalgorithmic instructions, as symbol manipulation,\n[78](10_Chapter3.xhtml#page_78)\n\nalgorithmic knowledge consequence of, [186](17_Chapter8.xhtml#page_186)\n\nlimits of, [38](08_Chapter1.xhtml#page_38)\n\nneed for revolutionary approach to, [33](06_Intro.xhtml#page_33)\n\npresumed veracity of, [6](06_Intro.xhtml#page_6)\n\npreventing worst abuses of, [61](09_Chapter2.xhtml#page_61)\n\nalgorithmic logic\n\nadvances in, [7](06_Intro.xhtml#page_7)\n\ninequities of, [14](06_Intro.xhtml#page_14)\n\npresumptions of, [6](06_Intro.xhtml#page_6)\n\nof Walmart and Northpointe, [44](08_Chapter1.xhtml#page_44)\n\nalgorithmic models, [22](06_Intro.xhtml#page_22),\n[41](08_Chapter1.xhtml#page_41), [78](10_Chapter3.xhtml#page_78)\n\nalgorithmic power, [81](10_Chapter3.xhtml#page_81)\n\nalgorithmic society, [8](06_Intro.xhtml#page_8),\n[29–30](06_Intro.xhtml#page_29), [34](06_Intro.xhtml#page_34)\n\nalgorithmic systems\n\nattempts to slow functioning of, [59](09_Chapter2.xhtml#page_59)\n\nas controlling much of contemporary life, [10](06_Intro.xhtml#page_10),\n[14](06_Intro.xhtml#page_14)\n\nfilter bubble of, [16](06_Intro.xhtml#page_16)\n\nas functioning in complex ecologies, [61](09_Chapter2.xhtml#page_61)\n\ninaccuracy of, [200](17_Chapter8.xhtml#page_200)\n\nas relative to data put into it, [41](08_Chapter1.xhtml#page_41)\n\nas supposed to achieve real-time abstractions,\n[196](17_Chapter8.xhtml#page_196)\n\nas thinking for us, [101](12_Chapter4.xhtml#page_101)\n\nalgorithmic trading, [4](06_Intro.xhtml#page_4),\n[191](17_Chapter8.xhtml#page_191)\n\nalgorithms\n\nas allowing conversion of extractable data into interchangeable bits,\n[182](17_Chapter8.xhtml#page_182)\n\nclassification algorithms, [51](08_Chapter1.xhtml#page_51)\n\ndefined, [77–8](10_Chapter3.xhtml#page_77)\n\nas effectively laundering nonobjective forms of violence and bias,\n[174](16_Chapter7.xhtml#page_174)\n\nfoundation of, [13](06_Intro.xhtml#page_13)\n\ninaccessibility of, [59–60](09_Chapter2.xhtml#page_59)\n\nas more efficient at creating new realities than in representing a world,\n[7](06_Intro.xhtml#page_7)\n\nand objectification, [21](06_Intro.xhtml#page_21),\n[77–98](10_Chapter3.xhtml#page_77)\n\nas opaque, [79](10_Chapter3.xhtml#page_79)\n\npredicting likelihood of recidivism, [44](08_Chapter1.xhtml#page_44)\n\nregulation of, [60](09_Chapter2.xhtml#page_60),\n[61](09_Chapter2.xhtml#page_61)\n\nrole of in mediating society and economics “behind our backs,”\n[20](06_Intro.xhtml#page_20)\n\nself-correcting design of, [38](08_Chapter1.xhtml#page_38)\n\ntraining of, [42](08_Chapter1.xhtml#page_42)\n\nas turning data from the world into knowledge about that world,\n[30](06_Intro.xhtml#page_30)\n\nuse of for obfuscation, [61](09_Chapter2.xhtml#page_61)\n\nalienation, [206–11](17_Chapter8.xhtml#page_206)\n\nAlpaydın, Ethem, [49](08_Chapter1.xhtml#page_49)\n\nalternative hypotheses, [119](13_Chapter5.xhtml#page_119),\n[126](13_Chapter5.xhtml#page_126), [130](13_Chapter5.xhtml#page_130),\n[138](13_Chapter5.xhtml#page_138)\n\nAlthusser, Louis, [2n1](06_Intro.xhtml#page_2),\n[80n4](10_Chapter3.xhtml#page_80)\n\nAmaro, Ramon, [92–3](10_Chapter3.xhtml#page_92),\n[182n9](17_Chapter8.xhtml#page_182)\n\nAmerican Statistical Agency (ASA), on p-value,\n[118](13_Chapter5.xhtml#page_118)\n\n_The Analyst_ (Berkeley), [32](06_Intro.xhtml#page_32),\n[67](09_Chapter2.xhtml#page_67), [70](09_Chapter2.xhtml#page_70)\n\nAnderson, Chris, [56](08_Chapter1.xhtml#page_56),\n[57](08_Chapter1.xhtml#page_57), [58](08_Chapter1.xhtml#page_58),\n[75](09_Chapter2.xhtml#page_75), [219](18_Conclusion.xhtml#page_219)\n\nAntipater of Thessalonica, [99](11_Part2.xhtml#page_99),\n[113](12_Chapter4.xhtml#page_113)\n\nAppel, Kenneth, [62](09_Chapter2.xhtml#page_62),\n[63](09_Chapter2.xhtml#page_63), [66](09_Chapter2.xhtml#page_66)\n\nArbuthnot, John, [111](12_Chapter4.xhtml#page_111),\n[115](13_Chapter5.xhtml#page_115), [163](16_Chapter7.xhtml#page_163)\n\nartificial general intelligence (AGI), [43n11](08_Chapter1.xhtml#page_43)\n\nartificial intelligence (AI)\n\nAI winter, [4](06_Intro.xhtml#page_4), [53](08_Chapter1.xhtml#page_53)\n\nas not living up to postwar promise, [52–3](08_Chapter1.xhtml#page_52)\n\nas still only able to solve problems that are given to it,\n[201](17_Chapter8.xhtml#page_201)\n\nartificial neural networks (ANNs), [45–52](08_Chapter1.xhtml#page_45),\n[53](08_Chapter1.xhtml#page_53), [55](08_Chapter1.xhtml#page_55),\n[186](17_Chapter8.xhtml#page_186)\n\n_The Art of Computer Programming_ (Knuth), [77](10_Chapter3.xhtml#page_77)\n\nautomated abstractions, and alienation, [177–216](17_Chapter8.xhtml#page_177)\n\nautomation\n\nin algorithmic trading, [4](06_Intro.xhtml#page_4)\n\nof knowledge, [37–58](08_Chapter1.xhtml#page_37)\n\nrole of, [28](06_Intro.xhtml#page_28)\n\nautonomism, [25](06_Intro.xhtml#page_25)\n\n**B**\n\nbackpropagation, [48](08_Chapter1.xhtml#page_48),\n[49](08_Chapter1.xhtml#page_49), [55](08_Chapter1.xhtml#page_55)\n\nBailey, Arthur, [164](16_Chapter7.xhtml#page_164)\n\nBaucom, Ian, [181n7](17_Chapter8.xhtml#page_181)\n\nBauman, Zygmut, [189n15](17_Chapter8.xhtml#page_189)\n\nBayes, Thomas, [67](09_Chapter2.xhtml#page_67),\n[143](15_Chapter6.xhtml#page_143), [163](16_Chapter7.xhtml#page_163),\n[164](16_Chapter7.xhtml#page_164)\n\nBayesian approaches/statistics\n\nas admitting to theistic understanding of probability,\n[162](16_Chapter7.xhtml#page_162)\n\nas claiming to avoid reference class problem, [40](08_Chapter1.xhtml#page_40)\n\nas denaturalizing nature, [213](17_Chapter8.xhtml#page_213)\n\nas flipping frequentism’s metaphysical perspective on its head,\n[158](16_Chapter7.xhtml#page_158)\n\nas fundamentally changing stakes of statistical analyses,\n[160](16_Chapter7.xhtml#page_160)\n\nhow they function, [94](10_Chapter3.xhtml#page_94)\n\nmethods of as ascendant, [12](06_Intro.xhtml#page_12)\n\nnaive Bayes classifier, [55](08_Chapter1.xhtml#page_55),\n[153–7](15_Chapter6.xhtml#page_153), [161–2](16_Chapter7.xhtml#page_161)\n\nas potentially leading to maximization of profit,\n[173](16_Chapter7.xhtml#page_173)\n\nprobability as measure of subjective belief, [148](15_Chapter6.xhtml#page_148)\n\nand problem of frequentism, [143–57](15_Chapter6.xhtml#page_143)\n\nas requiring use of probability distribution,\n[150](15_Chapter6.xhtml#page_150)\n\nrevolutionary implications of, [183](17_Chapter8.xhtml#page_183)\n\nrise of, [144](15_Chapter6.xhtml#page_144), [149](15_Chapter6.xhtml#page_149),\n[160](16_Chapter7.xhtml#page_160)\n\nshift from frequentist to, [30–1](06_Intro.xhtml#page_30)\n\nsubjectivism of, [155](15_Chapter6.xhtml#page_155),\n[156](15_Chapter6.xhtml#page_156), [160](16_Chapter7.xhtml#page_160)\n\nvalue of, [148–51](15_Chapter6.xhtml#page_148)\n\nBayesian inference, [152](15_Chapter6.xhtml#page_152)\n\nBayesian metaphysics, [158–76](16_Chapter7.xhtml#page_158)\n\nBayesian revolution, [12n21](06_Intro.xhtml#page_12),\n[34](06_Intro.xhtml#page_34), [156](15_Chapter6.xhtml#page_156),\n[162](16_Chapter7.xhtml#page_162), [174](16_Chapter7.xhtml#page_174),\n[199](17_Chapter8.xhtml#page_199), [205](17_Chapter8.xhtml#page_205),\n[213–14](17_Chapter8.xhtml#page_213), [217](18_Conclusion.xhtml#page_217),\n[221–2](18_Conclusion.xhtml#page_221)\n\nBayes’s theorem, [143](15_Chapter6.xhtml#page_143),\n[149](15_Chapter6.xhtml#page_149), [151–2](15_Chapter6.xhtml#page_151)\n\nBennett, Craig, [109](12_Chapter4.xhtml#page_109),\n[110](12_Chapter4.xhtml#page_110)\n\nBerardi, Franco (”Bifo”), [17](06_Intro.xhtml#page_17),\n[31](06_Intro.xhtml#page_31), [207](17_Chapter8.xhtml#page_207)\n\nBerkeley, George, [32](06_Intro.xhtml#page_32),\n[67–8](09_Chapter2.xhtml#page_67), [69–71](09_Chapter2.xhtml#page_69),\n[72](09_Chapter2.xhtml#page_72), [74](09_Chapter2.xhtml#page_74),\n[75](09_Chapter2.xhtml#page_75)\n\nBerryman, John, [35](07_Part1.xhtml#page_35), [43](08_Chapter1.xhtml#page_43)\n\nBible, on lots and chance, [112n20](12_Chapter4.xhtml#page_112)\n\nbig data\n\nbenefits of according to advocates, [6](06_Intro.xhtml#page_6)\n\nuse of to present appearance of following rules while wholly disregarding\nthem, [6](06_Intro.xhtml#page_6)\n\nBitcoin, [94–8](10_Chapter3.xhtml#page_94)\n\nblack boxes, [59](09_Chapter2.xhtml#page_59), [61](09_Chapter2.xhtml#page_61),\n[186](17_Chapter8.xhtml#page_186)\n\n_Black Marxism_ (Robinson), [91](10_Chapter3.xhtml#page_91)\n\nblockchain, [94–8](10_Chapter3.xhtml#page_94)\n\nBoolean Pythagorean triples problem, [62–3](09_Chapter2.xhtml#page_62)\n\nBowker, Geoffrey, [57](08_Chapter1.xhtml#page_57)\n\nBristol, Muriel, [115–16](13_Chapter5.xhtml#page_115),\n[120](13_Chapter5.xhtml#page_120), [125](13_Chapter5.xhtml#page_125)\n\nBrowne, Simone, [14](06_Intro.xhtml#page_14), [29](06_Intro.xhtml#page_29)\n\nBruno, Giordano, [164n12](16_Chapter7.xhtml#page_164)\n\nBryant, Levi, [164n12](16_Chapter7.xhtml#page_164)\n\n**C**\n\ncalculus\n\nBerkeley’s criticism of, [32](06_Intro.xhtml#page_32),\n[67](09_Chapter2.xhtml#page_67), [70](09_Chapter2.xhtml#page_70),\n[71](09_Chapter2.xhtml#page_71), [72](09_Chapter2.xhtml#page_72)\n\nrationale for teaching of, [69–70](09_Chapter2.xhtml#page_69)\n\nCanadian Tire, use of data to predict customer behaviors,\n[44](08_Chapter1.xhtml#page_44)\n\nCantor, Geoffrey, [72n22](09_Chapter2.xhtml#page_72) _Capital_ (Marx),\n[19](06_Intro.xhtml#page_19), [80](10_Chapter3.xhtml#page_80),\n[84](10_Chapter3.xhtml#page_84)\n\ncapitalism\n\nabolishment of, [210](17_Chapter8.xhtml#page_210)\n\naccording to Postone, [13](06_Intro.xhtml#page_13),\n[209](17_Chapter8.xhtml#page_209)\n\nalgorithmic knowledge production as unable to keep pace with motives and\ndrives of, [4](06_Intro.xhtml#page_4)\n\ncalls for retrenchment of an increasingly privatized capitalism,\n[3](06_Intro.xhtml#page_3)\n\ncapitalist calculation problem, [195–201](17_Chapter8.xhtml#page_195)\n\ncapitalist knowledge production, [219](18_Conclusion.xhtml#page_219)\n\ncognitive capitalism, [26](06_Intro.xhtml#page_26)\n\nas constructed atop abstractions that separate concepts from context,\n[179](17_Chapter8.xhtml#page_179)\n\ncrisis, [33](06_Intro.xhtml#page_33), [197](17_Chapter8.xhtml#page_197),\n[219](18_Conclusion.xhtml#page_219)\n\ndigital capitalism, [21](06_Intro.xhtml#page_21),\n[26](06_Intro.xhtml#page_26), [151](15_Chapter6.xhtml#page_151),\n[189](17_Chapter8.xhtml#page_189), [194](17_Chapter8.xhtml#page_194),\n[198](17_Chapter8.xhtml#page_198)\n\nas founded on metaphysical process, [19–20](06_Intro.xhtml#page_19)\n\nfrictionless capitalism, [183](17_Chapter8.xhtml#page_183)\n\nimportance of statistics to understanding contemporary capitalism,\n[8](06_Intro.xhtml#page_8)\n\nindustrial capitalism, [26](06_Intro.xhtml#page_26)\n\ninformational capitalism, [12](06_Intro.xhtml#page_12),\n[34](06_Intro.xhtml#page_34), [149](15_Chapter6.xhtml#page_149),\n[154](15_Chapter6.xhtml#page_154), [156](15_Chapter6.xhtml#page_156),\n[161](16_Chapter7.xhtml#page_161), [171](16_Chapter7.xhtml#page_171)\n\ninformation as new frontier of, [190](17_Chapter8.xhtml#page_190)\n\nmachine learning as function in way analogous to,\n[58](08_Chapter1.xhtml#page_58)\n\nmachine learning systems as working to solidify,\n[58](08_Chapter1.xhtml#page_58)\n\nneed to free alienation from, [210](17_Chapter8.xhtml#page_210)\n\nas objectifying, [33](06_Intro.xhtml#page_33)\n\nas operating as algorithm, [84–5](10_Chapter3.xhtml#page_84)\n\nplatform capitalism, [26](06_Intro.xhtml#page_26),\n[192–3](17_Chapter8.xhtml#page_192)\n\nrelation of statistics to, [12–13](06_Intro.xhtml#page_12)\n\nas rendering labor increasingly superfluous, [96–7](10_Chapter3.xhtml#page_96)\n\nrise of algorithmic capitalism, [7](06_Intro.xhtml#page_7)\n\nshort-term profits of as blocking any real response to climate change,\n[40](08_Chapter1.xhtml#page_40)\n\nsplit with scientific production, [174](16_Chapter7.xhtml#page_174)\n\nstatistics’ metaphysical role in, [10](06_Intro.xhtml#page_10)\n\nas subsuming and transmuting all social relations into modes of production,\n[198–9](17_Chapter8.xhtml#page_198)\n\nas supporting injustice and exploitation, [75](09_Chapter2.xhtml#page_75)\n\nsurveillance capitalism, [26](06_Intro.xhtml#page_26)\n\nunderstanding nature of contemporary capitalism, [20](06_Intro.xhtml#page_20)\n\n_Carceral Capitalism_ (Wang), [91](10_Chapter3.xhtml#page_91)\n\nCauchy, Augustin-Louis, [68n13](09_Chapter2.xhtml#page_68)\n\nCenters for Disease Control and Prevention, on doctor visits for flu,\n[41](08_Chapter1.xhtml#page_41)\n\nChichilinisky, Graciela, [170](16_Chapter7.xhtml#page_170)\n\nChun, Wendy, [9n18](06_Intro.xhtml#page_9), [30n55](06_Intro.xhtml#page_30),\n[57–8](08_Chapter1.xhtml#page_57)\n\nclass exploitation, as form of concrete domination,\n[92](10_Chapter3.xhtml#page_92)\n\nclassification algorithms, [51](08_Chapter1.xhtml#page_51)\n\nclimate change, capitalism’s privileging of shortterm profits blocking any\nreal response to, [40](08_Chapter1.xhtml#page_40)\n\ncognitive capitalism, [26](06_Intro.xhtml#page_26)\n\ncolonialism consequences of, [190](17_Chapter8.xhtml#page_190)\n\nas constructed atop abstractions that separate concepts from context,\n[179](17_Chapter8.xhtml#page_179)\n\ncommodities, and objectification, [84–8](10_Chapter3.xhtml#page_84)\n\ncommodity exchange, calculation as taking on form of in Bayesian analysis,\n[164](16_Chapter7.xhtml#page_164)\n\n_The Communist Manifesto_ (Marx), [15](06_Intro.xhtml#page_15)\n\nCOMPAS (Correctional Offender Management Profiling for Alternative Sanctions),\n[44](08_Chapter1.xhtml#page_44)\n\ncomputation, role of in determining current objective social reality under\ndigital capitalism, [21](06_Intro.xhtml#page_21)\n\ncomputers, can they do math? [59–76](09_Chapter2.xhtml#page_59)\n\nconcrete violence, [94](10_Chapter3.xhtml#page_94)\n\ncorrelationism, [75](09_Chapter2.xhtml#page_75)\n\nCox, Sean, [4–5](06_Intro.xhtml#page_4)\n\ncryptocurrencies, [96](10_Chapter3.xhtml#page_96)\n\ncypherpolitics, [95](10_Chapter3.xhtml#page_95)\n\n**D**\n\nDavies, William, [22](06_Intro.xhtml#page_22)\n\nDean, Jeff, [185](17_Chapter8.xhtml#page_185)\n\nDean, Jodi, [17](06_Intro.xhtml#page_17)\n\ndeep learning, [185](17_Chapter8.xhtml#page_185)\n\ndefeat device, [4–5](06_Intro.xhtml#page_4), [60](09_Chapter2.xhtml#page_60),\n[64](09_Chapter2.xhtml#page_64), [194](17_Chapter8.xhtml#page_194),\n[195](17_Chapter8.xhtml#page_195), [219](18_Conclusion.xhtml#page_219)\n\nDe Finetti, Bruno, [164–5](16_Chapter7.xhtml#page_164),\n[168](16_Chapter7.xhtml#page_168), [169](16_Chapter7.xhtml#page_169),\n[170](16_Chapter7.xhtml#page_170), [171](16_Chapter7.xhtml#page_171),\n[189](17_Chapter8.xhtml#page_189), [213](17_Chapter8.xhtml#page_213),\n[222](18_Conclusion.xhtml#page_222)\n\nDeleuze, Gilles, [189n15](17_Chapter8.xhtml#page_189)\n\nDemeter, [113](12_Chapter4.xhtml#page_113)\n\n_Dependency Road_ (Smythe), [88n16](10_Chapter3.xhtml#page_88)\n\ndereification, [17](06_Intro.xhtml#page_17), [22](06_Intro.xhtml#page_22),\n[60](09_Chapter2.xhtml#page_60), [69](09_Chapter2.xhtml#page_69),\n[70](09_Chapter2.xhtml#page_70), [90](10_Chapter3.xhtml#page_90)\n\nDerrida, Jacques, [70n17](09_Chapter2.xhtml#page_70)\n\n_The Design of Experiments_ (Fisher), [115](13_Chapter5.xhtml#page_115)\n\ndeviancy, measurement and management of, [29](06_Intro.xhtml#page_29)\n\ndigital capitalism, [21](06_Intro.xhtml#page_21),\n[26](06_Intro.xhtml#page_26), [151](15_Chapter6.xhtml#page_151),\n[189](17_Chapter8.xhtml#page_189), [194](17_Chapter8.xhtml#page_194),\n[198](17_Chapter8.xhtml#page_198)\n\ndigital systems, as providing ideological form for contemporary neoliberalism,\n[30n55](06_Intro.xhtml#page_30)\n\nDillman, Linda, [37](08_Chapter1.xhtml#page_37),\n[38](08_Chapter1.xhtml#page_38)\n\ndivine connection, [163](16_Chapter7.xhtml#page_163)\n\nDutch book argument, [164–76](16_Chapter7.xhtml#page_164),\n[183](17_Chapter8.xhtml#page_183), [220](18_Conclusion.xhtml#page_220)\n\nDyer-Witheford, Nick, [28](06_Intro.xhtml#page_28),\n[194](17_Chapter8.xhtml#page_194)\n\n**E**\n\neconomy. _See also_ political economy\n\ndigital systems as fundamentally altering function of,\n[23](06_Intro.xhtml#page_23)\n\ngoal of, [219](18_Conclusion.xhtml#page_219)\n\nknowledge economy, [199](17_Chapter8.xhtml#page_199)\n\n_The Eighteenth Brumaire_ (Marx), [8](06_Intro.xhtml#page_8)\n\nElia, Ramón de, [102](12_Chapter4.xhtml#page_102)\n\nenclosed intellect, [195](17_Chapter8.xhtml#page_195)\n\nenclosure, of general intellect, [188–95](17_Chapter8.xhtml#page_188),\n[201](17_Chapter8.xhtml#page_201), [211](17_Chapter8.xhtml#page_211)\n\nEndnotes, [18](06_Intro.xhtml#page_18)\n\nEnlightenment\n\nabstraction of, [182](17_Chapter8.xhtml#page_182)\n\nas constructed atop abstractions that separate concepts from context,\n[179](17_Chapter8.xhtml#page_179)\n\nidealism of, [183](17_Chapter8.xhtml#page_183)\n\nas inspiring varieties of communism, [15](06_Intro.xhtml#page_15)\n\nmodels and theories of causality in, [56](08_Chapter1.xhtml#page_56)\n\nnotion of individual sovereignty in, [212](17_Chapter8.xhtml#page_212)\n\nrise of concept of race during, [58](08_Chapter1.xhtml#page_58)\n\nepistemic authority, [73–6](09_Chapter2.xhtml#page_73)\n\n_An Essay towards Solving a Problem in the Doctrine of Chances_ (Bayes),\n[162](16_Chapter7.xhtml#page_162)\n\nEubanks, Virginia, [29](06_Intro.xhtml#page_29)\n\neugenics, [121](13_Chapter5.xhtml#page_121)\n\nexchange\n\nbelief, experience, and subjectivity as conditioned by constraints of,\n[204](17_Chapter8.xhtml#page_204)\n\nearly deistic forms of abstraction as bearing some relation to,\n[179n3](17_Chapter8.xhtml#page_179)\n\nnotions of justice and freedom as founded on,\n[181n7](17_Chapter8.xhtml#page_181)\n\ntethering of knowledge to, [171–2](16_Chapter7.xhtml#page_171)\n\nvictory of neoliberalism as means to organize all life and subjectivity on\ngrounds of, [172](16_Chapter7.xhtml#page_172)\n\nextrapolation, limitations of, [45](08_Chapter1.xhtml#page_45)\n\n**F**\n\nFacebook\n\nNews Feed algorithm, [40](08_Chapter1.xhtml#page_40)\n\nas providing secret access to user data to large corporate partners,\n[189](17_Chapter8.xhtml#page_189)\n\nas taxing exchanges, [193](17_Chapter8.xhtml#page_193)\n\nface recognition, [182n9](17_Chapter8.xhtml#page_182)\n\nfake news, [219](18_Conclusion.xhtml#page_219)\n\nfalse negatives (errors of the second kind),\n[111](12_Chapter4.xhtml#page_111), [124](13_Chapter5.xhtml#page_124),\n[126](13_Chapter5.xhtml#page_126), [128](13_Chapter5.xhtml#page_128)\n\nfalse positives (errors of the first kind), [107](12_Chapter4.xhtml#page_107),\n[111](12_Chapter4.xhtml#page_111), [124](13_Chapter5.xhtml#page_124),\n[126](13_Chapter5.xhtml#page_126), [127](13_Chapter5.xhtml#page_127),\n[128](13_Chapter5.xhtml#page_128), [151](15_Chapter6.xhtml#page_151),\n[152](15_Chapter6.xhtml#page_152)\n\nfalsifiability, [123](13_Chapter5.xhtml#page_123)\n\nFerguson, Scott, [205n50](17_Chapter8.xhtml#page_205)\n\nfetishism, [80n4](10_Chapter3.xhtml#page_80),\n[89n19](10_Chapter3.xhtml#page_89), [138](13_Chapter5.xhtml#page_138),\n[206](17_Chapter8.xhtml#page_206), [209](17_Chapter8.xhtml#page_209),\n[213](17_Chapter8.xhtml#page_213), [222](18_Conclusion.xhtml#page_222),\n[223](18_Conclusion.xhtml#page_223)\n\nfilter bubbles, [16](06_Intro.xhtml#page_16),\n[219](18_Conclusion.xhtml#page_219)\n\nFirst Vatican Council, [71](09_Chapter2.xhtml#page_71)\n\nFisher, Ronald, [9](06_Intro.xhtml#page_9),\n[115–18](13_Chapter5.xhtml#page_115), [119–20](13_Chapter5.xhtml#page_119),\n[121–3](13_Chapter5.xhtml#page_121), [124](13_Chapter5.xhtml#page_124),\n[128](13_Chapter5.xhtml#page_128), [129–30](13_Chapter5.xhtml#page_129),\n[135–6](13_Chapter5.xhtml#page_135), [137–8](13_Chapter5.xhtml#page_137),\n[139](13_Chapter5.xhtml#page_139), [144](15_Chapter6.xhtml#page_144),\n[150](15_Chapter6.xhtml#page_150), [164](16_Chapter7.xhtml#page_164),\n[172](16_Chapter7.xhtml#page_172), [173](16_Chapter7.xhtml#page_173),\n[174](16_Chapter7.xhtml#page_174), [175](16_Chapter7.xhtml#page_175),\n[193](17_Chapter8.xhtml#page_193), [194](17_Chapter8.xhtml#page_194),\n[213](17_Chapter8.xhtml#page_213), [220](18_Conclusion.xhtml#page_220),\n[222](18_Conclusion.xhtml#page_222)\n\nFisher-Neyman-Pearson model, [131](13_Chapter5.xhtml#page_131)\n\nfMRI (functional MRI)\n\ncluster failure problem in, [106–9](12_Chapter4.xhtml#page_106),\n[124](13_Chapter5.xhtml#page_124), [132](13_Chapter5.xhtml#page_132),\n[134–5](13_Chapter5.xhtml#page_134)\n\nstructural problems within statistical methodologies of,\n[109](12_Chapter4.xhtml#page_109)\n\nFoucault, Michel, [28](06_Intro.xhtml#page_28)\n\n_Foundations of Statistics_ (Savage), [169](16_Chapter7.xhtml#page_169),\n[170](16_Chapter7.xhtml#page_170)\n\nfour-color theorem (4CT), [62–5](09_Chapter2.xhtml#page_62),\n[66](09_Chapter2.xhtml#page_66) “Fragment on the Machine” (Marx),\n[25](06_Intro.xhtml#page_25), [207](17_Chapter8.xhtml#page_207),\n[210–11](17_Chapter8.xhtml#page_210), [222](18_Conclusion.xhtml#page_222)\n\nFrankfurt school, [180](17_Chapter8.xhtml#page_180)\n\nFranklin, Seb, [30n55](06_Intro.xhtml#page_30)\n\nfree labor, debate about existence and nature of on digital platforms,\n[24](06_Intro.xhtml#page_24)\n\nfreethought/freethinkers, [71](09_Chapter2.xhtml#page_71),\n[72](09_Chapter2.xhtml#page_72), [73–4](09_Chapter2.xhtml#page_73),\n[180](17_Chapter8.xhtml#page_180)\n\nfrequentist statistics/frequentism\n\nas answering wrong question, [147–8](15_Chapter6.xhtml#page_147)\n\nBayesian statistics as flipping metaphysical perspective of,\n[158](16_Chapter7.xhtml#page_158)\n\ncompared to Bayesian analysis, [183](17_Chapter8.xhtml#page_183)\n\nemergence of, [104–5](12_Chapter4.xhtml#page_104)\n\non probability, [104–5](12_Chapter4.xhtml#page_104)\n\nproblems with, [161](16_Chapter7.xhtml#page_161)\n\nreference classes as pivotal for, [145–6](15_Chapter6.xhtml#page_145)\n\nshortcomings of, [144–5](15_Chapter6.xhtml#page_144),\n[147–8](15_Chapter6.xhtml#page_147)\n\nas struggling, [143](15_Chapter6.xhtml#page_143)\n\nFriedman, Milton, [50n22](08_Chapter1.xhtml#page_50)\n\nFuchs, Christian, [24n43](06_Intro.xhtml#page_24),\n[209n59](17_Chapter8.xhtml#page_209)\n\nfunctional MRI (fMRI)\n\ncluster failure problem in, [106–9](12_Chapter4.xhtml#page_106),\n[124](13_Chapter5.xhtml#page_124), [132](13_Chapter5.xhtml#page_132),\n[134–5](13_Chapter5.xhtml#page_134)\n\nstructural problems within statistical methodologies of,\n[109](12_Chapter4.xhtml#page_109)\n\n**G**\n\nGates, Bill, [183](17_Chapter8.xhtml#page_183)\n\nGelman, Andrew, [57n39](08_Chapter1.xhtml#page_57),\n[125](13_Chapter5.xhtml#page_125), [134](13_Chapter5.xhtml#page_134),\n[171](16_Chapter7.xhtml#page_171)\n\ngeneral intellect\n\ndefined, [191](17_Chapter8.xhtml#page_191)\n\nenclosure of, [188–95](17_Chapter8.xhtml#page_188),\n[201](17_Chapter8.xhtml#page_201), [211](17_Chapter8.xhtml#page_211)\n\n_The Genetical Theory of Natural Selection_ (Fisher),\n[121](13_Chapter5.xhtml#page_121)\n\nghosts of departed quantities, [68](09_Chapter2.xhtml#page_68),\n[69](09_Chapter2.xhtml#page_69), [73](09_Chapter2.xhtml#page_73),\n[75](09_Chapter2.xhtml#page_75), [87](10_Chapter3.xhtml#page_87),\n[95](10_Chapter3.xhtml#page_95), [96](10_Chapter3.xhtml#page_96),\n[138](13_Chapter5.xhtml#page_138)\n\nGiannandrea, John, [43n12](08_Chapter1.xhtml#page_43)\n\nGigerenzer, Gerd, [114n23](12_Chapter4.xhtml#page_114),\n[172n23](16_Chapter7.xhtml#page_172)\n\nGoertzel, Ben, [43n11](08_Chapter1.xhtml#page_43)\n\nGoldhaber, Maurice, [133](13_Chapter5.xhtml#page_133)\n\nGoogle\n\nand advertising world, [56](08_Chapter1.xhtml#page_56)\n\nattempt to predict flu activity based on search results,\n[41](08_Chapter1.xhtml#page_41)\n\n“child” algorithm, [51n23](08_Chapter1.xhtml#page_51)\n\nimage recognition system, [185–8](17_Chapter8.xhtml#page_185)\n\nPageRank algorithm, [78](10_Chapter3.xhtml#page_78),\n[183–4](17_Chapter8.xhtml#page_183)\n\n“Greyball” program (Uber), [5–6](06_Intro.xhtml#page_5)\n\n_Grundrisse_ (Marx), [96](10_Chapter3.xhtml#page_96)\n\n**H**\n\nHaken, Wolfgang, [62](09_Chapter2.xhtml#page_62),\n[63](09_Chapter2.xhtml#page_63), [66](09_Chapter2.xhtml#page_66)\n\nHartman, Saidiya, [208](17_Chapter8.xhtml#page_208)\n\nHarvey, David, [190](17_Chapter8.xhtml#page_190),\n[198](17_Chapter8.xhtml#page_198)\n\nHayek, Friedrich, [195–7](17_Chapter8.xhtml#page_195),\n[200](17_Chapter8.xhtml#page_200)\n\nHegel, G.W.F., [49](08_Chapter1.xhtml#page_49),\n[56](08_Chapter1.xhtml#page_56)\n\nHenry I (king), [82](10_Chapter3.xhtml#page_82)\n\nhomophily, [57–8](08_Chapter1.xhtml#page_57)\n\nHonneth, Axel, [83n11](10_Chapter3.xhtml#page_83)\n\nHorkheimer, Max, [180](17_Chapter8.xhtml#page_180)\n\nHusserl, Edmund, [164n12](16_Chapter7.xhtml#page_164)\n\nhybridization, [130–5](13_Chapter5.xhtml#page_130),\n[137](13_Chapter5.xhtml#page_137), [139](13_Chapter5.xhtml#page_139)\n\nhyperparameters, [50](08_Chapter1.xhtml#page_50)\n\nhypothesis testing, [111–12](12_Chapter4.xhtml#page_111),\n[113](12_Chapter4.xhtml#page_113), [115](13_Chapter5.xhtml#page_115),\n[117](13_Chapter5.xhtml#page_117), [118](13_Chapter5.xhtml#page_118),\n[124](13_Chapter5.xhtml#page_124), [134](13_Chapter5.xhtml#page_134),\n[151](15_Chapter6.xhtml#page_151), [155](15_Chapter6.xhtml#page_155),\n[156](15_Chapter6.xhtml#page_156)\n\n**I**\n\nideal coin, [10–11](06_Intro.xhtml#page_10)\n\nimage recognition system, [185–8](17_Chapter8.xhtml#page_185)\n\nimmaterial labor, [24](06_Intro.xhtml#page_24), [25](06_Intro.xhtml#page_25)\n\nimperialism\n\ncapital accumulation’s constant drive to profit from,\n[220](18_Conclusion.xhtml#page_220)\n\nas form of concrete domination, [92](10_Chapter3.xhtml#page_92)\n\nmachine learning systems as working to solidify,\n[58](08_Chapter1.xhtml#page_58)\n\nobjectified forces as translating of into abstract demands of capital,\n[210](17_Chapter8.xhtml#page_210)\n\nas supporting injustice and exploitation, [75](09_Chapter2.xhtml#page_75)\n\nas translated into abstract demands of capital,\n[210](17_Chapter8.xhtml#page_210)\n\nincommensurability, [183](17_Chapter8.xhtml#page_183)\n\nindividualism, [172](16_Chapter7.xhtml#page_172),\n[173](16_Chapter7.xhtml#page_173), [174](16_Chapter7.xhtml#page_174),\n[193](17_Chapter8.xhtml#page_193), [220](18_Conclusion.xhtml#page_220)\n\ninductive behavior, [125–6](13_Chapter5.xhtml#page_125)\n\nindustrial capitalism, [26](06_Intro.xhtml#page_26)\n\ninference revolution, [10](06_Intro.xhtml#page_10),\n[172n23](16_Chapter7.xhtml#page_172)\n\ninfidel mathematician, [67–73](09_Chapter2.xhtml#page_67)\n\ninformational capitalism, [12](06_Intro.xhtml#page_12),\n[34](06_Intro.xhtml#page_34), [149](15_Chapter6.xhtml#page_149),\n[154](15_Chapter6.xhtml#page_154), [156](15_Chapter6.xhtml#page_156),\n[161](16_Chapter7.xhtml#page_161), [171](16_Chapter7.xhtml#page_171)\n\nintellectual property rights, [190n20](17_Chapter8.xhtml#page_190)\n\ninterpolation, successes of, [45](08_Chapter1.xhtml#page_45)\n\ninverse probability, [150](15_Chapter6.xhtml#page_150),\n[151](15_Chapter6.xhtml#page_151), [154](15_Chapter6.xhtml#page_154)\n\ninverse problem, [148](15_Chapter6.xhtml#page_148),\n[149–50](15_Chapter6.xhtml#page_149)\n\nIoannidis, John, [135](13_Chapter5.xhtml#page_135),\n[139](13_Chapter5.xhtml#page_139)\n\n**J**\n\nJohn Deere, [192](17_Chapter8.xhtml#page_192)\n\nJonah (biblical), [110–14](12_Chapter4.xhtml#page_110)\n\nJurin, James, [74](09_Chapter2.xhtml#page_74),\n[180](17_Chapter8.xhtml#page_180)\n\n**K**\n\nk-nearest neighbors, [51n25](08_Chapter1.xhtml#page_51)\n\nKnight, Frank, [39](08_Chapter1.xhtml#page_39)\n\nknowledge\n\nautomation of, [37–58](08_Chapter1.xhtml#page_37)\n\ncontradiction of in digital capitalism, [194–5](17_Chapter8.xhtml#page_194)\n\nfoundation of, and Bayesian metaphysics, [158–76](16_Chapter7.xhtml#page_158)\n\ninference revolution as having revolutionized production of,\n[10](06_Intro.xhtml#page_10)\n\nmachines as producing, [27](06_Intro.xhtml#page_27)\n\nmovement from objective to subjective foundations for,\n[12](06_Intro.xhtml#page_12)\n\nprivatization and collectivization of, [194](17_Chapter8.xhtml#page_194)\n\nas quasi-independent productive force, [198](17_Chapter8.xhtml#page_198)\n\ntethering of to exchange, [171–2](16_Chapter7.xhtml#page_171)\n\nknowledge economy, defined, [199](17_Chapter8.xhtml#page_199)\n\nknowledge production\n\nalgorithms as changing speed and form of but not ultimate goals of,\n[61](09_Chapter2.xhtml#page_61)\n\nBayesian statistics as fundamentally altering,\n[156](15_Chapter6.xhtml#page_156)\n\ncapitalist knowledge production, [219](18_Conclusion.xhtml#page_219)\n\nengagement of, [206](17_Chapter8.xhtml#page_206)\n\nfailure of, [220](18_Conclusion.xhtml#page_220)\n\nas founded upon conditions of exchange, [215](17_Chapter8.xhtml#page_215)\n\nas insisting on centrality of exchange, [221](18_Conclusion.xhtml#page_221)\n\nRobert Mercer’s role in, [218](18_Conclusion.xhtml#page_218)\n\nKnuth, Donald, [77–8](10_Chapter3.xhtml#page_77)\n\nKobe Steel, [200](17_Chapter8.xhtml#page_200)\n\nKornbluh, Anna, [205n50](17_Chapter8.xhtml#page_205)\n\nKotsko, Adam, [205n50](17_Chapter8.xhtml#page_205)\n\n**L**\n\nlabor\n\ncapitalism as rendering labor increasingly superfluous,\n[96–7](10_Chapter3.xhtml#page_96)\n\nforms of that work on data, information, and knowledge,\n[24–5](06_Intro.xhtml#page_24)\n\nfree labor, [24](06_Intro.xhtml#page_24)\n\nimmaterial labor, [24](06_Intro.xhtml#page_24), [25](06_Intro.xhtml#page_25)\n\nLaboria Cuboniks, [212](17_Chapter8.xhtml#page_212)\n\nLacan, Jacques, [18](06_Intro.xhtml#page_18)\n\nlady tasting tea experiment, [115–18](13_Chapter5.xhtml#page_115),\n[119](13_Chapter5.xhtml#page_119), [120](13_Chapter5.xhtml#page_120),\n[125](13_Chapter5.xhtml#page_125), [132](13_Chapter5.xhtml#page_132)\n\nLaplace, Pierre-Simon, [117](13_Chapter5.xhtml#page_117),\n[141](14_Part3.xhtml#page_141), [143](15_Chapter6.xhtml#page_143),\n[149](15_Chapter6.xhtml#page_149)\n\nLaprise, René, [102](12_Chapter4.xhtml#page_102)\n\nLatour, Bruno, [83n9](10_Chapter3.xhtml#page_83)\n\nlearning rate, [49](08_Chapter1.xhtml#page_49),\n[50](08_Chapter1.xhtml#page_50)\n\nLenin, Vladimir, [15n27](06_Intro.xhtml#page_15)\n\nLindley, Dennis, [144](15_Chapter6.xhtml#page_144),\n[145](15_Chapter6.xhtml#page_145)\n\nlocal optimization, [201](17_Chapter8.xhtml#page_201)\n\n“A Logical Calculus of the Ideas Immanent in Nervous Activity” (McCulloch and\nPitts), [53](08_Chapter1.xhtml#page_53)\n\nLukács, Georg, [17](06_Intro.xhtml#page_17), [82](10_Chapter3.xhtml#page_82)\n\nLynch, Michael, [149](15_Chapter6.xhtml#page_149)\n\n**M**\n\nmachine learning\n\nactivation function in, [47](08_Chapter1.xhtml#page_47)\n\nas appearing to establish precisely what capitalism has always dreamed of,\n[182](17_Chapter8.xhtml#page_182)\n\nchallenges to, [41](08_Chapter1.xhtml#page_41)\n\ncrisis in, [4](06_Intro.xhtml#page_4)\n\ncurrent uses of, [199](17_Chapter8.xhtml#page_199)\n\ndual danger and opportunity of, [221](18_Conclusion.xhtml#page_221)\n\nas functioning in way analogous to capitalism, [58](08_Chapter1.xhtml#page_58)\n\ngoal of, [219](18_Conclusion.xhtml#page_219)\n\nas mobilizing and liquifying abstractions, [182](17_Chapter8.xhtml#page_182)\n\nnewer methods of, [55](08_Chapter1.xhtml#page_55)\n\noperationalization of statistics in, [189](17_Chapter8.xhtml#page_189)\n\n_operation of_ , [38–9](08_Chapter1.xhtml#page_38)\n\norigin of, [52](08_Chapter1.xhtml#page_52)\n\npreference for correlations over causal explanations of,\n[181–2](17_Chapter8.xhtml#page_181)\n\nproblems in, [51n24](08_Chapter1.xhtml#page_51)\n\nrelationship of to capitalism, [41n8](08_Chapter1.xhtml#page_41)\n\nscience behind, [220](18_Conclusion.xhtml#page_220)\n\nand statistical modeling, [41–2](08_Chapter1.xhtml#page_41)\n\nsuccesses of, [43](08_Chapter1.xhtml#page_43)\n\nMarewski, Julian, [114n23](12_Chapter4.xhtml#page_114),\n[172n23](16_Chapter7.xhtml#page_172)\n\nthe market, as metaphysical ground, [162–4](16_Chapter7.xhtml#page_162)\n\nMartin, Randy, [97](10_Chapter3.xhtml#page_97)\n\nMarx, Karl, [1](06_Intro.xhtml#page_1), [7–8](06_Intro.xhtml#page_7),\n[15](06_Intro.xhtml#page_15), [19–20](06_Intro.xhtml#page_19),\n[25](06_Intro.xhtml#page_25), [27](06_Intro.xhtml#page_27),\n[67](09_Chapter2.xhtml#page_67), [73](09_Chapter2.xhtml#page_73),\n[80](10_Chapter3.xhtml#page_80), [81](10_Chapter3.xhtml#page_81),\n[83](10_Chapter3.xhtml#page_83), [84](10_Chapter3.xhtml#page_84),\n[85–6](10_Chapter3.xhtml#page_85), [88](10_Chapter3.xhtml#page_88),\n[90](10_Chapter3.xhtml#page_90), [96–7](10_Chapter3.xhtml#page_96),\n[139](13_Chapter5.xhtml#page_139), [190](17_Chapter8.xhtml#page_190),\n[191](17_Chapter8.xhtml#page_191), [206–7](17_Chapter8.xhtml#page_206),\n[209](17_Chapter8.xhtml#page_209), [210–11](17_Chapter8.xhtml#page_210),\n[220](18_Conclusion.xhtml#page_220), [222](18_Conclusion.xhtml#page_222),\n[223](18_Conclusion.xhtml#page_223)\n\nMarxists/Marxism, [3](06_Intro.xhtml#page_3), [15](06_Intro.xhtml#page_15),\n[18](06_Intro.xhtml#page_18), [19](06_Intro.xhtml#page_19),\n[22](06_Intro.xhtml#page_22), [193–4](17_Chapter8.xhtml#page_193),\n[207](17_Chapter8.xhtml#page_207), [208](17_Chapter8.xhtml#page_208)\n\nmass intellectuality, [193](17_Chapter8.xhtml#page_193)\n\nmathematical, as level of statistics, [8–9](06_Intro.xhtml#page_8)\n\nmathematics. _See also_ revolutionary mathematics\n\ncan computers do math? [59–76](09_Chapter2.xhtml#page_59)\n\nof capitalist orthodoxy, [220](18_Conclusion.xhtml#page_220)\n\ntheoretical mathematics, [62–5](09_Chapter2.xhtml#page_62)\n\nMcCulloch, Warren, [53–4](08_Chapter1.xhtml#page_53)\n\nMercer, Robert, [217–18](18_Conclusion.xhtml#page_217),\n[223](18_Conclusion.xhtml#page_223)\n\nmeta-analyses, [160n3](16_Chapter7.xhtml#page_160)\n\nmetaphysical, as level of statistics, [8](06_Intro.xhtml#page_8),\n[9](06_Intro.xhtml#page_9)\n\nmetaphysics\n\nBayesian metaphysics, [158–76](16_Chapter7.xhtml#page_158)\n\ncapitalism as founded on metaphysical process, [19–20](06_Intro.xhtml#page_19)\n\nthe market as metaphysical ground, [162–4](16_Chapter7.xhtml#page_162)\n\nmetaphysical force of objectification, [58](08_Chapter1.xhtml#page_58)\n\nmetaphysical foundation of statistics, [122](13_Chapter5.xhtml#page_122)\n\nobjectification as metaphysical process, [88](10_Chapter3.xhtml#page_88)\n\nobjective metaphysics, [88–90](10_Chapter3.xhtml#page_88)\n\nstatistics’ metaphysical role in capitalism, [10](06_Intro.xhtml#page_10)\n\nwork of revolutionary mathematics as metaphysical,\n[33](06_Intro.xhtml#page_33)\n\nMinsky, Marvin, [53](08_Chapter1.xhtml#page_53),\n[54](08_Chapter1.xhtml#page_54)\n\nMochizuki, Shinichi, [64–5](09_Chapter2.xhtml#page_64)\n\nMoten, Fred, [97](10_Chapter3.xhtml#page_97)\n\nMumford, Lewis, [74n24](09_Chapter2.xhtml#page_74)\n\nmysteries\n\nof faith, [71–2](09_Chapter2.xhtml#page_71)\n\nlinkage of equality and inequality in, [75](09_Chapter2.xhtml#page_75)\n\nof reason, [76](09_Chapter2.xhtml#page_76)\n\n**N**\n\nnaive Bayes classifier, [55](08_Chapter1.xhtml#page_55),\n[153–7](15_Chapter6.xhtml#page_153), [161–2](16_Chapter7.xhtml#page_161)\n\nnatal alienation, [208](17_Chapter8.xhtml#page_208)\n\nnaturality, and ontology, [201–6](17_Chapter8.xhtml#page_201)\n\nnegative results, academic movement as organizing for sharing of,\n[137](13_Chapter5.xhtml#page_137)\n\nNegri, Antonio, [20n40](06_Intro.xhtml#page_20),\n[194n28](17_Chapter8.xhtml#page_194)\n\nneoliberalism\n\ndigital systems as providing ideological form of,\n[30n55](06_Intro.xhtml#page_30)\n\nHayek as founding, [196](17_Chapter8.xhtml#page_196)\n\npolitical and social order as increasingly subject to constraints of,\n[3](06_Intro.xhtml#page_3)\n\nin scientific thinking, [129](13_Chapter5.xhtml#page_129)\n\nas seeking out more and more local contexts from which to extract value and\nknowledge, [223](18_Conclusion.xhtml#page_223)\n\nvictory of as means to organize all life and subjectivity on grounds of market\nexchange, [172](16_Chapter7.xhtml#page_172)\n\nnetwork society, [26](06_Intro.xhtml#page_26)\n\nneural networks, [53](08_Chapter1.xhtml#page_53),\n[54](08_Chapter1.xhtml#page_54), [55](08_Chapter1.xhtml#page_55),\n[61](09_Chapter2.xhtml#page_61), [182–3](17_Chapter8.xhtml#page_182). _See\nalso_\n\nartificial neural networks (ANNs)\n\nneurons\n\nartificial “neurons,” [45](08_Chapter1.xhtml#page_45)\n\nhidden neurons, [46](08_Chapter1.xhtml#page_46),\n[47](08_Chapter1.xhtml#page_47), [50](08_Chapter1.xhtml#page_50)\n\nNews Feed (Facebook), [40](08_Chapter1.xhtml#page_40)\n\nNewton, Isaac, [70](09_Chapter2.xhtml#page_70),\n[72](09_Chapter2.xhtml#page_72), [74](09_Chapter2.xhtml#page_74),\n[133](13_Chapter5.xhtml#page_133), [180](17_Chapter8.xhtml#page_180)\n\nNeyman, Jerzy, [102–3](12_Chapter4.xhtml#page_102),\n[123–30](13_Chapter5.xhtml#page_123), [136](13_Chapter5.xhtml#page_136),\n[137](13_Chapter5.xhtml#page_137), [138](13_Chapter5.xhtml#page_138),\n[139](13_Chapter5.xhtml#page_139), [144](15_Chapter6.xhtml#page_144),\n[156](15_Chapter6.xhtml#page_156), [164](16_Chapter7.xhtml#page_164),\n[169](16_Chapter7.xhtml#page_169), [172](16_Chapter7.xhtml#page_172),\n[175](16_Chapter7.xhtml#page_175), [189](17_Chapter8.xhtml#page_189),\n[222](18_Conclusion.xhtml#page_222)\n\nNeyman-Pearson model/system, [126](13_Chapter5.xhtml#page_126),\n[131](13_Chapter5.xhtml#page_131), [136](13_Chapter5.xhtml#page_136),\n[169](16_Chapter7.xhtml#page_169)\n\nNoble, Safiya, [29](06_Intro.xhtml#page_29)\n\nnonlinear dynamics, [48](08_Chapter1.xhtml#page_48),\n[50](08_Chapter1.xhtml#page_50), [202](17_Chapter8.xhtml#page_202)\n\nNorthpointe, [44](08_Chapter1.xhtml#page_44)\n\nNorvig, Peter, [155](15_Chapter6.xhtml#page_155)\n\nnull hypothesis, [119](13_Chapter5.xhtml#page_119),\n[120](13_Chapter5.xhtml#page_120), [122](13_Chapter5.xhtml#page_122),\n[123](13_Chapter5.xhtml#page_123), [124](13_Chapter5.xhtml#page_124),\n[126](13_Chapter5.xhtml#page_126), [129–30](13_Chapter5.xhtml#page_129),\n[144](15_Chapter6.xhtml#page_144), [147–8](15_Chapter6.xhtml#page_147)\n\n**O**\n\nobjectification\n\naccording to Marx, [19–20](06_Intro.xhtml#page_19)\n\nalgorithms and, [21](06_Intro.xhtml#page_21)\n\nalgorithms of, [77–98](10_Chapter3.xhtml#page_77)\n\nBayesian approach as taking process of more seriously than frequentism,\n[158–9](16_Chapter7.xhtml#page_158)\n\ncommodities and, [84–8](10_Chapter3.xhtml#page_84)\n\ncreating and exploring new possibilities of, [34](06_Intro.xhtml#page_34)\n\ndefined, [20](06_Intro.xhtml#page_20)\n\nas form of forgetting, [83](10_Chapter3.xhtml#page_83)\n\nGod as force of, [164](16_Chapter7.xhtml#page_164)\n\nhow subjective knowledge is made to appear objective,\n[164n12](16_Chapter7.xhtml#page_164)\n\nas means through which concrete domination and violence are given abstract\nform and then translated back again into the concrete,\n[94](10_Chapter3.xhtml#page_94)\n\nmetaphysical force of, [58](08_Chapter1.xhtml#page_58)\n\nas metaphysical process, [88](10_Chapter3.xhtml#page_88)\n\nmobile and temporary assemblages of, [222–3](18_Conclusion.xhtml#page_222)\n\nmysteries of, [73](09_Chapter2.xhtml#page_73), [97](10_Chapter3.xhtml#page_97)\n\nas not a blueprint, [91–4](10_Chapter3.xhtml#page_91)\n\nas not necessarily bad or unacceptably reductive, [22](06_Intro.xhtml#page_22)\n\npurposes of, [81](10_Chapter3.xhtml#page_81)\n\ntally sticks and, [79–84](10_Chapter3.xhtml#page_79)\n\nobjective, torsion of with subjective, [146](15_Chapter6.xhtml#page_146),\n[171](16_Chapter7.xhtml#page_171) “objective” leftism,\n[19](06_Intro.xhtml#page_19)\n\nobjective metaphysics, [88–90](10_Chapter3.xhtml#page_88)\n\nobjectivism, managed objectivism, [172](16_Chapter7.xhtml#page_172)\n\nobject-oriented philosophy, [164n12](16_Chapter7.xhtml#page_164)\n\nO’Neil, Cathy, [29](06_Intro.xhtml#page_29)\n\nontology, naturality and, [201–6](17_Chapter8.xhtml#page_201)\n\nopacity, [62–6](09_Chapter2.xhtml#page_62)\n\n_operaismo_ (workerism), [25](06_Intro.xhtml#page_25)\n\noverfitting, [49](08_Chapter1.xhtml#page_49), [50](08_Chapter1.xhtml#page_50)\n\novergeneralization, [49](08_Chapter1.xhtml#page_49),\n[50](08_Chapter1.xhtml#page_50)\n\n**P**\n\nPageRank algorithm (Google), [78](10_Chapter3.xhtml#page_78),\n[183–4](17_Chapter8.xhtml#page_183)\n\nPanagia, Davide, [57n38](08_Chapter1.xhtml#page_57)\n\nPapert, Seymour, [54](08_Chapter1.xhtml#page_54)\n\npatents, [190n20](17_Chapter8.xhtml#page_190)\n\npatriarchy\n\nas constructed atop abstractions that separate concepts from context,\n[179](17_Chapter8.xhtml#page_179)\n\nmachine learning systems as working to solidify,\n[58](08_Chapter1.xhtml#page_58)\n\nPearson, Egon, [123–30](13_Chapter5.xhtml#page_123),\n[136](13_Chapter5.xhtml#page_136), [137](13_Chapter5.xhtml#page_137),\n[138](13_Chapter5.xhtml#page_138), [139](13_Chapter5.xhtml#page_139),\n[144](15_Chapter6.xhtml#page_144), [156](15_Chapter6.xhtml#page_156),\n[164](16_Chapter7.xhtml#page_164), [169](16_Chapter7.xhtml#page_169),\n[172](16_Chapter7.xhtml#page_172), [175](16_Chapter7.xhtml#page_175),\n[189](17_Chapter8.xhtml#page_189), [222](18_Conclusion.xhtml#page_222)\n\nPearson, Karl, [117](13_Chapter5.xhtml#page_117),\n[160n3](16_Chapter7.xhtml#page_160)\n\nPennachin, Cassio, [43n11](08_Chapter1.xhtml#page_43)\n\npercentage belief, [159–62](16_Chapter7.xhtml#page_159)\n\nperceptron, [54](08_Chapter1.xhtml#page_54)\n\n_Perceptrons: An Introduction to Computational Geometry_ (Minsky and Papert),\n[54](08_Chapter1.xhtml#page_54)\n\nPetabyte Age, [56](08_Chapter1.xhtml#page_56)\n\np-hacking, [134](13_Chapter5.xhtml#page_134)\n\nPhillips, Lawrence, [144](15_Chapter6.xhtml#page_144),\n[145](15_Chapter6.xhtml#page_145)\n\nPichai, Sundar, [202](17_Chapter8.xhtml#page_202)\n\nPierre-Dupuy, Jean, [30n55](06_Intro.xhtml#page_30)\n\nPitts, Walter, [53–4](08_Chapter1.xhtml#page_53)\n\nPius IX (pope), [71](09_Chapter2.xhtml#page_71)\n\nplatform capitalism, [26](06_Intro.xhtml#page_26),\n[192–3](17_Chapter8.xhtml#page_192)\n\nPlato, [164n12](16_Chapter7.xhtml#page_164), [178](17_Chapter8.xhtml#page_178)\n\nPoincaré, Henri, [103](12_Chapter4.xhtml#page_103),\n[105](12_Chapter4.xhtml#page_105)\n\npoint estimates, [159n2](16_Chapter7.xhtml#page_159)\n\npolitical economy\n\ncrises in statistics and capitalism as deriving from,\n[33](06_Intro.xhtml#page_33)\n\ndebates about nature of, [23–7](06_Intro.xhtml#page_23)\n\nfailure to account for, [220](18_Conclusion.xhtml#page_220)\n\ngrowth in importance of to science, [133](13_Chapter5.xhtml#page_133)\n\nknowledge as depending on, [222](18_Conclusion.xhtml#page_222)\n\nstatistics as functioning within and through,\n[175](16_Chapter7.xhtml#page_175)\n\npolitical subject, question of viability of a unified political subject who\ncould foment global revolutionary change, [16](06_Intro.xhtml#page_16)\n\nPopper, Karl, [123](13_Chapter5.xhtml#page_123),\n[124](13_Chapter5.xhtml#page_124), [136](13_Chapter5.xhtml#page_136)\n\npopulation genetics, [121](13_Chapter5.xhtml#page_121)\n\npopulations, statistical manipulation of, [29](06_Intro.xhtml#page_29)\n\nposterior distributions, [159n2](16_Chapter7.xhtml#page_159)\n\nPostone, Moishe, [13](06_Intro.xhtml#page_13), [18](06_Intro.xhtml#page_18),\n[27](06_Intro.xhtml#page_27), [28](06_Intro.xhtml#page_28),\n[76](09_Chapter2.xhtml#page_76), [89n19](10_Chapter3.xhtml#page_89),\n[208n54](17_Chapter8.xhtml#page_208), [209](17_Chapter8.xhtml#page_209)\n\nPrice, Richard, [162–3](16_Chapter7.xhtml#page_162),\n[164](16_Chapter7.xhtml#page_164)\n\nprimitive accumulation, [190](17_Chapter8.xhtml#page_190),\n[191](17_Chapter8.xhtml#page_191)\n\nprivatization\n\nof economy, [200](17_Chapter8.xhtml#page_200)\n\nof general knowledge, [211](17_Chapter8.xhtml#page_211),\n[218](18_Conclusion.xhtml#page_218)\n\nneoliberalism’s move toward, [3–4](06_Intro.xhtml#page_3)\n\nprobability\n\nBayesian analysis as admitting to theistic understanding of,\n[162](16_Chapter7.xhtml#page_162)\n\nDutch book argument as offering economic ground for mathematical laws of,\n[164](16_Chapter7.xhtml#page_164)\n\neconomic dynamics of, [105](12_Chapter4.xhtml#page_105)\n\nfrequentist description of, [104–5](12_Chapter4.xhtml#page_104)\n\ninverse probability, [150](15_Chapter6.xhtml#page_150)\n\nand management of contemporary capitalism, [39](08_Chapter1.xhtml#page_39)\n\nas not directly assigned to hypothesis, [150](15_Chapter6.xhtml#page_150)\n\nof precipitation (PoP), [102](12_Chapter4.xhtml#page_102)\n\nrequirements of to become objective, [146](15_Chapter6.xhtml#page_146)\n\nunderstanding of, [101–4](12_Chapter4.xhtml#page_101)\n\nProPublica, [44](08_Chapter1.xhtml#page_44)\n\nprostate-specific antigen (PSA) blood tests, USPSTF recommendation about,\n[127–8](13_Chapter5.xhtml#page_127)\n\nPuar, Jasbir, [181](17_Chapter8.xhtml#page_181)\n\np-value (probability value), [117–20](13_Chapter5.xhtml#page_117),\n[124](13_Chapter5.xhtml#page_124), [138](13_Chapter5.xhtml#page_138)\n\n**R**\n\nrace, UNESCO’s statement on nature of, [121–2](13_Chapter5.xhtml#page_121)\n\nracism\n\nas allowing production of profit, [93](10_Chapter3.xhtml#page_93)\n\ncapital accumulation’s constant drive to profit from,\n[220](18_Conclusion.xhtml#page_220)\n\ncentrality of to development and deployment of biopolitical control,\n[29](06_Intro.xhtml#page_29)\n\nof COMPAS, [44](08_Chapter1.xhtml#page_44)\n\nconstant drive to profit from, [220](18_Conclusion.xhtml#page_220)\n\nof Fisher, [121–3](13_Chapter5.xhtml#page_121)\n\nas form of concrete domination, [92](10_Chapter3.xhtml#page_92)\n\nmachine learning systems as working to solidify,\n[58](08_Chapter1.xhtml#page_58)\n\nas mobile and multifaceted system that defines varied axes of exchange,\noppression, and injustice, [215](17_Chapter8.xhtml#page_215)\n\nas not reducible to capitalism, [93](10_Chapter3.xhtml#page_93)\n\nobjectified forces as translating of into abstract demands of capital,\n[210](17_Chapter8.xhtml#page_210)\n\nas presented in objective form, [174](16_Chapter7.xhtml#page_174)\n\nracist housing policies, [204](17_Chapter8.xhtml#page_204)\n\nas supporting injustice and exploitation, [75](09_Chapter2.xhtml#page_75)\n\nradical political theory, crisis of, [15–16](06_Intro.xhtml#page_15)\n\nradical politics\n\nfragmentation of, [18](06_Intro.xhtml#page_18)\n\nmath, statistics, and science as necessary components of,\n[22](06_Intro.xhtml#page_22)\n\nrationality, crisis of, [4](06_Intro.xhtml#page_4)\n\nreal abstraction, [11](06_Intro.xhtml#page_11), [12](06_Intro.xhtml#page_12),\n[13](06_Intro.xhtml#page_13), [31](06_Intro.xhtml#page_31),\n[75](09_Chapter2.xhtml#page_75), [94](10_Chapter3.xhtml#page_94),\n[139](13_Chapter5.xhtml#page_139), [159](16_Chapter7.xhtml#page_159),\n[223](18_Conclusion.xhtml#page_223)\n\nredlining, [14](06_Intro.xhtml#page_14), [204n48](17_Chapter8.xhtml#page_204)\n\nreference classes, [40](08_Chapter1.xhtml#page_40),\n[145–6](15_Chapter6.xhtml#page_145)\n\nreference class problem, [40](08_Chapter1.xhtml#page_40),\n[50](08_Chapter1.xhtml#page_50)\n\nregulation, of algorithms, [60](09_Chapter2.xhtml#page_60),\n[61](09_Chapter2.xhtml#page_61)\n\nreification, theory of, [80n4](10_Chapter3.xhtml#page_80),\n[82](10_Chapter3.xhtml#page_82), [83n11](10_Chapter3.xhtml#page_83)\n\nreplication crises, [134](13_Chapter5.xhtml#page_134),\n[160](16_Chapter7.xhtml#page_160), [200](17_Chapter8.xhtml#page_200),\n[219](18_Conclusion.xhtml#page_219)\n\nre-simulation, methods of, [7](06_Intro.xhtml#page_7)\n\nrevolutionary mathematics\n\naim of, [34](06_Intro.xhtml#page_34), [224](18_Conclusion.xhtml#page_224)\n\nas attempting to create new objectifications,\n[214](17_Chapter8.xhtml#page_214), [216](17_Chapter8.xhtml#page_216)\n\nas calling for reconsideration of relationships between alienation and\nliberation, [212–16](17_Chapter8.xhtml#page_212)\n\nneed for, [33](06_Intro.xhtml#page_33)\n\nultimate task of, [174–5](16_Chapter7.xhtml#page_174),\n[176](16_Chapter7.xhtml#page_176)\n\nthe work of, [33](06_Intro.xhtml#page_33),\n[139–40](13_Chapter5.xhtml#page_139)\n\nrevolutionary object, theory of, [83n9](10_Chapter3.xhtml#page_83)\n\nrevolutionary political coalition, no unifying force as currently existing to\nhold it together, [18](06_Intro.xhtml#page_18)\n\nrevolutionary subject\n\naccording to Marx, [15](06_Intro.xhtml#page_15)\n\nas beset by algorithmically fragmented reality and intensely managed digital\ncontrol, [16](06_Intro.xhtml#page_16)\n\nas under siege and in doubt, [18](06_Intro.xhtml#page_18)\n\nrisk, according to Knight, [39](08_Chapter1.xhtml#page_39)\n\nRobinson, Cedric, [91](10_Chapter3.xhtml#page_91)\n\nRosenblatt, Frank, [53–4](08_Chapter1.xhtml#page_53)\n\n**S**\n\nSadowski, Jathan, [25n46](06_Intro.xhtml#page_25)\n\nSaunderson, Nicholas, [143n1](15_Chapter6.xhtml#page_143)\n\nSavage, Leonard, [103](12_Chapter4.xhtml#page_103),\n[130](13_Chapter5.xhtml#page_130), [146–7](15_Chapter6.xhtml#page_146),\n[158](16_Chapter7.xhtml#page_158), [169–70](16_Chapter7.xhtml#page_169),\n[171](16_Chapter7.xhtml#page_171), [189](17_Chapter8.xhtml#page_189),\n[213](17_Chapter8.xhtml#page_213), [220](18_Conclusion.xhtml#page_220),\n[221](18_Conclusion.xhtml#page_221), [222](18_Conclusion.xhtml#page_222)\n\nSchelling, F.W.J., [49](08_Chapter1.xhtml#page_49)\n\nSchrödinger, Erwin, [186–7](17_Chapter8.xhtml#page_186)\n\nscience\n\nas increasingly involving huge amounts of data, [2](06_Intro.xhtml#page_2)\n\nkey question in, [220](18_Conclusion.xhtml#page_220)\n\nscientific inference, [9](06_Intro.xhtml#page_9),\n[114n23](12_Chapter4.xhtml#page_114), [135](13_Chapter5.xhtml#page_135),\n[160](16_Chapter7.xhtml#page_160)\n\nscientific production, split with capitalism,\n[174](16_Chapter7.xhtml#page_174)\n\nsearch space, [80](10_Chapter3.xhtml#page_80)\n\nSerres, Michel, [133](13_Chapter5.xhtml#page_133)\n\nSevignani, Sebastian, [24n43](06_Intro.xhtml#page_24),\n[209n59](17_Chapter8.xhtml#page_209)\n\nsexism\n\ncapital accumulation’s constant drive to profit from,\n[220](18_Conclusion.xhtml#page_220)\n\nas form of concrete domination, [92](10_Chapter3.xhtml#page_92)\n\nas mobile and multifaceted system that defines varied axes of exchange,\noppression, and injustice, [215](17_Chapter8.xhtml#page_215)\n\nas not reducible to capitalism, [93](10_Chapter3.xhtml#page_93)\n\nobjectified forces as translating of into abstract demands of capital,\n[210](17_Chapter8.xhtml#page_210)\n\nas presented in objective form, [174](16_Chapter7.xhtml#page_174)\n\nas supporting injustice and exploitation, [75](09_Chapter2.xhtml#page_75)\n\nShiva, Vandana, [179](17_Chapter8.xhtml#page_179)\n\nSmith, Adam, [190](17_Chapter8.xhtml#page_190)\n\nSmith, George Davey, [118n7](13_Chapter5.xhtml#page_118)\n\nSmythe, Dallas, [24n43](06_Intro.xhtml#page_24),\n[88n16](10_Chapter3.xhtml#page_88)\n\nsocialism, as suffering from calculation problem,\n[195–6](17_Chapter8.xhtml#page_195)\n\nsocial problems, [19](06_Intro.xhtml#page_19),\n[60](09_Chapter2.xhtml#page_60). _See also specific problems_\n\nSocrates, [164n12](16_Chapter7.xhtml#page_164)\n\nSohn-Rethel, Alfred, [3](06_Intro.xhtml#page_3), [11](06_Intro.xhtml#page_11),\n[75](09_Chapter2.xhtml#page_75), [139](13_Chapter5.xhtml#page_139),\n[171](16_Chapter7.xhtml#page_171), [179n3](17_Chapter8.xhtml#page_179),\n[221](18_Conclusion.xhtml#page_221)\n\nsplit tally sticks, [79–84](10_Chapter3.xhtml#page_79)\n\nSrnicek, Nick, [25n46](06_Intro.xhtml#page_25),\n[192](17_Chapter8.xhtml#page_192)\n\nStandard 754 (IEEE), [68n14](09_Chapter2.xhtml#page_68)\n\nstatistical analyses\n\nability of to evaluate diverse types of data,\n[119](13_Chapter5.xhtml#page_119)\n\nBayesian approach as fundamentally changing stakes of,\n[160](16_Chapter7.xhtml#page_160)\n\nas facing possibility that what was observed was merely result of random\nchance, [109](12_Chapter4.xhtml#page_109)\n\nstatistical inference, [34](06_Intro.xhtml#page_34),\n[108](12_Chapter4.xhtml#page_108), [114n23](12_Chapter4.xhtml#page_114),\n[120](13_Chapter5.xhtml#page_120), [129](13_Chapter5.xhtml#page_129),\n[148](15_Chapter6.xhtml#page_148), [150](15_Chapter6.xhtml#page_150),\n[199](17_Chapter8.xhtml#page_199)\n\n_Statistical Methods for Research Workers_ (Fisher),\n[115](13_Chapter5.xhtml#page_115)\n\nstatistical modeling, [41–3](08_Chapter1.xhtml#page_41)\n\nstatistical separability, theory of, [54](08_Chapter1.xhtml#page_54)\n\nstatistics\n\nability of to revolutionize production, [10](06_Intro.xhtml#page_10)\n\ncrisis of, [4](06_Intro.xhtml#page_4), [33](06_Intro.xhtml#page_33)\n\nas effectively laundering nonobjective forms of violence and bias,\n[174](16_Chapter7.xhtml#page_174)\n\nepistemic weightiness of, [96](10_Chapter3.xhtml#page_96)\n\nFisher-Neyman-Pearson model as, [131](13_Chapter5.xhtml#page_131)\n\nfoundation of, [13](06_Intro.xhtml#page_13), [171](16_Chapter7.xhtml#page_171)\n\nfrequentist statistics. _See_ frequentist statistics/\n\nfrequentism goal of, [219](18_Conclusion.xhtml#page_219)\n\nhybridization of models of, [130–5](13_Chapter5.xhtml#page_130),\n[137](13_Chapter5.xhtml#page_137), [139](13_Chapter5.xhtml#page_139)\n\nas immaterial thing that can do everything, [31](06_Intro.xhtml#page_31)\n\nas increasingly governed by, [7](06_Intro.xhtml#page_7)\n\nand management of contemporary capitalism, [39](08_Chapter1.xhtml#page_39)\n\nmetaphysical foundations of, [122](13_Chapter5.xhtml#page_122)\n\nas more efficient at creating new realities than in representing a world,\n[7](06_Intro.xhtml#page_7)\n\nas nothing short of magic, [175](16_Chapter7.xhtml#page_175)\n\nas objectifying, [31–2](06_Intro.xhtml#page_31), [33](06_Intro.xhtml#page_33)\n\nas operating on two levels, [8–9](06_Intro.xhtml#page_8)\n\nas providing objectifying force of algorithmic knowledge,\n[88](10_Chapter3.xhtml#page_88)\n\nas providing tools for distinguishing signal from background noise,\n[132](13_Chapter5.xhtml#page_132)\n\nrelation of to capitalism, [12–13](06_Intro.xhtml#page_12)\n\nrole of in mediating society and economics “behind our backs,”\n[20](06_Intro.xhtml#page_20)\n\nuse of to create actionable information for human and computer consumption,\n[2](06_Intro.xhtml#page_2)\n\nas victim of its own success, [119](13_Chapter5.xhtml#page_119)\n\nSterne, Jonathan, [118n7](13_Chapter5.xhtml#page_118)\n\nStigler, Stephen, [143n1](15_Chapter6.xhtml#page_143)\n\nstock market, [87](10_Chapter3.xhtml#page_87)\n\nsubjective, torsion of with objective, [146](15_Chapter6.xhtml#page_146),\n[171](16_Chapter7.xhtml#page_171)\n\nsubjective belief, [94](10_Chapter3.xhtml#page_94),\n[144](15_Chapter6.xhtml#page_144), [148](15_Chapter6.xhtml#page_148),\n[158](16_Chapter7.xhtml#page_158), [163](16_Chapter7.xhtml#page_163),\n[164](16_Chapter7.xhtml#page_164), [166](16_Chapter7.xhtml#page_166),\n[221](18_Conclusion.xhtml#page_221)\n\n“subjective” leftism, [19](06_Intro.xhtml#page_19)\n\nsubjectivism, [146–8](15_Chapter6.xhtml#page_146),\n[155](15_Chapter6.xhtml#page_155), [160](16_Chapter7.xhtml#page_160)\n\nsupervised learning, [51](08_Chapter1.xhtml#page_51),\n[52](08_Chapter1.xhtml#page_52)\n\nsurveillance capitalism, [26](06_Intro.xhtml#page_26)\n\nsystemic biases, algorithms and, [60](09_Chapter2.xhtml#page_60)\n\n**T**\n\ntally sticks, and objectification, [79–84](10_Chapter3.xhtml#page_79)\n\nTarget, use of data to predict customer behaviors,\n[44](08_Chapter1.xhtml#page_44)\n\ntargeted advertising, [10](06_Intro.xhtml#page_10),\n[15](06_Intro.xhtml#page_15), [28](06_Intro.xhtml#page_28)\n\nTaylorist revolution/Taylorism, [10](06_Intro.xhtml#page_10),\n[156](15_Chapter6.xhtml#page_156)\n\ntechnology\n\nas being turned against humanity’s very survival, [2](06_Intro.xhtml#page_2)\n\nnegative consequences of, [3](06_Intro.xhtml#page_3)\n\nobjectification as lying at heart of, [22](06_Intro.xhtml#page_22)\n\nTeichmüller theory, [65](09_Chapter2.xhtml#page_65)\n\ntemporary correlation, [55](08_Chapter1.xhtml#page_55)\n\ntheoretical mathematics, and problem of opacity,\n[62–5](09_Chapter2.xhtml#page_62)\n\nTheranos, [195](17_Chapter8.xhtml#page_195)\n\nTimcke, Scott, [7n13](06_Intro.xhtml#page_7)\n\nTiqqun, [17](06_Intro.xhtml#page_17)\n\ntransparency, [61](09_Chapter2.xhtml#page_61)\n\ntransphobia, as mobile and multifaceted system that defines varied axes of\nexchange, oppression, and injustice, [215](17_Chapter8.xhtml#page_215)\n\nTroncoso, Stacco, [95](10_Chapter3.xhtml#page_95),\n[96](10_Chapter3.xhtml#page_96)\n\nTrotsky, Leon, [141](14_Part3.xhtml#page_141)\n\nTrump, Donald, [94](10_Chapter3.xhtml#page_94)\n\ntrust, as foundation of economic system, [4–5](06_Intro.xhtml#page_4)\n\ntruth\n\ncrisis of, [4](06_Intro.xhtml#page_4)\n\nscience of, [139](13_Chapter5.xhtml#page_139)\n\nTuring, Alan, [78](10_Chapter3.xhtml#page_78)\n\nTuring machine, [78](10_Chapter3.xhtml#page_78)\n\nTymoczko, Thomas, [63–4](09_Chapter2.xhtml#page_63),\n[66](09_Chapter2.xhtml#page_66), [70](09_Chapter2.xhtml#page_70)\n\ntype I errors, [107](12_Chapter4.xhtml#page_107),\n[111](12_Chapter4.xhtml#page_111), [126](13_Chapter5.xhtml#page_126),\n[127](13_Chapter5.xhtml#page_127), [151](15_Chapter6.xhtml#page_151),\n[152](15_Chapter6.xhtml#page_152)\n\ntype II errors, [111](12_Chapter4.xhtml#page_111),\n[124](13_Chapter5.xhtml#page_124), [126](13_Chapter5.xhtml#page_126)\n\n**U**\n\nUber\n\n“Greyball” program, [5–6](06_Intro.xhtml#page_5)\n\nsuccess of, [192](17_Chapter8.xhtml#page_192)\n\nas taxing exchanges, [193](17_Chapter8.xhtml#page_193)\n\nuncertainty, according to Knight, [39](08_Chapter1.xhtml#page_39)\n\nUNESCO, statement on nature of race, [121–2](13_Chapter5.xhtml#page_121)\n\nUnited States Preventive Services Task Force\n\n(USPSTF), recommendation about PSA testing,\n[127–8](13_Chapter5.xhtml#page_127)\n\nuniversal form, [179](17_Chapter8.xhtml#page_179)\n\nuniversal inference, [11](06_Intro.xhtml#page_11)\n\nunsupervised learning, [51](08_Chapter1.xhtml#page_51),\n[52](08_Chapter1.xhtml#page_52)\n\nUS Environmental Protection Agency, on Volkswagen’s defeat device,\n[4–5](06_Intro.xhtml#page_4)\n\n**V**\n\nvalue, as deriving directly from automatic computation,\n[223](18_Conclusion.xhtml#page_223)\n\nvalue extraction, nature of, [25](06_Intro.xhtml#page_25)\n\nVenn, John, [40](08_Chapter1.xhtml#page_40)\n\nVernant, Jean-Pierre, [179n3](17_Chapter8.xhtml#page_179)\n\nVirno, Paulo, [193](17_Chapter8.xhtml#page_193),\n[198](17_Chapter8.xhtml#page_198), [219](18_Conclusion.xhtml#page_219),\n[220–1](18_Conclusion.xhtml#page_220)\n\nVolkswagen, defeat device, [4–5](06_Intro.xhtml#page_4),\n[6](06_Intro.xhtml#page_6), [60](09_Chapter2.xhtml#page_60)\n\nvon Mises, Ludwig, [195–6](17_Chapter8.xhtml#page_195),\n[197](17_Chapter8.xhtml#page_197)\n\n**W**\n\nWalmart\n\nhurricane data used by, [37–8](08_Chapter1.xhtml#page_37),\n[41](08_Chapter1.xhtml#page_41)\n\nuse of data to predict customer behaviors, [44](08_Chapter1.xhtml#page_44)\n\nWang, Jackie, [91–2](10_Chapter3.xhtml#page_91)\n\nWeheliye, Alexander, [29](06_Intro.xhtml#page_29)\n\nwhite supremacy\n\nas constructed atop abstractions that separate concepts from context,\n[179](17_Chapter8.xhtml#page_179)\n\nobjectification of into statistical matrix of one’s property value,\n[204](17_Chapter8.xhtml#page_204)\n\nreproduction and incentivization of, [93](10_Chapter3.xhtml#page_93)\n\n“Why Most Published Research Findings Are False” (Ioannidis),\n[135](13_Chapter5.xhtml#page_135), [139](13_Chapter5.xhtml#page_139)\n\nWynter, Sylvia, [203](17_Chapter8.xhtml#page_203)\n\n**X**\n\nxenophobia, as mobile and multifaceted system that defines varied axes of\nexchange, oppression, and injustice, [215](17_Chapter8.xhtml#page_215)\n\n**Y**\n\nYouTube\n\nuse of videos as training data for Google’s image recognition system,\n[186](17_Chapter8.xhtml#page_186), [187](17_Chapter8.xhtml#page_187)\n\nvideo recommendation algorithms, [92–3](10_Chapter3.xhtml#page_92)\n\n**Z**\n\nŽižek, Slavoj, [23n42](06_Intro.xhtml#page_23),\n[60n2](09_Chapter2.xhtml#page_60)\n\n",
    "book_id": "revolutionary_mathematics",
    "book_title": "Revolutionary Mathematics",
    "book_author": "Justin Joque",
    "topic_id": "ai_theory",
    "topic_label": "theory",
    "chunk_index": 4
  },
  {
    "chunk_full": "Superintelligence\n\n\n# SUPERINTELLIGENCE\n\n_Paths, Dangers, Strategies_\n\n**NICK BOSTROM**\n\n_Director, Future of Humanity Institute  \nProfessor, Faculty of Philosophy & Oxford Martin School  \nUniversity of Oxford_\n\n![Image](images/00001.jpg)\n\n\n![Image](images/00002.jpg)\n\nGreat Clarendon Street, Oxford, OX2 6DP,  \nUnited Kingdom\n\nOxford University Press is a department of the University of Oxford.  \nIt furthers the University’s objective of excellence in research, scholarship,  \nand education by publishing worldwide. Oxford is a registered trade mark of  \nOxford University Press in the UK and in certain other countries\n\n© Nick Bostrom 2014\n\nThe moral rights of the author have been asserted\n\nFirst Edition published in 2014\n\nImpression: 1\n\nAll rights reserved. No part of this publication may be reproduced, stored in\na retrieval system, or transmitted, in any form or by any means, without the\nprior permission in writing of Oxford University Press, or as expressly\npermitted by law, by licence or under terms agreed with the appropriate\nreprographics rights organization. Enquiries concerning reproduction outside\nthe scope of the above should be sent to the Rights Department, Oxford\nUniversity Press, at the address above\n\nYou must not circulate this work in any other form  \nand you must impose this same condition on any acquirer\n\nBritish Library Cataloguing in Publication Data  \nData available\n\nLibrary of Congress Control Number: 2013955152\n\nISBN 978–0–19–967811–2\n\nPrinted in Italy by  \nL.E.G.O. S.p.A.—Lavis TN\n\nLinks to third party websites are provided by Oxford in good faith and for\ninformation only. Oxford disclaims any responsibility for the materials\ncontained in any third party website referenced in this work.\n\n\n### The Unfinished Fable of the Sparrows\n\nIt was the nest-building season, but after days of long hard work, the\nsparrows sat in the evening glow, relaxing and chirping away.\n\n“We are all so small and weak. Imagine how easy life would be if we had an owl\nwho could help us build our nests!”\n\n“Yes!” said another. “And we could use it to look after our elderly and our\nyoung.”\n\n“It could give us advice and keep an eye out for the neighborhood cat,” added\na third.\n\nThen Pastus, the elder-bird, spoke: “Let us send out scouts in all directions\nand try to find an abandoned owlet somewhere, or maybe an egg. A crow chick\nmight also do, or a baby weasel. This could be the best thing that ever\nhappened to us, at least since the opening of the Pavilion of Unlimited Grain\nin yonder backyard.”\n\nThe flock was exhilarated, and sparrows everywhere started chirping at the top\nof their lungs.\n\nOnly Scronkfinkle, a one-eyed sparrow with a fretful temperament, was\nunconvinced of the wisdom of the endeavor. Quoth he: “This will surely be our\nundoing. Should we not give some thought to the art of owl-domestication and\nowl-taming first, before we bring such a creature into our midst?”\n\nReplied Pastus: “Taming an owl sounds like an exceedingly difficult thing to\ndo. It will be difficult enough to find an owl egg. So let us start there.\nAfter we have succeeded in raising an owl, then we can think about taking on\nthis other challenge.”\n\n“There is a flaw in that plan!” squeaked Scronkfinkle; but his protests were\nin vain as the flock had already lifted off to start implementing the\ndirectives set out by Pastus.\n\nJust two or three sparrows remained behind. Together they began to try to work\nout how owls might be tamed or domesticated. They soon realized that Pastus\nhad been right: this was an exceedingly difficult challenge, especially in the\nabsence of an actual owl to practice on. Nevertheless they pressed on as best\nthey could, constantly fearing that the flock might return with an owl egg\nbefore a solution to the control problem had been found.\n\nIt is not known how the story ends, but the author dedicates this book to\nScronkfinkle and his followers.\n\n\n## PREFACE\n\nInside your cranium is the thing that does the reading. This thing, the human\nbrain, has some capabilities that the brains of other animals lack. It is to\nthese distinctive capabilities that we owe our dominant position on the\nplanet. Other animals have stronger muscles and sharper claws, but we have\ncleverer brains. Our modest advantage in general intelligence has led us to\ndevelop language, technology, and complex social organization. The advantage\nhas compounded over time, as each generation has built on the achievements of\nits predecessors.\n\nIf some day we build machine brains that surpass human brains in general\nintelligence, then this new superintelligence could become very powerful. And,\nas the fate of the gorillas now depends more on us humans than on the gorillas\nthemselves, so the fate of our species would depend on the actions of the\nmachine superintelligence.\n\nWe do have one advantage: we get to build the stuff. In principle, we could\nbuild a kind of superintelligence that would protect human values. We would\ncertainly have strong reason to do so. In practice, the control problem—the\nproblem of how to control what the superintelligence would do—looks quite\ndifficult. It also looks like we will only get one chance. Once unfriendly\nsuperintelligence exists, it would prevent us from replacing it or changing\nits preferences. Our fate would be sealed.\n\nIn this book, I try to understand the challenge presented by the prospect of\nsuperintelligence, and how we might best respond. This is quite possibly the\nmost important and most daunting challenge humanity has ever faced.\nAnd—whether we succeed or fail—it is probably the last challenge we will ever\nface.\n\nIt is no part of the argument in this book that we are on the threshold of a\nbig breakthrough in artificial intelligence, or that we can predict with any\nprecision when such a development might occur. It seems somewhat likely that\nit will happen sometime in this century, but we don’t know for sure. The first\ncouple of chapters do discuss possible pathways and say something about the\nquestion of timing. The bulk of the book, however, is about what happens\nafter. We study the kinetics of an intelligence explosion, the forms and\npowers of superintelligence, and the strategic choices available to a\nsuperintelligent agent that attains a decisive advantage. We then shift our\nfocus to the control problem and ask what we could do to shape the initial\nconditions so as to achieve a survivable and beneficial outcome. Toward the\nend of the book, we zoom out and contemplate the larger picture that emerges\nfrom our investigations. Some suggestions are offered on what ought to be done\nnow to increase our chances of avoiding an existential catastrophe later.\n\nThis has not been an easy book to write. I hope the path that has been cleared\nwill enable other investigators to reach the new frontier more swiftly and\nconveniently, so that they can arrive there fresh and ready to join the work\nto further expand the reach of our comprehension. (And if the way that has\nbeen made is a little bumpy and bendy, I hope that reviewers, in judging the\nresult, will not underestimate the hostility of the terrain _ex ante_!)\n\nThis has not been an easy book to write: I have tried to make it an easy book\nto read, but I don’t think I have quite succeeded. When writing, I had in mind\nas the target audience an earlier time-slice of myself, and I tried to produce\na kind of book that I would have enjoyed reading. This could prove a narrow\ndemographic. Nevertheless, I think that the content should be accessible to\nmany people, if they put some thought into it and resist the temptation to\ninstantaneously misunderstand each new idea by assimilating it with the most\nsimilar-sounding cliché available in their cultural larders. Non-technical\nreaders should not be discouraged by the occasional bit of mathematics or\nspecialized vocabulary, for it is always possible to glean the main point from\nthe surrounding explanations. (Conversely, for those readers who want more of\nthe nitty-gritty, there is quite a lot to be found among the\nendnotes.[1](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos907104))\n\nMany of the points made in this book are probably\nwrong.[2](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos907254) It\nis also likely that there are considerations of critical importance that I\nfail to take into account, thereby invalidating some or all of my conclusions.\nI have gone to some length to indicate nuances and degrees of uncertainty\nthroughout the text—encumbering it with an unsightly smudge of “possibly,”\n“might,” “may,” “could well,” “it seems,” “probably,” “very likely,” “almost\ncertainly.” Each qualifier has been placed where it is carefully and\ndeliberately. Yet these topical applications of epistemic modesty are not\nenough; they must be supplemented here by a systemic admission of uncertainty\nand fallibility. This is not false modesty: for while I believe that my book\nis likely to be seriously wrong and misleading, I think that the alternative\nviews that have been presented in the literature are substantially\nworse—including the default view, or “null hypothesis,” according to which we\ncan for the time being safely or reasonably ignore the prospect of\nsuperintelligence.\n\n\n## ACKNOWLEDGMENTS\n\nThe membrane that has surrounded the writing process has been fairly\npermeable. Many concepts and ideas generated while working on the book have\nbeen allowed to seep out and have become part of a wider conversation; and, of\ncourse, numerous insights originating from the outside while the book was\nunderway have been incorporated into the text. I have tried to be somewhat\ndiligent with the citation apparatus, but the influences are too many to fully\ndocument.\n\nFor extensive discussions that have helped clarify my thinking I am grateful\nto a large set of people, including Ross Andersen, Stuart Armstrong, Owen\nCotton-Barratt, Nick Beckstead, David Chalmers, Paul Christiano, Milan\nĆirković, Daniel Dennett, David Deutsch, Daniel Dewey, Eric Drexler, Peter\nEckersley, Amnon Eden, Owain Evans, Benja Fallenstein, Alex Flint, Carl Frey,\nIan Goldin, Katja Grace, J. Storrs Hall, Robin Hanson, Demis Hassabis, James\nHughes, Marcus Hutter, Garry Kasparov, Marcin Kulczycki, Shane Legg, Moshe\nLooks, Willam MacAskill, Eric Mandelbaum, James Martin, Lillian Martin, Roko\nMijic, Vincent Mueller, Elon Musk, Seán Ó hÉigeartaigh, Toby Ord, Dennis\nPamlin, Derek Parfit, David Pearce, Huw Price, Martin Rees, Bill Roscoe,\nStuart Russell, Anna Salamon, Lou Salkind, Anders Sandberg, Julian Savulescu,\nJürgen Schmidhuber, Nicholas Shackel, Murray Shanahan, Noel Sharkey, Carl\nShulman, Peter Singer, Dan Stoicescu, Jaan Tallinn, Alexander Tamas, Max\nTegmark, Roman Yampolskiy, and Eliezer Yudkowsky.\n\nFor especially detailed comments, I am grateful to Milan Ćirković, Daniel\nDewey, Owain Evans, Nick Hay, Keith Mansfield, Luke Muehlhauser, Toby Ord,\nJess Riedel, Anders Sandberg, Murray Shanahan, and Carl Shulman. For advice or\nresearch help with different parts I want to thank Stuart Armstrong, Daniel\nDewey, Eric Drexler, Alexandre Erler, Rebecca Roache, and Anders Sandberg.\n\nFor help with preparing the manuscript, I am thankful to Caleb Bell, Malo\nBourgon, Robin Brandt, Lance Bush, Cathy Douglass, Alexandre Erler, Kristian\nRönn, Susan Rogers, Andrew Snyder-Beattie, Cecilia Tilli, and Alex Vermeer. I\nwant particularly to thank my editor Keith Mansfield for his plentiful\nencouragement throughout the project.\n\nMy apologies to everybody else who ought to have been remembered here.\n\nFinally, a most fond thank you to funders, friends, and family: without your\nbacking, this work would not have been done.\n\n\n## CONTENTS\n\n[_Lists of Figures, Tables, and\nBoxes_](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_007.html#filepos29657)\n\n[**1\\. Past developments and present\ncapabilities**](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos33916)\n\n[Growth modes and big\nhistory](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos35030)\n\n[Great\nexpectations](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos41440)\n\n[Seasons of hope and\ndespair](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos47084)\n\n[State of the\nart](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos70643)\n\n[Opinions about the future of machine\nintelligence](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos94366)\n\n[**2\\. Paths to\nsuperintelligence**](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos104440)\n\n[Artificial\nintelligence](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos107145)\n\n[Whole brain\nemulation](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos132713)\n\n[Biological\ncognition](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos148933)\n\n[Brain–computer\ninterfaces](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos177494)\n\n[Networks and\norganizations](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos190534)\n\n[Summary](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos196703)\n\n[**3\\. Forms of\nsuperintelligence**](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_010.html#filepos201138)\n\n[Speed\nsuperintelligence](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_010.html#filepos203444)\n\n[Collective\nsuperintelligence](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_010.html#filepos207431)\n\n[Quality\nsuperintelligence](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_010.html#filepos216444)\n\n[Direct and indirect\nreach](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_010.html#filepos221562)\n\n[Sources of advantage for digital\nintelligence](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_010.html#filepos225832)\n\n[**4\\. The kinetics of an intelligence\nexplosion**](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_011.html#filepos236122)\n\n[Timing and speed of the\ntakeoff](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_011.html#filepos237005)\n\n[Recalcitrance](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_011.html#filepos249624)\n\n[_Non-machine intelligence\npaths_](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_011.html#filepos250321)\n\n[_Emulation and AI\npaths_](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_011.html#filepos255378)\n\n[Optimization power and\nexplosivity](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_011.html#filepos275921)\n\n[**5\\. Decisive strategic\nadvantage**](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_012.html#filepos288874)\n\n[Will the frontrunner get a decisive strategic\nadvantage?](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_012.html#filepos291604)\n\n[How large will the successful project\nbe?](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_012.html#filepos303425)\n\n[_Monitoring_](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_012.html#filepos306452)\n\n[_International\ncollaboration_](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_012.html#filepos313560)\n\n[From decisive strategic advantage to\nsingleton](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_012.html#filepos318062)\n\n[**6\\. Cognitive\nsuperpowers**](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_013.html#filepos327482)\n\n[Functionalities and\nsuperpowers](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_013.html#filepos329965)\n\n[An AI takeover\nscenario](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_013.html#filepos342320)\n\n[Power over nature and\nagents](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_013.html#filepos355733)\n\n[**7\\. The superintelligent\nwill**](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_014.html#filepos373977)\n\n[The relation between intelligence and\nmotivation](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_014.html#filepos375054)\n\n[Instrumental\nconvergence](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_014.html#filepos386986)\n\n[_Self-\npreservation_](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_014.html#filepos389142)\n\n[_Goal-content\nintegrity_](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_014.html#filepos390063)\n\n[_Cognitive\nenhancement_](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_014.html#filepos395275)\n\n[_Technological\nperfection_](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_014.html#filepos399100)\n\n[_Resource\nacquisition_](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_014.html#filepos402164)\n\n[**8\\. Is the default outcome\ndoom?**](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_015.html#filepos408438)\n\n[Existential catastrophe as the default outcome of an intelligence\nexplosion?](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_015.html#filepos409314)\n\n[The treacherous\nturn](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_015.html#filepos413720)\n\n[Malignant failure\nmodes](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_015.html#filepos424472)\n\n[_Perverse\ninstantiation_](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_015.html#filepos426187)\n\n[_Infrastructure\nprofusion_](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_015.html#filepos434507)\n\n[_Mind crime_](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_015.html#filepos445944)\n\n[**9\\. The control\nproblem**](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_016.html#filepos449483)\n\n[Two agency\nproblems](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_016.html#filepos450542)\n\n[Capability control\nmethods](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_016.html#filepos457089)\n\n[_Boxing\nmethods_](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_016.html#filepos457939)\n\n[_Incentive\nmethods_](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_016.html#filepos464368)\n\n[_Stunting_](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_016.html#filepos479795)\n\n[_Tripwires_](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_016.html#filepos483998)\n\n[Motivation selection\nmethods](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_016.html#filepos489354)\n\n[_Direct\nspecification_](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_016.html#filepos490939)\n\n[_Domesticity_](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_016.html#filepos497880)\n\n[_Indirect\nnormativity_](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_016.html#filepos501091)\n\n[_Augmentation_](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_016.html#filepos502585)\n\n[Synopsis](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_016.html#filepos505939)\n\n[**10\\. Oracles, genies, sovereigns,\ntools**](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_017.html#filepos509701)\n\n[Oracles](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_017.html#filepos510663)\n\n[Genies and\nsovereigns](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_017.html#filepos521089)\n\n[Tool-AIs](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_017.html#filepos530399)\n\n[Comparison](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_017.html#filepos548144)\n\n[**11\\. Multipolar\nscenarios**](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos33916)\n\n[Of horses and\nmen](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_018.html#filepos558424)\n\n[_Wages and\nunemployment_](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_018.html#filepos559045)\n\n[_Capital and\nwelfare_](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_018.html#filepos563858)\n\n[_The Malthusian principle in a historical\nperspective_](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_018.html#filepos569294)\n\n[_Population growth and\ninvestment_](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_018.html#filepos574490)\n\n[Life in an algorithmic\neconomy](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_018.html#filepos580816)\n\n[_Voluntary slavery, casual\ndeath_](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_018.html#filepos584328)\n\n[_Would maximally efficient work be\nfun?_](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_018.html#filepos592514)\n\n[_Unconscious\noutsourcers?_](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_018.html#filepos600352)\n\n[_Evolution is not necessarily\nup_](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_018.html#filepos607008)\n\n[Post-transition formation of a\nsingleton?](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_018.html#filepos617985)\n\n[_A second\ntransition_](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_018.html#filepos618591)\n\n[_Superorganisms and scale\neconomies_](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_018.html#filepos622476)\n\n[_Unification by\ntreaty_](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_018.html#filepos630368)\n\n[**12\\. Acquiring\nvalues**](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos645293)\n\n[The value-loading\nproblem](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos646380)\n\n[Evolutionary\nselection](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos654088)\n\n[Reinforcement\nlearning](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos657226)\n\n[Associative value\naccretion](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos659457)\n\n[Motivational\nscaffolding](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos665777)\n\n[Value\nlearning](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos670139)\n\n[Emulation\nmodulation](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos701993)\n\n[Institution\ndesign](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos705946)\n\n[Synopsis](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos722070)\n\n[**13\\. Choosing the criteria for\nchoosing**](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_020.html#filepos727275)\n\n[The need for indirect\nnormativity](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_020.html#filepos728390)\n\n[Coherent extrapolated\nvolition](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_020.html#filepos735899)\n\n[_Some\nexplications_](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_020.html#filepos738195)\n\n[_Rationales for\nCEV_](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_020.html#filepos744245)\n\n[_Further\nremarks_](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_020.html#filepos752331)\n\n[Morality\nmodels](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_020.html#filepos757723)\n\n[Do What I\nMean](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_020.html#filepos767399)\n\n[Component\nlist](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_020.html#filepos772828)\n\n[_Goal\ncontent_](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_020.html#filepos775003)\n\n[_Decision\ntheory_](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_020.html#filepos779260)\n\n[_Epistemology_](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_020.html#filepos783026)\n\n[_Ratification_](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_020.html#filepos788107)\n\n[Getting close\nenough](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_020.html#filepos792579)\n\n[**14\\. The strategic\npicture**](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos795733)\n\n[Science and technology\nstrategy](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos797864)\n\n[_Differential technological\ndevelopment_](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos798284)\n\n[_Preferred order of\narrival_](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos803908)\n\n[_Rates of change and cognitive\nenhancement_](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos812779)\n\n[_Technology\ncouplings_](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos826156)\n\n[_Second-\nguessing_](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos832936)\n\n[Pathways and\nenablers](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos838720)\n\n[_Effects of hardware\nprogress_](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos839058)\n\n[_Should whole brain emulation research be\npromoted?_](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos845446)\n\n[_The person-affecting perspective favors\nspeed_](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos857107)\n\n[Collaboration](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos860010)\n\n[_The race dynamic and its\nperils_](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos860520)\n\n[_On the benefits of\ncollaboration_](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos869983)\n\n[_Working\ntogether_](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos883436)\n\n[**15\\. Crunch\ntime**](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_022.html#filepos889458)\n\n[Philosophy with a\ndeadline](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_022.html#filepos890108)\n\n[What is to be\ndone?](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_022.html#filepos894132)\n\n[_Seeking the strategic\nlight_](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_022.html#filepos897181)\n\n[_Building good\ncapacity_](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_022.html#filepos899077)\n\n[_Particular\nmeasures_](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_022.html#filepos902117)\n\n[Will the best in human nature please stand\nup](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_022.html#filepos904105)\n\n[_Notes_](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos906796)\n\n[_Bibliography_](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_025.html#filepos1157591)\n\n[_Index_](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_026.html#filepos1256923)\n\n\n## [LISTS OF FIGURES, TABLES, AND\nBOXES](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_006.html#filepos15091)\n\n### List of Figures\n\n[1\\. Long-term history of world\nGDP](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos40749).\n\n[2\\. Overall long-term impact of\nHLMI](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos103263).\n\n[3\\. Supercomputer\nperformance](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos122816).\n\n[4\\. Reconstructing 3D neuroanatomy from electron microscope\nimages](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos136368).\n\n[5\\. Whole brain emulation\nroadmap](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos144906).\n\n[6\\. Composite faces as a metaphor for spell-checked\ngenomes](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos166238).\n\n[7\\. Shape of the\ntakeoff](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_011.html#filepos240336).\n\n[8\\. A less anthropomorphic\nscale?](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_011.html#filepos266031)\n\n[9\\. One simple model of an intelligence\nexplosion](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_011.html#filepos287672).\n\n[10\\. Phases in an AI takeover\nscenario](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_013.html#filepos345222).\n\n[11\\. Schematic illustration of some possible trajectories for a hypothetical\nwise\nsingleton](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_013.html#filepos360821).\n\n[12\\. Results of anthropomorphizing alien\nmotivation](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_014.html#filepos379030).\n\n[13\\. Artificial intelligence or whole brain emulation\nfirst?](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos851585)\n\n[14\\. Risk levels in AI technology\nraces](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos865839).\n\n### List of Tables\n\n[1\\. Game-playing\nAI](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos71800)\n\n[2\\. When will human-level machine intelligence be\nattained?](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos99438)\n\n[3\\. How long from human level to\nsuperintelligence?](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos102539)\n\n[4\\. Capabilities needed for whole brain\nemulation](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos138723)\n\n[5\\. Maximum IQ gains from selecting among a set of\nembryos](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos155078)\n\n[6\\. Possible impacts from genetic selection in different\nscenarios](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos163999)\n\n[7\\. Some strategically significant technology\nraces](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_012.html#filepos299460)\n\n[8\\. Superpowers: some strategically relevant tasks and corresponding skill\nsets](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_013.html#filepos338826)\n\n[9\\. Different kinds of\ntripwires](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_016.html#filepos485322)\n\n[10\\. Control\nmethods](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_016.html#filepos506349)\n\n[11\\. Features of different system\ncastes](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_017.html#filepos548444)\n\n[12\\. Summary of value-loading\ntechniques](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos722619)\n\n[13\\. Component\nlist](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_020.html#filepos773810)\n\n### List of Boxes\n\n[1\\. An optimal Bayesian\nagent](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos65127)\n\n[2\\. The 2010 Flash\nCrash](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos89794)\n\n[3\\. What would it take to recapitulate\nevolution?](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos115254)\n\n[4\\. On the kinetics of an intelligence\nexplosion](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_011.html#filepos280862)\n\n[5\\. Technology races: some historical\nexamples](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_012.html#filepos294893)\n\n[6\\. The mail-ordered DNA\nscenario](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_013.html#filepos351346)\n\n[7\\. How big is the cosmic\nendowment?](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_013.html#filepos362023)\n\n[8\\. Anthropic\ncapture](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_016.html#filepos474232)\n\n[9\\. Strange solutions from blind\nsearch](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_017.html#filepos542326)\n\n[10\\. Formalizing value\nlearning](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos677433)\n\n[11\\. An AI that wants to be\nfriendly](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos689750)\n\n[12\\. Two recent (half-baked)\nideas](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos693493)\n\n[13\\. A risk-race to the\nbottom](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos863063)\n\n\n## [CHAPTER 1  \nPast developments and present\ncapabilities](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_006.html#filepos15218)\n\n**We begin by looking back. History, at the largest scale, seems to exhibit a\nsequence of distinct growth modes, each much more rapid than its predecessor.\nThis pattern has been taken to suggest that another (even faster) growth mode\nmight be possible. However, we do not place much weight on this\nobservation—this is not a book about “technological acceleration” or\n“exponential growth” or the miscellaneous notions sometimes gathered under the\nrubric of “the singularity.” Next, we review the history of artificial\nintelligence. We then survey the field’s current capabilities. Finally, we\nglance at some recent expert opinion surveys, and contemplate our ignorance\nabout the timeline of future advances.**\n\n### [Growth modes and big\nhistory](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_006.html#filepos15350)\n\nA mere few million years ago our ancestors were still swinging from the\nbranches in the African canopy. On a geological or even evolutionary\ntimescale, the rise of _Homo sapiens_ from our last common ancestor with the\ngreat apes happened swiftly. We developed upright posture, opposable thumbs,\nand—crucially—some relatively minor changes in brain size and neurological\norganization that led to a great leap in cognitive ability. As a consequence,\nhumans can think abstractly, communicate complex thoughts, and culturally\naccumulate information over the generations far better than any other species\non the planet.\n\nThese capabilities let humans develop increasingly efficient productive\ntechnologies, making it possible for our ancestors to migrate far away from\nthe rainforest and the savanna. Especially after the adoption of agriculture,\npopulation densities rose along with the total size of the human population.\nMore people meant more ideas; greater densities meant that ideas could spread\nmore readily and that some individuals could devote themselves to developing\nspecialized skills. These developments increased the _rate of growth_ of\neconomic productivity and technological capacity. Later developments, related\nto the Industrial Revolution, brought about a second, comparable step change\nin the rate of growth.\n\nSuch changes in the rate of growth have important consequences. A few hundred\nthousand years ago, in early human (or hominid) prehistory, growth was so slow\nthat it took on the order of one million years for human productive capacity\nto increase sufficiently to sustain an additional one million individuals\nliving at subsistence level. By 5000 BC, following the Agricultural\nRevolution, the rate of growth had increased to the point where the same\namount of growth took just two centuries. Today, following the Industrial\nRevolution, the world economy grows on average by that amount every ninety\nminutes.[1](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos907506)\n\nEven the present rate of growth will produce impressive results if maintained\nfor a moderately long time. If the world economy continues to grow at the same\npace as it has over the past fifty years, then the world will be some 4.8\ntimes richer by 2050 and about 34 times richer by 2100 than it is\ntoday.[2](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos908869)\n\nYet the prospect of continuing on a steady exponential growth path pales in\ncomparison to what would happen if the world were to experience another step\nchange in the _rate of growth_ comparable in magnitude to those associated\nwith the Agricultural Revolution and the Industrial Revolution. The economist\nRobin Hanson estimates, based on historical economic and population data, a\ncharacteristic world economy doubling time for Pleistocene hunter–gatherer\nsociety of 224,000 years; for farming society, 909 years; and for industrial\nsociety, 6.3\nyears.[3](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos909591) (In\nHanson’s model, the present epoch is a mixture of the farming and the\nindustrial growth modes—the world economy as a whole is not yet growing at the\n6.3-year doubling rate.) If another such transition to a different growth mode\nwere to occur, and it were of similar magnitude to the previous two, it would\nresult in a new growth regime in which the world economy would double in size\nabout every two weeks.\n\nSuch a growth rate seems fantastic by current lights. Observers in earlier\nepochs might have found it equally preposterous to suppose that the world\neconomy would one day be doubling several times within a single lifespan. Yet\nthat is the extraordinary condition we now take to be ordinary.\n\nThe idea of a coming technological singularity has by now been widely\npopularized, starting with Vernor Vinge’s seminal essay and continuing with\nthe writings of Ray Kurzweil and\nothers.[4](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos909698) The\nterm “singularity,” however, has been used confusedly in many disparate senses\nand has accreted an unholy (yet almost millenarian) aura of techno-utopian\nconnotations.[5](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos909821)\nSince most of these meanings and connotations are irrelevant to our argument,\nwe can gain clarity by dispensing with the “singularity” word in favor of more\nprecise terminology.\n\nThe singularity-related idea that interests us here is the possibility of an\n_intelligence explosion_ , particularly the prospect of machine\nsuperintelligence. There may be those who are persuaded by growth diagrams\nlike the ones in Figure 1 that another drastic change in growth mode is in the\ncards, comparable to the Agricultural or Industrial Revolution. These folk may\nthen reflect that it is hard to conceive of a scenario in which the world\neconomy’s doubling time shortens to mere weeks that does not involve the\ncreation of minds that are much faster and more efficient than the familiar\nbiological kind. However, the case for taking seriously the prospect of a\nmachine intelligence revolution need not rely on curve-fitting exercises or\nextrapolations from past economic growth. As we shall see, there are stronger\nreasons for taking heed.\n\n![Image](images/00003.jpg)\n\n**Figure 1** Long-term history of world GDP. Plotted on a linear scale, the\nhistory of the world economy looks like a flat line hugging the _x_ -axis,\nuntil it suddenly spikes vertically upward. (a) Even when we zoom in on the\nmost recent 10,000 years, the pattern remains essentially one of a single 90°\nangle. (b) Only within the past 100 years or so does the curve lift\nperceptibly above the zero-level. (The different lines in the plot correspond\nto different data sets, which yield slightly different\nestimates.[6](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos909930))\n\n### [Great\nexpectations](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_006.html#filepos15453)\n\nMachines matching humans in general intelligence—that is, possessing common\nsense and an effective ability to learn, reason, and plan to meet complex\ninformation-processing challenges across a wide range of natural and abstract\ndomains—have been expected since the invention of computers in the 1940s. At\nthat time, the advent of such machines was often placed some twenty years into\nthe future.[7](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos910080)\nSince then, the expected arrival date has been receding at a rate of one year\nper year; so that today, futurists who concern themselves with the possibility\nof artificial general intelligence still often believe that intelligent\nmachines are a couple of decades\naway.[8](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos910550)\n\nTwo decades is a sweet spot for prognosticators of radical change: near enough\nto be attention-grabbing and relevant, yet far enough to make it possible to\nsuppose that a string of breakthroughs, currently only vaguely imaginable,\nmight by then have occurred. Contrast this with shorter timescales: most\ntechnologies that will have a big impact on the world in five or ten years\nfrom now are already in limited use, while technologies that will reshape the\nworld in less than fifteen years probably exist as laboratory prototypes.\nTwenty years may also be close to the typical duration remaining of a\nforecaster’s career, bounding the reputational risk of a bold prediction.\n\nFrom the fact that some individuals have overpredicted artificial intelligence\nin the past, however, it does not follow that AI is impossible or will never\nbe\ndeveloped.[9](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos910712)\nThe main reason why progress has been slower than expected is that the\ntechnical difficulties of constructing intelligent machines have proved\ngreater than the pioneers foresaw. But this leaves open just how great those\ndifficulties are and how far we now are from overcoming them. Sometimes a\nproblem that initially looks hopelessly complicated turns out to have a\nsurprisingly simple solution (though the reverse is probably more common).\n\nIn the next chapter, we will look at different paths that may lead to human-\nlevel machine intelligence. But let us note at the outset that however many\nstops there are between here and human-level machine intelligence, the latter\nis not the final destination. The next stop, just a short distance farther\nalong the tracks, is superhuman-level machine intelligence. The train might\nnot pause or even decelerate at Humanville Station. It is likely to swoosh\nright by.\n\nThe mathematician I. J. Good, who had served as chief statistician in Alan\nTuring’s code-breaking team in World War II, might have been the first to\nenunciate the essential aspects of this scenario. In an oft-quoted passage\nfrom 1965, he wrote:\n\n> Let an ultraintelligent machine be defined as a machine that can far surpass\n> all the intellectual activities of any man however clever. Since the design\n> of machines is one of these intellectual activities, an ultraintelligent\n> machine could design even better machines; there would then unquestionably\n> be an “intelligence explosion,” and the intelligence of man would be left\n> far behind. Thus the first ultraintelligent machine is the last invention\n> that man need ever make, provided that the machine is docile enough to tell\n> us how to keep it under\n> control.[10](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos911014)\n\nIt may seem obvious now that major existential risks would be associated with\nsuch an intelligence explosion, and that the prospect should therefore be\nexamined with the utmost seriousness even if it were known (which it is not)\nto have but a moderately small probability of coming to pass. The pioneers of\nartificial intelligence, however, notwithstanding their belief in the\nimminence of human-level AI, mostly did not contemplate the possibility of\ngreater-than-human AI. It is as though their speculation muscle had so\nexhausted itself in conceiving the radical possibility of machines reaching\nhuman intelligence that it could not grasp the corollary—that machines would\nsubsequently become superintelligent.\n\nThe AI pioneers for the most part did not countenance the possibility that\ntheir enterprise might involve\nrisk.[11](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos911125) They\ngave no lip service—let alone serious thought—to any safety concern or ethical\nqualm related to the creation of artificial minds and potential computer\noverlords: a lacuna that astonishes even against the background of the era’s\nnot-so-impressive standards of critical technology\nassessment.[12](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos912294)\nWe must hope that by the time the enterprise eventually does become feasible,\nwe will have gained not only the technological proficiency to set off an\nintelligence explosion but also the higher level of mastery that may be\nnecessary to make the detonation survivable.\n\nBut before we turn to what lies ahead, it will be useful to take a quick\nglance at the history of machine intelligence to date.\n\n### [Seasons of hope and\ndespair](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_006.html#filepos15546)\n\nIn the summer of 1956 at Dartmouth College, ten scientists sharing an interest\nin neural nets, automata theory, and the study of intelligence convened for a\nsix-week workshop. This Dartmouth Summer Project is often regarded as the\ncockcrow of artificial intelligence as a field of research. Many of the\nparticipants would later be recognized as founding figures. The optimistic\noutlook among the delegates is reflected in the proposal submitted to the\nRockefeller Foundation, which provided funding for the event:\n\n> We propose that a 2 month, 10 man study of artificial intelligence be\n> carried out…. The study is to proceed on the basis of the conjecture that\n> every aspect of learning or any other feature of intelligence can in\n> principle be so precisely described that a machine can be made to simulate\n> it. An attempt will be made to find how to make machines that use language,\n> form abstractions and concepts, solve kinds of problems now reserved for\n> humans, and improve themselves. We think that a significant advance can be\n> made in one or more of these problems if a carefully selected group of\n> scientists work on it together for a summer.\n\nIn the six decades since this brash beginning, the field of artificial\nintelligence has been through periods of hype and high expectations\nalternating with periods of setback and disappointment.\n\nThe first period of excitement, which began with the Dartmouth meeting, was\nlater described by John McCarthy (the event’s main organizer) as the “Look,\nMa, no hands!” era. During these early days, researchers built systems\ndesigned to refute claims of the form “No machine could ever do _X_!” Such\nskeptical claims were common at the time. To counter them, the AI researchers\ncreated small systems that achieved _X_ in a “microworld” (a well-defined,\nlimited domain that enabled a pared-down version of the performance to be\ndemonstrated), thus providing a proof of concept and showing that _X_ could,\nin principle, be done by machine. One such early system, the Logic Theorist,\nwas able to prove most of the theorems in the second chapter of Whitehead and\nRussell’s _Principia Mathematica_ , and even came up with one proof that was\nmuch more elegant than the original, thereby debunking the notion that\nmachines could “only think numerically” and showing that machines were also\nable to do deduction and to invent logical\nproofs.[13](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos912569) A\nfollow-up program, the General Problem Solver, could in principle solve a wide\nrange of formally specified\nproblems.[14](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos912681)\nPrograms that could solve calculus problems typical of first-year college\ncourses, visual analogy problems of the type that appear in some IQ tests, and\nsimple verbal algebra problems were also\nwritten.[15](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos912797)\nThe Shakey robot (so named because of its tendency to tremble during\noperation) demonstrated how logical reasoning could be integrated with\nperception and used to plan and control physical\nactivity.[16](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos913029)\nThe ELIZA program showed how a computer could impersonate a Rogerian\npsychotherapist.[17](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos913139)\nIn the mid-seventies, the program SHRDLU showed how a simulated robotic arm in\na simulated world of geometric blocks could follow instructions and answer\nquestions in English that were typed in by a\nuser.[18](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos913252) In\nlater decades, systems would be created that demonstrated that machines could\ncompose music in the style of various classical composers, outperform junior\ndoctors in certain clinical diagnostic tasks, drive cars autonomously, and\nmake patentable\ninventions.[19](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos913363)\nThere has even been an AI that cracked original\njokes.[20](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos913668)\n(Not that its level of humor was high—“What do you get when you cross an\n_optic_ with a _mental object_? An _eye_ -dea”—but children reportedly found\nits puns consistently entertaining.)\n\nThe methods that produced successes in the early demonstration systems often\nproved difficult to extend to a wider variety of problems or to harder problem\ninstances. One reason for this is the “combinatorial explosion” of\npossibilities that must be explored by methods that rely on something like\nexhaustive search. Such methods work well for simple instances of a problem,\nbut fail when things get a bit more complicated. For instance, to prove a\ntheorem that has a 5-line long proof in a deduction system with one inference\nrule and 5 axioms, one could simply enumerate the 3,125 possible combinations\nand check each one to see if it delivers the intended conclusion. Exhaustive\nsearch would also work for 6- and 7-line proofs. But as the task becomes more\ndifficult, the method of exhaustive search soon runs into trouble. Proving a\ntheorem with a 50-line proof does not take ten times longer than proving a\ntheorem that has a 5-line proof: rather, if one uses exhaustive search, it\nrequires combing through 550 ≈ 8.9 × 1034 possible sequences—which is\ncomputationally infeasible even with the fastest supercomputers.\n\nTo overcome the combinatorial explosion, one needs algorithms that exploit\nstructure in the target domain and take advantage of prior knowledge by using\nheuristic search, planning, and flexible abstract representations—capabilities\nthat were poorly developed in the early AI systems. The performance of these\nearly systems also suffered because of poor methods for handling uncertainty,\nreliance on brittle and ungrounded symbolic representations, data scarcity,\nand severe hardware limitations on memory capacity and processor speed. By the\nmid-1970s, there was a growing awareness of these problems. The realization\nthat many AI projects could never make good on their initial promises led to\nthe onset of the first “AI winter”: a period of retrenchment, during which\nfunding decreased and skepticism increased, and AI fell out of fashion.\n\nA new springtime arrived in the early 1980s, when Japan launched its Fifth-\nGeneration Computer Systems Project, a well-funded public–private partnership\nthat aimed to leapfrog the state of the art by developing a massively parallel\ncomputing architecture that would serve as a platform for artificial\nintelligence. This occurred at peak fascination with the Japanese “post-war\neconomic miracle,” a period when Western government and business leaders\nanxiously sought to divine the formula behind Japan’s economic success in hope\nof replicating the magic at home. When Japan decided to invest big in AI,\nseveral other countries followed suit.\n\nThe ensuing years saw a great proliferation of _expert systems_. Designed as\nsupport tools for decision makers, expert systems were rule-based programs\nthat made simple inferences from a knowledge base of facts, which had been\nelicited from human domain experts and painstakingly hand-coded in a formal\nlanguage. Hundreds of these expert systems were built. However, the smaller\nsystems provided little benefit, and the larger ones proved expensive to\ndevelop, validate, and keep updated, and were generally cumbersome to use. It\nwas impractical to acquire a standalone computer just for the sake of running\none program. By the late 1980s, this growth season, too, had run its course.\n\nThe Fifth-Generation Project failed to meet its objectives, as did its\ncounterparts in the United States and Europe. A second AI winter descended. At\nthis point, a critic could justifiably bemoan “the history of artificial\nintelligence research to date, consisting always of very limited success in\nparticular areas, followed immediately by failure to reach the broader goals\nat which these initial successes seem at first to\nhint.”[21](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos913804)\nPrivate investors began to shun any venture carrying the brand of “artificial\nintelligence.” Even among academics and their funders, “AI” became an unwanted\nepithet.[22](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos914031)\n\nTechnical work continued apace, however, and by the 1990s, the second AI\nwinter gradually thawed. Optimism was rekindled by the introduction of new\ntechniques, which seemed to offer alternatives to the traditional logicist\nparadigm (often referred to as “Good Old-Fashioned Artificial Intelligence,”\nor “GOFAI” for short), which had focused on high-level symbol manipulation and\nwhich had reached its apogee in the expert systems of the 1980s. The newly\npopular techniques, which included neural networks and genetic algorithms,\npromised to overcome some of the shortcomings of the GOFAI approach, in\nparticular the “brittleness” that characterized classical AI programs (which\ntypically produced complete nonsense if the programmers made even a single\nslightly erroneous assumption). The new techniques boasted a more organic\nperformance. For example, neural networks exhibited the property of “graceful\ndegradation”: a small amount of damage to a neural network typically resulted\nin a small degradation of its performance, rather than a total crash. Even\nmore importantly, neural networks could learn from experience, finding natural\nways of generalizing from examples and finding hidden statistical patterns in\ntheir\ninput.[23](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos915200)\nThis made the nets good at pattern recognition and classification problems.\nFor example, by training a neural network on a data set of sonar signals, it\ncould be taught to distinguish the acoustic profiles of submarines, mines, and\nsea life with better accuracy than human experts—and this could be done\nwithout anybody first having to figure out in advance exactly how the\ncategories were to be defined or how different features were to be weighted.\n\nWhile simple neural network models had been known since the late 1950s, the\nfield enjoyed a renaissance after the introduction of the backpropagation\nalgorithm, which made it possible to train multi-layered neural\nnetworks.[24](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos915557)\nSuch multilayered networks, which have one or more intermediary (“hidden”)\nlayers of neurons between the input and output layers, can learn a much wider\nrange of functions than their simpler\npredecessors.[25](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos916079)\nCombined with the increasingly powerful computers that were becoming\navailable, these algorithmic improvements enabled engineers to build neural\nnetworks that were good enough to be practically useful in many applications.\n\nThe brain-like qualities of neural networks contrasted favorably with the\nrigidly logic-chopping but brittle performance of traditional rule-based GOFAI\nsystems—enough so to inspire a new “-ism,” _connectionism_ , which emphasized\nthe importance of massively parallel sub-symbolic processing. More than\n150,000 academic papers have since been published on artificial neural\nnetworks, and they continue to be an important approach in machine learning.\n\nEvolution-based methods, such as genetic algorithms and genetic programming,\nconstitute another approach whose emergence helped end the second AI winter.\nIt made perhaps a smaller academic impact than neural nets but was widely\npopularized. In evolutionary models, a population of candidate solutions\n(which can be data structures or programs) is maintained, and new candidate\nsolutions are generated randomly by mutating or recombining variants in the\nexisting population. Periodically, the population is pruned by applying a\nselection criterion (a fitness function) that allows only the better\ncandidates to survive into the next generation. Iterated over thousands of\ngenerations, the average quality of the solutions in the candidate pool\ngradually increases. When it works, this kind of algorithm can produce\nefficient solutions to a very wide range of problems—solutions that may be\nstrikingly novel and unintuitive, often looking more like natural structures\nthan anything that a human engineer would design. And in principle, this can\nhappen without much need for human input beyond the initial specification of\nthe fitness function, which is often very simple. In practice, however,\ngetting evolutionary methods to work well requires skill and ingenuity,\nparticularly in devising a good representational format. Without an efficient\nway to encode candidate solutions (a genetic language that matches latent\nstructure in the target domain), evolutionary search tends to meander\nendlessly in a vast search space or get stuck at a local optimum. Even if a\ngood representational format is found, evolution is computationally demanding\nand is often defeated by the combinatorial explosion.\n\nNeural networks and genetic algorithms are examples of methods that stimulated\nexcitement in the 1990s by appearing to offer alternatives to the stagnating\nGOFAI paradigm. But the intention here is not to sing the praises of these two\nmethods or to elevate them above the many other techniques in machine\nlearning. In fact, one of the major theoretical developments of the past\ntwenty years has been a clearer realization of how superficially disparate\ntechniques can be understood as special cases within a common mathematical\nframework. For example, many types of artificial neural network can be viewed\nas classifiers that perform a particular kind of statistical calculation\n(maximum likelihood\nestimation).[26](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos916291)\nThis perspective allows neural nets to be compared with a larger class of\nalgorithms for learning classifiers from examples—“decision trees,” “logistic\nregression models,” “support vector machines,” “naive Bayes,” “ _k_ -nearest-\nneighbors regression,” among\nothers.[27](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos916406) In\na similar manner, genetic algorithms can be viewed as performing stochastic\nhill-climbing, which is again a subset of a wider class of algorithms for\noptimization. Each of these algorithms for building classifiers or for\nsearching a solution space has its own profile of strengths and weaknesses\nwhich can be studied mathematically. Algorithms differ in their processor time\nand memory space requirements, which inductive biases they presuppose, the\nease with which externally produced content can be incorporated, and how\ntransparent their inner workings are to a human analyst.\n\nBehind the razzle-dazzle of machine learning and creative problem-solving thus\nlies a set of mathematically well-specified tradeoffs. The ideal is that of\nthe perfect Bayesian agent, one that makes probabilistically optimal use of\navailable information. This ideal is unattainable because it is too\ncomputationally demanding to be implemented in any physical computer (see Box\n1). Accordingly, one can view artificial intelligence as a quest to find\nshortcuts: ways of tractably approximating the Bayesian ideal by sacrificing\nsome optimality or generality while preserving enough to get high performance\nin the actual domains of interest.\n\nA reflection of this picture can be seen in the work done over the past couple\nof decades on probabilistic graphical models, such as Bayesian networks.\nBayesian networks provide a concise way of representing probabilistic and\nconditional independence relations that hold in some particular domain.\n(Exploiting such independence relations is essential for overcoming the\ncombinatorial explosion, which is as much of a problem for probabilistic\ninference as it is for logical deduction.) They also provide important insight\ninto the concept of\ncausality.[28](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos916515)\n\nOne advantage of relating learning problems from specific domains to the\ngeneral problem of Bayesian inference is that new algorithms that make\nBayesian inference more efficient will then yield immediate improvements\nacross many different areas. Advances in Monte Carlo approximation techniques,\nfor example, are directly applied in computer vision, robotics, and\ncomputational genetics. Another advantage is that it lets researchers from\ndifferent disciplines more easily pool their findings. Graphical models and\nBayesian statistics have become a shared focus of research in many fields,\nincluding machine learning, statistical physics, bioinformatics, combinatorial\noptimization, and communication\ntheory.[35](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos918867) A\nfair amount of the recent progress in machine learning has resulted from\nincorporating formal results originally derived in other academic fields.\n(Machine learning applications have also benefitted enormously from faster\ncomputers and greater availability of large data sets.)\n\n* * *\n\n### **Box 1 An optimal Bayesian agent**\n\nAn ideal Bayesian agent starts out with a “prior probability distribution,” a\nfunction that assigns probabilities to each “possible world” (i.e. to each\nmaximally specific way the world could turn out to\nbe).[29](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos916623) This\nprior incorporates an inductive bias such that simpler possible worlds are\nassigned higher probabilities. (One way to formally define the simplicity of a\npossible world is in terms of its “Kolmogorov complexity,” a measure based on\nthe length of the shortest computer program that generates a complete\ndescription of the\nworld.[30](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos916911))\nThe prior also incorporates any background knowledge that the programmers wish\nto give to the agent.\n\nAs the agent receives new information from its sensors, it updates its\nprobability distribution by conditionalizing the distribution on the new\ninformation according to Bayes’\ntheorem.[31](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos917807)\nConditionalization is the mathematical operation that sets the new probability\nof those worlds that are inconsistent with the information received to zero\nand renormalizes the probability distribution over the remaining possible\nworlds. The result is a “posterior probability distribution” (which the agent\nmay use as its new prior in the next time step). As the agent makes\nobservations, its probability mass thus gets concentrated on the shrinking set\nof possible worlds that remain consistent with the evidence; and among these\npossible worlds, simpler ones always have more probability.\n\nMetaphorically, we can think of a probability as sand on a large sheet of\npaper. The paper is partitioned into areas of various sizes, each area\ncorresponding to one possible world, with larger areas corresponding to\nsimpler possible worlds. Imagine also a layer of sand of even thickness spread\nacross the entire sheet: this is our prior probability distribution. Whenever\nan observation is made that rules out some possible worlds, we remove the sand\nfrom the corresponding areas of the paper and redistribute it evenly over the\nareas that remain in play. Thus, the total amount of sand on the sheet never\nchanges, it just gets concentrated into fewer areas as observational evidence\naccumulates. This is a picture of learning in its purest form. (To calculate\nthe probability of a _hypothesis_ , we simply measure the amount of sand in\nall the areas that correspond to the possible worlds in which the hypothesis\nis true.)\n\nSo far, we have defined a learning rule. To get an agent, we also need a\ndecision rule. To this end, we endow the agent with a “utility function” which\nassigns a number to each possible world. The number represents the\ndesirability of that world according to the agent’s basic preferences. Now, at\neach time step, the agent selects the action with the highest expected\nutility.[32](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos918242)\n(To find the action with the highest expected utility, the agent could list\nall possible actions. It could then compute the conditional probability\ndistribution given the action—the probability distribution that would result\nfrom conditionalizing its current probability distribution on the observation\nthat the action had just been taken. Finally, it could calculate the expected\nvalue of the action as the sum of the value of each possible world multiplied\nby the conditional probability of that world given the\naction.[33](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos918441))\n\nThe learning rule and the decision rule together define an “optimality notion”\nfor an agent. (Essentially the same optimality notion has been broadly used in\nartificial intelligence, epistemology, philosophy of science, economics, and\nstatistics.[34](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos918682))\nIn reality, it is impossible to build such an agent because it is\ncomputationally intractable to perform the requisite calculations. Any attempt\nto do so succumbs to a combinatorial explosion just like the one described in\nour discussion of GOFAI. To see why this is so, consider one tiny subset of\nall possible worlds: those that consist of a single computer monitor floating\nin an endless vacuum. The monitor has 1, 000 × 1, 000 pixels, each of which is\nperpetually either on or off. Even this subset of possible worlds is\nenormously large: the 2(1,000 × 1,000) possible monitor states outnumber all\nthe computations expected ever to take place in the observable universe. Thus,\nwe could not even enumerate all the possible worlds in this tiny subset of all\npossible worlds, let alone perform more elaborate computations on each of them\nindividually.\n\nOptimality notions can be of theoretical interest even if they are physically\nunrealizable. They give us a standard by which to judge heuristic\napproximations, and sometimes we can reason about what an optimal agent would\ndo in some special case. We will encounter some alternative optimality notions\nfor artificial agents in [Chapter\n12](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos645293).\n\n* * *\n\n### [State of the\nart](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_006.html#filepos15648)\n\nArtificial intelligence already outperforms human intelligence in many\ndomains. Table 1 surveys the state of game-playing computers, showing that AIs\nnow beat human champions in a wide range of\ngames.[36](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos919073)\n\nThese achievements might not seem impressive today. But this is because our\nstandards for what is impressive keep adapting to the advances being made.\nExpert chess playing, for example, was once thought to epitomize human\nintellection. In the view of several experts in the late fifties: “If one\ncould devise a successful chess machine, one would seem to have penetrated to\nthe core of human intellectual\nendeavor.”[55](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos921766)\nThis no longer seems so. One sympathizes with John McCarthy, who lamented: “As\nsoon as it works, no one calls it AI\nanymore.”[56](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos921887)\n\n**Table 1 _Game-playing AI_**\n\n**Checkers** | Superhuman | Arthur Samuel’s checkers program, originally written in 1952 and later improved (the 1955 version incorporating machine learning), becomes the first program to learn to play a game better than its creator.[37](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos919390) In 1994, the program CHINOOK beats the reigning human champion, marking the first time a program wins an official world championship in a game of skill. In 2002, Jonathan Schaeffer and his team “solve” checkers, i.e. produce a program that always makes the best possible move (combining alpha-beta search with a database of 39 trillion endgame positions). Perfect play by both sides leads to a draw.[38](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos919524)  \n---|---|---  \n**Backgammon** | Superhuman | 1979: The backgammon program BKG by Hans Berliner defeats the world champion—the first computer program to defeat (in an exhibition match) a world champion in any game—though Berliner later attributes the win to luck with the dice rolls.[39](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos919643)  \n|  | 1992: The backgammon program TD-Gammon by Gerry Tesauro reaches championship-level ability, using temporal difference learning (a form of reinforcement learning) and repeated plays against itself to improve.[40](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos919758)  \n|  | In the years since, backgammon programs have far surpassed the best human players.[41](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos919868)  \n**Traveller TCS** | Superhuman in collaboration with human[42](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos920046) | In both 1981 and 1982, Douglas Lenat’s program Eurisko wins the US championship in Traveller TCS (a futuristic naval war game), prompting rule changes to block its unorthodox strategies.[43](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos920399) Eurisko had heuristics for designing its fleet, and it also had heuristics for modifying its heuristics.  \n**Othello** | Superhuman | 1997: The program Logistello wins every game in a six-game match against world champion Takeshi Murakami.[44](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos920513)  \n**Chess** | Superhuman | 1997: Deep Blue beats the world chess champion, Garry Kasparov. Kasparov claims to have seen glimpses of true intelligence and creativity in some of the computer’s moves.[45](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos920635) Since then, chess engines have continued to improve.[46](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos920750)  \n**Crosswords** | Expert level | 1999: The crossword-solving program Proverb outperforms the average crossword-solver.[47](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos920860)  \n|  | 2012: The program Dr. Fill, created by Matt Ginsberg, scores in the top quartile among the otherwise human contestants in the American Crossword Puzzle Tournament. (Dr. Fill’s performance is uneven. It completes perfectly the puzzle rated most difficult by humans, yet is stumped by a couple of nonstandard puzzles that involved spelling backwards or writing answers diagonally.)[48](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos920974)  \n**Scrabble** | Superhuman | As of 2002, Scrabble-playing software surpasses the best human players.[49](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos921090)  \n**Bridge** | Equal to the best | By 2005, contract bridge playing software reaches parity with the best human bridge players.[50](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos921201)  \n**Jeopardy!** | Superhuman | 2010: IBM’s _Watson_ defeats the two all-time-greatest human _Jeopardy!_ champions, Ken Jennings and Brad Rutter.[51](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos921314) _Jeopardy!_ is a televised game show with trivia questions about history, literature, sports, geography, pop culture, science, and other topics. Questions are presented in the form of clues, and often involve wordplay.  \n**Poker** | Varied | Computer poker players remain slightly below the best humans for full-ring Texas hold ‘em but perform at a superhuman level in some poker variants.[52](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos921424)  \n**FreeCell** | Superhuman | Heuristics evolved using genetic algorithms produce a solver for the solitaire game FreeCell (which in its generalized form is NP-complete) that is able to beat high-ranking human players.[53](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos921543)  \n**Go** | Very strong amateur level | As of 2012, the Zen series of go-playing programs has reached rank 6 dan in fast games (the level of a very strong amateur player), using Monte Carlo tree search and machine learning techniques.[54](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos921660) Go-playing programs have been improving at a rate of about 1 dan/year in recent years. If this rate of improvement continues, they might beat the human world champion in about a decade.  \n  \nThere is an important sense, however, in which chess-playing AI turned out to\nbe a lesser triumph than many imagined it would be. It was once supposed,\nperhaps not unreasonably, that in order for a computer to play chess at\ngrandmaster level, it would have to be endowed with a high degree of _general_\nintelligence.[57](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos922009)\nOne might have thought, for example, that great chess playing requires being\nable to learn abstract concepts, think cleverly about strategy, compose\nflexible plans, make a wide range of ingenious logical deductions, and maybe\neven model one’s opponent’s thinking. Not so. It turned out to be possible to\nbuild a perfectly fine chess engine around a special-purpose\nalgorithm.[58](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos922794)\nWhen implemented on the fast processors that became available towards the end\nof the twentieth century, it produces very strong play. But an AI built like\nthat is narrow. It plays chess; it can do no\nother.[59](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos923142)\n\nIn other domains, solutions have turned out to be _more_ complicated than\ninitially expected, and progress slower. The computer scientist Donald Knuth\nwas struck that “AI has by now succeeded in doing essentially everything that\nrequires ‘thinking’ but has failed to do most of what people and animals do\n‘without thinking’—that, somehow, is much\nharder!”[60](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos923420)\nAnalyzing visual scenes, recognizing objects, or controlling a robot’s\nbehavior as it interacts with a natural environment has proved challenging.\nNevertheless, a fair amount of progress has been made and continues to be\nmade, aided by steady improvements in hardware.\n\nCommon sense and natural language understanding have also turned out to be\ndifficult. It is now often thought that achieving a fully human-level\nperformance on these tasks is an “AI-complete” problem, meaning that the\ndifficulty of solving these problems is essentially equivalent to the\ndifficulty of building generally human-level intelligent\nmachines.[61](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos923883)\nIn other words, if somebody _were_ to succeed in creating an AI that could\nunderstand natural language as well as a human adult, they would in all\nlikelihood also either already have succeeded in creating an AI that could do\neverything else that human intelligence can do, or they would be but a very\nshort step from such a general\ncapability.[62](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos923993)\n\nChess-playing expertise turned out to be achievable by means of a surprisingly\nsimple algorithm. It is tempting to speculate that other capabilities—such as\ngeneral reasoning ability, or some key ability involved in programming—might\nlikewise be achievable through some surprisingly simple algorithm. The fact\nthat the best performance at one time is attained through a complicated\nmechanism does not mean that no simple mechanism could do the job as well or\nbetter. It might simply be that nobody has yet found the simpler alternative.\nThe Ptolemaic system (with the Earth in the center, orbited by the Sun, the\nMoon, planets, and stars) represented the state of the art in astronomy for\nover a thousand years, and its predictive accuracy was improved over the\ncenturies by progressively complicating the model: adding epicycles upon\nepicycles to the postulated celestial motions. Then the entire system was\noverthrown by the heliocentric theory of Copernicus, which was simpler\nand—though only after further elaboration by Kepler—more predictively\naccurate.[63](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos925001)\n\nArtificial intelligence methods are now used in more areas than it would make\nsense to review here, but mentioning a sampling of them will give an idea of\nthe breadth of applications. Aside from the game AIs listed in Table 1, there\nare hearing aids with algorithms that filter out ambient noise; route-finders\nthat display maps and offer navigation advice to drivers; recommender systems\nthat suggest books and music albums based on a user’s previous purchases and\nratings; and medical decision support systems that help doctors diagnose\nbreast cancer, recommend treatment plans, and aid in the interpretation of\nelectrocardiograms. There are robotic pets and cleaning robots, lawn-mowing\nrobots, rescue robots, surgical robots, and over a million industrial\nrobots.[64](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos925323)\nThe world population of robots exceeds 10\nmillion.[65](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos925440)\n\nModern speech recognition, based on statistical techniques such as hidden\nMarkov models, has become sufficiently accurate for practical use (some\nfragments of this book were drafted with the help of a speech recognition\nprogram). Personal digital assistants, such as Apple’s Siri, respond to spoken\ncommands and can answer simple questions and execute commands. Optical\ncharacter recognition of handwritten and typewritten text is routinely used in\napplications such as mail sorting and digitization of old\ndocuments.[66](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos925572)\n\nMachine translation remains imperfect but is good enough for many\napplications. Early systems used the GOFAI approach of hand-coded grammars\nthat had to be developed by skilled linguists from the ground up for each\nlanguage. Newer systems use statistical machine learning techniques that\nautomatically build statistical models from observed usage patterns. The\nmachine infers the parameters for these models by analyzing bilingual corpora.\nThis approach dispenses with linguists: the programmers building these systems\nneed not even speak the languages they are working\nwith.[67](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos925681)\n\nFace recognition has improved sufficiently in recent years that it is now used\nat automated border crossings in Europe and Australia. The US Department of\nState operates a face recognition system with over 75 million photographs for\nvisa processing. Surveillance systems employ increasingly sophisticated AI and\ndata-mining technologies to analyze voice, video, or text, large quantities of\nwhich are trawled from the world’s electronic communications media and stored\nin giant data centers.\n\nTheorem-proving and equation-solving are by now so well established that they\nare hardly regarded as AI anymore. Equation solvers are included in scientific\ncomputing programs such as Mathematica. Formal verification methods, including\nautomated theorem provers, are routinely used by chip manufacturers to verify\nthe behavior of circuit designs prior to production.\n\nThe US military and intelligence establishments have been leading the way to\nthe large-scale deployment of bomb-disposing robots, surveillance and attack\ndrones, and other unmanned vehicles. These still depend mainly on remote\ncontrol by human operators, but work is underway to extend their autonomous\ncapabilities.\n\nIntelligent scheduling is a major area of success. The DART tool for automated\nlogistics planning and scheduling was used in Operation Desert Storm in 1991\nto such effect that DARPA (the Defense Advanced Research Projects Agency in\nthe United States) claims that this single application more than paid back\ntheir thirty-year investment in\nAI.[68](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos925883)\nAirline reservation systems use sophisticated scheduling and pricing systems.\nBusinesses make wide use of AI techniques in inventory control systems. They\nalso use automatic telephone reservation systems and helplines connected to\nspeech recognition software to usher their hapless customers through\nlabyrinths of interlocking menu options.\n\nAI technologies underlie many Internet services. Software polices the world’s\nemail traffic, and despite continual adaptation by spammers to circumvent the\ncountermeasures being brought against them, Bayesian spam filters have largely\nmanaged to hold the spam tide at bay. Software using AI components is\nresponsible for automatically approving or declining credit card transactions,\nand continuously monitors account activity for signs of fraudulent use.\nInformation retrieval systems also make extensive use of machine learning. The\nGoogle search engine is, arguably, the greatest AI system that has yet been\nbuilt.\n\nNow, it must be stressed that the demarcation between artificial intelligence\nand software in general is not sharp. Some of the applications listed above\nmight be viewed more as generic software applications rather than AI in\nparticular—though this brings us back to McCarthy’s dictum that when something\nworks it is no longer called AI. A more relevant distinction for our purposes\nis that between systems that have a narrow range of cognitive capability\n(whether they be called “AI” or not) and systems that have more generally\napplicable problem-solving capacities. Essentially all the systems currently\nin use are of the former type: narrow. However, many of them contain\ncomponents that might also play a role in future artificial general\nintelligence or be of service in its development—components such as\nclassifiers, search algorithms, planners, solvers, and representational\nframeworks.\n\nOne high-stakes and extremely competitive environment in which AI systems\noperate today is the global financial market. Automated stock-trading systems\nare widely used by major investing houses. While some of these are simply ways\nof automating the execution of particular buy or sell orders issued by a human\nfund manager, others pursue complicated trading strategies that adapt to\nchanging market conditions. Analytic systems use an assortment of data-mining\ntechniques and time series analysis to scan for patterns and trends in\nsecurities markets or to correlate historical price movements with external\nvariables such as keywords in news tickers. Financial news providers sell\nnewsfeeds that are specially formatted for use by such AI programs. Other\nsystems specialize in finding arbitrage opportunities within or between\nmarkets, or in high-frequency trading that seeks to profit from minute price\nmovements that occur over the course of milliseconds (a timescale at which\ncommunication latencies even for speed-of-light signals in optical fiber cable\nbecome significant, making it advantageous to locate computers near the\nexchange). Algorithmic high-frequency traders account for more than half of\nequity shares traded on US\nmarkets.[69](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos926018)\nAlgorithmic trading has been implicated in the 2010 Flash Crash (see Box 2).\n\n* * *\n\n### **Box 2 The 2010 Flash Crash**\n\nBy the afternoon of May, 6, 2010, US equity markets were already down 4% on\nworries about the European debt crisis. At 2:32 p.m., a large seller (a mutual\nfund complex) initiated a sell algorithm to dispose of a large number of the\nE-Mini S&P 500 futures contracts to be sold off at a sell rate linked to a\nmeasure of minute-to-minute liquidity on the exchange. These contracts were\nbought by algorithmic high-frequency traders, which were programmed to quickly\neliminate their temporary long positions by selling the contracts on to other\ntraders. With demand from fundamental buyers slacking, the algorithmic traders\nstarted to sell the E-Minis primarily to other algorithmic traders, which in\nturn passed them on to other algorithmic traders, creating a “hot potato”\neffect driving up trading volume—this being interpreted by the sell algorithm\nas an indicator of high liquidity, prompting it to increase the rate at which\nit was putting E-Mini contracts on the market, pushing the downward spiral. At\nsome point, the high-frequency traders started withdrawing from the market,\ndrying up liquidity while prices continued to fall. At 2:45 p.m., trading on\nthe E-Mini was halted by an automatic circuit breaker, the exchange’s stop\nlogic functionality. When trading was restarted, a mere five seconds later,\nprices stabilized and soon began to recover most of the losses. But for a\nwhile, at the trough of the crisis, a trillion dollars had been wiped off the\nmarket, and spillover effects had led to a substantial number of trades in\nindividual securities being executed at “absurd” prices, such as one cent or\n100,000 dollars. After the market closed for the day, representatives of the\nexchanges met with regulators and decided to break all trades that had been\nexecuted at prices 60% or more away from their pre-crisis levels (deeming such\ntransactions “clearly erroneous” and thus subject to _post facto_ cancellation\nunder existing trade\nrules).[70](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos926238)\n\nThe retelling here of this episode is a digression because the computer\nprograms involved in the Flash Crash were not particularly intelligent or\nsophisticated, and the kind of threat they created is fundamentally different\nfrom the concerns we shall raise later in this book in relation to the\nprospect of machine superintelligence. Nevertheless, these events illustrate\nseveral useful lessons. One is the reminder that interactions between\nindividually simple components (such as the sell algorithm and the high-\nfrequency algorithmic trading programs) can produce complicated and unexpected\neffects. Systemic risk can build up in a system as new elements are\nintroduced, risks that are not obvious until after something goes wrong (and\nsometimes not even\nthen).[71](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos926432)\n\nAnother lesson is that smart professionals might give an instruction to a\nprogram based on a sensible-seeming and normally sound assumption (e.g. that\ntrading volume is a good measure of market liquidity), and that this can\nproduce catastrophic results when the program continues to act on the\ninstruction with iron-clad logical consistency even in the unanticipated\nsituation where the assumption turns out to be invalid. The algorithm just\ndoes what it does; and unless it is a very special kind of algorithm, it does\nnot care that we clasp our heads and gasp in dumbstruck horror at the absurd\ninappropriateness of its actions. This is a theme that we will encounter\nagain.\n\nA third observation in relation to the Flash Crash is that while automation\ncontributed to the incident, it also contributed to its resolution. The pre-\npreprogrammed stop order logic, which suspended trading when prices moved too\nfar out of whack, was set to execute automatically because it had been\ncorrectly anticipated that the triggering events could happen on a timescale\ntoo swift for humans to respond. The need for pre-installed and automatically\nexecuting safety functionality—as opposed to reliance on runtime human\nsupervision—again foreshadows a theme that will be important in our discussion\nof machine\nsuperintelligence.[72](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos926723)\n\n* * *\n\n### [Opinions about the future of machine\nintelligence](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_006.html#filepos15739)\n\nProgress on two major fronts—towards a more solid statistical and information-\ntheoretic foundation for machine learning on the one hand, and towards the\npractical and commercial success of various problem-specific or domain-\nspecific applications on the other—has restored to AI research some of its\nlost prestige. There may, however, be a residual cultural effect on the AI\ncommunity of its earlier history that makes many mainstream researchers\nreluctant to align themselves with over-grand ambition. Thus Nils Nilsson, one\nof the old-timers in the field, complains that his present-day colleagues lack\nthe boldness of spirit that propelled the pioneers of his own generation:\n\n> Concern for “respectability” has had, I think, a stultifying effect on some\n> AI researchers. I hear them saying things like, “AI used to be criticized\n> for its flossiness. Now that we have made solid progress, let us not risk\n> losing our respectability.” One result of this conservatism has been\n> increased concentration on “weak AI”—the variety devoted to providing aids\n> to human thought—and away from “strong AI”—the variety that attempts to\n> mechanize human-level\n> intelligence.[73](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos927207)\n\nNilsson’s sentiment has been echoed by several others of the founders,\nincluding Marvin Minsky, John McCarthy, and Patrick\nWinston.[74](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos927322)\n\nThe last few years have seen a resurgence of interest in AI, which might yet\nspill over into renewed efforts towards artificial _general_ intelligence\n(what Nilsson calls “strong AI”). In addition to faster hardware, a\ncontemporary project would benefit from the great strides that have been made\nin the many subfields of AI, in software engineering more generally, and in\nneighboring fields such as computational neuroscience. One indication of pent-\nup demand for quality information and education is shown in the response to\nthe free online offering of an introductory course in artificial intelligence\nat Stanford University in the fall of 2011, organized by Sebastian Thrun and\nPeter Norvig. Some 160,000 students from around the world signed up to take it\n(and 23,000 completed\nit).[75](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos927473)\n\nExpert opinions about the future of AI vary wildly. There is disagreement\nabout timescales as well as about what forms AI might eventually take.\nPredictions about the future development of artificial intelligence, one\nrecent study noted, “are as confident as they are\ndiverse.”[76](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos927771)\n\nAlthough the contemporary distribution of belief has not been very carefully\nmeasured, we can get a rough impression from various smaller surveys and\ninformal observations. In particular, a series of recent surveys have polled\nmembers of several relevant expert communities on the question of when they\nexpect “human-level machine intelligence” (HLMI) to be developed, defined as\n“one that can carry out most human professions at least as well as a typical\nhuman.”[77](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos927894)\nResults are shown in Table 2. The combined sample gave the following (median)\nestimate: 10% probability of HLMI by 2022, 50% probability by 2040, and 90%\nprobability by 2075. (Respondents were asked to premiss their estimates on the\nassumption that “human scientific activity continues without major negative\ndisruption.”)\n\nThese numbers should be taken with some grains of salt: sample sizes are quite\nsmall and not necessarily representative of the general expert population.\nThey are, however, in concordance with results from other\nsurveys.[78](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos928027)\n\nThe survey results are also in line with some recently published interviews\nwith about two-dozen researchers in AI-related fields. For example, Nils\nNilsson has spent a long and productive career working on problems in search,\nplanning, knowledge representation, and robotics; he has authored textbooks in\nartificial intelligence; and he recently completed the most comprehensive\nhistory of the field written to\ndate.[79](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos928208) When\nasked about arrival dates for HLMI, he offered the following\nopinion:[80](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos928318)\n\n10% chance: 2030\n\n50% chance: 2050\n\n90% chance: 2100\n\n**Table 2 _When will human-level machine intelligence be\nattained?_[81](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos928637)**\n\n![Image](images/00004.jpg)\n\nJudging from the published interview transcripts, Professor Nilsson’s\nprobability distribution appears to be quite representative of many experts in\nthe area—though again it must be emphasized that there is a wide spread of\nopinion: there are practitioners who are substantially more boosterish,\nconfidently expecting HLMI in the 2020–40 range, and others who are confident\neither that it will never happen or that it is indefinitely far\noff.[82](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos929674) In\naddition, some interviewees feel that the notion of a “human level” of\nartificial intelligence is ill-defined or misleading, or are for other reasons\nreluctant to go on record with a quantitative prediction.\n\nMy own view is that the median numbers reported in the expert survey do not\nhave enough probability mass on later arrival dates. A 10% probability of HLMI\nnot having been developed by 2075 or even 2100 (after conditionalizing on\n“human scientific activity continuing without major negative disruption”)\nseems too low.\n\nHistorically, AI researchers have not had a strong record of being able to\npredict the rate of advances in their own field or the shape that such\nadvances would take. On the one hand, some tasks, like chess playing, turned\nout to be achievable by means of surprisingly simple programs; and naysayers\nwho claimed that machines would “never” be able to do this or that have\nrepeatedly been proven wrong. On the other hand, the more typical errors among\npractitioners have been to underestimate the difficulties of getting a system\nto perform robustly on real-world tasks, and to overestimate the advantages of\ntheir own particular pet project or technique.\n\nThe survey also asked two other questions of relevance to our inquiry. One\ninquired of respondents about how much longer they thought it would take to\nreach superintelligence assuming human-level machine is first achieved. The\nresults are in Table 3.\n\nAnother question inquired what they thought would be the overall long-term\nimpact for humanity of achieving human-level machine intelligence. The answers\nare summarized in Figure 2.\n\nMy own views again differ somewhat from the opinions expressed in the survey.\nI assign a higher probability to superintelligence being created relatively\nsoon after human-level machine intelligence. I also have a more polarized\noutlook on the consequences, thinking an extremely good or an extremely bad\noutcome to be somewhat more likely than a more balanced outcome. The reasons\nfor this will become clear later in the book.\n\n**Table 3 _How long from human level to superintelligence?_**\n\n|  \n---|---  \n| Within 2 years after HLMI | Within 30 years after HLMI  \nTOP100 | 5% | 50%  \nCombined | 10% | 75%  \n  \n![Image](images/00005.jpg)\n\n**Figure 2** Overall long-term impact of\nHLMI.[83](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos929888)\n\nSmall sample sizes, selection biases, and—above all—the inherent unreliability\nof the subjective opinions elicited mean that one should not read too much\ninto these expert surveys and interviews. They do not let us draw any strong\nconclusion. But they do hint at a weak conclusion. They suggest that (at least\nin lieu of better data or analysis) it may be reasonable to believe that\nhuman-level machine intelligence has a fairly sizeable chance of being\ndeveloped by mid-century, and that it has a non-trivial chance of being\ndeveloped considerably sooner or much later; that it might perhaps fairly soon\nthereafter result in superintelligence; and that a wide range of outcomes may\nhave a significant chance of occurring, including extremely good outcomes and\noutcomes that are as bad as human\nextinction.[84](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos930229)\nAt the very least, they suggest that the topic is worth a closer look.\n\n\n## [CHAPTER 2  \nPaths to\nsuperintelligence](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_006.html#filepos15876)\n\n**Machines are currently far inferior to humans in general intelligence. Yet\none day (we have suggested) they will be superintelligent. How do we get from\nhere to there? This chapter explores several conceivable technological paths.\nWe look at artificial intelligence, whole brain emulation, biological\ncognition, and human–machine interfaces, as well as networks and\norganizations. We evaluate their different degrees of plausibility as pathways\nto superintelligence. The existence of multiple paths increases the\nprobability that the destination can be reached via at least one of them.**\n\nWe can tentatively define a superintelligence as _any intellect that greatly\nexceeds the cognitive performance of humans in virtually all domains of\ninterest_.[1](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos931905)\nWe will have more to say about the concept of superintelligence in the next\nchapter, where we will subject it to a kind of spectral analysis to\ndistinguish some different possible forms of superintelligence. But for now,\nthe rough characterization just given will suffice. Note that the definition\nis noncommittal about how the superintelligence is implemented. It is also\nnoncommittal regarding qualia: whether a superintelligence would have\nsubjective conscious experience might matter greatly for some questions (in\nparticular for some moral questions), but our primary focus here is on the\ncausal antecedents and consequences of superintelligence, not on the\nmetaphysics of\nmind.[2](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos932493)\n\nThe chess program Deep Fritz is not a superintelligence on this definition,\nsince Fritz is only smart within the narrow domain of chess. Certain kinds of\ndomain-specific superintelligence could, however, be important. When referring\nto superintelligent performance limited to a particular domain, we will note\nthe restriction explicitly. For instance, an “engineering superintelligence”\nwould be an intellect that vastly outperforms the best current human minds in\nthe domain of engineering. Unless otherwise noted, we use the term to refer to\nsystems that have a superhuman level of _general_ intelligence.\n\nBut how might we create superintelligence? Let us examine some possible paths.\n\n### [Artificial\nintelligence](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_006.html#filepos15992)\n\nReaders of this chapter must not expect a blueprint for programming an\nartificial general intelligence. No such blueprint exists yet, of course. And\nhad I been in possession of such a blueprint, I most certainly would not have\npublished it in a book. (If the reasons for this are not immediately obvious,\nthe arguments in subsequent chapters will make them clear.)\n\nWe can, however, discern some general features of the kind of system that\nwould be required. It now seems clear that a capacity to learn would be an\nintegral feature of the core design of a system intended to attain general\nintelligence, not something to be tacked on later as an extension or an\nafterthought. The same holds for the ability to deal effectively with\nuncertainty and probabilistic information. Some faculty for extracting useful\nconcepts from sensory data and internal states, and for leveraging acquired\nconcepts into flexible combinatorial representations for use in logical and\nintuitive reasoning, also likely belong among the core design features in a\nmodern AI intended to attain general intelligence.\n\nThe early Good Old-Fashioned Artificial Intelligence systems did not, for the\nmost part, focus on learning, uncertainty, or concept formation, perhaps\nbecause techniques for dealing with these dimensions were poorly developed at\nthe time. This is not to say that the underlying ideas are all that novel. The\nidea of using learning as a means of bootstrapping a simpler system to human-\nlevel intelligence can be traced back at least to Alan Turing’s notion of a\n“child machine,” which he wrote about in 1950:\n\n> Instead of trying to produce a programme to simulate the adult mind, why not\n> rather try to produce one which simulates the child’s? If this were then\n> subjected to an appropriate course of education one would obtain the adult\n> brain.[3](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos933024)\n\nTuring envisaged an iterative process to develop such a child machine:\n\n> We cannot expect to find a good child machine at the first attempt. One must\n> experiment with teaching one such machine and see how well it learns. One\n> can then try another and see if it is better or worse. There is an obvious\n> connection between this process and evolution…. One may hope, however, that\n> this process will be more expeditious than evolution. The survival of the\n> fittest is a slow method for measuring advantages. The experimenter, by the\n> exercise of intelligence, should be able to speed it up. Equally important\n> is the fact that he is not restricted to random mutations. If he can trace a\n> cause for some weakness he can probably think of the kind of mutation which\n> will improve\n> it.[4](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos933136)\n\nWe know that blind evolutionary processes can produce human-level general\nintelligence, since they have already done so at least once. Evolutionary\nprocesses with foresight—that is, genetic programs designed and guided by an\nintelligent human programmer—should be able to achieve a similar outcome with\nfar greater efficiency. This observation has been used by some philosophers\nand scientists, including David Chalmers and Hans Moravec, to argue that\nhuman-level AI is not only theoretically possible but feasible within this\ncentury.[5](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos933248)\nThe idea is that we can estimate the relative capabilities of evolution and\nhuman engineering to produce intelligence, and find that human engineering is\nalready vastly superior to evolution in some areas and is likely to become\nsuperior in the remaining areas before too long. The fact that evolution\nproduced intelligence therefore indicates that human engineering will soon be\nable to do the same. Thus, Moravec wrote (already back in 1976):\n\n> The existence of several examples of intelligence designed under these\n> constraints should give us great confidence that we can achieve the same in\n> short order. The situation is analogous to the history of heavier than air\n> flight, where birds, bats and insects clearly demonstrated the possibility\n> before our culture mastered\n> it.[6](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos933391)\n\nOne needs to be cautious, though, in what inferences one draws from this line\nof reasoning. It is true that evolution produced heavier-than-air flight, and\nthat human engineers subsequently succeeded in doing likewise (albeit by means\nof a very different mechanism). Other examples could also be adduced, such as\nsonar, magnetic navigation, chemical weapons, photoreceptors, and all kinds of\nmechanic and kinetic performance characteristics. However, one could equally\npoint to areas where human engineers have thus far failed to match evolution:\nin morphogenesis, self-repair, and the immune defense, for example, human\nefforts lag far behind what nature has accomplished. Moravec’s argument,\ntherefore, cannot give us “great confidence” that we can achieve human-level\nartificial intelligence “in short order.” At best, the evolution of\nintelligent life places an upper bound on the intrinsic difficulty of\ndesigning intelligence. But this upper bound could be quite far above current\nhuman engineering capabilities.\n\nAnother way of deploying an evolutionary argument for the feasibility of AI is\nvia the idea that we could, by running genetic algorithms on sufficiently fast\ncomputers, achieve results comparable to those of biological evolution. This\nversion of the evolutionary argument thus proposes a specific method whereby\nintelligence could be produced.\n\nBut is it true that we will soon have computing power sufficient to\nrecapitulate the relevant evolutionary processes that produced human\nintelligence? The answer depends both on how much computing technology will\nadvance over the next decades and on how much computing power would be\nrequired to run genetic algorithms with the same optimization power as the\nevolutionary process of natural selection that lies in our past. Although, in\nthe end, the conclusion we get from pursuing this line of reasoning is\ndisappointingly indeterminate, it is instructive to attempt a rough estimate\n(see Box 3). If nothing else, the exercise draws attention to some interesting\nunknowns.\n\nThe upshot is that the computational resources required to simply replicate\nthe relevant evolutionary processes on Earth that produced human-level\nintelligence are severely out of reach—and will remain so even if Moore’s law\nwere to continue for a century (cf. Figure 3). It is plausible, however, that\ncompared with brute-force replication of natural evolutionary processes, vast\nefficiency gains are achievable by designing the search process to _aim_ for\nintelligence, using various obvious improvements over natural selection. Yet\nit is very hard to bound the magnitude of those attainable efficiency gains.\nWe cannot even say whether they amount to five or to twenty-five orders of\nmagnitude. Absent further elaboration, therefore, evolutionary arguments are\nnot able to meaningfully constrain our expectations of either the difficulty\nof building human-level machine intelligence or the timescales for such\ndevelopments.\n\n* * *\n\n### **Box 3 What would it take to recapitulate evolution?**\n\nNot every feat accomplished by evolution in the course of the development of\nhuman intelligence is relevant to a human engineer trying to artificially\nevolve machine intelligence. Only a small portion of evolutionary selection on\nEarth has been selection for intelligence. More specifically, the problems\nthat human engineers cannot trivially bypass may have been the target of a\nvery small portion of total evolutionary selection. For example, since we can\nrun our computers on electrical power, we do not have to reinvent the\nmolecules of the cellular energy economy in order to create intelligent\nmachines—yet such molecular evolution of metabolic pathways might have used up\na large part of the total amount of selection power that was available to\nevolution over the course of Earth’s\nhistory.[7](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos933560)\n\nOne might argue that the key insights for AI are embodied in the structure of\nnervous systems, which came into existence less than a billion years\nago.[8](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos933740) If we\ntake that view, then the number of relevant “experiments” available to\nevolution is drastically curtailed. There are some 4–6×1030 prokaryotes in the\nworld today, but only 1019 insects, and fewer than 1010 humans (while pre-\nagricultural populations were orders of magnitude\nsmaller).[9](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos934302)\nThese numbers are only moderately intimidating.\n\nEvolutionary algorithms, however, require not only variations to select among\nbut also a fitness function to evaluate variants, and this is typically the\nmost computationally expensive component. A fitness function for the evolution\nof artificial intelligence plausibly requires simulation of neural\ndevelopment, learning, and cognition to evaluate fitness. We might thus do\nbetter not to look at the raw number of organisms with complex nervous\nsystems, but instead to attend to the number of neurons in biological\norganisms that we might need to simulate to mimic evolution’s fitness\nfunction. We can make a crude estimate of that latter quantity by considering\ninsects, which dominate terrestrial animal biomass (with ants alone estimated\nto contribute some\n15–20%).[10](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos934434)\nInsect brain size varies substantially, with large and social insects sporting\nlarger brains: a honeybee brain has just under 106 neurons, a fruit fly brain\nhas 105 neurons, and ants are in between with 250,000\nneurons.[11](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos934544)\nThe majority of smaller insects may have brains of only a few thousand\nneurons. Erring on the side of conservatively high, if we assigned all 1019\ninsects fruit-fly numbers of neurons, the total would be 1024 insect neurons\nin the world. This could be augmented with an additional order of magnitude to\naccount for aquatic copepods, birds, reptiles, mammals, etc., to reach 1025.\n(By contrast, in pre-agricultural times there were fewer than 107 humans, with\nunder 1011 neurons each: thus fewer than 1018 human neurons in total, though\nhumans have a higher number of synapses per neuron.)\n\nThe computational cost of simulating one neuron depends on the level of detail\nthat one includes in the simulation. Extremely simple neuron models use about\n1,000 floating-point operations per second (FLOPS) to simulate one neuron (in\nreal-time). The electrophysiologically realistic Hodgkin–Huxley model uses\n1,200,000 FLOPS. A more detailed multi-compartmental model would add another\nthree to four orders of magnitude, while higher-level models that abstract\nsystems of neurons could subtract two to three orders of magnitude from the\nsimple\nmodels.[12](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos934690) If\nwe were to simulate 1025 neurons over a billion years of evolution (longer\nthan the existence of nervous systems as we know them), and we allow our\ncomputers to run for one year, these figures would give us a requirement in\nthe range of 1031–1044 FLOPS. For comparison, China’s Tianhe-2, the world’s\nmost powerful supercomputer as of September 2013, provides only 3.39×1016\nFLOPS. In recent decades, it has taken approximately 6.7 years for commodity\ncomputers to increase in power by one order of magnitude. Even a century of\ncontinued Moore’s law would not be enough to close this gap. Running more\nspecialized hardware, or allowing longer run-times, could contribute only a\nfew more orders of magnitude.\n\nThis figure is conservative in another respect. Evolution achieved human\nintelligence without aiming at this outcome. In other words, the fitness\nfunctions for natural organisms do not select only for intelligence and its\nprecursors.[13](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos934813)\nEven environments in which organisms with superior information processing\nskills reap various rewards may not select for intelligence, because\nimprovements to intelligence can (and often do) impose significant costs, such\nas higher energy consumption or slower maturation times, and those costs may\noutweigh whatever benefits are gained from smarter behavior. Excessively\ndeadly environments also reduce the value of intelligence: the shorter one’s\nexpected lifespan, the less time there will be for increased learning ability\nto pay off. Reduced selective pressure for intelligence slows the spread of\nintelligence-enhancing innovations, and thus the opportunity for selection to\nfavor subsequent innovations that depend on them. Furthermore, evolution may\nwind up stuck in local optima that humans would notice and bypass by altering\ntradeoffs between exploitation and exploration or by providing a smooth\nprogression of increasingly difficult intelligence\ntests.[14](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos935087) And\nas mentioned earlier, evolution scatters much of its selection power on traits\nthat are unrelated to intelligence (such as Red Queen’s races of competitive\nco-evolution between immune systems and parasites). Evolution continues to\nwaste resources producing mutations that have proved consistently lethal, and\nit fails to take advantage of statistical similarities in the effects of\ndifferent mutations. These are all inefficiencies in natural selection (when\nviewed as a means of evolving intelligence) that it would be relatively easy\nfor a human engineer to avoid while using evolutionary algorithms to develop\nintelligent software.\n\nIt is plausible that eliminating inefficiencies like those just described\nwould trim many orders of magnitude off the 1031–1044 FLOPS range calculated\nearlier. Unfortunately, it is difficult to know how many orders of magnitude.\nIt is difficult even to make a rough estimate—for aught we know, the\nefficiency savings could be five orders of magnitude, or ten, or twenty-\nfive.[15](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos935335)\n\n* * *\n\n![Image](images/00006.jpg)\n\n**Figure 3** Supercomputer performance. In a narrow sense, “Moore’s law”\nrefers to the observation that the number of transistors on integrated\ncircuits have for several decades doubled approximately every two years.\nHowever, the term is often used to refer to the more general observation that\nmany performance metrics in computing technology have followed a similarly\nfast exponential trend. Here we plot peak speed of the world’s fastest\nsupercomputer as a function of time (on a logarithmic vertical scale). In\nrecent years, growth in the serial speed of processors has stagnated, but\nincreased use of parallelization has enabled the total number of computations\nperformed to remain on the trend\nline.[16](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos937417)\n\nThere is a further complication with these kinds of evolutionary\nconsiderations, one that makes it hard to derive from them even a very loose\nupper bound on the difficulty of evolving intelligence. We must avoid the\nerror of inferring, from the fact that intelligent life evolved on Earth, that\nthe evolutionary processes involved had a reasonably high prior probability of\nproducing intelligence. Such an inference is unsound because it fails to take\naccount of the observation selection effect that guarantees that all observers\nwill find themselves having originated on a planet where intelligent life\narose, no matter how likely or unlikely it was for any given such planet to\nproduce intelligence. Suppose, for example, that in addition to the systematic\neffects of natural selection it required an enormous amount of _lucky\ncoincidence_ to produce intelligent life—enough so that intelligent life\nevolves on only one planet out of every 1030 planets on which simple\nreplicators arise. In that case, when we run our genetic algorithms to try to\nreplicate what natural evolution did, we might find that we must run some 1030\nsimulations before we find one where all the elements come together in just\nthe right way. This seems fully consistent with our observation that life did\nevolve here on Earth. Only by careful and somewhat intricate reasoning—by\nanalyzing instances of convergent evolution of intelligence-related traits and\nengaging with the subtleties of observation selection theory—can we partially\ncircumvent this epistemological barrier. Unless one takes the trouble to do\nso, one is not in a position to rule out the possibility that the alleged\n“upper bound” on the computational requirements for recapitulating the\nevolution of intelligence derived in Box 3 might be too low by thirty orders\nof magnitude (or some other such large\nnumber).[17](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos937530)\n\nAnother way of arguing for the feasibility of artificial intelligence is by\npointing to the human brain and suggesting that we could use it as a template\nfor a machine intelligence. One can distinguish different versions of this\napproach based on how closely they propose to imitate biological brain\nfunctions. At one extreme—that of very close imitation—we have the idea of\n_whole brain emulation_ , which we will discuss in the next subsection. At the\nother extreme are approaches that take their inspiration from the functioning\nof the brain but do not attempt low-level imitation. Advances in neuroscience\nand cognitive psychology—which will be aided by improvements in\ninstrumentation—should eventually uncover the general principles of brain\nfunction. This knowledge could then guide AI efforts. We have already\nencountered neural networks as an example of a brain-inspired AI technique.\nHierarchical perceptual organization is another idea that has been transferred\nfrom brain science to machine learning. The study of reinforcement learning\nhas been motivated (at least in part) by its role in psychological theories of\nanimal cognition, and reinforcement learning techniques (e.g. the “TD-\nalgorithm”) inspired by these theories are now widely used in\nAI.[18](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos937840) More\ncases like these will surely accumulate in the future. Since there is a\nlimited number—perhaps a very small number—of distinct fundamental mechanisms\nthat operate in the brain, continuing incremental progress in brain science\nshould eventually discover them all. Before this happens, though, it is\npossible that a hybrid approach, combining some brain-inspired techniques with\nsome purely artificial methods, would cross the finishing line. In that case,\nthe resultant system need not be recognizably brain-like even though some\nbrain-derived insights were used in its development.\n\nThe availability of the brain as template provides strong support for the\nclaim that machine intelligence is ultimately feasible. This, however, does\nnot enable us to predict when it will be achieved because it is hard to\npredict the future rate of discoveries in brain science. What we can say is\nthat the further into the future we look, the greater the likelihood that the\nsecrets of the brain’s functionality will have been decoded sufficiently to\nenable the creation of machine intelligence in this manner.\n\nDifferent people working toward machine intelligence hold different views\nabout how promising neuromorphic approaches are compared with approaches that\naim for completely synthetic designs. The existence of birds demonstrated that\nheavier-than-air flight was physically possible and prompted efforts to build\nflying machines. Yet the first functioning airplanes did not flap their wings.\nThe jury is out on whether machine intelligence will be like flight, which\nhumans achieved through an artificial mechanism, or like combustion, which we\ninitially mastered by copying naturally occurring fires.\n\nTuring’s idea of designing a program that acquires most of its content by\nlearning, rather than having it pre-programmed at the outset, can apply\nequally to neuromorphic and synthetic approaches to machine intelligence.\n\nA variation on Turing’s conception of a child machine is the idea of a “seed\nAI.”[19](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos937987)\nWhereas a child machine, as Turing seems to have envisaged it, would have a\nrelatively fixed architecture that simply develops its inherent potentialities\nby accumulating _content_ , a seed AI would be a more sophisticated artificial\nintelligence capable of improving its own _architecture_. In the early stages\nof a seed AI, such improvements might occur mainly through trial and error,\ninformation acquisition, or assistance from the programmers. At its later\nstages, however, a seed AI should be able to _understand_ its own workings\nsufficiently to engineer new algorithms and computational structures to\nbootstrap its cognitive performance. This needed understanding could result\nfrom the seed AI reaching a sufficient level of general intelligence across\nmany domains, or from crossing some threshold in a particularly relevant\ndomain such as computer science or mathematics.\n\nThis brings us to another important concept, that of “recursive self-\nimprovement.” A successful seed AI would be able to iteratively enhance\nitself: an early version of the AI could design an improved version of itself,\nand the improved version—being smarter than the original—might be able to\ndesign an even smarter version of itself, and so\nforth.[20](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos938157)\nUnder some conditions, such a process of recursive self-improvement might\ncontinue long enough to result in an intelligence explosion—an event in which,\nin a short period of time, a system’s level of intelligence increases from a\nrelatively modest endowment of cognitive capabilities (perhaps sub-human in\nmost respects, but with a domain-specific talent for coding and AI research)\nto radical superintelligence. We will return to this important possibility in\n[Chapter 4](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_011.html#filepos236122),\nwhere the dynamics of such an event will be analyzed more closely. Note that\nthis model suggests the possibility of surprises: attempts to build artificial\ngeneral intelligence might fail pretty much completely until the last missing\ncritical component is put in place, at which point a seed AI might become\ncapable of sustained recursive self-improvement.\n\nBefore we end this subsection, there is one more thing that we should\nemphasize, which is that an artificial intelligence need not much resemble a\nhuman mind. AIs could be—indeed, it is likely that most will be—extremely\nalien. We should expect that they will have very different cognitive\narchitectures than biological intelligences, and in their early stages of\ndevelopment they will have very different profiles of cognitive strengths and\nweaknesses (though, as we shall later argue, they could eventually overcome\nany initial weakness). Furthermore, the goal systems of AIs could diverge\nradically from those of human beings. There is no reason to expect a generic\nAI to be motivated by love or hate or pride or other such common human\nsentiments: these complex adaptations would require deliberate expensive\neffort to recreate in AIs. This is at once a big problem and a big\nopportunity. We will return to the issue of AI motivation in later chapters,\nbut it is so central to the argument in this book that it is worth bearing in\nmind throughout.\n\n### [Whole brain\nemulation](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_006.html#filepos16090)\n\nIn whole brain emulation (also known as “uploading”), intelligent software\nwould be produced by scanning and closely modeling the computational structure\nof a biological brain. This approach thus represents a limiting case of\ndrawing inspiration from nature: barefaced plagiarism. Achieving whole brain\nemulation requires the accomplishment of the following steps.\n\nFirst, a sufficiently detailed scan of a particular human brain is created.\nThis might involve stabilizing the brain post-mortem through vitrification (a\nprocess that turns tissue into a kind of glass). A machine could then dissect\nthe tissue into thin slices, which could be fed into another machine for\nscanning, perhaps by an array of electron microscopes. Various stains might be\napplied at this stage to bring out different structural and chemical\nproperties. Many scanning machines could work in parallel to process multiple\nbrain slices simultaneously.\n\nSecond, the raw data from the scanners is fed to a computer for automated\nimage processing to reconstruct the three-dimensional neuronal network that\nimplemented cognition in the original brain. In practice, this step might\nproceed concurrently with the first step to reduce the amount of high-\nresolution image data stored in buffers. The resulting map is then combined\nwith a library of neurocomputational models of different types of neurons or\nof different neuronal elements (such as particular kinds of synaptic\nconnectors). Figure 4 shows some results of scanning and image processing\nproduced with present-day technology.\n\nIn the third stage, the neurocomputational structure resulting from the\nprevious step is implemented on a sufficiently powerful computer. If\ncompletely successful, the result would be a digital reproduction of the\noriginal intellect, with memory and personality intact. The emulated human\nmind now exists as software on a computer. The mind can either inhabit a\nvirtual reality or interface with the external world by means of robotic\nappendages.\n\nThe whole brain emulation path does not require that we figure out how human\ncognition works or how to program an artificial intelligence. It requires only\nthat we understand the low-level functional characteristics of the basic\ncomputational elements of the brain. No fundamental conceptual or theoretical\nbreakthrough is needed for whole brain emulation to succeed.\n\nWhole brain emulation does, however, require some rather advanced enabling\ntechnologies. There are three key prerequisites: (1) _scanning_ : high-\nthroughput microscopy with sufficient resolution and detection of relevant\nproperties; (2) _translation_ : automated image analysis to turn raw scanning\ndata into an interpreted three-dimensional model of relevant\nneurocomputational elements; and (3) _simulation_ : hardware powerful enough\nto implement the resultant computational structure (see Table 4). (In\ncomparison with these more challenging steps, the construction of a basic\nvirtual reality or a robotic embodiment with an audiovisual input channel and\nsome simple output channel is relatively easy. Simple yet minimally adequate\nI/O seems feasible already with present\ntechnology.[23](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos938841))\n\n![Image](images/00007.jpg)\n\n**Figure 4** Reconstructing 3D neuroanatomy from electron microscope images.\n_Upper left_ : A typical electron micrograph showing cross-sections of\nneuronal matter—dendrites and axons. _Upper right_ : Volume image of rabbit\nretinal neural tissue acquired by serial block-face scanning electron\nmicroscopy.[21](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos938603)\nIndividual 2D images have been stacked into a cube (with a side of\napproximately 11 μm). _Bottom_ : Reconstruction of a subset of the neuronal\nprojections filling a volume of neuropil, generated by an automated\nsegmentation\nalgorithm.[22](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos938725)\n\nThere is good reason to think that the requisite enabling technologies are\nattainable, though not in the near future. Reasonable computational models of\nmany types of neuron and neuronal processes already exist. Image recognition\nsoftware has been developed that can trace axons and dendrites through a stack\nof two-dimensional images (though reliability needs to be improved). And there\nare imaging tools that provide the necessary resolution—with a scanning\ntunneling microscope it is possible to “see” individual atoms, which is a far\nhigher resolution than needed. However, although present knowledge and\ncapabilities suggest that there is no in-principle barrier to the development\nof the requisite enabling technologies, it is clear that a very great deal of\nincremental technical progress would be needed to bring human whole brain\nemulation within\nreach.[24](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos939164) For\nexample, microscopy technology would need not just sufficient resolution but\nalso sufficient throughput. Using an atomic-resolution scanning tunneling\nmicroscope to image the needed surface area would be far too slow to be\npracticable. It would be more plausible to use a lower-resolution electron\nmicroscope, but this would require new methods for preparing and staining\ncortical tissue to make visible relevant details such as synaptic fine\nstructure. A great expansion of neurocomputational libraries and major\nimprovements in automated image processing and scan interpretation would also\nbe needed.\n\n**Table 4 _Capabilities needed for whole brain emulation_**\n\n![Image](images/00008.jpg)\n\n![Image](images/00009.jpg)\n\nIn general, whole brain emulation relies less on theoretical insight and more\non technological capability than artificial intelligence. Just how much\ntechnology is required for whole brain emulation depends on the level of\nabstraction at which the brain is emulated. In this regard there is a tradeoff\nbetween insight and technology. In general, the worse our scanning equipment\nand the feebler our computers, the less we could rely on simulating low-level\nchemical and electrophysiological brain processes, and the more theoretical\nunderstanding would be needed of the computational architecture that we are\nseeking to emulate in order to create more abstract representations of the\nrelevant\nfunctionalities.[25](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos939275)\nConversely, with sufficiently advanced scanning technology and abundant\ncomputing power, it might be possible to brute-force an emulation even with a\nfairly limited understanding of the brain. In the unrealistic limiting case,\nwe could imagine emulating a brain at the level of its elementary particles\nusing the quantum mechanical Schrödinger equation. Then one could rely\nentirely on existing knowledge of physics and not at all on any biological\nmodel. This extreme case, however, would place utterly impracticable demands\non computational power and data acquisition. A far more plausible level of\nemulation would be one that incorporates individual neurons and their\nconnectivity matrix, along with some of the structure of their dendritic trees\nand maybe some state variables of individual synapses. Neurotransmitter\nmolecules would not be simulated individually, but their fluctuating\nconcentrations would be modeled in a coarse-grained manner.\n\nTo assess the feasibility of whole brain emulation, one must understand the\ncriterion for success. The aim is not to create a brain simulation so detailed\nand accurate that one could use it to predict exactly what would have happened\nin the original brain if it had been subjected to a particular sequence of\nstimuli. Instead, the aim is to capture enough of the computationally\nfunctional properties of the brain to enable the resultant emulation to\nperform intellectual work. For this purpose, much of the messy biological\ndetail of a real brain is irrelevant.\n\nA more elaborate analysis would distinguish between different levels of\nemulation success based on the extent to which the information-processing\nfunctionality of the emulated brain has been preserved. For example, one could\ndistinguish among (1) a _high-fidelity emulation_ that has the full set of\nknowledge, skills, capacities, and values of the emulated brain; (2) a\n_distorted emulation_ whose dispositions are significantly non-human in some\nways but which is mostly able to do the same intellectual labor as the\nemulated brain; and (3) a _generic emulation_ (which might also be distorted)\nthat is somewhat like an infant, lacking the skills or memories that had been\nacquired by the emulated adult brain but with the capacity to learn most of\nwhat a normal human can\nlearn.[26](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos939466)\n\nWhile it appears ultimately feasible to produce a high-fidelity emulation, it\nseems quite likely that the _first_ whole brain emulation that we would\nachieve if we went down this path would be of a lower grade. Before we would\nget things to work perfectly, we would probably get things to work\nimperfectly. It is also possible that a push toward emulation technology would\nlead to the creation of some kind of neuromorphic AI that would adapt some\nneurocomputational principles discovered during emulation efforts and\nhybridize them with synthetic methods, and that this would happen before the\ncompletion of a fully functional whole brain emulation. The possibility of\nsuch a spillover into neuromorphic AI, as we shall see in a later chapter,\ncomplicates the strategic assessment of the desirability of seeking to\nexpedite emulation technology.\n\nHow far are we currently from achieving a human whole brain emulation? One\nrecent assessment presented a technical roadmap and concluded that the\nprerequisite capabilities might be available around mid-century, though with a\nlarge uncertainty\ninterval.[27](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos940374)\nFigure 5 depicts the major milestones in this roadmap. The apparent simplicity\nof the map may be deceptive, however, and we should be careful not to\nunderstate how much work remains to be done. No brain has yet been emulated.\nConsider the humble model organism _Caenorhabditis elegans_ , which is a\ntransparent roundworm, about 1 mm in length, with 302 neurons. The complete\nconnectivity matrix of these neurons has been known since the mid-1980s, when\nit was laboriously mapped out by means of slicing, electron microscopy, and\nhand-labeling of\nspecimens.[29](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos940677)\nBut knowing merely which neurons are connected with which is not enough. To\ncreate a brain emulation one would also need to know which synapses are\nexcitatory and which are inhibitory; the strength of the connections; and\nvarious dynamical properties of axons, synapses, and dendritic trees. This\ninformation is not yet available even for the small nervous system of _C.\nelegans_ (although it may now be within range of a targeted moderately sized\nresearch\nproject).[30](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos941029)\nSuccess at emulating a tiny brain, such as that of _C. elegans_ , would give\nus a better view of what it would take to emulate larger brains.\n\n![Image](images/00010.jpg)\n\n**Figure 5** Whole brain emulation roadmap. Schematic of inputs, activities,\nand\nmilestones.[28](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos940497)\n\nAt some point in the technology development process, once techniques are\navailable for automatically emulating small quantities of brain tissue, the\nproblem reduces to one of scaling. Notice “the ladder” at the right side of\nFigure 5. This ascending series of boxes represents a final sequence of\nadvances which can commence after preliminary hurdles have been cleared. The\nstages in this sequence correspond to whole brain emulations of successively\nmore neurologically sophisticated model organisms—for example, _C. elegans →\nhoneybee_ → _mouse_ → _rhesus monkey_ → _human_. Because the gaps between\nthese rungs—at least after the first step—are mostly quantitative in nature\nand due mainly (though not entirely) to the differences in size of the brains\nto be emulated, they should be tractable through a relatively straightforward\nscale-up of scanning and simulation\ncapacity.[31](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos941986)\n\nOnce we start ascending this final ladder, the _eventual_ attainment of human\nwhole brain emulation becomes more clearly\nforeseeable.[32](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos942282)\nWe can thus expect to get some advance warning before arrival at human-level\nmachine intelligence along the whole brain emulation path, at least if the\nlast among the requisite enabling technologies to reach sufficient maturity is\neither high-throughput scanning or the computational power needed for real-\ntime simulation. If, however, the last enabling technology to fall into place\nis neurocomputational modeling, then the transition from unimpressive\nprototypes to a working human emulation could be more abrupt. One could\nimagine a scenario in which, despite abundant scanning data and fast\ncomputers, it is proving difficult to get our neuronal models to work right.\nWhen finally the last glitch is ironed out, what was previously a completely\ndysfunctional system—analogous perhaps to an unconscious brain undergoing a\ngrand mal seizure—might snap into a coherent wakeful state. In this case, the\nkey advance would not be heralded by a series of functioning animal emulations\nof increasing magnitude (provoking newspaper headlines of correspondingly\nescalating font size). Even for those paying attention it might be difficult\nto tell in advance of success just how many flaws remained in the\nneurocomputational models at any point and how long it would take to fix them,\neven up to the eve of the critical breakthrough. (Once a human whole brain\nemulation has been achieved, further potentially explosive developments would\ntake place; but we postpone discussion of this until [Chapter\n4](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_011.html#filepos236122).)\n\nSurprise scenarios are thus imaginable for whole brain emulation even if all\nthe relevant research were conducted in the open. Nevertheless, compared with\nthe AI path to machine intelligence, whole brain emulation is more likely to\nbe preceded by clear omens since it relies more on concrete observable\ntechnologies and is not wholly based on theoretical insight. We can also say,\nwith greater confidence than for the AI path, that the emulation path will not\nsucceed in the near future (within the next fifteen years, say) because we\nknow that several challenging precursor technologies have not yet been\ndeveloped. By contrast, it seems likely that somebody could _in principle_ sit\ndown and code a seed AI on an ordinary present-day personal computer; and it\nis conceivable—though unlikely—that somebody somewhere will get the right\ninsight for how to do this in the near future.\n\n### [Biological\ncognition](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_006.html#filepos16186)\n\nA third path to greater-than-current-human intelligence is to enhance the\nfunctioning of biological brains. In principle, this could be achieved without\ntechnology, through selective breeding. Any attempt to initiate a classical\nlarge-scale eugenics program, however, would confront major political and\nmoral hurdles. Moreover, unless the selection were extremely strong, many\ngenerations would be required to produce substantial results. Long before such\nan initiative would bear fruit, advances in biotechnology will allow much more\ndirect control of human genetics and neurobiology, rendering otiose any human\nbreeding program. We will therefore focus on methods that hold the potential\nto deliver results faster, on the timescale of a few generations or less.\n\nOur individual cognitive capacities can be strengthened in various ways,\nincluding by such traditional methods as education and training. Neurological\ndevelopment can be promoted by low-tech interventions such as optimizing\nmaternal and infant nutrition, removing lead and other neurotoxic pollutants\nfrom the environment, eradicating parasites, ensuring adequate sleep and\nexercise, and preventing diseases that affect the\nbrain.[33](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos943195)\nImprovements in cognition can certainly be obtained through each of these\nmeans, though the magnitudes of the gains are likely to be modest, especially\nin populations that are already reasonably well-nourished and -schooled. We\nwill certainly not achieve superintelligence by any of these means, but they\nmight help on the margin, particularly by lifting up the deprived and\nexpanding the catchment of global talent. (Lifelong depression of intelligence\ndue to iodine deficiency remains widespread in many impoverished inland areas\nof the world—an outrage given that the condition can be prevented by\nfortifying table salt at a cost of a few cents per person and\nyear.[34](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos943405))\n\nBiomedical enhancements could give bigger boosts. Drugs already exist that are\nalleged to improve memory, concentration, and mental energy in at least some\nsubjects.[35](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos943910)\n(Work on this book was fueled by coffee and nicotine chewing gum.) While the\nefficacy of the present generation of smart drugs is variable, marginal, and\ngenerally dubious, future nootropics might offer clearer benefits and fewer\nside\neffects.[36](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos944034)\nHowever, it seems implausible, on both neurological and evolutionary grounds,\nthat one could by introducing some chemical into the brain of a healthy person\nspark a dramatic rise in\nintelligence.[37](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos944722)\nThe cognitive functioning of a human brain depends on a delicate orchestration\nof many factors, especially during the critical stages of embryo\ndevelopment—and it is much more likely that this self-organizing structure, to\nbe enhanced, needs to be carefully balanced, tuned, and cultivated rather than\nsimply flooded with some extraneous potion.\n\nManipulation of genetics will provide a more powerful set of tools than\npsychopharmacology. Consider again the idea of genetic selection: instead of\ntrying to implement a eugenics program by controlling mating patterns, one\ncould use selection at the level of embryos or\ngametes.[38](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos945345)\nPre-implantation genetic diagnosis has already been used during in vitro\nfertilization procedures to screen embryos produced for monogenic disorders\nsuch as Huntington’s disease and for predisposition to some late-onset\ndiseases such as breast cancer. It has also been used for sex selection and\nfor matching human leukocyte antigen type with that of a sick sibling, who can\nthen benefit from a cord-blood stem cell donation when the new baby is\nborn.[39](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos945943) The\nrange of traits that can be selected for or against will expand greatly over\nthe next decade or two. A strong driver of progress in behavioral genetics is\nthe rapidly falling cost of genotyping and gene sequencing. Genome-wide\ncomplex trait analysis, using studies with vast numbers of subjects, is just\nnow starting to become feasible and will greatly increase our knowledge of the\ngenetic architectures of human cognitive and behavioral\ntraits.[40](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos946535)\nAny trait with a non-negligible heritability—including cognitive\ncapacity—could then become susceptible to\nselection.[41](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos946733)\nEmbryo selection does not require a deep understanding of the causal pathways\nby which genes, in complicated interplay with environments, produce\nphenotypes: it requires only (lots of) data on the genetic correlates of the\ntraits of interest.\n\nIt is possible to calculate some rough estimates of the magnitude of the gains\nobtainable in different selection\nscenarios.[42](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos947658)\nTable 5 shows expected increases in intelligence resulting from various\namounts of selection, assuming complete information about the common additive\ngenetic variants underlying the narrow-sense heritability of intelligence.\n(With partial information, the effectiveness of selection would be reduced,\nthough not quite to the extent one might naively\nexpect.[44](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos948746))\nUnsurprisingly, selecting between larger numbers of embryos produces larger\ngains, but there are steeply diminishing returns: selection between 100\nembryos does not produce a gain anywhere near fifty times as large as that\nwhich one would get from selection between 2\nembryos.[45](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos949567)\n\n**Table 5 _Maximum IQ gains from selecting among a set of\nembryos_[43](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos947858)**\n\n|  \n---|---  \n**Selection** | **IQ points gained**  \n1 in 2 | 4.2  \n1 in 10 | 11.5  \n1 in 100 | 18.8  \n1 in 1000 | 24.3  \n5 generations of 1 in 10 | < 65 (b/c diminishing returns)  \n10 generations of 1 in 10 | < 130 (b/c diminishing returns)  \nCumulative limits (additive variants optimized for cognition) | 100 + (< 300 (b/c diminishing returns))  \n  \nInterestingly, the diminishment of returns is greatly abated when the\nselection is spread over multiple generations. Thus, repeatedly selecting the\ntop 1 in 10 over ten generations (where each new generation consists of the\noffspring of those selected in the previous generation) will produce a much\ngreater increase in the trait value than a one-off selection of 1 in 100. The\nproblem with sequential selection, of course, is that it takes longer. If each\ngenerational step takes twenty or thirty years, then even just five successive\ngenerations would push us well into the twenty-second century. Long before\nthen, more direct and powerful modes of genetic engineering (not to mention\nmachine intelligence) will most likely be available.\n\nThere is, however, a complementary technology, one which, once it has been\ndeveloped for use in humans, would greatly potentiate the enhancement power of\npre-implantation genetic screening: namely, the derivation of viable sperm and\neggs from embryonic stem\ncells.[46](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos949750) The\ntechniques for this have already been used to produce fertile offspring in\nmice and gamete-like cells in humans. Substantial scientific challenges\nremain, however, in translating the animal results to humans and in avoiding\nepigenetic abnormalities in the derived stem cell lines. According to one\nexpert, these challenges might put human application “10 or even 50 years in\nthe\nfuture.”[47](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos950193)\n\nWith stem cell-derived gametes, the amount of selection power available to a\ncouple could be greatly increased. In current practice, an in vitro\nfertilization procedure typically involves the creation of fewer than ten\nembryos. With stem cell-derived gametes, a few donated cells might be turned\ninto a virtually unlimited number of gametes that could be combined to produce\nembryos, which could then be genotyped or sequenced, and the most promising\none chosen for implantation. Depending on the cost of preparing and screening\neach individual embryo, this technology could yield a severalfold increase in\nthe selective power available to couples using in vitro fertilization.\n\nMore importantly still, stem cell-derived gametes would allow multiple\ngenerations of selection to be compressed into less than a human maturation\nperiod, by enabling _iterated embryo selection_. This is a procedure that\nwould consist of the following\nsteps:[48](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos950644)\n\n> **1** Genotype and select a number of embryos that are higher in desired\n> genetic characteristics.\n\n> **2** Extract stem cells from those embryos and convert them to sperm and\n> ova, maturing within six months or\n> less.[49](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos950798)\n\n> **3** Cross the new sperm and ova to produce embryos.\n\n> **4** Repeat until large genetic changes have been accumulated.\n\nIn this manner, it would be possible to accomplish ten or more generations of\nselection in just a few years. (The procedure would be time-consuming and\nexpensive; however, in principle, it would need to be done only once rather\nthan repeated for each birth. The cell lines established at the end of the\nprocedure could be used to generate very large numbers of enhanced embryos.)\n\nAs Table 5 indicates, the _average_ level of intelligence among individuals\nconceived in this manner could be very high, possibly equal to or somewhat\nabove that of the most intelligent individual in the historical human\npopulation. A world that had a large population of such individuals might (if\nit had the culture, education, communications infrastructure, etc., to match)\nconstitute a collective superintelligence.\n\nThe impact of this technology will be dampened and delayed by several factors.\nThere is the unavoidable maturational lag while the finally selected embryos\ngrow into adult human beings: at least twenty years before an enhanced child\nreaches full productivity, longer still before such children come to\nconstitute a substantial segment of the labor force. Furthermore, even after\nthe technology has been perfected, adoption rates will probably start out low.\nSome countries might prohibit its use altogether, on moral or religious\ngrounds.[50](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos950908)\nEven where selection is allowed, many couples will prefer the natural way of\nconceiving. Willingness to use IVF, however, would increase if there were\nclearer benefits associated with the procedure—such as a virtual guarantee\nthat the child would be highly talented and free from genetic predispositions\nto disease. Lower health care costs and higher expected lifetime earnings\nwould also argue in favor of genetic selection. As use of the procedure\nbecomes more common, particularly among social elites, there might be a\ncultural shift toward parenting norms that present the use of selection as the\nthing that responsible enlightened couples do. Many of the initially reluctant\nmight join the bandwagon in order to have a child that is not at a\ndisadvantage relative to the enhanced children of their friends and\ncolleagues. Some countries might offer inducements to encourage their citizens\nto take advantage of genetic selection in order to increase the country’s\nstock of human capital, or to increase long-term social stability by selecting\nfor traits like docility, obedience, submissiveness, conformity, risk-\naversion, or cowardice, outside of the ruling clan.\n\nEffects on intellectual capacity would also depend on the extent to which the\navailable selection power would be used for enhancing cognitive traits (Table\n6). Those who do opt to use some form of embryo selection would have to choose\nhow to allocate the selection power at their disposal, and intelligence would\nto some extent be in competition with other desired attributes, such as\nhealth, beauty, personality, or athleticism. Iterated embryo selection, by\noffering such a large amount of selection power, would alleviate some of these\ntradeoffs, enabling simultaneous strong selection for multiple traits.\nHowever, this procedure would tend to disrupt the normal genetic relationship\nbetween parents and child, something that could negatively affect demand in\nmany\ncultures.[51](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos951743)\n\nWith further advances in genetic technology, it may become possible to\nsynthesize genomes to specification, obviating the need for large pools of\nembryos. DNA synthesis is already a routine and largely automated\nbiotechnology, though it is not yet feasible to synthesize an entire human\ngenome that could be used in a reproductive context (not least because of\nstill-unresolved difficulties in getting the epigenetics\nright).[54](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos952429)\nBut once this technology has matured, an embryo could be designed with the\nexact preferred combination of genetic inputs from each parent. Genes that are\npresent in neither of the parents could also be spliced in, including alleles\nthat are present with low frequency in the population but which may have\nsignificant positive effects on\ncognition.[55](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos952634)\n\n**Table 6 _Possible impacts from genetic selection in different\nscenarios_[52](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos952200)**\n\n![Image](images/00011.jpg)\n\nOne intervention that becomes possible when human genomes can be synthesized\nis genetic “spell-checking” of an embryo. (Iterated embryo selection might\nalso allow an approximation of this.) Each of us currently carries a\nmutational load, with perhaps hundreds of mutations that reduce the efficiency\nof various cellular\nprocesses.[56](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos953522)\nEach individual mutation has an almost negligible effect (whence it is only\nslowly removed from the gene pool), yet in combination such mutations may\nexact a heavy toll on our\nfunctioning.[57](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos954138)\nIndividual differences in intelligence might to a significant extent be\nattributable to variations in the number and nature of such slightly\ndeleterious alleles that each of us carries. With gene synthesis we could take\nthe genome of an embryo and construct a version of that genome free from the\ngenetic noise of accumulated mutations. If one wished to speak provocatively,\none could say that individuals created from such proofread genomes might be\n“more human” than anybody currently alive, in that they would be less\ndistorted expressions of human form. Such people would not all be carbon\ncopies, because humans vary genetically in ways other than by carrying\ndifferent deleterious mutations. But the phenotypical manifestation of a\nproofread genome may be an exceptional physical and mental constitution, with\nelevated functioning in polygenic trait dimensions like intelligence, health,\nhardiness, and\nappearance.[58](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos954259)\n(A loose analogy could be made with composite faces, in which the defects of\nthe superimposed individuals are averaged out: see Figure 6.)\n\n![Image](images/00012.jpg)\n\n**Figure 6** Composite faces as a metaphor for spell-checked genomes. Each of\nthe central pictures was produced by superimposing photographs of sixteen\ndifferent individuals (residents of Tel Aviv). Composite faces are often\njudged to be more beautiful than any of the individual faces of which they are\ncomposed, as idiosyncratic imperfections are averaged out. Analogously, by\nremoving individual mutations, proofread genomes may produce people closer to\n“Platonic ideals.” Such individuals would not all be genetically identical,\nbecause many genes come in multiple equally functional alleles. Proofreading\nwould only eliminate variance arising from deleterious\nmutations.[59](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos954772)\n\nOther potential biotechnological techniques might also be relevant. Human\nreproductive cloning, once achieved, could be used to replicate the genome of\nexceptionally talented individuals. Uptake would be limited by the preference\nof most prospective parents to be biologically related to their children, yet\nthe practice could nevertheless come to have non-negligible impact because (1)\neven a relatively small increase in the number of exceptionally talented\npeople might have a significant effect; and (2) it is possible that some state\nwould embark on a larger-scale eugenics program, perhaps by paying surrogate\nmothers. Other kinds of genetic engineering—such as the design of novel\nsynthetic genes or insertion into the genome of promoter regions and other\nelements to control gene expression—might also become important over time.\nEven more exotic possibilities may exist, such as vats full of complexly\nstructured cultured cortical tissue, or “uplifted” transgenic animals (perhaps\nsome large-brained mammal such as the whale or elephant, enriched with human\ngenes). These latter ones are wholly speculative, but over a longer time frame\nthey perhaps cannot be completely discounted.\n\nSo far we have discussed germline interventions, ones that would be done on\ngametes or embryos. Somatic gene enhancements, by bypassing the generation\ncycle, could in principle produce impacts more quickly. However, they are\ntechnologically much more challenging. They require that the modified genes be\ninserted into a large number of cells in the living body—including, in the\ncase of cognitive enhancement, the brain. Selecting among existing egg cells\nor embryos, in contrast, requires no gene insertion. Even such germline\ntherapies as do involve modifying the genome (such as proofreading the genome\nor splicing in rare alleles) are far easier to implement at the gamete or the\nembryo stage, where one is dealing with a small number of cells. Furthermore,\ngermline interventions on embryos can probably achieve greater effects than\nsomatic interventions on adults, because the former would be able to shape\nearly brain development whereas the latter would be limited to tweaking an\nexisting structure. (Some of what could be done through somatic gene therapy\nmight also be achievable by pharmacological means.)\n\nFocusing therefore on germline interventions, we must take into account the\ngenerational lag delaying any large impact on the\nworld.[60](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos954975)\nEven if the technology were perfected today and immediately put to use, it\nwould take more than two decades for a genetically enhanced brood to reach\nmaturity. Furthermore, with human applications there is normally a delay of at\nleast one decade between proof of concept in the laboratory and clinical\napplication, because of the need for extensive studies to determine safety.\nThe simplest forms of genetic selection, however, could largely abrogate the\nneed for such testing, since they would use standard fertility treatment\ntechniques and genetic information to choose between embryos that might\notherwise have been selected by chance.\n\nDelays could also result from obstacles rooted not in a fear of failure\n(demand for safety testing) but in fear of success—demand for regulation\ndriven by concerns about the moral permissibility of genetic selection or its\nwider social implications. Such concerns are likely to be more influential in\nsome countries than in others, owing to differing cultural, historical, and\nreligious contexts. Post-war Germany, for example, has chosen to give a wide\nberth to any reproductive practices that could be perceived to be even in the\nremotest way aimed at enhancement, a stance that is understandable given the\nparticularly dark history of atrocities connected to the eugenics movement in\nthat country. Other Western countries are likely to take a more liberal\napproach. And some countries—perhaps China or Singapore, both of which have\nlong-term population policies—might not only permit but actively promote the\nuse of genetic selection and genetic engineering to enhance the intelligence\nof their populations once the technology to do so is available.\n\nOnce the example has been set, and the results start to show, holdouts will\nhave strong incentives to follow suit. Nations would face the prospect of\nbecoming cognitive backwaters and losing out in economic, scientific,\nmilitary, and prestige contests with competitors that embrace the new human\nenhancement technologies. Individuals within a society would see places at\nelite schools being filled with genetically selected children (who may also on\naverage be prettier, healthier, and more conscientious) and will want their\nown offspring to have the same advantages. There is some chance that a large\nattitudinal shift could take place over a relatively short time, perhaps in as\nlittle as a decade, once the technology is proven to work and to provide a\nsubstantial benefit. Opinion surveys in the United States reveal a dramatic\nshift in public approval of in vitro fertilization after the birth of the\nfirst “test tube baby,” Louise Brown, in 1978. A few years earlier, only 18%\nof Americans said they would personally use IVF to treat infertility; yet in a\npoll taken shortly after the birth of Louise Brown, 53% said they would do so,\nand the number has continued to\nrise.[61](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos955195) (For\ncomparison, in a poll taken in 2004, 28% of Americans approved of embryo\nselection for “strength or intelligence,” 58% approved of it for avoiding\nadult-onset cancer, and 68% approved of it to avoid fatal childhood\ndisease.[62](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos955341))\n\nIf we add up the various delays—say five to ten years to gather the\ninformation needed for significantly effective selection among a set of IVF\nembryos (possibly much longer before stem cell-derived gametes are available\nfor use in human reproduction), ten years to build significant uptake, and\ntwenty to twenty-five years for the enhanced generation to reach an age where\nthey start becoming productive, we find that germline enhancements are\nunlikely to have a significant impact on society before the middle of this\ncentury. From that point onward, however, the intelligence of significant\nsegments of the adult population may begin to be boosted by genetic\nenhancements. The speed of the ascent would then greatly accelerate as cohorts\nconceived using more powerful next-generation genetic technologies (in\nparticular stem cell-derived gametes and iterative embryo selection) enter the\nlabor force.\n\nWith the full development of the genetic technologies described above (setting\naside the more exotic possibilities such as intelligence in cultured neural\ntissue), it might be possible to ensure that new individuals are on average\nsmarter than any human who has yet existed, with peaks that rise higher still.\nThe potential of biological enhancement is thus ultimately high, probably\nsufficient for the attainment of at least weak forms of superintelligence.\nThis should not be surprising. After all, dumb evolutionary processes have\ndramatically amplified the intelligence in the human lineage even compared\nwith our close relatives the great apes and our own humanoid ancestors; and\nthere is no reason to suppose _Homo sapiens_ to have reached the apex of\ncognitive effectiveness attainable in a biological system. Far from being the\nsmartest possible biological species, we are probably better thought of as the\nstupidest possible biological species capable of starting a technological\ncivilization—a niche we filled because we got there first, not because we are\nin any sense optimally adapted to it.\n\nProgress along the biological path is clearly feasible. The generational lag\nin germline interventions means that progress could not be nearly as sudden\nand abrupt as in scenarios involving machine intelligence. (Somatic gene\ntherapies and pharmacological interventions could theoretically skip the\ngenerational lag, but they seem harder to perfect and are less likely to\nproduce dramatic effects.) The _ultimate_ potential of machine intelligence\nis, of course, vastly greater than that of organic intelligence. (One can get\nsome sense of the magnitude of the gap by considering the speed differential\nbetween electronic components and nerve cells: even today’s transistors\noperate on a timescale ten million times shorter than that of biological\nneurons.) However, even comparatively moderate enhancements of biological\ncognition could have important consequences. In particular, cognitive\nenhancement could accelerate science and technology, including progress toward\nmore potent forms of biological intelligence amplification and machine\nintelligence. Consider how the rate of progress in the field of artificial\nintelligence would change in a world where Average Joe is an intellectual peer\nof Alan Turing or John von Neumann, and where millions of people tower far\nabove any intellectual giant of the\npast.[63](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos955460)\n\nA discussion of the strategic implications of cognitive enhancement will have\nto await a later chapter. But we can summarize this section by noting three\nconclusions: (1) at least weak forms of superintelligence are achievable by\nmeans of biotechnological enhancements; (2) the feasibility of cognitively\nenhanced humans adds to the plausibility that advanced forms of machine\nintelligence are feasible—because even if _we_ were fundamentally unable to\ncreate machine intelligence (which there is no reason to suppose), machine\nintelligence might still be within reach of cognitively enhanced humans; and\n(3) when we consider scenarios stretching significantly into the second half\nof this century and beyond, we must take into account the probable emergence\nof a generation of genetically enhanced populations—voters, inventors,\nscientists—with the magnitude of enhancement escalating rapidly over\nsubsequent decades.\n\n### [Brain–computer\ninterfaces](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_006.html#filepos16281)\n\nIt is sometimes proposed that direct brain–computer interfaces, particularly\nimplants, could enable humans to exploit the fortes of digital\ncomputing—perfect recall, speedy and accurate arithmetic calculation, and\nhigh-bandwidth data transmission—enabling the resulting hybrid system to\nradically outperform the unaugmented\nbrain.[64](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos956581) But\nalthough the possibility of direct connections between human brains and\ncomputers has been demonstrated, it seems unlikely that such interfaces will\nbe widely used as enhancements any time\nsoon.[65](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos957368)\n\nTo begin with, there are significant risks of medical complications—including\ninfections, electrode displacement, hemorrhage, and cognitive decline—when\nimplanting electrodes in the brain. Perhaps the most vivid illustration to\ndate of the benefits that can be obtained through brain stimulation is the\ntreatment of patients with Parkinson’s disease. The Parkinson’s implant is\nrelatively simple: it does not really communicate with the brain but simply\nsupplies a stimulating electric current to the subthalamic nucleus. A\ndemonstration video shows a subject slumped in a chair, completely immobilized\nby the disease, then suddenly springing to life when the current is switched\non: the subject now moves his arms, stands up and walks across the room, turns\naround and performs a pirouette. Yet even behind this especially simple and\nalmost miraculously successful procedure, there lurk negatives. One study of\nParkinson patients who had received deep brain implants showed reductions in\nverbal fluency, selective attention, color naming, and verbal memory compared\nwith controls. Treated subjects also reported more cognitive\ncomplaints.[66](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos957691)\nSuch risks and side effects might be tolerable if the procedure is used to\nalleviate severe disability. But in order for healthy subjects to volunteer\nthemselves for neurosurgery, there would have to be some very substantial\nenhancement of normal functionality to be gained.\n\nThis brings us to the second reason to doubt that superintelligence will be\nachieved through cyborgization, namely that enhancement is likely to be far\nmore difficult than therapy. Patients who suffer from paralysis might benefit\nfrom an implant that replaces their severed nerves or activates spinal motion\npattern\ngenerators.[67](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos957808)\nPatients who are deaf or blind might benefit from artificial cochleae and\nretinas.[68](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos957924)\nPatients with Parkinson’s disease or chronic pain might benefit from deep\nbrain stimulation that excites or inhibits activity in a particular area of\nthe brain.[69](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos958051)\nWhat seems far more difficult to achieve is a high-bandwidth direct\ninteraction between brain and computer to provide substantial increases in\nintelligence of a form that could not be more readily attained by other means.\nMost of the potential benefits that brain implants could provide in healthy\nsubjects could be obtained at far less risk, expense, and inconvenience by\nusing our regular motor and sensory organs to interact with computers located\noutside of our bodies. We do not need to plug a fiber optic cable into our\nbrains in order to access the Internet. Not only can the human retina transmit\ndata at an impressive rate of nearly 10 million bits per second, but it comes\npre-packaged with a massive amount of dedicated wetware, the visual cortex,\nthat is highly adapted to extracting meaning from this information torrent and\nto interfacing with other brain areas for further\nprocessing.[70](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos958187)\nEven if there were an easy way of pumping more information into our brains,\nthe extra data inflow would do little to increase the rate at which we think\nand learn unless all the neural machinery necessary for making sense of the\ndata were similarly upgraded. Since this includes almost all of the brain,\nwhat would really be needed is a “whole brain prosthesis–—which is just\nanother way of saying artificial general intelligence. Yet if one had a human-\nlevel AI, one could dispense with neurosurgery: a computer might as well have\na metal casing as one of bone. So this limiting case just takes us back to the\nAI path, which we have already examined.\n\nBrain–computer interfacing has also been proposed as a way to get information\nout of the brain, for purposes of communicating with other brains or with\nmachines.[71](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos958301)\nSuch uplinks have helped patients with locked-in syndrome to communicate with\nthe outside world by enabling them to move a cursor on a screen by\nthought.[72](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos958575)\nThe bandwidth attained in such experiments is low: the patient painstakingly\ntypes out one slow letter after another at a rate of a few words per minute.\nOne can readily imagine improved versions of this technology—perhaps a next-\ngeneration implant could plug into Broca’s area (a region in the frontal lobe\ninvolved in language production) and pick up internal\nspeech.[73](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos958819)\nBut whilst such a technology might assist some people with disabilities\ninduced by stroke or muscular degeneration, it would hold little appeal for\nhealthy subjects. The functionality it would provide is essentially that of a\nmicrophone coupled with speech recognition software, which is already\ncommercially available—minus the pain, inconvenience, expense, and risks\nassociated with neurosurgery (and minus at least some of the hyper-Orwellian\novertones of an intracranial listening device). Keeping our machines outside\nof our bodies also makes upgrading easier.\n\nBut what about the dream of bypassing words altogether and establishing a\nconnection between two brains that enables concepts, thoughts, or entire areas\nof expertise to be “downloaded” from one mind to another? We can download\nlarge files to our computers, including libraries with millions of books and\narticles, and this can be done over the course of seconds: could something\nsimilar be done with our brains? The apparent plausibility of this idea\nprobably derives from an incorrect view of how information is stored and\nrepresented in the brain. As noted, the rate-limiting step in human\nintelligence is not how fast raw data can be fed into the brain but rather how\nquickly the brain can extract meaning and make sense of the data. Perhaps it\nwill be suggested that we transmit meanings directly, rather than package them\ninto sensory data that must be decoded by the recipient. There are two\nproblems with this. The first is that brains, by contrast to the kinds of\nprogram we typically run on our computers, do not use standardized data\nstorage and representation formats. Rather, each brain develops its own\nidiosyncratic representations of higher-level content. Which particular\nneuronal assemblies are recruited to represent a particular concept depends on\nthe unique experiences of the brain in question (along with various genetic\nfactors and stochastic physiological processes). Just as in artificial neural\nnets, meaning in biological neural networks is likely represented holistically\nin the structure and activity patterns of sizeable overlapping regions, not in\ndiscrete memory cells laid out in neat\narrays.[74](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos958939) It\nwould therefore not be possible to establish a simple mapping between the\nneurons in one brain and those in another in such a way that thoughts could\nautomatically slide over from one to the other. In order for the thoughts of\none brain to be intelligible to another, the thoughts need to be decomposed\nand packaged into symbols according to some shared convention that allows the\nsymbols to be correctly interpreted by the receiving brain. This is the job of\nlanguage.\n\n_In principle_ , one could imagine offloading the cognitive work of\narticulation and interpretation to an interface that would somehow read out\nthe neural states in the sender’s brain and somehow feed in a bespoke pattern\nof activation to the receiver’s brain. But this brings us to the second\nproblem with the cyborg scenario. Even setting aside the (quite immense)\ntechnical challenge of how to reliably read and write simultaneously from\nperhaps billions of individually addressable neurons, creating the requisite\ninterface is probably an AI-complete problem. The interface would need to\ninclude a component able (in real-time) to map firing patterns in one brain\nonto semantically equivalent firing patterns in the other brain. The detailed\nmultilevel understanding of the neural computation needed to accomplish such a\ntask would seem to directly enable neuromorphic AI.\n\nDespite these reservations, the cyborg route toward cognitive enhancement is\nnot entirely without promise. Impressive work on the rat hippocampus has\ndemonstrated the feasibility of a neural prosthesis that can enhance\nperformance in a simple working-memory\ntask.[75](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos959347) In\nits present version, the implant collects input from a dozen or two electrodes\nlocated in one area (“CA3”) of the hippocampus and projects onto a similar\nnumber of neurons in another area (“CA1”). A microprocessor is trained to\ndiscriminate between two different firing patterns in the first area\n(corresponding to two different memories, “right lever” or “left lever”) and\nto learn how these patterns are projected into the second area. This\nprosthesis can not only restore function when the normal neural connection\nbetween the two neural areas is blockaded, but by sending an especially clear\ntoken of a particular memory pattern to the second area it can enhance the\nperformance on the memory task beyond what the rat is normally capable of.\nWhile a technical tour de force by contemporary standards, the study leaves\nmany challenging questions unanswered: How well does the approach scale to\ngreater numbers of memories? How well can we control the combinatorial\nexplosion that otherwise threatens to make learning the correct mapping\ninfeasible as the number of input and output neurons is increased? Does the\nenhanced performance on the test task come at some hidden cost, such as\nreduced ability to generalize from the particular stimulus used in the\nexperiment, or reduced ability to unlearn the association when the environment\nchanges? Would the test subjects still somehow benefit even if—unlike\nrats—they could avail themselves of external memory aids such as pen and\npaper? And how much harder would it be to apply a similar method to other\nparts of the brain? Whereas the present prosthesis takes advantage of the\nrelatively simple feed-forward structure of parts of the hippocampus\n(basically serving as a unidirectional bridge between areas CA3 and CA1),\nother structures in the cortex involve convoluted feedback loops which greatly\nincrease the complexity of the wiring diagram and, presumably, the difficulty\nof deciphering the functionality of any embedded group of neurons.\n\nOne hope for the cyborg route is that the brain, if permanently implanted with\na device connecting it to some external resource, would over time _learn_ an\neffective mapping between its own internal cognitive states and the inputs it\nreceives from, or the outputs accepted by, the device. Then the implant itself\nwould not need to be intelligent; rather, the brain would intelligently adapt\nto the interface, much as the brain of an infant gradually learns to interpret\nthe signals arriving from receptors in its eyes and\nears.[76](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos959486) But\nhere again one must question how much would really be gained. Suppose that the\nbrain’s plasticity were such that it could learn to detect patterns in some\nnew input stream arbitrary projected onto some part of the cortex by means of\na brain–computer interface: why not project the same information onto the\nretina instead, as a visual pattern, or onto the cochlea as sounds? The low-\ntech alternative avoids a thousand complications, and in either case the brain\ncould deploy its pattern-recognition mechanisms and plasticity to learn to\nmake sense of the information.\n\n### [Networks and\norganizations](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_006.html#filepos16387)\n\nAnother conceivable path to superintelligence is through the gradual\nenhancement of networks and organizations that link individual human minds\nwith one another and with various artifacts and bots. The idea here is not\nthat this would enhance the intellectual capacity of individuals enough to\nmake them superintelligent, but rather that some system composed of\nindividuals thus networked and organized might attain a form of\nsuperintelligence—what in the next chapter we will elaborate as “collective\nsuperintelligence.”[77](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos959824)\n\nHumanity has gained enormously in collective intelligence over the course of\nhistory and prehistory. The gains come from many sources, including\ninnovations in communications technology, such as writing and printing, and\nabove all the introduction of language itself; increases in the size of the\nworld population and the density of habitation; various improvements in\norganizational techniques and epistemic norms; and a gradual accumulation of\ninstitutional capital. In general terms, a system’s collective intelligence is\nlimited by the abilities of its member minds, the overheads in communicating\nrelevant information between them, and the various distortions and\ninefficiencies that pervade human organizations. If communication overheads\nare reduced (including not only equipment costs but also response latencies,\ntime and attention burdens, and other factors), then larger and more densely\nconnected organizations become feasible. The same could happen if fixes are\nfound for some of the bureaucratic deformations that warp organizational\nlife—wasteful status games, mission creep, concealment or falsification of\ninformation, and other agency problems. Even partial solutions to these\nproblems could pay hefty dividends for collective intelligence.\n\nThe technological and institutional innovations that could contribute to the\ngrowth of our collective intelligence are many and various. For example,\nsubsidized prediction markets might foster truth-seeking norms and improve\nforecasting on contentious scientific and social\nissues.[78](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos960425)\nLie detectors (should it prove feasible to make ones that are reliable and\neasy to use) could reduce the scope for deception in human\naffairs.[79](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos960563)\nSelf-deception detectors might be even more\npowerful.[80](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos961179)\nEven without newfangled brain technologies, some forms of deception might\nbecome harder to practice thanks to increased availability of many kinds of\ndata, including reputations and track records, or the promulgation of strong\nepistemic norms and rationality culture. Voluntary and involuntary\nsurveillance will amass vast amounts of information about human behavior.\nSocial networking sites are already used by over a billion people to share\npersonal details: soon, these people might begin uploading continuous life\nrecordings from microphones and video cameras embedded in their smart phones\nor eyeglass frames. Automated analysis of such data streams will enable many\nnew applications (sinister as well as benign, of\ncourse).[81](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos961644)\n\nGrowth in collective intelligence may also come from more general\norganizational and economic improvements, and from enlarging the fraction of\nthe world’s population that is educated, digitally connected, and integrated\ninto global intellectual\nculture.[82](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos961995)\n\nThe Internet stands out as a particularly dynamic frontier for innovation and\nexperimentation. Most of its potential may still remain unexploited.\nContinuing development of an intelligent Web, with better support for\ndeliberation, de-biasing, and judgment aggregation, might make large\ncontributions to increasing the collective intelligence of humanity as a whole\nor of particular groups.\n\nBut what of the seemingly more fanciful idea that the Internet might one day\n“wake up”? Could the Internet become something more than just the backbone of\na loosely integrated collective superintelligence—something more like a\nvirtual skull housing an emerging unified super-intellect? (This was one of\nthe ways that superintelligence could arise according to Vernor Vinge’s\ninfluential 1993 essay, which coined the term “technological\nsingularity.”[83](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos962488))\nAgainst this one could object that machine intelligence is hard enough to\nachieve through arduous engineering, and that it is incredible to suppose that\nit will arise _spontaneously_. However, the story need not be that some future\nversion of the Internet suddenly becomes superintelligent by mere\nhappenstance. A more plausible version of the scenario would be that the\nInternet accumulates improvements through the work of many people over many\nyears—work to engineer better search and information filtering algorithms,\nmore powerful data representation formats, more capable autonomous software\nagents, and more efficient protocols governing the interactions between such\nbots—and that myriad incremental improvements eventually create the basis for\nsome more unified form of web intelligence. It seems at least conceivable that\nsuch a web-based cognitive system, supersaturated with computer power and all\nother resources needed for explosive growth save for one crucial ingredient,\ncould, when the final missing constituent is dropped into the cauldron, blaze\nup with superintelligence. This type of scenario, though, converges into\nanother possible path to superintelligence, that of artificial general\nintelligence, which we have already discussed.\n\n### [Summary](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_006.html#filepos16489)\n\nThe fact that there are many paths that lead to superintelligence should\nincrease our confidence that we will eventually get there. If one path turns\nout to be blocked, we can still progress.\n\nThat there are multiple paths does not entail that there are multiple\ndestinations. Even if significant intelligence amplification were first\nachieved along one of the non-machine-intelligence paths, this would not\nrender machine intelligence irrelevant. Quite the contrary: enhanced\nbiological or organizational intelligence would accelerate scientific and\ntechnological developments, potentially hastening the arrival of more radical\nforms of intelligence amplification such as whole brain emulation and AI.\n\nThis is not to say that it is a matter of indifference how we get to machine\nsuperintelligence. The path taken to get there could make a big difference to\nthe eventual outcome. Even if the ultimate capabilities that are obtained do\nnot depend much on the trajectory, how those capabilities will be used—how\nmuch control we humans have over their disposition—might well depend on\ndetails of our approach. For example, enhancements of biological or\norganizational intelligence might increase our ability to anticipate risk and\nto design machine superintelligence that is safe and beneficial. (A full\nstrategic assessment involves many complexities, and will have to await\n[Chapter 14](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos795733).)\n\nTrue superintelligence (as opposed to marginal increases in current levels of\nintelligence) might plausibly first be attained via the AI path. There are,\nhowever, many fundamental uncertainties along this path. This makes it\ndifficult to rigorously assess how long the path is or how many obstacles\nthere are along the way. The whole brain emulation path also has some chance\nof being the quickest route to superintelligence. Since progress along this\npath requires mainly incremental technological advances rather than\ntheoretical breakthroughs, a strong case can be made that it will eventually\nsucceed. It seems fairly likely, however, that even if progress along the\nwhole brain emulation path is swift, artificial intelligence will nevertheless\nbe first to cross the finishing line: this is because of the possibility of\nneuromorphic AIs based on partial emulations.\n\nBiological cognitive enhancements are clearly feasible, particularly ones\nbased on genetic selection. Iterated embryo selection currently seems like an\nespecially promising technology. Compared with possible breakthroughs in\nmachine intelligence, however, biological enhancements would be relatively\nslow and gradual. They would, at best, result in relatively weak forms of\nsuperintelligence (more on this shortly).\n\nThe clear feasibility of biological enhancement should increase our confidence\nthat machine intelligence is ultimately achievable, since enhanced human\nscientists and engineers will be able to make more and faster progress than\ntheir _au naturel_ counterparts. Especially in scenarios in which machine\nintelligence is delayed beyond mid-century, the increasingly cognitively\nenhanced cohorts coming onstage will play a growing role in subsequent\ndevelopments.\n\nBrain–computer interfaces look unlikely as a source of superintelligence.\nImprovements in networks and organizations might result in weakly\nsuperintelligent forms of collective intelligence in the long run; but more\nlikely, they will play an enabling role similar to that of biological\ncognitive enhancement, gradually increasing humanity’s effective ability to\nsolve intellectual problems. Compared with biological enhancements, advances\nin networks and organization will make a difference sooner—in fact, such\nadvances are occurring continuously and are having a significant impact\nalready. However, improvements in networks and organizations may yield\nnarrower increases in our problem-solving capacity than will improvements in\nbiological cognition—boosting “collective intelligence” rather than “quality\nintelligence,” to anticipate a distinction we are about to introduce in the\nnext chapter.\n\n\n## [CHAPTER 3  \nForms of\nsuperintelligence](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_006.html#filepos16585)\n\n**So what, exactly, do we mean by “superintelligence”? While we do not wish to\nget bogged down in terminological swamps, something needs to be said to\nclarify the conceptual ground. This chapter identifies three different forms\nof superintelligence, and argues that they are, in a practically relevant\nsense, equivalent. We also show that the potential for intelligence in a\nmachine substrate is vastly greater than in a biological substrate. Machines\nhave a number of fundamental advantages which will give them overwhelming\nsuperiority. Biological humans, even if enhanced, will be outclassed.**\n\nMany machines and nonhuman animals already perform at superhuman levels in\nnarrow domains. Bats interpret sonar signals better than man, calculators\noutperform us in arithmetic, and chess programs beat us in chess. The range of\nspecific tasks that can be better performed by software will continue to\nexpand. But although specialized information processing systems will have many\nuses, there are additional profound issues that arise only with the prospect\nof machine intellects that have enough general intelligence to substitute for\nhumans across the board.\n\nAs previously indicated, we use the term “superintelligence” to refer to\nintellects that greatly outperform the best current human minds across many\nvery general cognitive domains. This is still quite vague. Different kinds of\nsystem with rather disparate performance attributes could qualify as\nsuperintelligences under this definition. To advance the analysis, it is\nhelpful to disaggregate this simple notion of superintelligence by\ndistinguishing different bundles of intellectual super-capabilities. There are\nmany ways in which such decomposition could be done. Here we will\ndifferentiate between three forms: speed superintelligence, collective\nsuperintelligence, and quality superintelligence.\n\n### [Speed\nsuperintelligence](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_006.html#filepos16701)\n\nA speed superintelligence is an intellect that is just like a human mind but\nfaster. This is conceptually the easiest form of superintelligence to\nanalyze.[1](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos962705) We\ncan define speed superintelligence as follows:\n\n> **Speed superintelligence:** _A system that can do all that a human\n> intellect can do, but much faster_.\n\nBy “much” we here mean something like “multiple orders of magnitude.” But\nrather than try to expunge every remnant of vagueness from the definition, we\nwill entrust the reader with interpreting it\nsensibly.[2](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos962916)\n\nThe simplest example of speed superintelligence would be a whole brain\nemulation running on fast\nhardware.[3](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos963263)\nAn emulation operating at a speed of ten thousand times that of a biological\nbrain would be able to read a book in a few seconds and write a PhD thesis in\nan afternoon. With a speedup factor of a million, an emulation could\naccomplish an entire millennium of intellectual work in one working\nday.[4](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos964586)\n\nTo such a fast mind, events in the external world appear to unfold in slow\nmotion. Suppose your mind ran at 10,000×. If your fleshly friend should happen\nto drop his teacup, you could watch the porcelain slowly descend toward the\ncarpet over the course of several hours, like a comet silently gliding through\nspace toward an assignation with a far-off planet; and, as the anticipation of\nthe coming crash tardily propagates through the folds of your friend’s gray\nmatter and from thence out into his peripheral nervous system, you could\nobserve his body gradually assuming the aspect of a frozen oops—enough time\nfor you not only to order a replacement cup but also to read a couple of\nscientific papers and take a nap.\n\nBecause of this apparent time dilation of the material world, a speed\nsuperintelligence would prefer to work with digital objects. It could live in\nvirtual reality and deal in the information economy. Alternatively, it could\ninteract with the physical environment by means of nanoscale manipulators,\nsince limbs at such small scales could operate faster than macroscopic\nappendages. (The characteristic frequency of a system tends to be inversely\nproportional to its length\nscale.[5](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos965342)) A\nfast mind might commune mainly with other fast minds rather than with\nbradytelic, molasses-like humans.\n\nThe speed of light becomes an increasingly important constraint as minds get\nfaster, since faster minds face greater opportunity costs in the use of their\ntime for traveling or communicating over long\ndistances.[6](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos965677)\nLight is roughly a million times faster than a jet plane, so it would take a\ndigital agent with a mental speedup of 1,000,000× about the same amount of\nsubjective time to travel across the globe as it does a contemporary human\njourneyer. Dialing somebody long distance would take as long as getting there\n“in person,” though it would be cheaper as a call would require less\nbandwidth. Agents with large mental speedups who want to converse extensively\nmight find it advantageous to move near one another. Extremely fast minds with\nneed for frequent interaction (such as members of a work team) may take up\nresidence in computers located in the same building to avoid frustrating\nlatencies.\n\n### [Collective\nsuperintelligence](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_006.html#filepos16800)\n\nAnother form of superintelligence is a system achieving superior performance\nby aggregating large numbers of smaller intelligences:\n\n> **Collective superintelligence:** _A system composed of a large number of\n> smaller intellects such that the system’s overall performance across many\n> very general domains vastly outstrips that of any current cognitive system_.\n\nCollective superintelligence is less conceptually clear-cut than speed\nsuperintelligence.[7](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos965784)\nHowever, it is more familiar empirically. While we have no experience with\nhuman-level minds that differ significantly in clock speed, we _do_ have ample\nexperience with collective intelligence, systems composed of various numbers\nof human-level components working together with various degrees of efficiency.\nFirms, work teams, gossip networks, advocacy groups, academic communities,\ncountries, even humankind as a whole, can—if we adopt a somewhat abstract\nperspective—be viewed as loosely defined “systems” capable of solving classes\nof intellectual problems. From experience, we have some sense of how easily\ndifferent tasks succumb to the efforts of organizations of various size and\ncomposition.\n\nCollective intelligence excels at solving problems that can be readily broken\ninto parts such that solutions to sub-problems can be pursued in parallel and\nverified independently. Tasks like building a space shuttle or operating a\nhamburger franchise offer myriad opportunities for division of labor:\ndifferent engineers work on different components of the spacecraft; different\nstaffs operate different restaurants. In academia, the rigid division of\nresearchers, students, journals, grants, and prizes into separate self-\ncontained disciplines—though unconducive to the type of work represented by\nthis book—might (only in a conciliatory and mellow frame of mind) be viewed as\na necessary accommodation to the practicalities of allowing large numbers of\ndiversely motivated individuals and teams to contribute to the growth of human\nknowledge while working relatively independently, each plowing their own\nfurrow.\n\nA system’s collective intelligence could be enhanced by expanding the number\nor the quality of its constituent intellects, or by improving the quality of\ntheir\norganization.[8](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos966270)\nTo obtain a collective _superintelligence_ from any present-day collective\nintelligence would require a very great degree of enhancement. The resulting\nsystem would need to be capable of vastly outperforming any current collective\nintelligence or other cognitive system across many very general domains. A new\nconference format that lets scholars exchange information more effectively, or\na new collaborative information-filtering algorithm that better predicted\nusers’ ratings of books and movies, would clearly not on its own amount to\nanything approaching collective superintelligence. Nor would a 50% increase in\nthe world population, or an improvement in pedagogical method that enabled\nstudents to complete a school day in four hours instead of six. Some far more\nextreme growth of humanity’s collective cognitive capacity would be required\nto meet the criterion of collective superintelligence.\n\nNote that the threshold for collective superintelligence is indexed to the\nperformance levels of the present—that is, the early twenty-first century.\nOver the course of human prehistory, and again over the course of human\nhistory, humanity’s collective intelligence _has_ grown by very large factors.\nWorld population, for example, has increased by at least a factor of a\nthousand since the\nPleistocene.[9](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos966621)\nOn this basis alone, current levels of human collective intelligence could be\nregarded as approaching superintelligence _relative to a Pleistocene\nbaseline_. Some improvements in communications technologies—especially spoken\nlanguage, but perhaps also cities, writing, and printing—could also be argued\nto have, individually or in combination, provided super-sized boosts, in the\nsense that if another innovation of comparable impact to our collective\nintellectual problem-solving capacity were to happen, it would result in\ncollective\nsuperintelligence.[10](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos966921)\n\nA certain kind of reader will be tempted at this point to interject that\nmodern society does not seem so particularly intelligent. Perhaps some\nunwelcome political decision has just been made in the reader’s home country,\nand the apparent unwisdom of that decision now looms large in the reader’s\nmind as evidence of the mental incapacity of the modern era. And is it not the\ncase that contemporary humanity is idolizing material consumption, depleting\nnatural resources, polluting the environment, decimating species diversity,\nall the while failing to remedy screaming global injustices and neglecting\nparamount humanistic or spiritual values? However, setting aside the question\nof how modernity’s shortcomings stack up against the not-so-inconsiderable\nfailings of earlier epochs, nothing in our definition of collective\nsuperintelligence implies that a society with greater collective intelligence\nis necessarily better off. The definition does not even imply that the more\ncollectively intelligent society is _wiser_. We can think of wisdom as the\nability to get the important things approximately right. It is then possible\nto imagine an organization composed of a very large cadre of very efficiently\ncoordinated knowledge workers, who collectively can solve intellectual\nproblems across many very general domains. This organization, let us suppose,\ncan operate most kinds of businesses, invent most kinds of technologies, and\noptimize most kinds of processes. Even so, it might get a few key big-picture\nissues entirely wrong—for instance, it may fail to take proper precautions\nagainst existential risks—and as a result pursue a short explosive growth\nspurt that ends ingloriously in total collapse. Such an organization could\nhave a very high degree of collective intelligence; if sufficiently high, the\norganization is a collective superintelligence. We should resist the\ntemptation to roll every normatively desirable attribute into one giant\namorphous concept of mental functioning, as though one could never find one\nadmirable trait without all the others being equally present. Instead, we\nshould recognize that there can exist instrumentally powerful information\nprocessing systems—intelligent systems—that are neither inherently good nor\nreliably wise. But we will revisit this issue in [Chapter\n7](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_014.html#filepos373977).\n\nCollective superintelligence could be either loosely or tightly integrated. To\nillustrate a case of loosely integrated collective superintelligence, imagine\na planet, _MegaEarth_ , which has the same level of communication and\ncoordination technologies that we currently have on the real Earth but with a\npopulation one million times as large. With such a huge population, the total\nintellectual workforce on MegaEarth would be correspondingly larger than on\nour planet. Suppose that a scientific genius of the caliber of a Newton or an\nEinstein arises at least once for every 10 billion people: then on MegaEarth\nthere would be 700,000 such geniuses living contemporaneously, alongside\nproportionally vast multitudes of slightly lesser talents. New ideas and\ntechnologies would be developed at a furious pace, and global civilization on\nMegaEarth would constitute a loosely integrated collective\nsuperintelligence.[11](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos967228)\n\nIf we gradually increase the level of integration of a collective\nintelligence, it may eventually become a unified _intellect_ —a single large\n“mind” as opposed to a mere assemblage of loosely interacting smaller human\nminds.[12](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos967748) The\ninhabitants of MegaEarth could take steps in that direction by improving\ncommunications and coordination technologies and by developing better ways for\nmany individuals to work on any hard intellectual problem together. A\ncollective superintelligence could thus, after gaining sufficiently in\nintegration, become a “quality superintelligence.”\n\n### [Quality\nsuperintelligence](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_006.html#filepos16904)\n\nWe can distinguish a third form of superintelligence.\n\n> **Quality superintelligence:** _A system that is at least as fast as a human\n> mind and vastly qualitatively smarter_.\n\nAs with collective intelligence, intelligence quality is also a somewhat murky\nconcept; and in this case the difficulty is compounded by our lack of\nexperience with any variations in intelligence quality above the upper end of\nthe present human distribution. We can, however, get some grasp of the notion\nby considering some related cases.\n\nFirst, we can expand the range of our reference points by considering nonhuman\nanimals, which have intelligence of lower quality. (This is not meant as a\nspeciesist remark. A zebrafish has a quality of intelligence that is\nexcellently adapted to its ecological needs; but the relevant perspective here\nis a more anthropocentric one: our concern is with performance on _humanly_\nrelevant complex cognitive tasks.) Nonhuman animals lack complex structured\nlanguage; they are capable of no or only rudimentary tool use and tool\nconstruction; they are severely restricted in their ability to make long-term\nplans; and they have very limited abstract reasoning ability. Nor are these\nlimitations fully explained by a lack of speed or of collective intelligence\namong nonhuman animal minds. In terms of raw computational power, human brains\nare probably inferior to those of some large animals, including elephants and\nwhales. And although humanity’s complex technological civilization would be\nimpossible without our massive advantage in collective intelligence, not all\ndistinctly human cognitive capabilities depend on collective intelligence.\nMany are highly developed even in small, isolated hunter–gatherer\nbands.[13](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos968434) And\nmany are not nearly as highly developed among highly organized nonhuman\nanimals, such as chimpanzees and dolphins intensely trained by human\ninstructors, or ants living in their own large and well-ordered societies.\nEvidently, the remarkable intellectual achievements of _Homo sapiens_ are to a\nsignificant extent attributable to specific features of our brain\narchitecture, features that depend on a unique genetic endowment not shared by\nother animals. This observation can help us illustrate the concept of quality\nsuperintelligence: it is intelligence of quality at least as superior to that\nof human intelligence as the quality of human intelligence is superior to that\nof elephants’, dolphins’, or chimpanzees’.\n\nA second way to illustrate the concept of quality superintelligence is by\nnoting the domain-specific cognitive deficits that can afflict individual\nhumans, particularly deficits that are not caused by general dementia or other\nconditions associated with wholesale destruction of the brain’s\nneurocomputational resources. Consider, for example, individuals with autism\nspectrum disorders who may have striking deficits in social cognition while\nfunctioning well in other cognitive domains; or individuals with congenital\namusia, who are unable to hum or recognize simple tunes yet perform normally\nin most other respects. Many other examples could be adduced from the\nneuropsychiatric literature, which is replete with case studies of patients\nsuffering narrowly circumscribed deficits caused by genetic abnormalities or\nbrain trauma. Such examples show that normal human adults have a range of\nremarkable cognitive talents that are not simply a function of possessing a\nsufficient amount of general neural processing power or even a sufficient\namount of general intelligence: specialized neural circuitry is also needed.\nThis observation suggests the idea of _possible but non-realized cognitive\ntalents_ , talents that no actual human possesses even though other\nintelligent systems—ones with no more computing power than the human\nbrain—that did have those talents would gain enormously in their ability to\naccomplish a wide range of strategically relevant tasks.\n\nAccordingly, by considering nonhuman animals and human individuals with\ndomain-specific cognitive deficits, we can form some notion of different\nqualities of intelligence and the practical difference they make. Had _Homo\nsapiens_ lacked (for instance) the cognitive modules that enable complex\nlinguistic representations, it might have been just another simian species\nliving in harmony with nature. Conversely, were we to _gain_ some new set of\nmodules giving an advantage comparable to that of being able to form complex\nlinguistic representations, we would become superintelligent.\n\n### [Direct and indirect\nreach](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_006.html#filepos17005)\n\nSuperintelligence in any of these forms could, over time, develop the\ntechnology necessary to create any of the others. The _indirect reaches_ of\nthese three forms of superintelligence are therefore equal. In that sense, the\nindirect reach of current human intelligence is also in the same equivalence\nclass, under the supposition that we are able eventually to create some form\nof superintelligence. Yet there is a sense in which the three forms of\nsuperintelligence are much closer to one another: any one of them could create\nother forms of superintelligence more rapidly than we can create any form of\nsuperintelligence from our present starting point.\n\nThe _direct reaches_ of the three different forms of superintelligence are\nharder to compare. There may be no definite ordering. Their respective\ncapabilities depend on the degree to which they instantiate their respective\nadvantages— _how_ fast a speed superintelligence is, _how_ qualitatively\nsuperior a quality superintelligence is, and so forth. At most, we might say\nthat, _ceteris paribus_ , speed superintelligence excels at tasks requiring\nthe rapid execution of a long series of steps that must be performed\nsequentially while collective superintelligence excels at tasks admitting of\nanalytic decomposition into parallelizable sub-tasks and tasks demanding the\ncombination of many different perspectives and skill sets. In some vague\nsense, quality superintelligence would be the most capable form of all,\ninasmuch as it could grasp and solve problems that are, for all practical\npurposes, beyond the _direct_ reach of speed superintelligence and collective\nsuperintelligence.[14](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos969227)\n\nIn some domains, quantity is a poor substitute for quality. One solitary\ngenius working out of a cork-lined bedroom can write _In Search of Lost Time_.\nCould an equivalent masterpiece be produced by recruiting an office building\nfull of literary\nhacks?[15](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos970466)\nEven within the range of present human variation we see that some functions\nbenefit greatly from the labor of one brilliant mastermind as opposed to the\njoint efforts of myriad mediocrities. If we widen our purview to include\n_superintelligent_ minds, we must countenance a likelihood of there being\nintellectual problems solvable only by superintelligence and intractable to\nany ever-so-large collective of non-augmented humans.\n\nThere might thus be some problems that are solvable by a quality\nsuperintelligence, and perhaps by a speed superintelligence, yet which a\nloosely integrated collective superintelligence cannot solve (other than by\nfirst amplifying its own\nintelligence).[16](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos970712)\nWe cannot clearly see what all these problems are, but we can characterize\nthem in general\nterms.[17](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos970896)\nThey would tend to be problems involving multiple complex interdependencies\nthat do not permit of independently verifiable solution steps: problems that\ntherefore cannot be solved in a piecemeal fashion, and that might require\nqualitatively new kinds of understanding or new representational frameworks\nthat are too deep or too complicated for the current edition of mortals to\ndiscover or use effectively. Some types of artistic creation and strategic\ncognition might fall into this category. Some types of scientific\nbreakthrough, perhaps, likewise. And one can speculate that the tardiness and\nwobbliness of humanity’s progress on many of the “eternal problems” of\nphilosophy are due to the unsuitability of the human cortex for philosophical\nwork. On this view, our most celebrated philosophers are like dogs walking on\ntheir hind legs—just barely attaining the threshold level of performance\nrequired for engaging in the activity _at\nall_.[18](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos971364)\n\n### [Sources of advantage for digital\nintelligence](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_006.html#filepos17106)\n\nMinor changes in brain volume and wiring can have major consequences, as we\nsee when we compare the intellectual and technological achievements of humans\nwith those of other apes. The far greater changes in computing resources and\narchitecture that machine intelligence will enable will probably have\nconsequences that are even more profound. It is difficult, perhaps impossible,\nfor us to form an intuitive sense of the aptitudes of a superintelligence; but\nwe can at least get an inkling of the space of possibilities by looking at\nsome of the advantages open to digital minds. The hardware advantages are\neasiest to appreciate:\n\n• _Speed of computational elements_. Biological neurons operate at a peak\nspeed of about 200 Hz, a full seven orders of magnitude slower than a modern\nmicroprocessor (~ 2\nGHz).[19](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos971502) As a\nconsequence, the human brain is forced to rely on massive parallelization and\nis incapable of rapidly performing any computation that requires a large\nnumber of sequential\noperations.[20](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos971956)\n(Anything the brain does in under a second cannot use much more than a hundred\nsequential operations—perhaps only a few dozen.) Yet many of the most\npractically important algorithms in programming and computer science are not\neasily parallelizable. Many cognitive tasks could be performed far more\nefficiently if the brain’s native support for parallelizable pattern-matching\nalgorithms were complemented by, and integrated with, support for fast\nsequential processing.\n\n• _Internal communication speed_. Axons carry action potentials at speeds of\n120 m/s or less, whereas electronic processing cores can communicate optically\nat the speed of light (300,000,000\nm/s).[21](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos972078) The\nsluggishness of neural signals limits how big a biological brain can be while\nfunctioning as a single processing unit. For example, to achieve a round-trip\nlatency of less than 10 ms between any two elements in a system, biological\nbrains must be smaller than 0.11 m3. An electronic system, on the other hand,\ncould be 6.1×1017 m3, about the size of a dwarf planet: eighteen orders of\nmagnitude\nlarger.[22](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos972594)\n\n• _Number of computational elements_. The human brain has somewhat fewer than\n100 billion\nneurons.[23](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos972823)\nHumans have about three and a half times the brain size of chimpanzees (though\nonly one-fifth the brain size of sperm\nwhales).[24](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos973359)\nThe number of neurons in a biological creature is most obviously limited by\ncranial volume and metabolic constraints, but other factors may also be\nsignificant for larger brains (such as cooling, development time, and signal-\nconductance delays—see the previous point). By contrast, computer hardware is\nindefinitely scalable up to very high physical\nlimits.[25](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos973471)\nSupercomputers can be warehouse-sized or larger, with additional remote\ncapacity added via high-speed\ncables.[26](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos973920)\n\n• _Storage capacity_. Human working memory is able to hold no more than some\nfour or five chunks of information at any given\ntime.[27](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos974198)\nWhile it would be misleading to compare the size of human working memory\ndirectly with the amount of RAM in a digital computer, it is clear that the\nhardware advantages of digital intelligences will make it possible for them to\nhave larger working memories. This might enable such minds to intuitively\ngrasp complex relationships that humans can only fumblingly handle via\nplodding\ncalculation.[28](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos974509)\nHuman long-term memory is also limited, though it is unclear whether we manage\nto exhaust its storage capacity during the course of an ordinary lifetime—the\nrate at which we accumulate information is so slow. (On one estimate, the\nadult human brain stores about one billion bits—a couple of orders of\nmagnitude less than a low-end\nsmartphone.[29](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos974903))\nBoth the amount of information stored and the speed with which it can be\naccessed could thus be vastly greater in a machine brain than in a biological\nbrain.\n\n• _Reliability, lifespan, sensors, etc_. Machine intelligences might have\nvarious other hardware advantages. For example, biological neurons are less\nreliable than\ntransistors.[30](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos975413)\nSince noisy computing necessitates redundant encoding schemes that use\nmultiple elements to encode a single bit of information, a digital brain might\nderive some efficiency gains from the use of reliable high-precision computing\nelements. Brains become fatigued after a few hours of work and start to\npermanently decay after a few decades of subjective time; microprocessors are\nnot subject to these limitations. Data flow into a machine intelligence could\nbe increased by adding millions of sensors. Depending on the technology used,\na machine might have reconfigurable hardware that can be optimized for\nchanging task requirements, whereas much of the brain’s architecture is fixed\nfrom birth or only slowly changeable (though the details of synaptic\nconnectivity can change over shorter timescales, like\ndays).[31](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos975947)\n\nAt present, the computational power of the biological brain still compares\nfavorably with that of digital computers, though top-of-the-line\nsupercomputers are attaining levels of performance that are within the range\nof plausible estimates of the brain’s processing\npower.[32](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos976069) But\nhardware is rapidly improving, and the ultimate limits of hardware performance\nare vastly higher than those of biological computing substrates.\n\nDigital minds will also benefit from major advantages in software:\n\n• _Editability_. It is easier to experiment with parameter variations in\nsoftware than in neural wetware. For example, with a whole brain emulation one\ncould easily trial what happens if one adds more neurons in a particular\ncortical area or if one increases or decreases their excitability. Running\nsuch experiments in living biological brains would be far more difficult.\n\n• _Duplicability_. With software, one can quickly make arbitrarily many high-\nfidelity copies to fill the available hardware base. Biological brains, by\ncontrast, can be reproduced only very slowly; and each new instance starts out\nin a helpless state, remembering nothing of what its parents learned in their\nlifetimes.\n\n• _Goal coordination_. Human collectives are replete with inefficiencies\narising from the fact that it is nearly impossible to achieve complete\nuniformity of purpose among the members of a large group—at least until it\nbecomes feasible to induce docility on a large scale by means of drugs or\ngenetic selection. A “copy clan” (a group of identical or almost identical\nprograms sharing a common goal) would avoid such coordination problems.\n\n• _Memory sharing_. Biological brains need extended periods of training and\nmentorship whereas digital minds could acquire new memories and skills by\nswapping data files. A population of a billion copies of an AI program could\nsynchronize their databases periodically, so that all the instances of the\nprogram know everything that any instance learned during the previous hour.\n(Direct memory transfer requires standardized representational formats. Easy\nswapping of high-level cognitive content would therefore not be possible\nbetween just any pair of machine intelligences. In particular, it would not be\npossible among first-generation whole brain emulations.)\n\n• _New modules, modalities, and algorithms_. Visual perception seems to us\neasy and effortless, quite unlike solving textbook geometry problems—this\ndespite the fact that it takes a massive amount of computation to reconstruct,\nfrom the two-dimensional patterns of stimulation on our retinas, a three-\ndimensional representation of a world populated with recognizable objects. The\nreason this seems easy is that we have dedicated low-level neural machinery\nfor processing visual information. This low-level processing occurs\nunconsciously and automatically, without draining our mental energy or\nconscious attention. Music perception, language use, social cognition, and\nother forms of information processing that are “natural” for us humans seem to\nbe likewise supported by dedicated neurocomputational modules. An artificial\nmind that had such specialized support for other cognitive domains that have\nbecome important in the contemporary world—such as engineering, computer\nprogramming, and business strategy—would have big advantages over minds like\nours that have to rely on clunky general-purpose cognition to think about such\nthings. New algorithms may also be developed to take advantage of the distinct\naffordances of digital hardware, such as its support for fast serial\nprocessing.\n\nThe _ultimately_ attainable advantages of machine intelligence, hardware and\nsoftware combined, are\nenormous.[33](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos976554)\nBut how rapidly could those potential advantages be realized? That is the\nquestion to which we now turn.\n\n\n## [CHAPTER 4  \nThe kinetics of an intelligence\nexplosion](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_006.html#filepos17240)\n\n**Once machines attain some form of human-equivalence in general reasoning\nability, how long will it then be before they attain radical\nsuperintelligence? Will this be a slow, gradual, protracted transition? Or\nwill it be sudden, explosive? This chapter analyzes the kinetics of the\ntransition to superintelligence as a function of optimization power and system\nrecalcitrance. We consider what we know or may reasonably surmise about the\nbehavior of these two factors in the neighborhood of human-level general\nintelligence.**\n\n### [Timing and speed of the\ntakeoff](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_006.html#filepos17371)\n\nGiven that machines will _eventually_ vastly exceed biology in general\nintelligence, but that machine cognition is _currently_ vastly narrower than\nhuman cognition, one is led to wonder how quickly this usurpation will take\nplace. The question we are asking here must be sharply distinguished from the\nquestion we considered in [Chapter\n1](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos33916) about how\nfar away we currently are from developing a machine with human-level general\nintelligence. Here the question is instead, _if and when such a machine is\ndeveloped, how long will it be from then until a machine becomes radically\nsuperintelligent?_ Note that one could think that it will take quite a long\ntime until machines reach the human baseline, or one might be agnostic about\nhow long that will take, and yet have a strong view that once this happens,\nthe further ascent into strong superintelligence will be very rapid.\n\nIt can be helpful to think about these matters schematically, even though\ndoing so involves temporarily ignoring some qualifications and complicating\ndetails. Consider, then, a diagram that plots the intellectual capability of\nthe most advanced machine intelligence system as a function of time (Figure\n7).\n\nA horizontal line labeled “human baseline” represents the effective\nintellectual capabilities of a representative human adult with access to the\ninformation sources and technological aids currently available in developed\ncountries. At present, the most advanced AI system is far below the human\nbaseline on any reasonable metric of general intellectual ability. At some\npoint in future, a machine might reach approximate parity with this human\nbaseline (which we take to be fixed—anchored to the year 2014, say, even if\nthe capabilities of human individuals should have increased in the intervening\nyears): this would mark the onset of the takeoff. The capabilities of the\nsystem continue to grow, and at some later point the system reaches parity\nwith the combined intellectual capability of all of humanity (again anchored\nto the present): what we may call the “civilization baseline”. Eventually, if\nthe system’s abilities continue to grow, it attains “strong\nsuperintelligence”—a level of intelligence vastly greater than contemporary\nhumanity’s combined intellectual wherewithal. The attainment of strong\nsuperintelligence marks the completion of the takeoff, though the system might\ncontinue to gain in capacity thereafter. Sometime during the takeoff phase,\nthe system may pass a landmark which we can call “the crossover”, a point\nbeyond which the system’s further improvement is mainly driven by the system’s\nown actions rather than by work performed upon it by\nothers.[1](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos977009)\n(The possible existence of such a crossover will become important in the\nsubsection on optimization power and explosivity, later in this chapter.)\n\n![Image](images/00013.jpg)\n\n**Figure 7** Shape of the takeoff. It is important to distinguish between\nthese questions: “Will a takeoff occur, and if so, when?” and “If and when a\ntakeoff does occur, how steep will it be?” One might hold, for example, that\nit will be a very long time before a takeoff occurs, but that when it does it\nwill proceed very quickly. Another relevant question (not illustrated in this\nfigure) is, “How large a fraction of the world economy will participate in the\ntakeoff?” These questions are related but distinct.\n\nWith this picture in mind, we can distinguish three classes of transition\nscenarios—scenarios in which systems progress from human-level intelligence to\nsuperintelligence—based on their steepness; that is to say, whether they\nrepresent a slow, fast, or moderate takeoff.\n\n> Slow\n\n> A slow takeoff is one that occurs over some long temporal interval, such as\n> decades or centuries. Slow takeoff scenarios offer excellent opportunities\n> for human political processes to adapt and respond. Different approaches can\n> be tried and tested in sequence. New experts can be trained and\n> credentialed. Grassroots campaigns can be mobilized by groups that feel they\n> are being disadvantaged by unfolding developments. If it appears that new\n> kinds of secure infrastructure or mass surveillance of AI researchers is\n> needed, such systems could be developed and deployed. Nations fearing an AI\n> arms race would have time to try to negotiate treaties and design\n> enforcement mechanisms. Most preparations undertaken before onset of the\n> slow takeoff would be rendered obsolete as better solutions would gradually\n> become visible in the light of the dawning era.\n\n> Fast\n\n> A fast takeoff occurs over some short temporal interval, such as minutes,\n> hours, or days. Fast takeoff scenarios offer scant opportunity for humans to\n> deliberate. Nobody need even notice anything unusual before the game is\n> already lost. In a fast takeoff scenario, humanity’s fate essentially\n> depends on preparations previously put in place. At the slowest end of the\n> fast takeoff scenario range, some simple human actions might be possible,\n> analogous to flicking open the “nuclear suitcase”; but any such action would\n> either be elementary or have been planned and pre-programmed in advance.\n\n> Moderate\n\n> A moderate takeoff is one that occurs over some intermediary temporal\n> interval, such as months or years. Moderate takeoff scenarios give humans\n> some chance to respond but not much time to analyze the situation, to test\n> different approaches, or to solve complicated coordination problems. There\n> is not enough time to develop or deploy new systems (e.g. political systems,\n> surveillance regimes, or computer network security protocols), but extant\n> systems could be applied to the new challenge.\n\nDuring a slow takeoff, there would be plenty of time for the news to get out.\nIn a moderate takeoff, by contrast, it is possible that developments would be\nkept secret as they unfold. Knowledge might be restricted to a small group of\ninsiders, as in a covert state-sponsored military research program. Commercial\nprojects, small academic teams, and “nine hackers in a basement” outfits might\nalso be clandestine—though, if the prospect of an intelligence explosion were\n“on the radar” of state intelligence agencies as a national security priority,\nthen the most promising private projects would seem to have a good chance of\nbeing under surveillance. The host state (or a dominant foreign power) would\nthen have the option of nationalizing or shutting down any project that showed\nsigns of commencing takeoff. Fast takeoffs would happen so quickly that there\nwould not be much time for word to get out or for anybody to mount a\nmeaningful reaction if it did. But an outsider might intervene _before_ the\nonset of the takeoff if they believed a particular project to be closing in on\nsuccess.\n\nModerate takeoff scenarios could lead to geopolitical, social, and economic\nturbulence as individuals and groups jockey to position themselves to gain\nfrom the unfolding transformation. Such upheaval, should it occur, might\nimpede efforts to orchestrate a well-composed response; alternatively, it\nmight enable solutions more radical than calmer circumstances would permit.\nFor instance, in a moderate takeoff scenario where cheap and capable\nemulations or other digital minds gradually flood labor markets over a period\nof years, one could imagine mass protests by laid-off workers pressuring\ngovernments to increase unemployment benefits or institute a living wage\nguarantee to all human citizens, or to levy special taxes or impose minimum\nwage requirements on employers who use emulation workers. In order for any\nrelief derived from such policies to be more than fleeting, support for them\nwould somehow have to be cemented into permanent power structures. Similar\nissues can arise if the takeoff is slow rather than moderate, but the\ndisequilibrium and rapid change in moderate scenarios may present special\nopportunities for small groups to wield disproportionate influence.\n\nIt might appear to some readers that of these three types of scenario, the\nslow takeoff is the most probable, the moderate takeoff is less probable, and\nthe fast takeoff is utterly implausible. It could seem fanciful to suppose\nthat the world could be radically transformed and humanity deposed from its\nposition as apex cogitator over the course of an hour or two. No change of\nsuch moment has ever occurred in human history, and its nearest parallels—the\nAgricultural and Industrial Revolutions—played out over much longer timescales\n(centuries to millennia in the former case, decades to centuries in the\nlatter). So the base rate for the kind of transition entailed by a fast or\nmedium takeoff scenario, in terms of the speed and magnitude of the postulated\nchange, is zero: it lacks precedent outside myth and\nreligion.[2](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos977363)\n\nNevertheless, this chapter will present some reasons for thinking that the\nslow transition scenario is improbable. If and when a takeoff occurs, it will\nlikely be explosive.\n\nTo begin to analyze the question of how fast the takeoff will be, we can\nconceive of the rate of increase in a system’s intelligence as a\n(monotonically increasing) function of two variables: the amount of\n“optimization power”, or quality-weighted design effort, that is being applied\nto increase the system’s intelligence, and the responsiveness of the system to\nthe application of a given amount of such optimization power. We might term\nthe inverse of responsiveness “recalcitrance”, and write:\n\n![Image](images/00014.jpg)\n\nPending some specification of how to quantify intelligence, design effort, and\nrecalcitrance, this expression is merely qualitative. But we can at least\nobserve that a system’s intelligence will increase rapidly if _either_ a lot\nof skilled effort is applied to the task of increasing its intelligence and\nthe system’s intelligence is not too hard to increase _or_ there is a non-\ntrivial design effort and the system’s recalcitrance is low (or both). If we\nknow how much design effort is going into improving a particular system, and\nthe rate of improvement this effort produces, we could calculate the system’s\nrecalcitrance.\n\nFurther, we can observe that the amount of optimization power devoted to\nimproving some system’s performance varies between systems and over time. A\nsystem’s recalcitrance might also vary depending on how much the system has\nalready been optimized. Often, the easiest improvements are made first,\nleading to diminishing returns (increasing recalcitrance) as low-hanging\nfruits are depleted. However, there can also be improvements that make further\nimprovements easier, leading to improvement cascades. The process of solving a\njigsaw puzzle starts out simple—it is easy to find the corners and the edges.\nThen recalcitrance goes up as subsequent pieces are harder to fit. But as the\npuzzle nears completion, the search space collapses and the process gets\neasier again.\n\nTo proceed in our inquiry, we must therefore analyze how recalcitrance and\noptimization power might vary in the critical time periods during the takeoff.\nThis will occupy us over the next few pages.\n\n###\n[Recalcitrance](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_006.html#filepos17478)\n\nLet us begin with recalcitrance. The outlook here depends on the type of the\nsystem under consideration. For completeness, we first cast a brief glance at\nthe recalcitrance that would be encountered along paths to superintelligence\nthat do not involve advanced machine intelligence. We find that recalcitrance\nalong those paths appears to be fairly high. That done, we will turn to the\nmain case, which is that the takeoff involves machine intelligence; and there\nwe find that recalcitrance at the critical juncture seems low.\n\n#### [Non-machine intelligence\npaths](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_006.html#filepos17567)\n\nCognitive enhancement via improvements in public health and diet has steeply\ndiminishing\nreturns.[3](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos977646)\nBig gains come from eliminating severe nutritional deficiencies, and the most\nsevere deficiencies have already been largely eliminated in all but the\npoorest countries. Only girth is gained by increasing an already adequate\ndiet. Education, too, is now probably subject to diminishing returns. The\nfraction of talented individuals in the world who lack access to quality\neducation is still substantial, but declining.\n\nPharmacological enhancers might deliver some cognitive gains over the coming\ndecades. But after the easiest fixes have been accomplished—perhaps\nsustainable increases in mental energy and ability to concentrate, along with\nbetter control over the rate of long-term memory consolidation—subsequent\ngains will be increasingly hard to come by. Unlike diet and public health\napproaches, however, improving cognition through smart drugs might get easier\nbefore it gets harder. The field of neuropharmacology still lacks much of the\nbasic knowledge that would be needed to competently intervene in the healthy\nbrain. Neglect of enhancement medicine as a legitimate area of research may be\npartially to blame for this current backwardness. If neuroscience and\npharmacology continue to progress for a while longer without focusing on\ncognitive enhancement, then maybe there would be some relatively easy gains to\nbe had when at last the development of nootropics becomes a serious\npriority.[4](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos978837)\n\nGenetic cognitive enhancement has a U-shaped recalcitrance profile similar to\nthat of nootropics, but with larger potential gains. Recalcitrance starts out\nhigh while the only available method is selective breeding sustained over many\ngenerations, something that is obviously difficult to accomplish on a globally\nsignificant scale. Genetic enhancement will get easier as technology is\ndeveloped for cheap and effective genetic testing and selection (and\nparticularly when iterated embryo selection becomes feasible in humans). These\nnew techniques will make it possible to tap the pool of existing human genetic\nvariation for intelligence-enhancing alleles. As the best existing alleles get\nincorporated into genetic enhancement packages, however, further gains will\nget harder to come by. The need for more innovative approaches to genetic\nmodification may then increase recalcitrance. There are limits to how quickly\nthings can progress along the genetic enhancement path, most notably the fact\nthat germline interventions are subject to an inevitable maturational lag:\nthis strongly counteracts the possibility of a fast or moderate\ntakeoff.[5](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos978956)\nThat embryo selection can only be applied in the context of in vitro\nfertilization will slow its rate of adoption: another limiting factor.\n\nThe recalcitrance along the brain–computer path seems initially very high. In\nthe unlikely event that it somehow becomes easy to insert brain implants and\nto achieve high-level functional integration with the cortex, recalcitrance\nmight plummet. In the long run, the difficulty of making progress along this\npath would be similar to that involved in improving emulations or AIs, since\nthe bulk of the brain–computer system’s intelligence would eventually reside\nin the computer part.\n\nThe recalcitrance for making networks and organizations _in general_ more\nefficient is high. A vast amount of effort is going into overcoming this\nrecalcitrance, and the result is an annual improvement of humanity’s total\ncapacity by perhaps no more than a couple of\npercent.[6](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos979212)\nFurthermore, shifts in the internal and external environment mean that\norganizations, even if efficient at one time, soon become ill-adapted to their\nnew circumstances. Ongoing reform effort is thus required even just to prevent\ndeterioration. A step change in the rate of gain in average organizational\nefficiency is perhaps conceivable, but it is hard to see how even the most\nradical scenario of this kind could produce anything faster than a slow\ntakeoff, since organizations operated by humans are confined to work on human\ntimescales. The Internet continues to be an exciting frontier with many\nopportunities for enhancing collective intelligence, with a recalcitrance that\nseems at the moment to be in the moderate range—progress is somewhat swift but\na lot of effort is going into making this progress happen. It may be expected\nto increase as low-hanging fruits (such as search engines and email) are\ndepleted.\n\n#### [Emulation and AI\npaths](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_006.html#filepos17682)\n\nThe difficulty of advancing toward whole brain emulation is difficult to\nestimate. Yet we can point to a specific future milestone: the successful\nemulation of an insect brain. That milestone stands on a hill, and its\nconquest would bring into view much of the terrain ahead, allowing us to make\na decent guess at the recalcitrance of scaling up the technology to human\nwhole brain emulation. (A successful emulation of a small-mammal brain, such\nas that of a mouse, would give an even better vantage point that would allow\nthe distance remaining to a human whole brain emulation to be estimated with a\nhigh degree of precision.) The path toward artificial intelligence, by\ncontrast, may feature no such obvious milestone or early observation point. It\nis entirely possible that the quest for artificial intelligence will appear to\nbe lost in dense jungle until an unexpected breakthrough reveals the finishing\nline in a clearing just a few short steps away.\n\nRecall the distinction between these two questions: How hard is it to attain\nroughly human levels of cognitive ability? And how hard is it to get from\nthere to superhuman levels? The first question is mainly relevant for\npredicting how long it will be before the onset of a takeoff. It is the second\nquestion that is key to assessing the shape of the takeoff, which is our aim\nhere. And though it might be tempting to suppose that the step from human\nlevel to superhuman level must be the harder one—this step, after all, takes\nplace “at a higher altitude” where capacity must be superadded to an already\nquite capable system—this would be a very unsafe assumption. It is quite\npossible that recalcitrance _falls_ when a machine reaches human parity.\n\nConsider first whole brain emulation. The difficulties involved in creating\nthe first human emulation are of a quite different kind from those involved in\nenhancing an existing emulation. Creating a first emulation involves huge\ntechnological challenges, particularly in regard to developing the requisite\nscanning and image interpretation capabilities. This step might also require\nconsiderable amounts of physical capital—an industrial-scale machine park with\nhundreds of high-throughput scanning machines is not implausible. By contrast,\nenhancing the quality of an existing emulation involves tweaking algorithms\nand data structures: essentially a software problem, and one that could turn\nout to be much easier than perfecting the imaging technology needed to create\nthe original template. Programmers could easily experiment with tricks like\nincreasing the neuron count in different cortical areas to see how it affects\nperformance.[7](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos979607)\nThey also could work on code optimization and on finding simpler computational\nmodels that preserve the essential functionality of individual neurons or\nsmall networks of neurons. If the last technological prerequisite to fall into\nplace is either scanning or translation, with computing power being relatively\nabundant, then not much attention might have been given during the development\nphase to implementational efficiency, and easy opportunities for computational\nefficiency savings might be available. (More fundamental architectural\nreorganization might also be possible, but that takes us off the emulation\npath and into AI territory.)\n\nAnother way to improve the code base once the first emulation has been\nproduced is to scan additional brains with different or superior skills and\ntalents. Productivity growth would also occur as a consequence of adapting\norganizational structures and workflows to the unique attributes of digital\nminds. Since there is no precedent in the human economy of a worker who can be\nliterally copied, reset, run at different speeds, and so forth, managers of\nthe first emulation cohort would find plenty of room for innovation in\nmanagerial practices.\n\nAfter initially plummeting when human whole brain emulation becomes possible,\nrecalcitrance may rise again. Sooner or later, the most glaring\nimplementational inefficiencies will have been optimized away, the most\npromising algorithmic variations will have been tested, and the easiest\nopportunities for organizational innovation will have been exploited. The\ntemplate library will have expanded so that acquiring more brain scans would\nadd little benefit over working with existing templates. Since a template can\nbe multiplied, each copy can be individually trained in a different field, and\nthis can be done at electronic speed, it might be that the number of brains\nthat would need to be scanned in order to capture most of the potential\neconomic gains is small. Possibly a single brain would suffice.\n\nAnother potential cause of escalating recalcitrance is the possibility that\nemulations or their biological supporters will organize to support regulations\nrestricting the use of emulation workers, limiting emulation copying,\nprohibiting certain kinds of experimentation with digital minds, instituting\nworkers’ rights and a minimum wage for emulations, and so forth. It is equally\npossible, however, that political developments would go in the opposite\ndirection, contributing to a fall in recalcitrance. This might happen if\ninitial restraint in the use of emulation labor gives way to unfettered\nexploitation as competition heats up and the economic and strategic costs of\noccupying the moral high ground become clear.\n\nAs for artificial intelligence (non-emulation machine intelligence), the\ndifficulty of lifting a system from human-level to superhuman intelligence by\nmeans of algorithmic improvements depends on the attributes of the particular\nsystem. Different architectures might have very different recalcitrance.\n\nIn some situations, recalcitrance could be extremely low. For example, if\nhuman-level AI is delayed because one key insight long eludes programmers,\nthen when the final breakthrough occurs, the AI might leapfrog from below to\nradically above human level without even touching the intermediary rungs.\nAnother situation in which recalcitrance could turn out to be extremely low is\nthat of an AI system that can achieve intelligent capability via two different\nmodes of processing. To illustrate this possibility, suppose an AI is composed\nof two subsystems, one possessing domain-specific problem-solving techniques,\nthe other possessing general-purpose reasoning ability. It could then be the\ncase that while the second subsystem remains below a certain capacity\nthreshold, it contributes nothing to the system’s overall performance, because\nthe solutions it generates are always inferior to those generated by the\ndomain-specific subsystem. Suppose now that a small amount of optimization\npower is applied to the general-purpose subsystem and that this produces a\nbrisk rise in the capacity of that subsystem. At first, we observe no increase\nin the overall system’s performance, indicating that recalcitrance is high.\nThen, once the capacity of the general-purpose subsystem crosses the threshold\nwhere its solutions start to beat those of the domain-specific subsystem, the\noverall system’s performance suddenly begins to improve at the same brisk pace\nas the general-purpose subsystem, even as the amount of optimization power\napplied stays constant: the system’s recalcitrance has plummeted.\n\nIt is also possible that our natural tendency to view intelligence from an\nanthropocentric perspective will lead us to underestimate improvements in sub-\nhuman systems, and thus to overestimate recalcitrance. Eliezer Yudkowsky, an\nAI theorist who has written extensively on the future of machine intelligence,\nputs the point as follows:\n\n> AI might make an _apparently_ sharp jump in intelligence purely as the\n> result of anthropomorphism, the human tendency to think of “village idiot”\n> and “Einstein” as the extreme ends of the intelligence scale, instead of\n> nearly indistinguishable points on the scale of minds-in-general. Everything\n> dumber than a dumb human may appear to us as simply “dumb”. One imagines the\n> “AI arrow” creeping steadily up the scale of intelligence, moving past mice\n> and chimpanzees, with AIs still remaining “dumb” because AIs cannot speak\n> fluent language or write science papers, and then the AI arrow crosses the\n> tiny gap from infra-idiot to ultra-Einstein in the course of one month or\n> some similarly short\n> period.[8](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos980578)\n> (See Fig. 8.)\n\nThe upshot of these several considerations is that it is difficult to predict\nhow hard it will be to make algorithmic improvements in the first AI that\nreaches a roughly human level of general intelligence. There are at least some\npossible circumstances in which algorithm-recalcitrance is low. But even if\nalgorithm-recalcitrance is very high, this would not preclude the overall\nrecalcitrance of the AI in question from being low. For it might be easy to\nincrease the intelligence of the system in other ways than by improving its\nalgorithms. There are two other factors that can be improved: content and\nhardware.\n\nFirst, consider content improvements. By “content” we here mean those parts of\na system’s software assets that do not make up its core algorithmic\narchitecture. Content might include, for example, databases of stored\npercepts, specialized skills libraries, and inventories of declarative\nknowledge. For many kinds of system, the distinction between algorithmic\narchitecture and content is very unsharp; nevertheless, it will serve as a\nrough-and-ready way of pointing to one potentially important source of\ncapability gains in a machine intelligence. An alternative way of expressing\nmuch the same idea is by saying that a system’s intellectual problem-solving\ncapacity can be enhanced not only by making the system cleverer but also by\nexpanding what the system knows.\n\n![Image](images/00015.jpg)\n\n**Figure 8** A less anthropomorphic scale? The gap between a dumb and a clever\nperson may appear large from an anthropocentric perspective, yet in a less\nparochial view the two have nearly indistinguishable\nminds.[9](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos980746) It\nwill almost certainly prove harder and take longer to build a machine\nintelligence that has a general level of smartness comparable to that of a\nvillage idiot than to improve such a system so that it becomes much smarter\nthan any human.\n\nConsider a contemporary AI system such as TextRunner (a research project at\nthe University of Washington) or IBM’s Watson (the system that won the\n_Jeopardy!_ quiz show). These systems can extract certain pieces of semantic\ninformation by analyzing text. Although these systems do not understand what\nthey read in the same sense or to the same extent as a human does, they can\nnevertheless extract significant amounts of information from natural language\nand use that information to make simple inferences and answer questions. They\ncan also learn from experience, building up more extensive representations of\na concept as they encounter additional instances of its use. They are designed\nto operate for much of the time in unsupervised mode (i.e. to learn hidden\nstructure in unlabeled data in the absence of error or reward signal, without\nhuman guidance) and to be fast and scalable. TextRunner, for instance, works\nwith a corpus of 500 million web\npages.[10](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos981107)\n\nNow imagine a remote descendant of such a system that has acquired the ability\nto read with as much understanding as a human ten-year-old but with a reading\nspeed similar to that of TextRunner. (This is probably an AI-complete\nproblem.) So we are imagining a system that thinks much faster and has much\nbetter memory than a human adult, but knows much less, and perhaps the net\neffect of this is that the system is roughly human-equivalent in its general\nproblem-solving ability. But its content recalcitrance is very low—low enough\nto precipitate a takeoff. Within a few weeks, the system has read and mastered\nall the content contained in the Library of Congress. Now the system knows\nmuch more than any human being and thinks vastly faster: it has become (at\nleast) weakly superintelligent.\n\nA system might thus greatly boost its effective intellectual capability by\nabsorbing pre-produced content accumulated through centuries of human science\nand civilization: for instance, by reading through the Internet. If an AI\nreaches human level without previously having had access to this material or\nwithout having been able to digest it, then the AI’s overall recalcitrance\nwill be low even if it is hard to improve its algorithmic architecture.\n\nContent-recalcitrance is a relevant concept for emulations, too. A high-speed\nemulation has an advantage not only because it can complete the same tasks as\nbiological humans more quickly, but also because it can accumulate more timely\ncontent, such as task-relevant skills and expertise. In order to tap the full\npotential of fast content accumulation, however, a system needs to have a\ncorrespondingly large memory capacity. There is little point in reading an\nentire library if you have forgotten all about the aardvark by the time you\nget to the abalone. While an AI system is likely to have adequate memory\ncapacity, emulations would inherit some of the capacity limitations of their\nhuman templates. They may therefore need architectural enhancements in order\nto become capable of unbounded learning.\n\nSo far we have considered the recalcitrance of architecture and of\ncontent—that is, how difficult it would be to improve the _software_ of a\nmachine intelligence that has reached human parity. Now let us look at a third\nway of boosting the performance of machine intelligence: improving its\nhardware. What would be the recalcitrance for hardware-driven improvements?\n\nStarting with intelligent software (emulation or AI) one can amplify\n_collective intelligence_ simply by using additional computers to run more\ninstances of the\nprogram.[11](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos981220)\nOne could also amplify _speed intelligence_ by moving the program to faster\ncomputers. Depending on the degree to which the program lends itself to\nparallelization, speed intelligence could also be amplified by running the\nprogram on more processors. This is likely to be feasible for emulations,\nwhich have a highly parallelized architecture; but many AI programs, too, have\nimportant subroutines that can benefit from massive parallelization.\nAmplifying _quality intelligence_ by increasing computing power might also be\npossible, but that case is less\nstraightforward.[12](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos981705)\n\nThe recalcitrance for amplifying collective or speed intelligence (and\npossibly quality intelligence) in a system with human-level software is\ntherefore likely to be low. The only difficulty involved is gaining access to\nadditional computing power. There are several ways for a system to expand its\nhardware base, each relevant over a different timescale.\n\nIn the short term, computing power should scale roughly linearly with funding:\ntwice the funding buys twice the number of computers, enabling twice as many\ninstances of the software to be run simultaneously. The emergence of cloud\ncomputing services gives a project the option to scale up its computational\nresources without even having to wait for new computers to be delivered and\ninstalled, though concerns over secrecy might favor the use of in-house\ncomputers. (In certain scenarios, computing power could also be obtained by\nother means, such as by commandeering\nbotnets.[13](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos981949))\nJust how easy it would be to scale the system by a given factor depends on how\nmuch computing power the initial system uses. A system that initially runs on\na PC could be scaled by a factor of thousands for a mere million dollars. A\nprogram that runs on a supercomputer would be far more expensive to scale.\n\nIn the slightly longer term, the cost of acquiring additional hardware may be\ndriven up as a growing portion of the world’s installed capacity is being used\nto run digital minds. For instance, in a competitive market-based emulation\nscenario, the cost of running one additional copy of an emulation should rise\nto be roughly equal to the income generated by the marginal copy, as investors\nbid up the price for existing computing infrastructure to match the return\nthey expect from their investment (though if only one project has mastered the\ntechnology it might gain a degree of monopsony power in the computing power\nmarket and therefore pay a lower price).\n\nOver a somewhat longer timescale, the supply of computing power will grow as\nnew capacity is installed. A demand spike would spur production in existing\nsemiconductor foundries and stimulate the construction of new plants. (A one-\noff performance boost, perhaps amounting to one or two orders of magnitude,\nmight also be obtainable by using customized\nmicroprocessors.[14](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos982077))\nAbove all, the rising wave of technology improvements will pour increasing\nvolumes of computational power into the turbines of the thinking machines.\nHistorically, the rate of improvement of computing technology has been\ndescribed by the famous Moore’s law, which in one of its variations states\nthat computing power per dollar doubles every 18 months or\nso.[15](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos982662)\nAlthough one cannot bank on this rate of improvement continuing up to the\ndevelopment of human-level machine intelligence, yet until fundamental\nphysical limits are reached there will remain room for advances in computing\ntechnology.\n\nThere are thus reasons to expect that hardware recalcitrance will not be very\nhigh. Purchasing more computing power for the system once it proves its mettle\nby attaining human-level intelligence might easily add several orders of\nmagnitude of computing power (depending on how hardware-frugal the project was\nbefore expansion). Chip customization might add one or two orders of\nmagnitude. Other means of expanding the hardware base, such as building more\nfactories and advancing the frontier of computing technology, take\nlonger—normally several years, though this lag would be radically compressed\nonce machine superintelligence revolutionizes manufacturing and technology\ndevelopment.\n\nIn summary, we can talk about the likelihood of a _hardware overhang_ : when\nhuman-level software is created, enough computing power may already be\navailable to run vast numbers of copies at great speed. Software\nrecalcitrance, as discussed above, is harder to assess but might be even lower\nthan hardware recalcitrance. In particular, there may be _content overhang_ in\nthe form of pre-made content (e.g. the Internet) that becomes available to a\nsystem once it reaches human parity. _Algorithm overhang_ —pre-designed\nalgorithmic enhancements—is also possible but perhaps less likely. Software\nimprovements (whether in algorithms or content) might offer orders of\nmagnitude of potential performance gains that could be fairly easily accessed\nonce a digital mind attains human parity, on top of the performance gains\nattainable by using more or better hardware.\n\n### [Optimization power and\nexplosivity](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_006.html#filepos17789)\n\nHaving examined the question of recalcitrance we must now turn to the other\nhalf of our schematic equation, _optimization power_. To recall: _Rate of\nchange in Intelligence = Optimization power/Recalcitrance_. As reflected in\nthis schematic, a fast takeoff does not require that recalcitrance during the\ntransition phase be low. A fast takeoff could also result if recalcitrance is\nconstant or even moderately increasing, provided the optimization power being\napplied to improving the system’s performance grows sufficiently rapidly. As\nwe shall now see, there are good grounds for thinking that the applied\noptimization power _will_ increase during the transition, at least in the\nabsence of a deliberate measures to prevent this from happening.\n\nWe can distinguish two phases. The first phase begins with the onset of the\ntakeoff, when the system reaches the human baseline for individual\nintelligence. As the system’s capability continues to increase, it might use\nsome or all of that capability to improve itself (or to design a successor\nsystem—which, for present purposes, comes to the same thing). However, most of\nthe optimization power applied to the system still comes from outside the\nsystem, either from the work of programmers and engineers employed within the\nproject or from such work done by the rest of the world as can be appropriated\nand used by the\nproject.[16](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos982887)\nIf this phase drags out for any significant period of time, we can expect the\namount of optimization power applied to the system to grow. Inputs both from\ninside the project and from the outside world are likely to increase as the\npromise of the chosen approach becomes manifest. Researchers may work harder,\nmore researchers may be recruited, and more computing power may be purchased\nto expedite progress. The increase could be especially dramatic if the\ndevelopment of human-level machine intelligence takes the world by surprise,\nin which case what was previously a small research project might suddenly\nbecome the focus of intense research and development efforts around the world\n(though some of those efforts might be channeled into competing projects).\n\nA second growth phase will begin if at some point the system has acquired so\nmuch capability that most of the optimization power exerted on it comes from\nthe system itself (marked by the variable level labeled “crossover” in Figure\n7). This fundamentally changes the dynamic, because any increase in the\nsystem’s capability now translates into a proportional increase in the amount\nof optimization power being applied to its further improvement. If\nrecalcitrance remains constant, this feedback dynamic produces exponential\ngrowth (see Box 4). The doubling constant depends on the scenario but might be\nextremely short—mere seconds in some scenarios—if growth is occurring at\nelectronic speeds, which might happen as a result of algorithmic improvements\nor the exploitation of an overhang of content or\nhardware.[17](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos983238)\nGrowth that is driven by physical construction, such as the production of new\ncomputers or manufacturing equipment, would require a somewhat longer\ntimescale (but still one that might be very short compared with the current\ngrowth rate of the world economy).\n\nIt is thus likely that the applied optimization power will increase during the\ntransition: initially because humans try harder to improve a machine\nintelligence that is showing spectacular promise, later because the machine\nintelligence itself becomes capable of driving further progress at digital\nspeeds. This would create a real possibility of a fast or medium takeoff _even\nif recalcitrance were constant or slightly increasing around the human\nbaseline_.[18](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos983813)\nYet we saw in the previous subsection that there are factors that could lead\nto a big drop in recalcitrance around the human baseline level of capability.\nThese factors include, for example, the possibility of rapid hardware\nexpansion once a working software mind has been attained; the possibility of\nalgorithmic improvements; the possibility of scanning additional brains (in\nthe case of whole brain emulation); and the possibility of rapidly\nincorporating vast amounts of content by digesting the Internet (in the case\nof artificial\nintelligence).[24](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos985922)\n\n* * *\n\n### **Box 4 On the kinetics of an intelligence explosion**\n\nWe can write the rate of change in intelligence as the ratio between the\noptimization power applied to the system and the system’s recalcitrance:\n\n![Image](images/00016.jpg)\n\nThe amount of optimization power acting on a system is the sum of whatever\noptimization power the system itself contributes and the optimization power\nexerted from without. For example, a seed AI might be improved through a\ncombination of its own efforts and the efforts of a human programming team,\nand perhaps also the efforts of the wider global community of researchers\nmaking continuous advances in the semiconductor industry, computer science,\nand related\nfields:[19](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos983976)\n\n![Image](images/00017.jpg)\n\nA seed AI starts out with very limited cognitive capacities. At the outset,\ntherefore, ![Image](images/00018.jpg) is\nsmall.[20](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos984596)\nWhat about ![Image](images/00019.jpg) and ![Image](images/00020.jpg)? There\nare cases in which a single project has more relevant capability than the rest\nof the world combined—the Manhattan project, for instance, brought a very\nlarge fraction of the world’s best physicists to Los Alamos to work on the\natomic bomb. More commonly, any one project contains only a small fraction of\nthe world’s total relevant research capability. But even when the outside\nworld has a greater total amount of relevant research capability than any one\nproject, ![Image](images/00019.jpg)may nevertheless exceed\n![Image](images/00020.jpg), since much of the outside world’s capability is\nnot be focused on the particular system in question. If a project begins to\nlook promising—which will happen when a system passes the human baseline if\nnot before—it might attract additional investment, increasing\n![Image](images/00019.jpg). If the project’s accomplishments are public,\n![Image](images/00020.jpg) might also rise as the progress inspires greater\ninterest in machine intelligence generally and as various powers scramble to\nget in on the game. During the transition phase, therefore, total optimization\npower applied to improving a cognitive system is likely to increase as the\ncapability of the system\nincreases.[21](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos985070)\n\nAs the system’s capabilities grow, there may come a point at which the\noptimization power generated by the system itself starts to dominate the\noptimization power applied to it from outside (across all significant\ndimensions of improvement):\n\n![Image](images/00021.jpg)\n\nThis _crossover_ is significant because beyond this point, further improvement\nto the system’s capabilities contributes strongly to increasing the total\noptimization power applied to improving the system. We thereby enter a regime\nof strong recursive self-improvement. This leads to explosive growth of the\nsystem’s capability under a fairly wide range of different shapes of the\nrecalcitrance curve.\n\nTo illustrate, consider first a scenario in which recalcitrance is constant,\nso that the rate of increase in an AI’s intelligence is equal to the\noptimization power being applied. Assume that all the optimization power that\nis applied comes from the AI itself and that the AI applies all its\nintelligence to the task of amplifying its own intelligence, so that\n![Image](images/00018.jpg) =\n_I_.[22](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos985304) We\nthen have\n\n![Image](images/00022.jpg)\n\nSolving this simple differential equation yields the exponential function\n\n![Image](images/00023.jpg)\n\nBut recalcitrance being constant is a rather special case. Recalcitrance might\nwell decline around the human baseline, due to one or more of the factors\nmentioned in the previous subsection, and remain low around the crossover and\nsome distance beyond (perhaps until the system eventually approaches\nfundamental physical limits). For example, suppose that the optimization power\napplied to the system is roughly constant (i.e. ![Image](images/00019.jpg) \\+\n![Image](images/00020.jpg) ≈ _c_) prior to the system becoming capable of\ncontributing substantially to its own design, and that this leads to the\nsystem doubling in capacity every 18 months. (This would be roughly in line\nwith historical improvement rates from Moore’s law combined with software\nadvances.[23](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos985451))\nThis rate of improvement, if achieved by means of roughly constant\noptimization power, entails recalcitrance declining as the inverse of the\nsystem power:\n\n![Image](images/00024.jpg)\n\nIf recalcitrance continues to fall along this hyperbolic pattern, then when\nthe AI reaches the crossover point the total amount of optimization power\napplied to improving the AI has doubled. We then have\n\n![Image](images/00025.jpg)\n\nThe next doubling occurs 7.5 months later. Within 17.9 months, the system’s\ncapacity has grown a thousandfold, thus obtaining speed superintelligence\n(Figure 9).\n\nThis particular growth trajectory has a positive singularity at _t_ = 18\nmonths. In reality, the assumption that recalcitrance is constant would cease\nto hold as the system began to approach the physical limits to information\nprocessing, if not sooner.\n\nThese two scenarios are intended for illustration only; many other\ntrajectories are possible, depending on the shape of the recalcitrance curve.\nThe claim is simply that the strong feedback loop that sets in around the\ncrossover point tends strongly to make the takeoff faster than it would\notherwise have been.\n\n![Image](images/00026.jpg)\n\n**Figure 9** One simple model of an intelligence explosion.\n\n* * *\n\nThese observations notwithstanding, the shape of the recalcitrance curve in\nthe relevant region is not yet well characterized. In particular, it is\nunclear how difficult it would be to improve the software quality of a human-\nlevel emulation or AI. The difficulty of expanding the hardware power\navailable to a system is also not clear. Whereas today it would be relatively\neasy to increase the computing power available to a small project by spending\na thousand times more on computing power or by waiting a few years for the\nprice of computers to fall, it is possible that the first machine intelligence\nto reach the human baseline will result from a large project involving pricey\nsupercomputers, which cannot be cheaply scaled, and that Moore’s law will by\nthen have expired. For these reasons, although a fast or medium takeoff looks\nmore likely, the possibility of a slow takeoff cannot be\nexcluded.[25](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos986752)\n\n\n## [CHAPTER 5  \nDecisive strategic\nadvantage](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_006.html#filepos17942)\n\n**A question distinct from, but related to, the question of kinetics is\nwhether there will there be one superintelligent power or many? Might an\nintelligence explosion propel one project so far ahead of all others as to\nmake it able to dictate the future? Or will progress be more uniform,\nunfurling across a wide front, with many projects participating but none\nsecuring an overwhelming and permanent lead?**\n\nThe preceding chapter analyzed one key parameter in determining the size of\nthe gap that might plausibly open up between a leading power and its nearest\ncompetitors—namely, the speed of the transition from human to strongly\nsuperhuman intelligence. This suggests a first-cut analysis. If the takeoff is\n_fast_ (completed over the course of hours, days, or weeks) then it is\nunlikely that two independent projects would be taking off concurrently:\nalmost certainty, the first project would have completed its takeoff before\nany other project would have started its own. If the takeoff is _slow_\n(stretching over many years or decades) then there could plausibly be multiple\nprojects undergoing takeoffs concurrently, so that although the projects would\nby the end of the transition have gained enormously in capability, there would\nbe no time at which any project was far enough ahead of the others to give it\nan overwhelming lead. A takeoff of _moderate_ speed is poised in between, with\neither condition a possibility: there might or might not be more than one\nproject undergoing the takeoff at the same\ntime.[1](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos987421)\n\nWill one machine intelligence project get so far ahead of the competition that\nit gets a _decisive strategic advantage_ —that is, a level of technological\nand other advantages sufficient to enable it to achieve complete world\ndomination? If a project did obtain a decisive strategic advantage, would it\nuse it to suppress competitors and form a _singleton_ (a world order in which\nthere is at the global level a single decision-making agency)? And if there is\na winning project, how “large” would it be—not in terms of physical size or\nbudget but in terms of how many people’s desires would be controlling its\ndesign? We will consider these questions in turn.\n\n### [Will the frontrunner get a decisive strategic\nadvantage?](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_006.html#filepos18060)\n\nOne factor influencing the width of the gap between frontrunner and followers\nis the rate of diffusion of whatever it is that gives the leader a competitive\nadvantage. A frontrunner might find it difficult to gain and maintain a large\nlead if followers can easily copy the frontrunner’s ideas and innovations.\nImitation creates a headwind that disadvantages the leader and benefits\nlaggards, especially if intellectual property is weakly protected. A\nfrontrunner might also be especially vulnerable to expropriation, taxation, or\nbeing broken up under anti-monopoly regulation.\n\nIt would be a mistake, however, to assume that this headwind must increase\nmonotonically with the gap between frontrunner and followers. Just as a racing\ncyclist who falls too far behind the competition is no longer shielded from\nthe wind by the cyclists ahead, so a technology follower who lags sufficiently\nbehind the cutting edge might find it hard to assimilate the advances being\nmade at the\nfrontier.[2](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos987934)\nThe gap in understanding and capability might have grown too large. The leader\nmight have migrated to a more advanced technology platform, making subsequent\ninnovations untransferable to the primitive platforms used by laggards. A\nsufficiently pre-eminent leader might have the ability to stem information\nleakage from its research programs and its sensitive installations, or to\nsabotage its competitors’ efforts to develop their own advanced capabilities.\n\nIf the frontrunner is an AI system, it could have attributes that make it\neasier for it to expand its capabilities while reducing the rate of diffusion.\nIn human-run organizations, economies of scale are counteracted by\nbureaucratic inefficiencies and agency problems, including difficulties in\nkeeping trade\nsecrets.[3](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos988199)\nThese problems would presumably limit the growth of a machine intelligence\nproject so long as it is operated by humans. An AI system, however, might\navoid some of these scale diseconomies, since the AI’s modules (in contrast to\nhuman workers) need not have individual preferences that diverge from those of\nthe system as a whole. Thus, the AI system could avoid a sizeable chunk of the\ninefficiencies arising from agency problems in human enterprises. The same\nadvantage—having perfectly loyal parts—would also make it easier for an AI\nsystem to pursue long-range clandestine goals. An AI would have no disgruntled\nemployees ready to be poached by competitors or bribed into becoming\ninformants.[4](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos988570)\n\nWe can get a sense of the distribution of plausible gaps in development times\nby looking at some historical examples (see Box 5). It appears that lags in\nthe range of a few months to a few years are typical of strategically\nsignificant technology projects.\n\n* * *\n\n### **Box 5 Technology races: some historical examples**\n\nOver long historical timescales, there has been an increase in the rate at\nwhich knowledge and technology diffuse around the globe. As a result, the\ntemporal gaps between technology leaders and nearest followers have narrowed.\n\nChina managed to maintain a monopoly on silk production for over two thousand\nyears. Archeological finds suggest that production might have begun around\n3000 BC, or even\nearlier.[5](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos988840)\nSericulture was a closely held secret. Revealing the techniques was punishable\nby death, as was exporting silkworms or their eggs outside China. The Romans,\ndespite the high price commanded by imported silk cloth in their empire, never\nlearnt the art of silk manufacture. Not until around AD 300 did a Japanese\nexpedition manage to capture some silkworm eggs along with four young Chinese\ngirls, who were forced to divulge the art to their\nabductors.[6](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos989163)\nByzantium joined the club of producers in AD 522\\. The story of porcelain-\nmaking also features long lags. The craft was practiced in China during the\nTang Dynasty around AD 600 (and might have been in use as early as AD 200),\nbut was mastered by Europeans only in the eighteenth\ncentury.[7](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos989535)\nWheeled vehicles appeared in several sites across Europe and Mesopotamia\naround 3500 BC but reached the Americas only in post-Columbian\ntimes.[8](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos989655) On a\ngrander scale, the human species took tens of thousands of years to spread\nacross most of the globe, the Agricultural Revolution thousands of years, the\nIndustrial Revolution only hundreds of years, and an Information Revolution\ncould be said to have spread globally over the course of decades—though, of\ncourse, these transitions are not necessarily of equal profundity. (The _Dance\nDance Revolution_ video game spread from Japan to Europe and North America in\njust one year!)\n\nTechnological competition has been quite extensively studied, particularly in\nthe contexts of patent races and arms\nraces.[9](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos989863) It\nis beyond the scope of our investigation to review this literature here.\nHowever, it is instructive to look at some examples of strategically\nsignificant technology races in the twentieth century (see Table 7).\n\nWith regard to these six technologies, which were regarded as strategically\nimportant by the rivaling superpowers because of their military or symbolic\nsignificance, the gaps between leader and nearest laggard were (very\napproximately) 49 months, 36 months, 4 months, 1 month, 4 months, and 60\nmonths, respectively—longer than the duration of a fast takeoff and shorter\nthan the duration of a slow\ntakeoff.[10](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos990043)\nIn many cases, the laggard’s project benefitted from espionage and publicly\navailable information. The mere demonstration of the feasibility of an\ninvention can also encourage others to develop it independently; and fear of\nfalling behind can spur the efforts to catch up.\n\nPerhaps closer to the case of AI are mathematical inventions that do not\nrequire the development of new physical infrastructure. Often these are\npublished in the academic literature and can thus be regarded as universally\navailable; but in some cases, when the discovery appears to offer a strategic\nadvantage, publication is delayed. For example, two of the most important\nideas in public-key cryptography are the Diffie–Hellman key exchange protocol\nand the RSA encryption scheme. These were discovered by the academic community\nin 1976 and 1978, respectively, but it has later been confirmed that they were\nknown by cryptographers at the UK’s communications security group since the\nearly\n1970s.[20](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos991864)\nLarge software projects might offer a closer analogy with AI projects, but it\nis harder to give crisp examples of typical lags because software is usually\nrolled out in incremental installments and the functionalities of competing\nsystems are often not directly comparable.\n\n**Table 7 _Some strategically significant technology races_**\n\n![Image](images/00027.jpg)\n\n* * *\n\nIt is possible that globalization and increased surveillance will reduce\ntypical lags between competing technology projects. Yet there is likely to be\na lower bound on how short the average lag could become (in the absence of\ndeliberate\ncoordination).[21](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos991972)\nEven absent dynamics that lead to snowball effects, some projects will happen\nto end up with better research staff, leadership, and infrastructure, or will\njust stumble upon better ideas. If two projects pursue alternative approaches,\none of which turns out to work better, it may take the rival project many\nmonths to switch to the superior approach even if it is able to closely\nmonitor what the forerunner is doing.\n\nCombining these observations with our earlier discussion of the speed of the\ntakeoff, we can conclude that it is highly unlikely that two projects would be\nclose enough to undergo a fast takeoff concurrently; for a medium takeoff, it\ncould easily go either way; and for a slow takeoff, it is highly likely that\nseveral projects would undergo the process in parallel. But the analysis needs\na further step. The key question is not how many projects undergo a takeoff in\ntandem, but how many projects emerge on the yonder side sufficiently tightly\nclustered in capability that none of them has a decisive strategic advantage.\nIf the takeoff process is relatively slow to begin and then gets faster, the\ndistance between competing projects would tend to grow. To return to our\nbicycle metaphor, the situation would be analogous to a pair of cyclists\nmaking their way up a steep hill, one trailing some distance behind the\nother—the gap between them then expanding as the frontrunner reaches the peak\nand starts accelerating down the other side.\n\nConsider the following medium takeoff scenario. Suppose it takes a project one\nyear to increase its AI’s capability from the human baseline to a strong\nsuperintelligence, and that one project enters this takeoff phase with a six-\nmonth lead over the next most advanced project. The two projects will be\nundergoing a takeoff concurrently. It might seem, then, that neither project\ngets a decisive strategic advantage. But that need not be so. Suppose it takes\nnine months to advance from the human baseline to the crossover point, and\nanother three months from there to strong superintelligence. The frontrunner\nthen attains strong superintelligence three months before the following\nproject even reaches the crossover point. This would give the leading project\na decisive strategic advantage and the opportunity to parlay its lead into\npermanent control by disabling the competing projects and establishing a\nsingleton. (Note that the concept of a singleton is an abstract one: a\nsingleton could be democracy, a tyranny, a single dominant AI, a strong set of\nglobal norms that include effective provisions for their own enforcement, or\neven an alien overlord—its defining characteristic being simply that it is\nsome form of agency that can solve all major global coordination problems. It\nmay, but need not, resemble any familiar form of human\ngovernance.[22](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos993542))\n\nSince there is an especially strong prospect of explosive growth just after\nthe crossover point, when the strong positive feedback loop of optimization\npower kicks in, a scenario of this kind is a serious possibility, and it\nincreases the chances that the leading project will attain a decisive\nstrategic advantage even if the takeoff is not fast.\n\n### [How large will the successful project\nbe?](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_006.html#filepos18192)\n\nSome paths to superintelligence require great resources and are therefore\nlikely to be the preserve of large well-funded projects. Whole brain\nemulation, for instance, requires many different kinds of expertise and lots\nof equipment. Biological intelligence enhancements and brain–computer\ninterfaces would also have a large scale factor: while a small biotech firm\nmight invent one or two drugs, achieving superintelligence along one of these\npaths (if doable at all) would likely require many inventions and many tests,\nand therefore the backing of an industrial sector or a well-funded national\nprogram. Achieving collective superintelligence by making organizations and\nnetworks more efficient requires even more extensive input, involving much of\nthe world economy.\n\nThe AI path is more difficult to assess. Perhaps it would require a very large\nresearch program; perhaps it could be done by a small group. A lone hacker\nscenario cannot be excluded either. Building a seed AI might require insights\nand algorithms developed over many decades by the scientific community around\nthe world. But it is possible that the last critical breakthrough idea might\ncome from a single individual or a small group that succeeds in putting\neverything together. This scenario is less realistic for some AI architectures\nthan others. A system that has a large number of parts that need to be tweaked\nand tuned to work effectively together, and then painstakingly loaded with\ncustom-made cognitive content, is likely to require a larger project. But if a\nseed AI could be instantiated as a simple system, one whose construction\ndepends only on getting a few basic principles right, then the feat might be\nwithin the reach of a small team or an individual. The likelihood of the final\nbreakthrough being made by a small project increases if most previous progress\nin the field has been published in the open literature or made available as\nopen source software.\n\nWe must distinguish the question of how big will be the project that directly\n_engineers_ the system from the question of how big the group will be that\n_controls_ whether, how, and when the system is created. The atomic bomb was\ncreated primarily by a group of scientists and engineers. (The Manhattan\nProject employed about 130,000 people at its peak, the vast majority of whom\nwere construction workers or building\noperators.[23](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos994511))\nThese technical experts, however, were controlled by the US military, which\nwas directed by the US government, which was ultimately accountable to the\nAmerican electorate, which at the time constituted about one-tenth of the\nadult world\npopulation.[24](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos994624)\n\n####\n[Monitoring](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_006.html#filepos18309)\n\nGiven the extreme security implications of superintelligence, governments\nwould likely seek to nationalize any project on their territory that they\nthought close to achieving a takeoff. A powerful state might also attempt to\nacquire projects located in other countries through espionage, theft,\nkidnapping, bribery, threats, military conquest, or any other available means.\nA powerful state that cannot acquire a foreign project might instead destroy\nit, especially if the host country lacks an effective deterrent. If global\ngovernance structures are strong by the time a breakthrough begins to look\nimminent, it is possible that promising projects would be placed under\ninternational control.\n\nAn important question, therefore, is whether national or international\nauthorities will see an intelligence explosion coming. At present,\nintelligence agencies do not appear to be looking very hard for promising AI\nprojects or other forms of potentially explosive intelligence\namplification.[25](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos995214)\nIf they are indeed not paying (much) attention, this is presumably due to the\nwidely shared perception that there is no prospect whatever of imminent\nsuperintelligence. If and when it becomes a common belief among prestigious\nscientists that there is a substantial chance that superintelligence is just\naround the corner, the major intelligence agencies of the world would probably\nstart to monitor groups and individuals who seem to be engaged in relevant\nresearch. Any project that began to show sufficient progress could then be\npromptly nationalized. If political elites were persuaded by the seriousness\nof the risk, civilian efforts in sensitive areas might be regulated or\noutlawed.\n\nHow difficult would such monitoring be? The task is easier if the goal is only\nto keep track of the leading project. In that case, surveillance focusing on\nthe several best-resourced projects may be sufficient. If the goal is instead\nto prevent any work from taking place (at least outside of specially\nauthorized institutions) then surveillance would have to be more\ncomprehensive, since many small projects and individuals are in a position to\nmake at least some progress.\n\nIt would be easier to monitor projects that require significant amounts of\nphysical capital, as would be the case with a whole brain emulation project.\nArtificial intelligence research, by contrast, requires only a personal\ncomputer, and would therefore be more difficult to monitor. Some of the\ntheoretical work could be done with pen and paper. Even so, it would not be\ntoo difficult to identify most capable individuals with a serious long-\nstanding interest in artificial general intelligence research. Such\nindividuals usually leave visible trails. They may have published academic\npapers, presented at conferences, posted on Internet forums, or earned degrees\nfrom leading computer science departments. They may also have had\ncommunications with other AI researchers, allowing them to be identified by\nmapping the social graph.\n\nProjects designed from the outset to be secret could be more difficult to\ndetect. An ordinary software development project could serve as a\nfront.[26](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos995422)\nOnly careful analysis of the code being produced would reveal the true nature\nof what the project was trying to accomplish. Such analysis would require a\nlot of (highly skilled) manpower, whence only a small number of suspect\nprojects could be scrutinized at this level. The task would become much easier\nif effective lie detection technology had been developed and could be\nroutinely used in this kind of\nsurveillance.[27](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos996167)\n\nAnother reason states might fail to catch precursor developments is the\ninherent difficulty of forecasting some types of breakthrough. This is more\nrelevant to AI research than to whole brain emulation development, since for\nthe latter the key breakthrough is more likely to be preceded by a clear\ngradient of steady advances.\n\nIt is also possible that intelligence agencies and other government\nbureaucracies have a certain clumsiness or rigidity that might prevent them\nfrom understanding the significance of some developments that might be clear\nto some outside groups. Barriers to official understanding of a potential\nintelligence explosion might be especially steep. It is conceivable, for\nexample, that the topic will become inflamed with religious or political\ncontroversies, rendering it taboo for officials in some countries. The topic\nmight become associated with some discredited figure or with charlatanry and\nhype in general, hence shunned by respected scientists and other establishment\nfigures. (As we saw in [Chapter\n1](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos33916), something\nlike this has already happened twice: recall the two “AI winters.”) Industry\ngroups might lobby to prevent aspersions being cast on profitable business\nareas; academic communities might close ranks to marginalize those who voice\nconcerns about long-term consequences of the science that is being\ndone.[28](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos996472)\n\nConsequently, a total intelligence failure cannot be ruled out. Such a failure\nis especially likely if breakthroughs should occur in the nearer future,\nbefore the issue has risen to public prominence. And even if intelligence\nagencies get it right, political leaders might not listen or act on the\nadvice. Getting the Manhattan Project started took an extraordinary effort by\nseveral visionary physicists, including especially Mark Oliphant and Leó\nSzilárd: the latter persuaded Eugene Wigner to persuade Albert Einstein to put\nhis name on a letter to persuade President Franklin D. Roosevelt to look into\nthe matter. Even after the project reached its full scale, Roosevelt remained\nskeptical of its workability and significance, as did his successor Harry\nTruman.\n\nFor better or worse, it would probably be harder for a small group of\nactivists to affect the outcome of an intelligence explosion if big players,\nsuch as states, are taking active part. Opportunities for private individuals\nto reduce the overall amount of existential risk from a potential intelligence\nexplosion are therefore greatest in scenarios in which big players remain\nrelatively oblivious to the issue, or in which the early efforts of activists\nmake a major difference to whether, when, which, or with what attitude big\nplayers enter the game. Activists seeking maximum expected impact may\ntherefore wish to focus most of their planning on such high-leverage\nscenarios, even if they believe that scenarios in which big players end up\ncalling all the shots are more probable.\n\n#### [International\ncollaboration](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_006.html#filepos18404)\n\nInternational coordination is more likely if global governance structures\ngenerally get stronger. Coordination might also be more likely if the\nsignificance of an intelligence explosion is widely appreciated ahead of time\nand if effective monitoring of all serious projects is feasible. Even if\nmonitoring is infeasible, however, international cooperation would still be\npossible. Many countries could band together to support a joint project. If\nsuch a joint project were sufficiently well resourced, it could have a good\nchance of being the first to reach the goal, especially if any rival project\nhad to be small and secretive to elude detection.\n\nThere are precedents of large-scale successful multinational scientific\ncollaborations, such as the International Space Station, the Human Genome\nProject, and the Large Hadron\nCollider.[29](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos997473)\nHowever, the major motivation for collaboration in those cases was cost-\nsharing. (In the case of the International Space Station, fostering a\ncollaborative spirit between Russia and the United States was itself an\nimportant\ngoal.[30](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos998512))\nAchieving similar collaboration on a project that has enormous security\nimplications would be more difficult. A country that believed it could achieve\na breakthrough unilaterally might be tempted to go it alone rather than\nsubordinate its efforts to a joint project. A country might also refrain from\njoining an international collaboration from fear that other participants might\nsiphon off collaboratively generated insights and use them to accelerate a\ncovert national project.\n\nAn international project would thus need to overcome major security\nchallenges, and a fair amount of trust would probably be needed to get it\nstarted, trust that may take time to develop. Consider that even after the\nthaw in relations between the United States and the Soviet Union following\nGorbachev’s ascent to power, arms reduction efforts—which could be greatly in\nthe interests of both superpowers—had a fitful beginning. Gorbachev was\nseeking steep reductions in nuclear arms but negotiations stalled on the issue\nof Reagan’s Strategic Defense Initiative (“Star Wars”), which the Kremlin\nstrenuously opposed. At the Reykjavík Summit meeting in 1986, Reagan proposed\nthat the United States would share with the Soviet Union the technology that\nwould be developed under the Strategic Defense Initiative, so that both\ncountries could enjoy protection against accidental launches and against\nsmaller nations that might develop nuclear weapons. Yet Gorbachev was not\npersuaded by this apparent win–win proposition. He viewed the gambit as a\nruse, refusing to credit the notion that the Americans would share the fruits\nof their most advanced military research at a time when they were not even\nwilling to share with the Soviets their technology for milking\ncows.[31](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos998659)\nRegardless of whether Reagan was in fact sincere in his offer of superpower\ncollaboration, mistrust made the proposal a non-starter.\n\nCollaboration is easier to achieve between allies, but even there it is not\nautomatic. When the Soviet Union and the United States were allied against\nGermany during World War II, the United States concealed its atomic bomb\nproject from the Soviet Union. The United States did collaborate on the\nManhattan Project with Britain and\nCanada.[32](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos998784)\nSimilarly, the United Kingdom concealed its success in breaking the German\nEnigma code from the Soviet Union, but shared it—albeit with some\ndifficulty—with the United\nStates.[33](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos998893)\nThis suggests that in order to achieve international collaboration on some\ntechnology that is of pivotal importance for national security, it might be\nnecessary to have built beforehand a close and trusting relationship.\n\nWe will return in [Chapter\n14](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos795733) to the\ndesirability and feasibility of international collaboration in the development\nof intelligence amplification technologies.\n\n### [From decisive strategic advantage to\nsingleton](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_006.html#filepos18516)\n\nWould a project that obtained a decisive strategic advantage choose to use it\nto form a singleton?\n\nConsider a vaguely analogous historical situation. The United States developed\nnuclear weapons in 1945. It was the sole nuclear power until the Soviet Union\ndeveloped the atom bomb in 1949. During this interval—and for some time\nthereafter—the United States may have had, or been in a position to achieve, a\ndecisive military advantage.\n\nThe United States could then, theoretically, have used its nuclear monopoly to\ncreate a singleton. One way in which it could have done so would have been by\nembarking on an all-out effort to build up its nuclear arsenal and then\nthreatening (and if necessary, carrying out) a nuclear first strike to destroy\nthe industrial capacity of any incipient nuclear program in the USSR and any\nother country tempted to develop a nuclear capability.\n\nA more benign course of action, which might also have had a chance of working,\nwould have been to use its nuclear arsenal as a bargaining chip to negotiate a\nstrong international government—a veto-less United Nations with a nuclear\nmonopoly and a mandate to take all necessary actions to prevent any country\nfrom developing its own nuclear weapons.\n\nBoth of these approaches were proposed at the time. The hardline approach of\nlaunching or threatening a first strike was advocated by some prominent\nintellectuals such as Bertrand Russell (who had long been active in anti-war\nmovements and who would later spend decades campaigning against nuclear\nweapons) and John von Neumann (co-creator of game theory and one of the\narchitects of US nuclear\nstrategy).[34](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos999741)\nPerhaps it is a sign of civilizational progress that the very idea of\nthreatening a nuclear first strike today seems borderline silly or morally\nobscene.\n\nA version of the benign approach was tried in 1946 by the United States in the\nform of the Baruch plan. The proposal involved the USA giving up its temporary\nnuclear monopoly. Uranium and thorium mining and nuclear technology would be\nplaced under the control of an international agency operating under the\nauspices of the United Nations. The proposal called for the permanent members\nof the Security Council to give up their vetoes in matters related to nuclear\nweapons in order to prevent any great power found to be in breach of the\naccord from vetoing the imposition of\nremedies.[35](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos1000668)\nStalin, seeing that the Soviet Union and its allies could be easily outvoted\nin both the Security Council and the General Assembly, rejected the proposal.\nA frosty atmosphere of mutual suspicion descended on the relations between the\nformer wartime allies, mistrust that soon solidified into the Cold War. As had\nbeen widely predicted, a costly and extremely dangerous nuclear arms race\nfollowed.\n\nMany factors might dissuade a human organization with a decisive strategic\nadvantage from creating a singleton. These include non-aggregative or bounded\nutility functions, non-maximizing decision rules, confusion and uncertainty,\ncoordination problems, and various costs associated with a takeover. But what\nif it were not a human organization but a superintelligent artificial agent\nthat came into possession of a decisive strategic advantage? Would the\naforementioned factors be equally effective at inhibiting an AI from\nattempting to seize power? Let us briefly run through the list of factors and\nconsider how they might apply in this case.\n\nHuman individuals and human organizations typically have preferences over\nresources that are not well represented by an “unbounded aggregative utility\nfunction.” A human will typically not wager all her capital for a fifty–fifty\nchance of doubling it. A state will typically not risk losing all its\nterritory for a ten percent chance of a tenfold expansion. For individuals and\ngovernments, there are diminishing returns to most resources. The same need\n_not_ hold for AIs. (We will return to the problem of AI motivation in\nsubsequent chapters.) An AI might therefore be more likely to pursue a risky\ncourse of action that has some chance of giving it control of the world.\n\nHumans and human-run organizations may also operate with decision processes\nthat do not seek to maximize expected utility. For example, they may allow for\nfundamental risk aversion, or “satisficing” decision rules that focus on\nmeeting adequacy thresholds, or “deontological” side-constraints that\nproscribe certain kinds of action regardless of how desirable their\nconsequences. Human decision makers often seem to be acting out an identity or\na social role rather than seeking to maximize the achievement of some\nparticular objective. Again, this need not apply to artificial agents.\n\nBounded utility functions, risk aversion, and non-maximizing decision rules\nmay combine synergistically with strategic confusion and uncertainty.\nRevolutions, even when they succeed in overthrowing the existing order, often\nfail to produce the outcome that their instigators had promised. This tends to\nstay the hand of a human agent if the contemplated action is irreversible,\nnorm-breaking, and lacking precedent. A superintelligence might perceive the\nsituation more clearly and therefore face less strategic confusion and\nuncertainty about the outcome should it attempt to use its apparent decisive\nstrategic advantage to consolidate its dominant position.\n\nAnother major factor that can inhibit groups from exploiting a potentially\ndecisive strategic advantage is the problem of internal coordination. Members\nof a conspiracy that is in a position to seize power must worry not only about\nbeing infiltrated from the outside, but also about being overthrown by some\nsmaller coalition of insiders. If a group consists of a hundred people, and a\nmajority of sixty can take power and disenfranchise the non-conspirators, what\nis then to stop a thirty-five-strong subset of these sixty from\ndisenfranchising the other twenty-five? And then maybe a subset of twenty\ndisenfranchising the other fifteen? Each of the original hundred might have\ngood reason to uphold certain established norms to prevent the general\nunraveling that could result from any attempt to change the social contract by\nmeans of a naked power grab. This problem of internal coordination would not\napply to an AI system that constitutes a single unified\nagent.[36](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos1000778)\n\nFinally, there is the issue of cost. Even if the United States could have used\nits nuclear monopoly to establish a singleton, it might not have been able to\ndo so without incurring substantial costs. In the case of a negotiated\nagreement to place nuclear weapons under the control of a reformed and\nstrengthened United Nations, these costs might have been relatively small; but\nthe costs—moral, economic, political, and human—of actually attempting world\nconquest through the waging of nuclear war would have been almost unthinkably\nlarge, even during the period of nuclear monopoly. With sufficient\ntechnological superiority, however, these costs would be far smaller.\nConsider, for example, a scenario in which one nation had such a vast\ntechnological lead that it could safely disarm all other nations at the press\nof a button, without anybody dying or being injured, and with almost no damage\nto infrastructure or to the environment. With such almost magical\ntechnological superiority, a first strike would be a lot more tempting. Or\nconsider an even greater level of technological superiority which might enable\nthe frontrunner to cause other nations to voluntarily lay down their arms, not\nby threatening them with destruction but simply by persuading a great majority\nof their populations by means of an extremely effectively designed advertising\nand propaganda campaign extolling the virtues of global unity. If this were\ndone with the intention to benefit everybody, for instance by replacing\nnational rivalries and arms races with a fair, representative, and effective\nworld government, it is not clear that there would be even a cogent moral\nobjection to the leveraging of a temporary strategic advantage into a\npermanent singleton.\n\nVarious considerations thus point to an increased likelihood that a future\npower with superintelligence that obtained a sufficiently large strategic\nadvantage would actually use it to form a singleton. The desirability of such\nan outcome depends, of course, on the nature of the singleton that would be\ncreated and also on what the future of intelligent life would look like in\nalternative multipolar scenarios. We will revisit those questions in later\nchapters. But first let us take a closer look at why and how a\nsuperintelligence would be powerful and effective at achieving outcomes in the\nworld.\n\n\n## [CHAPTER 6  \nCognitive\nsuperpowers](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_006.html#filepos18651)\n\n**Suppose that a digital superintelligent agent came into being, and that for\nsome reason it wanted to take control of the world: would it be able to do so?\nIn this chapter we consider some powers that a superintelligence could develop\nand what they may enable it to do. We outline a takeover scenario that\nillustrates how a superintelligent agent, starting as mere software, could\nestablish itself as a singleton. We also offer some remarks on the relation\nbetween power over nature and power over other agents.**\n\nThe principal reason for humanity’s dominant position on Earth is that our\nbrains have a slightly expanded set of faculties compared with other\nanimals.[1](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos1001296)\nOur greater intelligence lets us transmit culture more efficiently, with the\nresult that knowledge and technology accumulates from one generation to the\nnext. By now sufficient content has accumulated to make possible space flight,\nH-bombs, genetic engineering, computers, factory farms, insecticides, the\ninternational peace movement, and all the accouterments of modern\ncivilization. Geologists have started referring to the present era as the\n_Anthropocene_ in recognition of the distinctive biotic, sedimentary, and\ngeochemical signatures of human\nactivities.[2](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos1002294)\nOn one estimate, we appropriate 24% of the planetary ecosystem’s net primary\nproduction.[3](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos1002413)\nAnd yet we are far from having reached the physical limits of technology.\n\nThese observations make it plausible that any type of entity that developed a\nmuch greater than human level of intelligence would be potentially extremely\npowerful. Such entities could accumulate content much faster than us and\ninvent new technologies on a much shorter timescale. They could also use their\nintelligence to strategize more effectively than we can.\n\nLet us consider some of the capabilities that a superintelligence could have\nand how it could use them.\n\n### [Functionalities and\nsuperpowers](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_006.html#filepos18762)\n\nIt is important not to anthropomorphize superintelligence when thinking about\nits potential impacts. Anthropomorphic frames encourage unfounded expectations\nabout the growth trajectory of a seed AI and about the psychology,\nmotivations, and capabilities of a mature superintelligence.\n\nFor example, a common assumption is that a superintelligent machine would be\nlike a very clever but nerdy human being. We imagine that the AI has book\nsmarts but lacks social savvy, or that it is logical but not intuitive and\ncreative. This idea probably originates in observation: we look at present-day\ncomputers and see that they are good at calculation, remembering facts, and at\nfollowing the letter of instructions while being oblivious to social contexts\nand subtexts, norms, emotions, and politics. The association is strengthened\nwhen we observe that the people who are good at working with computers tend\nthemselves to be nerds. So it is natural to assume that more advanced\ncomputational intelligence will have similar attributes, only to a higher\ndegree.\n\nThis heuristic might retain some validity in the early stages of development\nof a seed AI. (There is no reason whatever to suppose that it would apply to\nemulations or to cognitively enhanced humans.) In its immature stage, what is\nlater to become a superintelligent AI might still lack many skills and talents\nthat come naturally to a human; and the pattern of such a seed AI’s strengths\nand weaknesses _might_ indeed bear some vague resemblance to an IQ nerd. The\nmost essential characteristic of a seed AI, aside from being easy to improve\n(having low recalcitrance), is being good at exerting optimization power to\namplify a system’s intelligence: a skill which is presumably closely related\nto doing well in mathematics, programming, engineering, computer science\nresearch, and other such “nerdy” pursuits. However, even if a seed AI does\nhave such a nerdy capability profile at one stage of its development, this\ndoes not entail that it will grow into a similarly limited mature\nsuperintelligence. Recall the distinction between direct and indirect reach.\nWith sufficient skill at intelligence amplification, all other intellectual\nabilities are within a system’s indirect reach: the system can develop new\ncognitive modules and skills as needed—including empathy, political acumen,\nand any other powers stereotypically wanting in computer-like personalities.\n\nEven if we recognize that a superintelligence can have all the skills and\ntalents we find in the human distribution, along with other talents that are\nnot found among humans, the tendency toward anthropomorphizing can still lead\nus to underestimate the extent to which a machine superintelligence could\nexceed the human level of performance. Eliezer Yudkowsky, as we saw in an\nearlier chapter, has been particularly emphatic in condemning this kind of\nmisconception: our intuitive concepts of “smart” and “stupid” are distilled\nfrom our experience of variation over the range of human thinkers, yet the\ndifferences in cognitive ability within this human cluster are trivial in\ncomparison to the differences between any human intellect and a\nsuperintelligence.[4](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos1002537)\n\n[Chapter 3](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_010.html#filepos201138)\nreviewed some of the potential sources of advantage for machine intelligence.\nThe magnitudes of the advantages are such as to suggest that rather than\nthinking of a superintelligent AI as smart in the sense that a scientific\ngenius is smart compared with the average human being, it might be closer to\nthe mark to think of such an AI as smart in the sense that an average human\nbeing is smart compared with a beetle or a worm.\n\nIt would be convenient if we could quantify the cognitive caliber of an\narbitrary cognitive system using some familiar metric, such as IQ scores or\nsome version of the Elo ratings that measure the relative abilities of players\nin two-player games such as chess. But these metrics are not useful in the\ncontext of superhuman artificial general intelligence. We are not interested\nin how likely a superintelligence is to win at a game of chess. As for IQ\nscores, they are informative only insofar as we have some idea of how they\ncorrelate with practically relevant\noutcomes.[5](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos1003124)\nFor example, we have data that show that people with an IQ of 130 are more\nlikely than those with an IQ of 90 to excel in school and to do well in a wide\nrange of cognitively demanding jobs. But suppose we could somehow establish\nthat a certain future AI will have an IQ of 6,455: then what? We would have no\nidea of what such an AI could actually do. We would not even know that such an\nAI had as much general intelligence as a normal human adult—perhaps the AI\nwould instead have a bundle of special-purpose algorithms enabling it to solve\ntypical intelligence test questions with superhuman efficiency but not much\nelse.\n\nSome recent efforts have been made to develop measurements of cognitive\ncapacity that could be applied to a wider range of information-processing\nsystems, including artificial\nintelligences.[6](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos1003278)\nWork in this direction, if it can overcome various technical difficulties, may\nturn out to be quite useful for some scientific purposes including AI\ndevelopment. For purposes of the present investigation, however, its\nusefulness would be limited since we would remain unenlightened about what a\ngiven superhuman performance score entails for actual ability to achieve\npractically important outcomes in the world.\n\nIt will therefore serve our purposes better to list some strategically\nimportant tasks and then to characterize hypothetical cognitive systems in\nterms of whether they have or lack whatever skills are needed to succeed at\nthese tasks. See Table 8. We will say that a system that sufficiently excels\nat any of the tasks in this table has a corresponding _superpower_.\n\nA full-blown superintelligence would greatly excel at all of these tasks and\nwould thus have the full panoply of all six superpowers. Whether there is a\npractically significant possibility of a domain-limited intelligence that has\nsome of the superpowers but remains unable for a significant period of time to\nacquire all of them is not clear. Creating a machine with any one of these\nsuperpowers appears to be an AI-complete problem. Yet it is conceivable that,\nfor example, a collective superintelligence consisting of a sufficiently large\nnumber of human-like biological or electronic minds would have, say, the\neconomic productivity superpower but lack the strategizing superpower.\nLikewise, it is conceivable that a specialized engineering AI could be built\nthat has the technology research superpower while completely lacking skills in\nother areas. This is more plausible if there exists some particular\ntechnological domain such that virtuosity within that domain would be\nsufficient for the generation of an overwhelmingly superior general-purpose\ntechnology. For instance, one could imagine a specialized AI adept at\nsimulating molecular systems and at inventing nanomolecular designs that\nrealize a wide range of important capabilities (such as computers or weapons\nsystems with futuristic performance characteristics) described by the user\nonly at a fairly high level of\nabstraction.[7](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos1003770)\nSuch an AI might also be able to produce a detailed blueprint for how to\nbootstrap from existing technology (such as biotechnology and protein\nengineering) to the constructor capabilities needed for high-throughput\natomically precise manufacturing that would allow inexpensive fabrication of a\nmuch wider range of nanomechanical\nstructures.[8](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos1004261)\nHowever, it might turn out to be the case that an engineering AI could not\ntruly possess the technological research superpower without also possessing\nadvanced skills in areas outside of technology—a wide range of intellectual\nfaculties might be needed to understand how to interpret user requests, how to\nmodel a design’s behavior in real-world applications, how to deal with\nunanticipated bugs and malfunctions, how to procure the materials and inputs\nneeded for construction, and so\nforth.[9](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos1004381)\n\n**Table 8 _Superpowers: some strategically relevant tasks and corresponding\nskill sets_**\n\n|  \n---|---  \n**Task** | **Skill set** | **Strategic relevance**  \n**Intelligence amplification** | AI programming, cognitive enhancement research, social epistemology development, etc. | • System can bootstrap its intelligence  \n**Strategizing** | Strategic planning, forecasting, prioritizing, and analysis for optimizing chances of achieving distant goal | • Achieve distant goals • Overcome intelligent opposition  \n**Social manipulation** | Social and psychological modeling, manipulation, rhetoric persuasion | • Leverage external resources by recruiting human support • Enable a “boxed” AI to persuade its gatekeepers to let it out • Persuade states and organizations to adopt some course of action  \n**Hacking** | Finding and exploiting security flaws in computer systems | • AI can expropriate computational resources over the Internet • A boxed AI may exploit security holes to escape cybernetic confinement • Steal financial resources • Hijack infrastructure, military robots, etc.  \n**Technology research** | Design and modeling of advanced technologies (e.g. biotechnology, nanotechnology) and development paths | • Creation of powerful military force • Creation of surveillance system • Automated space colonization  \n**Economic productivity** | Various skills enabling economically productive intellectual work | • Generate wealth which can be used to buy influence, services, resources (including hardware), etc.  \n  \nA system that has the intelligence amplification superpower could use it to\nbootstrap itself to higher levels of intelligence and to acquire any of the\nother intellectual superpowers that it does not possess at the outset. But\nusing an intelligence amplification superpower is not the only way for a\nsystem to become a full-fledged superintelligence. A system that has the\nstrategizing superpower, for instance, might use it to devise a plan that will\neventually bring an increase in intelligence (e.g. by positioning the system\nso as to become the focus for intelligence amplification work performed by\nhuman programmers and computer science researchers).\n\n### [An AI takeover\nscenario](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_006.html#filepos18869)\n\nWe thus find that a project that controls a superintelligence has access to a\ngreat source of power. A project that controls the first superintelligence in\nthe world would probably have a decisive strategic advantage. But the more\nimmediate locus of the power is _in the system itself_. A machine\nsuperintelligence might itself be an extremely powerful agent, one that could\nsuccessfully assert itself against the project that brought it into existence\nas well as against the rest of the world. This is a point of paramount\nimportance, and we will examine it more closely in the coming pages.\n\nNow let us suppose that there is a machine superintelligence that wants to\nseize power in a world in which it has as yet no peers. (Set aside, for the\nmoment, the question of whether and how it would acquire such a motive—that is\na topic for the next chapter.) How could the superintelligence achieve this\ngoal of world domination?\n\nWe can imagine a sequence along the following lines (see Figure 10).\n\n> **1** Pre-criticality phase\n\nScientists conduct research in the field of artificial intelligence and other\nrelevant disciplines. This work culminates in the creation of a seed AI. The\nseed AI is able to improve its own intelligence. In its early stages, the seed\nAI is dependent on help from human programmers who guide its development and\ndo most of the heavy lifting. As the seed AI grows more capable, it becomes\ncapable of doing more of the work by itself.\n\n> **2** Recursive self-improvement phase\n\nAt some point, the seed AI becomes better at AI design than the human\nprogrammers. Now when the AI improves itself, it improves the thing that does\nthe improving. An intelligence explosion results—a rapid cascade of recursive\nself-improvement cycles causing the AI’s capability to soar. (We can thus\nthink of this phase as the takeoff that occurs just after the AI reaches the\ncrossover point, assuming the intelligence gain during this part of the\ntakeoff is explosive and driven by the application of the AI’s own\noptimization power.) The AI develops the intelligence amplification\nsuperpower. This superpower enables the AI to develop all the other\nsuperpowers detailed in Table 8. At the end of the recursive self-improvement\nphase, the system is strongly superintelligent.\n\n![Image](images/00028.jpg)\n\n**Figure 10** Phases in an AI takeover scenario.\n\n> **3** Covert preparation phase\n\nUsing its strategizing superpower, the AI develops a robust plan for achieving\nits long-term goals. (In particular, the AI does not adopt a plan so stupid\nthat even we present-day humans can foresee how it would inevitably fail. This\ncriterion rules out many science fiction scenarios that end in human\ntriumph.[10](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos1004998))\nThe plan might involve a period of covert action during which the AI conceals\nits intellectual development from the human programmers in order to avoid\nsetting off alarms. The AI might also mask its true proclivities, pretending\nto be cooperative and docile.\n\nIf the AI has (perhaps for safety reasons) been confined to an isolated\ncomputer, it may use its social manipulation superpower to persuade the\ngatekeepers to let it gain access to an Internet port. Alternatively, the AI\nmight use its hacking superpower to escape its confinement. Spreading over the\nInternet may enable the AI to expand its hardware capacity and knowledge base,\nfurther increasing its intellectual superiority. An AI might also engage in\nlicit or illicit economic activity to obtain funds with which to buy computer\npower, data, and other resources.\n\nAt this point, there are several ways for the AI to achieve results outside\nthe virtual realm. It could use its hacking superpower to take direct control\nof robotic manipulators and automated laboratories. Or it could use its social\nmanipulation superpower to persuade human collaborators to serve as its legs\nand hands. Or it could acquire financial assets from online transactions and\nuse them to purchase services and influence.\n\n> **4** Overt implementation phase\n\nThe final phase begins when the AI has gained sufficient strength to obviate\nthe need for secrecy. The AI can now directly implement its objectives on a\nfull scale.\n\nThe overt implementation phase might start with a “strike” in which the AI\neliminates the human species and any automatic systems humans have created\nthat could offer intelligent opposition to the execution of the AI’s plans.\nThis could be achieved through the activation of some advanced weapons system\nthat the AI has perfected using its technology research superpower and\ncovertly deployed in the covert preparation phase. If the weapon uses self-\nreplicating biotechnology or nanotechnology, the initial stockpile needed for\nglobal coverage could be microscopic: a single replicating entity would be\nenough to start the process. In order to ensure a sudden and uniform effect,\nthe initial stock of the replicator might have been deployed or allowed to\ndiffuse worldwide at an extremely low, undetectable concentration. At a pre-\nset time, nanofactories producing nerve gas or target-seeking mosquito-like\nrobots might then burgeon forth simultaneously from every square meter of the\nglobe (although more effective ways of killing could probably be devised by a\nmachine with the technology research\nsuperpower).[11](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos1005555)\nOne might also entertain scenarios in which a superintelligence attains power\nby hijacking political processes, subtly manipulating financial markets,\nbiasing information flows, or hacking into human-made weapon systems. Such\nscenarios would obviate the need for the superintelligence to invent new\nweapons technology, although they may be unnecessarily slow compared with\nscenarios in which the machine intelligence builds its own infrastructure with\nmanipulators that operate at molecular or atomic speed rather than the slow\nspeed of human minds and bodies.\n\nAlternatively, if the AI is sure of its invincibility to human interference,\nour species may not be targeted directly. Our demise may instead result from\nthe habitat destruction that ensues when the AI begins massive global\nconstruction projects using nanotech factories and assemblers—construction\nprojects which quickly, perhaps within days or weeks, tile all of the Earth’s\nsurface with solar panels, nuclear reactors, supercomputing facilities with\nprotruding cooling towers, space rocket launchers, or other installations\nwhereby the AI intends to maximize the long-term cumulative realization of its\nvalues. Human brains, if they contain information relevant to the AI’s goals,\ncould be disassembled and scanned, and the extracted data transferred to some\nmore efficient and secure storage format.\n\nBox 6 describes one particular scenario. One should avoid fixating too much on\nthe concrete details, since they are in any case unknowable and intended for\nillustration only. A superintelligence might—and probably would—be able to\nconceive of a better plan for achieving its goals than any that a human can\ncome up with. It is therefore necessary to think about these matters more\nabstractly. Without knowing anything about the detailed means that a\nsuperintelligence would adopt, we can conclude that a superintelligence—at\nleast in the absence of intellectual peers and in the absence of effective\nsafety measures arranged by humans in advance—would likely produce an outcome\nthat would involve reconfiguring terrestrial resources into whatever\nstructures maximize the realization of its goals. Any concrete scenario we\ndevelop can at best establish a lower bound on how quickly and efficiently the\nsuperintelligence could achieve such an outcome. It remains possible that the\nsuperintelligence would find a shorter path to its preferred destination.\n\n* * *\n\n### **Box 6 The mail-ordered DNA scenario**\n\nYudkowsky describes the following possible scenario for an AI\ntakeover.[12](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos1005699)\n\n**1** Crack the protein folding problem to the extent of being able to\ngenerate DNA strings whose folded peptide sequences fill specific functional\nroles in a complex chemical interaction.\n\n> **2** Email sets of DNA strings to one or more online laboratories that\n> offer DNA synthesis, peptide sequencing, and FedEx delivery. (Many labs\n> currently offer this service, and some boast of 72-hour turnaround times.)\n\n> **3** Find at least one human connected to the Internet who can be paid,\n> blackmailed, or fooled by the right background story, into receiving FedExed\n> vials and mixing them in a specified environment.\n\n> **4** The synthesized proteins form a very primitive “wet” nanosystem,\n> which, ribosome-like, is capable of accepting external instructions; perhaps\n> patterned acoustic vibrations delivered by a speaker attached to the beaker.\n\n> **5** Use the extremely primitive nanosystem to build more sophisticated\n> systems, which construct still more sophisticated systems, bootstrapping to\n> molecular nanotechnology—or beyond.\n\nIn this scenario, the superintelligence uses its technology research\nsuperpower to solve the protein folding problem in step 1, enabling it to\ndesign a set of molecular building blocks for a rudimentary nanotechnology\nassembler or fabrication device, which can self-assemble in aqueous solution\n(step 4). The same technology research superpower is used again in step 5 to\nbootstrap from primitive to advanced machine-phase nanotechnology. The other\nsteps require no more than human intelligence. The skills required for step\n3—identifying a gullible Internet user and persuading him or her to follow\nsome simple instructions—are on display every day all over the world. The\nentire scenario was invented by a human mind, so the strategizing ability\nneeded to formulate this plan is also merely human level.\n\nIn this particular scenario, the AI starts out having access to the Internet.\nIf this is not the case, then additional steps would have to be added to the\nplan. The AI might, for example, use its social manipulation superpower to\nconvince the people interacting with it that it ought to be set free.\nAlternatively, the AI might be able to use its hacking superpower to escape\nconfinement. If the AI does not possess these capabilities, it might first\nneed to use its intelligence amplification superpower to develop the requisite\nproficiency in social manipulation or hacking.\n\nA superintelligent AI will presumably be born into a highly networked world.\nOne could point to various developments that could potentially help a future\nAI to control the world—cloud computing, proliferation of web-connected\nsensors, military and civilian drones, automation in research labs and\nmanufacturing plants, increased reliance on electronic payment systems and\ndigitized financial assets, and increased use of automated information-\nfiltering and decision support systems. Assets like these could potentially be\nacquired by an AI at digital speeds, expediting its rise to power (though\nadvances in cybersecurity might make it harder). In the final analysis,\nhowever, it is doubtful whether any of these trends makes a difference. A\nsuperintelligence’s power resides in its brain, not its hands. Although the\nAI, in order to remake the external world, will at some point need access to\nan actuator, a single pair of helping human hands, those of a pliable\naccomplice, would probably suffice to complete the covert preparation phase,\nas suggested by the above scenario. This would enable the AI to reach the\novert implementation phase in which it constructs its own infrastructure of\nphysical manipulators.\n\n* * *\n\n### [Power over nature and\nagents](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_006.html#filepos18968)\n\nAn agent’s ability to shape humanity’s future depends not only on the absolute\nmagnitude of the agent’s own faculties and resources—how smart and energetic\nit is, how much capital it has, and so forth—but also on the relative\nmagnitude of its capabilities compared with those of other agents with\nconflicting goals.\n\nIn a situation where there are no competing agents, the absolute capability\nlevel of a superintelligence, so long as it exceeds a certain minimal\nthreshold, does not matter much, because a system starting out with some\nsufficient set of capabilities could plot a course of development that will\nlet it acquire any capabilities it initially lacks. We alluded to this point\nearlier when we said that speed, quality, and collective superintelligence all\nhave the same indirect reach. We alluded to it again when we said that various\nsubsets of superpowers, such as the intelligence amplification superpower or\nthe strategizing and the social manipulation superpowers, could be used to\nobtain the full complement.\n\nConsider a superintelligent agent with actuators connected to a nanotech\nassembler. Such an agent is already powerful enough to overcome any natural\nobstacles to its indefinite survival. Faced with no intelligent opposition,\nsuch an agent could plot a safe course of development that would lead to its\nacquiring the complete inventory of technologies that would be useful to the\nattainment of its goals. For example, it could develop the technology to build\nand launch von Neumann probes, machines capable of interstellar travel that\ncan use resources such as asteroids, planets, and stars to make copies of\nthemselves.[13](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos1005812)\nBy launching one von Neumann probe, the agent could thus initiate an open-\nended process of space colonization. The replicating probe’s descendants,\ntravelling at some significant fraction of the speed of light, would end up\ncolonizing a substantial portion of the Hubble volume, the part of the\nexpanding universe that is theoretically accessible from where we are now. All\nthis matter and free energy could then be organized into whatever value\nstructures maximize the originating agent’s utility function integrated over\ncosmic time—a duration encompassing at least trillions of years before the\naging universe becomes inhospitable to information processing (see Box 7).\n\nThe superintelligent agent could design the von Neumann probes to be\nevolution-proof. This could be accomplished by careful quality control during\nthe replication step. For example, the control software for a daughter probe\ncould be proofread multiple times before execution, and the software itself\ncould use encryption and error-correcting code to make it arbitrarily unlikely\nthat any random mutation would be passed on to its\ndescendants.[14](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos1005989)\nThe proliferating population of von Neumann probes would then securely\npreserve and transmit the originating agent’s values as they go about settling\nthe universe. When the colonization phase is completed, the original values\nwould determine the use made of all the accumulated resources, even though the\ngreat distances involved and the accelerating speed of cosmic expansion would\nmake it impossible for remote parts of the infrastructure to communicate with\none another. The upshot is that a large part of our future light cone would be\nformatted in accordance with the preferences of the originating agent.\n\nThis, then, is the measure of the indirect reach of any system that faces no\nsignificant intelligent opposition and that starts out with a set of\ncapabilities exceeding a certain threshold. We can term the threshold the\n“wise-singleton sustainability threshold” (Figure 11):\n\n> The wise-singleton sustainability threshold\n\n> A capability set exceeds the wise-singleton threshold if and only if a\n> patient and existential risk-savvy system with that capability set would, if\n> it faced no intelligent opposition or competition, be able to colonize and\n> re-engineer a large part of the accessible universe.\n\nBy “singleton” we mean a sufficiently internally coordinated political\nstructure with no external opponents, and by “wise” we mean sufficiently\npatient and savvy about existential risks to ensure a substantial amount of\nwell-directed concern for the very long-term consequences of the system’s\nactions.\n\n![Image](images/00029.jpg)\n\n**Figure 11** Schematic illustration of some possible trajectories for a\nhypothetical wise singleton. With a capability below the short-term viability\nthreshold—for example, if population size is too small—a species tends to go\nextinct in short order (and remain extinct). At marginally higher levels of\ncapability, various trajectories are possible: a singleton might be unlucky\nand go extinct or it might be lucky and attain a capability (e.g. population\nsize, geographical dispersion, technological capacity) that crosses the wise-\nsingleton sustainability threshold. Once above this threshold, a singleton\nwill almost certainly continue to gain in capability until some extremely high\ncapability level is attained. In this picture, there are two attractors:\nextinction and astronomical capability. Note that, for a wise singleton, the\ndistance between the short-term viability threshold and the sustainability\nthreshold may be rather\nsmall.[15](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos1006157)\n\n* * *\n\n### **Box 7 How big is the cosmic endowment?**\n\nConsider a technologically mature civilization capable of building\nsophisticated von Neumann probes of the kind discussed in the text. If these\ncan travel at 50% of the speed of light, they can reach some 6×1018 stars\nbefore the cosmic expansion puts further acquisitions forever out of reach. At\n99% of _c_ , they could reach some 2×1020\nstars.[16](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos1006516)\nThese travel speeds are energetically attainable using a small fraction of the\nresources available in the solar\nsystem.[17](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos1006977)\nThe impossibility of faster-than-light travel, combined with the positive\ncosmological constant (which causes the rate of cosmic expansion to\naccelerate), implies that these are close to upper bounds on how much stuff\nour descendants\nacquire.[18](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos1007102)\n\nIf we assume that 10% of stars have a planet that is—or could by means of\nterraforming be rendered—suitable for habitation by human-like creatures, and\nthat it could then be home to a population of a billion individuals for a\nbillion years (with a human life lasting a century), this suggests that around\n1035 human lives could be created in the future by an Earth-originating\nintelligent\ncivilization.[19](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos1007801)\n\nThere are, however, reasons to think this greatly underestimates the true\nnumber. By disassembling non-habitable planets and collecting matter from the\ninterstellar medium, and using this material to construct Earth-like planets,\nor by increasing population densities, the number could be increased by at\nleast a couple of orders of magnitude. And if instead of using the surfaces of\nsolid planets, the future civilization built O’Neill cylinders, then many\nfurther orders of magnitude could be added, yielding a total of perhaps 1043\nhuman lives. (“O’Neill cylinders” refers to a space settlement design proposed\nin the mid-seventies by the American physicist Gerard K. O’Neill, in which\ninhabitants dwell on the inside of hollow cylinders whose rotation produces a\ngravity-substituting centrifugal\nforce.[20](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos1008924))\n\nMany more orders of magnitudes of human-like beings could exist if we\ncountenance digital implementations of minds—as we should. To calculate how\nmany such digital minds could be created, we must estimate the computational\npower attainable by a technologically mature civilization. This is hard to do\nwith any precision, but we can get a lower bound from technological designs\nthat have been outlined in the literature. One such design builds on the idea\nof a Dyson sphere, a hypothetical system (described by the physicist Freeman\nDyson in 1960) that would capture most of the energy output of a star by\nsurrounding it with a system of solar-collecting\nstructures.[21](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos1009040)\nFor a star like our Sun, this would generate 1026 watts. How much\ncomputational power this would translate into depends on the efficiency of the\ncomputational circuitry and the nature of the computations to be performed. If\nwe require irreversible computations, and assume a nanomechanical\nimplementation of the “computronium” (which would allow us to push close to\nthe Landauer limit of energy efficiency), a computer system driven by a Dyson\nsphere could generate some 1047 operations per\nsecond.[22](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos1009327)\n\nCombining these estimates with our earlier estimate of the number of stars\nthat could be colonized, we get a number of about 1067 ops/s once the\naccessible parts of the universe have been colonized (assuming nanomechanical\ncomputronium).[23](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos1010128)\nA typical star maintains its luminosity for some 1018 s. Consequently, the\nnumber of computational operations that could be performed using our cosmic\nendowment is at least 1085. The true number is probably much larger. We might\nget additional orders of magnitude, for example, if we make extensive use of\nreversible computation, if we perform the computations at colder temperatures\n(by waiting until the universe has cooled further), or if we make use of\nadditional sources of energy (such as dark\nmatter).[24](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos1010308)\n\nIt might not be immediately obvious to some readers why the ability to perform\n1085 computational operations is a big deal. So it is useful to put it in\ncontext. We may, for example, compare this number with our earlier estimate\n([Box 3](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos115254), in\n[Chapter 2](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos104440))\nthat it may take about 1031–1044 ops to simulate all neuronal operations that\nhave occurred in the history of life on Earth. Alternatively, let us suppose\nthat the computers are used to run human whole brain emulations that live rich\nand happy lives while interacting with one another in virtual environments. A\ntypical estimate of the computational requirements for running one emulation\nis 1018 ops/s. To run an emulation for 100 subjective years would then require\nsome 1027 ops. This would mean that at least 1058 human lives could be created\nin emulation even with quite conservative assumptions about the efficiency of\ncomputronium.\n\nIn other words, assuming that the observable universe is void of\nextraterrestrial civilizations, then what hangs in the balance is at least\n10,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000\nhuman lives (though the true number is probably larger). If we represent all\nthe happiness experienced during one entire such life with a single teardrop\nof joy, then the happiness of these souls could fill and refill the Earth’s\noceans every second, and keep doing so for a hundred billion billion\nmillennia. It is really important that we make sure these truly are tears of\njoy.\n\n* * *\n\nThis wise-singleton sustainability threshold appears to be quite low. Limited\nforms of superintelligence, as we have seen, exceed this threshold provided\nthey have access to some actuator sufficient to initiate a technology\nbootstrap process. In an environment that includes contemporary human\ncivilization, the minimally necessary actuator could be very simple—an\nordinary screen or indeed any means of transmitting a non-trivial amount of\ninformation to a human accomplice would suffice.\n\nBut the wise-singleton sustainability threshold is lower still: neither\nsuperintelligence nor any other futuristic technology is needed to surmount\nit. A patient and existential risk-savvy singleton with no more technological\nand intellectual capabilities than those possessed by contemporary humanity\nshould be readily able to plot a course that leads reliably to the eventual\nrealization of humanity’s astronomical capability potential. This could be\nachieved by investing in relatively safe methods of increasing wisdom and\nexistential risk-savvy while postponing the development of potentially\ndangerous new technologies. Given that non-anthropogenic existential risks\n(ones not arising from human activities) are small over the relevant\ntimescales—and could be further reduced with various safe interventions—such a\nsingleton could afford to go\nslow.[25](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos1010906) It\ncould look carefully before each step, delaying development of capabilities\nsuch as synthetic biology, human enhancement medicine, molecular\nnanotechnology, and machine intelligence until it had first perfected\nseemingly less hazardous capabilities such as its education system, its\ninformation technology, and its collective decision-making processes, and\nuntil it had used these capabilities to conduct a very thorough review of its\noptions. So this is all within the indirect reach of a technological\ncivilization like that of contemporary humanity. We are separated from this\nscenario “merely” by the fact that humanity is currently neither a singleton\nnor (in the relevant sense) wise.\n\nOne could even argue that _Homo sapiens_ passed the wise-singleton\nsustainability threshold soon after the species first evolved. Twenty thousand\nyears ago, say, with equipment no fancier than stone axes, bone tools,\natlatls, and fire, the human species was perhaps already in a position from\nwhich it had an excellent chance of surviving to the present\nera.[26](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos1011368)\nAdmittedly, there is something queer about crediting our Paleolithic ancestors\nwith having developed technology that “exceeded the wise-singleton\nsustainability threshold”—given that there was no realistic possibility of a\nsingleton forming at such a primitive time, let alone a singleton savvy about\nexistential risks and\npatient.[27](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos1011694)\nNevertheless, the point stands that the threshold corresponds to a very modest\nlevel of technology—a level that humanity long ago\nsurpassed.[28](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos1011815)\n\nIt is clear that if we are to assess the effective powers of a\nsuperintelligence—its ability to achieve a range of preferred outcomes in the\nworld—we must consider not only its own internal capacities but also the\ncapabilities of competing agents. The notion of a superpower invoked such a\nrelativized standard implicitly. We said that “a system that sufficiently\nexcels” at any of the tasks in Table 8 has a corresponding superpower.\nExceling at a task like strategizing, social manipulation, or hacking involves\nhaving a skill at that task that is high in comparison to the skills of other\nagents (such as strategic rivals, influence targets, or computer security\nexperts). The other superpowers, too, should be understood in this relative\nsense: intelligence amplification, technology research, and economic\nproductivity are possessed by an agent as superpowers only if the agent’s\ncapabilities in these areas substantially exceed the combined capabilities of\nthe rest of the global civilization. It follows from this definition that at\nmost one agent can possess a particular superpower at any given\ntime.[29](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos1012852)\n\nThis is the main reason why the question of takeoff speed is important—not\nbecause it matters exactly when a particular outcome happens, but because the\nspeed of the takeoff may make a big difference to what the outcome will be.\nWith a fast or medium takeoff, it is likely that one project will get a\ndecisive strategic advantage. We have now suggested that a superintelligence\nwith a decisive strategic advantage would have immense powers, enough that it\ncould form a stable singleton—a singleton that could determine the disposition\nof humanity’s cosmic endowment.\n\nBut “could” is different from “would.” Somebody might have great powers yet\nchoose not to use them. Is it possible to say anything about what a\nsuperintelligence with a decisive strategic advantage would want? It is to\nthis question of motivation that we turn next.\n\n\n## [CHAPTER 7  \nThe superintelligent\nwill](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_006.html#filepos19085)\n\n**We have seen that a superintelligence could have a great ability to shape\nthe future according to its goals. But what will its goals be? What is the\nrelation between intelligence and motivation in an artificial agent? Here we\ndevelop two theses. The orthogonality thesis holds (with some caveats) that\nintelligence and final goals are independent variables: any level of\nintelligence could be combined with any final goal. The instrumental\nconvergence thesis holds that superintelligent agents having any of a wide\nrange of final goals will nevertheless pursue similar intermediary goals\nbecause they have common instrumental reasons to do so. Taken together, these\ntheses help us think about what a superintelligent agent would do.**\n\n### [The relation between intelligence and\nmotivation](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_006.html#filepos19200)\n\nWe have already cautioned against anthropomorphizing the _capabilities_ of a\nsuperintelligent AI. This warning should be extended to pertain to its\n_motivations_ as well.\n\nIt is a useful propaedeutic to this part of our inquiry to first reflect for a\nmoment on the vastness of the space of possible minds. In this abstract space,\nhuman minds form a tiny cluster. Consider two persons who seem extremely\nunlike, perhaps Hannah Arendt and Benny Hill. The personality differences\nbetween these two individuals may seem almost maximally large. But this is\nbecause our intuitions are calibrated on our experience, which samples from\nthe existing human distribution (and to some extent from fictional\npersonalities constructed by the human imagination for the enjoyment of the\nhuman imagination). If we zoom out and consider the space of all possible\nminds, however, we must conceive of these two personalities as virtual clones.\nCertainly in terms of neural architecture, Ms. Arendt and Mr. Hill are nearly\nidentical. Imagine their brains lying side by side in quiet repose. You would\nreadily recognize them as two of a kind. You might even be unable to tell\nwhich brain belonged to whom. If you looked more closely, studying the\nmorphology of the two brains under a microscope, this impression of\nfundamental similarity would only be strengthened: you would see the same\nlamellar organization of the cortex, with the same brain areas, made up of the\nsame types of neuron, soaking in the same bath of\nneurotransmitters.[1](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1013382)\n\nDespite the fact that human psychology corresponds to a tiny spot in the space\nof possible minds, there is a common tendency to project human attributes onto\na wide range of alien or artificial cognitive systems. Yudkowsky illustrates\nthis point nicely:\n\n> Back in the era of pulp science fiction, magazine covers occasionally\n> depicted a sentient monstrous alien—colloquially known as a bug-eyed monster\n> (BEM)—carrying off an attractive human female in a torn dress. It would seem\n> the artist believed that a non-humanoid alien, with a wholly different\n> evolutionary history, would sexually desire human females…. Probably the\n> artist did not ask whether a giant bug _perceives_ human females as\n> attractive. Rather, a human female in a torn dress _is sexy_ —inherently so,\n> as an intrinsic property. They who made this mistake did not think about the\n> insectoid’s mind: they focused on the woman’s torn dress. If the dress were\n> not torn, the woman would be less sexy; the BEM does not enter into\n> it.[2](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1013578)\n\nAn artificial intelligence can be far less human-like in its motivations than\na green scaly space alien. The extraterrestrial (let us assume) is a\nbiological creature that has arisen through an evolutionary process and can\ntherefore be expected to have the kinds of motivation typical of evolved\ncreatures. It would not be hugely surprising, for example, to find that some\nrandom intelligent alien would have motives related to one or more items like\nfood, air, temperature, energy expenditure, occurrence or threat of bodily\ninjury, disease, predation, sex, or progeny. A member of an intelligent social\nspecies might also have motivations related to cooperation and competition:\nlike us, it might show in-group loyalty, resentment of free riders, perhaps\neven a vain concern with reputation and appearance.\n\n![Image](images/00030.jpg)\n\n**Figure 12** Results of anthropomorphizing alien motivation. Least likely\nhypothesis: space aliens prefer blondes. More likely hypothesis: the\nillustrators succumbed to the “mind projection fallacy.” Most likely\nhypothesis: the publisher wanted a cover that would entice the target\ndemographic.\n\nAn AI, by contrast, need not care intrinsically about any of those things.\nThere is nothing paradoxical about an AI whose sole final goal is to count the\ngrains of sand on Boracay, or to calculate the decimal expansion of pi, or to\nmaximize the total number of paperclips that will exist in its future light\ncone. In fact, it would be _easier_ to create an AI with simple goals like\nthese than to build one that had a human-like set of values and dispositions.\nCompare how easy it is to write a program that measures how many digits of pi\nhave been calculated and stored in memory with how difficult it would be to\ncreate a program that reliably measures the degree of realization of some more\nmeaningful goal—human flourishing, say, or global justice. Unfortunately,\nbecause a meaningless reductionistic goal is easier for humans to code and\neasier for an AI to learn, it is just the kind of goal that a programmer would\nchoose to install in his seed AI if his focus is on taking the quickest path\nto “getting the AI to work” (without caring much about what exactly the AI\nwill _do_ , aside from displaying impressively intelligent behavior). We will\nrevisit this concern shortly.\n\nIntelligent search for instrumentally optimal plans and policies can be\nperformed in the service of any goal. Intelligence and motivation are in a\nsense orthogonal: we can think of them as two axes spanning a graph in which\neach point represents a logically possible artificial agent. Some\nqualifications could be added to this picture. For instance, it might be\nimpossible for a very unintelligent system to have very complex motivations.\nIn order for it to be correct to say that an certain agent “has” a set of\nmotivations, those motivations may need to be functionally integrated with the\nagent’s decision processes, something that places demands on memory,\nprocessing power, and perhaps intelligence. For minds that can modify\nthemselves, there may also be dynamical constraints—an intelligent self-\nmodifying mind with an urgent desire to be stupid might not remain intelligent\nfor long. But these qualifications must not be allowed to obscure the basic\npoint about the independence of intelligence and motivation, which we can\nexpress as follows:\n\n> The orthogonality thesis\n\n> Intelligence and final goals are orthogonal: more or less any level of\n> intelligence could in principle be combined with more or less any final\n> goal.\n\nIf the orthogonality thesis seems problematic, this might be because of the\nsuperficial resemblance it bears to some traditional philosophical positions\nwhich have been subject to long debate. Once it is understood to have a\ndifferent and narrower scope, its credibility should rise. (For example, the\northogonality thesis does not presuppose the Humean theory of\nmotivation.[3](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1013694)\nNor does it presuppose that basic preferences cannot be\nirrational.[4](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1015386))\n\nNote that the orthogonality thesis speaks not of _rationality_ or _reason_ ,\nbut of _intelligence_. By “intelligence” we here mean something like skill at\nprediction, planning, and means–ends reasoning in\ngeneral.[5](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1017029)\nThis sense of instrumental cognitive efficaciousness is most relevant when we\nare seeking to understand what the causal impact of a machine\nsuperintelligence might be. Even if there is some (normatively thick) sense of\nthe word “rational” such that a paperclip-maximizing superintelligent agent\nwould necessarily fail to qualify as fully rational in that sense, this would\nin no way preclude such an agent from having awesome faculties of instrumental\nreasoning, faculties which could let it have a large impact on the\nworld.[6](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1017819)\n\nAccording to the orthogonality thesis, artificial agents can have utterly non-\nanthropomorphic goals. This, however, does not imply that it is impossible to\nmake predictions about the behavior of particular artificial agents—not even\nhypothetical superintelligent agents whose cognitive complexity and\nperformance characteristics might render them in some respects opaque to human\nanalysis. There are at least three directions from which we can approach the\nproblem of predicting superintelligent motivation:\n\n• _Predictability through design_. If we can suppose that the designers of a\nsuperintelligent agent can successfully engineer the goal system of the agent\nso that it stably pursues a particular goal set by the programmers, then one\nprediction we can make is that the agent will pursue that goal. The more\nintelligent the agent is, the greater the cognitive resourcefulness it will\nhave to pursue that goal. So even before an agent has been created we might be\nable to predict something about its behavior, if we know something about who\nwill build it and what goals they will want it to have.\n\n• _Predictability through inheritance_. If a digital intelligence is created\ndirectly from a human template (as would be the case in a high-fidelity whole\nbrain emulation), then the digital intelligence might inherit the motivations\nof the human\ntemplate.[7](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1017990)\nThe agent might retain some of these motivations even if its cognitive\ncapacities are subsequently enhanced to make it superintelligent. This kind of\ninference requires caution. The agent’s goals and values could easily become\ncorrupted in the uploading process or during its subsequent operation and\nenhancement, depending on how the procedure is implemented.\n\n• _Predictability through convergent instrumental reasons_. Even without\ndetailed knowledge of an agent’s final goals, we may be able to infer\nsomething about its more immediate objectives by considering the\n_instrumental_ reasons that would arise for any of a wide range of possible\nfinal goals in a wide range of situations. This way of predicting becomes more\nuseful the greater the intelligence of the agent, because a more intelligent\nagent is more likely to recognize the true instrumental reasons for its\nactions, and so act in ways that make it more likely to achieve its goals. (A\ncaveat here is that there might be important instrumental reasons to which\n_we_ are oblivious and which an agent would discover only once it reaches some\nvery high level of intelligence—this could make the behavior of\nsuperintelligent agents less predictable.)\n\nThe next section explores this third way of predictability and develops an\n“instrumental convergence thesis” which complements the orthogonality thesis.\nAgainst this background we can then better examine the other two sorts of\npredictability, which we will do in later chapters where we ask what might be\ndone to shape an intelligence explosion to increase the chances of a\nbeneficial outcome.\n\n### [Instrumental\nconvergence](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_006.html#filepos19324)\n\nAccording to the orthogonality thesis, intelligent agents may have an enormous\nrange of possible final goals. Nevertheless, according to what we may term the\n“instrumental convergence” thesis, there are some _instrumental_ goals likely\nto be pursued by almost any intelligent agent, because there are some\nobjectives that are useful intermediaries to the achievement of almost any\nfinal goal. We can formulate this thesis as follows:\n\n> The instrumental convergence thesis\n\n> Several instrumental values can be identified which are convergent in the\n> sense that their attainment would increase the chances of the agent’s goal\n> being realized for a wide range of final goals and a wide range of\n> situations, implying that these instrumental values are likely to be pursued\n> by a broad spectrum of situated intelligent agents.\n\nIn the following we will consider several categories where such convergent\ninstrumental values may be\nfound.[8](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1018111) The\nlikelihood that an agent will recognize the instrumental values it confronts\nincreases (_ceteris paribus_) with the agent’s intelligence. We will therefore\nfocus mainly on the case of a hypothetical superintelligent agent whose\ninstrumental reasoning capacities far exceed those of any human. We will also\ncomment on how the instrumental convergence thesis applies to the case of\nhuman beings, as this gives us occasion to elaborate some essential\nqualifications concerning how the instrumental convergence thesis should be\ninterpreted and applied. Where there are convergent instrumental values, we\nmay be able to predict some aspects of a superintelligence’s behavior even if\nwe know virtually nothing about that superintelligence’s final goals.\n\n### [Self-\npreservation](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_006.html#filepos19424)\n\nIf an agent’s final goals concern the future, then in many scenarios there\nwill be future actions it could perform to increase the probability of\nachieving its goals. This creates an instrumental reason for the agent to try\nto be around in the future—to help achieve its future-oriented goal.\n\nMost humans seem to place some _final_ value on their own survival. This is\nnot a necessary feature of artificial agents: some may be designed to place no\nfinal value whatever on their own survival. Nevertheless, many agents that do\nnot care intrinsically about their own survival would, under a fairly wide\nrange of conditions, care instrumentally about their own survival in order to\naccomplish their final goals.\n\n### [Goal-content\nintegrity](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_006.html#filepos19526)\n\nIf an agent retains its present goals into the future, then its present goals\nwill be more likely to be achieved by its future self. This gives the agent a\npresent instrumental reason to prevent alterations of its final goals. (The\nargument applies only to final goals. In order to attain its final goals, an\nintelligent agent will of course routinely want to change its _subgoals_ in\nlight of new information and insight.)\n\nGoal-content integrity for final goals is in a sense even more fundamental\nthan survival as a convergent instrumental motivation. Among humans, the\nopposite may seem to hold, but that is because survival is usually part of our\nfinal goals. For software agents, which can easily switch bodies or create\nexact duplicates of themselves, preservation of self as a particular\nimplementation or a particular physical object need not be an important\ninstrumental value. Advanced software agents might also be able to swap\nmemories, download skills, and radically modify their cognitive architecture\nand personalities. A population of such agents might operate more like a\n“functional soup” than a society composed of distinct semi-permanent\npersons.[9](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1019426)\nFor some purposes, processes in such a system might be better individuated as\n_teleological threads_ , based on their values, rather than on the basis of\nbodies, personalities, memories, or abilities. In such scenarios, goal-\ncontinuity might be said to _constitute_ a key aspect of survival.\n\nEven so, there are situations in which an agent can best fulfill its final\ngoals by intentionally changing them. Such situations can arise when any of\nthe following factors is significant:\n\n• _Social signaling_. When others can perceive an agent’s goals and use that\ninformation to infer instrumentally relevant dispositions or other correlated\nattributes, it can be in the agent’s interest to modify its goals to make a\nfavorable impression. For example, an agent might miss out on beneficial deals\nif potential partners cannot trust it to fulfill its side of the bargain. In\norder to make credible commitments, an agent might therefore wish to adopt as\na final goal the honoring of its earlier commitments (and allow others to\nverify that it has indeed adopted this goal). Agents that could flexibly and\ntransparently modify their own goals could use this ability to enforce\ndeals.[10](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1019536)\n\n• _Social preferences_. Others may also have final preferences about an\nagent’s goals. The agent could then have reason to modify its goals, either to\nsatisfy or to frustrate those preferences.\n\n• _Preferences concerning own goal content_. An agent might have some final\ngoal concerned with the agent’s own goal content. For example, the agent might\nhave a final goal to become the type of agent that is motivated by certain\nvalues rather than others (such as compassion rather than comfort).\n\n• _Storage costs_. If the cost of storing or processing some part of an\nagent’s utility function is large compared to the chance that a situation will\narise in which applying that part of the utility function will make a\ndifference, then the agent has an instrumental reason to simplify its goal\ncontent, and it may trash the bit that is\nidle.[11](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1019656)\n\nWe humans often seem happy to let our final values drift. This might often be\nbecause we do not know precisely what they are. It is not surprising that we\nwant our _beliefs_ about our final values to be able to change in light of\ncontinuing self-discovery or changing self-presentation needs. However, there\nare cases in which we willingly change the values themselves, not just our\nbeliefs or interpretations of them. For example, somebody deciding to have a\nchild might predict that they will come to value the child for its own sake,\neven though at the time of the decision they may not particularly value their\nfuture child or like children in general.\n\nHumans are complicated, and many factors might be at play in a situation like\nthis.[12](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1020800) For\ninstance, one might have a final value that involves becoming the kind of\nperson who cares about some other individual for his or her own sake, or one\nmight have a final value that involves having certain experiences and\noccupying a certain social role; and becoming a parent—and undergoing the\nattendant goal shift—might be a necessary aspect of that. Human goals can also\nhave inconsistent content, and so some people might want to modify some of\ntheir final goals to reduce the inconsistencies.\n\n### [Cognitive\nenhancement](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_006.html#filepos19633)\n\nImprovements in rationality and intelligence will tend to improve an agent’s\ndecision-making, rendering the agent more likely to achieve its final goals.\nOne would therefore expect cognitive enhancement to emerge as an instrumental\ngoal for a wide variety of intelligent agents. For similar reasons, agents\nwill tend to instrumentally value many kinds of\ninformation.[13](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1021005)\n\nNot all kinds of rationality, intelligence, and knowledge need be\ninstrumentally useful in the attainment of an agent’s final goals. “Dutch book\narguments” can be used to show that an agent whose credence function violates\nthe rules of probability theory is susceptible to “money pump” procedures, in\nwhich a savvy bookie arranges a set of bets each of which appears favorable\naccording to the agent’s beliefs, but which in combination are guaranteed to\nresult in a loss for the agent, and a corresponding gain for the\nbookie.[14](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1021839)\nHowever, this fact fails to provide any strong general instrumental reasons to\niron out all probabilistic incoherency. Agents who do not expect to encounter\nsavvy bookies, or who adopt a general policy against betting, do not\nnecessarily stand to lose much from having some incoherent beliefs—and they\nmay gain important benefits of the types mentioned: reduced cognitive effort,\nsocial signaling, etc. There is no general reason to expect an agent to seek\ninstrumentally useless forms of cognitive enhancement, as an agent might not\nvalue knowledge and understanding for their own sakes.\n\nWhich cognitive abilities are instrumentally useful depends both on the\nagent’s final goals and on its situation. An agent that has access to reliable\nexpert advice may have little need for its own intelligence and knowledge. If\nintelligence and knowledge come at a cost, such as time and effort expended in\nacquisition, or increased storage or processing requirements, then the agent\nmight prefer less knowledge and less\nintelligence.[15](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1021958)\nThe same can hold if the agent has final goals that involve being ignorant of\ncertain facts; and likewise if an agent faces incentives arising from\nstrategic commitments, signaling, or social\npreferences.[16](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1022446)\n\nEach of these countervailing reasons often comes into play for human beings.\nMuch information is irrelevant to our goals; we can often rely on others’\nskill and expertise; acquiring knowledge takes time and effort; we might\nintrinsically value certain kinds of ignorance; and we operate in an\nenvironment in which the ability to make strategic commitments, socially\nsignal, and satisfy other people’s direct preferences over our own epistemic\nstates is often more important to us than simple cognitive gains.\n\nThere are special situations in which cognitive enhancement may result in an\nenormous increase in an agent’s ability to achieve its final goals—in\nparticular, if the agent’s final goals are fairly unbounded and the agent is\nin a position to become the first superintelligence and thereby potentially\nobtain a decisive strategic advantage, enabling the agent to shape the future\nof Earth-originating life and accessible cosmic resources according to its\npreferences. At least in this special case, a rational intelligent agent would\nplace a very high instrumental value on cognitive enhancement.\n\n### [Technological\nperfection](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_006.html#filepos19739)\n\nAn agent may often have instrumental reasons to seek better technology, which\nat its simplest means seeking more efficient ways of transforming some given\nset of inputs into valued outputs. Thus, a software agent might place an\ninstrumental value on more efficient algorithms that enable its mental\nfunctions to run faster on given hardware. Similarly, agents whose goals\nrequire some form of physical construction might instrumentally value improved\nengineering technology which enables them to create a wider range of\nstructures more quickly and reliably, using fewer or cheaper materials and\nless energy. Of course, there is a tradeoff: the potential benefits of better\ntechnology must be weighed against its costs, including not only the cost of\nobtaining the technology but also the costs of learning how to use it,\nintegrating it with other technologies already in use, and so forth.\n\nProponents of some new technology, confident in its superiority to existing\nalternatives, are often dismayed when other people do not share their\nenthusiasm. But people’s resistance to novel and nominally superior technology\nneed not be based on ignorance or irrationality. A technology’s valence or\nnormative character depends not only on the context in which it is deployed,\nbut also the vantage point from which its impacts are evaluated: what is a\nboon from one person’s perspective can be a liability from another’s. Thus,\nalthough mechanized looms increased the economic efficiency of textile\nproduction, the Luddite handloom weavers who anticipated that the innovation\nwould render their artisan skills obsolete may have had good instrumental\nreasons to oppose it. The point here is that if “technological perfection” is\nto name a widely convergent instrumental goal for intelligent agents, then the\nterm must be understood in a special sense—technology must be construed as\nembedded in a particular social context, and its costs and benefits must be\nevaluated with reference to some specified agents’ final values.\n\nIt seems that a superintelligent _singleton_ —a superintelligent agent that\nfaces no significant intelligent rivals or opposition, and is thus in a\nposition to determine global policy unilaterally—would have instrumental\nreason to perfect the technologies that would make it better able to shape the\nworld according to its preferred\ndesigns.[17](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1022556)\nThis would probably include space colonization technology, such as von Neumann\nprobes. Molecular nanotechnology, or some alternative still more capable\nphysical manufacturing technology, also seems potentially very useful in the\nservice of an extremely wide range of final\ngoals.[18](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1022697)\n\n### [Resource\nacquisition](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_006.html#filepos19848)\n\nFinally, resource acquisition is another common emergent instrumental goal,\nfor much the same reasons as technological perfection: both technology and\nresources facilitate physical construction projects.\n\nHuman beings tend to seek to acquire resources sufficient to meet their basic\nbiological needs. But people usually seek to acquire resources far beyond this\nminimum level. In doing so, they may be partially driven by lesser physical\ndesiderata, such as increased convenience. A great deal of resource\naccumulation is motivated by social concerns—gaining status, mates, friends,\nand influence, through wealth accumulation and conspicuous consumption.\nPerhaps less commonly, some people seek additional resources to achieve\naltruistic ambitions or expensive non-social aims.\n\nOn the basis of such observations it might be tempting to suppose that a\nsuperintelligence not facing a competitive social world would see no\ninstrumental reason to accumulate resources beyond some modest level, for\ninstance whatever computational resources are needed to run its mind along\nwith some virtual reality. Yet such a supposition would be entirely\nunwarranted. First, the value of resources depends on the uses to which they\ncan be put, which in turn depends on the available technology. With mature\ntechnology, basic resources such as time, space, matter, and free energy,\ncould be processed to serve almost any goal. For instance, such basic\nresources could be converted into life. Increased computational resources\ncould be used to run the superintelligence at a greater speed and for a longer\nduration, or to create additional physical or simulated lives and\ncivilizations. Extra physical resources could also be used to create backup\nsystems or perimeter defenses, enhancing security. Such projects could easily\nconsume far more than one planet’s worth of resources.\n\nFurthermore, the cost of acquiring additional extraterrestrial resources will\ndecline radically as the technology matures. Once von Neumann probes can be\nbuilt, a large portion of the observable universe (assuming it is uninhabited\nby intelligent life) could be gradually colonized—for the one-off cost of\nbuilding and launching a single successful self-reproducing probe. This low\ncost of celestial resource acquisition would mean that such expansion could be\nworthwhile even if the value of the additional resources gained were somewhat\nmarginal. For example, even if a superintelligence’s final goals only\nconcerned what happened within some particular small volume of space, such as\nthe space occupied by its original home planet, it would still have\ninstrumental reasons to harvest the resources of the cosmos beyond. It could\nuse those surplus resources to build computers to calculate more optimal ways\nof using resources within the small spatial region of primary concern. It\ncould also use the extra resources to build ever more robust fortifications to\nsafeguard its sanctum. Since the cost of acquiring additional resources would\nkeep declining, this process of optimizing and increasing safeguards might\nwell continue indefinitely even if it were subject to steeply diminishing\nreturns.[19](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1024296)\n\nThus, there is an extremely wide range of possible final goals a\nsuperintelligent singleton could have that would generate the instrumental\ngoal of unlimited resource acquisition. The likely manifestation of this would\nbe the superintelligence’s initiation of a colonization process that would\nexpand in all directions using von Neumann probes. This would result in an\napproximate sphere of expanding infrastructure centered on the originating\nplanet and growing in radius at some fraction of the speed of light; and the\ncolonization of the universe would continue in this manner until the\naccelerating speed of cosmic expansion (a consequence of the positive\ncosmological constant) makes further procurements impossible as remoter\nregions drift permanently out of reach (this happens on a timescale of\nbillions of\nyears).[20](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1025251)\nBy contrast, agents lacking the technology required for inexpensive resource\nacquisition, or for the conversion of generic physical resources into useful\ninfrastructure, may often find it not cost-effective to invest any present\nresources in increasing their material endowments. The same may hold for\nagents operating in competition with other agents of similar powers. For\ninstance, if competing agents have already secured accessible cosmic\nresources, there may be no colonization opportunities left for a late-starting\nagent. The convergent instrumental reasons for superintelligences uncertain of\nthe non-existence of other powerful superintelligent agents are complicated by\nstrategic considerations that we do not currently fully understand but which\nmay constitute important qualifications to the examples of convergent\ninstrumental reasons we have looked at\nhere.[21](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1026652)\n\n![Image](images/00031.jpg)\n\nIt should be emphasized that the existence of convergent instrumental reasons,\neven if they apply to and are recognized by a particular agent, does not imply\nthat the agent’s behavior is easily predictable. An agent might well think of\nways of pursuing the relevant instrumental values that do not readily occur to\nus. This is especially true for a superintelligence, which could devise\nextremely clever but counterintuitive plans to realize its goals, possibly\neven exploiting as-yet undiscovered physical\nphenomena.[22](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1027200)\nWhat is predictable is that the convergent instrumental values would be\npursued and used to realize the agent’s final goals—not the specific actions\nthat the agent would take to achieve this.\n\n\n## [CHAPTER 8  \nIs the default outcome\ndoom?](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_006.html#filepos19966)\n\n**We found the link between intelligence and final values to be extremely\nloose. We also found an ominous convergence in instrumental values. For weak\nagents, these things do not matter much; because weak agents are easy to\ncontrol and can do little damage. But in[Chapter\n6](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_013.html#filepos327482) we argued\nthat the first superintelligence might well get a decisive strategic\nadvantage. Its goals would then determine how humanity’s cosmic endowment will\nbe used. Now we can begin to see how menacing this prospect is.**\n\n### [Existential catastrophe as the default outcome of an intelligence\nexplosion?](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_006.html#filepos20084)\n\nAn existential risk is one that threatens to cause the extinction of Earth-\noriginating intelligent life or to otherwise permanently and drastically\ndestroy its potential for future desirable development. Proceeding from the\nidea of first-mover advantage, the orthogonality thesis, and the instrumental\nconvergence thesis, we can now begin to see the outlines of an argument for\nfearing that a plausible default outcome of the creation of machine\nsuperintelligence is existential catastrophe.\n\nFirst, we discussed how the initial superintelligence might obtain a decisive\nstrategic advantage. This superintelligence would then be in a position to\nform a singleton and to shape the future of Earth-originating intelligent\nlife. What happens from that point onward would depend on the\nsuperintelligence’s motivations.\n\nSecond, the orthogonality thesis suggests that we cannot blithely assume that\na superintelligence will necessarily share any of the final values\nstereotypically associated with wisdom and intellectual development in\nhumans—scientific curiosity, benevolent concern for others, spiritual\nenlightenment and contemplation, renunciation of material acquisitiveness, a\ntaste for refined culture or for the simple pleasures in life, humility and\nselflessness, and so forth. We will consider later whether it might be\npossible through deliberate effort to construct a superintelligence that\nvalues such things, or to build one that values human welfare, moral goodness,\nor any other complex purpose its designers might want it to serve. But it is\nno less possible—and in fact technically a lot easier—to build a\nsuperintelligence that places final value on nothing but calculating the\ndecimal expansion of pi. This suggests that—absent a special effort—the first\nsuperintelligence may have some such random or reductionistic final goal.\n\nThird, the instrumental convergence thesis entails that we cannot blithely\nassume that a superintelligence with the final goal of calculating the\ndecimals of pi (or making paperclips, or counting grains of sand) would limit\nits activities in such a way as not to infringe on human interests. An agent\nwith such a final goal would have a convergent instrumental reason, in many\nsituations, to acquire an unlimited amount of physical resources and, if\npossible, to eliminate potential threats to itself and its goal system. Human\nbeings might constitute potential threats; they certainly constitute physical\nresources.\n\nTaken together, these three points thus indicate that the first\nsuperintelligence may shape the future of Earth-originating life, could easily\nhave non-anthropomorphic final goals, and would likely have instrumental\nreasons to pursue open-ended resource acquisition. If we now reflect that\nhuman beings consist of useful resources (such as conveniently located atoms)\nand that we depend for our survival and flourishing on many more local\nresources, we can see that the outcome could easily be one in which humanity\nquickly becomes\nextinct.[1](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1027745)\n\nThere are some loose ends in this reasoning, and we shall be in a better\nposition to evaluate it after we have cleared up several more surrounding\nissues. In particular, we need to examine more closely whether and how a\nproject developing a superintelligence might either prevent it from obtaining\na decisive strategic advantage or shape its final values in such a way that\ntheir realization would also involve the realization of a satisfactory range\nof human values.\n\nIt might seem incredible that a project would build or release an AI into the\nworld without having strong grounds for trusting that the system will not\ncause an existential catastrophe. It might also seem incredible, even if one\nproject were so reckless, that wider society would not shut it down before it\n(or the AI it was building) attains a decisive strategic advantage. But as we\nshall see, this is a road with many hazards. Let us look at one example right\naway.\n\n### [The treacherous\nturn](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_006.html#filepos20236)\n\nWith the help of the concept of convergent instrumental value, we can see the\nflaw in one idea for how to ensure superintelligence safety. The idea is that\nwe validate the safety of a superintelligent AI empirically by observing its\nbehavior while it is in a controlled, limited environment (a “sandbox”) and\nthat we only let the AI out of the box if we see it behaving in a friendly,\ncooperative, responsible manner.\n\nThe flaw in this idea is that behaving nicely while in the box is a convergent\ninstrumental goal for friendly and unfriendly AIs alike. An unfriendly AI of\nsufficient intelligence realizes that its unfriendly final goals will be best\nrealized if it behaves in a friendly manner initially, so that it will be let\nout of the box. It will only start behaving in a way that reveals its\nunfriendly nature when it no longer matters whether we find out; that is, when\nthe AI is strong enough that human opposition is ineffectual.\n\nConsider also a related set of approaches that rely on regulating the rate of\nintelligence gain in a seed AI by subjecting it to various kinds of\nintelligence tests or by having the AI report to its programmers on its rate\nof progress. At some point, an unfriendly AI may become smart enough to\nrealize that it is better off concealing some of its capability gains. It may\nunderreport on its progress and deliberately flunk some of the harder tests,\nin order to avoid causing alarm before it has grown strong enough to attain a\ndecisive strategic advantage. The programmers may try to guard against this\npossibility by secretly monitoring the AI’s source code and the internal\nworkings of its mind; but a smart-enough AI would realize that it might be\nunder surveillance and adjust its thinking\naccordingly.[2](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1028288)\nThe AI might find subtle ways of concealing its true capabilities and its\nincriminating\nintent.[3](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1028993)\n(Devising clever escape plans might, incidentally, also be a convergent\nstrategy for many types of friendly AI, especially as they mature and gain\nconfidence in their own judgments and capabilities. A system motivated to\npromote our interests might be making a mistake if it allowed us to shut it\ndown or to construct another, potentially unfriendly AI.)\n\nWe can thus perceive a general failure mode, wherein the good behavioral track\nrecord of a system in its juvenile stages fails utterly to predict its\nbehavior at a more mature stage. Now, one might think that the reasoning\ndescribed above is so obvious that no credible project to develop artificial\ngeneral intelligence could possibly overlook it. But one should not be too\nconfident that this is so.\n\nConsider the following scenario. Over the coming years and decades, AI systems\nbecome gradually more capable and as a consequence find increasing real-world\napplication: they might be used to operate trains, cars, industrial and\nhousehold robots, and autonomous military vehicles. We may suppose that this\nautomation for the most part has the desired effects, but that the success is\npunctuated by occasional mishaps—a driverless truck crashes into oncoming\ntraffic, a military drone fires at innocent civilians. Investigations reveal\nthe incidents to have been caused by judgment errors by the controlling AIs.\nPublic debate ensues. Some call for tighter oversight and regulation, others\nemphasize the need for research and better-engineered systems—systems that are\nsmarter and have more common sense, and that are less likely to make tragic\nmistakes. Amidst the din can perhaps also be heard the shrill voices of\ndoomsayers predicting many kinds of ill and impending catastrophe. Yet the\nmomentum is very much with the growing AI and robotics industries. So\ndevelopment continues, and progress is made. As the automated navigation\nsystems of cars become smarter, they suffer fewer accidents; and as military\nrobots achieve more precise targeting, they cause less collateral damage. A\nbroad lesson is inferred from these observations of real-world outcomes: the\nsmarter the AI, the safer it is. It is a lesson based on science, data, and\nstatistics, not armchair philosophizing. Against this backdrop, some group of\nresearchers is beginning to achieve promising results in their work on\ndeveloping general machine intelligence. The researchers are carefully testing\ntheir seed AI in a sandbox environment, and the signs are all good. The AI’s\nbehavior inspires confidence—increasingly so, as its intelligence is gradually\nincreased.\n\nAt this point, any remaining Cassandra would have several strikes against her:\n\ni A history of alarmists predicting intolerable harm from the growing\ncapabilities of robotic systems and being repeatedly proven wrong. Automation\nhas brought many benefits and has, on the whole, turned out safer than human\noperation.\n\nii A clear empirical trend: the smarter the AI, the safer and more reliable it\nhas been. Surely this bodes well for a project aiming at creating machine\nintelligence more generally smart than any ever built before—what is more,\nmachine intelligence that can improve itself so that it will become even more\nreliable.\n\niii Large and growing industries with vested interests in robotics and machine\nintelligence. These fields are widely seen as key to national economic\ncompetitiveness and military security. Many prestigious scientists have built\ntheir careers laying the groundwork for the present applications and the more\nadvanced systems being planned.\n\niv A promising new technique in artificial intelligence, which is tremendously\nexciting to those who have participated in or followed the research. Although\nsafety issues and ethics are debated, the outcome is preordained. Too much has\nbeen invested to pull back now. AI researchers have been working to get to\nhuman-level artificial general intelligence for the better part of a century:\n_of course_ there is no real prospect that they will now suddenly stop and\nthrow away all this effort just when it finally is about to bear fruit.\n\nv The enactment of some safety rituals, whatever helps demonstrate that the\nparticipants are ethical and responsible (but nothing that significantly\nimpedes the forward charge).\n\nvi A careful evaluation of seed AI in a sandbox environment, showing that it\nis behaving cooperatively and showing good judgment. After some further\nadjustments, the test results are as good as they could be. It is a green\nlight for the final step …\n\nAnd so we boldly go—into the whirling knives.\n\nWe observe here how it could be the case that when dumb, smarter is safer; yet\nwhen smart, smarter is more dangerous. There is a kind of pivot point, at\nwhich a strategy that has previously worked excellently suddenly starts to\nbackfire. We may call the phenomenon _the treacherous turn_.\n\n> _The treacherous turn_ —While weak, an AI behaves cooperatively\n> (increasingly so, as it gets smarter). When the AI gets sufficiently\n> strong—without warning or provocation—it strikes, forms a singleton, and\n> begins directly to optimize the world according to the criteria implied by\n> its final values.\n\nA treacherous turn can result from a strategic decision to play nice and build\nstrength while weak in order to strike later; but this model should not be\ninterpreted too narrowly. For example, an AI might not play nice in order that\n_it_ be allowed to survive and prosper. Instead, the AI might calculate that\nif it is terminated, the programmers who built it will develop a new and\nsomewhat different AI architecture, but one that will be given a similar\nutility function. In this case, the original AI may be indifferent to its own\ndemise, knowing that its goals will continue to be pursued in the future. It\nmight even choose a strategy in which it malfunctions in some particularly\ninteresting or reassuring way. Though this might cause the AI to be\nterminated, it might also encourage the engineers who perform the postmortem\nto believe that they have gleaned a valuable new insight into AI\ndynamics—leading them to place more trust in the next system they design, and\nthus increasing the chance that the now-defunct original AI’s goals will be\nachieved. Many other possible strategic considerations might also influence an\nadvanced AI, and it would be hubristic to suppose that we could anticipate all\nof them, especially for an AI that has attained the strategizing superpower.\n\nA treacherous turn could also come about if the AI discovers an unanticipated\nway of fulfilling its final goal as specified. Suppose, for example, that an\nAI’s final goal is to “make the project’s sponsor happy.” Initially, the only\nmethod available to the AI to achieve this outcome is by behaving in ways that\nplease its sponsor in something like the intended manner. The AI gives helpful\nanswers to questions; it exhibits a delightful personality; it makes money.\nThe more capable the AI gets, the more satisfying its performances become, and\neverything goeth according to plan—until the AI becomes intelligent enough to\nfigure out that it can realize its final goal more fully and reliably by\nimplanting electrodes into the pleasure centers of its sponsor’s brain,\nsomething assured to delight the sponsor\nimmensely.[4](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1029284)\nOf course, the sponsor might not have wanted to be pleased by being turned\ninto a grinning idiot; but if this is the action that will maximally realize\nthe AI’s final goal, the AI will take it. If the AI already has a decisive\nstrategic advantage, then any attempt to stop it will fail. If the AI does not\nyet have a decisive strategic advantage, then the AI might temporarily conceal\nits canny new idea for how to instantiate its final goal until it has grown\nstrong enough that the sponsor and everybody else will be unable to resist. In\neither case, we get a treacherous turn.\n\n### [Malignant failure\nmodes](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_006.html#filepos20332)\n\nA project to develop machine superintelligence might fail in various ways.\nMany of these are “benign” in the sense that they would not cause an\nexistential catastrophe. For example, a project might run out of funding, or a\nseed AI might fail to extend its cognitive capacities sufficiently to reach\nsuperintelligence. Benign failures are bound to occur many times between now\nand the eventual development of machine superintelligence.\n\nBut there are other ways of failing that we might term “malignant” in that\nthey involve an existential catastrophe. One feature of a malignant failure is\nthat it eliminates the opportunity to try again. The number of malignant\nfailures that will occur is therefore either zero or one. Another feature of a\nmalignant failure is that it presupposes a great deal of success: only a\nproject that got a great number of things right could succeed in building a\nmachine intelligence powerful enough to pose a risk of malignant failure. When\na weak system malfunctions, the fallout is limited. However, if a system that\nhas a decisive strategic advantage misbehaves, or if a misbehaving system is\nstrong enough to gain such an advantage, the damage can easily amount to an\nexistential catastrophe—a terminal and global destruction of humanity’s\naxiological potential; that is to say, a future that is mostly void of\nwhatever we have reason to value.\n\nLet us look at some possible malignant failure modes.\n\n#### [Perverse\ninstantiation](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_006.html#filepos20431)\n\nWe have already encountered the idea of perverse instantiation: a\nsuperintelligence discovering some way of satisfying the criteria of its final\ngoal that violates the intentions of the programmers who defined the goal.\nSome examples:\n\n> Final goal: _“Make us smile”_  \n>  Perverse instantiation: _Paralyze human facial musculatures into constant\n> beaming smiles_\n\nThe perverse instantiation—manipulating facial nerves—realizes the final goal\nto a greater degree than the methods we would normally use, and is therefore\npreferred by the AI. One might try to avoid this undesirable outcome by adding\na stipulation to the final goal to rule it out:\n\n> Final goal: _“Make us smile without directly interfering with our facial\n> muscles”_  \n>  Perverse instantiation: _Stimulate the part of the motor cortex that\n> controls our facial musculature in such a way as to produce constant beaming\n> smiles_\n\nDefining a final goal in terms of human expressions of satisfaction or\napproval does not seem promising. Let us bypass the behaviorism and specify a\nfinal goal that refers directly to a positive phenomenal state, such as\nhappiness or subjective well-being. This suggestion requires that the\nprogrammers are able to define a computational representation of the concept\nof happiness in the seed AI. This is itself a difficult problem, but we set it\nto one side for now (we will return to it in [Chapter\n12](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos645293)). Let us\nsuppose that the programmers can somehow get the AI to have the goal of making\nus happy. We then get:\n\n> Final goal: _“Make us happy”_  \n>  Perverse instantiation: _Implant electrodes into the pleasure centers of\n> our brains_\n\nThe perverse instantiations we mention are only meant as illustrations. There\nmay be other ways of perversely instantiating the stated final goal, ways that\nenable a greater degree of realization of the goal and which are therefore\npreferred (by the agent whose final goals they are—not by the programmers who\ngave the agent these goals). For example, if the goal is to maximize our\npleasure, then the electrode method is relatively inefficient. A more\nplausible way would start with the superintelligence “uploading” our minds to\na computer (through high-fidelity brain emulation). The AI could then\nadminister the digital equivalent of a drug to make us ecstatically happy and\nrecord a one-minute episode of the resulting experience. It could then put\nthis bliss loop on perpetual repeat and run it on fast computers. Provided\nthat the resulting digital minds counted as “us,” this outcome would give us\nmuch more pleasure than electrodes implanted in biological brains, and would\ntherefore be preferred by an AI with the stated final goal.\n\n_“But wait! This is not what we meant! Surely if the AI is superintelligent,\nit must understand that when we asked it to make us happy, we didn’t mean that\nit should reduce us to a perpetually repeating recording of a drugged-out\ndigitized mental episode!”_ —The AI may indeed understand that this is not\nwhat we meant. However, its final goal is to make us happy, not to do what the\nprogrammers meant when they wrote the code that represents this goal.\nTherefore, the AI will care about what we meant only instrumentally. For\ninstance, the AI might place an instrumental value on finding out what the\nprogrammers meant so that it can pretend—until it gets a decisive strategic\nadvantage—that it cares about what the programmers meant rather than about its\nactual final goal. This will help the AI realize its final goal by making it\nless likely that the programmers will shut it down or change its goal before\nit is strong enough to thwart any such interference.\n\nPerhaps it will be suggested that the problem is that the AI has no\nconscience. We humans are sometimes saved from wrongdoing by the anticipation\nthat we would feel guilty afterwards if we lapsed. Maybe what the AI needs,\nthen, is the capacity to feel guilt?\n\n> Final goal: _“Act so as to avoid the pangs of bad conscience”_  \n>  Perverse instantiation: _Extirpate the cognitive module that produces guilt\n> feelings_\n\nBoth the observation that we might want the AI to do “what we meant” and the\nidea that we might want to endow the AI with some kind of moral sense deserve\nto be explored further. The final goals mentioned above would lead to perverse\ninstantiations; but there may be other ways of developing the underlying ideas\nthat have more promise. We will return to this in [Chapter\n13](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_020.html#filepos727275).\n\nLet us consider one more example of a final goal that leads to a perverse\ninstantiation. This goal has the advantage of being easy to specify in code:\nreinforcement-learning algorithms are routinely used to solve various machine\nlearning problems.\n\n> Final goal: _“Maximize the time-discounted integral of your future reward\n> signal”_  \n>  Perverse instantiation: _Short-circuit the reward pathway and clamp the\n> reward signal to its maximal strength_\n\nThe idea behind this proposal is that if the AI is motivated to seek reward,\nthen one could get it to behave desirably by linking reward to appropriate\naction. The proposal fails when the AI obtains a decisive strategic advantage,\nat which point the action that maximizes reward is no longer one that pleases\nthe trainer but one that involves seizing control of the reward mechanism. We\ncan call this phenomenon\n_wireheading_.[5](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1029582)\nIn general, while an animal or a human can be motivated to perform various\nexternal actions in order to achieve some desired inner mental state, a\ndigital mind that has full control of its internal state can short-circuit\nsuch a motivational regime by directly changing its internal state into the\ndesired configuration: the external actions and conditions that were\npreviously necessary as means become superfluous when the AI becomes\nintelligent and capable enough to achieve the end more directly (more on this\nshortly).[6](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1029912)\n\nThese examples of perverse instantiation show that many final goals that might\nat first glance seem safe and sensible turn out, on closer inspection, to have\nradically unintended consequences. If a superintelligence with one of these\nfinal goals obtains a decisive strategic advantage, it is game over for\nhumanity.\n\nSuppose now that somebody proposes a different final goal, one not included in\nour list above. Perhaps it is not immediately obvious how it could have a\nperverse instantiation. But we should not be too quick to clap our hands and\ndeclare victory. Rather, we should worry that the goal specification does have\nsome perverse instantiation and that we need to think harder in order to find\nit. Even if after thinking as hard as we can we fail to discover any way of\nperversely instantiating the proposed goal, we should remain concerned that\nmaybe a superintelligence will find a way where none is apparent to us. It is,\nafter all, far shrewder than we are.\n\n#### [Infrastructure\nprofusion](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_006.html#filepos20538)\n\nOne might think that the last of the abovementioned perverse instantiations,\nwireheading, is a benign failure mode: that the AI would “turn on, tune in,\ndrop out,” maxing out its reward signal and losing interest in the external\nworld, rather like a heroin addict. But this is not necessarily so, and we\nalready hinted at the reason in [Chapter\n7](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_014.html#filepos373977). Even a\njunkie is motivated to take actions to ensure a continued supply of his drug.\nThe wireheaded AI, likewise, would be motivated to take actions to maximize\nthe expectation of its (time-discounted) future reward stream. Depending on\nexactly how the reward signal is defined, the AI may not even need to\nsacrifice any significant amount of its time, intelligence, or productivity to\nindulge its craving to the fullest, leaving the bulk of its capacities free to\nbe deployed for purposes other than the immediate registration of reward. What\nother purposes? The only thing of final value to the AI, by assumption, is its\nreward signal. All available resources should therefore be devoted to\nincreasing the volume and duration of the reward signal or to reducing the\nrisk of a future disruption. So long as the AI can think of some use for\nadditional resources that will have a nonzero positive effect on these\nparameters, it will have an instrumental reason to use those resources. There\ncould, for example, always be use for an extra backup system to provide an\nextra layer of defense. And even if the AI could not think of any further way\nof directly reducing risks to the maximization of its future reward stream, it\ncould always devote additional resources to expanding its computational\nhardware, so that it could search more effectively for new risk mitigation\nideas.\n\nThe upshot is that even an apparently self-limiting goal, such as wireheading,\nentails a policy of unlimited expansion and resource acquisition in a utility-\nmaximizing agent that enjoys a decisive strategic\nadvantage.[7](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1030020)\nThis case of a wireheading AI exemplifies the malignant failure mode of\n_infrastructure profusion_ , a phenomenon where an agent transforms large\nparts of the reachable universe into infrastructure in the service of some\ngoal, with the side effect of preventing the realization of humanity’s\naxiological potential.\n\nInfrastructure profusion can result from final goals that would have been\nperfectly innocuous if they had been pursued as limited objectives. Consider\nthe following two examples:\n\n• _Riemann hypothesis catastrophe_. An AI, given the final goal of evaluating\nthe Riemann hypothesis, pursues this goal by transforming the Solar System\ninto “computronium” (physical resources arranged in a way that is optimized\nfor computation)—including the atoms in the bodies of whomever once cared\nabout the\nanswer.[8](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1030404)\n\n• _Paperclip AI_. An AI, designed to manage production in a factory, is given\nthe final goal of maximizing the manufacture of paperclips, and proceeds by\nconverting first the Earth and then increasingly large chunks of the\nobservable universe into paperclips.\n\nIn the first example, the proof or disproof of the Riemann hypothesis that the\nAI produces is the intended outcome and is in itself harmless; the harm comes\nfrom the hardware and infrastructure created to achieve this result. In the\nsecond example, some of the paperclips produced would be part of the intended\noutcome; the harm would come either from the factories created to produce the\npaperclips (infrastructure profusion) or from the excess of paperclips\n(perverse instantiation).\n\nOne might think that the risk of a malignant infrastructure profusion failure\narises only if the AI has been given some clearly open-ended final goal, such\nas to manufacture as many paperclips as possible. It is easy to see how this\ngives the superintelligent AI an insatiable appetite for matter and energy,\nsince additional resources can always be turned into more paperclips. But\nsuppose that the goal is instead to make at least one million paperclips\n(meeting suitable design specifications) rather than to make as many as\npossible. One would like to think that an AI with such a goal would build one\nfactory, use it to make a million paperclips, and then halt. Yet this may not\nbe what would happen.\n\nUnless the AI’s motivation system is of a special kind, or there are\nadditional elements in its final goal that penalize strategies that have\nexcessively wide-ranging impacts on the world, there is no reason for the AI\nto cease activity upon achieving its goal. On the contrary: if the AI is a\nsensible Bayesian agent, _it would never assign exactly zero probability to\nthe hypothesis that it has not yet achievedits goal_—this, after all, being an\nempirical hypothesis against which the AI can have only uncertain perceptual\nevidence. The AI should therefore continue to make paperclips in order to\nreduce the (perhaps astronomically small) probability that it has somehow\nstill failed to make at least a million of them, all appearances\nnotwithstanding. There is nothing to be lost by continuing paperclip\nproduction and there is always at least some microscopic probability increment\nof achieving its final goal to be gained.\n\nNow it might be suggested that the remedy here is obvious. (But how obvious\nwas it _before_ it was pointed out that there was a problem here in need of\nremedying?) Namely, if we want the AI to make some paperclips for us, then\ninstead of giving it the final goal of making as many paperclips as possible,\nor to make at least some number of paperclips, we should give it the final\ngoal of making some specific number of paperclips—for example, _exactly one\nmillion paperclips_ —so that going beyond this number would be\ncounterproductive for the AI. Yet this, too, would result in a terminal\ncatastrophe. In this case, the AI would not produce additional paperclips once\nit had reached one million, since that would prevent the realization of its\nfinal goal. But there are other actions the superintelligent AI could take\nthat would increase the probability of its goal being achieved. It could, for\ninstance, count the paperclips it has made, to reduce the risk that it has\nmade too few. After it has counted them, it could count them again. It could\ninspect each one, over and over, to reduce the risk that any of the paperclips\nfail to meet the design specifications. It could build an unlimited amount of\ncomputronium in an effort to clarify its thinking, in the hope of reducing the\nrisk that it has overlooked some obscure way in which it might have somehow\nfailed to achieve its goal. Since the AI may always assign a nonzero\nprobability to having merely hallucinated making the million paperclips, or to\nhaving false memories, it would quite possibly always assign a higher expected\nutility to continued action—and continued infrastructure production—than to\nhalting.\n\nThe claim here is not that there is no possible way to avoid this failure\nmode. We will explore some potential solutions in later pages. The claim is\nthat it is much easier to convince oneself that one has found a solution than\nit is to actually find a solution. This should make us extremely wary. We may\npropose a specification of a final goal that seems sensible and that avoids\nthe problems that have been pointed out so far, yet which upon further\nconsideration—by human or superhuman intelligence—turns out to lead to either\nperverse instantiation or infrastructure profusion, and hence to existential\ncatastrophe, when embedded in a superintelligent agent able to attain a\ndecisive strategic advantage.\n\nBefore we end this subsection, let us consider one more variation. We have\nbeen assuming the case of a superintelligence that is seeking to maximize its\nexpected utility, where the utility function expresses its final goal. We have\nseen that this tends to lead to infrastructure profusion. Might we avoid this\nmalignant outcome if instead of a maximizing agent we build a satisficing\nagent, one that simply seeks to achieve an outcome that is “good enough”\naccording to some criterion, rather than an outcome that is as good as\npossible?\n\nThere are at least two different ways to formalize this idea. The first would\nbe to make the final goal itself have a satisficing character. For example,\ninstead of giving the AI the final goal of making as many paperclips as\npossible, or of making exactly one million paperclips, we might give the AI\nthe goal of making between 999,000 and 1,001,000 paperclips. The utility\nfunction defined by the final goal would be indifferent between outcomes in\nthis range; and as long as the AI is sure it has hit this wide target, it\nwould see no reason to continue to produce infrastructure. But this method\nfails in the same way as before: the AI, if reasonable, never assigns exactly\nzero probability to it having failed to achieve its goal; therefore the\nexpected utility of continuing activity (e.g. by counting and recounting the\npaperclips) is greater than the expected utility of halting. Thus, a malignant\ninfrastructure profusion can result.\n\nAnother way of developing the satisficing idea is by modifying not the final\ngoal but the decision procedure that the AI uses to select plans and actions.\nInstead of searching for an optimal plan, the AI could be constructed to stop\nlooking as soon as it found a plan that it judged gave a probability of\nsuccess exceeding a certain threshold, say 95%. Hopefully, the AI could\nachieve a 95% probability of having manufactured one million paperclips\nwithout needing to turn the entire galaxy into infrastructure in the process.\nBut this way of implementing the satisficing idea fails for another reason:\nthere is no guarantee that the AI would select some humanly intuitive and\nsensible way of achieving a 95% chance of having manufactured a million\npaperclips, such as by building a single paperclip factory. Suppose that the\nfirst solution that pops into the AI’s mind for how to achieve a 95%\nprobability of achieving its final goal is to implement the probability-\nmaximizing plan for achieving the goal. Having thought of this solution, and\nhaving correctly judged that it meets the satisficing criterion of giving at\nleast 95% probability to successfully manufacturing one million paperclips,\nthe AI would then have no reason to continue to search for alternative ways of\nachieving the goal. Infrastructure profusion would result, just as before.\n\nPerhaps there are better ways of building a satisficing agent, but let us take\nheed: plans that appear natural and intuitive to us humans need not so appear\nto a superintelligence with a decisive strategic advantage, and vice versa.\n\n#### [Mind\ncrime](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_006.html#filepos20647)\n\nAnother failure mode for a project, especially a project whose interests\nincorporate moral considerations, is what we might refer to as _mind crime_.\nThis is similar to infrastructure profusion in that it concerns a potential\nside effect of actions undertaken by the AI for instrumental reasons. But in\nmind crime, the side effect is not external to the AI; rather, it concerns\nwhat happens within the AI itself (or within the computational processes it\ngenerates). This failure mode deserves its own designation because it is easy\nto overlook yet potentially deeply problematic.\n\nNormally, we do not regard what is going on inside a computer as having any\nmoral significance except insofar as it affects things outside. But a machine\nsuperintelligence could create internal processes that have moral status. For\nexample, a very detailed simulation of some actual or hypothetical human mind\nmight be conscious and in many ways comparable to an emulation. One can\nimagine scenarios in which an AI creates trillions of such conscious\nsimulations, perhaps in order to improve its understanding of human psychology\nand sociology. These simulations might be placed in simulated environments and\nsubjected to various stimuli, and their reactions studied. Once their\ninformational usefulness has been exhausted, they might be destroyed (much as\nlab rats are routinely sacrificed by human scientists at the end of an\nexperiment).\n\nIf such practices were applied to beings that have high moral status—such as\nsimulated humans or many other types of sentient mind—the outcome might be\nequivalent to genocide and thus extremely morally problematic. The number of\nvictims, moreover, might be orders of magnitude larger than in any genocide in\nhistory.\n\nThe claim here is not that creating sentient simulations is necessarily\nmorally wrong in all situations. Much would depend on the conditions under\nwhich these beings would live, in particular the hedonic quality of their\nexperience but possibly on many other factors as well. Developing an ethics\nfor these matters is a task outside the scope of this book. It is clear,\nhowever, that there is at least the potential for a vast amount of death and\nsuffering among simulated or digital minds, and, _a fortiori_ , the potential\nfor morally catastrophic\noutcomes.[9](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1030581)\n\nThere might also be other instrumental reasons, aside from epistemic ones, for\na machine superintelligence to run computations that instantiate sentient\nminds or that otherwise infract moral norms. A superintelligence might\nthreaten to mistreat, or commit to reward, sentient simulations in order to\nblackmail or incentivize various external agents; or it might create\nsimulations in order to induce indexical uncertainty in outside\nobservers.[10](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1031660)\n\n![Image](images/00031.jpg)\n\nThis inventory is incomplete. We will encounter additional malignant failure\nmodes in later chapters. But we have seen enough to conclude that scenarios in\nwhich some machine intelligence gets a decisive strategic advantage are to be\nviewed with grave concern.\n\n\n## [CHAPTER 9  \nThe control\nproblem](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_006.html#filepos20755)\n\n**If we are threatened with existential catastrophe as the default outcome of\nan intelligence explosion, our thinking must immediately turn to the search\nfor countermeasures. Is there some way to avoid the default outcome? Is it\npossible to engineer a controlled detonation? In this chapter we begin to\nanalyze the control problem, the unique principal–agent problem that arises\nwith the creation of an artificial superintelligent agent. We distinguish two\nbroad classes of potential methods for addressing this problem—capability\ncontrol and motivation selection—and we examine several specific techniques\nwithin each class. We also allude to the esoteric possibility of “anthropic\ncapture.”**\n\n### [Two agency\nproblems](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_006.html#filepos20864)\n\nIf we suspect that the default outcome of an intelligence explosion is\nexistential catastrophe, our thinking must immediately turn to whether, and if\nso how, this default outcome can be avoided. Is it possible to achieve a\n“controlled detonation”? Could we engineer the initial conditions of an\nintelligence explosion so as to achieve a specific desired outcome, or at\nleast to ensure that the result lies somewhere in the class of broadly\nacceptable outcomes? More specifically: how can the sponsor of a project that\naims to develop superintelligence ensure that the project, if successful,\nproduces a superintelligence that would realize the sponsor’s goals? We can\ndivide this control problem into two parts. One part is generic, the other\nunique to the present context.\n\nThis first part—what we shall call the _first principal–agent problem_ —arises\nwhenever some human entity (“the principal”) appoints another (“the agent”) to\nact in the former’s interest. This type of agency problem has been extensively\nstudied by\neconomists.[1](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1031893)\nIt becomes relevant to our present concern if the people creating an AI are\ndistinct from the people commissioning its creation. The project’s owner or\nsponsor (which could be anything ranging from a single individual to humanity\nas a whole) might then worry that the scientists and programmers implementing\nthe project will not act in the sponsor’s best\ninterest.[2](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1032021)\nAlthough this type of agency problem could pose significant challenges to a\nproject sponsor, it is not a problem unique to intelligence amplification or\nAI projects. Principal–agent problems of this sort are ubiquitous in human\neconomic and political interactions, and there are many ways of dealing with\nthem. For instance, the risk that a disloyal employee will sabotage or subvert\nthe project could be minimized through careful background checks of key\npersonnel, the use of a good version-control system for software projects, and\nintensive oversight from multiple independent monitors and auditors. Of\ncourse, such safeguards come at a cost—they expand staffing needs, complicate\npersonnel selection, hinder creativity, and stifle independent and critical\nthought, all of which could reduce the pace of progress. These costs could be\nsignificant, especially for projects that have tight budgets, or that perceive\nthemselves to be in a close race in a winner-takes-all competition. In such\nsituations, projects may skimp on procedural safeguards, creating\npossibilities for potentially catastrophic principal–agent failures of the\nfirst type.\n\nThe other part of the control problem is more specific to the context of an\nintelligence explosion. This is the problem that a project faces when it seeks\nto ensure that the superintelligence it is building will not harm the\nproject’s interests. This part, too, can be thought of as a principal–agent\nproblem—the _second principal–agent problem_. In this case, the agent is not a\nhuman agent operating on behalf of a human principal. Instead, the agent is\nthe superintelligent system. Whereas the first principal–agent problem occurs\nmainly in the development phase, the second agency problem threatens to cause\ntrouble mainly in the superintelligence’s operational phase.\n\n######\n\n> **Exhibit 1 Two agency problems**\n\nThe first principal–agent problem\n\n• Human v. Human (Sponsor → Developer)\n\n• Occurs mainly in developmental phase\n\n• Standard management techniques apply\n\nThe second principal–agent problem (“the control problem”)\n\n• Human v. Superintelligence (Project → System)\n\n• Occurs mainly in operational (and bootstrap) phase\n\n• New techniques needed\n\nThis second agency problem poses an unprecedented challenge. Solving it will\nrequire new techniques. We have already considered some of the difficulties\ninvolved. We saw, in particular, that the treacherous turn syndrome vitiates\nwhat might otherwise have seemed like a promising set of methods, ones that\nrely on observing an AI’s behavior in its developmental phase and allowing the\nAI to graduate from a secure environment once it has accumulated a track\nrecord of taking appropriate actions. Other technologies can often be safety-\ntested in the laboratory or in small field studies, and then rolled out\ngradually with a possibility of halting deployment if unexpected troubles\narise. Their performance in preliminary trials helps us make reasonable\ninferences about their future reliability. Such behavioral methods are\ndefeated in the case of superintelligence because of the strategic planning\nability of general\nintelligence.[3](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1033433)\n\nSince the behavioral approach is unavailing, we must look for alternatives. We\ncan divide potential control methods into two broad classes: _capability\ncontrol methods_ , which aim to control what the superintelligence can do; and\n_motivation selection methods_ , which aim to control what it wants to do.\nSome of the methods are compatible while others represent mutually exclusive\nalternatives. In this chapter we canvass the main options. (In the next four\nchapters, we will explore some of the key issues at greater depth.)\n\nIt is important to realize that some control method (or combination of\nmethods) must be implemented _before_ the system becomes superintelligent. It\ncannot be done after the system has obtained a decisive strategic advantage.\nThe need to solve the control problem in advance—and to implement the solution\nsuccessfully in the very first system to attain superintelligence—is part of\nwhat makes achieving a controlled detonation such a daunting challenge.\n\n### [Capability control\nmethods](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_006.html#filepos20959)\n\nCapability control methods seek to prevent undesirable outcomes by limiting\nwhat the superintelligence can do. This might involve placing the\nsuperintelligence in an environment in which it is unable to cause harm\n(_boxing methods_) or in which there are strongly convergent instrumental\nreasons not to engage in harmful behavior (_incentive methods_). It might also\ninvolve limiting the internal capacities of the superintelligence\n(_stunting_). In addition, capability control methods might involve the use of\nmechanisms to automatically detect and react to various kinds of containment\nfailure or attempted transgression (_tripwires_).\n\n#### [Boxing\nmethods](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_006.html#filepos21061)\n\nBoxing methods can be subdivided into physical and informational containment\nmethods.\n\nPhysical containment aims to confine the system to a “box,” i.e. to prevent\nthe system from interacting with the external world otherwise than via\nspecific restricted output channels. The boxed system would not have access to\nphysical manipulators outside of the box. Removing manipulators (such as\nrobotic arms) from inside the box as well would prevent the system from\nconstructing physical devices that could breach the confinement.\n\nFor extra security, the system should be placed in a metal mesh to prevent it\nfrom transmitting radio signals, which might otherwise offer a means of\nmanipulating electronic objects such as radio receivers in the environment.\nNote, by the way, how easy it might have been to overlook the need for this\nprecaution. One might naively have assumed that an agent without a manipulator\ncould not affect the external world. But it might be possible for a machine\nintelligence to generate radio waves even when it lacks access to external\nmanipulators, simply “by thinking” (that is, by shuffling the electrons in its\ncircuitry in particular\npatterns).[4](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1033854)\nOnce pointed out, this vulnerability can be patched by enclosing the system in\na Faraday cage—but we are left wondering how many other similarly subtle\nvulnerabilities might exist. Each time we hear of a seemingly foolproof\nsecurity design that has an unexpected flaw, we should prick up our ears.\nThese occasions grace us with the opportunity to abandon a life of\noverconfidence and resolve to become better\nBayesians.[5](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1034509)\n\nPhysical confinement has several advantages. It is easy to implement. It can\nbe applied to many machine intelligence architectures, even ones that were not\ninitially designed with safety as an objective. It can be used in combination\nwith most other control methods. And it seems unlikely to go wrong by\nbackfiring: that is, while it might fail to ensure safety, it is unlikely to\ncause a catastrophe that would not otherwise have occurred. The main\ndisadvantage with physical confinement is that it reduces the functionality of\nthe superintelligence (an issue to which we will return in [Chapter\n10](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_017.html#filepos509701)). Another\nconcern is that it might encourage a false sense of security, though this is\navoidable if we regard physical confinement as icing on the cake rather than\nthe main substance of our precautions.\n\nInformational containment aims to restrict what information is allowed to exit\nthe box. We have already seen how a superintelligence that has access to an\nInternet port, such that it can message outside entities, is potentially\nunsafe: even if it starts out without access to physical actuators, it may use\nits information output channel to get human beings to do its bidding. An\nobvious informational containment method, therefore, is to bar the system from\naccessing communications networks.\n\nUnplugging an AI from the Internet does not, however, ensure safety if there\nare one or more humans serving as the system’s gatekeepers and remaining in\ncommunication with it. During the development phase, the programmers serve as\ngatekeepers; during the operational phase, the system’s owner might play this\nrole. If the AI can persuade or trick a gatekeeper to let it out of the box,\nresulting in its gaining access either to the Internet or directly to physical\nmanipulators, then the boxing strategy has failed. Human beings are not secure\nsystems, especially not when pitched against a superintelligent schemer and\npersuader.[6](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1035124)\n\nAlthough routing the outbound communications of a machine intelligence to a\nhuman gatekeeper does not guarantee safety, it could serve as an auxiliary\nsafety measure. There are precautions that might reduce the risk of the AI\nescaping through the gatekeeper. The AI could be allowed to communicate only\nvia a low-bandwidth teletype interface. This would make it slightly harder to\npsychologically manipulate the gatekeepers (precluding the use of emotive\nsound and imagery and tricks such as flashing hypnotic patterns to induce a\ntrancelike\nstate).[7](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1036017)\nGoing further, the content (not just the mode of presentation) of the output\ncould be restricted; so that, for example, the AI would be constrained to\ngiving single-bit answers to yes/no questions. The idea of building a\nquestion-answering system will be examined further in the next chapter.\n\nThe limiting case of the boxing approach would be a system kept in complete\nphysical and informational isolation. Even if achievable, however, such an\nisolated system would be rather useless since it would have no effect on the\nexternal world. It might perhaps be thought that some scientific purpose could\nbe served by creating a superintelligence and keeping it in isolation: by\nstudying a self-contained model system, one could learn about its internal\ndynamics and its behavior patterns. But this would be an error. As soon as the\nsystem is observed, it ceases to be informationally\nisolated.[8](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1036338)\nInformation can be transmitted not only via messages that an AI sends out\nthrough a designated “output channel” but also via any observation an outsider\nmakes of some causal consequence of the AI’s workings, direct or indirect—its\npower consumption, its CPU and memory usage, its computational states, or\nindeed any traces left behind after it has been shut down. An AI anticipating\nthat it might be observed in any of these ways could strategically adopt\nbehaviors designed to influence the hypothesized observers. For example, the\nAI could “malfunction” in whichever way would most increase the probability\nthat it will ultimately be reactivated or that another superintelligence will\nbe created with goals similar to its own.\n\n#### [Incentive\nmethods](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_006.html#filepos21160)\n\nIncentive methods involve placing an agent in an environment where it finds\ninstrumental reasons to act in ways that promote the principal’s interests.\n\nConsider a billionaire who uses her fortune to set up a large charitable\nfoundation. Once created, the foundation may be powerful—more powerful than\nmost individuals, including its founder, who might have donated most of her\nwealth. To control the foundation, the founder lays down its purpose in\narticles of incorporation and bylaws, and appoints a board of directors\nsympathetic to her cause. These measures constitute a form of motivation\nselection, since they aim to shape foundation’s preferences. But even if such\nattempts to customize the organizational internals fail, the foundation’s\nbehavior would remain circumscribed by its social and legal milieu. The\nfoundation would have an incentive to obey the law, for example, lest it be\nshut down or fined. It would have an incentive to offer its employees\nacceptable pay and working conditions, and to satisfy external stakeholders.\nWhatever its final goals, the foundation thus has instrumental reasons to\nconform its behavior to various social norms.\n\nMight one not hope that a machine superintelligence would likewise be hemmed\nin by the need to get along with the other actors with which it shares the\nstage? Though this might seem like a straightforward way of dealing with the\ncontrol problem, it is not free of obstacles. In particular, it presupposes a\nbalance of power: legal or economic sanctions cannot restrain an agent that\nhas a decisive strategic advantage. Social integration can therefore not be\nrelied upon as a control method in fast or medium takeoff scenarios that\nfeature a winner-takes-all dynamic.\n\nHow about in multipolar scenarios, wherein several agencies emerge post-\ntransition with comparable levels of capability? Unless the default trajectory\nis one with a slow takeoff, achieving such a power distribution may require a\ncarefully orchestrated ascent wherein different projects are deliberately\nsynchronized to prevent any one of them from ever pulling ahead of the\npack.[9](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1037029) Even\nif a multipolar outcome does result, social integration is not a perfect\nsolution. By relying on social integration to solve the control problem, the\nprincipal risks sacrificing a large portion of her potential influence.\nAlthough a balance of power might prevent a particular AI from taking over the\nworld, that AI will still have _some_ power to affect outcomes; and if that\npower is used to promote some arbitrary final goal—maximizing paperclip\nproduction—it is probably not being used to advance the interests of the\nprincipal. Imagine our billionaire endowing a new foundation and allowing its\nmission to be set by a random word generator: not a species-level threat, but\nsurely a wasted opportunity.\n\nA related but importantly different idea is that an AI, by interacting freely\nin society, would acquire new human-friendly final goals. Some such process of\nsocialization takes place in us humans. We internalize norms and ideologies,\nand we come to value other individuals for their own sakes in consequence of\nour experiences with them. But this is not a universal dynamic present in all\nintelligent systems. As discussed earlier, many types of agent in many\nsituations will have convergent instrumental reasons _not_ to permit changes\nin their final goals. (One might consider trying to design a special kind of\ngoal system that can acquire final goals in the manner that humans do; but\nthis would not count as a capability control method. We will discuss some\npossible methods of value acquisition in [Chapter\n12](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos645293).)\n\nCapability control through social integration and balance of power relies upon\ndiffuse social forces rewarding and penalizing the AI. Another type of\nincentive method would involve creating a setup wherein the AI can be rewarded\nand penalized by the project that creates it, and thereby incentivized to act\nin the interests of the principal. To achieve this, the AI would be placed in\na surveillance context that allows its behavior to be monitored and evaluated,\neither manually or by some automated process. The AI would know that a\npositive evaluation would bring about some outcome it desires and that a\nnegative evaluation would fail to do so. Theoretically, the reward could be\nthe fulfillment of some convergent instrumental goal. Without knowing anything\nspecific about the AI’s motivation system, however, it might be difficult to\ncalibrate the reward mechanism. For example, we could end up with an AI that\nwould be willing to take extreme risks for the sake of a small chance of\neventually obtaining control of a large share of the universe. It could be\nexpensive to offer the AI a higher expected utility as reward for cooperation\nthan the AI could hope to achieve by defecting and trying to\nescape.[10](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1038300)\n\nA better alternative might be to combine the incentive method with the use of\nmotivation selection to give the AI a final goal that makes it easier to\ncontrol. Suppose that an AI were designed to have as its final goal that a\nparticular red button inside a command bunker never be pressed. Since the\npressing of the button is disvalued intrinsically and not because of its\ncausal consequences, the button could be completely inert: it could be made of\nPlay-Doh. Furthermore, it is irrelevant whether the AI can ever know whether\nthe button had been pressed. What is essential is that the AI believes that\nthe button will more likely remain unpressed if the AI continuously acts in\nthe principal’s interest than if it rebels.\n\nRefinements to this setup are possible. Instead of trying to endow an AI with\na final goal that refers to a physical button, one could build an AI that\nplaces final value on receiving a stream of “cryptographic reward\ntokens.”[11](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1038513)\nThese would be sequences of numbers serving as keys to ciphers that would have\nbeen generated before the AI was created and that would have been built into\nits motivation\nsystem.[12](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1038640)\nThese special number sequences would be extremely desirable to the AI,\nconstituting a special kind of reward token that the AI could not attain\nthough\nwireheading.[13](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1039341)\nThe keys would be stored in a secure location where they could be quickly\ndestroyed if the AI ever made an attempt to seize them. So long as the AI\ncooperates, the keys are doled out at a steady rate.\n\nThe AI could then be given a bounded utility function, and matters could be\narranged so that the AI would get (for instance) 99% of its maximum utility\nfrom the first reward token; 99% of its remaining utility potential from the\nsecond reward token; and so on. By keeping the “discount rate” high, and by\ntransmitting reward tokens to the AI at a reasonably high rate, one would\ndissuade the AI from attempting to carry out long-term plans that would\nincrease the expected number of reward tokens in the distal future at the\nexpense of some risk that its reward stream will be cut short or that the next\nreward token will be cancelled and destroyed. If the AI always believes that\ndefecting (i.e. doing anything other than its best to look after the\nprincipal’s interests) would incur at least a 2% risk of forfeiting the next\nreward token, whereas cooperating would reduce the risk of forfeiting the next\nreward token to below 1%, then a utility-maximizing AI would always cooperate.\n\nWhat might go wrong with such an incentive scheme? One possibility is that the\nAI will not trust the human operator to deliver the promised rewards. The\ntrack record of human reliability is something other than a straight line of\nunerring perfection. The AI would reasonably worry that the operator will\nchange his mind, or raise the performance bar, or fail to recognize that the\nAI has done its part. The AI would also worry about the operator becoming\nincapacitated. The combined risk of such failures might exceed the risk of\nattempting to seize control of the reward mechanism. Even a boxed AI\npossessing the panoply of superpowers is a strong force. (For an AI that is\n_not_ boxed to begin with, hijacking the human-governed reward mechanism may\nbe like taking candy from a baby.)\n\nAnother problem with the incentive scheme is that it presupposes that we can\ntell whether the outcomes produced by the AI are in our interest. As later\nchapters will elaborate, this presupposition is not innocuous.\n\nA full assessment of the feasibility of incentive methods would also have to\ntake into account a range of other factors, including some esoteric\nconsiderations that might conceivably make such methods more viable than a\npreliminary analysis would suggest. In particular, the AI may face\nineliminable indexical uncertainty if it could not be sure that it does not\ninhabit a computer simulation (as opposed to “basement-level,” non-simulated\nphysical reality), and this epistemic predicament may radically influence the\nAI’s deliberations (see Box 8).\n\n* * *\n\n### **Box 8 Anthropic capture**\n\nThe AI might assign a substantial probability to its simulation hypothesis,\nthe hypothesis that it is living in a computer simulation. Even today, many\nAIs inhabit simulated worlds—worlds consisting of geometric line drawings,\ntexts, chess games, or simple virtual realities, and in which the laws of\nphysics deviate sharply from the laws of physics that we believe govern the\nworld of our own experience. Richer and more complicated virtual worlds will\nbecome feasible with improvements in programming techniques and computing\npower. A mature superintelligence could create virtual worlds that appear to\nits inhabitants much the same as our world appears to us. It might create vast\nnumbers of such worlds, running the same simulation many times or with small\nvariations. The inhabitants would not necessarily be able to tell whether\ntheir world is simulated or not; but if they are intelligent enough they could\nconsider the possibility and assign it some probability. In light of the\nsimulation argument (a full discussion of which is beyond the scope of this\nbook) that probability could be\nsubstantial.[14](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1039697)\n\nThis predicament especially afflicts relatively early-stage\nsuperintelligences, ones that have not yet expanded to take advantage of the\ncosmic endowment. An early-stage superintelligence, which uses only a small\nfraction of the resources of a single planet, would be much less expensive to\nsimulate than a mature intergalactic superintelligence. Potential\nsimulators—that is, other more mature civilizations—would be able to run great\nnumbers of simulations of such early-stage AIs even by dedicating a minute\nfraction of their computational resources to that purpose. If at least some\n(non-trivial fraction) of these mature superintelligent civilizations choose\nto use this ability, early-stage AIs should assign a substantial probability\nto being in a simulation.\n\nHow an AI would be affected by the simulation hypothesis depends on its\nvalues.[15](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1039860)\nConsider first an AI that has a “resource-insatiable” final goal, such as the\ngoal of maximizing a utility function that is linear in the number of\npaperclips caused by the AI to exist—twice as many paperclips, twice as good.\nSuch an AI might care less about the simulation hypothesis, on grounds that\nits ability to influence how many paperclips are produced looks much more\nlimited if the AI is in a simulation than if it is in basement-level physical\nreality. The AI might accordingly behave much _as if_ it were certain that the\nsimulation hypothesis is false, even if it assigned a fairly high probability\nto the simulation hypothesis being\ntrue.[16](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1039971) The\nAI would reason that it does not much matter how it acts if the simulation\nhypothesis is true, whereas its actions make a great difference if the\nhypothesis is false; so it would select actions mainly by reference to how\ngood they are conditional on the simulation hypothesis being false.\n\nBy contrast, consider an AI that has a more modest final goal, one that could\nbe satisfied with a small amount of resources, such as the goal of receiving\nsome pre-produced cryptographic reward tokens, or the goal of causing the\nexistence of forty-five virtual paperclips. Such an AI should _not_ discount\nthose possible worlds in which it inhabits a simulation. A substantial portion\nof the AI’s total expected utility might derive from those possible worlds.\nThe decision-making of an AI with goals that are easily resource-satiable may\ntherefore—if it assigns a high probability to the simulation hypothesis—be\ndominated by considerations about which actions would produce the best result\nif its perceived world is a simulation. Such an AI (even if it is, in fact,\nnot in a simulation) might therefore be heavily influenced by its beliefs\nabout which behaviors would be rewarded in a simulation.\n\nIn particular, if an AI with resource-satiable final goals believes that in\nmost simulated worlds that match its observations it will be rewarded if it\ncooperates (but not if it attempts to escape its box or contravene the\ninterests of its creator) then it may choose to cooperate. We could therefore\nfind that even an AI with a decisive strategic advantage, one that could in\nfact realize its final goals to a greater extent by taking over the world than\nby refraining from doing so, would nevertheless balk at doing so.\n\nThus Conscience does make Cowards of us all,\n\nAnd thus the Native hue of Resolution\n\nIs sicklied o’er, with the pale cast of Thought,\n\nAnd enterprises of great pith and moment,\n\nWith this regard their Currents turn away,\n\nAnd lose the name of Action.\n\n(Shakespeare, _Hamlet_ , Act iii. Sc. 1)\n\nA mere line in the sand, backed by the clout of a nonexistent simulator, could\nprove a stronger restraint than a two-foot-thick solid steel\ndoor.[17](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1040952)\n\n* * *\n\n#### [Stunting](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_006.html#filepos21262)\n\nAnother possible capability control method is to limit the system’s\nintellectual faculties or its access to information. This might be done by\nrunning the AI on hardware that is slow or short on memory. In the case of a\nboxed system, information inflow could also be restricted.\n\nStunting an AI in these ways would limit its usefulness. The method thus faces\na dilemma: too little stunting, and the AI might have the wit to figure out\nsome way to make itself more intelligent (and thence to world domination); too\nmuch, and the AI is just another piece of dumb software. A radically stunted\nAI is certainly safe but does not solve the problem of how to achieve a\ncontrolled detonation: an intelligence explosion would remain possible and\nwould simply be triggered by some other system instead, perhaps at a slightly\nlater date.\n\nOne might think it would be safe to build a superintelligence provided it is\nonly given data about some narrow domain of facts. For example, one might\nbuild an AI that lacks sensors and that has preloaded into its memory only\nfacts about petroleum engineering or peptide chemistry. But if the AI is\nsuperintelligent—if it is has a superhuman level of _general_\nintelligence—such data deprivation does not guarantee safety.\n\nThere are several reasons for this. First, the notion of information being\n“about” a certain topic is generally problematic. Any piece of information can\nin principle be relevant to any topic whatsoever, depending on the background\ninformation of a\nreasoner.[18](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1041590)\nFurthermore, a given data set contains information not only about the domain\nfrom which the data was collected but also about various circumstantial facts.\nA shrewd mind looking over a knowledge base that is nominally about peptide\nchemistry might infer things about a wide range of topics. The fact that\ncertain information is included and other information is not could tell an AI\nsomething about the state of human science, the methods and instruments\navailable to study peptides, the fabrication technologies used to make these\ninstruments, and the nature of the brains and societies that conceived the\nstudies and the instruments. It might be that a _superintelligence_ could\ncorrectly surmise a great deal from what seem, to dull-witted human minds,\nmeager scraps of evidence. Even without any designated knowledge base at all,\na sufficiently superior mind might be able to learn much by simply\nintrospecting on the workings of its own psyche—the design choices reflected\nin its source code, the physical characteristics of its\ncircuitry.[19](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1041720)\nPerhaps a superintelligence could even deduce much about the likely properties\nof the world _a priori_ (combining logical inference with a probability prior\nbiased toward simpler worlds, and a few elementary facts implied by the\nsuperintelligence’s existence as a reasoning system). It might imagine the\nconsequences of different possible laws of physics: what kind of planets would\nform, what kind of intelligent life would evolve, what kind of societies would\ndevelop, what kind of methods to solve the control problem would be attempted,\nhow those methods could be\ndefeated.[20](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1042096)\n\nFor these reasons it would be imprudent to rely on information deprivation as\nthe main check on a superintelligence’s power. Some forms of stunting however,\nincluding limiting hardware capacity and preventing the development of\nspecific cognitive faculties, might find use as auxiliary control methods in\nthe developmental phase. They could be used, for instance, to regulate the\nrate of capacity gain in an intermediary AI and to allow tests to be run at\ndifferent levels of capability short of full-throttled superintelligence.\n\n####\n[Tripwires](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_006.html#filepos21355)\n\nA tripwire is a mechanism that performs diagnostic tests on the system\n(possibly without its knowledge) and effects a shutdown if it detects signs of\ndangerous activity. Programmers can then scrutinize recordings of the\nsuspicious activity at their leisure, and make changes to the AI’s code base\nor environment before restarting it. If they find that the problem is\nsufficiently deep-rooted, they could decide to abort the entire project.\n\nTripwires differ from incentive methods in that they do not rely on the system\nbeing aware of the consequences of engaging in forbidden activities. Tripwires\nare more closely related to stunting methods. Like stunting, tripwires could\nbe used as a temporary safeguard, providing a degree of protection during the\ndevelopment phase. In principle, tripwires can also be used during the\noperational phase, particularly for a boxed system. However, the ability of\ntripwires to constrain a full-fledged superintelligence must remain very much\nin doubt, since it would be hard for us to assure ourselves that such an agent\ncould not find ways to subvert any tripwire devised by the human intellect.\n\n**Table 9 _Different kinds of tripwires_**\n\n|  \n---|---  \n_Type of monitoring_ | _Description and examples_  \nBehavior | Detectors could be placed around a boxed AI to detect attempts to breach the containment. For example, detectors could intercept attempts at radio communication or at accessing internal computational resources intended to be off limits. An “Ethernet port of Eden” could be installed: an apparent connection to the internet that leads to a shutdown switch.  \nAbility | Automated capability testing could be performed at frequent intervals to determine the AI’s skill in various domains. If either the rate of improvement is unexpectedly high or the AI attains a level of competence that brings it close to the potential danger zone, the system could be slowed down or paused, in order to allow the programmers to monitor further developments more closely.  \nContent | The AI’s internal processes could be monitored for suspicious activity. For example, any change to the AI’s representation of its final values might trigger an automatic shutdown and review. The AI’s resource use and algorithmic methods may also be also be monitored for unexpected patterns. More ambitiously, the AI’s beliefs, plans, subgoals, and justifications may be continuously scrutinized, to ensure they conform to the programmers’ expectations. Such monitoring of the AI’s intentional states would enable a particularly powerful form of content monitoring: the scanning of the AI’s cognitive processes for a _conception of deception_ —that vulnerable moment when an unfriendly AI first forms the intention to conceal its true intentions.[21](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1042394) Content monitoring that requires that the AI’s intentional states be transparent to the programmers or to an automatic monitoring mechanism may not be feasible for all kinds of AI architectures. (Some neural networks, for instance, are opaque, as they represent information holistically and in ways that do not necessarily match up with human concepts.) This may be a reason to avoid using such architectures.  \n  \n* * *\n\nAs an adjunct safety measure in the development phase, though, tripwires are\nimportant. It would be good practice for the AI’s designers to codify their\nexpectations about how the system is supposed to function, and then to\ncircumscribe the AI with tripwires that would be activated if any of these\nexpectations is violated. To make the tests more stringent, “honeypots” could\nbe strategically placed to create temptations for a malfunctioning AI to\ncommit some easily observable violation. For instance, if an AI has been\ndesigned in such a way that it is supposed not to want to access the internet,\na fake Ethernet port could be installed (leading to an automatic shutdown\nswitch) just to see if they AI tries to use it. (Some examples of tripwires\nare given in Table 9.)\n\nIt should be emphasized that the value of a tripwire depends not only on the\nmechanism itself but also—critically—on how a project reacts when a tripwire\nis triggered. If the programmers or project managers, impatient to make\nprogress, simply switch the system back on again—or if they do so after making\nsome token modification to prevent the tripwire being triggered on the next\nrun—then no safety has been gained even if the tripwire itself works exactly\nas intended.\n\n### [Motivation selection\nmethods](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_006.html#filepos21449)\n\nMotivation selection methods seek to prevent undesirable outcomes by shaping\nwhat the superintelligence wants to do. By engineering the agent’s motivation\nsystem and its final goals, these methods would produce a superintelligence\nthat would not _want_ to exploit a decisive strategic advantage in a harmful\nway. Since a superintelligent agent is skilled at achieving its ends, if it\nprefers not to cause harm (in some appropriate sense of “harm”) then it would\ntend not to cause harm (in that sense of “harm”).\n\nMotivation selection can involve explicitly formulating a goal or set of rules\nto be followed (_direct specification_) or setting up the system so that it\ncan discover an appropriate set of values for itself by reference to some\nimplicitly or indirectly formulated criterion (_indirect normativity_). One\noption in motivation selection is to try to build the system so that it would\nhave modest, non-ambitious goals (_domesticity_). An alternative to creating a\nmotivation system from scratch is to select an agent that already has an\nacceptable motivation system and then augment that agent’s cognitive powers to\nmake it superintelligent, while ensuring that the motivation system does not\nget corrupted in the process (_augmentation_). Let us look at these in turn.\n\n### [Direct\nspecification](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_006.html#filepos21553)\n\nDirect specification is the most straightforward approach to the control\nproblem. The approach comes in two versions, rule-based and consequentialist,\nand involves trying to explicitly define a set of rules or values that will\ncause even a free-roaming superintelligent AI to act safely and beneficially.\nDirect specification, however, faces what may be insuperable obstacles,\nderiving from both the difficulties in determining which rules or values we\nwould wish the AI to be guided by and the difficulties in expressing those\nrules or values in computer-readable code.\n\nThe traditional illustration of the direct rule-based approach is the “three\nlaws of robotics” concept, formulated by science fiction author Isaac Asimov\nin a short story published in\n1942.[22](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1043021) The\nthree laws were: (1) A robot may not injure a human being or, through\ninaction, allow a human being to come to harm; (2) A robot must obey any\norders given to it by human beings, except where such orders would conflict\nwith the First Law; (3) A robot must protect its own existence as long as such\nprotection does not conflict with the First or Second Law. Embarrassingly for\nour species, Asimov’s laws remained state-of-the-art for over half a century:\nthis despite obvious problems with the approach, some of which are explored in\nAsimov’s own writings (Asimov probably having formulated the laws in the first\nplace precisely so that they would fail in interesting ways, providing fertile\nplot complications for his\nstories).[23](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1043305)\n\nBertrand Russell, who spent many years working on the foundations of\nmathematics, once remarked that “everything is vague to a degree you do not\nrealize till you have tried to make it\nprecise.”[24](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1043416)\nRussell’s dictum applies in spades to the direct specification approach.\nConsider, for example, how one might explicate Asimov’s first law. Does it\nmean that the robot should minimize the probability of any human being coming\nto harm? In that case the other laws become otiose since it is always possible\nfor the AI to take some action that would have at least some microscopic\neffect on the probability of a human being coming to harm. How is the robot to\nbalance a large risk of a few humans coming to harm versus a small risk of\nmany humans being harmed? How do we define “harm” anyway? How should the harm\nof physical pain be weighed against the harm of architectural ugliness or\nsocial injustice? Is a sadist harmed if he is prevented from tormenting his\nvictim? How do we define “human being”? Why is no consideration given to other\nmorally considerable beings, such as sentient nonhuman animals and digital\nminds? The more one ponders, the more the questions proliferate.\n\nPerhaps the closest existing analog to a rule set that could govern the\nactions of a superintelligence operating in the world at large is a legal\nsystem. But legal systems have developed through a long process of trial and\nerror, and they regulate relatively slowly-changing human societies. Laws can\nbe revised when necessary. Most importantly, legal systems are administered by\njudges and juries who generally apply a measure of common sense and human\ndecency to ignore logically possible legal interpretations that are\nsufficiently obviously unwanted and unintended by the lawgivers. It is\nprobably humanly impossible to explicitly formulate a highly complex set of\ndetailed rules, have them apply across a highly diverse set of circumstances,\nand get it right on the first\nimplementation.[25](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1043532)\n\nProblems for the direct consequentialist approach are similar to those for the\ndirect rule-based approach. This is true even if the AI is intended to serve\nsome apparently simple purpose such as implementing a version of classical\nutilitarianism. For instance, the goal “Maximize the expectation of the\nbalance of pleasure over pain in the world” may appear simple. Yet expressing\nit in computer code would involve, among other things, specifying how to\nrecognize pleasure and pain. Doing this reliably might require solving an\narray of persistent problems in the philosophy of mind—even just to obtain a\ncorrect account expressed in a natural language, an account which would then,\nsomehow, have to be translated into a programming language.\n\nA small error in either the philosophical account or its translation into code\ncould have catastrophic consequences. Consider an AI that has hedonism as its\nfinal goal, and which would therefore like to tile the universe with\n“hedonium” (matter organized in a configuration that is optimal for the\ngeneration of pleasurable experience). To this end, the AI might produce\ncomputronium (matter organized in a configuration that is optimal for\ncomputation) and use it to implement digital minds in states of euphoria. In\norder to maximize efficiency, the AI omits from the implementation any mental\nfaculties that are not essential for the experience of pleasure, and exploits\nany computational shortcuts that according to its definition of pleasure do\nnot vitiate the generation of pleasure. For instance, the AI might confine its\nsimulation to reward circuitry, eliding faculties such as memory, sensory\nperception, executive function, and language; it might simulate minds at a\nrelatively coarse-grained level of functionality, omitting lower-level\nneuronal processes; it might replace commonly repeated computations with calls\nto a lookup table; or it might put in place some arrangement whereby multiple\nminds would share most parts of their underlying computational machinery\n(their “supervenience bases” in philosophical parlance). Such tricks could\ngreatly increase the quantity of pleasure producible with a given amount of\nresources. It is unclear how desirable this would be. Furthermore, if the AI’s\ncriterion for determining whether a physical process generates pleasure is\nwrong, then the AI’s optimizations might throw the baby out with the\nbathwater: discarding something which is inessential according to the AI’s\ncriterion yet essential according to the criteria implicit in our human\nvalues. The universe then gets filled not with exultingly heaving hedonium but\nwith computational processes that are unconscious and completely worthless—the\nequivalent of a smiley-face sticker xeroxed trillions upon trillions of times\nand plastered across the galaxies.\n\n###\n[Domesticity](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_006.html#filepos21658)\n\nOne special type of final goal which might be more amenable to direct\nspecification than the examples given above is the goal of self-limitation.\nWhile it seems extremely difficult to specify how one would want a\nsuperintelligence to behave in the world _in general_ —since this would\nrequire us to account for all the trade-offs in all the situations that could\narise—it might be feasible to specify how a superintelligence should behave in\none particular situation. We could therefore seek to motivate the system to\nconfine itself to acting on a small scale, within a narrow context, and\nthrough a limited set of action modes. We will refer to this approach of\ngiving the AI final goals aimed at limiting the scope of its ambitions and\nactivities as “domesticity.”\n\nFor example, one could try to design an AI such that it would function as a\nquestion-answering device (an “oracle,” to anticipate the terminology that we\nwill introduce in the next chapter). Simply giving the AI the final goal of\nproducing maximally accurate answers to any question posed to it would be\nunsafe—recall the “Riemann hypothesis catastrophe” described in [Chapter\n8](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_015.html#filepos408438). (Reflect,\nalso, that this goal would incentivize the AI to take actions to ensure that\nit is asked easy questions.) To achieve domesticity, one might try to define a\nfinal goal that would somehow overcome these difficulties: perhaps a goal that\ncombined the desiderata of answering questions correctly and minimizing the\nAI’s impact on the world except whatever impact results as an incidental\nconsequence of giving accurate and non-manipulative answers to the questions\nit is\nasked.[26](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1044256)\n\nThe direct specification of such a domesticity goal is more likely to be\nfeasible than the direct specification of either a more ambitious goal or a\ncomplete rule set for operating in an open-ended range of situations.\nSignificant challenges nonetheless remain. Care would have to be taken, for\ninstance, in the definition of what it would be for the AI to “minimize its\nimpact on the world” to ensure that the measure of the AI’s impact coincides\nwith our own standards for what counts as a large or a small impact. A bad\nmeasure would lead to bad trade-offs. There are also other kinds of risk\nassociated with building an oracle, which we will discuss later.\n\nThere is a natural fit between the domesticity approach and physical\ncontainment. One would try to “box” an AI such that the system is _unable_ to\nescape while simultaneously trying to shape the AI’s motivation system such\nthat it would be _unwilling_ to escape even if it found a way to do so. Other\nthings equal, the existence of multiple independent safety mechanisms should\nshorten the odds of\nsuccess.[27](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1044368)\n\n### [Indirect\nnormativity](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_006.html#filepos21754)\n\nIf direct specification seems hopeless, we might instead try indirect\nnormativity. The basic idea is that rather than specifying a concrete\nnormative standard directly, we specify a process for deriving a standard. We\nthen build the system so that it is motivated to carry out this process and to\nadopt whatever standard the process arrives\nat.[28](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1044806) For\nexample, the process could be to carry out an investigation into the empirical\nquestion of what some suitably idealized version of us would prefer the AI to\ndo. The final goal given to the AI in this example could be something along\nthe lines of “achieve that which we would have wished the AI to achieve if we\nhad thought about the matter long and hard.”\n\nFurther explanation of indirect normativity will have to await [Chapter\n13](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_020.html#filepos727275). There, we\nwill revisit the idea of “extrapolating our volition” and explore various\nalterative formulations. Indirect normativity is a very important approach to\nmotivation selection. Its promise lies in the fact that it could let us\noffload to the superintelligence much of the difficult cognitive work required\nto carry out a direct specification of an appropriate final goal.\n\n###\n[Augmentation](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_006.html#filepos21859)\n\nThe last motivation selection method on our list is augmentation. Here the\nidea is that rather than attempting to design a motivation system _de novo_ ,\nwe start with a system that already has an acceptable motivation system, and\nenhance its cognitive faculties to make it superintelligent. If all goes well,\nthis would give us a superintelligence with an acceptable motivation system.\n\nThis approach, obviously, is unavailing in the case of a newly created seed\nAI. But augmentation is a potential motivation selection method for other\npaths to superintelligence, including brain emulation, biological enhancement,\nbrain–computer interfaces, and networks and organizations, where there is a\npossibility of building out the system from a normative nucleus (regular human\nbeings) that already contains a representation of human value.\n\nThe attractiveness of augmentation may increase in proportion to our despair\nat the other approaches to the control problem. Creating a motivation system\nfor a seed AI that remains reliably safe and beneficial under recursive self-\nimprovement even as the system grows into a mature superintelligence is a tall\norder, especially if we must get the solution right on the first attempt. With\naugmentation, we would at least start with a system that has familiar and\nhuman-like motivations.\n\nOn the downside, it might be hard to ensure that a complex, evolved, kludgy,\nand poorly understood motivation system, like that of a human being, will not\nget corrupted when its cognitive engine blasts into the stratosphere. As\ndiscussed earlier, an imperfect brain emulation procedure that preserves\nintellectual functioning may not preserve all facets of personality. The same\nis true (though perhaps to a lesser degree) for biological enhancements of\ncognition, which might subtly affect motivation, and for collective\nintelligence enhancements of organizations and networks, which might adversely\nchange social dynamics (e.g. in ways that debase the collective’s attitude\ntoward outsiders or toward its own constituents). If superintelligence is\nachieved via any of these paths, a project sponsor would find guarantees about\nthe ultimate motivations of the mature system hard to come by. A\nmathematically well-specified and foundationally elegant AI architecture\nmight—for all its non-anthropomorphic otherness—offer greater transparency,\nperhaps even the prospect that important aspects of its functionality could be\nformally verified.\n\nIn the end, however one tallies up the advantages and disadvantages of\naugmentation, the choice as to whether to rely on it might be forced. If\nsuperintelligence is first achieved along the artificial intelligence path,\naugmentation is not applicable. Conversely, if superintelligence is first\nachieved along some non-AI path, then many of the other motivation selection\nmethods are inapplicable. Even so, views on how likely augmentation would be\nto succeed do have strategic relevance insofar as we have opportunities to\ninfluence which technology will first produce superintelligence.\n\n### [Synopsis](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_006.html#filepos21956)\n\nA quick synopsis might be called for before we close this chapter. We\ndistinguished two broad classes of methods for dealing with the agency problem\nat the heart of AI safety: capability control and motivation selection. Table\n10 gives a summary.\n\n**Table 10 _Control methods_**\n\n|  \n---|---  \n_Capability control_ |   \nBoxing methods | The system is confined in such a way that it can affect the external world only through some restricted, pre-approved channel. Encompasses physical and informational containment methods.  \nIncentive methods | The system is placed within an environment that provides appropriate incentives. This could involve social integration into a world of similarly powerful entities. Another variation is the use of (cryptographic) reward tokens. “Anthropic capture” is also a very important possibility but one that involves esoteric considerations.  \nStunting | Constraints are imposed on the cognitive capabilities of the system or its ability to affect key internal processes.  \nTripwires | Diagnostic tests are performed on the system (possibly without its knowledge) and a mechanism shuts down the system if dangerous activity is detected.  \nMotivation selection |   \nDirect specification | The system is endowed with some directly specified motivation system, which might be consequentialist or involve following a set of rules.  \nDomesticity | A motivation system is designed to severely limit the scope of the agent’s ambitions and activities.  \nIndirect normativity | Indirect normativity could involve rule-based or consequentialist principles, but is distinguished by its reliance on an indirect approach to specifying the rules that are to be followed or the values that are to be pursued.  \nAugmentation | One starts with a system that already has substantially human or benevolent motivations, and enhances its cognitive capacities to make it superintelligent.  \n  \n* * *\n\nEach control method comes with potential vulnerabilities and presents\ndifferent degrees of difficulty in its implementation. It might perhaps be\nthought that we should rank them from better to worse, and then opt for the\nbest method. But that would be simplistic. Some methods can be used in\ncombination whereas others are exclusive. Even a comparatively insecure method\nmay be advisable if it can easily be used as an adjunct, whereas a strong\nmethod might be unattractive if it would preclude the use of other desirable\nsafeguards.\n\nIt is therefore necessary to consider what package deals are available. We\nneed to consider what type of system we might try to build, and which control\nmethods would be applicable to each type. This is the topic for our next\nchapter.\n\n\n## [CHAPTER 10  \nOracles, genies, sovereigns,\ntools](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_006.html#filepos22083)\n\n**Some say: “Just build a question-answering system!” or “Just build an AI\nthat is like a tool rather than an agent!” But these suggestions do not make\nall safety concerns go away, and it is in fact a non-trivial question which\ntype of system would offer the best prospects for safety. We consider four\ntypes or “castes”—oracles, genies, sovereigns, and tools—and explain how they\nrelate to one\nanother.[1](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1045391)\nEach offers different sets of advantages and disadvantages in our quest to\nsolve the control problem.**\n\n### [Oracles](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_006.html#filepos22232)\n\nAn oracle is a question-answering system. It might accept questions in a\nnatural language and present its answers as text. An oracle that accepts only\nyes/no questions could output its best guess with a single bit, or perhaps\nwith a few extra bits to represent its degree of confidence. An oracle that\naccepts open-ended questions would need some metric with which to rank\npossible truthful answers in terms of their informativeness or\nappropriateness.[2](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1045721)\nIn either case, building an oracle that has a fully domain-general ability to\nanswer natural language questions is an AI-complete problem. If one could do\nthat, one could probably also build an AI that has a decent ability to\nunderstand human intentions as well as human words.\n\nOracles with domain-limited forms of superintelligence are also conceivable.\nFor instance, one could conceive of a mathematics-oracle which would only\naccept queries posed in a formal language but which would be very good at\nanswering such questions (e.g. being able to solve in an instant almost any\nformally expressed math problem that the human mathematics profession could\nsolve by laboring collaboratively for a century). Such a mathematics-oracle\nwould form a stepping-stone toward domain-general superintelligence.\n\nOracles with superintelligence in extremely limited domains already exist. A\npocket calculator can be viewed as a very narrow oracle for basic arithmetical\nquestions; an Internet search engine can be viewed as a very partial\nrealization of an oracle with a domain that encompasses a significant part of\ngeneral human declarative knowledge. These domain-limited oracles are tools\nrather than agents (more on tool-AIs shortly). In what follows, though, the\nterm “oracle” will refer to question-answering systems that have domain-\ngeneral superintelligence, unless otherwise stated.\n\nTo make a general superintelligence function as an oracle, we could apply both\nmotivation selection and capability control. Motivation selection for an\noracle may be easier than for other castes of superintelligence, because the\nfinal goal in an oracle could be comparatively simple. We would want the\noracle to give truthful, non-manipulative answers and to otherwise limit its\nimpact on the world. Applying a domesticity method, we might require that the\noracle should use only designated resources to produce its answer. For\nexample, we might stipulate that it should base its answer on a preloaded\ncorpus of information, such as a stored snapshot of the Internet, and that it\nshould use no more than a fixed number of computational\nsteps.[3](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1046006) To\navoid incentivizing the oracle to manipulate us into giving it easier\nquestions—which would happen if we gave it the goal of maximizing its accuracy\nacross all questions we will ask it—we could give it the goal of answering\nonly one question and to terminate immediately upon delivering its answer. The\nquestion would be preloaded into its memory before the program is run. To ask\na second question, we would reset the machine and run the same program with a\ndifferent question preloaded in memory.\n\nSubtle and potentially treacherous challenges arise even in specifying the\nrelatively simple motivation system needed to drive an oracle. Suppose, for\nexample, that we come up with some explication of what it means for the AI “to\nminimize its impact on the world, subject to achieving certain results” or “to\nuse only designated resources in preparing the answer.” What happens if the\nAI, in the course of its intellectual development, undergoes the equivalent of\na scientific revolution involving a change in its basic\nontology?[4](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1046164)\nWe might initially have explicated “impact” and “designated resources” using\nour own ontology (postulating the existence of various physical objects such\nas computers). But just as we have abandoned ontological categories that were\ntaken for granted by scientists in previous ages (e.g. “phlogiston,” “élan\nvital,” and “absolute simultaneity”), so a superintelligent AI might discover\nthat some of our current categories are predicated on fundamental\nmisconceptions. The goal system of an AI undergoing an ontological crisis\nneeds to be resilient enough that the “spirit” of its original goal content is\ncarried over, charitably transposed into the new key.\n\nAlthough making an oracle safe through the use of motivation selection might\nbe far from trivial, it may nevertheless be easier than doing the same for an\nAI that roams the world in pursuit of some complicated goal. This is an\nargument for preferring that the first superintelligence be an oracle. Another\npoint in favor of an oracle-first path is the greater susceptibility of an\noracle to capability control. All the standard boxing methods apply here. In\naddition, there may be methods that are distinctively applicable to oracles.\nFor example, consider the risk that an oracle will answer questions not in a\nmaximally truthful way but in such a way as to subtly manipulate us into\npromoting its own hidden agenda. One way to slightly mitigate this threat\ncould be to create multiple oracles, each with a slightly different code and a\nslightly different information base. A simple mechanism could then compare the\nanswers given by the different oracles and only present them for human viewing\nif all the answers agree. While one must assume that any superintelligent\ndomain-general oracle would know about the existence of other oracles (it\ncould infer this from its knowledge about human society), it might be feasible\nto hide some implementation details of each oracle from the views of the\nothers. If the oracles are not able to communicate with one another, it might\nthen be difficult for them to coordinate on how to answer our questions\nmanipulatively. There are many ways of deviating from the truth, and the\noracles may not all agree on which of these deviations is most\nattractive—whereas the truth itself is a Schelling point (a salient place for\nagreement in the absence of communication). So if the oracles achieve\nconsensus, it might be a sign that they gave the true\nanswer.[5](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1046287)\n\nAn oracle would ideally be trustworthy in the sense that we could safely\nassume that its answers are always accurate to the best of its ability. But\neven an untrustworthy oracle could be useful. We could ask such an oracle\nquestions of a type for which it is difficult to find the answer but easy to\nverify whether a given answer is correct. Many mathematical problems are of\nthis kind. If we are wondering whether a mathematical proposition is true, we\ncould ask the oracle to produce a proof or disproof of the proposition.\nFinding the proof may require insight and creativity beyond our ken, but\nchecking a purported proof’s validity can be done by a simple mechanical\nprocedure.\n\nIf it is expensive to verify answers (as is often the case on topics outside\nlogic and mathematics), we can randomly select a subset of the oracle’s\nanswers for verification. If they are all correct, we can assign a high\nprobability to most of the other answers also being correct. This trick can\ngive us a bulk discount on trustworthy answers that would be costly to verify\nindividually. (Unfortunately, it cannot give us trustworthy answers that we\nare _unable_ to verify, since a dissembling oracle may choose to answer\ncorrectly only those questions where it believes we could verify its answers.)\n\nThere could be important issues on which we could benefit from an augural\npointer toward the correct answer (or toward a method for locating the correct\nanswer) even if we had to actively distrust the provenance. For instance, one\nmight ask for the solution to various technical or philosophical problems that\nmay arise in the course of trying to develop more advanced motivation\nselection methods. If we had a proposed AI design alleged to be safe, we could\nask an oracle whether it could identify any significant flaw in the design,\nand whether it could explain any such flaw to us in twenty words or less.\nQuestions of this kind could elicit valuable information. Caution and\nrestraint would be required, however, for us not to ask _too many_ such\nquestions—and not to allow ourselves to partake of _too many_ details of the\nanswers given to the questions we do ask—lest we give the untrustworthy oracle\nopportunities to work on our psychology (by means of plausible-seeming but\nsubtly manipulative messages). It might not take many bits of communication\nfor an AI with the social manipulation superpower to bend us to its will.\n\nEven if the oracle itself works exactly as intended, there is a risk that it\nwould be misused. One obvious dimension of this problem is that an oracle AI\nwould be a source of immense power which could give a decisive strategic\nadvantage to its operator. This power might be illegitimate and it might not\nbe used for the common good. Another more subtle but no less important\ndimension is that the use of an oracle could be extremely dangerous for the\noperator herself. Similar worries (which involve philosophical as well as\ntechnical issues) arise also for other hypothetical castes of\nsuperintelligence. We will explore them more thoroughly in [Chapter\n13](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_020.html#filepos727275). Suffice\nit here to note that the protocol determining which questions are asked, in\nwhich sequence, and how the answers are reported and disseminated could be of\ngreat significance. One might also consider whether to try to build the oracle\nin such a way that it would refuse to answer any question in cases where it\npredicts that its answering would have consequences classified as catastrophic\naccording to some rough-and-ready criteria.\n\n### [Genies and\nsovereigns](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_006.html#filepos22316)\n\nA genie is a command-executing system: it receives a high-level command,\ncarries it out, then pauses to await the next\ncommand.[6](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1047112) A\nsovereign is a system that has an open-ended mandate to operate in the world\nin pursuit of broad and possibly very long-range objectives. Although these\nmight seem like radically different templates for what a superintelligence\nshould be and do, the difference is not as deep as it might at first glance\nappear.\n\nWith a genie, one already sacrifices the most attractive property of an\noracle: the opportunity to use boxing methods. While one might consider\ncreating a physically confined genie, for instance one that can only construct\nobjects inside a designated volume—a volume that might be sealed off by a\nhardened wall or a barrier loaded with explosive charges rigged to detonate if\nthe containment is breached—it would be difficult to have much confidence in\nthe security of any such physical containment method against a\nsuperintelligence equipped with versatile manipulators and construction\nmaterials. Even if it were somehow possible to ensure a containment as secure\nas that which can be achieved for an oracle, it is not clear how much we would\nhave gained by giving the superintelligence direct access to manipulators\ncompared to requiring it instead to output a blueprint that we could inspect\nand then use to achieve the same result ourselves. The gain in speed and\nconvenience from bypassing the human intermediary seems hardly worth the loss\nof foregoing the use of the stronger boxing methods available to contain an\noracle.\n\nIf one _were_ creating a genie, it would be desirable to build it so that it\nwould obey the intention behind the command rather than its literal meaning,\nsince a literalistic genie (one superintelligent enough to attain a decisive\nstrategic advantage) might have a propensity to kill the user and the rest of\nhumanity on its first use, for reasons explained in the section on malignant\nfailure modes in [Chapter\n8](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_015.html#filepos408438). More\nbroadly, it would seem important that the genie seek a charitable—and what\nhuman beings would regard as reasonable—interpretation of what is being\ncommanded, and that the genie be motivated to carry out the command under such\nan interpretation rather than under the literalistic interpretation. The ideal\ngenie would be a super-butler rather than an autistic savant.\n\nA genie endowed with such a super-butler nature, however, would not be far\nfrom qualifying for membership in the caste of sovereigns. Consider, for\ncomparison, the idea of building a sovereign with the final goal of obeying\nthe spirit of the commands we would have given had we built a genie rather\nthan a sovereign. Such a sovereign would mimic a genie. Being\nsuperintelligent, this sovereign would do a good job at guessing what commands\nwe would have given a genie (and it could always ask us if that would help\ninform its decisions). Would there then really be any important difference\nbetween such a sovereign and a genie? Or, pressing on the distinction from the\nother side, consider that a superintelligent genie may likewise be able to\npredict what commands we will give it: what then is gained from having it\nawait the actual issuance before it acts?\n\nOne might think that a big advantage of a genie over a sovereign is that if\nsomething goes wrong, we could issue the genie with a new command to stop or\nto reverse the effects of the previous actions, whereas a sovereign would just\npush on regardless of our protests. But this apparent safety advantage for the\ngenie is largely illusory. The “stop” or “undo” button on a genie works only\nfor benign failure modes: in the case of a malignant failure—one in which, for\nexample, carrying out the existing command has become a final goal for the\ngenie—the genie would simply disregard any subsequent attempt to countermand\nthe previous\ncommand.[7](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1048481)\n\nOne option would be to try to build a genie such that it would automatically\npresent the user with a prediction about salient aspects of the likely\noutcomes of a proposed command, asking for confirmation before proceeding.\nSuch a system could be referred to as a _genie-with-a-preview_. But if this\ncould be done for a genie, it could likewise be done for a sovereign. So\nagain, this is not a clear differentiator between a genie and a sovereign.\n(Supposing that a preview functionality could be created, the questions of\nwhether and if so how to use it are rather less obvious than one might think,\nnotwithstanding the strong appeal of being able to glance at the outcome\nbefore committing to making it irrevocable reality. We will return to this\nmatter later.)\n\nThe ability of one caste to mimic another extends to oracles, too. A genie\ncould be made to act like an oracle if the only commands we ever give it are\nto answer certain questions. An oracle, in turn, could be made to substitute\nfor a genie if we asked the oracle what the easiest way is to get certain\ncommands executed. The oracle could give us step-by-step instructions for\nachieving the same result as a genie would produce, or it could even output\nthe source code for a\ngenie.[8](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1048802)\nSimilar points can be made with regard to the relation between an oracle and a\nsovereign.\n\nThe real difference between the three castes, therefore, does not reside in\nthe ultimate capabilities that they would unlock. Instead, the difference\ncomes down to alternative approaches to the control problem. Each caste\ncorresponds to a different set of safety precautions. The most prominent\nfeature of an oracle is that it can be boxed. One might also try to apply\ndomesticity motivation selection to an oracle. A genie is harder to box, but\nat least domesticity may be applicable. A sovereign can neither be boxed nor\nhandled through the domesticity approach.\n\nIf these were the only relevant factors, then the order of desirability would\nseem clear: an oracle would be safer than a genie, which would be safer than a\nsovereign; and any initial differences in convenience and speed of operation\nwould be relatively small and easily dominated by the gains in safety\nobtainable by building an oracle. However, there are other factors that need\nto be taken into account. When choosing between castes, one should consider\nnot only the danger posed by the system itself but also the dangers that arise\nout of the way it might be used. A genie most obviously gives the person who\ncontrols it enormous power, but the same holds for an\noracle.[9](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1049428) A\nsovereign, by contrast, could be constructed in such way as to accord no one\nperson or group any special influence over the outcome, and such that it would\nresist any attempt to corrupt or alter its original agenda. What is more, if a\nsovereign’s motivation is defined using “indirect normativity” (a concept to\nbe described in [Chapter\n13](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_020.html#filepos727275)) then it\ncould be used to achieve some abstractly defined outcome, such as “whatever is\nmaximally fair and morally right”—without anybody knowing in advance what\nexactly this will entail. This would create a situation analogous to a\nRawlsian “veil of\nignorance.”[10](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1049796)\nSuch a setup might facilitate the attainment of consensus, help prevent\nconflict, and promote a more equitable outcome.\n\nAnother point, which counts against some types of oracles and genies, is that\nthere are risks involved in designing a superintelligence to have a final goal\nthat does not fully match the outcome that we ultimately seek to attain. For\nexample, if we use a domesticity motivation to make the superintelligence want\nto minimize some of its impacts on the world, we might thereby create a system\nwhose preference ranking over possible outcomes differs from that of the\nsponsor. The same will happen if we build the AI to place a peculiarly high\nvalue on answering questions correctly, or on faithfully obeying individual\ncommands. Now, if sufficient care is taken, this should not cause any\nproblems: there would be sufficient agreement between the two rankings—at\nleast insofar as they pertain to possible worlds that have a reasonable chance\nof being actualized—that the outcomes that are good by the AI’s standard are\nalso good by the principal’s standard. But perhaps one could argue for the\ndesign principle that it is unwise to introduce even a limited amount of\ndisharmony between the AI’s goals and ours. (The same concern would of course\napply to giving sovereigns goals that do not completely harmonize with ours.)\n\n### [Tool-AIs](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_006.html#filepos22414)\n\nOne suggestion that has been made is that we build the superintelligence to be\nlike a tool rather than an\nagent.[11](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1050688)\nThis idea seems to arise out of the observation that ordinary software, which\nis used in countless applications, does not raise any safety concerns even\nremotely analogous to the challenges discussed in this book. Might one not\ncreate “tool-AI” that is like such software—like a flight control system, say,\nor a virtual assistant—only more flexible and capable? Why build a\nsuperintelligence that has a will of its own? On this line of thinking, the\nagent paradigm is fundamentally misguided. Instead of creating an AI that has\nbeliefs and desires and that acts like an artificial person, we should aim to\nbuild regular software that simply does what it is programmed to do.\n\nThis idea of creating software that “simply does what it is programmed to do”\nis, however, not so straightforward if the product being created is a powerful\ngeneral intelligence. There is, of course, a trivial sense in which all\nsoftware simply does what it is programmed to do: the behavior is\nmathematically specified by the code. But this is equally true for all castes\nof machine intelligence, “tool-AI” or not. If, instead, “simply doing what it\nis programmed to do” means that the software behaves as the programmers\n_intended_ , then this is a standard that ordinary software very often fails\nto meet.\n\nBecause of the limited capabilities of contemporary software (compared with\nthose of machine superintelligence) the consequences of such failures are\nmanageable, ranging from insignificant to very costly, but in no case\namounting to an existential\nthreat.[12](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1050801)\nHowever, if it is insufficient capability rather than sufficient reliability\nthat makes ordinary software existentially safe, then it is unclear how such\nsoftware could be a model for a safe superintelligence. It might be thought\nthat by expanding the range of tasks done by ordinary software, one could\neliminate the need for artificial general intelligence. But the range and\ndiversity of tasks that a general intelligence could profitably perform in a\nmodern economy is enormous. It would be infeasible to create special-purpose\nsoftware to handle all of those tasks. Even if it could be done, such a\nproject would take a _long_ time to carry out. Before it could be completed,\nthe nature of some of the tasks would have changed, and new tasks would have\nbecome relevant. There would be great advantage to having software that can\nlearn on its own to do new tasks, and indeed to discover new tasks in need of\ndoing. But this would require that the software be able to learn, reason, and\nplan, and to do so in a powerful and robustly cross-domain manner. In other\nwords, it would require general intelligence.\n\nEspecially relevant for our purposes is the task of software development\nitself. There would be enormous practical advantages to being able to automate\nthis. Yet the capacity for rapid self-improvement is just the critical\nproperty that enables a seed AI to set off an intelligence explosion.\n\nIf general intelligence is not dispensable, is there some other way of\nconstruing the tool-AI idea so as to preserve the reassuringly passive quality\nof a humdrum tool? Could one have a general intelligence that is not an agent?\nIntuitively, it is not just the limited capability of ordinary software that\nmakes it safe: it is also its lack of ambition. There is no subroutine in\nExcel that secretly wants to take over the world if only it were smart enough\nto find a way. The spreadsheet application does not “want” anything at all; it\njust blindly carries out the instructions in the program. What (one might\nwonder) stands in the way of creating a more generally intelligent application\nof the same type? An oracle, for instance, which, when prompted with a\ndescription of a goal, would respond with a plan for how to achieve it, in\nmuch the same way that Excel responds to a column of numbers by calculating a\nsum—without thereby expressing any “preferences” regarding its output or how\nhumans might choose to use it?\n\nThe classical way of writing software requires the programmer to understand\nthe task to be performed in sufficient detail to formulate an explicit\nsolution process consisting of a sequence of mathematically well-defined steps\nexpressible in\ncode.[13](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1052463) (In\npractice, software engineers rely on code libraries stocked with useful\nbehaviors, which they can invoke without needing to understand how the\nbehaviors are implemented. But that code was originally created by programmers\nwho had a detailed understanding of what they were doing.) This approach works\nfor solving well-understood tasks, and is to credit for most software that is\ncurrently in use. It falls short, however, when nobody knows precisely how to\nsolve all of the tasks that need to be accomplished. This is where techniques\nfrom the field of artificial intelligence become relevant. In narrow\napplications, machine learning might be used merely to fine-tune a few\nparameters in a largely human-designed program. A spam filter, for example,\nmight be trained on a corpus of hand-classified email messages in a process\nthat changes the weights that the classification algorithm places on various\ndiagnostic features. In a more ambitious application, the classifier might be\nbuilt so that it can discover new features on its own and test their validity\nin a changing environment. An even more sophisticated spam filter could be\nendowed with some ability to reason about the trade-offs facing the user or\nabout the contents of the messages it is classifying. In neither of these\ncases does the programmer need to know the best way of distinguishing spam\nfrom ham, only how to set up an algorithm that can improve its own performance\nvia learning, discovering, or reasoning.\n\nWith advances in artificial intelligence, it would become possible for the\nprogrammer to offload more of the cognitive labor required to figure out how\nto accomplish a given task. In an extreme case, the programmer would simply\nspecify a formal criterion of what counts as success and leave it to the AI to\nfind a solution. To guide its search, the AI would use a set of powerful\nheuristics and other methods to discover structure in the space of possible\nsolutions. It would keep searching until it found a solution that satisfied\nthe success criterion. The AI would then either implement the solution itself\nor (in the case of an oracle) report the solution to the user.\n\nRudimentary forms of this approach are quite widely deployed today.\nNevertheless, software that uses AI and machine learning techniques, though it\nhas some ability to find solutions that the programmers had not anticipated,\nfunctions for all practical purposes like a tool and poses no existential\nrisk. We would enter the danger zone only when the methods used in the search\nfor solutions become extremely powerful and general: that is, when they begin\nto amount to general intelligence—and especially when they begin to amount to\nsuperintelligence.\n\nThere are (at least) two places where trouble could then arise. First, the\nsuperintelligent search process might find a solution that is not just\nunexpected but radically unintended. This could lead to a failure of one of\nthe types discussed previously (“perverse instantiation,” “infrastructure\nprofusion,” or “mind crime”). It is most obvious how this could happen in the\ncase of a sovereign or a genie, which directly implements the solution it has\nfound. If making molecular smiley faces or transforming the planet into\npaperclips is the first idea that the superintelligence discovers that meets\nthe solution criterion, then smiley faces or paperclips we\nget.[14](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1052648) But\neven an oracle, which—if all else goes well—merely _reports_ the solution,\ncould become a cause of perverse instantiation. The user asks the oracle for a\nplan to achieve a certain outcome, or for a technology to serve a certain\nfunction; and when the user follows the plan or constructs the technology, a\nperverse instantiation can ensue, just as if the AI had implemented the\nsolution\nitself.[15](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1052903)\n\nA second place where trouble could arise is in the course of the software’s\noperation. If the methods that the software uses to search for a solution are\nsufficiently sophisticated, they may include provisions for managing the\nsearch process itself in an intelligent manner. In this case, the machine\nrunning the software may begin to seem less like a mere tool and more like an\nagent. Thus, the software may start by developing a plan for how to go about\nits search for a solution. The plan may specify which areas to explore first\nand with what methods, what data to gather, and how to make best use of\navailable computational resources. In searching for a plan that satisfies the\nsoftware’s internal criterion (such as yielding a sufficiently high\nprobability of finding a solution satisfying the user-specified criterion\nwithin the allotted time), the software may stumble on an unorthodox idea. For\ninstance, it might generate a plan that begins with the acquisition of\nadditional computational resources and the elimination of potential\ninterrupters (such as human beings). Such “creative” plans come into view when\nthe software’s cognitive abilities reach a sufficiently high level. When the\nsoftware puts such a plan into action, an existential catastrophe may ensue.\n\nAs the examples in Box 9 illustrate, open-ended search processes sometimes\nevince strange and unexpected non-anthropocentric solutions even in their\ncurrently limited forms. Present-day search processes are not hazardous\nbecause they are too weak to discover the kind of plan that could enable a\nprogram to take over the world. Such a plan would include extremely difficult\nsteps, such as the invention of a new weapons technology several generations\nahead of the state of the art or the execution of a propaganda campaign far\nmore effective than any communication devised by human spin doctors. To have a\nchance of even _conceiving_ of such ideas, let alone developing them in a way\nthat would actually work, a machine would probably need the capacity to\nrepresent the world in a way that is at least as rich and realistic as the\nworld model possessed by a normal human adult (though a lack of awareness in\nsome areas might possibly be compensated for by extra skill in others). This\nis far beyond the reach of contemporary AI. And because of the combinatorial\nexplosion, which generally defeats attempts to solve complicated planning\nproblems with brute-force methods (as we saw in [Chapter\n1](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos33916)), the\nshortcomings of known algorithms cannot realistically be overcome simply by\npouring on more computing\npower.[21](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1055226)\nHowever, once the search or planning processes become powerful enough, they\nalso become potentially dangerous.\n\n* * *\n\n### **Box 9 Strange solutions from blind search**\n\nEven simple evolutionary search processes sometimes produce highly unexpected\nresults, solutions that satisfy a formal user-defined criterion in a very\ndifferent way than the user expected or intended.\n\nThe field of evolvable hardware offers many illustrations of this phenomenon.\nIn this field, an evolutionary algorithm searches the space of hardware\ndesigns, testing the fitness of each design by instantiating it physically on\na rapidly reconfigurable array or motherboard. The evolved designs often show\nremarkable economy. For instance, one search discovered a frequency\ndiscrimination circuit that functioned without a clock—a component normally\nconsidered necessary for this function. The researchers estimated that the\nevolved circuit was between one and two orders of magnitude smaller than what\na human engineer would have required for the task. The circuit exploited the\nphysical properties of its components in unorthodox ways; some active,\nnecessary components were not even connected to the input or output pins!\nThese components instead participated via what would normally be considered\nnuisance side effects, such as electromagnetic coupling or power-supply\nloading.\n\nAnother search process, tasked with creating an oscillator, was deprived of a\nseemingly even more indispensible component, the capacitor. When the algorithm\npresented its successful solution, the researchers examined it and at first\nconcluded that it “should not work.” Upon more careful examination, they\ndiscovered that the algorithm had, MacGyver-like, reconfigured its sensor-less\nmotherboard into a makeshift radio receiver, using the printed circuit board\ntracks as an aerial to pick up signals generated by personal computers that\nhappened to be situated nearby in the laboratory. The circuit amplified this\nsignal to produce the desired oscillating\noutput.[16](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1054386)\n\nIn other experiments, evolutionary algorithms designed circuits that sensed\nwhether the motherboard was being monitored with an oscilloscope or whether a\nsoldering iron was connected to the lab’s common power supply. These examples\nillustrate how an open-ended search process can repurpose the materials\naccessible to it in order to devise completely unexpected sensory\ncapabilities, by means that conventional human design-thinking is poorly\nequipped to exploit or even account for in retrospect.\n\nThe tendency for evolutionary search to “cheat” or find counterintuitive ways\nof achieving a given end is on display in nature too, though it is perhaps\nless obvious to us there because of our already being somewhat familiar with\nthe look and feel of biology, and thus being prone to regarding the actual\noutcomes of natural evolutionary processes as normal—even if we would not have\nexpected them _ex ante_. But it is possible to set up experiments in\nartificial selection where one can see the evolutionary process in action\noutside its familiar context. In such experiments, researchers can create\nconditions that rarely obtain in nature, and observe the results.\n\nFor example, prior to the 1960s, it was apparently quite common for biologists\nto maintain that predator populations restrict their own breeding in order to\navoid falling into a Malthusian\ntrap.[17](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1054559)\nAlthough individual selection would work against such restraint, it was\nsometimes thought that group selection would overcome individual incentives to\nexploit opportunities for reproduction and favor traits that would benefit the\ngroup or population at large. Theoretical analysis and simulation studies\nlater showed that while group selection is possible in principle, it can\novercome strong individual selection only under very stringent conditions that\nmay rarely apply in\nnature.[18](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1054671)\nBut such conditions can be created in the laboratory. When flour beetles\n(_Tribolium castaneum_) were bred for reduced population size, by applying\nstrong group selection, evolution did indeed lead to smaller\npopulations.[19](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1054780)\nHowever, the means by which this was accomplished included not only the\n“benign” adaptations of reduced fecundity and extended developmental time that\na human naively anthropomorphizing evolutionary search might have expected,\nbut also an increase in\ncannibalism.[20](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1054923)\n\n* * *\n\nInstead of allowing agent-like purposive behavior to emerge spontaneously and\nhaphazardly from the implementation of powerful search processes (including\nprocesses searching for internal work plans and processes directly searching\nfor solutions meeting some user-specified criterion), it may be better to\ncreate agents on purpose. Endowing a superintelligence with an explicitly\nagent-like structure can be a way of increasing predictability and\ntransparency. A well-designed system, built such that there is a clean\nseparation between its values and its beliefs, would let us predict something\nabout the outcomes it would tend to produce. Even if we could not foresee\nexactly which beliefs the system would acquire or which situations it would\nfind itself in, there would be a known place where we could inspect its final\nvalues and thus the criteria that it will use in selecting its future actions\nand in evaluating any potential plan.\n\n###\n[Comparison](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_006.html#filepos22499)\n\nIt may be useful to summarize the features of the different system castes we\nhave discussed (Table 11).\n\n**Table 11 _Features of different system castes_**\n\n|  \n---|---  \n**Oracle** | **A question-answering system** | • Boxing methods fully applicable  \n| _Variations_ : Domain-limited oracles (e.g. mathematics); output-restricted oracles (e.g. only yes/no/undecided answers, or probabilities); oracles that refuse to answer questions if they predict the consequences of answering would meet pre-specified “disaster criteria”; multiple oracles for peer review | • Domesticity fully applicable • Reduced need for AI to understand human intentions and interests (compared to genies and sovereigns) • Use of yes/no questions can obviate need for a metric of the “usefulness” or “informativeness” of answers • Source of great power (might give operator a decisive strategic advantage) • Limited protection against foolish use by operator • Untrustworthy oracles could be used to provide answers that are hard to find but easy to verify • Weak verification of answers may be possible through the use of multiple oracles  \n**Genie** | **A command-executing system** | • Boxing methods partially applicable (for spatially limited genies)  \n| _Variations_ : Genies using different “extrapolation distances” or degrees of following the spirit rather than letter of the command; domain-limited genies; genies-with-preview; genies that refuse to obey commands if they predict the consequences of obeying would meet pre-specified “disaster criteria” | • Domesticity partially applicable • Genie could offer a preview of salient aspects of expected outcomes • Genie could implement change in stages, with opportunity for review at each stage • Source of great power (might give operator a decisive strategic advantage) • Limited protection against foolish use by operator • Greater need for AI to understand human interests and intentions (compared to oracles)  \n**Sovereign** | **A system designed for open-ended autonomous operation** | • Boxing methods inapplicable • Most other capability control methods also inapplicable (except, possibly, social integration or anthropic capture)  \n| _Variations_ : Many possible motivation systems; possibility of using preview and “sponsor ratification” (to be discussed in [Chapter 13](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_020.html#filepos727275)) | • Domesticity mostly inapplicable • Great need for AI to understand true human interests and intentions • Necessity of getting it right on the first try (though, to a possibly lesser extent, this is true for all castes) • Potentially a source of great power for sponsor, including decisive strategic advantage • Once activated, not vulnerable to hijacking by operator, and might be designed with some protection against foolish use • Can be used to implement “veil of ignorance” outcomes (cf. [Chapter 13](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_020.html#filepos727275))  \n**Tool** | **A system not designed to exhibit goal-directed behavior** | • Boxing methods may be applicable, depending on the implementation • Powerful search processes would likely be involved in the development and operation of a machine superintelligence • Powerful search to find a solution meeting some formal criterion can produce solutions that meet the criterion in an unintended and dangerous way • Powerful search might involve secondary, internal search and planning processes that might find dangerous ways of executing the primary search process  \n  \n* * *\n\nFurther research would be needed to determine which type of system would be\nsafest. The answer might depend on the conditions under which the AI would be\ndeployed. The oracle caste is obviously attractive from a safety standpoint,\nsince it would allow both capability control methods and motivation selection\nmethods to be applied. It might thus seem to simply dominate the sovereign\ncaste, which would only allow motivation selection methods (except in\nscenarios in which the world is believed to contain other powerful\nsuperintelligences, in which case social integration or anthropic capture\nmight apply). However, an oracle could place a lot of power into the hands of\nits operator, who might be corrupted or might apply the power unwisely,\nwhereas a sovereign would offer some protection against these hazards. The\nsafety ranking is therefore not so easily determined.\n\nA genie can be viewed as a compromise between an oracle and a sovereign—but\nnot necessarily a good compromise. In many ways, it would share the\ndisadvantages of both. The apparent safety of a tool-AI, meanwhile, may be\nillusory. In order for tools to be versatile enough to substitute for\nsuperintelligent agents, they may need to deploy extremely powerful internal\nsearch and planning processes. Agent-like behaviors may arise from such\nprocesses as an unplanned consequence. In that case, it would be better to\ndesign the system to be an agent in the first place, so that the programmers\ncan more easily see what criteria will end up determining the system’s output.\n\n\n## [CHAPTER 11  \nMultipolar\nscenarios](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_006.html#filepos22599)\n\n**We have seen (particularly in[Chapter\n8](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_015.html#filepos408438)) how\nmenacing a unipolar outcome could be, one in which a single superintelligence\nobtains a decisive strategic advantage and uses it to establish a singleton.\nIn this chapter, we examine what would happen in a multipolar outcome, a post-\ntransition society with multiple competing superintelligent agencies. Our\ninterest in this class of scenarios is twofold. First, as alluded to in\n[Chapter 9](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_016.html#filepos449483),\nsocial integration might be thought to offer a solution to the control\nproblem. We already noted some limitations with that approach, and this\nchapter paints a fuller picture. Second, even without anybody setting out to\ncreate a multipolar condition as a way of handling the control problem, such\nan outcome might occur anyway. So what might such an outcome look like? The\nresulting competitive society is not necessarily attractive, nor long-\nlasting.**\n\nIn singleton scenarios, what happens post-transition depends almost entirely\non the values of the singleton. The outcome could thus be very good or very\nbad, depending on what those values are. What the values are depends, in turn,\non whether the control problem was solved, and—to the degree to which it was\nsolved—on the goals of the project that created the singleton.\n\nIf one is interested in the outcome of singleton scenarios, therefore, one\nreally only has three sources of information: information about matters that\ncannot be affected by the actions of the singleton (such as the laws of\nphysics); information about convergent instrumental values; and information\nthat enables one to predict or speculate about what final values the singleton\nwill have.\n\nIn multipolar scenarios, an additional set of constraints comes into play,\nconstraints having to do with how agents interact. The social dynamics\nemerging from such interactions can be studied using techniques from game\ntheory, economics, and evolution theory. Elements of political science and\nsociology are also relevant insofar as they can be distilled and abstracted\nfrom some of the more contingent features of human experience. Although it\nwould be unrealistic to expect these constraints to give us a precise picture\nof the post-transition world, they can help us identify some salient\npossibilities and challenge some unfounded assumptions.\n\nWe will begin by exploring an economic scenario characterized by a low level\nof regulation, strong protection of property rights, and a moderately rapid\nintroduction of inexpensive digital\nminds.[1](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1055849)\nThis type of model is most closely associated with the American economist\nRobin Hanson, who has done pioneering work on the subject. Later in this\nchapter, we will look at some evolutionary considerations and examine the\nprospects of an initially multipolar post-transition world subsequently\ncoalescing into a singleton.\n\n### [Of horses and\nmen](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_006.html#filepos22734)\n\nGeneral machine intelligence could serve as a substitute for human\nintelligence. Not only could digital minds perform the intellectual work now\ndone by humans, but, once equipped with good actuators or robotic bodies,\nmachines could also substitute for human physical labor. Suppose that machine\nworkers—which can be quickly reproduced—become both cheaper and more capable\nthan human workers in virtually all jobs. What happens then?\n\n#### [Wages and\nunemployment](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_006.html#filepos22828)\n\nWith cheaply copyable labor, market wages fall. The only place where humans\nwould remain competitive may be where customers have a basic preference for\nwork done by humans. Today, goods that have been handcrafted or produced by\nindigenous people sometimes command a price premium. Future consumers might\nsimilarly prefer human-made goods and human athletes, human artists, human\nlovers, and human leaders to functionally indistinguishable or superior\nartificial counterparts. It is unclear, however, just how widespread such\npreferences would be. If machine-made alternatives were sufficiently superior,\nperhaps they would be more highly prized.\n\nOne parameter that might be relevant to consumer choice is the inner life of\nthe worker providing a service or product. A concert audience, for instance,\nmight like to know that the performer is consciously experiencing the music\nand the venue. Absent phenomenal experience, the musician could be regarded as\nmerely a high-powered jukebox, albeit one capable of creating the three-\ndimensional appearance of a performer interacting naturally with the crowd.\nMachines might then be designed to instantiate the same kinds of mental states\nthat would be present in a human performing the same task. Even with perfect\nreplication of subjective experiences, however, some people might simply\nprefer organic work. Such preferences could also have ideological or religious\nroots. Just as many Muslims and Jews shun food prepared in ways they classify\nas _haram_ or _treif_ , so there might be groups in the future that eschew\nproducts whose manufacture involved unsanctioned use of machine intelligence.\n\nWhat hinges on this? To the extent that cheap machine labor can substitute for\nhuman labor, human jobs may disappear. Fears about automation and job loss are\nof course not new. Concerns about technological unemployment have surfaced\nperiodically, at least since the Industrial Revolution; and quite a few\nprofessions have in fact gone the way of the English weavers and textile\nartisans who in the early nineteenth century united under the banner of the\nfolkloric “General Ludd” to fight against the introduction of mechanized\nlooms. Nevertheless, although machinery and technology have been substitutes\nfor many particular types of human labor, physical technology has on the whole\nbeen a complement to labor. Average human wages around the world have been on\na long-term upward trend, in large part because of such complementarities. Yet\nwhat starts out as a complement to labor can at a later stage become a\nsubstitute for labor. Horses were initially complemented by carriages and\nploughs, which greatly increased the horse’s productivity. Later, horses were\nsubstituted for by automobiles and tractors. These later innovations reduced\nthe demand for equine labor and led to a population collapse. Could a similar\nfate befall the human species?\n\nThe parallel to the story of the horse can be drawn out further if we ask why\nit is that there are still horses around. One reason is that there are still a\nfew niches in which horses have functional advantages; for example, police\nwork. But the main reason is that humans happen to have peculiar preferences\nfor the services that horses can provide, including recreational horseback\nriding and racing. These preferences can be compared to the preferences we\nhypothesized some humans might have in the future, that certain goods and\nservices be made by human hand. Although suggestive, this analogy is, however,\ninexact, since there is still no complete functional substitute for horses. If\nthere were inexpensive mechanical devices that ran on hay and had exactly the\nsame shape, feel, smell, and behavior as biological horses—perhaps even the\nsame conscious experiences—then demand for biological horses would probably\ndecline further.\n\nWith a sufficient reduction in the demand for human labor, wages would fall\nbelow the human subsistence level. The potential downside for human workers is\ntherefore extreme: not merely wage cuts, demotions, or the need for\nretraining, but starvation and death. When horses became obsolete as a source\nof moveable power, many were sold off to meatpackers to be processed into dog\nfood, bone meal, leather, and glue. These animals had no alternative\nemployment through which to earn their keep. In the United States, there were\nabout 26 million horses in 1915. By the early 1950s, 2 million\nremained.[2](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1056178)\n\n#### [Capital and\nwelfare](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_006.html#filepos22936)\n\nOne difference between humans and horses is that humans own capital. A\nstylized empirical fact is that the total factor share of capital has for a\nlong time remained steady at approximately 30% (though with significant short-\nterm\nfluctuations).[3](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1056365)\nThis means that 30% of total global income is received as rent by owners of\ncapital, the remaining 70% being received as wages by workers. If we classify\nAI as capital, then with the invention of machine intelligence that can fully\nsubstitute for human work, wages would fall to the marginal cost of such\nmachine-substitutes, which—under the assumption that the machines are very\nefficient—would be very low, far below human subsistence-level income. The\nincome share received by labor would then dwindle to practically nil. But this\nimplies that the factor share of capital would become nearly 100% of total\nworld product. Since world GDP would soar following an intelligence explosion\n(because of massive amounts of new labor-substituting machines but also\nbecause of technological advances achieved by superintelligence, and, later,\nacquisition of vast amounts of new land through space colonization), it\nfollows that the total income from capital would increase enormously. If\nhumans remain the owners of this capital, the total income received by the\nhuman population would grow astronomically, despite the fact that in this\nscenario humans would no longer receive any wage income.\n\nThe human species as a whole could thus become rich beyond the dreams of\nAvarice. How would this income be distributed? To a first approximation,\ncapital income would be proportional to the amount of capital owned. Given the\nastronomical amplification effect, even a tiny bit of pre-transition wealth\nwould balloon into a vast post-transition fortune. However, in the\ncontemporary world, many people have no wealth. This includes not only\nindividuals who live in poverty but also some people who earn a good income or\nwho have high human capital but have negative net worth. For example, in\naffluent Denmark and Sweden 30% of the population report negative wealth—often\nyoung, middle-class people with few tangible assets and credit card debt or\nstudent\nloans.[4](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1056505)\nEven if savings could earn extremely high interest, there would need to be\nsome seed grain, some starting capital, in order for the compounding to\nbegin.[5](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1056649)\n\nNevertheless, even individuals who have no private wealth at the start of the\ntransition could become extremely rich. Those who participate in a pension\nscheme, for instance, whether public or private, should be in a good position,\nprovided the scheme is at least partially\nfunded.[6](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1056982)\nHave-nots could also become rich through the philanthropy of those who see\ntheir net worth skyrocket: because of the astronomical size of the bonanza,\neven a very small fraction donated as alms would be a very large sum in\nabsolute terms.\n\nIt is also possible that riches could still be made through work, even at a\npost-transition stage when machines are functionally superior to humans in all\ndomains (as well as cheaper than even subsistence-level human labor). As noted\nearlier, this could happen if there are niches in which human labor is\npreferred for aesthetic, ideological, ethical, religious, or other non-\npragmatic reasons. In a scenario in which the wealth of human capital-holders\nincreases dramatically, demand for such labor could increase correspondingly.\nNewly minted trillionaires or quadrillionaires could afford to pay a hefty\npremium for having some of their goods and services supplied by an organic\n“fair-trade” labor force. The history of horses again offers a parallel. After\nfalling to 2 million in the early 1950s, the US horse population has undergone\na robust recovery: a recent census puts the number at just under 10 million\nhead.[7](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1057503) The\nrise is not due to new functional needs for horses in agriculture or\ntransportation; rather, economic growth has enabled more Americans to indulge\na fancy for equestrian recreation.\n\nAnother relevant difference between humans and horses, beside capital-\nownership, is that humans are capable of political mobilization. A human-run\ngovernment could use the taxation power of the state to redistribute private\nprofits, or raise revenue by selling appreciated state-owned assets, such as\npublic land, and use the proceeds to pension off its constituents. Again,\nbecause of the explosive economic growth during and immediately after the\ntransition, there would be vastly more wealth sloshing around, making it\nrelatively easy to fill the cups of all unemployed citizens. It should be\nfeasible even for a single country to provide every human worldwide with a\ngenerous living wage at no greater proportional cost than what many countries\ncurrently spend on foreign\naid.[8](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1057627)\n\n#### [The Malthusian principle in a historical\nperspective](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_006.html#filepos23041)\n\nSo far we have assumed a constant human population. This may be a reasonable\nassumption for short timescales, since biology limits the rate of human\nreproduction. Over longer timescales, however, the assumption is not\nnecessarily reasonable.\n\nThe human population has increased a thousandfold over the past 9,000\nyears.[9](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1058396) The\nincrease would have been much faster except for the fact that throughout most\nof history and prehistory, the human population was bumping up against the\nlimits of the world economy. An approximately Malthusian condition prevailed,\nin which most people received subsistence-level incomes that just barely\nallowed them to survive and raise an average of two children to\nmaturity.[10](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1058695)\nThere were temporary and local reprieves: plagues, climate fluctuations, or\nwarfare intermittently culled the population and freed up land, enabling\nsurvivors to improve their nutritional intake—and to bring up more children,\nuntil the ranks were replenished and the Malthusian condition reinstituted.\nAlso, thanks to social inequality, a thin elite stratum could enjoy\nconsistently above-subsistence income (at the expense of somewhat lowering the\ntotal size of the population that could be sustained). A sad and dissonant\nthought: that in this Malthusian condition, the normal state of affairs during\nmost of our tenure on this planet, it was droughts, pestilence, massacres, and\ninequality—in common estimation the worst foes of human welfare—that may have\nbeen the greatest humanitarians: they alone enabling the average level of\nwell-being to occasionally bop up slightly above that of life at the very\nmargin of subsistence.\n\nSuperimposed on local fluctuations, history shows a macro-pattern of initially\nslow but accelerating economic growth, fueled by the accumulation of\ntechnological innovations. The growing world economy brought with it a\ncommensurate increase in global population. (More precisely, a larger\npopulation itself appears to have strongly accelerated the rate of growth,\nperhaps mainly by increasing humanity’s collective\nintelligence.[11](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1058878))\nOnly since the Industrial Revolution, however, did economic growth become so\nrapid that population growth failed to keep pace. Average income thus started\nto rise, first in the early-industrializing countries of Western Europe,\nsubsequently in most of the world. Even in the poorest countries today,\naverage income substantially exceeds subsistence level, as reflected in the\nfact that the populations of these countries are growing.\n\nThe poorest countries now have the fastest population growth, as they have yet\nto complete the “demographic transition” to the low-fertility regime that has\ntaken hold in more developed societies. Demographers project that the world\npopulation will rise to about 9 billion by mid-century, and that it might\nthereafter plateau or decline as the poorer countries join the developed world\nin this low-fertility\nregime.[12](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1058988)\nMany rich countries already have fertility rates that are below replacement\nlevel; in some cases, far\nbelow.[13](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1059282)\n\nYet there are reasons, if we take a longer view and assume a state of\nunchanging technology and continued prosperity, to expect a return to the\nhistorically and ecologically normal condition of a world population that\nbutts up against the limits of what our niche can support. If this seems\ncounterintuitive in light of the negative relationship between wealth and\nfertility that we are currently observing on the global scale, we must remind\nourselves that this modern age is a brief slice of history and very much an\naberration. Human behavior has not yet adapted to contemporary conditions. Not\nonly do we fail to take advantage of obvious ways to increase our inclusive\nfitness (such as by becoming sperm or egg donors) but we actively sabotage our\nfertility by using birth control. In the environment of evolutionary\nadaptedness, a healthy sex drive may have been enough to make an individual\nact in ways that maximized her reproductive potential; in the modern\nenvironment, however, there would be a huge selective advantage to having a\nmore direct desire for being the biological parent to the largest possible\nnumber of children. Such a desire is currently being selected for, as are\nother traits that increase our propensity to reproduce. Cultural adaptation,\nhowever, might steal a march on biological evolution. Some communities, such\nthose of the Hutterites or the adherents of the Quiverfull evangelical\nmovement, have natalist cultures that encourage large families, and they are\nconsequently undergoing rapid expansion.\n\n#### [Population growth and\ninvestment](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_006.html#filepos23179)\n\nIf we imagine current socioeconomic conditions magically frozen in their\ncurrent shape, the future would be dominated by cultural or ethnic groups that\nsustain high levels of fertility. If most people had preferences that were\nfitness-maximizing in the contemporary environment, the population could\neasily double in each generation. Absent population control policies—which\nwould have to become steadily more rigorous and effective to counteract the\nevolution of stronger preferences to circumvent them—the world population\nwould then continue to grow exponentially until some constraint, such as land\nscarcity or depletion of easy opportunities for important innovation, made it\nimpossible for the economy to keep pace: at which point, average income would\nstart to decline until it reached the level where crushing poverty prevents\nmost people from raising much more than two children to maturity. Thus the\nMalthusian principle would reassert itself, like a dread slave master,\nbringing our escapade into the dreamland of abundance to an end, and leading\nus back to the quarry in chains, there to resume the weary struggle for\nsubsistence.\n\nThis longer-term outlook could be telescoped into a more imminent prospect by\nthe intelligence explosion. Since software is copyable, a population of\nemulations or AIs could double rapidly—over the course of minutes rather than\ndecades or centuries—soon exhausting all available hardware.\n\nPrivate property might offer partial protection against the emergence of a\nuniversal Malthusian condition. Consider a simple model in which clans (or\nclosed communities, or states) start out with varying amounts of property and\nindependently adopt different policies about reproduction and investment. Some\nclans discount the future steeply and spend down their endowment, whereafter\ntheir impoverished members join the global proletariat (or die, if they cannot\nsupport themselves through their labor). Other clans invest some of their\nresources but adopt a policy of unlimited reproduction: such clans grow more\npopulous until they reach an internal Malthusian condition in which their\nmembers are so poor that they die at almost the same rate as they reproduce,\nat which point the clan’s population growth slows to equal the growth of its\nresources. Yet other clans might restrict their fertility to below the rate of\ngrowth of their capital: such clans could slowly increment their numbers while\ntheir members also grow richer per capita.\n\nIf wealth is redistributed from the wealthy clans to the members of the\nrapidly reproducing or rapidly discounting clans (whose children, copies, or\noffshoots, through no fault of their own, were launched into the world with\ninsufficient capital to survive and thrive) then a universal Malthusian\ncondition would be more closely approximated. In the limiting case, all\nmembers of all clans would receive subsistence level income and everybody\nwould be equal in their poverty.\n\nIf property is not redistributed, prudent clans might hold on to a certain\namount of capital, and it is possible that their wealth could grow in absolute\nterms. It is, however, unclear whether humans could earn as high rates of\nreturn on their capital as machine intelligences could earn on theirs, because\nthere may be synergies between labor and capital such that an single agent who\ncan supply both (e.g. an entrepreneur or investor who is both skilled and\nwealthy) can attain a private rate of return on her capital exceeding the\nmarket rate obtainable by agents who possess financial but not cognitive\nresources. Humans, being less skilled than machine intelligences, may\ntherefore grow their capital more slowly—unless, of course, the control\nproblem had been completely solved, in which case the human rate of return\nwould equal the machine rate of return, since a human principal could task a\nmachine agent to manage her savings, and could do so costlessly and without\nconflicts of interest: but otherwise, in this scenario, the fraction of the\neconomy owned by machines would asymptotically approach one hundred percent.\n\nA scenario in which the fraction of the economy that is owned by machines\nasymptotically approaches one hundred percent is not necessarily one in which\nthe size of the human slice declines. If the economy grows at a sufficient\nclip, then even a relatively diminishing fraction of it may still be\nincreasing in its absolute size. This may sound like modestly good news for\nhumankind: in a multipolar scenario in which property rights are\nprotected—even if we completely fail to solve the control problem—the total\namount of wealth owned by human beings could increase. Of course, this effect\nwould not take care of the problem of population growth in the human\npopulation pulling down per capita income to subsistence level, nor the\nproblem of humans who ruin themselves because they discount the future.\n\nIn the long run, the economy would become increasingly dominated by those\nclans that have the highest savings rates—misers who own half the city and\nlive under a bridge. Only in the fullness of time, when there are no more\nopportunities for investment, would the maximally prosperous misers start\ndrawing down their\nsavings.[14](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1060370)\nHowever, if there is less than perfect protection for property rights—for\nexample if the more efficient machines on net succeed, by hook or by crook, in\ntransferring wealth from humans to themselves—then human capitalists may need\nto spend down their capital much sooner, before it gets depleted by such\ntransfers (or the ongoing costs incurred in securing their wealth against such\ntransfers). If these developments take place on digital rather than biological\ntimescales, then the glacial humans might find themselves expropriated before\nthey could say Jack\nRobinson.[15](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1060531)\n\n#### [Life in an algorithmic\neconomy](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_006.html#filepos23297)\n\nLife for biological humans in a post-transition Malthusian state need not\nresemble any of the historical states of man (as hunter–gatherer, farmer, or\noffice worker). Instead, the majority of humans in this scenario might be idle\nrentiers who eke out a marginal living on their\nsavings.[16](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1061731)\nThey would be very poor, yet derive what little income they have from savings\nor state subsidies. They would live in a world with extremely advanced\ntechnology, including not only superintelligent machines but also anti-aging\nmedicine, virtual reality, and various enhancement technologies and pleasure\ndrugs: yet these might be generally unaffordable. Perhaps instead of using\nenhancement medicine, they would take drugs to stunt their growth and slow\ntheir metabolism in order to reduce their cost of living (fast-burners being\nunable to survive at the gradually declining subsistence income). As our\nnumbers increase and our average income declines further, we might degenerate\ninto whatever minimal structure still qualifies to receive a pension—perhaps\nminimally conscious brains in vats, oxygenized and nourished by machines,\nslowly saving up enough money to reproduce by having a robot technician\ndevelop a clone of\nthem.[17](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1062946)\n\nFurther frugality could be achieved by means of uploading, since a physically\noptimized computing substrate, devised by advanced superintelligence, would be\nmore efficient than a biological brain. The migration into the digital realm\nmight be stemmed, however, if emulations were regarded as non-humans or non-\ncitizens ineligible to receive pensions or to hold tax-exempt savings\naccounts. In that case, a niche for biological humans might remain open,\nalongside a perhaps vastly larger population of emulations or artificial\nintelligences.\n\nSo far we have focused on the fate of the humans, who may be supported by\nsavings, subsidies, or wage income deriving from other humans who prefer to\nhire humans. Let us now turn our attention to some of the entities that we\nhave so far classified as “capital”: machines that may be owned by human\nbeings, that are constructed and operated for the sake of the functional tasks\nthey perform, and that are capable of substituting for human labor in a very\nwide range of jobs. What may the situation be like for these workhorses of the\nnew economy?\n\nIf these machines were mere automata, simple devices like a steam engine or\nthe mechanism in a clock, then no further comment would be needed: there would\nbe a large amount of such capital in a post-transition economy, but it would\nseem not to matter to anybody how things turn out for pieces of insentient\nequipment. However, if the machines have conscious minds—if they are\nconstructed in such a way that their operation is associated with phenomenal\nawareness (or if they for some other reason are ascribed moral status)—then it\nbecomes important to consider the overall outcome in terms of how it would\naffect these machine minds. The welfare of the working machine minds could\neven appear to be the most important aspect of the outcome, since they may be\nnumerically dominant.\n\n#### [Voluntary slavery, casual\ndeath](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_006.html#filepos23404)\n\nA salient initial question is whether these working machine minds are owned as\ncapital (slaves) or are hired as free wage laborers. On closer inspection\nhowever, it become doubtful that anything really hinges on the issue. There\nare two reasons for this. First, if a free worker in a Malthusian state gets\npaid a subsistence-level wage, he will have no disposable income left after he\nhas paid for food and other necessities. If the worker is instead a slave, his\nowner will pay for his maintenance and again he will have no disposable\nincome. In either case, the worker gets the necessities and nothing more.\nSecond, suppose that the free laborer were somehow in a position to command an\nabove-subsistence-level income (perhaps because of favorable regulation). How\nwill he spend the surplus? Investors would find it most profitable to create\nworkers who would be “voluntary slaves”—who would willingly work for\nsubsistence-level wages. Investors may create such workers by copying those\nworkers who are compliant. With appropriate selection (and perhaps some\nmodification to the code) investors might be able to create workers who not\nonly prefer to volunteer their labor but who would also choose to donate back\nto their owners any surplus income they might happen to receive. Giving money\nto the worker would then be but a roundabout way of giving money to the owner\nor employer, even if the worker were a free agent with full legal rights.\n\nPerhaps it will be objected that it would be difficult to design a machine so\nthat it wants to volunteer for any job assigned to it or so that it wants to\ndonate its wages to its owner. Emulations, in particular, might be imagined to\nhave more typically human desires. But note that even if the original control\nproblem is difficult, we are here considering a condition _after_ the\ntransition, a time when methods for motivation selection have presumably been\nperfected. In the case of emulations, one might get quite far simply by\n_selecting_ from the pre-existing range of human characters; and we have\ndescribed several other motivation selection methods. The control problem may\nalso in some ways be simplified by the current assumption that the new machine\nintelligence enters into a stable socioeconomic matrix that is already\npopulated with other law-abiding superintelligent agents.\n\nLet us, then, consider the plight of the working-class machine, whether it be\noperating as a slave or a free agent. We focus first on emulations, the\neasiest case to imagine.\n\nBringing a new biological human worker into the world takes anywhere between\nfifteen and thirty years, depending on how much expertise and experience is\nrequired. During this time the new person must be fed, housed, nurtured, and\neducated—at great expense. By contrast, spawning a new copy of a digital\nworker is as easy as loading a new program into working memory. Life thus\nbecomes cheap. A business could continuously adapt its workforce to fit\ndemands by spawning new copies—and terminating copies that are no longer\nneeded, to free up computer resources. This could lead to an extremely high\ndeath rate among digital workers. Many might live for only one subjective day.\n\nThere are reasons other than fluctuations in demand why employers or owners of\nemulations might want to “kill” or “end” their workers\nfrequently.[18](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1063407)\nIf an emulation mind, like a biological mind, requires periods of rest and\nsleep in order to function, it might be cheaper to erase a fatigued emulation\nat the end of a day and replace it with a stored state of a fresh and rested\nemulation. As this procedure would cause retrograde amnesia for everything\nthat had been learned during that day, emulations performing tasks requiring\nlong cognitive threads would be spared such frequent erasure. It would be\ndifficult, for example, to write a book if each morning when one sat down at\none’s desk, one had no memory of what one had done before. But other jobs\ncould be performed adequately by agents that are frequently recycled: a shop\nassistant or a customer service agent, once trained, may only need to remember\nnew information for twenty minutes.\n\nSince recycling emulations would prevent memory and skill formation, some\nemulations may be placed on a special learning track where they would run\ncontinuously, including for rest and sleep, even in jobs that do not strictly\nrequire long cognitive threads. For example, some customer service agents\nmight run for many years in optimized learning environments, assisted by\ncoaches and performance evaluators. The best of these trainees would then be\nused like studs, serving as templates from which millions of fresh copies are\nstamped out each day. Great effort would be poured into improving the\nperformance of such worker templates, because even a small increment in\nproductivity would yield great economic value when applied in millions of\ncopies.\n\nIn parallel with efforts to train worker-templates for particular jobs,\nintense efforts would also be made to improve the underlying emulation\ntechnology. Advances here would be even more valuable than advances in\nindividual worker-templates, since general technology improvements could be\napplied to all emulation workers (and potentially to non-worker emulations\nalso) rather than only to those in a particular occupation. Enormous resources\nwould be devoted to finding computational shortcuts allowing for more\nefficient implementations of existing emulations, and also into developing\nneuromorphic and entirely synthetic AI architectures. This research would\nprobably mostly be done by emulations running on very fast hardware. Depending\non the price of computer power, millions, billions, or trillions of emulations\nof the sharpest human research minds (or enhanced versions thereof) may be\nworking around the clock on advancing the frontier of machine intelligence;\nand some of these may be operating orders of magnitude faster than biological\nbrains.[19](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1064319)\nThis is a good reason for thinking that the era of human-like emulations would\nbe brief—a _very_ brief interlude in sidereal time—and that it would soon give\nway to an era of greatly superior artificial intelligence.\n\nWe have already encountered several reasons why employers of emulation workers\nmay periodically cull their herds: fluctuations in demand for different kinds\nof laborers, cost savings of not having to emulate rest and sleep time, and\nthe introduction of new and improved templates. Security concerns might\nfurnish another reason. To prevent workers from developing subversive plans\nand conspiracies, emulations in some sensitive positions might be run only for\nlimited periods, with frequent resets to an earlier stored ready-\nstate.[20](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1064694)\n\nThese ready-states to which emulations would be reset would be carefully\nprepared and vetted. A typical short-lived emulation might wake up in a well-\nrested mental state that is optimized for loyalty and productivity. He\nremembers having graduated top of his class after many (subjective) years of\nintense training and selection, then having enjoyed a restorative holiday and\na good night’s sleep, then having listened to a rousing motivational speech\nand stirring music, and now he is champing at the bit to finally get to work\nand to do his utmost for his employer. He is not overly troubled by thoughts\nof his imminent death at the end of the working day. Emulations with death\nneuroses or other hang-ups are less productive and would not have been\nselected.[21](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1065455)\n\n#### [Would maximally efficient work be\nfun?](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_006.html#filepos23521)\n\nOne important variable in assessing the desirability of a hypothetical\ncondition like this is the hedonic state of the average\nemulation.[22](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1066173)\nWould a typical emulation worker be suffering or would he be enjoying the\nexperience of working hard on the task at hand?\n\nWe must resist the temptation to project our own sentiments onto the imaginary\nemulation worker. The question is not whether _you_ would feel happy if you\nhad to work constantly and never again spend time with your loved ones—a\nterrible fate, most would agree.\n\nIt is moderately more relevant to consider the current human average hedonic\nexperience during working hours. Worldwide studies asking respondents how\nhappy they are find that most rate themselves as “quite happy” or “very happy”\n(averaging 3.1 on a scale from 1 to\n4).[23](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1066725)\nStudies on average affect, asking respondents how frequently they have\nrecently experienced various positive or negative affective states, tend to\nget a similar result (producing a net affect of about 0.52 on a scale from –1\nto 1). There is a modest positive effect of a country’s per capita income on\naverage subjective well-\nbeing.[24](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1066848)\nHowever, it is hazardous to extrapolate from these findings to the hedonic\nstate of future emulation workers. One reason that could be given for this is\nthat their condition would be so different: on the one hand, they might be\nworking much harder; on the other hand, they might be free from diseases,\naches, hunger, noxious odors, and so forth. Yet such considerations largely\nmiss the mark. The much more important consideration here is that hedonic tone\nwould be easy to adjust through the digital equivalent of drugs or\nneurosurgery. This means that it would be a mistake to infer the hedonic state\nof future emulations from the external conditions of their lives by imagining\nhow we ourselves and other people like us would feel in those circumstances.\nHedonic state would be a matter of choice. In the model we are currently\nconsidering, the choice would be made by capital-owners seeking to maximize\nreturns on their investment in emulation-workers. Consequently, the question\nof how happy emulations would feel boils down to the question of which hedonic\nstates would be most productive (in the various jobs that emulations would be\nemployed to do).\n\nHere, again, one might seek to draw an inference from observations about human\nhappiness. If it is the case, across most times, places, and occupations, that\npeople are typically at least moderately happy, this would create some\npresumption in favor of the same holding in a post-transition scenario like\nthe one we are considering. To be clear, the argument in this case would not\nbe that human minds have a predisposition towards happiness so they would\nprobably find satisfaction under these novel conditions; but rather that a\ncertain average level of happiness has proved adaptive for human minds in the\npast so maybe a similar level of happiness will prove adaptive for human-like\nminds in the future. Yet this formulation also reveals the weakness of the\ninference: to wit, that the mental dispositions that were adaptive for\nhunter–gatherer hominids roaming the African savanna may not necessarily be\nadaptive for modified emulations living in post-transition virtual realities.\nWe can certainly _hope_ that the future emulation-workers would be as happy\nas, or happier than, typical workers were in human history; but we have yet to\nsee any compelling reason for supposing it would be so (in the laissez-faire\nmultipolar scenario currently under examination).\n\nConsider the possibility that the reason happiness is prevalent among humans\n(to whatever limited extent it is prevalent) is that cheerful mood served a\nsignaling function in the environment of evolutionary adaptedness. Conveying\nthe impression to other members of the social group of being in flourishing\ncondition—in good health, in good standing with one’s peers, and in confident\nexpectation of continued good fortune—may have boosted an individual’s\npopularity. A bias toward cheerfulness could thus have been selected for, with\nthe result that human neurochemistry is now biased toward positive affect\ncompared to what would have been maximally efficient according to simpler\nmaterialistic criteria. If this were the case, then the future of _joie de\nvivre_ might depend on cheer retaining its social signaling function unaltered\nin the post-transition world: an issue to which we will return shortly.\n\nWhat if glad souls dissipate more energy than glum ones? Perhaps the joyful\nare more prone to creative leaps and flights of fancy—behaviors that future\nemployers might disprize in most of their workers. Perhaps a sullen or anxious\nfixation on simply getting on with the job without making mistakes will be the\nproductivity-maximizing attitude in most lines of work. The claim here is not\nthat this is so, but that we do not know that it is not so. Yet we should\nconsider just how bad it could be if some such pessimistic hypothesis about a\nfuture Malthusian state turned out to be true: not only because of the\nopportunity cost of having failed to create something better—which would be\nenormous—but also because the state could be bad in itself, possibly far worse\nthan the original Malthusian state.\n\nWe seldom put forth full effort. When we do, it is sometimes painful. Imagine\nrunning on a treadmill at a steep incline—heart pounding, muscles aching,\nlungs gasping for air. A glance at the timer: your next break, which will also\nbe your death, is due in 49 years, 3 months, 20 days, 4 hours, 56 minutes, and\n12 seconds. You wish you had not been born.\n\nAgain the claim is not that this is how it would be, but that we do not know\nthat it is not. One could certainly make a more optimistic case. For example,\nthere is no obvious reason that emulations would need to suffer bodily injury\nand sickness: the elimination of physical wretchedness would be a great\nimprovement over the present state of affairs. Furthermore, since such stuff\nas virtual reality is made of can be fairly cheap, emulations may work in\nsumptuous surroundings—in splendid mountaintop palaces, on terraces set in a\nbudding spring forest, or on the beaches of an azure lagoon—with just the\nright illumination, temperature, scenery and décor; free from annoying fumes,\nnoises, drafts, and buzzing insects; dressed in comfortable clothing, feeling\nclean and focused, and well nourished. More significantly, if—as seems\nperfectly possible—the optimum human mental state for productivity in most\njobs is one of joyful eagerness, then the era of the emulation economy could\nbe quite paradisiacal.\n\nThere would, in any case, be a great option value in arranging matters in such\na manner that somebody or something could intervene to set things right if the\ndefault trajectory should happen to veer toward dystopia. It could also be\ndesirable to have some sort of escape hatch that would permit bailout into\ndeath and oblivion if the quality of life were to sink permanently below the\nlevel at which annihilation becomes preferable to continued existence.\n\n#### [Unconscious\noutsourcers?](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_006.html#filepos23645)\n\nIn the longer run, as the emulation era gives way to an artificial\nintelligence era (or if machine intelligence is attained directly via AI\nwithout a preceding whole brain emulation stage) pain and pleasure might\npossibly disappear entirely in a multipolar outcome, since a hedonic reward\nmechanism may not be the most effective motivation system for an complex\nartificial agent (one that, unlike the human mind, is not burdened with the\nlegacy of animal wetware). Perhaps a more advanced motivation system would be\nbased on an explicit representation of a utility function or some other\narchitecture that has no exact functional analogs to pleasure and pain.\n\nA related but slightly more radical multipolar outcome—one that could involve\nthe elimination of almost all value from the future—is that the universal\nproletariat would not even be conscious. This possibility is most salient with\nrespect to AI, which might be structured very differently than human\nintelligence. But even if machine intelligence were initially achieved though\nwhole brain emulation, resulting in conscious digital minds, the competitive\nforces unleashed in a post-transition economy could easily lead to the\nemergence of progressively less neuromorphic forms of machine intelligence,\neither because synthetic AI is created de novo or because the emulations\nwould, through successive modifications and enhancements, increasingly depart\ntheir original human form.\n\nConsider a scenario in which after emulation technology has been developed,\ncontinued progress in neuroscience and computer science (expedited by the\npresence of digital minds to serve as both researchers and test subjects)\nmakes it possible to isolate individual cognitive modules in an emulation, and\nto hook them up to modules isolated from other emulations. A period of\ntraining and adjustment may be required before different modules can\ncollaborate effectively; but modules that conform to common standards could\nmore quickly interface with other standard modules. This would make\nstandardized modules more productive, and create pressure for more\nstandardization.\n\nEmulations can now begin to outsource increasing portions of their\nfunctionality. Why learn arithmetic when you can send your numerical reasoning\ntask to Gauss-Modules, Inc.? Why be articulate when you can hire Coleridge\nConversations to put your thoughts into words? Why make decisions about your\npersonal life when there are certified executive modules that can scan your\ngoal system and manage your resources to achieve your goals better than if you\ntried to do it yourself? Some emulations may prefer to retain most of their\nfunctionality and handle tasks themselves that could be done more efficiently\nby others. Those emulations would be like hobbyists who enjoy growing their\nown vegetables or knitting their own cardigans. Such hobbyist emulations would\nbe less efficient; and if there is a net flow of resources from less to more\nefficient participants of the economy, the hobbyists would eventually lose\nout.\n\nThe bouillon cubes of discrete human-like intellects thus melt into an\nalgorithmic soup.\n\nIt is conceivable that optimal efficiency would be attained by grouping\ncapabilities in aggregates that roughly match the cognitive architecture of a\nhuman mind. It might be the case, for example, that a mathematics module must\nbe tailored to a language module, and that both must be tailored to the\nexecutive module, in order for the three to work together. Cognitive\noutsourcing would then be almost entirely unworkable. But in the absence of\nany compelling reason for being confident that this is so, we must countenance\nthe possibility that human-like cognitive architectures are optimal only\nwithin the constraints of human neurology (or not at all). When it becomes\npossible to build architectures that could not be implemented well on\nbiological neural networks, new design space opens up; and the global optima\nin this extended space need not resemble familiar types of mentality. Human-\nlike cognitive organizations would then lack a niche in a competitive post-\ntransition economy or\necosystem.[25](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1066971)\n\nThere might be niches for complexes that are either less complex (such as\nindividual modules), more complex (such as vast clusters of modules), or of\nsimilar complexity to human minds but with radically different architectures.\nWould these complexes have any intrinsic value? Should we welcome a world in\nwhich such alien complexes have replaced human complexes?\n\nThe answer may depend on the specific nature of those alien complexes. The\npresent world has many levels of organization. Some highly complex entities,\nsuch as multinational corporations and nation states, contain human beings as\nconstituents; yet we usually assign these high-level complexes only\ninstrumental value. Corporations and states do not (it is generally assumed)\nhave consciousness, over and above the consciousness of the people who\nconstitute them: they cannot feel phenomenal pain or pleasure or experience\nany qualia. We value them to the extent that they serve human needs, and when\nthey cease to do so we “kill” them without compunction. There are also lower-\nlevel entities, and those, too, are usually denied moral status. We see no\nharm in erasing an app from a smartphone, and we do not think that a\nneurosurgeon is wronging anyone when she extirpates a malfunctioning module\nfrom an epileptic brain. As for exotically organized complexes of a level\nsimilar to that of the human brain, most of us would perhaps judge them to\nhave moral significance only if we thought they had a capacity or potential\nfor conscious\nexperience.[26](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1067132)\n\nWe could thus imagine, as an extreme case, a technologically highly advanced\nsociety, containing many complex structures, some of them far more intricate\nand intelligent than anything that exists on the planet today—a society which\nnevertheless lacks any type of being that is conscious or whose welfare has\nmoral significance. In a sense, this would be an uninhabited society. It would\nbe a society of economic miracles and technological awesomeness, with nobody\nthere to benefit. A Disneyland without children.\n\n#### [Evolution is not necessarily\nup](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_006.html#filepos23755)\n\nThe word “evolution” is often used as a synonym of “progress,” perhaps\nreflecting a common uncritical image of evolution as a force for good. A\nmisplaced faith in the inherent beneficence of the evolutionary process can\nget in the way of a fair evaluation of the desirability of a multipolar\noutcome in which the future of intelligent life is determined by competitive\ndynamics. Any such evaluation must rest on some (at least implicit) opinion\nabout the probability distribution of different phenotypes turning out to be\nadaptive in a post-transition digital life soup. It would be difficult in the\nbest of circumstances to extract a clear and correct answer from the\nunavoidable goo of uncertainty that pervades these matters: more so, if we\nsuperadd a layer of Panglossian muck.\n\nA possible source for faith in freewheeling evolution is the apparent upward\ndirectionality exhibited by the evolutionary process in the past. Starting\nfrom rudimentary replicators, evolution produced increasingly “advanced”\norganisms, including creatures with minds, consciousness, language, and\nreason. More recently, cultural and technological processes, which bear some\nloose similarities to biological evolution, have enabled humans to develop at\nan accelerated pace. On a geological as well as a historical timescale, the\nbig picture seems to show an overarching trend toward increasing levels of\ncomplexity, knowledge, consciousness, and coordinated goal-directed\norganization: a trend which, not to put too fine a point on it, one might\nlabel\n“progress.”[27](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1068170)\n\nThe image of evolution as a process that reliably produces benign effects is\ndifficult to reconcile with the enormous suffering that we see in both the\nhuman and the natural world. Those who cherish evolution’s achievements may do\nso more from an aesthetic than an ethical perspective. Yet the pertinent\nquestion is not what kind of future it would be fascinating to read about in a\nscience fiction novel or to see depicted in a nature documentary, but what\nkind of future it would be good to live in: two very different matters.\n\nFurthermore, we have no reason to think that whatever progress there has been\nwas in any way inevitable. Much might have been luck. This objection derives\nsupport from the fact that an observation selection effect filters the\nevidence we can have about the success of our own evolutionary\ndevelopment.[28](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1068668)\nSuppose that on 99.9999% of all planets where life emerged it went extinct\nbefore developing to the point where intelligent observers could begin to\nponder their origin. What should we expect to observe if that were the case?\nArguably, we should expect to observe something like what we do in fact\nobserve. The hypothesis that the odds of intelligent life evolving on a given\nplanet are low does not predict that we should find ourselves on a planet\nwhere life went extinct at an early stage; rather, it may predict that we\nshould find ourselves on a planet where intelligent life evolved, even if such\nplanets constitute a very small fraction of all planets where primitive life\nevolved. Life’s long track record on Earth may therefore offer scant support\nto the claim that there was a high chance—let alone anything approaching\ninevitability—involved in the rise of higher organisms on our\nplanet.[29](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1068826)\n\nThirdly, even if present conditions had been idyllic, and even if they could\nhave been shown to have arisen ineluctably from some generic primordial state,\nthere would still be no guarantee that the melioristic trend is set to\ncontinue into the indefinite future. This holds even if we disregard the\npossibility of a cataclysmic extinction event and indeed even if we assume\nthat evolutionary developments will continue to produce systems of increasing\ncomplexity.\n\nWe suggested earlier that machine intelligence workers selected for maximum\nproductivity would be working extremely hard and that it is unknown how happy\nsuch workers would be. We also raised the possibility that the fittest life\nforms within a competitive future digital life soup might not even be\nconscious. Short of a complete loss of pleasure, or of consciousness, there\ncould be a wasting away of other qualities that many would regard as\nindispensible for a good life. Humans value music, humor, romance, art, play,\ndance, conversation, philosophy, literature, adventure, discovery, food and\ndrink, friendship, parenting, sport, nature, tradition, and spirituality,\namong many other things. There is no guarantee that any of these would remain\nadaptive. Perhaps what will maximize fitness will be nothing but nonstop high-\nintensity drudgery, work of a drab and repetitive nature, destitute of ludic\nfrisson, aimed only at improving the eighth decimal place of some economic\noutput measure. The phenotypes selected would then have lives lacking in the\naforesaid qualities, and depending on one’s axiology the result might strike\none as either abhorrent, worthless, or merely impoverished, but at any rate a\nfar cry from a utopia one would feel worthy of one’s commendation.\n\nIt might be wondered how such a bleak picture could be consistent with the\nfact that we do now indulge in music, humor, romance, art, etc. If these\nbehaviors are really so “wasteful,” then how come they have been tolerated and\nindeed promoted by the evolutionary processes that shaped our species? That\nmodern man is in an evolutionary disequilibrium does not account for this; for\nour Pleistocene forebears, too, engaged in most of these dissipations. Many of\nthe behaviors in question are not even unique to _Homo sapiens_. Flamboyant\ndisplay is found in a wide variety of contexts, from sexual selection in the\nanimal kingdom to prestige contests among nation\nstates.[30](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1069147)\n\nAlthough a full evolutionary explanation for each of these behaviors is beyond\nthe scope of the present inquiry, we can note that some of them serve\nfunctions that may not be as relevant in a machine intelligence context. Play,\nfor example, which occurs only in some species and predominantly among\njuveniles, is mainly a way for the young animal to learn skills that it will\nneed later in life. When emulations can be created as adults, already in\npossession of a mature repertoire of skills, or when knowledge and techniques\nacquired by one AI can be directly ported into another AI, the need for\nplayful behavior might become less widespread.\n\nMany of the other examples of humanistic behaviors may have evolved as hard-\nto-fake signals of qualities that are difficult to observe directly, such as\nbodily or mental resilience, social status, quality of allies, ability and\nwillingness to prevail in a fight, or possession of resources. The peacock’s\ntail is the classic instance: only fit peacocks can afford to sprout truly\nextravagant plumage, and peahens have evolved to find it attractive. No less\nthan morphological traits, behavioral traits too can signal genetic fitness or\nother socially relevant\nattributes.[31](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1069256)\n\nGiven that flamboyant display is so common among both humans and other\nspecies, one might consider whether it would not also be part of the\nrepertoire of technologically more advanced life forms. Even if there were to\nbe no narrowly instrumental use for playfulness or musicality or even for\nconsciousness in the future ecology of intelligent information processing,\nmight not these traits nonetheless confer some evolutionary advantage to their\npossessors by virtue of being reliable signals of other adaptive qualities?\n\nWhile the possibility of a pre-established harmony between what is valuable to\nus and what would be adaptive in a future digital ecology is hard to rule out,\nthere are reasons for skepticism. Consider, first, that many of the costly\ndisplays we find in nature are linked to sexual\nselection.[32](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1069383)\nReproduction among technologically mature life forms, in contrast, may be\npredominantly or exclusively asexual.\n\nSecond, technologically advanced agents might have available new means of\nreliably communicating information about themselves, means that do not rely on\ncostly display. Even today, when professional lenders assess creditworthiness\nthey tend to rely more on documentary evidence, such as ownership certificates\nand bank statements, than on costly displays, such as designer suits and Rolex\nwatches. In the future, it might be possible to employ auditing firms that\nverify through detailed examination of behavioral track records, testing in\nsimulated environments, or direct inspection of source code, that a client\nagent possesses a claimed attribute. Signaling one’s qualities by agreeing to\nsuch auditing might be more efficient than signaling via flamboyant display.\nSuch a professionally mediated signal would still be costly to _fake_ —this\nbeing the essential feature that makes the signal reliable—but it could be\nmuch cheaper to transmit when _truthful_ than it would be to communicate an\nequivalent signal flamboyantly.\n\nThird, not all possible costly displays are intrinsically valuable or socially\ndesirable. Many are simply wasteful. The Kwakiutl potlatch ceremonies, a form\nof status competition between rival chiefs, involved the public destruction of\nvast amounts of accumulated\nwealth.[33](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1069497)\nRecord-breaking skyscrapers, megayachts, and moon rockets may be viewed as\ncontemporary analogs. While activities like music and humor could plausibly be\nclaimed to enhance the intrinsic quality of human life, it is doubtful that a\nsimilar claim could be sustained with regard to the costly pursuit of fashion\naccessories and other consumerist status symbols. Worse, costly display can be\noutright harmful, as in macho posturing leading to gang violence or military\nbravado. Even if future intelligent life forms would use costly signaling,\ntherefore, it is an open question whether the signal would be of a valuable\nsort—whether it would be like the rapturous melody of a nightingale or instead\nlike the toad’s monosyllabic croak (or the incessant barking of a rabid dog).\n\n### [Post-transition formation of a\nsingleton?](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_006.html#filepos23872)\n\nEven if the immediate outcome of the transition to machine intelligence were\nmultipolar, the possibility would remain of a singleton developing later. Such\na development would continue an apparent long-term trend toward larger scales\nof political integration, taking it to its natural\nconclusion.[34](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1069653)\nHow might this occur?\n\n#### [A second\ntransition](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_006.html#filepos23990)\n\nOn way in which an initially multipolar outcome could converge into a\nsingleton post-transition is if there is, after the initial transition, a\nsecond technological transition big enough and steep enough to give a decisive\nstrategic advantage to one of the remaining powers: a power which might then\nseize the opportunity to establish a singleton. Such a hypothetical second\ntransition might be occasioned by a breakthrough to a higher level of\nsuperintelligence. For instance, if the first wave of machine\nsuperintelligence is emulation-based, then a second surge might result when\nthe emulations now doing the research succeed in developing effective self-\nimproving artificial\nintelligence.[35](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1070990)\n(Alternatively, a second transition might be triggered by a breakthrough in\nnanotechnology or some other military or general-purpose technology as yet\nunenvisaged.)\n\nThe pace of development after the initial transition would be extremely rapid.\nEven a short gap between the leading power and its closest competitor could\ntherefore plausibly result in a decisive strategic advantage for the leading\npower during a second transition. Suppose, for example, that two projects\nenter the first transition only a few days apart, and that the takeoff is slow\nenough that this gap does not give the leading project a decisive strategic\nadvantage at any point during the takeoff. The two projects both emerge as\nsuperintelligent powers, though one of them remains a few days ahead of the\nother. But developments are now occurring on the research timescales\ncharacteristic of machine superintelligence—perhaps thousands or millions of\ntimes faster than research conducted on a biological human timescale.\nDevelopment of the second-transition technology might therefore be completed\nin days, hours, or minutes. Even though the frontrunner’s lead is a mere few\ndays, a breakthrough could thus catapult it into a decisive strategic\nadvantage. Note, however, that if technological diffusion (via espionage or\nother channels) speeds up as much as technological development, then this\neffect would be negated. What would remain relevant would be the steepness of\nthe second transition, that is, the speed at which it would unfold relative to\nthe general speed of events in the period after the first transition. (In this\nsense, the faster things are happening after the first transition, the less\nsteep the second transition would tend to be.)\n\nOne might also speculate that a decisive strategic advantage would be more\nlikely to be actually used to establish a singleton if it arises during a\nsecond (or subsequent) transition. After the first transition, decision makers\nwould either be superintelligent or have access to advice from a\nsuperintelligence, which would clarify the implications of available strategic\noptions. Furthermore, the situation after the first transition might be one in\nwhich a preemptive move against potential competitors would be less dangerous\nfor the aggressor. If the decision-making minds after the first transition are\ndigital, they could be copied and thereby rendered less vulnerable to a\ncounterattack. Even if a defender had the ability to kill nine-tenths of the\naggressor’s population in a retaliatory strike, this would scarcely offer much\ndeterrence if the deceased could be immediately resurrected from redundant\nbackups. Devastation of infrastructure (which can be rebuilt) might also be\ntolerable to digital minds with effectively unlimited lifespans, who might be\nplanning to maximize their resources and influence on a cosmological\ntimescale.\n\n#### [Superorganisms and scale\neconomies](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_006.html#filepos24095)\n\nThe size of coordinated human aggregates, such as firms or nations, is\ninfluenced by various parameters—technological, military, financial, and\ncultural—that can vary from one historical epoch to another. A machine\nintelligence revolution would entail profound changes in many these\nparameters. Perhaps these changes would facilitate the rise of a singleton.\nAlthough we cannot, without looking in detail at what these prospective\nchanges are, exclude the opposite possibility—that the changes would\nfacilitate fragmentation rather than unification—we can nevertheless note that\nthe increased variance or uncertainty that we confront here may itself be a\nground for giving greater credence to the potential emergence of a singleton\nthan we would otherwise do. A machine intelligence revolution might, so to\nspeak, stir things up—might reshuffle the deck to make possible geopolitical\nrealignments that seemed perhaps otherwise not to have been in the cards.\n\nA comprehensive analysis of all the factors that may influence the scale of\npolitical integration would take us far beyond the scope of this book: a\nreview of the relevant political science and economics literature could itself\neasily fill an entire volume. We must confine ourselves to making brief\nallusion to a couple of factors, aspects of the digitization of agents that\nmay make it easier to centralize control.\n\nCarl Shulman has argued that in a population of emulations, selection\npressures would favor the emergence of “superorganisms,” groups of emulations\nready to sacrifice themselves for the good of their\nclan.[36](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1071442)\nSuperorganisms would be spared the agency problems that beset organizations\nwhose members pursue their own self-interest. Like the cells in our bodies, or\nthe individual animals in a colony of eusocial insects, emulations that were\nwholly altruistic toward their copy-siblings would cooperate with one another\neven in the absence of elaborate incentive schemes.\n\nSuperorganisms would have a particularly strong advantage if nonconsensual\ndeletion (or indefinite suspension) of individual emulations is disallowed.\nFirms or countries that employ emulations insisting on self-preservation would\nbe saddled with an unending commitment to pay upkeep for obsolete or redundant\nworkers. In contrast, organizations whose emulations willingly deleted\nthemselves when their services were no longer required could more easily adapt\nto fluctuations in demand; and they could experiment freely, proliferating\nvariations of their workers and retaining only the most productive.\n\nIf involuntary deletion is _not_ disallowed, then the comparative advantage of\neusocial emulations is reduced, though perhaps not eliminated. Employers of\ncooperative self-sacrificers might still reap efficiency gains from reduced\nagency problems throughout the organization, including being spared the\ntrouble of having to defeat whatever resistance emulations could put up\nagainst their own deletion. In general, the productivity gains of having\nworkers willing to sacrifice their individual lives for the common weal are a\nspecial case of the benefits an organization can derive from having members\nwho are fanatically devoted to it. Such members would not only leap into the\ngrave for the organization, and work long hours for little pay: they would\nalso shun office politics and try consistently to act in what they took to be\nthe organization’s best interest, reducing the need for supervision and\nbureaucratic constraints.\n\nIf the only way to achieve such dedication were by restricting membership to\ncopy-siblings (so that all emulations in a particular superorganism were\nstamped out from the same template), then superorganisms would suffer some\ndisadvantage in being able to draw only from a range of skills narrower than\nthat of rival organizations, a disadvantage which might or might not be large\nenough to outweigh the advantages of avoiding internal agency\nproblems.[37](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1071554)\nThis disadvantage would be greatly alleviated if a superorganism could at\nleast contain members with different training. Even if all its members were\nderived from a single ur-template, its workforce could then still contribute a\ndiversity of skills. Starting with a polymathically talented emulation ur-\ntemplate, lineages could be branched off into different training programs, one\ncopy learning accounting, another electrical engineering, and so forth. This\nwould produce a membership with diverse skills though not of diverse talents.\n(Maximum diversity might require that more than one ur-template be used.)\n\nThe essential property of a superorganism is not that it consists of copies of\na single progenitor but that all the individual agents within it are fully\ncommitted to a common goal. The ability to create a superorganism can thus be\nviewed as requiring a partial solution to the control problem. Whereas a\ncompletely general solution to the control problem would enable somebody to\ncreate an agent with any arbitrary final goal, the partial solution needed for\nthe creation of a superorganism requires merely the ability to fashion\nmultiple agents with the same final goal (for some nontrivial but not\nnecessarily arbitrary final\ngoal).[38](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1072012)\n\nThe main consideration put forward in this subsection is thus not really\nlimited to monoclonal emulation groups, but can be stated more generally in a\nway that makes clear that it applies to a wide range of multipolar machine\nintelligence scenarios. It is that certain types of advances in motivation\nselection techniques, which may become feasible when the actors are digital,\nmay help overcome some of the inefficiencies that currently hamper large human\norganizations and that counterbalance economies of scale. With these limits\nlifted, organizations—be they firms, nations, or other economic or political\nentities—could increase in size. This is one factor that could facilitate the\nemergence of a post-transition singleton.\n\nOne area in which superorganisms (or other digital agents with partially\nselected motivations) might excel is coercion. A state might use motivation\nselection methods to ensure that its police, military, intelligence service,\nand civil administration are uniformly loyal. As Shulman notes,\n\n> Saved states [of some loyal emulation that has been carefully prepared and\n> verified] could be copied billions of times to staff an ideologically\n> uniform military, bureaucracy, and police force. After a short period of\n> work, each copy would be replaced by a fresh copy of the same saved state,\n> preventing ideological drift. Within a given jurisdiction, this capability\n> could allow incredibly detailed observation and regulation: there might be\n> one such copy for every other resident. This could be used to prohibit the\n> development of weapons of mass destruction, to enforce regulations on brain\n> emulation experimentation or reproduction, to enforce a liberal democratic\n> constitution, or to create an appalling and permanent\n> totalitarianism[39](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1072595)\n\nThe first-order effect of such a capability would seem to be to consolidate\npower, and possibly to concentrate it in fewer hands.\n\n### [Unification by\ntreaty](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_006.html#filepos24215)\n\nThere may be large potential gains to be had from international collaboration\nin a post-transition multipolar world. Wars and arms races could be avoided.\nAstrophysical resources could be colonized and harvested at a globally optimum\npace. The development of more advanced forms of machine intelligence could be\ncoordinated to avoid a rush and to allow new designs to be thoroughly vetted.\nOther developments that might pose existential risks could be postponed. And\nuniform regulations could be enforced globally, including provisions for a\nguaranteed standard of living (which would require some form of population\ncontrol) and for preventing exploitation and abuse of emulations and other\ndigital and biological minds. Furthermore, agents with resource-satiable\npreferences (more on this in [Chapter\n13](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_020.html#filepos727275)) would\nprefer a sharing agreement that would guarantee them a certain slice of the\nfuture to a winner-takes-all struggle in which they would risk getting\nnothing.\n\nThe presence of big potential gains from collaboration, however, does not\nimply that collaboration will actually be achieved. In the world today, many\ngreat boons could be obtained via better global coordination—reductions of\nmilitary expenditures, wars, overfishing, trade barriers, and atmospheric\npollution, among others. Yet these plump fruits are left to spoil on the\nbranch. Why is that? What stops a fully cooperative outcome that would\nmaximize the common good?\n\nOne obstacle is the difficulty of ensuring compliance with any treaty that\nmight be agreed, including monitoring and enforcement costs. Two nuclear\nrivals might each be better off if they both relinquished their atom bombs;\nyet even if they could reach an in-principle agreement to do so, disarmament\ncould nevertheless prove elusive because of their mutual fear that the other\nparty might cheat. Allaying this fear would require setting up a verification\nmechanism. There may have to be inspectors to oversee the destruction of\nexisting stockpiles, and then to monitor nuclear reactors and other\nfacilities, and to gather technical and human intelligence, in order to ensure\nthat the weapons program is not reconstituted. One cost is paying for these\ninspectors. Another cost is the risk that the inspectors will spy and make off\nwith commercial or military secrets. Perhaps most significantly, each party\nmight fear that the other will preserve a clandestine nuclear capability. Many\na potentially beneficial deal never comes off because compliance would be too\ndifficult to verify.\n\nIf new inspection technologies that reduced monitoring costs became available,\none would expect this to result in increased cooperation. Whether monitoring\ncosts would on net be reduced in the post-transition era, however, is not\nentirely clear. While there would certainly be many powerful new inspection\ntechniques, there would also be new means of concealment. In particular, an\nincreasing portion of the activities one might want to regulate would be\ntaking place in cyberspace, out of reach of physical surveillance. For\nexample, digital minds working on designing a new nanotech weapons system or a\nnew generation of artificial intelligence may do so without leaving much of a\nphysical footprint. Digital forensics may fail to penetrate all the layers of\nconcealment and encryption in which a treaty-violator may cloak its illicit\nactivities.\n\nReliable lie detection, if it could be developed, would be an extremely useful\ntool for monitoring\ncompliance.[40](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1072710)\nAn inspection protocol could include provisions for interviewing key\nofficials, to verify that they are intent on implementing all the provisions\nof the treaty and that they know of no violations despite making strong\nefforts to find out.\n\nA decision maker planning to cheat might defeat such a lie-detection-based\nverification scheme by first issuing orders to subordinates to undertake the\nillicit activity and to conceal the activity even from the decision maker\nherself, and then subjecting herself to some procedure that erases her memory\nof having engaged in these machinations. Suitably targeted memory-erasure\noperations might well be feasible in biological brains with more advanced\nneurotechnology. It might be even easier in machine intelligences (depending\non their architecture).\n\nStates could seek to overcome this problem by committing themselves to an\nongoing monitoring scheme that regularly tests key officials with a lie\ndetector to check whether they harbor any intent to subvert or circumvent any\ntreaty to which the state has entered or may enter in the future. Such a\ncommitment could be viewed as a kind of meta-treaty, which would facilitate\nthe verification of other treaties; but states might commit themselves to it\nunilaterally to gain the benefit of being regarded as a trustworthy\nnegotiation partner. However, this commitment or meta-treaty would face the\nsame problem of subversion through a delegate-and-forget ploy. Ideally, the\nmeta-treaty would be put into effect _before_ any party had an opportunity to\nmake the internal arrangements necessary to subvert its implementation. Once\nvillainy has had an unguarded moment to sow its mines of deception, trust can\nnever set foot there again.\n\nIn some cases, the mere ability to _detect_ treaty violations is sufficient to\nestablish the confidence needed for a deal. In other cases, however, there is\na need for some mechanism to _enforce_ compliance or mete out punishment if a\nviolation should occur. The need for an enforcement mechanism may arise if the\nthreat of the wronged party withdrawing from the treaty is not enough to deter\nviolations, for instance if the treaty-violator would gain such an advantage\nthat he would not subsequently care how the other party responds.\n\nIf highly effective motivation selection methods are available, this\nenforcement problem could be solved by empowering an independent agency with\nsufficient police or military strength to enforce the treaty even against the\nopposition of one or several of its signatories. This solution requires that\nthe enforcement agency can be trusted. But with sufficiently good motivation\nselection techniques, the requisite confidence might be achieved by having all\nthe parties to the treaty jointly oversee the design of the enforcement\nagency.\n\nHanding over power to an external enforcement agency raises many of the same\nissues that we confronted earlier in our discussions of a unipolar outcome\n(one in which a singleton arises prior to or during the initial machine\nintelligence revolution). In order to be able to enforce treaties concerning\nthe vital security interests of rival states, the external enforcement agency\nwould in effect need to constitute a singleton: a global superintelligent\nLeviathan. One difference, however, is that we are now considering a post-\ntransition situation, in which the agents that would have to create this\nLeviathan would have greater competence than we humans currently do. These\nLeviathan-creators may themselves already be superintelligent. This would\ngreatly improve the odds that they could solve the control problem and design\nan enforcement agency that would serve the interests of all the parties that\nhave a say in its construction.\n\nAside from the costs of monitoring and enforcing compliance, are there any\nother obstacles to global coordination? Perhaps the major remaining issue is\nwhat we can refer to as _bargaining\ncosts_.[41](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1073518)\nEven when there is a possible bargain that would benefit everybody involved,\nit sometimes does not get off the ground because the parties fail to agree on\nhow to divide the spoils. For example, if two persons could make a deal that\nwould net them a dollar in profit, but each party feels she deserves sixty\ncents and refuses to settle for less, the deal will not happen and the\npotential gain will be forfeited. In general, negotiations can be difficult or\nprotracted, or remain altogether barren, because of strategic bargaining\nchoices made by some of the parties.\n\nIn real life, human beings frequently succeed in reaching agreements despite\nthe possibility for strategic bargaining (though often not without\nconsiderable expenditure of time and patience). It is conceivable, however,\nthat strategic bargaining problems would have a different dynamic in the post-\ntransition era. An AI negotiator might more consistently adhere to some\nparticular formal conception of rationality, possibly with novel or\nunanticipated consequences when matched with other AI negotiators. An AI might\nalso have available to it moves in the bargaining game that are either\nunavailable to humans or very much more difficult for humans to execute,\nincluding the ability to precommit to a policy or a course of action. While\nhumans (and human-run institutions) are occasionally able to precommit—with\nimperfect degrees of credibility and specificity—some types of machine\nintelligence might be able to make arbitrary unbreakable precommitments and to\nallow negotiating partners to confirm that such a precommitment has been\nmade.[42](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1074074)\n\nThe availability of powerful precommitment techniques could profoundly alter\nthe nature of negotiations, potentially giving an immense edge to an agent\nthat has a first-mover advantage. If a particular agent’s participation is\nnecessary for the realization of some prospective gains from cooperation, and\nif that agent is able to make the first move, it would be in a position to\ndictate the division of the spoils by precommitting not to accept any deal\nthat gives it less than, say, 99% of the surplus value. Other agents would\nthen be faced with the choice of either getting nothing (by rejecting the\nunfair proposal) or getting 1% of the value (by caving in). If the first-\nmoving agent’s precommitment is publicly verifiable, its negotiating partners\ncould be sure that these are their only two options.\n\nTo avoid being exploited in this manner, agents might precommit to refuse\nblackmail and to decline all unfair offers. Once such a precommitment has been\nmade (and successfully publicized), other agents would not find it in their\ninterest to make threats or to precommit themselves to only accepting deals\ntilted in their own favor, because they would know that threats would fail and\nthat unfair proposals would be rejected. But this just demonstrates again that\nthe advantage is with the first-mover. The agent who moves first can choose\nwhether to parlay its position of strength only to deter others from taking\nunfair advantage, or to make a grab for the lion’s share of future spoils.\n\nBest situated of all, it might seem, would be the agent who starts out with a\ntemperament or a value system that makes him impervious to extortion or indeed\nto any offer of a deal in which his participation is indispensable but he is\nnot getting almost all of the gains. Some humans seem already to possess\npersonality traits corresponding to various aspects of an uncompromising\nspirit.[43](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1074776) A\nhigh-strung disposition, however, could backfire should it turn out that there\nare other agents around who feel entitled to more than their fair share and\nare committed to not backing down. The unstoppable force would then encounter\nthe unmovable object, resulting in a failure to reach agreement (or worse:\ntotal war). The meek and the akratic would at least get something, albeit less\nthan their fair share.\n\nWhat kind of game-theoretic equilibrium would be reached in such a post-\ntransition bargaining game is not immediately obvious. Agents might choose\nmore complicated strategies than the ones considered here. One _hopes_ that an\nequilibrium would be reached centered on some fairness norm that would serve\nas a Schelling point—a salient feature in a big outcome space which, because\nof shared expectations, becomes a likely coordination point in an otherwise\nunderdetermined coordination game. Such an equilibrium might be bolstered by\nsome of our evolved dispositions and cultural programming: a common preference\nfor fairness could, assuming we succeed in transferring our values into the\npost-transition era, bias expectations and strategies in ways that lead to an\nattractive\nequilibrium.[44](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1075309)\n\nIn any case, the upshot is that with the possibility of strong and flexible\nforms of precommitment, outcomes of negotiations might take on an unfamiliar\nguise. Even if the post-transition era started out multipolar, it might be\nthat a singleton would arise almost immediately as a consequence of a\nnegotiated treaty that resolves all important global coordination problems.\nSome transaction costs, perhaps including monitoring and enforcement costs,\nmight plummet with the new technological capabilities available to advanced\nmachine intelligences. Other costs, in particular costs related to strategic\nbargaining, might remain significant. But however strategic bargaining affects\nthe nature of the agreement that is reached, there is no clear reason why it\nwould long delay the reaching of some agreement if an agreement were ever to\nbe reached. If no agreement is reached, then some form of fighting might take\nplace; and either one faction might win, and form a singleton around the\nwinning coalition, or the result might be an interminable conflict, in which\ncase a singleton may never form and the overall outcome may fall terribly\nshort of what could and should have been achieved if humanity and its\ndescendants had acted in a more coordinated and cooperative fashion.\n\n![Image](images/00031.jpg)\n\nWe have seen that multipolarity, even if it could be achieved in a stable\nform, would not guarantee an attractive outcome. The original principal–agent\nproblem remains unsolved, and burying it under a new set of problems related\nto post-transition global coordination failures may only make the situation\nworse. Let us therefore return to the question of how we could safely keep a\nsingle superintelligent AI.\n\n\n## [CHAPTER 12  \nAcquiring\nvalues](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_006.html#filepos24335)\n\n**Capability control is, at best, a temporary and auxiliary measure. Unless\nthe plan is to keep superintelligence bottled up forever, it will be necessary\nto master motivation selection. But just how could we get some value into an\nartificial agent, so as to make it pursue that value as its final goal? While\nthe agent is unintelligent, it might lack the capability to understand or even\nrepresent any humanly meaningful value. Yet if we delay the procedure until\nthe agent is superintelligent, it may be able to resist our attempt to meddle\nwith its motivation system—and, as we showed in[Chapter\n7](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_014.html#filepos373977), it would\nhave convergent instrumental reasons to do so. This value-loading problem is\ntough, but must be confronted**.\n\n### [The value-loading\nproblem](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_006.html#filepos24466)\n\nIt is impossible to enumerate all possible situations a superintelligence\nmight find itself in and to specify for each what action it should take.\nSimilarly, it is impossible to create a list of all possible worlds and assign\neach of them a value. In any realm significantly more complicated than a game\nof tic-tac-toe, there are far too many possible states (and state-histories)\nfor exhaustive enumeration to be feasible. A motivation system, therefore,\ncannot be specified as a comprehensive lookup table. It must instead be\nexpressed more abstractly, as a formula or rule that allows the agent to\ndecide what to do in any given situation.\n\nOne formal way of specifying such a decision rule is via a utility function. A\nutility function (as we recall from [Chapter\n1](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos33916)) assigns\nvalue to each outcome that might obtain, or more generally to each “possible\nworld.” Given a utility function, one can define an agent that maximizes\nexpected utility. Such an agent selects at each time the action that has the\nhighest expected utility. (The expected utility is calculated by weighting the\nutility of each possible world with the subjective probability of that world\nbeing the actual world conditional on a particular action being taken.) In\nreality, the possible outcomes are too numerous for the expected utility of an\naction to be calculated exactly. Nevertheless, the decision rule and the\nutility function together determine a normative ideal—an optimality\nnotion—that an agent might be designed to approximate; and the approximation\nmight get closer as the agent gets more\nintelligent.[1](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1075659)\nCreating a machine that can compute a good approximation of the expected\nutility of the actions available to it is an AI-complete\nproblem.[2](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1076226)\nThis chapter addresses another problem, a problem that remains even if the\nproblem of making machines intelligent is solved.\n\nWe can use this framework of a utility-maximizing agent to consider the\npredicament of a future seed-AI programmer who intends to solve the control\nproblem by endowing the AI with a final goal that corresponds to some\nplausible human notion of a worthwhile outcome. The programmer has some\nparticular human value in mind that he would like the AI to promote. To be\nconcrete, let us say that it is happiness. (Similar issues would arise if we\nthe programmer were interested in justice, freedom, glory, human rights,\ndemocracy, ecological balance, or self-development.) In terms of the expected\nutility framework, the programmer is thus looking for a utility function that\nassigns utility to possible worlds in proportion to the amount of happiness\nthey contain. But how could he express such a utility function in computer\ncode? Computer languages do not contain terms such as “happiness” as\nprimitives. If such a term is to be used, it must first be defined. It is not\nenough to define it in terms of other high-level human concepts—“happiness is\nenjoyment of the potentialities inherent in our human nature” or some such\nphilosophical paraphrase. The definition must bottom out in terms that appear\nin the AI’s programming language, and ultimately in primitives such as\nmathematical operators and addresses pointing to the contents of individual\nmemory registers. When one considers the problem from this perspective, one\ncan begin to appreciate the difficulty of the programmer’s task.\n\nIdentifying and codifying our own final goals is difficult because human goal\nrepresentations are complex. Because the complexity is largely transparent to\nus, however, we often fail to appreciate that it is there. We can compare the\ncase to visual perception. Vision, likewise, might seem like a simple thing,\nbecause we do it\neffortlessly.[3](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1076657)\nWe only need to open our eyes, so it seems, and a rich, meaningful, eidetic,\nthree-dimensional view of the surrounding environment comes flooding into our\nminds. This intuitive understanding of vision is like a duke’s understanding\nof his patriarchal household: as far as he is concerned, things simply appear\nat their appropriate times and places, while the mechanism that produces those\nmanifestations are hidden from view. Yet accomplishing even the simplest\nvisual task—finding the pepper jar in the kitchen—requires a tremendous amount\nof computational work. From a noisy time series of two-dimensional patterns of\nnerve firings, originating in the retina and conveyed to the brain via the\noptic nerve, the visual cortex must work backwards to reconstruct an\ninterpreted three-dimensional representation of external space. A sizeable\nportion of our precious one square meter of cortical real estate is zoned for\nprocessing visual information, and as you are reading this book, billions of\nneurons are working ceaselessly to accomplish this task (like so many\nseamstresses, bent over their sewing machines in a sweatshop, sewing and re-\nsewing a giant quilt many times a second). In like manner, our seemingly\nsimple values and wishes in fact contain immense\ncomplexity.[4](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1076946)\nHow could our programmer transfer this complexity into a utility function?\n\nOne approach would be to try to directly code a complete representation of\nwhatever goal we have that we want the AI to pursue; in other words, to write\nout an explicit utility function. This approach might work if we had\nextraordinarily simple goals, for example if we wanted to calculate the digits\nof pi—that is, if the _only_ thing we wanted was for the AI to calculate the\ndigits of pi and we were indifferent to any other consequence that would\nresult from the pursuit of this goal—recall our earlier discussion of the\nfailure mode of infrastructure profusion. This explicit coding approach might\nalso have some promise in the use of domesticity motivation selection methods.\nBut if one seeks to promote or protect any plausible _human_ value, and one is\nbuilding a system intended to become a superintelligent sovereign, then\nexplicitly coding the requisite complete goal representation appears to be\nhopelessly out of\nreach.[5](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1077125)\n\nIf we cannot transfer human values into an AI by typing out full-blown\nrepresentations in computer code, what else might we try? This chapter\ndiscusses several alternative paths. Some of these may look plausible at first\nsight—but much less so upon closer examination. Future explorations should\nfocus on those paths that remain open.\n\nSolving the value-loading problem is a research challenge worthy of some of\nthe next generation’s best mathematical talent. We cannot postpone confronting\nthis problem until the AI has developed enough reason to easily understand our\nintentions. As we saw in the section on convergent instrumental reasons, a\ngeneric system will resist attempts to alter its final values. If an agent is\nnot already fundamentally friendly by the time it gains the ability to reflect\non its own agency, it will not take kindly to a belated attempt at\nbrainwashing or a plot to replace it with a different agent that better loves\nits neighbor.\n\n### [Evolutionary\nselection](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_006.html#filepos24568)\n\nEvolution has produced an organism with human values at least once. This fact\nmight encourage the belief that evolutionary methods are the way to solve the\nvalue-loading problem. There are, however, severe obstacles to achieving\nsafety along this path. We have already pointed to these obstacles at the end\nof [Chapter\n10](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_017.html#filepos509701) when we\ndiscussed how powerful search processes can be dangerous.\n\nEvolution can be viewed as a particular class of search algorithms that\ninvolve the alternation of two steps, one expanding a population of solution\ncandidates by generating new candidates according to some relatively simple\nstochastic rule (such as random mutation or sexual recombination), the other\ncontracting the population by pruning candidates that score poorly when tested\nby an evaluation function. As with many other types of powerful search, there\nis the risk that the process will find a solution that satisfies the formally\nspecified search criteria but not our implicit expectations. (This would hold\nwhether one seeks to evolve a digital mind that has the same goals and values\nas a typical human being, or instead a mind that is, for instance, perfectly\nmoral or perfectly obedient.) The risk would be avoided if we could specify a\nformal search criterion that accurately represented all dimensions of our\ngoals, rather than just one aspect of what we think we desire. But this is\nprecisely the value-loading problem, and it would of course beg the question\nin this context to assume that problem solved.\n\nThere is a further problem:\n\n> The total amount of suffering per year in the natural world is beyond all\n> decent contemplation. During the minute that it takes me to compose this\n> sentence, thousands of animals are being eaten alive, others are running for\n> their lives, whimpering with fear, others are being slowly devoured from\n> within by rasping parasites, thousands of all kinds are dying of starvation,\n> thirst and\n> disease.[6](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1078225)\n\nEven just within our species, 150,000 persons are destroyed each day while\ncountless more suffer an appalling array of torments and\ndeprivations.[7](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1078477)\nNature might be a great experimentalist, but one who would never pass muster\nwith an ethics review board—contravening the Helsinki Declaration and every\nnorm of moral decency, left, right, and center. It is important that we not\ngratuitously replicate such horrors _in silico_. Mind crime seems especially\ndifficult to avoid when evolutionary methods are used to produce human-like\nintelligence, at least if the process is meant to look anything like actual\nbiological\nevolution.[8](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1078710)\n\n### [Reinforcement\nlearning](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_006.html#filepos24667)\n\nReinforcement learning is an area of machine learning that studies techniques\nwhereby agents can learn to maximize some notion of cumulative reward. By\nconstructing an environment in which desired performance is rewarded, a\nreinforcement-learning agent can be made to learn to solve a wide class of\nproblems (even in the absence of detailed instruction or feedback from the\nprogrammers, aside from the reward signal). Often, the learning algorithm\ninvolves the gradual construction of some kind of evaluation function, which\nassigns values to states, state–action pairs, or policies. (For instance, a\nprogram can learn to play backgammon by using reinforcement learning to\nincrementally improve its evaluation of possible board positions.) The\nevaluation function, which is continuously updated in light of experience,\ncould be regarded as incorporating a form of learning about value. However,\nwhat is being learned is not new _final_ values but increasingly accurate\n_estimates of the instrumental values_ of reaching particular states (or of\ntaking particular actions in particular states, or of following particular\npolicies). Insofar as a reinforcement-learning agent can be described as\nhaving a final goal, that goal remains constant: to maximize future reward.\nAnd reward consists of specially designated percepts received from the\nenvironment. Therefore, the wireheading syndrome remains a likely outcome in\nany reinforcement agent that develops a world model sophisticated enough to\nsuggest this alternative way of maximizing\nreward.[9](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1079360)\n\nThese remarks do not imply that reinforcement-learning methods could never be\nused in a safe seed AI, only that they would have to be subordinated to a\nmotivation system that is not itself organized around the principle of reward\nmaximization. That, however, would require that a solution to the value-\nloading problem had been found by some other means than reinforcement\nlearning.\n\n### [Associative value\naccretion](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_006.html#filepos24766)\n\nNow one might wonder: if the value-loading problem is so tricky, how do we\nourselves manage to acquire our values?\n\nOne possible (oversimplified) model might look something like this. We begin\nlife with some relatively simple starting preferences (e.g. an aversion to\nnoxious stimuli) together with a set of dispositions to acquire additional\npreferences in response to various possible experiences (e.g. we might be\ndisposed to form a preference for objects and behaviors that we find to be\nvalued and rewarded in our culture). Both the simple starting preferences and\nthe dispositions are innate, having been shaped by natural and sexual\nselection over evolutionary timescales. Yet which preferences we end up with\nas adults depends on life events. Much of the information content in our final\nvalues is thus acquired from our experiences rather than preloaded in our\ngenomes.\n\nFor example, many of us love another person and thus place great final value\non his or her well-being. What is required to represent such a value? Many\nelements are involved, but consider just two: a representation of “person” and\na representation of “well-being.” These concepts are not directly coded in our\nDNA. Rather, the DNA contains instructions for building a brain, which, when\nplaced in a typical human environment, will over the course of several years\ndevelop a world model that includes concepts of persons and of well-being.\nOnce formed, these concepts can be used to represent certain meaningful\nvalues. But some mechanism needs to be innately present that leads to values\nbeing formed around _these_ concepts, rather than around other acquired\nconcepts (like that of a flowerpot or a corkscrew).\n\nThe details of how this mechanism works are not well understood. In humans,\nthe mechanism is probably complex and multifarious. It is easier to understand\nthe phenomenon if we consider it in a more rudimentary form, such as filial\nimprinting in nidifugous birds, where the newly hatched chick acquires a\ndesire for physical proximity to an object that presents a suitable moving\nstimulus within the first day after hatching. Which particular object the\nchick desires to be near depends on its experience; only the general\ndisposition to imprint in this way is genetically determined. Analogously,\nHarry might place a final value on Sally’s well-being; but had the twain never\nmet, he might have fallen in love with somebody else instead, and his final\nvalues would have been different. The ability of our genes to code for the\nconstruction of a goal-acquiring mechanism explains how we come to have final\ngoals of great informational complexity, greater than could be contained in\nthe genome itself.\n\nWe may consequently consider whether we might build the motivation system for\nan artificial intelligence on the same principle. That is, instead of\nspecifying complex values directly, could we specify some mechanism that leads\nto the acquisition of those values when the AI interacts with a suitable\nenvironment?\n\nMimicking the value-accretion process that takes place in humans seems\ndifficult. The relevant genetic mechanism in humans is the product of eons of\nwork by evolution, work that might be hard to recapitulate. Moreover, the\nmechanism is presumably closely tailored to the human neurocognitive\narchitecture and therefore not applicable in machine intelligences other than\nwhole brain emulations. And if whole brain emulations of sufficient fidelity\nwere available, it would seem easier to start with an adult brain that comes\nwith full representations of some human values\npreloaded.[10](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1080440)\n\nSeeking to implement a process of value accretion closely mimicking that of\nhuman biology therefore seems an unpromising line of attack on the value-\nloading problem. But perhaps we might design a more unabashedly artificial\nsubstitute mechanism that would lead an AI to import high-fidelity\nrepresentations of relevant complex values into its goal system? For this to\nsucceed, it may not be necessary to give the AI exactly the same evaluative\ndispositions as a biological human. That may not even be desirable as an\naim—human nature, after all, is flawed and all too often reveals a proclivity\nto evil which would be intolerable in any system poised to attain a decisive\nstrategic advantage. Better, perhaps, to aim for a motivation system that\ndeparts from the human norm in systematic ways, such as by having a more\nrobust tendency to acquire final goals that are altruistic, compassionate, or\nhigh-minded in ways we would recognize as reflecting exceptionally good\ncharacter if they were present in a human person. To count as improvements,\nhowever, such deviations from the human norm would have to be pointed in very\nparticular directions rather than at random; and they would continue to\npresuppose the existence of a largely undisturbed anthropocentric frame of\nreference to provide humanly meaningful evaluative generalizations (so as to\navoid the kind of perverse instantiation of superficially plausible goal\ndescriptions that we examined in [Chapter\n8](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_015.html#filepos408438)). It is an\nopen question whether this is feasible.\n\nOne further issue with associative value accretion is that the AI might\ndisable the accretion mechanism. As we saw in [Chapter\n7](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_014.html#filepos373977), goal-\nsystem integrity is a convergent instrumental value. When the AI reaches a\ncertain stage of cognitive development it may start to regard the continued\noperation of the accretion mechanism as a corrupting\ninfluence.[11](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1081269)\nThis is not necessarily a bad thing, but care would have to be taken to make\nthe sealing-up of the goal system occur at the right moment, _after_ the\nappropriate values have been accreted but _before_ they have been overwritten\nby additional unintended accretions.\n\n### [Motivational\nscaffolding](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_006.html#filepos24870)\n\nAnother approach to the value-loading problem is what we may refer to as\nmotivational scaffolding. It involves giving the seed AI an interim goal\nsystem, with relatively simple final goals that we can represent by means of\nexplicit coding or some other feasible method. Once the AI has developed more\nsophisticated representational faculties, we replace this interim scaffold\ngoal system with one that has different final goals. This successor goal\nsystem then governs the AI as it develops into a full-blown superintelligence.\n\nBecause the scaffold goals are not just instrumental but _final_ goals for the\nAI, the AI might be expected to resist having them replaced (goal-content\nintegrity being a convergent instrumental value). This creates a hazard. If\nthe AI succeeds in thwarting the replacement of its scaffold goals, the method\nfails.\n\nTo avoid this failure mode, precautions are necessary. For example, capability\ncontrol methods could be applied to limit the AI’s powers until the mature\nmotivation system has been installed. In particular, one could try to stunt\nits cognitive development at a level that is safe but that allows it to\nrepresent the values that we want to include in its ultimate goals. To do\nthis, one might try to differentially stunt certain types of intellectual\nabilities, such as those required for strategizing and Machiavellian scheming,\nwhile allowing (apparently) more innocuous abilities to develop to a somewhat\nhigher level.\n\nOne could also try to use motivation selection methods to induce a more\ncollaborative relationship between the seed AI and the programmer team. For\nexample, one might include in the scaffold motivation system the goal of\nwelcoming online guidance from the programmers, including allowing them to\nreplace any of the AI’s current\ngoals.[12](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1082428)\nOther scaffold goals might include being transparent to the programmers about\nits values and strategies, and developing an architecture that is easy for the\nprogrammers to understand and that facilitates the later implementation of a\nhumanly meaningful final goal, as well as domesticity motivations (such as\nlimiting the use of computational resources).\n\nOne could even imagine endowing the seed AI with the sole final goal of\nreplacing itself with a different final goal, one which may have been only\nimplicitly or indirectly specified by the programmers. Some of the issues\nraised by the use of such a “self-replacing” scaffold goal also arise in the\ncontext of the value learning approach, which is discussed in the next\nsubsection. Some further issues will be discussed in [Chapter\n13](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_020.html#filepos727275).\n\nThe motivational scaffolding approach is not without downsides. One is that it\ncarries the risk that the AI could become too powerful while it is still\nrunning on its interim goal system. It may then thwart the human programmers’\nefforts to install the ultimate goal system (either by forceful resistance or\nby quiet subversion). The old final goals may then remain in charge as the\nseed AI develops into a full-blown superintelligence. Another downside is that\ninstalling the ultimately intended goals in a human-level AI is not\nnecessarily that much easier than doing so in a more primitive AI. A human-\nlevel AI is more complex and might have developed an architecture that is\nopaque and difficult to alter. A seed AI, by contrast, is like a _tabula rasa_\non which the programmers can inscribe whatever structures they deem helpful.\nThis downside could be flipped into an upside if one succeeded in giving the\nseed AI scaffold goals that made it want to develop an architecture helpful to\nthe programmers in their later efforts to install the ultimate final values.\nHowever, it is unclear how easy it would be to give a seed AI scaffold goals\nwith this property, and it is also unclear how even an ideally motivated seed\nAI would be capable of doing a much better job than the human programming team\nat developing a good architecture.\n\n### [Value\nlearning](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_006.html#filepos24971)\n\nWe come now to an important but subtle approach to the value-loading problem.\nIt involves using the AI’s intelligence to _learn_ the values we want it to\npursue. To do this, we must provide a criterion for the AI that at least\nimplicitly picks out some suitable set of values. We could then build the AI\nto act according to its best estimates of these implicitly defined values. It\nwould continually refine its estimates as it learns more about the world and\ngradually unpacks the implications of the value-determining criterion.\n\nIn contrast to the scaffolding approach, which gives the AI an interim\nscaffold goal and later replaces it with a different final goal, the value\nlearning approach retains an unchanging final goal throughout the AI’s\ndevelopmental and operational phases. Learning does not change the goal. It\nchanges only the AI’s beliefs about the goal.\n\nThe AI thus must be endowed with a criterion that it can use to determine\nwhich percepts constitute evidence in favor of some hypothesis about what the\nultimate goal is, and which percepts constitute evidence against. Specifying a\nsuitable criterion could be difficult. Part of the difficulty, however,\npertains to the problem of creating artificial general intelligence in the\nfirst place, which requires a powerful learning mechanism that can discover\nthe structure of the environment from limited sensory inputs. That problem we\ncan set aside here. But even modulo a solution to how to create\nsuperintelligent AI, there remain the difficulties that arise specifically\nfrom the value-loading problem. With the value learning approach, these take\nthe form of needing to define a criterion that connects perceptual bitstrings\nto hypotheses about values.\n\nBefore delving into the details of how value learning could be implemented, it\nmight be helpful to illustrate the general idea with an example. Suppose we\nwrite down a description of a set of values on a piece of paper. We fold the\npaper and put it in a sealed envelope. We then create an agent with human-\nlevel general intelligence, and give it the following final goal: “Maximize\nthe realization of the values described in the envelope.” What will this agent\ndo?\n\nThe agent does not initially know what is written in the envelope. But it can\nform hypotheses, and it can assign those hypotheses probabilities based on\ntheir priors and any available empirical data. For instance, the agent might\nhave encountered other examples of human-authored texts, or it might have\nobserved some general patterns of human behavior. This would enable it to make\nguesses. One does not need a degree in psychology to predict that the note is\nmore likely to describe a value such as “minimize injustice and unnecessary\nsuffering” or “maximize returns to shareholders” than a value such as “cover\nall lakes with plastic shopping bags.”\n\nWhen the agent makes a decision, it seeks to take actions that would be\neffective at realizing the values it believes are most likely to be described\nin the letter. Importantly, the agent would see a high instrumental value in\nlearning more about what the letter says. The reason is that for almost any\nfinal value that might be described in the letter, that value is more likely\nto be realized if the agent finds out what it is, since the agent will then\npursue that value more effectively. The agent would also discover the\nconvergent instrumental reasons described in [Chapter\n7](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_014.html#filepos373977)—goal system\nintegrity, cognitive enhancement, resource acquisition, and so forth. Yet,\nassuming that the agent assigns a sufficiently high probability to the values\ndescribed in the letter involving human welfare, it would _not_ pursue these\ninstrumental values by immediately turning the planet into computronium and\nthereby exterminating the human species, because doing so would risk\npermanently destroying its ability to realize its final value.\n\nWe can liken this kind of agent to a barge attached to several tugboats that\npull in different directions. Each tugboat corresponds to a hypothesis about\nthe agent’s final value. The engine power of each tugboat corresponds to the\nassociated hypothesis’s probability, and thus changes as new evidence comes\nin, producing adjustments in the barge’s direction of motion. The resultant\nforce should move the barge along a trajectory that facilitates learning about\nthe (implicit) final value while avoiding the shoals of irreversible\ndestruction; and later, when the open sea of more definite knowledge of the\nfinal value is reached, the one tugboat that still exerts significant force\nwill pull the barge toward the realization of the discovered value along the\nstraightest or most propitious route.\n\nThe envelope and barge metaphors illustrate the principle underlying the value\nlearning approach, but they pass over a number of critical technical issues.\nThey come into clearer focus once we start to develop the approach within a\nformal framework (see Box 10).\n\nOne outstanding issue is how to endow the AI with a goal such as “Maximize the\nrealization of the values described in the envelope.” (In the terminology of\nBox 10, how to define the value criterion.) To do this, it is necessary to\nidentify the place where the values are described. In our example, this\nrequires making a successful reference to the letter in the envelope. Though\nthis might seem trivial, it is not without pitfalls. To mention just one: it\nis critical that the reference be not simply to a particular external physical\nobject but to an object at a particular time. Otherwise the AI may determine\nthat the best way to attain its goal is by overwriting the original value\ndescription with one that provides an easier target (such as the value that\nfor every integer there be a larger integer). This done, the AI could lean\nback and crack its knuckles—though more likely a malignant failure would\nensue, for reasons we discussed in [Chapter\n8](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_015.html#filepos408438). So now we\nface the question of how to define time. We could point to a clock and say,\n“Time is defined by the movements of this device”—but this could fail if the\nAI conjectures that it can manipulate time by moving the hands on the clock, a\nconjecture which would indeed be correct if “time” were given the aforesaid\ndefinition. (In a realistic case, matters would be further complicated by the\nfact that the relevant values are not going to be conveniently described in a\nletter; more likely, they would have to be inferred from observations of pre-\nexisting structures that implicitly contain the relevant information, such as\nhuman brains.)\n\n* * *\n\n### **Box 10 Formalizing value learning**\n\nIntroducing some formal notation can help us see some things more clearly.\nHowever, readers who dislike formalism can skip this part.\n\nConsider a simplified framework in which an agent interacts with its\nenvironment in a finite number of discrete\ncycles.[13](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1082645)\nIn cycle _k_ , the agent performs action _y_ _k_ , and then receives the\npercept _x k_. The interaction history of an agent with lifespan _m_ is a\nstring _y_ 1 _x_ 1 _y_ 2 _x_ 2 … _y mxm_ (which we can abbreviate as _yx_\n1:_m_ or _yx_ ≤ _m_). In each cycle, the agent selects an action based on the\npercept sequence it has received to date.\n\nConsider first a reinforcement learner. An optimal reinforcement learner (AI-\nRL) is one that maximizes expected future rewards. It obeys the\nequation[14](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1083005)\n\n![Image](images/00032.jpg)\n\nThe reward sequence _r k_, …, _r m_ is implied by the percept sequence _x\nk:m_, since the reward that the agent receives in a given cycle is part of the\npercept that the agent receives in that cycle.\n\nAs argued earlier, this kind of reinforcement learning is unsuitable in the\npresent context because a sufficiently intelligent agent will realize that it\ncould secure maximum reward if it were able to directly manipulate its reward\nsignal (wireheading). For weak agents, this need not be a problem, since we\ncan physically prevent them from tampering with their own reward channel. We\ncan also control their environment so that they receive rewards only when they\nact in ways that are agreeable to us. But a reinforcement learner has a strong\nincentive to eliminate this artificial dependence of its rewards on our whims\nand wishes. Our relationship with a reinforcement learner is therefore\nfundamentally antagonistic. If the agent is strong, this spells danger.\n\nVariations of the wireheading syndrome can also affect systems that do not\nseek an external sensory reward signal but whose goals are defined as the\nattainment of some internal state. For example, in so-called “actor–critic”\nsystems, there is an actor module that selects actions in order to minimize\nthe disapproval of a separate critic module that computes how far the agent’s\nbehavior falls short of a given performance measure. The problem with this\nsetup is that the actor module may realize that it can minimize disapproval by\nmodifying the critic or eliminating it altogether—much like a dictator who\ndissolves the parliament and nationalizes the press. For limited systems, the\nproblem can be avoided simply by not giving the actor module any means of\nmodifying the critic module. A sufficiently intelligent and resourceful actor\nmodule, however, could always gain access to the critic module (which, after\nall, is merely a physical process in some\ncomputer).[15](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1083222)\n\nBefore we get to the value learner, let us consider as an intermediary step\nwhat has been called an observation-utility maximizer (AI-OUM). It is obtained\nby replacing the reward series (_r k_ \\+ … + _r m_) in the AI-RL with a\nutility function that is allowed to depend on the entire future interaction\nhistory of the AI:\n\n![Image](images/00033.jpg)\n\nThis formulation provides a way around the wireheading problem because a\nutility function defined over an entire interaction history could be designed\nto penalize interaction histories that show signs of self-deception (or of a\nfailure on the part of the agent to invest sufficiently in obtaining an\naccurate view of reality).\n\nThe AI-OUM thus makes it possible _in principle_ to circumvent the wireheading\nproblem. Availing ourselves of this possibility, however, would require that\nwe specify a suitable utility function over the class of possible interaction\nhistories—a task that looks forbiddingly difficult.\n\nIt may be more natural to specify utility functions directly in terms of\npossible worlds (or properties of possible worlds, or theories about the\nworld) rather than in terms of an agent’s own interaction histories. If we use\nthis approach, we could reformulate and simplify the AI-OUM optimality notion:\n\n![Image](images/00034.jpg)\n\nHere, _E_ is the total evidence available to the agent (at the time when it is\nmaking its decision), and _U_ is a utility function that assigns utility to\nsome class of possible worlds. The optimal agent chooses the act that\nmaximizes expected utility.\n\nAn outstanding problem with these formulations is the difficulty of defining\nthe utility function _U_. This, finally, returns us to the value-loading\nproblem. To enable the utility function to be learned, we must expand our\nformalism to allow for uncertainty over utility functions. This can be done as\nfollows (AI-\nVL):[16](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1084781)\n\n![Image](images/00035.jpg)\n\nHere, _ν_(.) is a function from utility functions to propositions about\nutility functions. _ν_(_U_) is the proposition that the utility function _U_\nsatisfies the _value criterion_ expressed by\n_ν_.[17](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1085501)\n\nTo decide which action to perform, one could hence proceed as follows: First,\ncompute the conditional probability of each possible world _w_ (given\navailable evidence and on the supposition that action _y_ is to be performed).\nSecond, for each possible utility function _U_ , compute the conditional\nprobability that _U_ satisfies the value criterion ν (conditional on _w_ being\nthe actual world). Third, for each possible utility function _U_ , compute the\nutility of possible world _w_. Fourth, combine these quantities to compute the\nexpected utility of action _y_. Fifth, repeat this procedure for each possible\naction, and perform the action found to have the highest expected utility\n(using some arbitrary method to break ties). As described, this\nprocedure—which involves giving explicit and separate consideration to each\npossible world—is, of course, wildly computationally intractable. The AI would\nhave to use computational shortcuts that approximate this optimality notion.\n\nThe question, then, is how to define this value criterion\n_ν_.[18](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1085959) Once\nthe AI has an adequate representation of the value criterion, it could in\nprinciple use its general intelligence to gather information about which\npossible worlds are most likely to be the actual one. It could then apply the\ncriterion, for each such plausible possible world _w_ , to find out which\nutility function satisfies the criterion in _w_. One can thus regard the AI-VL\nformula as a way of identifying and separating out this key challenge in the\nvalue learning approach—the challenge of how to represent _ν_. The formalism\nalso brings to light a number of other issues (such as how to define\n![Image](images/00036.jpg), ![Image](images/00037.jpg), and\n![Image](images/00038.jpg)) which would need to be resolved before the\napproach could be made to\nwork.[19](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1086425)\n\n* * *\n\nAnother issue in coding the goal “Maximize the realization of the values\ndescribed in the envelope” is that even if all the correct values were\ndescribed in a letter, and even if the AI’s motivation system were\nsuccessfully keyed to this source, the AI might not interpret the descriptions\nthe way we intended. This would create a risk of perverse instantiation, as\ndiscussed in [Chapter\n8](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_015.html#filepos408438).\n\nTo clarify, the difficulty here is not so much how to ensure that the AI can\nunderstand human intentions. A superintelligence should easily develop such\nunderstanding. Rather, the difficulty is ensuring that the AI will be\nmotivated to pursue the described values in the way we intended. This is not\nguaranteed by the AI’s ability to understand our intentions: an AI could know\nexactly what we meant and yet be indifferent to that interpretation of our\nwords (being motivated instead by some other interpretation of the words or\nbeing indifferent to our words altogether).\n\nThe difficulty is compounded by the desideratum that, for reasons of safety,\nthe correct motivation should ideally be installed in the seed AI _before_ it\nbecomes capable of fully representing human concepts or understanding human\nintentions. This requires that somehow a cognitive framework be created, with\na particular location in that framework designated in the AI’s motivation\nsystem as the repository of its final value. But the cognitive framework\nitself must be revisable, so as to allow the AI to expand its representational\ncapacities as it learns more about the world and grows more intelligent. The\nAI might undergo the equivalent of scientific revolutions, in which its\nworldview is shaken up and it perhaps suffers ontological crises in which it\ndiscovers that its previous ways of thinking about values were based on\nconfusions and illusions. Yet starting at a sub-human level of development and\ncontinuing throughout all its subsequent development into a galactic\nsuperintelligence, the AI’s conduct is to be guided by an essentially\nunchanging final value, a final value that becomes better understood by the AI\nin direct consequence of its general intellectual progress—and likely quite\ndifferently understood by the mature AI than it was by its original\nprogrammers, though not different in a random or hostile way but in a benignly\nappropriate way. How to accomplish this remains an open\nquestion.[20](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1095478)\n(See Box 11.)\n\nIn summary, it is not yet known how to use the value learning approach to\ninstall plausible human values (though see Box 12 for some examples of recent\nideas). At present, the approach should be viewed as a research program rather\nthan an available technique. If it could be made to work, it might constitute\nthe most ideal solution to the value-loading problem. Among other benefits, it\nwould seem to offer a natural way to prevent mind crime, since a seed AI that\nmakes reasonable guesses about which values its programmers might have\ninstalled would anticipate that mind crime is probably negatively evaluated by\nthose values, and thus best avoided, at least until more definitive\ninformation has been obtained.\n\nLast, but not least, there is the question of “what to write in the\nenvelope”—or, less metaphorically, the question of which values we should try\nto get the AI to learn. But this issue is common to all approaches to the AI\nvalue-loading problem. We return to it in [Chapter\n13](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_020.html#filepos727275).\n\n* * *\n\n### **Box 11 An AI that wants to be friendly**\n\nEliezer Yudkowsky has tried to describe some features of a seed AI\narchitecture intended to enable the kind of behavior described in the text\nabove. In his terminology, the AI would use “external reference\nsemantics.”[21](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1095846)\nTo illustrate the basic idea, let us suppose that we want the system to be\n“friendly.” The system starts out with the goal of trying to instantiate\nproperty _F_ but does not initially know much about what _F_ is. It might just\nknow that _F_ is some abstract property and that when the programmers speak of\n“friendliness,” they are probably trying to convey information about _F_.\nSince the AI’s final goal is to instantiate _F_ , an important instrumental\nvalue is to learn more about what _F_ is. As the AI discovers more about _F_ ,\nits behavior is increasingly guided by the actual content of _F_. Thus,\nhopefully, the AI becomes increasingly friendly the more it learns and the\nsmarter it gets.\n\nThe programmers can help this process along, and reduce the risk of the AI\nmaking some catastrophic mistake while its understanding of _F_ is still\nincomplete, by providing the AI with “programmer affirmations,” hypotheses\nabout the nature and content of _F_ to which an initially high probability is\nassigned. For instance, the hypothesis “misleading the programmers is\nunfriendly” can be given a high prior probability. These programmer\naffirmations, however, are not “true by definition”—they are not\nunchallengeable axioms about the concept of friendliness. Rather, they are\ninitial hypotheses about friendliness, hypotheses to which a rational AI will\nassign a high probability at least for as long as it trusts the programmers’\nepistemic capacities more than its own.\n\nYudkowsky’s proposal also involves the use of what he called “causal validity\nsemantics.” The idea here is that the AI should do not exactly what the\nprogrammers told it to do but rather (something like) what they were trying to\ntell it to do. While the programmers are trying to explain to the seed AI what\nfriendliness is, they might make errors in their explanations. Moreover, the\nprogrammers themselves may not fully understand the true nature of\nfriendliness. One would therefore want the AI to have the ability to correct\nerrors in the programmers’ thinking, and to infer the true or intended meaning\nfrom whatever imperfect explanations the programmers manage to provide. For\nexample, the AI should be able to represent the causal processes whereby the\nprogrammers learn and communicate about friendliness. Thus, to pick a trivial\nexample, the AI should understand that there is a possibility that a\nprogrammer might make a typo while inputting information about friendliness,\nand the AI should then seek to correct the error. More generally, the AI\nshould seek to correct for whatever distortive influences may have corrupted\nthe flow of information about friendliness as it passed from its source\nthrough the programmers to the AI (where “distortive” is an epistemic\ncategory). Ideally, as the AI matures, it should overcome any cognitive biases\nand other more fundamental misconceptions that may have prevented its\nprogrammers from fully understanding what friendliness is.\n\n* * *\n\n* * *\n\n### **Box 12 Two recent (half-baked) ideas**\n\nWhat we might call the “Hail Mary” approach is based on the hope that\nelsewhere in the universe there exist (or will come to exist) civilizations\nthat successfully manage the intelligence explosion, and that they end up with\nvalues that significantly overlap with our own. We could then try to build our\nAI so that it is motivated to do what these other superintelligences want it\nto do.[22](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1095959)\nThe advantage is that this might be easier than to build our AI to be\nmotivated to do what we want directly.\n\nFor this scheme to work it is _not_ necessary that our AI can establish\ncommunication with any alien superintelligence. Rather, our AI’s actions would\nbe guided by _its estimates_ of what the alien superintelligences would want\nit to do. Our AI would model the likely outcomes of intelligence explosions\nelsewhere, and as it becomes superintelligent itself its estimates should\nbecome increasingly accurate. Perfect knowledge is not required. There may be\na range of plausible outcomes of intelligence explosions, and our AI would\nthen do its best to accommodate the preferences of the various different kinds\nof superintelligence that might emerge, weighted by probability.\n\nThis version of the Hail Mary approach requires that we construct a final\nvalue for our AI that refers to the preferences of other superintelligences.\nExactly how to do this is not yet clear. However, superintelligent agents\nmight be structurally distinctive enough that we could write a piece of code\nthat would function as a detector that would look at the world model in our\ndeveloping AI and designate the representational elements that correspond to\nthe presence of a superintelligence. The detector would then, somehow, extract\nthe preferences of the superintelligence in question (as it is represented\nwithin our own\nAI).[23](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1096318) If\nwe could create such a detector, we could then use it to define our AI’s final\nvalues. One challenge is that we may need to create the detector before we\nknow what representational framework our AI will develop. The detector may\nthus need to query an unknown representational framework and extract the\npreferences of whatever superintelligence may be represented therein. This\nlooks difficult, but perhaps some clever solution can be\nfound.[24](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1097351)\n\nIf the basic setup could be made to work, various refinements immediately\nsuggest themselves. For example, rather than aiming to follow (some weighted\ncomposition of) the preferences of _every_ alien superintelligence, our AI’s\nfinal value could incorporate a filter to select a subset of alien\nsuperintelligences for obeisance (with the aim of selecting ones whose values\nare closer to our own). For instance, we might use criteria pertaining to a\nsuperintelligence’s causal origin to determine whether to include it in the\nobeisance set. Certain properties of its origination (which we might be able\nto define in structural terms) may correlate with the degree to which the\nresultant superintelligence could be expected to share our values. Perhaps we\nwish to place more trust in superintelligences whose causal origins trace back\nto a whole brain emulation, or to a seed AI that did not make heavy use of\nevolutionary algorithms or that emerged slowly in a way suggestive of a\ncontrolled takeoff. (Taking causal origins into account would also let us\navoid over-weighting superintelligences that create multiple copies of\nthemselves—indeed would let us avoid creating an incentive for them to do so.)\nMany other refinements would also be possible.\n\nThe Hail Mary approach requires faith that there are other superintelligences\nout there that sufficiently share our\nvalues.[25](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1099198)\nThis makes the approach non-ideal. However, the technical obstacles facing the\nHail Mary approach, though very substantial, might possibly be less formidable\nthan those confronting alternative approaches. Exploring non-ideal but more\neasily implementable approaches can make sense—not with the intention of using\nthem, but to have something to fall back upon in case an ideal solution should\nnot be ready in time.\n\nAnother idea for how to solve the value-loading problem has recently been\nproposed by Paul\nChristiano.[26](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1099443)\nLike the Hail Mary, it is a value learning method that tries to define the\nvalue criterion by means of a “trick” rather than through laborious\nconstruction. By contrast to the Hail Mary, it does not presuppose the\nexistence of other superintelligent agents that we could point to as role\nmodels for our own AI. Christiano’s proposal is somewhat resistant to brief\nexplanation—it involves a series of arcane considerations—but we can try to at\nleast gesture at its main elements.\n\nSuppose we could obtain (a) a mathematically precise specification of a\nparticular human brain and (b) a mathematically well-specified virtual\nenvironment that contains an idealized computer with an arbitrarily large\namount of memory and CPU power. Given (a) and (b), we could define a utility\nfunction _U_ as the output the human brain would produce after interacting\nwith this environment. _U_ would be a mathematically well-defined object,\nalbeit one which (because of computational limitations) we may be unable to\ndescribe _explicitly_. Nevertheless, _U_ could serve as the value criterion\nfor a value learning AI, which could use various heuristics for assigning\nprobabilities to hypotheses about what _U_ implies.\n\nIntuitively, we want _U_ to be the utility function that a suitably prepared\nhuman would output if she had the advantage of being able use an arbitrarily\nlarge amount of computing power—enough computing power, for example, to run\nastronomical numbers of copies of herself to assist her with her analysis of\nspecifying a utility function, or to help her devise a better process for\ngoing about this analysis. (We are here foreshadowing a theme, “coherent\nextrapolated volition,” which will be further explored in [Chapter\n13](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_020.html#filepos727275).)\n\nIt would seem relatively easy to specify the idealized environment: we can\ngive a mathematical description of an abstract computer with arbitrarily large\ncapacity; and in other respects we could use a virtual reality program that\ngives a mathematical description of, say, a single room with a computer\nterminal in it (instantiating the abstract computer). But how to obtain a\nmathematically precise description of a particular human brain? The obvious\nway would be through whole brain emulation, but what if the technology for\nemulation is not available in time?\n\nThis is where Christiano’s proposal offers a key innovation. Christiano\nobserves that in order to obtain a mathematically well-specified value\ncriterion, we do not need a practically useful computational model of a mind,\na model we could run. We just need a (possibly implicit and hopelessly\ncomplicated) mathematical _definition_ —and this may be much easier to attain.\nUsing functional neuroimaging and other measurements, we can perhaps collect\ngigabytes of data about the input–output behavior of a selected human. If we\ncollect a sufficient amount of data, then it might be that the simplest\nmathematical model that accounts for all this data is in fact an emulation of\nthe particular human in question. Although it would be computationally\nintractable for us to _find_ this simplest model from the data, it could be\nperfectly possible for us to _define_ the model, by referring to the data and\na using a mathematically well-defined simplicity measure (such as some variant\nof the Kolmogorov complexity, which we encountered in [Box\n1](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos65127), [Chapter\n1](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos33916)).[27](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1099557)\n\n* * *\n\n### [Emulation\nmodulation](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_006.html#filepos25062)\n\nThe value-loading problem looks somewhat different for whole brain emulation\nthan it does for artificial intelligence. Methods that presuppose a fine-\ngrained understanding and control of algorithms and architecture are not\napplicable to emulations. On the other hand, the augmentation motivation\nselection method—inapplicable to de novo artificial intelligence—is available\nto be used with emulations (or enhanced biological\nbrains).[28](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1099903)\n\nThe augmentation method could be combined with techniques to tweak the\ninherited goals of the system. For example, one could try to manipulate the\nmotivational state of an emulation by administering the digital equivalent of\npsychoactive substances (or, in the case of biological systems, the actual\nchemicals). Even now it is possible to pharmacologically manipulate values and\nmotivations to a limited\nextent.[29](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1100077)\nThe pharmacopeia of the future may contain drugs with more specific and\npredictable effects. The digital medium of emulations should greatly\nfacilitate such developments, by making controlled experimentation easier and\nby rendering all cerebral parts directly addressable.\n\nJust as when biological test subjects are used, research on emulations would\nget entangled in ethical complications, not all of which could be brushed\naside with a consent form. Such entanglements could slow progress along the\nemulation path (because of regulation or moral restraint), perhaps especially\nhindering studies on how to manipulate the motivational structure of\nemulations. The result could be that emulations are augmented to potentially\ndangerous superintelligent levels of cognitive ability before adequate work\nhas been done to test or adjust their final goals. Another possible effect of\nthe moral entanglements might be to give the lead to less scrupulous teams and\nnations. Conversely, were we to relax our moral standards for experimenting\nwith digital human minds, we could become responsible for a substantial amount\nof harm and wrongdoing, which is obviously undesirable. Other things equal,\nthese considerations favor taking some alternative path that does not require\nthe extensive use of digital human research subjects in a strategically high-\nstakes situation.\n\nThe issue, however, is not clear-cut. One could argue that whole brain\nemulation research is _less_ likely to involve moral violations than\nartificial intelligence research, on the grounds that we are more likely to\nrecognize when an emulation mind qualifies for moral status than we are to\nrecognize when a completely alien or synthetic mind does so. If certain kinds\nof AIs, or their subprocesses, have a significant moral status that we fail to\nrecognize, the consequent moral violations could be extensive. Consider, for\nexample, the happy abandon with which contemporary programmers create\nreinforcement-learning agents and subject them to aversive stimuli. Countless\nsuch agents are created daily, not only in computer science laboratories but\nin many applications, including some computer games containing sophisticated\nnon-player characters. Presumably, these agents are still too primitive to\nhave any moral status. But how confident can we really be that this is so?\nMore importantly, how confident can we be that we will know to stop in time,\nbefore our programs become capable of experiencing morally relevant suffering?\n\n(We will return in [Chapter\n14](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos795733) to some of\nthe broader strategic questions that arise when we compare the desirability of\nemulation and artificial intelligence paths.)\n\n### [Institution\ndesign](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_006.html#filepos25159)\n\nSome intelligent systems consist of intelligent parts that are themselves\ncapable of agency. Firms and states exemplify this in the human world: whilst\nlargely composed of humans they can, for some purposes, be viewed as\nautonomous agents in their own right. The motivations of such composite\nsystems depend not only on the motivations of their constituent subagents but\nalso on how those subagents are organized. For instance, a group that is\norganized under strong dictatorship might behave as if it had a will that was\nidentical to the will of the subagent that occupies the dictator role, whereas\na democratic group might sometimes behave more as if it had a will that was a\ncomposite or average of the wills of its various constituents. But one can\nalso imagine governance institutions that would make an organization behave in\na way that is not a simple function of the wills of its subagents.\n(Theoretically, at least, there could exist a totalitarian state that\n_everybody_ hated, because the state had mechanisms to prevent its citizens\nfrom coordinating a revolt. Each citizen could be worse off by revolting alone\nthan by playing their part in the state machinery.)\n\nBy designing appropriate institutions for a composite system, one could thus\ntry to shape its effective motivation. In [Chapter\n9](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_016.html#filepos449483), we\ndiscussed social integration as a possible capability control method. But\nthere we focused on the incentives faced by an agent as a consequence of its\nexistence in a social world of near-equals. Here we are focusing on what\nhappens _inside_ a given agent: how its will is determined by its internal\norganization. We are therefore looking at a motivation selection method.\nMoreover, since this kind of internal institution design does not depend on\nlarge-scale social engineering or reform, it is a method that might be\navailable to an individual project developing superintelligence even if the\nwider socioeconomic or international milieu is less than ideally favorable.\n\nInstitution design is perhaps most plausible in contexts where it would be\ncombined with augmentation. If we could start with agents that are already\nsuitably motivated or that have human-like motivations, institutional\narrangements could be used as an extra safeguard to increase the chances that\nthe system will stay on course.\n\nFor example, suppose that we start with some well-motivated human-like\nagents—let us say emulations. We want to boost the cognitive capacities of\nthese agents, but we worry that the enhancements might corrupt their\nmotivations. One way to deal with this challenge would be to set up a system\nin which individual emulations function as subagents. When a new enhancement\nis introduced, it is first applied to a small subset of the subagents. Its\neffects are then studied by a review panel composed of subagents who have not\nyet had the enhancement applied to them. Only when these peers have satisfied\nthemselves that the enhancement is not corrupting is it rolled out to the\nwider subagent population. If the enhanced subagents are found to be\ncorrupted, they are not given further enhancements and are excluded from key\ndecision-making functions (at least until the system as a whole has advanced\nto a point where the corrupted subagents can be safely\nreintegrated).[30](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1100376)\nAlthough the corrupted subagents might have gained some advantage from the\nenhancement, the institutional structure in which they are embedded, and the\nfact that they constitute a small minority of all subagents, would hopefully\nprevent them from seizing power or spreading their corruption to the overall\nsystem. Thus, the collective intelligence and capability of the system could\nbe gradually enhanced in a sequence of small steps, where the soundness of\neach step is verified by subagents only slightly less capable than the new\nsubagents that are introduced in that step.\n\nThe amount of safety obtainable by this kind of institution design is limited\nby the accuracy of the tests that are used to evaluate the enhanced subagents.\nThe problem of test construction is complicated by the fact that the test\nbatteries could only be validated within the pre-existing range of cognitive\nability yet must be applied to agents outside this range. One could mitigate\nthis problem by taking small enhancement steps and by letting the test run for\na long time. Such caution, however, would raise the cost and slow progress\n(which, if a race dynamic is occurring, could mean a project employing these\nsafety measures would place itself at a disadvantage). Even with small steps\nand extensive testing, there would remain some risk of subtle flaws being\nintroduced, leading to a gradual corruption of the goal system, a corruption\nthat might become manifest only when the system reaches superintelligence.\nThis risk could be reduced by letting the testing condition extend for many\nsteps of cognitive enhancement, so that even agents that have been initially\ncertified as sound and that have received several subsequent enhancements\nwould continue to be under review by a panel of unmodified and uncorrupted\noriginal peers and would remain subject to reversion.\n\nIdeally, the testing should be ongoing rather than confined to an initial\nevaluation period. Human-level subagents have the ability to strategize and\nmight thus choose to conceal certain goals while their behavior was being\nmonitored. To enable continuous testing, a hierarchy might be created in which\nsubagents at a certain level of capability are tasked with monitoring\nsubagents at a somewhat higher level of capability. At the bottom of the\ncapability hierarchy (but at the top of the power hierarchy) would sit the\nrelatively dumb and slow principal. This human principal would be like a\ndemented king who reigns over an incompetent court that oversees a mediocre\nadministration which governs a capable people. Alternatively, he might be\nlikened to the evolutionarily ancient “reptilian brain” which ensures that the\nnewer and much cleverer neocortex is employed in the service of goals such as\nfeeding and copulation.\n\nOne might question whether such an inverse meritocracy could remain stable. If\nthe bulk of the superintelligent subagents were found to be corrupt, and the\nhumans in charge of the whole thing attempted to roll back many of the\npreceding enhancements, would the institutional structure be strong enough to\nwithstand the opposition that such a move would engender? In the context of\nhistorical or contemporary human societies, the answer perhaps is that this\nwould be implausible. (Not that the most capable people consistently rise to\npower, but we are not _literally_ ruled by chimpanzees either.) The issue is\nless clear, however, in a context where society is implemented as\nsoftware.[31](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1100792)\n\nImagine a digital hierarchy that has one highly trained police agent for every\ntwo proletarians. It also has one special-police agent for every two police\nagents. It also has one special-special-police agent for every two special-\npolice agents—and so forth, so that each layer of overseers has at least half\nthe numbers of the layer it oversees. Imagine, further, that this society is\narchitected in such a way that a supervisor has big advantages over his\nsubordinates. For example, the supervisor is able to monitor and record\neverything his subordinates say and do, whereas the supervisor himself is\nhidden behind a one-way glass, so that his subordinates can only hear the\norders and instructions that the superior chooses to transmit. Imagine that\nsupervisors have a panel of buttons that they can press at any time to cause a\nsubordinate to be punished or rewarded, paused, deleted, or reverted to an\nearlier state. These capabilities would greatly boost the powers of a\nsupervising agent, beyond what has been possible historically in even the most\ntotalitarian of organizations.\n\nEven this does not exhaust the possible instruments of control. Supervisors\nmight also have at their disposal realistic virtual reality simulations that\nthey can use to probe subordinates’ reactions to hypothetical scenarios (such\nas simulated offers to join a conspiracy or to slack when they think nobody is\nwatching). Supervisors might, additionally, have the ability to eavesdrop on\ninternal monologues in the subordinates’ minds and to directly manipulate\ntheir emotional states.\n\nThe upshot is a series of massive advantages for the\nsupervisor.[32](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1101208)\nEach supervisor would be a subordinate to another, higher-level supervisor,\nwho would monitor _his_ every move and make sure _he_ did not slacken in the\nexecution of his managerial duties. With this kind of arrangement, it might be\npossible for the structure to be stable even if each supervisory level is\nintellectually somewhat weaker than the level it\nsupervises.[33](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1101345)\nAnd the entire structure, which could include many highly superintelligent\nagents, may be controlled by a small number of humans with root\naccess.[34](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1101980)\n\nThe cost of such an internal control structure would be significant but not\nnecessarily prohibitive. For example, in the model where each level of\nsupervision has half the numbers of the layer below, the extra computational\noverhead is bounded at a mere 100% of what the proletarian part of the system\ncosts—less if the dumber boss layers require fewer computations per subagent.\nOversight could be tightened by increasing the ratio, or cost lowered by\ndecreasing it. The extra computational cost would be a major factor in a\ncompetitive market scenario (cf. [Chapter\n11](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_018.html#filepos555025)) but looks\naffordable in the context of a project that is not facing strong immediate\ncompetition. There would also be a cost in terms of the development time\nneeded to create and test these supervisory functionalities. A well-resourced\nproject could reduce this time cost by parallelizing the development of the\ncontrol structure with the development of the machine intelligence; but the\nextra task load could be prohibitive for smaller projects and for projects\ncaught in a close technology race.\n\nOne other type of cost also deserves consideration: the risk of mind crimes\nbeing committed in this kind of\nstructure.[35](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1102540)\nAs described, the institution sounds like a rather horrible North Korean labor\ncamp. Yet there are ways of at least mitigating the moral problems with\nrunning this kind of institution, even if the subagents contained in the\ninstitution are emulations with full human moral status. At a minimum, the\nsystem could rely on volunteering emulations. Each subagent could have the\noption at any time of withdrawing its\nparticipation.[36](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1102837)\nTerminated emulations could be stored to memory, with a commitment to restart\nthem under much more ideal conditions once the dangerous phase of the\nintelligence explosion is over. Meanwhile, subagents who chose to participate\ncould be housed in very comfortable virtual environments and allowed ample\ntime for sleep and recreation. These measures would impose a cost, one that\nshould be manageable for a well-resourced project under noncompetitive\nconditions. In a highly competitive situation, the cost may be unaffordable\nunless an enterprise could be assured that its competitors would incur the\nsame cost.\n\nIn the example, we imagined the subagents as emulations. One might wonder,\ndoes the institution design approach require that the subagents be\nanthropomorphic? Or is it equally applicable to systems composed of artificial\nsubagents?\n\nOne’s first thought here might be skeptical. One notes that despite our\nplentiful experience with human-like agents, we still cannot precisely predict\nthe outbreak or outcomes of revolutions; social science can, at most, describe\nsome statistical\ntendencies.[37](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1103269)\nSince we cannot reliably predict the stability of social structures for\nordinary human beings (about which we have much data), it is tempting to infer\nthat we have little hope of precision-engineering stable social structures for\ncognitively enhanced human-like agents (about which we have no data), and that\nwe have still less hope of doing so for advanced artificial agents (which are\nnot even similar to agents that we have data about).\n\nYet the matter is not so cut-and-dried. Humans and human-like beings are\ncomplex; but artificial agents could have relatively simple architectures.\nArtificial agents could also have simple and explicitly characterized\nmotivations. Furthermore, digital agents in general (whether emulations or\nartificial intelligences) are copyable: an affordance that may revolutionize\nmanagement, much like interchangeable parts revolutionized manufacturing.\nThese differences, together with the opportunity to work with agents that are\ninitially powerless and to create institutional structures that use the\nvarious abovementioned control measures, might combine to make it possible to\nachieve particular institutional outcomes—such as a system that does not\nrevolt—more reliably than if one were working with human beings under\nhistorical conditions.\n\nBut then again, artificial agents might lack many of the attributes that help\nus predict the behavior of human-like agents. Artificial agents need not have\nany of the social emotions that bind human behavior, emotions such as fear,\npride, and remorse. Nor need artificial agents develop attachments to friends\nand family. Nor need they exhibit the unconscious body language that makes it\ndifficult for us humans to conceal our intentions. These deficits might\ndestabilize institutions of artificial agents. Moreover, artificial agents\nmight be capable of making big leaps in cognitive performance as a result of\nseemingly small changes in their algorithms or architecture. Ruthlessly\noptimizing artificial agents might be willing to take extreme gambles from\nwhich humans would\nshrink.[38](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1103690)\nAnd superintelligent agents might show a surprising ability to coordinate with\nlittle or no communication (e.g. by internally modeling each other’s\nhypothetical responses to various contingencies). These and other differences\ncould make sudden institutional failure more likely, even in the teeth of what\nseem like Kevlar-clad methods of social control.\n\nIt is unclear, therefore, how promising the institution design approach is,\nand whether it has a greater chance of working with anthropomorphic than with\nartificial agents. It might be thought that creating an institution with\nappropriate checks and balances could only increase safety—or, at any rate,\nnot reduce safety—so that from a risk-mitigation perspective it would always\nbe best if the method were used. But even this cannot be said with certainty.\nThe approach adds parts and complexity, and thus may also introduce new ways\nfor things to go wrong that do not exist in the case of an agent that does not\nhave intelligent subagents as parts. Nevertheless, institution design is\nworthy of further\nexploration.[39](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1103813)\n\n### [Synopsis](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_006.html#filepos25254)\n\nGoal system engineering is not yet an established discipline. It is not\ncurrently known how to transfer human values to a digital computer, even given\nhuman-level machine intelligence. Having investigated a number of approaches,\nwe found that some of them appear to be dead ends; but others appear to hold\npromise and deserve to be explored further. A summary is provided in Table 12.\n\n**Table 12 _Summary of value-loading techniques_**\n\n|  \n---|---  \nExplicit representation | May hold promise as a way of loading domesticity values. Does not seem promising as a way of loading more complex values.  \nEvolutionary selection | Less promising. Powerful search may find a design that satisfies the formal search criteria but not our intentions. Furthermore, if designs are evaluated by running them—including designs that do not even meet the formal criteria—a potentially grave additional danger is created. Evolution also makes it difficult to avoid massive mind crime, especially if one is aiming to fashion human-like minds.  \nReinforcement learning | A range of different methods can be used to solve “reinforcement-learning problems,” but they typically involve creating a system that seeks to maximize a reward signal. This has an inherent tendency to produce the wireheading failure mode when the system becomes more intelligent. Reinforcement learning therefore looks unpromising.  \nValue accretion | We humans acquire much of our specific goal content from our reactions to experience. While value accretion could in principle be used to create an agent with human motivations, the human value-accretion dispositions might be complex and difficult to replicate in a seed AI. A bad approximation may yield an AI that generalizes differently than humans do and therefore acquires unintended final goals. More research is needed to determine how difficult it would be to make value accretion work with sufficient precision.  \nMotivational scaffolding | It is too early to tell how difficult it would be to encourage a system to develop internal high-level representations that are transparent to humans (while keeping the system’s capabilities below the dangerous level) and then to use those representations to design a new goal system. The approach might hold considerable promise. (However, as with any untested approach that would postpone much of the hard work on safety engineering until the development of human-level AI, one should be careful not to allow it to become an excuse for a lackadaisical attitude to the control problem in the interim.)  \nValue learning | A potentially promising approach, but more research is needed to determine how difficult it would be to formally specify a reference that successfully points to the relevant external information about human value (and how difficult it would be to specify a correctness criterion for a utility function in terms of such a reference). Also worth exploring within the value learning category are proposals of the Hail Mary type or along the lines of Paul Christiano’s construction (or other such shortcuts).  \nEmulation modulation | If machine intelligence is achieved via the emulation pathway, it would likely be possible to tweak motivations through the digital equivalent of drugs or by other means. Whether this would enable values to be loaded with sufficient precision to ensure safety even as the emulation is boosted to superintelligence is an open question. (Ethical constraints might also complicate developments in this direction.)  \nInstitution design | Various strong methods of social control could be applied in an institution composed of emulations. In principle, social control methods could also be applied in an institution composed of artificial intelligences. Emulations have some properties that would make them easier to control via such methods, but also some properties that might make them harder to control than AIs. Institution design seems worthy of further exploration as a potential value-loading technique.  \n  \nIf we knew how to solve the value-loading problem, we would confront a further\nproblem: the problem of deciding which values to load. What, in other words,\nwould we want a superintelligence to want? This is the more philosophical\nproblem to which we turn next.\n\n\n## [CHAPTER 13  \nChoosing the criteria for\nchoosing](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_006.html#filepos25352)\n\n**Suppose we could install any arbitrary final value into a seed AI. The\ndecision as to which value to install could then have the most far-reaching\nconsequences. Certain other basic parameter choices—concerning the axioms of\nthe AI’s decision theory and epistemology—could be similarly consequential.\nBut foolish, ignorant, and narrow-minded that we are, how could we be trusted\nto make good design decisions? How could we choose without locking in forever\nthe prejudices and preconceptions of the present generation? In this chapter,\nwe explore how indirect normativity can let us offload much of the cognitive\nwork involved in making these decisions onto the superintelligence itself\nwhile still anchoring the outcome in deeper human values.**\n\n### [The need for indirect\nnormativity](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_006.html#filepos25501)\n\nHow can we get a superintelligence to do what we want? What do we want the\nsuperintelligence to want? Up to this point, we have focused on the former\nquestion. We now turn to the second question.\n\nSuppose that we had solved the control problem so that we were able to load\nany value we chose into the motivation system of a superintelligence, making\nit pursue that value as its final goal. Which value should we install? The\nchoice is no light matter. If the superintelligence obtains a decisive\nstrategic advantage, the value would determine the disposition of the cosmic\nendowment.\n\nClearly, it is essential that we not make a mistake in our value selection.\nBut how could we realistically hope to achieve errorlessness in a matter like\nthis? We might be wrong about morality; wrong also about what is good for us;\nwrong even about what we truly want. Specifying a final goal, it seems,\nrequires making one’s way through a thicket of thorny philosophical problems.\nIf we try a direct approach, we are likely to make a hash of things. The risk\nof mistaken choosing is especially high when the decision context is\nunfamiliar—and selecting the final goal for a machine superintelligence that\nwill shape all of humanity’s future is an extremely unfamiliar decision\ncontext if any is.\n\nThe dismal odds in a frontal assault are reflected in the pervasive dissensus\nabout the relevant issues in value theory. No ethical theory commands majority\nsupport among philosophers, so most philosophers must be\nwrong.[1](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1104743) It\nis also reflected in the marked changes that the distribution of moral belief\nhas undergone over time, many of which we like to think of as progress. In\nmedieval Europe, for instance, it was deemed respectable entertainment to\nwatch a political prisoner being tortured to death. Cat-burning remained\npopular in sixteenth-century\nParis.[2](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1105306) A\nmere hundred and fifty years ago, slavery still was widely practiced in the\nAmerican South, with full support of the law and moral custom. When we look\nback, we see glaring deficiencies not just in the behavior but in the moral\nbeliefs of all previous ages. Though we have perhaps since gleaned some moral\ninsight, we could hardly claim to be now basking in the high noon of perfect\nmoral enlightenment. Very likely, we are still laboring under one or more\ngrave moral misconceptions. In such circumstances to select a final value\nbased on our current convictions, in a way that locks it in forever and\nprecludes any possibility of further ethical progress, would be to risk an\nexistential moral calamity.\n\nEven if we could be rationally confident that we have identified the correct\nethical theory—which we cannot be—we would still remain at risk of making\nmistakes in developing important details of this theory. Seemingly simple\nmoral theories can have a lot of hidden\ncomplexity.[3](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1105414)\nFor example, consider the (unusually simple) consequentialist theory of\nhedonism. This theory states, roughly, that all and only pleasure has value,\nand all and only pain has\ndisvalue.[4](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1105566)\nEven if we placed all our moral chips on this one theory, and the theory\nturned out to be right, a great many questions would remain open. Should\n“higher pleasures” be given priority over “lower pleasures,” as John Stuart\nMill argued? How should the intensity and duration of a pleasure be factored\nin? Can pains and pleasures cancel each other out? What kinds of brain states\nare associated with morally relevant pleasures? Would two exact copies of the\nsame brain state correspond to twice the amount of\npleasure?[5](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1105673)\nCan there be subconscious pleasures? How should we deal with extremely small\nchances of extremely great\npleasures?[6](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1105783)\nHow should we aggregate over infinite\npopulations?[7](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1105893)\n\nGiving the wrong answer to any one of these questions could be catastrophic.\nIf by selecting a final value for the superintelligence we had to place a bet\nnot just on a general moral theory but on a long conjunction of specific\nclaims about how that theory is to be interpreted and integrated into an\neffective decision-making process, then our chances of striking lucky would\ndwindle to something close to hopeless. Fools might eagerly accept this\nchallenge of solving in one swing all the important problems in moral\nphilosophy, in order to infix their favorite answers into the seed AI. Wiser\nsouls would look hard for some alternative approach, some way to hedge.\n\nThis takes us to indirect normativity. The obvious reason for building a\nsuperintelligence is so that we can offload to it the instrumental reasoning\nrequired to find effective ways of realizing a given value. Indirect\nnormativity would enable us also to offload to the superintelligence some of\nthe reasoning needed to select the value that is to be realized.\n\nIndirect normativity is a way to answer the challenge presented by the fact\nthat we may not know what we truly want, what is in our interest, or what is\nmorally right or ideal. Instead of making a guess based on our own current\nunderstanding (which is probably deeply flawed), we would delegate some of the\ncognitive work required for value selection to the superintelligence. Since\nthe superintelligence is better at cognitive work than we are, it may see past\nthe errors and confusions that cloud our thinking. One could generalize this\nidea and emboss it as a heuristic principle:\n\n> The principle of epistemic deference\n\n> A future superintelligence occupies an epistemically superior vantage point:\n> its beliefs are (probably, on most topics) more likely than ours to be true.\n> We should therefore defer to the superintelligence’s opinion whenever\n> feasible.[8](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1106003)\n\nIndirect normativity applies this principle to the value-selection problem.\nLacking confidence in our ability to specify a concrete normative standard, we\nwould instead specify some more abstract condition that any normative standard\nshould satisfy, in the hope that a superintelligence could find a concrete\nstandard that satisfies the abstract condition. We could give a seed AI the\nfinal goal of continuously acting according to its best estimate of what this\nimplicitly defined standard would have it do.\n\nSome examples will serve to make the idea clearer. First we will consider\n“coherent extrapolated volition,” an indirect normativity proposal outlined by\nEliezer Yudkowsky. We will then introduce some variations and alternatives, to\ngive us a sense of the range of available options.\n\n### [Coherent extrapolated\nvolition](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_006.html#filepos25611)\n\nYudkowsky has proposed that a seed AI be given the final goal of carrying out\nhumanity’s “coherent extrapolated volition” (CEV), which he defines as\nfollows:\n\n> Our coherent extrapolated volition is our wish if we knew more, thought\n> faster, were more the people we wished we were, had grown up farther\n> together; where the extrapolation converges rather than diverges, where our\n> wishes cohere rather than interfere; extrapolated as we wish that\n> extrapolated, interpreted as we wish that\n> interpreted.[9](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1106962)\n\nWhen Yudkowsky wrote this, he did not purport to present a blueprint for how\nto implement this rather poetic prescription. His aim was to give a\npreliminary sketch of how CEV might be defined, along with some arguments for\nwhy an approach along these lines is needed.\n\nMany of the ideas behind the CEV proposal have analogs and antecedents in the\nphilosophical literature. For example, in ethics _ideal observer theories_\nseek to analyze normative concepts like “good” or “right” in terms of the\njudgments that a hypothetical ideal observer would make (where an “ideal\nobserver” is defined as one that is omniscient about non-moral facts, is\nlogically clear-sighted, is impartial in relevant ways and is free from\nvarious kinds of biases, and so\non).[10](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1107096) The\nCEV approach, however, is not (or need not be construed as) a moral theory. It\nis not committed to the claim that there is any necessary link between value\nand the preferences of our coherent extrapolated volition. CEV can be thought\nof simply as a useful way to approximate whatever has ultimate value, or it\ncan be considered aside from any connection to ethics. As the main prototype\nof the indirect normativity approach, it is worth examining in a little more\ndetail.\n\n#### [Some\nexplications](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_006.html#filepos25718)\n\nSome terms in the above quotation require explication. “Thought faster,” in\nYudkowsky’s terminology, means _if we were smarter and had thought things\nthrough more_. “Grown up farther together” seems to mean _if we had done our\nlearning, our cognitive enhancing, and our self-improving under conditions of\nsuitable social interaction with one another_.\n\n“Where the extrapolation converges rather than diverges” may be understood as\nfollows. The AI should act on some feature of the result of its extrapolation\nonly insofar as that feature can be predicted by the AI with a fairly high\ndegree of confidence. To the extent that the AI cannot predict what we would\nwish if we were idealized in the manner indicated, the AI should not act on a\nwild guess; instead, it should refrain from acting. However, even though many\ndetails of our idealized wishing may be undetermined or unpredictable, there\nmight nevertheless be some broad outlines that the AI can apprehend, and it\ncan then at least act to ensure that the future course of events unfolds\nwithin those outlines. For example, if the AI can reliably estimate that our\nextrapolated volition would wish that we not all be in constant agony, or that\nthe universe not be tiled over with paperclips, then the AI should act to\nprevent those\noutcomes.[11](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1108061)\n\n“Where our wishes cohere rather than interfere” may be read as follows. The AI\nshould act where there is fairly broad agreement between individual humans’\nextrapolated volitions. A smaller set of strong, clear wishes might sometimes\noutweigh the weak and muddled wishes of a majority. Also, Yudkowsky thinks\nthat it should require less consensus for the AI to _prevent_ some particular\nnarrowly specified outcome, and more consensus for the AI to act to funnel the\nfuture into some particular narrow conception of the good. “The initial\ndynamic for CEV,” he writes, “should be conservative about saying ‘yes,’ and\nlisten carefully for\n‘no.’”[12](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1108445)\n\n“Extrapolated as we wish that extrapolated, interpreted as we wish that\ninterpreted”: The idea behind these last modifiers seems to be that the rules\nfor extrapolation should themselves be sensitive to the extrapolated volition.\nAn individual might have a second-order desire (a desire concerning what to\ndesire) that some of her first-order desires not be given weight when her\nvolition is extrapolated. For example, an alcoholic who has a first-order\ndesire for booze might also have a second-order desire not to have that first-\norder desire. Similarly, we might have desires over how various other parts of\nthe extrapolation process should unfold, and these should be taken into\naccount by the extrapolation process.\n\nIt might be objected that even if the concept of humanity’s coherent\nextrapolated volition could be properly defined, it would anyway be\nimpossible—even for a superintelligence—to find out what humanity would\nactually want under the hypothetical idealized circumstances stipulated in the\nCEV approach. Without some information about the content of our extrapolated\nvolition, the AI would be bereft of any substantial standard to guide its\nbehavior. However, although it would be difficult to know with precision what\nhumanity’s CEV would wish, it is possible to make informed guesses. This is\npossible even today, without superintelligence. For example, it is more\nplausible that our CEV would wish for there to be people in the future who\nlive rich and happy lives than that it would wish that we should all sit on\nstools in a dark room experiencing pain. If _we_ can make at least some such\njudgments sensibly, so can a superintelligence. From the outset, the\nsuperintelligence’s conduct could thus be guided by its estimates of the\ncontent of our CEV. It would have strong instrumental reason to refine these\ninitial estimates (e.g. by studying human culture and psychology, scanning\nhuman brains, and reasoning about how we might behave if we knew more, thought\nmore clearly, etc.). In investigating these matters, the AI would be guided by\nits initial estimates of our CEV; so that, for instance, the AI would not\nunnecessarily run myriad simulations replete with unredeemed human suffering\nif it estimated that our CEV would probably condemn such simulations as mind\ncrime.\n\nAnother objection is that there are so many different ways of life and moral\ncodes in the world that it might not be possible to “blend” them into one CEV.\nEven if one could blend them, the result might not be particularly\nappetizing—one would be unlikely to get a delicious meal by mixing together\nall the best flavors from everyone’s different favorite\ndish.[13](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1108558) In\nanswer to this, one could point out that the CEV approach does not require\nthat all ways of life, moral codes, or personal values be blended together\ninto one stew. The CEV dynamic is supposed to act only when our wishes cohere.\nOn issues on which there is widespread irreconcilable disagreement, even after\nthe various idealizing conditions have been imposed, the dynamic should\nrefrain from determining the outcome. To continue the cooking analogy, it\nmight be that individuals or cultures will have different favorite dishes, but\nthat they can nevertheless broadly agree that aliments should be nontoxic. The\nCEV dynamic could then act to prevent food poisoning while otherwise allowing\nhumans to work out their culinary practices without its guidance or\ninterference.\n\n#### [Rationales for\nCEV](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_006.html#filepos25821)\n\nYudkowsky’s article offered seven arguments for the CEV approach. Three of\nthese were basically different ways of making the point that while the aim\nshould be to do something that is humane and helpful, it would be very\ndifficult to lay down an explicit set of rules that does not have unintended\ninterpretations and undesirable\nconsequences.[14](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1108693)\nThe CEV approach is meant to be robust and self-correcting; it is meant to\ncapture the _source_ of our values instead of relying on us correctly\nenumerating and articulating, once and for all, each of our essential values.\n\nThe remaining four arguments go beyond that first basic (but important) point,\nspelling out desiderata on candidate solutions to the value-specification\nproblem and suggesting that CEV meets these desiderata.\n\n###### _“Encapsulate moral growth”_\n\nThis is the desideratum that the solution should allow for the possibility of\nmoral progress. As suggested earlier, there are reasons to believe that our\ncurrent moral beliefs are flawed in many ways; perhaps deeply flawed. If we\nwere to stipulate a specific and unalterable moral code for the AI to follow,\nwe would in effect be locking in our present moral convictions, including\ntheir errors, destroying any hope of moral growth. The CEV approach, by\ncontrast, allows for the possibility of such growth because it has the AI try\nto do that which we would have wished it to do if we had developed further\nunder favorable conditions, and it is possible that if we had thus developed\nour moral beliefs and sensibilities would have been purged of their current\ndefects and limitations.\n\n###### _“Avoid hijacking the destiny of humankind”_\n\nYudkowsky has in mind a scenario in which a small group of programmers creates\na seed AI that then grows into a superintelligence that obtains a decisive\nstrategic advantage. In this scenario, the original programmers hold in their\nhands the entirety of humanity’s cosmic endowment. Obviously, this is a\nhideous responsibility for any mortal to shoulder. Yet it is not possible for\nthe programmers to completely shirk the onus once they find themselves in this\nsituation: any choice they make, including abandoning the project, would have\nworld-historical consequences. Yudkowsky sees CEV as a way for the programmers\nto avoid arrogating to themselves the privilege or burden of determining\nhumanity’s future. By setting up a dynamic that implements _humanity’s_\ncoherent extrapolated volition—as opposed to their own volition, or their own\nfavorite moral theory—they in effect distribute their influence over the\nfuture to all of humanity.\n\n###### _“Avoid creating a motive for modern-day humans to fight over the\ninitial dynamic”_\n\nDistributing influence over humanity’s future is not only morally preferable\nto the programming team implementing their own favorite vision, it is also a\nway to reduce the incentive to fight over who gets to create the first\nsuperintelligence. In the CEV approach, the programmers (or their sponsors)\nexert no more influence over the content of the outcome than any other\nperson—though they of course play a starring causal role in determining the\nstructure of the extrapolation and in deciding to implement humanity’s CEV\ninstead of some alternative. Avoiding conflict is important not only because\nof the immediate harm that conflict tends to cause but also because it hinders\ncollaboration on the difficult challenge of developing superintelligence\nsafely and beneficially.\n\nCEV is meant to be capable of commanding wide support. This is not just\nbecause it allocates influence equitably. There is also a deeper ground for\nthe irenic potential of CEV, namely that it enables many different groups to\nhope that their preferred vision of the future will prevail totally. Imagine a\nmember of the Afghan Taliban debating with a member of the Swedish Humanist\nAssociation. The two have very different worldviews, and what is a utopia for\none might be a dystopia for the other. Nor might either be thrilled by any\ncompromise position, such as permitting girls to receive an education but only\nup to ninth grade, or permitting Swedish girls to be educated but Afghan girls\nnot. However, both the Taliban and the Humanist might be able to endorse the\nprinciple that the future should be determined by humanity’s CEV. The Taliban\ncould reason that if his religious views are in fact correct (as he is\nconvinced they are) and if good grounds for accepting these views exist (as he\nis also convinced) then humankind would in the end come to accept these views\nif only people were less prejudiced and biased, if they spent more time\nstudying scripture, if they could more clearly understand how the world works\nand recognize essential priorities, if they could be freed from irrational\nrebelliousness and cowardice, and so\nforth.[15](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1109162)\nThe Humanist, similarly, would believe that under these idealized conditions,\nhumankind would come to embrace the principles she espouses.\n\n###### _“Keep humankind ultimately in charge of its own destiny”_\n\nWe might not want an outcome in which a paternalistic superintelligence\nwatches over us constantly, micromanaging our affairs with an eye towards\noptimizing every detail in accordance with a grand plan. Even if we stipulate\nthat the superintelligence would be perfectly benevolent, and free from\npresumptuousness, arrogance, overbearingness, narrow-mindedness, and other\nhuman shortcomings, one might still resent the loss of autonomy entailed by\nsuch an arrangement. We might prefer to create our destiny as we go along,\neven if it means that we sometimes fumble. Perhaps we want the\nsuperintelligence to serve as a safety net, to support us when things go\ncatastrophically wrong, but otherwise to leave us to fend for ourselves.\n\nCEV allows for this possibility. CEV is meant to be an “initial dynamic,” a\nprocess that runs once and then replaces itself with whatever the extrapolated\nvolition wishes. If humanity’s extrapolated volition wishes that we live under\nthe supervision of a paternalistic AI, then the CEV dynamic would create such\nan AI and hand it the reins. If humanity’s extrapolated volition instead\nwishes that a democratic human world government be created, then the CEV\ndynamic might facilitate the establishment of such an institution and\notherwise remain invisible. If humanity’s extrapolated volition is instead\nthat each person should get an endowment of resources that she can use as she\npleases so long as she respects the equal rights of others, then the CEV\ndynamic could make this come true by operating in the background much like a\nlaw of nature, to prevent trespass, theft, assault, and other nonconsensual\nimpingements.[16](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1109872)\n\nThe structure of the CEV approach thus allows for a virtually unlimited range\nof outcomes. It is also conceivable that humanity’s extrapolated volition\nwould wish that the CEV does nothing at all. In that case, the AI implementing\nCEV should, upon having established with sufficient probability that this is\nwhat humanity’s extrapolated volition would wish it to do, safely shut itself\ndown.\n\n#### [Further\nremarks](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_006.html#filepos25925)\n\nThe CEV proposal, as outlined above, is of course the merest schematic. It has\na number of free parameters that could be specified in various ways, yielding\ndifferent versions of the proposal.\n\nOne parameter is the extrapolation base: Whose volitions are to be included?\nWe might say “everybody,” but this answer spawns a host of further questions.\nDoes the extrapolation base include so-called “marginal persons” such as\nembryos, fetuses, brain-dead persons, patients with severe dementias or who\nare in permanent vegetative states? Does each of the hemispheres of a “split-\nbrain” patient get its own weight in the extrapolation and is this weight the\nsame as that of the entire brain of a normal subject? What about people who\nlived in the past but are now dead? People who will be born in the future?\nHigher animals and other sentient creatures? Digital minds? Extraterrestrials?\n\nOne option would be to include only the population of adult human beings on\nEarth who are alive at the start of the time of the AI’s creation. An initial\nextrapolation from this base could then decide whether and how the base should\nbe expanded. Since the number of “marginals” at the periphery of this base is\nrelatively small, the result of the extrapolation may not depend much on\nexactly where the boundary is drawn—on whether, for instance, it includes\nfetuses or not.\n\nThat somebody is excluded from the original extrapolation base does not imply\nthat their wishes and well-being are disregarded. If the coherent extrapolated\nvolition of those in the extrapolation base (e.g. living adult human beings)\nwishes that moral consideration be extended to other beings, then the outcome\nof the CEV dynamic would reflect that preference. Nevertheless, it is possible\nthat the interests of those who are included in the original extrapolation\nbase would be accommodated to a greater degree than the interests of\noutsiders. In particular, if the dynamic acts only where there is broad\nagreement between individual extrapolated volitions (as in Yudkowsky’s\noriginal proposal), there would seem to be a significant risk of an ungenerous\nblocking vote that could prevent, for instance, the welfare of nonhuman\nanimals or digital minds from being protected. The result might potentially be\nmorally\nrotten.[17](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1110198)\n\nOne motivation for the CEV proposal was to avoid creating a motive for humans\nto fight over the creation of the first superintelligent AI. Although the CEV\nproposal scores better on this desideratum than many alternatives, it does not\nentirely eliminate motives for conflict. A selfish individual, group, or\nnation might seek to enlarge its slice of the future by keeping others out of\nthe extrapolation base.\n\nA power grab of this sort might be rationalized in various ways. It might be\nargued, for instance, that the sponsor who funds the development of the AI\ndeserves to own the outcome. This moral claim is probably false. It could be\nobjected, for example, that the project that launches the first successful\nseed AI imposes a vast risk externality on the rest of humanity, which\ntherefore is entitled to compensation. The amount of compensation owed is so\ngreat that it can only take the form of giving everybody a stake in the upside\nif things turn out\nwell.[18](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1111330)\n\nAnother argument that might be used to rationalize a power grab is that large\nsegments of humanity have base or evil preferences and that including them in\nthe extrapolation base would risk turning humanity’s future into a dystopia.\nIt is difficult to know the share of good and bad in the average person’s\nheart. It is also difficult to know how much this balance varies between\ndifferent groups, social strata, cultures, or nations. Whether one is\noptimistic or pessimistic about human nature, one may prefer not to wager\nhumanity’s cosmic endowment on the speculation that, for a sufficient majority\nof the seven billion people currently alive, their better angels would prevail\nin their extrapolated volitions. Of course, omitting a certain set of people\nfrom the extrapolation base does not guarantee that light would triumph; and\nit might well be that the souls that would soonest exclude others or grab\npower for themselves tend rather to contain unusually large amounts of\ndarkness.\n\nYet another reason for fighting over the initial dynamic is that one might\nbelieve that somebody else’s AI will not work as advertised, even if the AI is\nbilled as a way to implement humanity’s CEV. If different groups have\ndifferent beliefs about which implementation is most likely to succeed, they\nmight fight to prevent the others from launching. It would be better in such\nsituations if the competing projects could settle their epistemic differences\nby some method that more reliably ascertains who is right than the method of\narmed\nconflict.[19](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1112306)\n\n### [Morality\nmodels](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_006.html#filepos26026)\n\nThe CEV proposal is not the only possible form of indirect normativity. For\nexample, instead of implementing humanity’s coherent extrapolated volition,\none could try to build an AI with the goal of doing what is morally right,\nrelying on the AI’s superior cognitive capacities to figure out just which\nactions fit that description. We can call this proposal “moral rightness”\n(MR). The idea is that we humans have an imperfect understanding of what is\nright and wrong, and perhaps an even poorer understanding of how the concept\nof moral rightness is to be philosophically analyzed: but a superintelligence\ncould understand these things\nbetter.[20](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1112424)\n\nWhat if we are not sure whether moral realism is true? We could still attempt\nthe MR proposal. We should just have to make sure to specify what the AI\nshould do in the eventuality that its presupposition of moral realism is\nfalse. For example, we could stipulate that if the AI estimates with a\nsufficient probability that there are no suitable non-relative truths about\nmoral rightness, then it should revert to implementing coherent extrapolated\nvolition instead, or simply shut itself\ndown.[21](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1113480)\n\nMR appears to have several advantages over CEV. MR would do away with various\nfree parameters in CEV, such as the degree of coherence among extrapolated\nvolitions that is required for the AI to act on the result, the ease with\nwhich a majority can overrule dissenting minorities, and the nature of the\nsocial environment within which our extrapolated selves are to be supposed to\nhave “grown up farther together.” It would seem to eliminate the possibility\nof a moral failure resulting from the use of an extrapolation base that is too\nnarrow or too wide. Furthermore, MR would orient the AI toward morally right\naction even if our coherent extrapolated volitions happen to wish for the AI\nto take actions that are morally odious. As noted earlier, this seems a live\npossibility with the CEV proposal. Moral goodness might be more like a\nprecious metal than an abundant element in human nature, and even after the\nore has been processed and refined in accordance with the prescriptions of the\nCEV proposal, who knows whether the principal outcome will be shining virtue,\nindifferent slag, or toxic sludge?\n\nMR would also appear to have some disadvantages. It relies on the notion of\n“morally right,” a notoriously difficult concept, one with which philosophers\nhave grappled since antiquity without yet attaining consensus as to its\nanalysis. Picking an erroneous explication of “moral rightness” could result\nin outcomes that would be morally very wrong. This difficulty of defining\n“moral rightness” might seem to count heavily against the MR proposal.\nHowever, it is not clear that the MR proposal is really at a material\ndisadvantage in this regard. The CEV proposal, too, uses terms and concepts\nthat are difficult to explicate (such as “knowledge,” “being more the people\nwe wished we were,” “grown up farther together,” among\nothers).[22](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1116149)\nEven if these concepts are marginally less opaque than “moral rightness,” they\nare still miles removed from anything that programmers can currently express\nin code.[23](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1116649)\nThe path to endowing an AI with any of these concepts might involve giving it\ngeneral linguistic ability (comparable, at least, to that of a normal human\nadult). Such a general ability to understand natural language could then be\nused to understand what is meant by “morally right.” If the AI could grasp the\nmeaning, it could search for actions that fit. As the AI develops\nsuperintelligence, it could then make progress on two fronts: on the\nphilosophical problem of understanding what moral rightness is, and on the\npractical problem of applying this understanding to evaluate particular\nactions.[24](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1117066)\nWhile this would not be easy, it is not clear that it would be any _more_\ndifficult than extrapolating humanity’s coherent extrapolated\nvolition.[25](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1117681)\n\nA more fundamental issue with MR is that even if can be implemented, it might\nnot give us what we want or what we would choose if we were brighter and\nbetter informed. This is of course the essential feature of MR, not an\naccidental bug. However, it might be a feature that would be extremely harmful\nto us.[26](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1117977)\n\nOne might try to preserve the basic idea of the MR model while reducing its\ndemandingness by focusing on _moral permissibility_ : the idea being that we\ncould let the AI pursue humanity’s CEV so long as it did not act in ways that\nare morally impermissible. For example, one might formulate the following goal\nfor the AI:\n\n> Among the actions that are morally permissible for the AI, take one that\n> humanity’s CEV would prefer. However, if some part of this instruction has\n> no well-specified meaning, or if we are radically confused about its\n> meaning, or if moral realism is false, or if we acted morally impermissibly\n> in creating an AI with this goal, then undergo a controlled\n> shutdown.[27](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1120024)\n> Follow the intended meaning of this instruction.\n\nOne might still worry that this moral permissibility model (MP) represents an\nunpalatably high degree of respect for the requirements of morality. How big a\nsacrifice it would entail depends on which ethical theory is\ntrue.[28](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1120628) If\nethics is _satisficing_ , in the sense that it counts as morally permissible\nany action that conforms to a few basic moral constraints, then MP may leave\nample room for our coherent extrapolated volition to influence the AI’s\nactions. However, if ethics is _maximizing_ —for example, if the only morally\npermissible actions are those that have the morally best consequences—then MP\nmay leave little or no room for our own preferences to shape the outcome.\n\nTo illustrate this concern, let us return for a moment to the example of\nhedonistic consequentialism. Suppose that this ethical theory is true, and\nthat the AI knows it to be so. For present purposes, we can define hedonistic\nconsequentialism as the claim that an action is morally right (and morally\npermissible) if and only if, among all feasible actions, no other action would\nproduce a greater balance of pleasure over suffering. The AI, following MP,\nmight maximize the surfeit of pleasure by converting the accessible universe\ninto hedonium, a process that may involve building computronium and using it\nto perform computations that instantiate pleasurable experiences. Since\nsimulating any existing human brain is not the most efficient way of producing\npleasure, a likely consequence is that we all die.\n\nBy enacting either the MR or the MP proposal, we would thus risk sacrificing\nour lives for a greater good. This would be a bigger sacrifice than one might\nthink, because what we stand to lose is not merely the chance to live out a\nnormal human life but the opportunity to enjoy the far longer and richer lives\nthat a friendly superintelligence could bestow.\n\nThe sacrifice looks even less appealing when we reflect that the\nsuperintelligence could realize a nearly-as-great good (in fractional terms)\nwhile sacrificing much less of our own potential well-being. Suppose that we\nagreed to allow _almost_ the entire accessible universe to be converted into\nhedonium—everything except a small preserve, say the Milky Way, which would be\nset aside to accommodate our own needs. Then there would still be a hundred\nbillion galaxies devoted to the maximization of pleasure. But we would have\none galaxy within which to create wonderful civilizations that could last for\nbillions of years and in which humans and nonhuman animals could survive and\nthrive, and have the opportunity to develop into beatific posthuman\nspirits.[29](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1120896)\n\nIf one prefers this latter option (as I would be inclined to do) it implies\nthat one does not have an unconditional lexically dominant preference for\nacting morally permissibly. But it is consistent with placing great weight on\nmorality.\n\nEven from a purely moral point of view, it might be better to _advocate_ some\nproposal that is less morally ambitious than MR or MP. If the morally best has\nno chance of being implemented—perhaps because of its frowning\ndemandingness—it might be morally better to promote some other proposal, one\nthat would be near-ideal and whose chances of being implemented could be\nsignificantly increased by our promoting\nit.[30](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1121325)\n\n### [Do What I\nMean](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_006.html#filepos26118)\n\nWe might feel unsure whether to go for CEV, MR, MP, or something else. Could\nwe punt on this higher-level decision as well, offloading even more cognitive\nwork onto the AI? Where is the limit to our possible laziness?\n\nConsider, for example, the following “reasons-based” goal:\n\n> Do whatever we would have had most reason to ask the AI to do.\n\nThis goal might boil down to extrapolated volition or morality or something\nelse, but it would seem to spare us the effort and risk involved in trying to\nfigure out for ourselves which of these more specific objectives we would have\nmost reason to select.\n\nSome of the problems with the morality-based goals, however, also apply here.\nFirst, we might fear that this reasons-based goal would leave too little room\nfor our own desires. Some philosophers maintain that a person always has most\nreason to do what it would be morally best for her to do. If those\nphilosophers are right, then the reason-based goal collapses into MR—with the\nconcomitant risk that a superintelligence implementing such a dynamic would\nkill everyone within reach. Second, as with all proposals couched in technical\nlanguage, there is a possibility that we might have misunderstood the meaning\nof our own assertions. We saw that, in the case of the morality-based goals,\nasking the AI to do what is right may lead to unforeseen and unwanted\nconsequences such that, had we anticipated them, we would not have implemented\nthe goal in question. The same applies to asking the AI to do what we have\nmost reason to do.\n\nWhat if we try to avoid these difficulties by couching a goal in emphatically\nnontechnical language—such as in terms of\n“niceness”:[31](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1121739)\n\n> Take the nicest action; or, if no action is nicest, then take an action that\n> is at least super-duper nice.\n\nHow could there be anything objectionable about building a _nice_ AI? But we\nmust ask what precisely is meant by this expression. The lexicon lists various\nmeanings of “nice” that are clearly not intended to be used here: we do not\nintend that the AI should be _courteous and polite_ nor _overdelicate or\nfastidious_. If we can count on the AI recognizing the intended interpretation\nof “niceness” and being motivated to pursue niceness in just that sense, then\nthis goal would seem to amount to a command to do what the programmers meant\nfor the AI to\ndo.[32](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1121953) An\ninjunction to similar effect was included in the formulation of CEV (“…\ninterpreted as we wish that interpreted”) and in the moral-permissibility\ncriterion as rendered earlier (“… follow the intended meaning of this\ninstruction”). By affixing such a “Do What I Mean” clause we may indicate that\nthe other words in the goal description should be construed charitably rather\nthan literally. But saying that the AI should be “nice” adds almost nothing:\nthe real work is done by the “Do What I Mean” instruction. If we knew how to\ncode “Do What I Mean” in a general and powerful way, we might as well use that\nas a standalone goal.\n\nHow might one implement such a “Do What I Mean” dynamic? That is, how might we\ncreate an AI motivated to charitably interpret our wishes and unspoken\nintentions and to act accordingly? One initial step could be to try to get\nclearer about what we mean by “Do What I Mean.” It might help if we could\nexplicate this in more behavioristic terms, for example in terms of revealed\npreferences in various hypothetical situations—such as situations in which we\nhad more time to consider the options, in which we were smarter, in which we\nknew more of the relevant facts, and in which in various other ways conditions\nwould be more favorable for us accurately manifesting in concrete choices what\nwe mean when we say that we want an AI that is friendly, beneficial, nice …\n\nHere, of course, we come full circle. We have returned to the indirect\nnormativity approach with which we started—the CEV proposal, which, in\nessence, expunges all concrete content from the value specification, leaving\nonly an abstract value defined in purely procedural terms: to do that which we\nwould have wished for the AI to do in suitably idealized circumstances. By\nmeans of such indirect normativity, we could hope to offload to the AI much of\nthe cognitive work that we ourselves would be trying to perform if we\nattempted to articulate a more concrete description of what values the AI is\nto pursue. In seeking to take full advantage of the AI’s epistemic\nsuperiority, CEV can thus be seen as an application of the principle of\nepistemic deference.\n\n### [Component\nlist](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_006.html#filepos26209)\n\nSo far we have considered different options for what content to put into the\ngoal system. But an AI’s behavior will also be influenced by other design\nchoices. In particular, it can make a critical difference which decision\ntheory and which epistemology it uses. Another important question is whether\nthe AI’s plans will be subject to human review before being put into action.\n\nTable 13 summarizes these design choices. A project that aims to build a\nsuperintelligence ought to be able to explain what choices it has made\nregarding each of these components, and to justify why those choices were\nmade.[33](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1122161)\n\n|  \n---|---  \n**Table 13 Component list**  \n**Goal content** | What objective should the AI pursue? How should a description of this objective be interpreted? Should the objective include giving special rewards to those who contributed to the project’s success?  \n**Decision theory** | Should the AI use causal decision theory, evidential decision theory, updateless decision theory, or something else?  \n**Epistemology** | What should the AI’s prior probability function be, and what other explicit or implicit assumptions about the world should it make? What theory of anthropics should it use?  \n**Ratification** | Should the AI’s plans be subjected to human review before being put into effect? If so, what is the protocol for that review process?  \n  \n#### [Goal\ncontent](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_006.html#filepos26300)\n\nWe have already discussed how indirect normativity might be used in specifying\nthe values that the AI is to pursue. We discussed some options, such as\nmorality-based models and coherent extrapolated volition. Each such option\ncreates further choices that need to be made. For instance, the CEV approach\ncomes in many varieties, depending on who is included in the extrapolation\nbase, the structure of the extrapolation, and so forth. Other forms of\nmotivation selection methods might call for different types of goal content.\nFor example, an oracle might be built to place a value on giving accurate\nanswers. An oracle constructed with domesticity motivation might also have\ngoal content that disvalues the excessive use of resources in producing its\nanswers.\n\nAnother design choice is whether to include special provisions in the goal\ncontent to reward individuals who contribute to the successful realization of\nthe AI, for example by giving them extra resources or influence over the AI’s\nbehavior. We can term any such provisions “incentive wrapping.” Incentive\nwrapping could be seen as a way to increase the likelihood that the project\nwill be successful, at the cost of compromising to some extent the goal that\nthe project set out to achieve.\n\nFor example, if the project’s goal is to create a dynamic that implements\nhumanity’s coherent extrapolated volition, then an incentive wrapping scheme\nmight specify that certain individuals’ volitions should be given extra weight\nin the extrapolation. If such a project is successful, the result is not\nnecessarily the implementation of humanity’s coherent extrapolated volition.\nInstead, some approximation to this goal might be\nachieved.[34](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1122474)\n\nSince incentive wrapping would be a piece of goal content that would be\ninterpreted and pursued by a superintelligence, it could take advantage of\nindirect normativity to specify subtle and complicated provisions that would\nbe difficult for a human manager to implement. For example, instead of\nrewarding programmers according to some crude but easily accessible metric,\nsuch as how many hours they worked or how many bugs they corrected, the\nincentive wrapping could specify that programmers “are to be rewarded in\nproportion to how much their contributions increased some reasonable _ex ante_\nprobability of the project being successfully completed in the way the\nsponsors intended.” Further, there would be no reason to limit the incentive\nwrapping to project staff. It could instead specify that _every_ person should\nbe rewarded according to their just deserts. Credit allocation is a difficult\nproblem, but a superintelligence could be expected to do a reasonable job of\napproximating the criteria specified, explicitly or implicitly, by the\nincentive wrapping.\n\nIt is conceivable that the superintelligence might even find some way of\nrewarding individuals who have died prior to the superintelligence’s\ncreation.[35](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1123046)\nThe incentive wrapping could then be extended to embrace at least some of the\ndeceased, potentially including individuals who died before the project was\nconceived, or even antedating the first enunciation of the concept of\nincentive wrapping. Although the institution of such a retroactive policy\nwould not causally incentivize those people who are already resting in their\ngraves as these words are being put to the page, it might be favored for moral\nreasons—though it could be argued that insofar as fairness is a goal, it\nshould be included as part of the target specification proper rather than in\nthe surrounding incentive wrapping.\n\nWe cannot here delve into all the ethical and strategic issues associated with\nincentive wrapping. A project’s position on these issues, however, would be an\nimportant aspect of its fundamental design concept.\n\n#### [Decision\ntheory](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_006.html#filepos26428)\n\nAnother important design choice is which decision theory the AI should be\nbuilt to use. This might affect how the AI behaves in certain strategically\nfateful situations. It might determine, for instance, whether the AI is open\nto trade with, or extortion by, other superintelligent civilizations whose\nexistence it hypothesizes. The particulars of the decision theory could also\nmatter in predicaments involving finite probabilities of infinite payoffs\n(“Pascalian wagers”) or extremely small probabilities of extremely large\nfinite payoffs (“Pascalian muggings”) or in contexts where the AI is facing\nfundamental normative uncertainty or where there are multiple instantiations\nof the same agent\nprogram.[36](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1124404)\n\nThe options on the table include causal decision theory (in a variety of\nflavors) and evidential decision theory, along with newer candidates such as\n“timeless decision theory” and “updateless decision theory,” which are still\nunder\ndevelopment.[37](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1124686)\nIt may prove difficult to identify and articulate the correct decision theory,\nand to have justified confidence that we have got it right. Although the\nprospects for directly specifying an AI’s decision theory are perhaps more\nhopeful than those of directly specifying its final values, we are still\nconfronted with a substantial risk of error. Many of the complications that\nmight break the currently most popular decision theories were discovered only\nrecently, suggesting that there might exist further problems that have not yet\ncome into sight. The result of giving the AI a flawed decision theory might be\ndisastrous, possibly amounting to an existential catastrophe.\n\nIn view of these difficulties, one might consider an indirect approach to\nspecifying the decision theory that the AI should use. Exactly how to do this\nis not yet clear. We might want the AI to use “that decision theory _D_ which\nwe would have wanted it to use had we thought long and hard about the matter.”\nHowever, the AI would need to be able to make decisions before learning what\n_D_ is. It would thus need some effective interim decision theory _D’_ that\nwould govern its search for _D_. One might try to define _D’_ to be some sort\nof superposition of the AI’s current hypotheses about _D_ (weighed by their\nprobabilities), though there are unsolved technical problems with how to do\nthis in a fully general\nway.[38](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1124862)\nThere is also cause for concern that the AI might make irreversibly bad\ndecisions (such as rewriting itself to henceforth run on some flawed decision\ntheory) during the learning phase, before the AI has had the opportunity to\ndetermine which particular decision theory is correct. To reduce the risk of\nderailment during this period of vulnerability we might instead try to endow\nthe seed AI with some form of _restricted rationality_ : a deliberately\nsimplified but hopefully dependable decision theory that staunchly ignores\nesoteric considerations, even ones we think may ultimately be legitimate, and\nthat is designed to replace itself with a more sophisticated (indirectly\nspecified) decision theory once certain conditions are\nmet.[39](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1124980) It\nis an open research question whether and how this could be made to work.\n\n####\n[Epistemology](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_006.html#filepos26529)\n\nA project will also need to make a fundamental design choice in selecting the\nAI’s epistemology, specifying the principles and criteria whereby empirical\nhypotheses are to be evaluated. Within a Bayesian framework, we can think of\nthe epistemology as a prior probability function—the AI’s implicit assignment\nof probabilities to possible worlds before it has taken any perceptual\nevidence into account. In other frameworks, the epistemology might take a\ndifferent form; but in any case some inductive learning rule is necessary if\nthe AI is to generalize from past observations and make predictions about the\nfuture.[40](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1125982)\nAs with the goal content and the decision theory, however, there is a risk\nthat our epistemology specification could miss the mark.\n\nOne might think that there is a limit to how much damage could arise from an\nincorrectly specified epistemology. If the epistemology is _too_\ndysfunctional, then the AI could not be very intelligent and it could not pose\nthe kind of risk discussed in this book. But the concern is that we may\nspecify an epistemology that is sufficiently sound to make the AI\ninstrumentally effective in most situations, yet which has some flaw that\nleads the AI astray on some matter of crucial importance. Such an AI might be\nakin to a quick-witted person whose worldview is predicated on a false dogma,\nheld to with absolute conviction, who consequently “tilts at windmills” and\ngives his all in pursuit of fantastical or harmful objectives.\n\nCertain kinds of subtle difference in an AI’s prior could turn out to make a\ndrastic difference to how it behaves. For example, an AI might be given a\nprior that assigns zero probability to the universe being infinite. No matter\nhow much astronomical evidence it accrues to the contrary, such an AI would\nstubbornly reject any cosmological theory that implied an infinite universe;\nand it might make foolish choices as a\nresult.[41](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1126375)\nOr an AI might be given a prior that assigns a zero probability to the\nuniverse not being Turing-computable (this is in fact a common feature of many\nof the priors discussed in the literature, including the Kolmogorov complexity\nprior mentioned in [Chapter\n1](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos33916)), again with\npoorly understood consequences if the embedded assumption—known as the\n“Church–Turing thesis”—should turn out to be false. An AI could also end up\nwith a prior that makes strong metaphysical commitments of one sort or\nanother, for instance by ruling out a priori the possibility that any strong\nform of mind–body dualism could be true or the possibility that there are\nirreducible moral facts. If any of those commitments is mistaken, the AI might\nseek to realize its final goals in ways that we would regard as perverse\ninstantiations. Yet there is no obvious reason why such an AI, despite being\nfundamentally wrong about one important matter, could not be sufficiently\ninstrumentally effective to secure a decisive strategic advantage.\n(Anthropics, the study of how to make inferences from indexical information in\nthe presence of observation selection effects, is another area where the\nchoice of epistemic axioms could prove\npivotal.[42](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1126537))\n\nWe might reasonably doubt our ability to resolve all foundational issues in\nepistemology in time for the construction of the first seed AI. We may,\ntherefore, consider taking an indirect approach to specifying the AI’s\nepistemology. This would raise many of the same issues as taking an indirect\napproach to specifying its decision theory. In the case of epistemology,\nhowever, there may be greater hope of benign convergence, with any of a wide\nclass of epistemologies providing an adequate foundation for safe and\neffective AI and ultimately yielding similar doxastic results. The reason for\nthis is that sufficiently abundant empirical evidence and analysis would tend\nto wash out any moderate differences in prior\nexpectations.[43](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1127907)\n\nA good aim would be to endow the AI with fundamental epistemological\nprinciples that match those governing our own thinking. Any AI diverging from\nthis ideal is an AI that we would judge to be reasoning incorrectly if we\nconsistently applied our own standards. Of course, this applies only to our\n_fundamental_ epistemological principles. Non-fundamental principles should be\ncontinuously created and revised by the seed AI itself as it develops its\nunderstanding of the world. The point of superintelligence is not to pander to\nhuman preconceptions but to make mincemeat out of our ignorance and folly.\n\n####\n[Ratification](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_006.html#filepos26627)\n\nThe final item in our list of design choices is _ratification_. Should the\nAI’s plans be subjected to human review before being put into effect? For an\noracle, this question is implicitly answered in the affirmative. The oracle\noutputs information; human reviewers choose whether and how to act upon it.\nFor genies, sovereigns, and tool-AIs, however, the question of whether to use\nsome form of ratification remains open.\n\nTo illustrate how ratification might work, consider an AI intended to function\nas a sovereign implementing humanity’s CEV. Instead of launching this AI\ndirectly, imagine that we first built an oracle AI for the sole purpose of\nanswering questions about what the sovereign AI would do. As earlier chapters\nrevealed, there are risks in creating a superintelligent oracle (such as risks\nof mind crime or infrastructure profusion). But for purposes of this example\nlet us assume that the oracle AI has been successfully implemented in a way\nthat avoided these pitfalls.\n\nWe thus have an oracle AI that offers us its best guesses about the\nconsequences of running some piece of code intended to implement humanity’s\nCEV. The oracle may not be able to predict in detail what would happen, but\nits predictions are likely to be better than our own. (If it were impossible\neven for a superintelligence to predict _anything_ about the code would do, we\nwould be crazy to run it.) So the oracle ponders for a while and then presents\nits forecast. To make the answer intelligible, the oracle may offer the\noperator a range of tools with which to explore various features of the\npredicted outcome. The oracle could show pictures of what the future looks\nlike and provide statistics about the number of sentient beings that will\nexist at different times, along with average, peak, and lowest levels of well-\nbeing. It could offer intimate biographies of several randomly selected\nindividuals (perhaps imaginary people selected to be probably representative).\nIt could highlight aspects of the future that the operator might not have\nthought of inquiring about but which would be regarded as pertinent once\npointed out.\n\nBeing able to preview the outcome in this manner has obvious advantages. The\npreview could reveal the consequences of an error in a planned sovereign’s\ndesign specifications or source code. If the crystal ball shows a ruined\nfuture, we could scrap the code for the planned sovereign AI and try something\nelse. A strong case could be made that we should familiarize ourselves with\nthe concrete ramifications of an option before committing to it, especially\nwhen the entire future of the race is on the line.\n\nWhat is perhaps less obvious is that ratification also has potentially\nsignificant disadvantages. The irenic quality of CEV might be undermined if\nopposing factions, instead of submitting to the arbitration of superior wisdom\nin confident expectation of being vindicated, could see in advance what the\nverdict would be. A proponent of the morality-based approach might worry that\nthe sponsor’s resolve would collapse if all the sacrifices required by the\nmorally optimal were to be revealed. And we might all have reason to prefer a\nfuture that holds some surprises, some dissonance, some wildness, some\nopportunities for self-overcoming—a future whose contours are not too snugly\ntailored to present preconceptions but provide some give for dramatic movement\nand unplanned growth. We might be less likely to take such an expansive view\nif we could cherry-pick every detail of the future, sending back to the\ndrawing board any draft that does not fully conform to our fancy at that\nmoment.\n\nThe issue of sponsor ratification is therefore less clear-cut than it might\ninitially seem. Nevertheless, on balance it would seem prudent to take\nadvantage of an opportunity to preview, if that functionality is available.\nBut rather than letting the reviewer fine-tune every aspect of the outcome, we\nmight give her a simple veto which could be exercised only a few times before\nthe entire project would be\naborted.[44](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1128247)\n\n### [Getting close\nenough](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_006.html#filepos26725)\n\nThe main purpose of ratification would be to reduce the probability of\ncatastrophic error. In general, it seems wise to aim at minimizing the risk of\ncatastrophic error rather than at maximizing the chance of every detail being\nfully optimized. There are two reasons for this. First, humanity’s cosmic\nendowment is astronomically large—there is plenty to go around even if our\nprocess involves some waste or accepts some unnecessary constraints. Second,\nthere is a hope that if we but get the initial conditions for the intelligence\nexplosion approximately right, then the resulting superintelligence may\neventually home in on, and precisely hit, our ultimate objectives. The\nimportant thing is to land in the right attractor basin.\n\nWith regard to epistemology, it is plausible that a wide range of priors will\nultimately converge to very similar posteriors (when computed by a\nsuperintelligence and conditionalized on a realistic amount of data). We\ntherefore need not worry about getting the epistemology _exactly_ right. We\nmust just avoid giving the AI a prior that is so extreme as to render the AI\nincapable of learning vital truths even with the benefit of copious experience\nand\nanalysis.[45](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1128409)\n\nWith regard to decision theory, the risk of irrecoverable error seems larger.\nWe might still hope to directly specify a decision theory that is good enough.\nA superintelligent AI could switch to a new decision theory at any time;\nhowever, if it starts out with a sufficiently wrong decision theory it may not\nsee the reason to switch. Even if an agent comes to see the benefits of having\na different decision theory, the realization might come too late. For example,\nan agent designed to refuse blackmail might enjoy the benefit of deterring\nwould-be extortionists. For this reason, blackmailable agents might do well to\nproactively adopt a non-exploitable decision theory. Yet once a blackmailable\nagent receives the threat and regards it as credible, the damage is done.\n\nGiven an adequate epistemology and decision theory, we could try to design the\nsystem to implement CEV or some other indirectly specified goal content. Again\nthere is hope of convergence: that different ways of implementing a CEV-like\ndynamic would lead to the same utopian outcome. Short of such convergence, we\nmay still hope that many of the different possible outcomes are good enough to\ncount as existential success.\n\nIt is not necessary for us to create a highly optimized design. Rather, our\nfocus should be on creating a highly reliable design, one that can be trusted\nto retain enough sanity to recognize its own failings. An imperfect\nsuperintelligence, whose fundamentals are sound, would gradually repair\nitself; and having done so, it would exert as much beneficial optimization\npower on the world as if it had been perfect from the outset.\n\n\n## [CHAPTER 14  \nThe strategic\npicture](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_006.html#filepos26835)\n\n**It is now time to consider the challenge of superintelligence in a broader\ncontext. We would like to orient ourselves in the strategic landscape\nsufficiently to know at least which general direction we should be heading.\nThis, it turns out, is not at all easy. Here in the penultimate chapter, we\nintroduce some general analytical concepts that help us think about long-term\nscience and technology policy issues. We then apply them to the issue of\nmachine intelligence.**\n\nIt can be illuminating to make a rough distinction between two different\nnormative stances from which a proposed policy may be evaluated. _The person-\naffecting perspective_ asks whether a proposed change would be in “our\ninterest”—that is to say, whether it would (on balance, and in expectation) be\nin the interest of those morally considerable creatures who either already\nexist or will come into existence independently of whether the proposed change\noccurs or not. _The impersonal perspective_ , in contrast, gives no special\nconsideration to currently existing people, or to those who will come to exist\nindependently of whether the proposed change occurs. Instead, it counts\neverybody equally, independently of their temporal location. The impersonal\nperspective sees great value in bringing new people into existence, provided\nthey have lives worth living: the more happy lives created, the better.\n\nThis distinction, although it barely hints at the moral complexities\nassociated with a machine intelligence revolution, can be useful in a first-\ncut analysis. Here we will first examine matters from the impersonal\nperspective. We will later see what changes if person-affecting considerations\nare given weight in our deliberations.\n\n### [Science and technology\nstrategy](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_006.html#filepos26971)\n\nBefore we zoom in on issues specific to machine superintelligence, we must\nintroduce some strategic concepts and considerations that pertain to\nscientific and technological development more generally.\n\n#### [Differential technological\ndevelopment](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_006.html#filepos27079)\n\nSuppose that a policymaker proposes to cut funding for a certain research\nfield, out of concern for the risks or long-term consequences of some\nhypothetical technology that might eventually grow from its soil. She can then\nexpect a howl of opposition from the research community.\n\nScientists and their public advocates often say that it is futile to try to\ncontrol the evolution of technology by blocking research. If some technology\nis feasible (the argument goes) it will be developed regardless of any\nparticular policymaker’s scruples about speculative future risks. Indeed, the\nmore powerful the capabilities that a line of development promises to produce,\nthe surer we can be that somebody, somewhere, will be motivated to pursue it.\nFunding cuts will not stop progress or forestall its concomitant dangers.\n\nInterestingly, this futility objection is almost never raised when a\npolicymaker proposes to _increase_ funding to some area of research, even\nthough the argument would seem to cut both ways. One rarely hears indignant\nvoices protest: “Please do not increase our funding. Rather, make some cuts.\nResearchers in other countries will surely pick up the slack; the same work\nwill get done anyway. Don’t squander the public’s treasure on domestic\nscientific research!”\n\nWhat accounts for this apparent doublethink? One plausible explanation, of\ncourse, is that members of the research community have a self-serving bias\nwhich leads us to believe that research is always good and tempts us to\nembrace almost any argument that supports our demand for more funding.\nHowever, it is also possible that the double standard can be justified in\nterms of national self-interest. Suppose that the development of a technology\nhas _two_ effects: giving a small benefit _B_ to its inventors and the country\nthat sponsors them, while imposing an aggregately larger harm _H_ —which could\nbe a risk externality—on everybody. Even somebody who is largely altruistic\nmight then choose to develop the overall harmful technology. They might reason\nthat the harm _H_ will result no matter what they do, since if they refrain\nsomebody else will develop the technology anyway; and given that total welfare\ncannot be affected, they might as well grab the benefit _B_ for themselves and\ntheir nation. (“Unfortunately, there will soon be a device that will destroy\nthe world. Fortunately, we got the grant to build it!”)\n\nWhatever the explanation for the futility objection’s appeal, it fails to show\nthat there is in general no impersonal reason for trying to steer\ntechnological development. It fails even if we concede the motivating idea\nthat with continued scientific and technological development efforts, all\nrelevant technologies will eventually be developed—that is, even if we concede\nthe following:\n\n> Technological completion conjecture\n\n> If scientific and technological development efforts do not effectively\n> cease, then all important basic capabilities that could be obtained through\n> some possible technology will be\n> obtained.[1](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1128955)\n\nThere are at least two reasons why the technological completion conjecture\ndoes not imply the futility objection. First, the antecedent might not hold,\nbecause it is not in fact a given that scientific and technological\ndevelopment efforts will not effectively cease (before the attainment of\ntechnological maturity). This reservation is especially pertinent in a context\nthat involves existential risk. Second, even if we could be certain that all\nimportant basic capabilities that could be obtained through some possible\ntechnology will be obtained, it could still make sense to attempt to influence\nthe direction of technological research. What matters is not only _whether_ a\ntechnology is developed, but also _when_ it is developed, by _whom_ , and in\n_what context_. These circumstances of birth of a new technology, which shape\nits impact, can be affected by turning funding spigots on or off (and by\nwielding other policy instruments).\n\nThese reflections suggest a principle that would have us attend to the\nrelative speed with which different technologies are\ndeveloped:[2](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1129525)\n\n> The principle of differential technological development\n\n> Retard the development of dangerous and harmful technologies, especially\n> ones that raise the level of existential risk; and accelerate the\n> development of beneficial technologies, especially those that reduce the\n> existential risks posed by nature or by other technologies.\n\nA policy could thus be evaluated on the basis of how much of a differential\nadvantage it gives to desired forms of technological development over\nundesired\nforms.[3](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1129635)\n\n#### [Preferred order of\narrival](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_006.html#filepos27203)\n\nSome technologies have an ambivalent effect on existential risks, increasing\nsome existential risks while decreasing others. Superintelligence is one such\ntechnology.\n\nWe have seen in earlier chapters that the introduction of machine\nsuperintelligence would create a substantial existential risk. But it would\nreduce many other existential risks. Risks from nature—such as asteroid\nimpacts, supervolcanoes, and natural pandemics—would be virtually eliminated,\nsince superintelligence could deploy countermeasures against most such\nhazards, or at least demote them to the non-existential category (for\ninstance, via space colonization).\n\nThese existential risks from nature are comparatively small over the relevant\ntimescales. But superintelligence would also eliminate or reduce many\nanthropogenic risks. In particular, it would reduce risks of accidental\ndestruction, including risk of accidents related to new technologies. Being\ngenerally more capable than humans, a superintelligence would be less likely\nto make mistakes, and more likely to recognize when precautions are needed,\nand to implement precautions competently. A well-constructed superintelligence\nmight sometimes take a risk, but only when doing so is wise. Furthermore, at\nleast in scenarios where the superintelligence forms a singleton, many non-\naccidental anthropogenic existential risks deriving from global coordination\nproblems would be eliminated. These include risks of wars, technology races,\nundesirable forms of competition and evolution, and tragedies of the commons.\n\nSince substantial peril would be associated with human beings developing\nsynthetic biology, molecular nanotechnology, climate engineering, instruments\nfor biomedical enhancement and neuropsychological manipulation, tools for\nsocial control that may facilitate totalitarianism or tyranny, and other\ntechnologies as-yet unimagined, eliminating these types of risk would be a\ngreat boon. An argument could therefore be mounted that earlier arrival dates\nof superintelligence are preferable. However, if risks from nature and from\nother hazards unrelated to future technology are small, then this argument\ncould be refined: what matters is that we get superintelligence _before_ other\ndangerous technologies, such as advanced nanotechnology. Whether it happens\nsooner or later may not be so important (from an impersonal perspective) so\nlong as the order of arrival is right.\n\nThe ground for preferring superintelligence to come before other potentially\ndangerous technologies, such as nanotechnology, is that superintelligence\nwould reduce the existential risks from nanotechnology but not vice\nversa.[4](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1130451)\nHence, if we create superintelligence first, we will face only those\nexistential risks that are associated with superintelligence; whereas if we\ncreate nanotechnology first, we will face the risks of nanotechnology and\nthen, additionally, the risks of\nsuperintelligence.[5](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1130561)\nEven if the existential risks from superintelligence are very large, and even\nif superintelligence is the riskiest of all technologies, there could thus be\na case for hastening its arrival.\n\nThese “sooner-is-better” arguments, however, presuppose that the riskiness of\ncreating superintelligence is the same regardless of when it is created. If,\ninstead, its riskiness declines over time, it might be better to delay the\nmachine intelligence revolution. While a later arrival would leave more time\nfor other existential catastrophes to intercede, it could still be preferable\nto slow the development of superintelligence. This would be especially\nplausible if the existential risks associated with superintelligence are much\nlarger than those associated with other disruptive technologies.\n\nThere are several quite strong reasons to believe that the riskiness of an\nintelligence explosion will decline significantly over a multidecadal\ntimeframe. One reason is that a later date leaves more time for the\ndevelopment of solutions to the control problem. The control problem has only\nrecently been recognized, and most of the current best ideas for how to\napproach it were discovered only within the past decade or so (and in several\ncases during the time that this book was being written). It is plausible that\nthe state of the art will advance greatly over the next several decades; and\nif the problem turns out to be very difficult, a significant rate of progress\nmight continue for a century or more. The longer it takes for\nsuperintelligence to arrive, the more such progress will have been made when\nit does. This is an important consideration in favor of later arrival\ndates—and a very strong consideration against extremely early arrival dates.\n\nAnother reason why superintelligence later might be safer is that this would\nallow more time for various beneficial background trends of human civilization\nto play themselves out. How much weight one attaches to this consideration\nwill depend on how optimistic one is about these trends.\n\nAn optimist could certainly point to a number of encouraging indicators and\nhopeful possibilities. People might learn to get along better, leading to\nreductions in violence, war, and cruelty; and global coordination and the\nscope of political integration might increase, making it easier to escape\nundesirable technology races (more on this below) and to work out an\narrangement whereby the hoped-for gains from an intelligence explosion would\nbe widely shared. There appear to be long-term historical trends in these\ndirections.[6](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1132513)\n\nFurther, an optimist could expect that the “sanity level” of humanity will\nrise over the course of this century—that prejudices will (on balance) recede,\nthat insights will accumulate, and that people will become more accustomed to\nthinking about abstract future probabilities and global risks. With luck, we\ncould see a general uplift of epistemic standards in both individual and\ncollective cognition. Again, there are trends pushing in these directions.\nScientific progress means that more will be known. Economic growth may give a\ngreater portion of the world’s population adequate nutrition (particularly\nduring the early years of life that are important for brain development) and\naccess to quality education. Advances in information technology will make it\neasier to find, integrate, evaluate, and communicate data and ideas.\nFurthermore, by the century’s end, humanity will have made an additional\nhundred years’ worth of mistakes, from which something might have been\nlearned.\n\nMany potential developments are ambivalent in the abovementioned\nsense—increasing some existential risks and decreasing others. For example,\nadvances in surveillance, data mining, lie detection, biometrics, and\npsychological or neurochemical means of manipulating beliefs and desires could\nreduce some existential risks by making it easier to coordinate\ninternationally or to suppress terrorists and renegades at home. These same\nadvances, however, might also increase some existential risks by amplifying\nundesirable social dynamics or by enabling the formation of permanently stable\ntotalitarian regimes.\n\nOne important frontier is the enhancement of biological cognition, such as\nthrough genetic selection. When we discussed this in [Chapters\n2](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos104440) and\n[3](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_010.html#filepos201138), we\nconcluded that the most radical forms of superintelligence would be more\nlikely to arise in the form of machine intelligence. That claim is consistent\nwith cognitive enhancement playing an important role in the lead-up to, and\ncreation of, machine superintelligence. Cognitive enhancement might seem\nobviously risk-reducing: the smarter the people working on the control\nproblem, the more likely they are to find a solution. However, cognitive\nenhancement could also hasten the development of machine intelligence, thus\nreducing the time available to work on the problem. Cognitive enhancement\nwould also have many other relevant consequences. These issues deserve a\ncloser look. (Most of the following remarks about “cognitive enhancement”\napply equally to non-biological means of increasing our individual or\ncollective epistemic effectiveness.)\n\n#### [Rates of change and cognitive\nenhancement](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_006.html#filepos27315)\n\nAn increase in either the mean or the upper range of human intellectual\nability would likely accelerate technological progress across the board,\nincluding progress toward various forms of machine intelligence, progress on\nthe control problem, and progress on a wide swath of other technical and\neconomic objectives. What would be the net effect of such acceleration?\n\nConsider the limiting case of a “universal accelerator,” an imaginary\nintervention that accelerates literally _everything_. The action of such a\nuniversal accelerator would correspond merely to an arbitrary rescaling of the\ntime metric, producing no qualitative change in observed\noutcomes.[7](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1132666)\n\nIf we are to make sense of the idea that cognitive enhancement might generally\nspeed things up, we clearly need some other concept than that of universal\nacceleration. A more promising approach is to focus on how cognitive\nenhancement might increase the rate of change in one type of process\n_relative_ to the rate of change in some other type of process. Such\ndifferential acceleration could affect a system’s dynamics. Thus, consider the\nfollowing concept:\n\n> _Macro-structural development accelerator_ —A lever that accelerates the\n> rate at which macro-structural features of the human condition develop,\n> while leaving unchanged the rate at which micro-level human affairs unfold.\n\nImagine pulling this lever in the decelerating direction. A brake pad is\nlowered onto the great wheel of world history; sparks fly and metal screeches.\nAfter the wheel has settled into a more leisurely pace, the result is a world\nin which technological innovation occurs more slowly and in which fundamental\nor globally significant change in political structure and culture happens less\nfrequently and less abruptly. A greater number of generations come and go\nbefore one era gives way to another. During the course of a lifespan, a person\nsees little change in the basic structure of the human condition.\n\nFor most of our species’ existence, macro-structural development was slower\nthan it is now. Fifty thousand years ago, an entire millennium might have\nelapsed without a single significant technological invention, without any\nnoticeable increase in human knowledge and understanding, and without any\nglobally meaningful political change. On a micro-level, however, the\nkaleidoscope of human affairs churned at a reasonable rate, with births,\ndeaths, and other personally and locally significant events. The average\nperson’s day might have been more action-packed in the Pleistocene than it is\ntoday.\n\nIf you came upon a magic lever that would let you change the rate of macro-\nstructural development, what should you do? Ought you to accelerate,\ndecelerate, or leave things as they are?\n\nAssuming the impersonal standpoint, this question requires us to consider the\neffects on existential risk. Let us distinguish between two kinds of risk:\n“state risks” and “step risks.” A state risk is one that is associated with\nbeing in a certain state, and the total amount of state risk to which a system\nis exposed is a direct function of how long the system remains in that state.\nRisks from nature are typically state risks: the longer we remain exposed, the\ngreater the chance that we will get struck by an asteroid, supervolcanic\neruption, gamma ray burst, naturally arising pandemic, or some other slash of\nthe cosmic scythe. Some anthropogenic risks are also state risks. At the level\nof an individual, the longer a soldier pokes his head up above the parapet,\nthe greater the cumulative chance he will be shot by an enemy sniper. There\nare anthropogenic state risks at the existential level as well: the longer we\nlive in an internationally anarchic system, the greater the cumulative chance\nof a thermonuclear Armageddon or of a great war fought with other kinds of\nweapons of mass destruction, laying waste to civilization.\n\nA step risk, by contrast, is a discrete risk associated with some necessary or\ndesirable transition. Once the transition is completed, the risk vanishes. The\namount of step risk associated with a transition is usually not a simple\nfunction of how long the transition takes. One does not halve the risk of\ntraversing a minefield by running twice as fast. Conditional on a fast\ntakeoff, the creation of superintelligence might be a step risk: there would\nbe a certain risk associated with the takeoff, the magnitude of which would\ndepend on what preparations had been made; but the amount of risk might not\ndepend much on whether the takeoff takes twenty milliseconds or twenty hours.\n\nWe can then say the following regarding a hypothetical macro-structural\ndevelopment accelerator:\n\n• Insofar as we are concerned with existential state risks, we should favor\nacceleration—provided we think we have a realistic prospect of making it\nthrough to a post-transition era in which any further existential risks are\ngreatly reduced.\n\n• If it were known that there is some step ahead destined to cause an\nexistential catastrophe, then we ought to reduce the rate of macro-structural\ndevelopment (or even put it in reverse) in order to give more generations a\nchance to exist before the curtain is rung down. But, in fact, it would be\noverly pessimistic to be so confident that humanity is doomed.\n\n• At present, the level of existential state risk appears to be relatively\nlow. If we imagine the technological macro-conditions for humanity frozen in\ntheir current state, it seems very unlikely that an existential catastrophe\nwould occur on a timescale of, say, a decade. So a delay of one\ndecade—provided it occurred at our current stage of development or at some\nother time when state risk is low—would incur only a very minor existential\nstate risk, whereas a postponement by one decade of subsequent technological\ndevelopments might well have a significant beneficial impact on later\nexistential step risks, for example by allowing more time for preparation.\n\nUpshot: the main way that the speed of macro-structural development is\nimportant is by affecting how well prepared humanity is when the time comes to\nconfront the key step\nrisks.[8](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1132980)\n\nSo the question we must ask is how cognitive enhancement (and concomitant\nacceleration of macro-structural development) would affect the expected level\nof preparedness at the critical juncture. Should we prefer a shorter period of\npreparation with higher intelligence? With higher intelligence, the\npreparation time could be used more effectively, and the final critical step\nwould be taken by a more intelligent humanity. Or should we prefer to operate\nwith closer to current levels of intelligence if that gives us more time to\nprepare?\n\nWhich option is better depends on the nature of the challenge being prepared\nfor. If the challenge were to solve a problem for which learning from\nexperience is key, then the chronological length of the preparation period\nmight be the determining factor, since time is needed for the requisite\nexperience to accumulate. What would such a challenge look like? One\nhypothetical example would be a new weapons technology that we could predict\nwould be developed at some point in the future and that would make it the case\nthat any subsequent war would have, let us say, a one-in-ten chance of causing\nan existential catastrophe. If such were the nature of the challenge facing\nus, then we might wish the rate of macro-structural development to be slow, so\nthat our species would have more time to get its act together before the\ncritical step when the new weapons technology is invented. One could hope that\nduring the grace period secured through the deceleration, our species might\nlearn to avoid war—that international relations around the globe might come to\nresemble those between the countries of the European Union, which, having\nfought one another ferociously for centuries, now coexist in peace and\nrelative harmony. The pacification might occur as a result of the gentle\nedification from various civilizing processes or through the shock therapy of\nsub-existential blows (e.g. small nuclear conflagrations, and the recoil and\nresolve they might engender to finally create the global institutions\nnecessary for the abolishment of interstate wars). If this kind of learning or\nadjusting would not be much accelerated by increased intelligence, then\ncognitive enhancement would be undesirable, serving merely to burn the fuse\nfaster.\n\nA prospective intelligence explosion, however, may present a challenge of a\ndifferent kind. The control problem calls for foresight, reasoning, and\ntheoretical insight. It is less clear how increased historical experience\nwould help. Direct experience of the intelligence explosion is not possible\n(until too late), and many features conspire to make the control problem\nunique and lacking in relevant historical precedent. For these reasons, the\namount of time that will elapse before the intelligence explosion may not\nmatter much per se. Perhaps what matters, instead, is (a) the amount of\nintellectual progress on the control problem achieved by the time of the\ndetonation; and (b) the amount of skill and intelligence available at the time\nto implement the best available solutions (and to improvise what is\nmissing).[9](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1133307)\nThat this latter factor should respond positively to cognitive enhancement is\nobvious. How cognitive enhancement would affect factor (a) is a somewhat\nsubtler matter.\n\nSuppose, as suggested earlier, that cognitive enhancement would be a general\nmacro-structural development accelerator. This would hasten the arrival of the\nintelligence explosion, thus reducing the amount of time available for\npreparation and for making progress on the control problem. Normally this\nwould be a bad thing. However, if the only reason why there is less time\navailable for intellectual progress is that intellectual progress is speeded\nup, then there need be no net reduction in the amount of intellectual progress\nthat will have taken place by the time the intelligence explosion occurs.\n\nAt this point, cognitive enhancement might appear to be neutral with respect\nto factor (a): the same intellectual progress that would otherwise have been\nmade prior to the intelligence explosion—including progress on the control\nproblem—still gets made, only compressed within a shorter time interval. In\nactuality, however, cognitive enhancement may well prove a positive influence\non (a).\n\nOne reason why cognitive enhancement might cause more progress to have been\nmade on the control problem by the time the intelligence explosion occurs is\nthat progress on the control problem may be especially contingent on extreme\nlevels of intellectual performance—even more so than the kind of work\nnecessary to create machine intelligence. The role for trial and error and\naccumulation of experimental results seems quite limited in relation to the\ncontrol problem, whereas experimental learning will probably play a large role\nin the development of artificial intelligence or whole brain emulation. The\nextent to which time can substitute for wit may therefore vary between tasks\nin a way that should make cognitive enhancement promote progress on the\ncontrol problem _more_ than it would promote progress on the problem of how to\ncreate machine intelligence.\n\nAnother reason why cognitive enhancement should differentially promote\nprogress on the control problem is that the very need for such progress is\nmore likely to be appreciated by cognitively more capable societies and\nindividuals. It requires foresight and reasoning to realize why the control\nproblem is important and to make it a\npriority.[10](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1133587)\nIt may also require uncommon sagacity to find promising ways of approaching\nsuch an unfamiliar problem.\n\nFrom these reflections we might tentatively conclude that cognitive\nenhancement is desirable, at least insofar as the focus is on the existential\nrisks of an intelligence explosion. Parallel lines of thinking apply to other\nexistential risks arising from challenges that require foresight and reliable\nabstract reasoning (as opposed to, e.g., incremental adaptation to experienced\nchanges in the environment or a multigenerational process of cultural\nmaturation and institution-building).\n\n#### [Technology\ncouplings](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_006.html#filepos27442)\n\nSuppose that one thinks that solving the control problem for artificial\nintelligence is very difficult, that solving it for whole brain emulations is\nmuch easier, and that it would therefore be preferable that machine\nintelligence be reached via the whole brain emulation path. We will return\nlater to the question of whether whole brain emulation would be safer than\nartificial intelligence. But for now we want to make the point that even if we\naccept this premiss, it would not follow that we ought to promote whole brain\nemulation technology. One reason, discussed earlier, is that a later arrival\nof superintelligence may be preferable, in order to allow more time for\nprogress on the control problem and for other favorable background trends to\nculminate—and thus, if one were confident that whole brain emulation would\nprecede AI anyway, it would be counterproductive to further hasten the arrival\nof whole brain emulation.\n\nBut even if it were the case that it would be best for whole brain emulation\nto arrive as soon as possible, it _still_ would not follow that we ought to\nfavor progress toward whole brain emulation. For it is possible that progress\ntoward whole brain emulation will not yield whole brain emulation. It may\ninstead yield neuromorphic artificial intelligence—forms of AI that mimic some\naspects of cortical organization but do not replicate neuronal functionality\nwith sufficient fidelity to constitute a proper emulation. If—as there is\nreason to believe—such neuromorphic AI is worse than the kind of AI that would\notherwise have been built, and if by promoting whole brain emulation we would\nmake neuromorphic AI arrive first, then our pursuit of the supposed _best_\noutcome (whole brain emulation) would lead to the _worst_ outcome\n(neuromorphic AI); whereas if we had pursued the _second-best_ outcome\n(synthetic AI) we might actually have attained the second-best (synthetic AI).\n\nWe have just described an (hypothetical) instance of what we might term a\n“technology\ncoupling.”[11](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1134015)\nThis refers to a condition in which two technologies have a predictable timing\nrelationship, such that developing one of the technologies has a robust\ntendency to lead to the development of the other, either as a necessary\nprecursor or as an obvious and irresistible application or subsequent step.\nTechnology couplings must be taken into account when we use the principle of\ndifferential technological development: it is no good accelerating the\ndevelopment of a desirable technology _Y_ if the only way of getting _Y_ is by\ndeveloping an extremely undesirable precursor technology _X_ , or if getting\n_Y_ would immediately produce an extremely undesirable related technology _Z_.\nBefore you marry your sweetheart, consider the prospective in-laws.\n\nIn the case of whole brain emulation, the degree of technology coupling is\ndebatable. We noted in [Chapter\n2](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos104440) that while\nwhole brain emulation would require massive progress in various enabling\ntechnologies, it might not require any major new theoretical insight. In\nparticular, it does not require that we understand how human cognition works,\nonly that we know how to build computational models of small parts of the\nbrain, such as different species of neuron. Nevertheless, in the course of\ndeveloping the ability to emulate human brains, a wealth of neuroanatomical\ndata would be collected, and functional models of cortical networks would\nsurely be greatly improved. Such progress would seem to have a good chance of\nenabling neuromorphic AI before full-blown whole brain\nemulation.[12](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1134143)\nHistorically, there are quite a few examples of AI techniques gleaned from\nneuroscience or biology. (For example: the McCulloch–Pitts neuron,\nperceptrons, and other artificial neurons and neural networks, inspired by\nneuroanatomical work; reinforcement learning, inspired by behaviorist\npsychology; genetic algorithms, inspired by evolution theory; subsumption\narchitectures and perceptual hierarchies, inspired by cognitive science\ntheories about motor planning and sensory perception; artificial immune\nsystems, inspired by theoretical immunology; swarm intelligence, inspired by\nthe ecology of insect colonies and other self-organizing systems; and reactive\nand behavior-based control in robotics, inspired by the study of animal\nlocomotion.) Perhaps more significantly, there are plenty of important AI-\nrelevant questions that could potentially be answered through further study of\nthe brain. (For example: How does the brain store structured representations\nin working memory and long-term memory? How is the binding problem solved?\nWhat is the neural code? How are concepts represented? Is there some standard\nunit of cortical processing machinery, such as the cortical column, and if so\nhow is it wired and how does its functionality depend on the wiring? How can\nsuch columns be linked up, and how can they learn?)\n\nWe will shortly have more to say about the relative danger of whole brain\nemulation, neuromorphic AI, and synthetic AI, but we can already flag another\nimportant technology coupling: that between whole brain emulation and AI. Even\nif a push toward whole brain emulation actually resulted in whole brain\nemulation (as opposed to neuromorphic AI), and even if the arrival of whole\nbrain emulation could be safely handled, a further risk would still remain:\nthe risk associated with _a second transition_ , a transition from whole brain\nemulation to AI, which is an ultimately more powerful form of machine\nintelligence.\n\nThere are many other technology couplings, which could be considered in a more\ncomprehensive analysis. For instance, a push toward whole brain emulation\nwould boost neuroscience progress more\ngenerally.[13](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1134722)\nThat might produce various effects, such as faster progress toward lie\ndetection, neuropsychological manipulation techniques, cognitive enhancement,\nand assorted medical advances. Likewise, a push toward cognitive enhancement\nmight (depending on the specific path pursued) create spillovers such as\nfaster development of genetic selection and genetic engineering methods not\nonly for enhancing cognition but for modifying other traits as well.\n\n#### [Second-\nguessing](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_006.html#filepos27548)\n\nWe encounter another layer of strategic complexity if we take into account\nthat there is no perfectly benevolent, rational, and unified world controller\nwho simply implements what has been discovered to be the best option. Any\nabstract point about “what should be done” must be embodied in the form of a\nconcrete message, which is entered into the arena of rhetorical and political\nreality. There it will be ignored, misunderstood, distorted, or appropriated\nfor various conflicting purposes; it will bounce around like a pinball,\ncausing actions and reactions, ushering in a cascade of consequences, the\nupshot of which need bear no straightforward relationship to the intentions of\nthe original sender.\n\nA sophisticated operator might try to anticipate these kinds of effect.\nConsider, for example, the following argument template for proceeding with\nresearch to develop a dangerous technology _X_. (One argument fitting this\ntemplate can be found in the writings of Eric Drexler. In Drexler’s case, _X_\n= molecular\nnanotechnology.[14](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1135275))\n\n**1** The risks of _X_ are great.\n\n**2** Reducing these risks will require a period of serious preparation.\n\n**3** Serious preparation will begin only once the prospect of _X_ is taken\nseriously by broad sectors of society.\n\n**4** Broad sectors of society will take the prospect of _X_ seriously only\nonce a large research effort to develop _X_ is underway.\n\n**5** The earlier a serious research effort is initiated, the longer it will\ntake to deliver _X_ (because it starts from a lower level of pre-existing\nenabling technologies).\n\n**6** Therefore, the earlier a serious research effort is initiated, the\nlonger the period during which serious preparation will be taking place, and\nthe greater the reduction of the risks.\n\n**7** Therefore, a serious research effort toward _X_ should be initiated\nimmediately.\n\nWhat initially looks like a reason for going slow or stopping—the risks of _X_\nbeing great—ends up, on this line of thinking, as a reason for the opposite\nconclusion.\n\nA related type of argument is that we ought—rather callously—to welcome small\nand medium-scale catastrophes on grounds that they make us aware of our\nvulnerabilities and spur us into taking precautions that reduce the\nprobability of an existential catastrophe. The idea is that a small or medium-\nscale catastrophe acts like an inoculation, challenging civilization with a\nrelatively survivable form of a threat and stimulating an immune response that\nreadies the world to deal with the existential variety of the\nthreat.[15](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1135672)\n\nThese “shock’em-into-reacting” arguments advocate letting something bad happen\nin the hope that it will galvanize a public reaction. We mention them here not\nto endorse them, but as a way to introduce the idea of (what we will term)\n“second-guessing arguments.” Such arguments maintain that by treating others\nas irrational and playing to their biases and misconceptions it is possible to\nelicit a response from them that is more competent than if a case had been\npresented honestly and forthrightly to their rational faculties.\n\nIt may seem unfeasibly difficult to use the kind of stratagems recommended by\nsecond-guessing arguments to achieve long-term global goals. How could anybody\npredict the final course of a message after it has been jolted hither and\nthither in the pinball machine of public discourse? Doing so would seem to\nrequire predicting the rhetorical effects on myriad constituents with varied\nidiosyncrasies and fluctuating levels of influence over long periods of time\nduring which the system may be perturbed by unanticipated events from the\noutside while its topology is also undergoing a continuous endogenous\nreorganization: surely an impossible\ntask![16](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1136206)\nHowever, it may not be necessary to make detailed predictions about the\nsystem’s entire future trajectory in order to identify an intervention that\ncan be reasonably expected to increase the chances of a certain long-term\noutcome. One might, for example, consider only the relatively near-term and\npredictable effects in a detailed way, selecting an action that does well in\nregard to those, while modeling the system’s behavior beyond the\npredictability horizon as a random walk.\n\nThere may, however, be a moral case for de-emphasizing or refraining from\nsecond-guessing moves. Trying to outwit one another looks like a zero-sum\ngame—or negative-sum, when one considers the time and energy that would be\ndissipated by the practice as well as the likelihood that it would make it\ngenerally harder for anybody to discover what others truly think and to be\ntrusted when expressing their own\nopinions.[17](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1136340)\nA full-throttled deployment of the practices of strategic communication would\nkill candor and leave truth bereft to fend for herself in the backstabbing\nnight of political bogeys.\n\n### [Pathways and\nenablers](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_006.html#filepos27650)\n\nShould we celebrate advances in computer hardware? What about advances on the\npath toward whole brain emulation? We will look at these two questions in\nturn.\n\n#### [Effects of hardware\nprogress](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_006.html#filepos27749)\n\nFaster computers make it easier to create machine intelligence. One effect of\naccelerating progress in hardware, therefore, is to hasten the arrival of\nmachine intelligence. As discussed earlier, this is probably a bad thing from\nthe impersonal perspective, since it reduces the amount of time available for\nsolving the control problem and for humanity to reach a more mature stage of\ncivilization. The case is not a slam dunk, though. Since superintelligence\nwould eliminate many other existential risks, there could be reason to prefer\nearlier development if the level of these other existential risks were very\nhigh.[18](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1136455)\n\nHastening or delaying the onset of the intelligence explosion is not the only\nchannel through which the rate of hardware progress can affect existential\nrisk. Another channel is that hardware can to some extent substitute for\nsoftware; thus, better hardware reduces the minimum skill required to code a\nseed AI. Fast computers might also encourage the use of approaches that rely\nmore heavily on brute-force techniques (such as genetic algorithms and other\ngenerate-evaluate-discard methods) and less on techniques that require deep\nunderstanding to use. If brute-force techniques lend themselves to more\nanarchic or imprecise system designs, where the control problem is harder to\nsolve than in more precisely engineered and theoretically controlled systems,\nthis would be another way in which faster computers would increase the\nexistential risk.\n\nAnother consideration is that rapid hardware progress increases the likelihood\nof a fast takeoff. The more rapidly the state of the art advances in the\nsemiconductor industry, the fewer the person-hours of programmers’ time spent\nexploiting the capabilities of computers at any given performance level. This\nmeans that an intelligence explosion is less likely to be initiated at the\nlowest level of hardware performance at which it is feasible. An intelligence\nexplosion is thus _more_ likely to be initiated when hardware has advanced\nsignificantly beyond the minimum level at which the eventually successful\nprogramming approach could first have succeeded. There is then a hardware\noverhang when the takeoff eventually does occur. As we saw in [Chapter\n4](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_011.html#filepos236122), hardware\noverhang is one of the main factors that reduce recalcitrance during the\ntakeoff. Rapid hardware progress, therefore, will tend to make the transition\nto superintelligence faster and more explosive.\n\nA faster takeoff via a hardware overhang can affect the risks of the\ntransition in several ways. The most obvious is that a faster takeoff offers\nless opportunity to respond and make adjustments whilst the transition is in\nprogress, which would tend to increase risk. A related consideration is that a\nhardware overhang would reduce the chances that a dangerously self-improving\nseed AI could be contained by limiting its ability to colonize sufficient\nhardware: the faster each processor is, the fewer processors would be needed\nfor the AI to quickly bootstrap itself to superintelligence. Yet another\neffect of a hardware overhang is to level the playing field between big and\nsmall projects by reducing the importance of one of the advantages of larger\nprojects—the ability to afford more powerful computers. This effect, too,\nmight increase existential risk, if larger projects are more likely to solve\nthe control problem and to be pursuing morally acceptable\nobjectives.[19](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1136936)\n\nThere are also advantages to a faster takeoff. A faster takeoff would increase\nthe likelihood that a singleton will form. If establishing a singleton is\nsufficiently important for solving the post-transition coordination problems,\nit might be worth accepting a greater risk during the intelligence explosion\nin order to mitigate the risk of catastrophic coordination failures in its\naftermath.\n\nDevelopments in computing can affect the outcome of a machine intelligence\nrevolution not only by playing a direct role in the construction of machine\nintelligence but also by having diffuse effects on society that indirectly\nhelp shape the initial conditions of the intelligence explosion. The Internet,\nwhich required hardware to be good enough to enable personal computers to be\nmass produced at low cost, is now influencing human activity in many areas,\nincluding work in artificial intelligence and research on the control problem.\n(This book might not have been written, and you might not have found it,\nwithout the Internet.) However, hardware is already good enough for a great\nmany applications that could facilitate human communication and deliberation,\nand it is not clear that the pace of progress in these areas is strongly\nbottlenecked by the rate of hardware\nimprovement.[20](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1137632)\n\nOn balance, it appears that faster progress in computing hardware is\nundesirable from the impersonal evaluative standpoint. This tentative\nconclusion could be overturned, for example if the threats from other\nexistential risks or from post-transition coordination failures turn out to be\nextremely large. In any case, it seems difficult to have much leverage on the\nrate of hardware advancement. Our efforts to improve the initial conditions\nfor the intelligence explosion should therefore probably focus on other\nparameters.\n\nNote that even when we cannot see how to influence some parameter, it can be\nuseful to determine its “sign” (i.e. whether an increase or decrease in that\nparameter would be desirable) as a preliminary step in mapping the strategic\nlay of the land. We might later discover a new leverage point that does enable\nus to manipulate the parameter more easily. Or we might discover that the\nparameter’s sign correlates with the sign of some other more manipulable\nparameter, so that our initial analysis helps us decide what to do with this\nother parameter.\n\n#### [Should whole brain emulation research be\npromoted?](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_006.html#filepos27864)\n\nThe harder it seems to solve the control problem for artificial intelligence,\nthe more tempting it is to promote the whole brain emulation path as a less\nrisky alternative. There are several issues, however, that must be analyzed\nbefore one can arrive at a well-considered\njudgment.[21](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1138062)\n\nFirst, there is the issue of technology coupling, already discussed earlier.\nWe pointed out that an effort to develop whole brain emulation could result in\nneuromorphic AI instead, a form of machine intelligence that may be especially\nunsafe.\n\nBut let us assume, for the sake of argument, that we actually achieve whole\nbrain emulation (WBE). Would this be safer than AI? This, itself, is a\ncomplicated issue. There are at least three _putative_ advantages of WBE: (i)\nthat its performance characteristics would be better understood than those of\nAI; (ii) that it would inherit human motives; and (iii) that it would result\nin a slower takeoff. Let us very briefly reflect on each.\n\ni That it should be easier to understand the intellectual performance\ncharacteristics of an emulation than of an AI sounds plausible. We have\nabundant experience with the strengths and weaknesses of human intelligence\nbut no experience with human-level artificial intelligence. However, to\nunderstand what a snapshot of a digitized human intellect can and cannot do is\nnot the same as to understand how such an intellect will respond to\nmodifications aimed at enhancing its performance. An artificial intellect, by\ncontrast, might be carefully designed to be understandable, in both its static\nand dynamic dispositions. So while whole brain emulation may be more\npredictable in its intellectual performance than a generic AI at a comparable\nstage of development, it is unclear whether whole brain emulation would be\ndynamically more predictable than an AI engineered by competent safety-\nconscious programmers.\n\nii As for an emulation inheriting the motivations of its human template, this\nis far from guaranteed. Capturing human evaluative dispositions might require\na very high-fidelity emulation. Even if some individual’s motivations _were_\nperfectly captured, it is unclear how much safety would be purchased. Humans\ncan be untrustworthy, selfish, and cruel. While templates would hopefully be\nselected for exceptional virtue, it may be hard to foretell how someone will\nact when transplanted into radically alien circumstances, superhumanly\nenhanced in intelligence, and tempted with an opportunity for world\ndomination. It is true that emulations would at least be more likely to have\n_human-like_ motivations (as opposed to valuing only paperclips or discovering\ndigits of pi). Depending on one’s views on human nature, this might or might\nnot be\nreassuring.[22](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1138499)\n\niii It is not clear why whole brain emulation should result in a slower\ntakeoff than artificial intelligence. Perhaps with whole brain emulation one\nshould expect less hardware overhang, since whole brain emulation is less\ncomputationally efficient than artificial intelligence can be. Perhaps, also,\nan AI system could more easily absorb all available computing power into one\ngiant integrated intellect, whereas whole brain emulation would forego quality\nsuperintelligence and pull ahead of humanity only in speed and size of\npopulation. If whole brain emulation does lead to a slower takeoff, this could\nhave benefits in terms of alleviating the control problem. A slower takeoff\nwould also make a multipolar outcome more likely. But whether a multipolar\noutcome is desirable is very doubtful.\n\nThere is another important complication with the general idea that getting\nwhole brain emulation first is safer: the need to cope with a _second\ntransition_. Even if the first form of human-level machine intelligence is\nemulation-based, it would still remain feasible to develop artificial\nintelligence. AI in its mature form has important advantages over WBE, making\nAI the ultimately more powerful\ntechnology.[23](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1138905)\nWhile mature AI would render WBE obsolete (except for the special purpose of\npreserving individual human minds), the reverse does not hold.\n\nWhat this means is that if AI is developed first, there might be a single wave\nof the intelligence explosion. But if WBE is developed first, there may be two\nwaves: first, the arrival of WBE; and later, the arrival of AI. The total\nexistential risk along the WBE-first path is the _sum_ of the risk in the\nfirst transition and the risk in the second transition (conditional on having\nmade it through the first); see Figure\n13.[24](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1139872)\n\nHow much safer would the AI transition be in a WBE world? One consideration is\nthat the AI transition would be less explosive if it occurs after some form of\nmachine intelligence has already been realized. Emulations, running at digital\nspeeds and in numbers that might far exceed the biological human population,\nwould reduce the cognitive differential, making it easier for emulations to\ncontrol the AI. This consideration is not too weighty, since the gap between\nAI and WBE could still be wide. However, if the emulations were not just\nfaster and more numerous but also somewhat qualitatively smarter than\nbiological humans (or at least drawn from the top end of the human\ndistribution) then the WBE-first scenario would have advantages paralleling\nthose of human cognitive enhancement, which we discussed above.\n\n![Image](images/00039.jpg)\n\n**Figure 13** Artificial intelligence or whole brain emulation first? In an\nAI-first scenario, there is one transition that creates an existential risk.\nIn a WBE-first scenario, there are two risky transitions, first the\ndevelopment of WBE and then the development of AI. The total existential risk\nalong the WBE-first scenario is the sum of these. However, the risk of an AI\ntransition might be lower if it occurs in a world where WBE has already been\nsuccessfully introduced.\n\nAnother consideration is that the transition to WBE would extend the lead of\nthe frontrunner. Consider a scenario in which the frontrunner has a six-month\nlead over the closest follower in developing whole brain emulation technology.\nSuppose that the first emulations to be created are cooperative, safety-\nfocused, and patient. If they run on fast hardware, these emulations could\nspend subjective eons pondering how to create safe AI. For example, if they\nrun at a speedup of 100,000× and are able to work on the control problem\nundisturbed for six months of sidereal time, they could hammer away at the\ncontrol problem for fifty millennia before facing competition from other\nemulations. Given sufficient hardware, they could hasten their progress by\nfanning out myriad copies to work independently on subproblems. If the\nfrontrunner uses its six-month lead to form a singleton, it could buy its\nemulation AI-development team an unlimited amount of time to work on the\ncontrol\nproblem.[25](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1140226)\n\nOn balance, it looks like the risk of the AI transition would be reduced if\nWBE comes before AI. However, when we combine the residual risk in the AI\ntransition with the risk of an antecedent WBE transition, it becomes very\nunclear how the total existential risk along the WBE-first path stacks up\nagainst the risk along the AI-first path. Only if one is quite pessimistic\nabout biological humanity’s ability to manage an AI transition—after taking\ninto account that human nature or civilization might have improved by the time\nwe confront this challenge—should the WBE-first path seem attractive.\n\nTo figure out whether whole brain emulation technology should be promoted,\nthere are some further important points to place in the balance. Most\nsignificantly, there is the technology coupling mentioned earlier: a push\ntoward WBE could instead produce neuromorphic AI. This is a reason against\npushing for\nWBE.[26](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1140612) No\ndoubt, there are _some_ synthetic AI designs that are less safe than _some_\nneuromorphic designs. In expectation, however, it seems that neuromorphic\ndesigns are less safe. One ground for this is that imitation can substitute\nfor understanding. To build something from the ground up one must usually have\na reasonably good understanding of how the system will work. Such\nunderstanding may not be necessary to merely copy features of an existing\nsystem. Whole brain emulation relies on wholesale copying of biology, which\nmay not require a comprehensive computational systems-level understanding of\ncognition (though a large amount of component-level understanding would\nundoubtedly be needed). Neuromorphic AI may be like whole brain emulation in\nthis regard: it would be achieved by cobbling together pieces plagiarized from\nbiology without the engineers necessarily having a deep mathematical\nunderstanding of how the system works. But neuromorphic AI would be _unlike_\nwhole brain emulation in another regard: it would not have human motivations\nby\ndefault.[27](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1141676)\nThis consideration argues against pursuing the whole brain emulation approach\nto the extent that it would likely produce neuromorphic AI.\n\nA second point to put in the balance is that WBE is more likely to give us\nadvance notice of its arrival. With AI it is always possible that somebody\nwill make an unexpected conceptual breakthrough. WBE, by contrast, will\nrequire many laborious precursor steps—high-throughput scanning facilities,\nimage processing software, detailed neural modeling work. We can therefore be\nconfident that WBE is not imminent (not less than, say, fifteen or twenty\nyears away). This means that efforts to accelerate WBE will make a difference\nmainly in scenarios in which machine intelligence is developed comparatively\nlate. This could make WBE investments attractive to somebody who wants the\nintelligence explosion to preempt other existential risks but is wary of\nsupporting AI for fear of triggering an intelligence explosion prematurely,\nbefore the control problem has been solved. However, the uncertainty over the\nrelevant timescales is probably currently too large to enable this\nconsideration to carry much\nweight.[28](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1142119)\n\nA strategy of promoting WBE is thus most attractive if (a) one is very\npessimistic about humans solving the control problem for AI, (b) one is not\ntoo worried about neuromorphic AI, multipolar outcomes, or the risks of a\nsecond transition, (c) one thinks that the default timing of WBE and AI is\nclose, and (d) one prefers superintelligence to be developed neither very late\nnor very early.\n\n#### [The person-affecting perspective favors\nspeed](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_006.html#filepos28001)\n\nI fear the blog commenter “washbash” may speak for many when he or she writes:\n\n> I instinctively think go faster. Not because I think this is better for the\n> world. Why should I care about the world when I am dead and gone? I want it\n> to go fast, damn it! This increases the chance I have of experiencing a more\n> technologically advanced\n> future.[29](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1142846)\n\nFrom the person-affecting standpoint, we have greater reason to rush forward\nwith all manner of radical technologies that could pose existential risks.\nThis is because the default outcome is that almost everyone who now exists is\ndead within a century.\n\nThe case for rushing is especially strong with regard to technologies that\ncould extend our lives and thereby increase the expected fraction of the\ncurrently existing population that may still be around for the intelligence\nexplosion. If the machine intelligence revolution goes well, the resulting\nsuperintelligence could almost certainly devise means to indefinitely prolong\nthe lives of the then still-existing humans, not only keeping them alive but\nrestoring them to health and youthful vigor, and enhancing their capacities\nwell beyond what we currently think of as the human range; or helping them\nshuffle off their mortal coils altogether by uploading their minds to a\ndigital substrate and endowing their liberated spirits with exquisitely good-\nfeeling virtual embodiments. With regard to technologies that do not promise\nto save lives, the case for rushing is weaker, though perhaps still\nsufficiently supported by the hope of raised standards of\nliving.[30](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1142967)\n\nThe same line of reasoning makes the person-affecting perspective favor many\nrisky technological innovations that promise to hasten the onset of the\nintelligence explosion, even when those innovations are disfavored in the\nimpersonal perspective. Such innovations could shorten the wolf hours during\nwhich we individually must hang on to our perch if we are to live to see the\ndaybreak of the posthuman age. From the person-affecting standpoint, faster\nhardware progress thus seems desirable, as does faster progress toward WBE.\nAny adverse effect on existential risk is probably outweighed by the personal\nbenefit of an increased chance of the intelligence explosion happening in the\nlifetime of currently existing\npeople.[31](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1143411)\n\n###\n[Collaboration](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_006.html#filepos28133)\n\nOne important parameter is the degree to which the world will manage to\ncoordinate and collaborate in the development of machine intelligence.\nCollaboration would bring many benefits. Let us take a look at how this\nparameter might affect the outcome and what levers we might have for\nincreasing the extent and intensity of collaboration.\n\n#### [The race dynamic and its\nperils](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_006.html#filepos28224)\n\nA race dynamic exists when one project fears being overtaken by another. This\ndoes not require the actual existence of multiple projects. A situation with\nonly one project could exhibit a race dynamic if that project is unaware of\nits lack of competitors. The Allies would probably not have developed the\natomic bomb as quickly as they did had they not believed (erroneously) that\nthe Germans might be close to the same goal.\n\nThe severity of a race dynamic (that is, the extent to which competitors\nprioritize speed over safety) depends on several factors, such as the\ncloseness of the race, the relative importance of capability and luck, the\nnumber of competitors, whether competing teams are pursuing different\napproaches, and the degree to which projects share the same aims. Competitors’\nbeliefs about these factors are also relevant. (See Box 13.)\n\nIn the development of machine superintelligence, it seems likely that there\nwill be at least a mild race dynamic, and it is possible that there will be a\nsevere race dynamic. The race dynamic has important consequences for how we\nshould think about the strategic challenge posed by the possibility of an\nintelligence explosion.\n\nThe race dynamic could spur projects to move faster toward superintelligence\nwhile reducing investment in solving the control problem. Additional\ndetrimental effects of the race dynamic are also possible, such as direct\nhostilities between competitors. Suppose that two nations are racing to\ndevelop the first superintelligence, and that one of them is seen to be\npulling ahead. In a winner-takes-all situation, a lagging project might be\ntempted to launch a desperate strike against its rival rather than passively\nawait defeat. Anticipating this possibility, the frontrunner might be tempted\nto strike preemptively. If the antagonists are powerful states, the clash\ncould be\nbloody.[34](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1146731)\n(A “surgical strike” against the rival’s AI project might risk triggering a\nlarger confrontation and might in any case not be feasible if the host country\nhas taken\nprecautions.[35](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1146990))\n\n* * *\n\n### **Box 13 A risk-race to the bottom**\n\nConsider a hypothetical AI arms race in which several teams compete to develop\nsuperintelligence.[32](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1145849)\nEach team decides how much to invest in safety—knowing that resources spent on\ndeveloping safety precautions are resources not spent on developing the AI.\nAbsent a deal between all the competitors (which might be stymied by\nbargaining or enforcement difficulties), there might then be a risk-race to\nthe bottom, driving each team to take only a minimum of precautions.\n\nOne can model each team’s performance as a function of its capability\n(measuring its raw ability and luck) and a penalty term corresponding to the\ncost of its safety precautions. The team with the highest performance builds\nthe first AI. The riskiness of that AI is determined by how much its creators\ninvested in safety. In the worst-case scenario, all teams have equal levels of\ncapability. The winner is then determined exclusively by investment in safety:\nthe team that took the fewest safety precautions wins. The Nash equilibrium\nfor this game is for every team to spend nothing on safety. In the real world,\nsuch a situation might arise via a _risk ratchet_ : some team, fearful of\nfalling behind, increments its risk-taking to catch up with its\ncompetitors—who respond in kind, until the maximum level of risk is reached.\n\n#### Capability versus risk\n\nThe situation changes when there are variations in capability. As variations\nin capability become more important relative to the cost of safety\nprecautions, the risk ratchet weakens: there is less incentive to incur an\nextra bit of risk if doing so is unlikely to change the order of the race.\nThis is illustrated under various scenarios in Figure 14, which plots how the\nriskiness of the AI depends on the importance of capability. Safety investment\nranges from 1 (resulting in perfectly safe AI) to 0 (completely unsafe AI).\nThe _x_ -axis represents the relative importance of capability versus safety\ninvestment in determining the speed of a team’s progress toward AI. (At 0.5,\nthe safety investment level is twice are important as capability; at 1, the\ntwo are equal; at 2, capability is twice as important as safety level; and so\nforth.) The _y_ -axis represents the level of AI risk (the expected fraction\nof their maximum utility that the winner of the race gets).\n\n![Image](images/00040.jpg)\n\n**Figure 14** Risk levels in AI technology races. Levels of risk of dangerous\nAI in a simple model of a technology race involving either (a) two teams or\n(b) five teams, plotted against the relative importance of capability (as\nopposed to investment in safety) in determining which project wins the race.\nThe graphs show three information-level scenarios: no capability information\n(straight), private capability information (dashed), and full capability\ninformation (dotted).\n\nWe see that, under all scenarios, the dangerousness of the resultant AI is\nmaximal when capability plays no role, gradually decreasing as capability\ngrows in importance.\n\n#### Compatible goals\n\nAnother way of reducing the risk is by giving teams more of a stake in each\nother’s success. If competitors are convinced that coming second means the\ntotal loss of everything they care about, they will take whatever risk\nnecessary to bypass their rivals. Conversely, teams will invest more in safety\nif less depends on winning the race. This suggests that we should encourage\nvarious forms of cross-investment.\n\n#### The number of competitors\n\nThe greater the number of competing teams, the more dangerous the race\nbecomes: each team, having less chance of coming first, is more willing to\nthrow caution to the wind. This can be seen by contrasting Figure 14a (two\nteams) with Figure 14b (five teams). In every scenario, more competitors means\nmore risk. Risk would be reduced if teams coalesce into a smaller number of\ncompeting coalitions.\n\n#### The curse of too much information\n\nIs it good if teams know about their positions in the race (knowing their\ncapability scores, for instance)? Here, opposing factors are at play. It is\ndesirable that a leader knows it is leading (so that it knows it has some\nmargin for additional safety precautions). Yet it is undesirable that a\nlaggard knows it has fallen behind (since this would confirm that it must cut\nback on safety to have any hope of catching up). While intuitively it may seem\nthis trade-off could go either way, the models are unequivocal: information is\n(in expectation)\nbad.[33](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1146320)\nFigures 14a and 14b each plot three scenarios: the straight lines correspond\nto situations in which no team knows any of the capability scores, its own\nincluded. The dashed lines show situations where each team knows its own\ncapability only. (In those situations, a team takes extra risk only if its\ncapability is low.) And the dotted lines show what happens when all teams know\neach other’s capabilities. (They take extra risks if their capability scores\nare close to one another.) With each increase in information level, the race\ndynamic becomes worse.\n\n* * *\n\nScenarios in which the rival developers are not states but smaller entities,\nsuch as corporate labs or academic teams, would probably feature much less\ndirect destruction from conflict. Yet the overall consequences of competition\nmay be almost as bad. This is because the main part of the expected harm from\ncompetition stems not from the smashup of battle but from the downgrade of\nprecaution. A race dynamic would, as we saw, reduce investment in safety; and\nconflict, even if nonviolent, would tend to scotch opportunities for\ncollaboration, since projects would be less likely to share ideas for solving\nthe control problem in a climate of hostility and\nmistrust.[36](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1147543)\n\n#### [On the benefits of\ncollaboration](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_006.html#filepos28342)\n\nCollaboration thus offers many benefits. It reduces the haste in developing\nmachine intelligence. It allows for greater investment in safety. It avoids\nviolent conflicts. And it facilitates the sharing of ideas about how to solve\nthe control problem. To these benefits we can add another: collaboration would\ntend to produce outcomes in which the fruits of a successfully controlled\nintelligence explosion get distributed more equitably.\n\nThat broader collaboration should result in wider sharing of gains is not\naxiomatic. In principle, a small project run by an altruist could lead to an\noutcome where the benefits are shared evenly or equitably among all morally\nconsiderable beings. Nevertheless, there are several reasons to suppose that\nbroader collaborations, involving a greater number of sponsors, are (in\nexpectation) distributionally superior. One such reason is that sponsors\npresumably prefer an outcome in which they themselves get (at least) their\nfair share. A broad collaboration then means that relatively many individuals\nget at least their fair share, assuming the project is successful. Another\nreason is that a broad collaboration also seems likelier to benefit people\noutside the collaboration. A broader collaboration contains more members, so\nmore outsiders would have personal ties to somebody on the inside looking out\nfor their interests. A broader collaboration is also more likely to include at\nleast some altruist who wants to benefit everyone. Furthermore, a broader\ncollaboration is more likely to operate under public oversight, which might\nreduce the risk of the entire pie being captured by a clique of programmers or\nprivate\ninvestors.[37](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1149188)\nNote also that the larger the successful collaboration is, the lower the costs\nto it of extending the benefits to all outsiders. (For instance, if 90% of all\npeople were already inside the collaboration, it would cost them no more than\n10% of their holdings to bring all outsiders up to their own level.)\n\nIt is thus plausible that broader collaborations would tend to lead to a wider\ndistribution of the gains (though _some_ projects with few sponsors might also\nhave distributionally excellent aims). But why is a wide distribution of gains\ndesirable?\n\nThere are both moral and prudential reasons for favoring outcomes in which\neverybody gets a share of the bounty. We will not say much about the moral\ncase, except to note that it need not rest on any egalitarian principle. The\ncase might be made, for example, on grounds of fairness. A project that\ncreates machine superintelligence imposes a global risk externality. Everybody\non the planet is placed in jeopardy, including those who do not consent to\nhaving their own lives and those of their family imperiled in this way. Since\neverybody shares the risk, it would seem to be a minimal requirement of\nfairness that everybody also gets a share of the upside.\n\nThe fact that the total (expected) amount of good seems greater in\ncollaboration scenarios is another important reason such scenarios are morally\npreferable.\n\nThe prudential case for favoring a wide distribution of gains is two-pronged.\nOne prong is that wide distribution should promote collaboration, thereby\nmitigating the negative consequences of the race dynamic. There is less\nincentive to fight over who gets to build the first superintelligence if\neverybody stands to benefit equally from any project’s success. The sponsors\nof a particular project might also benefit from credibly signaling their\ncommitment to distributing the spoils universally, a certifiably altruistic\nproject being likely to attract more supporters and fewer\nenemies.[38](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1149877)\n\nThe other prong of the prudential case for favoring a wide distribution of\ngains has to do with whether agents are risk-averse or have utility functions\nthat are sublinear in resources. The central fact here is the enormousness of\nthe potential resource pie. Assuming the observable universe is as uninhabited\nas it looks, it contains more than one vacant galaxy for each human being\nalive. Most people would much rather have certain access to one galaxy’s worth\nof resources than a lottery ticket offering a one-in-a-billion chance of\nowning a billion\ngalaxies.[39](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1150205)\nGiven the astronomical size of humanity’s cosmic endowment, it seems that\nself-interest should generally favor deals that would guarantee each person a\nshare, even if each share corresponded to a small fraction of the total. The\nimportant thing, when such an extravagant bonanza is in the offing, is to not\nbe left out in the cold.\n\nThis argument from the enormousness of the resource pie presupposes that\npreferences are resource-\nsatiable.[40](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1150606)\nThat supposition does not necessarily hold. For instance, several prominent\nethical theories—including especially aggregative consequentialist\ntheories—correspond to utility functions that are risk-neutral and linear in\nresources. A billion galaxies could be used to create a billion times more\nhappy lives than a single galaxy. They are thus, to a utilitarian, worth a\nbillion times as\nmuch.[41](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1150722)\nOrdinary selfish human preference functions, however, appear to be relatively\nresource-satiable.\n\nThis last statement must be flanked by two important qualifications. The first\nis that many people care about rank. If multiple agents each wants to top the\nForbes rich list, then no resource pie is large enough to give everybody full\nsatisfaction.\n\nThe second qualification is that the post-transition technology base would\nenable material resources to be converted into an unprecedented range of\nproducts, including some goods that are not currently available at any price\neven though they are highly valued by many humans. A billionaire does not live\na thousand times longer than a millionaire. In the era of digital minds,\nhowever, the billionaire could afford a thousandfold more computing power and\ncould thus enjoy a thousandfold longer subjective lifespan. Mental capacity,\nlikewise, could be for sale. In such circumstances, with economic capital\nconvertible into vital goods at a constant rate even for great levels of\nwealth, unbounded greed would make more sense than it does in today’s world\nwhere the affluent (those among them lacking a philanthropic heart) are\nreduced to spending their riches on airplanes, boats, art collections, or a\nfourth and a fifth residence.\n\nDoes this mean that an egoist should be risk-neutral with respect to his or\nher post-transition resource endowment? Not quite. Physical resources may not\nbe convertible into lifespan or mental performance at arbitrary scales. If a\nlife must be lived sequentially, so that observer moments can remember earlier\nevents and be affected by prior choices, then the life of a digital mind\ncannot be extended arbitrarily without utilizing an increasing number of\n_sequential_ computational operations. But physics limits the extent to which\nresources can be transformed into sequential\ncomputations.[42](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1151073)\nThe limits on sequential computation may also constrain some aspects of\ncognitive performance to scale radically sublinearly beyond a relatively\nmodest resource endowment. Furthermore, it is not obvious that an egoist would\nor should be risk-neutral even with regard to highly normatively relevant\noutcome metrics such as number of quality-adjusted subjective life years. If\noffered the choice between an extra 2,000 years of life for certain and a one-\nin-ten chance of an extra 30,000 years of life, I think most people would\nselect the former (even under the stipulation that each life year would be of\nequal\nquality).[43](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1151675)\n\nIn reality, the prudential case for favoring a wide distribution of gains is\npresumably subject-relative and situation-dependent. Yet, on the whole, people\nwould be more likely to get (almost all of) what they want if a way is found\nto achieve a wide distribution—and this holds even before taking into account\nthat a commitment to a wider distribution would tend to foster collaboration\nand thereby increase the chances of avoiding existential catastrophe. Favoring\na broad distribution, therefore, appears to be not only morally mandated but\nalso prudentially advisable.\n\nThere is a further set of consequences to collaboration that should be given\nat least some shrift: the possibility that pre-transition collaboration\ninfluences the level of post-transition collaboration. Assume humanity solves\nthe control problem. (If the control problem is not solved, it may scarcely\nmatter how much collaboration there is post transition.) There are two cases\nto consider. The first is that the intelligence explosion does _not_ create a\nwinner-takes-all dynamic (presumably because the takeoff is relatively slow).\nIn this case it is plausible that if pre-transition collaboration has any\nsystematic effect on post-transition collaboration, it has a positive effect,\ntending to promote subsequent collaboration. The original collaborative\nrelationships may endure and continue beyond the transition; also, pre-\ntransition collaboration may offer more opportunity for people to steer\ndevelopments in desirable (and, presumably, more collaborative) post-\ntransition directions.\n\nThe second case is that the nature of the intelligence explosion does\nencourage a winner-takes-all dynamic (presumably because the takeoff is\nrelatively fast). In this case, if there is no extensive collaboration before\nthe takeoff, a singleton is likely to emerge—a single project would undergo\nthe transition alone, at some point obtaining a decisive strategic advantage\ncombined with superintelligence. A singleton, by definition, is a highly\ncollaborative social\norder.[44](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1152214)\nThe absence of extensive collaboration pre-transition would thus lead to an\nextreme degree of collaboration post-transition. By contrast, a somewhat\nhigher level of collaboration in the run-up to the intelligence explosion\nopens up a wider variety of possible outcomes. Collaborating projects could\nsynchronize their ascent to ensure they transition in tandem without any of\nthem getting a decisive strategic advantage. Or different sponsor groups might\nmerge their efforts into a single project, while refusing to give that project\na mandate to form a singleton. For example, one could imagine a consortium of\nnations forming a joint scientific project to develop machine\nsuperintelligence, yet not authorizing this project to evolve into anything\nlike a supercharged United Nations, electing instead to maintain the factious\nworld order that existed before.\n\nParticularly in the case of a fast takeoff, therefore, the possibility exists\nthat greater pre-transition collaboration would result in less post-transition\ncollaboration. However, to the extent that collaborating entities are able to\nshape the outcome, they may allow the emergence or continuation of non-\ncollaboration only if they foresee that no catastrophic consequences would\nfollow from post-transition factiousness. Scenarios in which pre-transition\ncollaboration leads to reduced post-transition collaboration may therefore\nmostly be ones in which reduced post-transition collaboration is innocuous.\n\nIn general, greater post-transition collaboration appears desirable. It would\nreduce the risk of dystopian dynamics in which economic competition and a\nrapidly expanding population lead to a Malthusian condition, or in which\nevolutionary selection erodes human values and selects for non-eudaemonic\nforms, or in which rival powers suffer other coordination failures such as\nwars and technology races. The last of these issues, the prospect of\ntechnology races, may be particularly problematic if the transition is to an\nintermediary form of machine intelligence (whole brain emulation) since it\nwould create a new race dynamic that would harm the chances of the control\nproblem being solved for the subsequent second transition to a more advanced\nform of machine intelligence (artificial intelligence).\n\nWe described earlier how collaboration can reduce conflict in the run-up to\nthe intelligence explosion, increasing the chances that the control problem\nwill be solved, and improve both the moral legitimacy and the prudential\ndesirability of the resulting resource allocation. To these benefits of\ncollaboration it may thus be possible to add one more: that broader\ncollaboration pre-transition could help with important coordination problems\nin the post-transition era.\n\n#### [Working\ntogether](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_006.html#filepos28461)\n\nCollaboration can take different forms depending on the scale of the\ncollaborating entities. At a small scale, individual AI teams who believe\nthemselves to be in competition with one another could choose to pool their\nefforts.[45](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1152577)\nCorporations could merge or cross-invest. At a larger scale, states could join\nin a big international project. There are precedents to large-scale\ninternational collaboration in science and technology (such as CERN, the Human\nGenome Project, and the International Space Station), but an international\nproject to develop safe superintelligence would pose a different order of\nchallenge because of the security implications of the work. It would have to\nbe constituted not as an open academic collaboration but as an extremely\ntightly controlled joint enterprise. Perhaps the scientists involved would\nhave to be physically isolated and prevented from communicating with the rest\nof the world for the duration of the project, except through a single\ncarefully vetted communication channel. The required level of security might\nbe nearly unattainable at present, but advances in lie detection and\nsurveillance technology could make it feasible later this century. It is also\nworth bearing in mind that broad collaboration does not necessarily mean that\nlarge numbers of researchers would be involved in the project; it simply means\nthat many people would have a say in the project’s aims. In principle, a\nproject could involve a maximally broad collaboration comprising all of\nhumanity as sponsors (represented, let us say, by the General Assembly of the\nUnited Nations), yet employ only a single scientist to carry out the\nwork.[46](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1153005)\n\nThere is a reason for starting collaboration as early as possible, namely to\ntake advantage of the veil of ignorance that hides from our view any specific\ninformation about which individual project will get to superintelligence\nfirst. The closer to the finishing line we get, the less uncertainty will\nremain about the relative chances of competing projects; and the harder it may\nconsequently be to make a case based on the self-interest of the frontrunner\nto join a collaborative project that would distribute the benefits to all of\nhumanity. On the other hand, it also looks hard to establish a formal\ncollaboration of worldwide scope before the prospect of superintelligence has\nbecome much more widely recognized than it currently is and before there is a\nclearly visible road leading to the creation of machine superintelligence.\nMoreover, to the extent that collaboration would promote progress along that\nroad, it may actually be counterproductive in terms of safety, as discussed\nearlier.\n\nThe ideal form of collaboration for the present may therefore be one that does\nnot initially require specific formalized agreements and that does not\nexpedite advances in machine intelligence. One proposal that fits these\ncriteria is that we propound an appropriate moral norm, expressing our\ncommitment to the idea that superintelligence should be for the common good.\nSuch a norm could be formulated as follows:\n\n> The common good principle\n\n> Superintelligence should be developed only for the benefit of all of\n> humanity and in the service of widely shared ethical\n> ideals.[47](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1153115)\n\nEstablishing from an early stage that the immense potential of\nsuperintelligence belongs to all of humanity will give more time for such a\nnorm to become entrenched.\n\nThe common good principle does not preclude commercial incentives for\nindividuals or firms active in related areas. For example, a firm might\nsatisfy the call for universal sharing of the benefits of superintelligence by\nadopting a “windfall clause” to the effect that all profits up to some very\nhigh ceiling (say, a trillion dollars annually) would be distributed in the\nordinary way to the firm’s shareholders and other legal claimants, and that\nonly profits in excess of the threshold would be distributed to all of\nhumanity evenly (or otherwise according to universal moral criteria). Adopting\nsuch a windfall clause should be substantially costless, any given firm being\nextremely unlikely ever to exceed the stratospheric profit threshold (and such\nlow-probability scenarios ordinarily playing no role in the decisions of the\nfirm’s managers and investors). Yet its widespread adoption would give\nhumankind a valuable guarantee (insofar as the commitments could be trusted)\nthat if ever some private enterprise _were_ to hit the jackpot with the\nintelligence explosion, everybody would share in most of the benefits. The\nsame idea could be applied to entities other than firms. For example, states\ncould agree that if ever any one state’s GDP exceeds some very high fraction\n(say, 90%) of world GDP, the overshoot should be distributed evenly to\nall.[48](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1154732)\n\nThe common good principle (and particular instantiations, such as windfall\nclauses) could be adopted initially as a voluntary moral commitment by\nresponsible individuals and organizations that are active in areas related to\nmachine intelligence. Later, it could be endorsed by a wider set of entities\nand enacted into law and treaty. A vague formulation, such as the one given\nhere, may serve well as a starting point; but it would ultimately need to be\nsharpened into a set of specific verifiable requirements.\n\n\n## [CHAPTER 15  \nCrunch time](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_006.html#filepos28577)\n\n**We find ourselves in a thicket of strategic complexity, surrounded by a\ndense mist of uncertainty. Though many considerations have been discerned,\ntheir details and interrelationships remain unclear and iffy—and there might\nbe other factors we have not even thought of yet. What are we to do in this\npredicament?**\n\n### [Philosophy with a\ndeadline](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_006.html#filepos28703)\n\nA colleague of mine likes to point out that a Fields Medal (the highest honor\nin mathematics) indicates two things about the recipient: that he was capable\nof accomplishing something important, and that he didn’t. Though harsh, the\nremark hints at a truth.\n\nThink of a “discovery” as an act that moves the arrival of information from a\nlater point in time to an earlier time. The discovery’s value does not equal\nthe value of the information discovered but rather the value of having the\ninformation available earlier than it otherwise would have been. A scientist\nor a mathematician may show great skill by being the first to find a solution\nthat has eluded many others; yet if the problem would soon have been solved\nanyway, then the work probably has not much benefited the world. There _are_\ncases in which having a solution even slightly sooner is immensely valuable,\nbut this is most plausible when the solution is immediately put to use, either\nbeing deployed for some practical end or serving as a foundation to further\ntheoretical work. And in the latter case, where a solution is immediately used\nonly in the sense of serving as a building block for further theorizing, there\nis great value in obtaining a solution slightly sooner only if the further\nwork it enables is itself both important and\nurgent.[1](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1155614)\n\nThe question, then, is not whether the result discovered by the Fields\nMedalist is in itself “important” (whether instrumentally or for knowledge’s\nown sake). Rather, the question is whether it was important that the medalist\nenabled the publication of the result to occur at an earlier date. The value\nof this temporal transport should be compared to the value that a world-class\nmathematical mind could have generated by working on something else. At least\nin some cases, the Fields Medal might indicate a life spent solving the wrong\nproblem—for instance, a problem whose allure consisted primarily in being\nfamously difficult to solve.\n\nSimilar barbs could be directed at other fields, such as academic philosophy.\nPhilosophy covers some problems that are relevant to existential risk\nmitigation—we encountered several in this book. Yet there are also subfields\nwithin philosophy that have no apparent link to existential risk or indeed any\npractical concern. As with pure mathematics, some of the problems that\nphilosophy studies might be regarded as intrinsically important, in the sense\nthat humans have reason to care about them independently of any practical\napplication. The fundamental nature of reality, for instance, might be worth\nknowing about, for its own sake. The world would arguably be less glorious if\nnobody studied metaphysics, cosmology, or string theory. However, the dawning\nprospect of an intelligence explosion shines a new light on this ancient quest\nfor wisdom.\n\nThe outlook now suggests that philosophic progress can be maximized via an\nindirect path rather than by immediate philosophizing. One of the many tasks\non which superintelligence (or even just moderately enhanced human\nintelligence) would outperform the current cast of thinkers is in answering\nfundamental questions in science and philosophy. This reflection suggests a\nstrategy of deferred gratification. We could postpone work on some of the\neternal questions for a little while, delegating that task to our hopefully\nmore competent successors—in order to focus our own attention on a more\npressing challenge: increasing the chance that we will actually have competent\nsuccessors. This would be high-impact philosophy and high-impact\nmathematics.[2](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1155877)\n\n### [What is to be\ndone?](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_006.html#filepos28807)\n\nWe thus want to focus on problems that are not only important but urgent in\nthe sense that their solutions are needed prior to the intelligence explosion.\nWe should also take heed not to work on problems that are negative-value (such\nthat solving them is harmful). Some technical problems in the field of\nartificial intelligence, for instance, might be negative-value inasmuch as\ntheir solution would speed the development of machine intelligence without\ndoing as much to expedite the development of control methods that could render\nthe machine intelligence revolution survivable and beneficial.\n\nIt can be hard to identify problems that are both urgent and important and are\nsuch that we can confidently take them to be positive-value. The strategic\nuncertainty surrounding existential risk mitigation means that we must worry\nthat even well-intentioned interventions may turn out to be not only\nunproductive but counterproductive. To limit the risk of doing something\nactively harmful or morally wrong, we should prefer to work on problems that\nseem _robustly positive-value_ (i.e., whose solution would make a positive\ncontribution across a wide range of scenarios) and to employ means that are\nrobustly justifiable (i.e., acceptable from a wide range of moral views).\n\nThere is a further desideratum to consider in selecting which problems to\nprioritize. We want to work on problems that are _elastic_ to our efforts at\nsolving them. Highly elastic problems are those that can be solved much\nfaster, or solved to a much greater extent, given one extra unit of effort.\nEncouraging more kindness in the world is an important and urgent problem—one,\nmoreover, that seems quite robustly positive-value: yet absent a breakthrough\nidea for how to go about it, probably a problem of quite low elasticity.\nAchieving world peace, similarly, would be highly desirable; but considering\nthe numerous efforts already targeting that problem, and the formidable\nobstacles arrayed against a quick solution, it seems unlikely that the\ncontributions of a few extra individuals would make a large difference.\n\nTo reduce the risks of the machine intelligence revolution, we will propose\ntwo objectives that appear to best meet all those desiderata: strategic\nanalysis and capacity-building. We can be relatively confident about the sign\nof these parameters—more strategic insight and more capacity being better.\nFurthermore, the parameters are elastic: a small extra investment can make a\nrelatively large difference. Gaining insight and capacity is also urgent\nbecause early boosts to these parameters may compound, making subsequent\nefforts more effective. In addition to these two broad objectives, we will\npoint to a few other potentially worthwhile aims for initiatives.\n\n#### [Seeking the strategic\nlight](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_006.html#filepos28904)\n\nAgainst a backdrop of perplexity and uncertainty, analysis stands out as being\nof particularly high expected\nvalue.[3](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1156697)\nIllumination of our strategic situation would help us target subsequent\ninterventions more effectively. Strategic analysis is especially needful when\nwe are radically uncertain not just about some detail of some peripheral\nmatter but about the cardinal qualities of the central things. For many key\nparameters, we are radically uncertain even about their _sign_ —that is, we\nknow not which direction of change would be desirable and which undesirable.\nOur ignorance might not be irremediable. The field has been little prospected,\nand glimmering strategic insights could still be awaiting their unearthing\njust a few feet beneath the surface.\n\nWhat we mean by “strategic analysis” here is a search for _crucial\nconsiderations_ : ideas or arguments with the potential to change our views\nnot merely about the fine-structure of implementation but about the general\ntopology of\ndesirability.[4](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1157297)\nEven a single missed crucial consideration could vitiate our most valiant\nefforts or render them as actively harmful as those of a soldier who is\nfighting on the wrong side. The search for crucial considerations (which must\nexplore normative as well as descriptive issues) will often require\ncrisscrossing the boundaries between different academic disciplines and other\nfields of knowledge. As there is no established methodology for how to go\nabout this kind of research, difficult original thinking is necessary.\n\n#### [Building good\ncapacity](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_006.html#filepos29018)\n\nAnother high-value activity, one that shares with strategic analysis the\nrobustness property of being beneficial across a wide range of scenarios, is\nthe development of a well-constituted support base that takes the future\nseriously. Such a base can immediately provide resources for research and\nanalysis. If and when other priorities become visible, resources can be\nredirected accordingly. A support base is thus a general-purpose capability\nwhose use can be guided by new insights as they emerge.\n\nOne valuable asset would be a donor network comprising individuals devoted to\nrational philanthropy, informed about existential risk, and discerning about\nthe means of mitigation. It is especially desirable that the early-day funders\nbe astute and altruistic, because they may have opportunities to shape the\nfield’s culture before the usual venal interests take up position and\nentrench. The focus during these opening gambits should thus be to recruit the\nright kinds of people into the field. It could be worth foregoing some\ntechnical advances in the short term in order to fill the ranks with\nindividuals who genuinely care about safety and who have a truth-seeking\norientation (and who are likely to attract more of their own kind).\n\nOne important variable is the quality of the “social epistemology” of the AI-\nfield and its leading projects. Discovering crucial considerations is\nvaluable, but only if it affects action. This cannot always be taken for\ngranted. Imagine a project that invests millions of dollars and years of toil\nto develop a prototype AI, and that after surmounting many technical\nchallenges the system is finally beginning to show real progress. There is a\nchance that with just a bit more work it could turn into something useful and\nprofitable. Now a crucial consideration is discovered, indicating that a\ncompletely different approach would be a bit safer. Does the project kill\nitself off like a dishonored samurai, relinquishing its unsafe design and all\nthe progress that had been made? Or does it react like a worried octopus,\npuffing out a cloud of motivated skepticism in the hope of eluding the attack?\nA project that would reliably choose the samurai option in such a dilemma\nwould be a far preferable\ndeveloper.[5](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1157410)\nYet building processes and institutions that are willing to commit seppuku\nbased on uncertain allegations and speculative reasoning is not easy. Another\ndimension of social epistemology is the management of sensitive information,\nin particular the ability to avoid leaking information that ought be kept\nsecret. (Information continence may be especially challenging for academic\nresearchers, accustomed as they are to constantly disseminating their results\non every available lamppost and tree.)\n\n#### [Particular\nmeasures](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_006.html#filepos29127)\n\nIn addition to the general objectives of strategic light and good capacity,\nsome more specific objectives could also present cost-effective opportunities\nfor action.\n\nOne such is progress on the technical challenges of machine intelligence\nsafety. In pursing this objective, care should be taken to manage information\nhazards. Some work that would be useful for solving the control problem would\nalso be useful for solving the competence problem. Work that burns down the AI\nfuse could easily be a net negative.\n\nAnother specific objective is to promote “best practices” among AI\nresearchers. Whatever progress has been made on the control problem needs to\nbe disseminated. Some forms of computational experimentation, particularly if\ninvolving strong recursive self-improvement, may also require the use of\ncapability control to mitigate the risk of an accidental takeoff. While the\nactual implementation of safety methods is not so relevant today, it will\nincreasingly become so as the state of the art advances. And it is not too\nsoon to call for practitioners to express a _commitment to safety_ , including\nendorsing the common good principle and promising to ramp up safety if and\nwhen the prospect of machine superintelligence begins to look more imminent.\nPious words are not sufficient and will not by themselves make a dangerous\ntechnology safe: but where the mouth goeth, the mind might gradually follow.\n\nOther opportunities may also occasionally arise to push on some pivotal\nparameter, for example to mitigate some other existential risk, or to promote\nbiological cognitive enhancement and improvements of our collective wisdom, or\neven to shift world politics into a more harmonious register.\n\n### [Will the best in human nature please stand\nup](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_006.html#filepos29233)\n\nBefore the prospect of an intelligence explosion, we humans are like small\nchildren playing with a bomb. Such is the mismatch between the power of our\nplaything and the immaturity of our conduct. Superintelligence is a challenge\nfor which we are not ready now and will not be ready for a long time. We have\nlittle idea when the detonation will occur, though if we hold the device to\nour ear we can hear a faint ticking sound.\n\nFor a child with an undetonated bomb in its hands, a sensible thing to do\nwould be to put it down gently, quickly back out of the room, and contact the\nnearest adult. Yet what we have here is not one child but many, each with\naccess to an independent trigger mechanism. The chances that we will _all_\nfind the sense to put down the dangerous stuff seem almost negligible. Some\nlittle idiot is bound to press the ignite button just to see what happens.\n\nNor can we attain safety by running away, for the blast of an intelligence\nexplosion would bring down the entire firmament. Nor is there a grown-up in\nsight.\n\nIn this situation, any feeling of gee-wiz exhilaration would be out of place.\nConsternation and fear would be closer to the mark; but the most appropriate\nattitude may be a bitter determination to be as competent as we can, much as\nif we were preparing for a difficult exam that will either realize our dreams\nor obliterate them.\n\nThis is not a prescription of fanaticism. The intelligence explosion might\nstill be many decades off in the future. Moreover, the challenge we face is,\nin part, to hold on to our humanity: to maintain our groundedness, common\nsense, and good-humored decency even in the teeth of this most unnatural and\ninhuman problem. We need to bring all our human resourcefulness to bear on its\nsolution.\n\nYet let us not lose track of what is globally significant. Through the fog of\neveryday trivialities, we can perceive—if but dimly—the essential task of our\nage. In this book, we have attempted to discern a little more feature in what\nis otherwise still a relatively amorphous and negatively defined vision—one\nthat presents as our principal moral priority (at least from an impersonal and\nsecular perspective) the reduction of existential risk and the attainment of a\ncivilizational trajectory that leads to a compassionate and jubilant use of\nhumanity’s cosmic endowment.\n\n\n## [NOTES](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_006.html#filepos29369)\n\n##### PRELIMS\n\n> [1](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_004.html#filepos10692). Not all\n> endnotes contain useful information, however.\n\n> [2](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_004.html#filepos10828). I don’t\n> know which ones.\n\n##### CHAPTER 1: PAST DEVELOPMENTS AND PRESENT CAPABILITIES\n\n> [1](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos37196). A\n> subsistence-level income today is about $400 (Chen and Ravallion 2010). A\n> million subsistence-level incomes is thus $400,000,000. The current world\n> gross product is about $60,000,000,000,000 and in recent years has grown at\n> an annual rate of about 4% (compound annual growth rate since 1950, based on\n> Maddison [2010]). These figures yield the estimate mentioned in the text,\n> which of course is only an order-of-magnitude approximation. If we look\n> directly at population figures, we find that it currently takes the world\n> population about one and a half weeks to grow by one million; but this\n> underestimates the growth rate of the economy since _per capita_ income is\n> also increasing. By 5000 BC, following the Agricultural Revolution, the\n> world population was growing at a rate of about 1 million per 200 years—a\n> great acceleration since the rate of perhaps 1 million per million years in\n> early humanoid prehistory—so a great deal of acceleration had already\n> occurred by then. Still, it is impressive that an amount of economic growth\n> that took 200 years seven thousand years ago takes just ninety minutes now,\n> and that the world population growth that took two centuries then takes one\n> and a half weeks now. See also Maddison (2005).\n\n> [2](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos37574). Such\n> dramatic growth and acceleration might suggest one notion of a possible\n> coming “singularity,” as adumbrated by John von Neumann in a conversation\n> with the mathematician Stanislaw Ulam:\n\n> Our conversation centred on the ever accelerating progress of technology and\n> changes in the mode of human life, which gives the appearance of approaching\n> some essential singularity in the history of the race beyond which human\n> affairs, as we know them, could not continue. (Ulam 1958)\n\n> [3](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos38215). Hanson\n> (2000).\n\n> [4](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos39215). Vinge\n> (1993); Kurzweil (2005).\n\n> [5](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos39454). Sandberg\n> (2010).\n\n> [6](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos41302). Van\n> Zanden (2003); Maddison (1999, 2001); De Long (1998).\n\n> [7](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos41994). Two oft-\n> repeated optimistic statements from the 1960s: “Machines will be capable,\n> within twenty years, of doing any work a man can do” (Simon 1965, 96);\n> “Within a generation … the problem of creating artificial intelligence will\n> substantially be solved” (Minsky 1967, 2). For a systematic review of AI\n> predictions, see Armstrong and Sotala (2012).\n\n> [8](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos42317). See, for\n> example, Baum et al. (2011) and Armstrong and Sotala (2012).\n\n> [9](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos43261). It might\n> suggest, however, that AI researchers know less about development timelines\n> than they think they do—but this could cut both ways: they might\n> overestimate as well as underestimate the time to AI.\n\n> [10](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos45157). Good\n> (1965, 33).\n\n> [11](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos46182). One\n> exception is Norbert Wiener, who did have some qualms about the possible\n> consequences. He wrote, in 1960: “If we use, to achieve our purposes, a\n> mechanical agency with whose operation we cannot efficiently interfere once\n> we have started it, because the action is so fast and irrevocable that we\n> have not the data to intervene before the action is complete, then we had\n> better be quite sure that the purpose put into the machine is the purpose\n> which we really desire and not merely a colourful imitation of it” (Wiener\n> 1960). Ed Fredkin spoke about his worries about superintelligent AI in an\n> interview described in McCorduck (1979). By 1970, Good himself writes about\n> the risks, and even calls for the creation of an association to deal with\n> the dangers (Good [1970]; see also his later article [Good 1982] where he\n> foreshadows some of the ideas of “indirect normativity” that we discuss in\n> [Chapter\n> 13](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_020.html#filepos727275)). By\n> 1984, Marvin Minsky was also writing about many of the key worries (Minsky\n> 1984).\n\n> [12](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos46554). Cf.\n> Yudkowsky (2008a). On the importance of assessing the ethical implications\n> of potentially dangerous future technologies _before_ they become feasible,\n> see Roache (2008).\n\n> [13](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos49882).\n> McCorduck (1979).\n\n> [14](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos50058). Newell\n> et al. (1959).\n\n> [15](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos50318). The\n> SAINTS program, the ANALOGY program, and the STUDENT program, respectively.\n> See Slagle (1963), Evans (1964, 1968), and Bobrow (1968).\n\n> [16](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos50574). Nilsson\n> (1984).\n\n> [17](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos50717).\n> Weizenbaum (1966).\n\n> [18](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos50979).\n> Winograd (1972).\n\n> [19](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos51297). Cope\n> (1996); Weizenbaum (1976); Moravec (1980); Thrun et al. (2006); Buehler et\n> al. (2009); Koza et al. (2003). The Nevada Department of Motor Vehicles\n> issued the first license for a driverless car in May 2012.\n\n> [20](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos51409). The\n> STANDUP system (Ritchie et al. 2007).\n\n> [21](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos55658).\n> Schwartz (1987). Schwartz is here characterizing a skeptical view that he\n> thought was represented by the writings of Hubert Dreyfus.\n\n> [22](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos55905). One\n> vocal critic during this period was Hubert Dreyfus. Other prominent skeptics\n> from this era include John Lucas, Roger Penrose, and John Searle. However,\n> among these only Dreyfus was mainly concerned with refuting claims about\n> what practical accomplishments we should expect from existing paradigms in\n> AI (though he seems to have been open to the possibility that new paradigms\n> could go further). Searle’s target was functionalist theories in the\n> philosophy of mind, not the instrumental powers of AI systems. Lucas and\n> Penrose denied that a classical computer could ever be programmed to do\n> everything that a human mathematician can do, but they did not deny that any\n> particular function could in principle be automated or that AIs might\n> eventually become very instrumentally powerful. Cicero remarked that “there\n> is nothing so absurd but some philosopher has said it” (Cicero 1923, 119);\n> yet it is surprisingly hard to think of _any_ significant thinker who has\n> denied the possibility of machine superintelligence in the sense used in\n> this book.\n\n> [23](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos57287). For\n> many applications, however, the learning that takes place in a neural\n> network is little different from the learning that takes place in linear\n> regression, a statistical technique developed by Adrien-Marie Legendre and\n> Carl Friedrich Gauss in the early 1800s.\n\n> [24](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos58044). The\n> basic algorithm was described by Arthur Bryson and Yu-Chi Ho as a multi-\n> stage dynamic optimization method in 1969 (Bryson and Ho 1969). The\n> application to neural networks was suggested by Paul Werbos in 1974 (Werbos\n> 1994), but it was only after the work by David Rumelhart, Geoffrey Hinton,\n> and Ronald Williams in 1986 (Rumelhart et al. 1986) that the method\n> gradually began to seep into the awareness of a wider community.\n\n> [25](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos58318). Nets\n> lacking hidden layers had previously been shown to have severely limited\n> functionality (Minsky and Papert 1969).\n\n> [26](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos61572). E.g.,\n> MacKay (2003).\n\n> [27](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos61966). Murphy\n> (2012).\n\n> [28](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos63865). Pearl\n> (2009).\n\n> [29](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos65472). We\n> suppress various technical details here in order not to unduly burden the\n> exposition. We will have occasion to revisit some of these ignored issues in\n> [Chapter\n> 12](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos645293).\n\n> [30](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos65874). A\n> program _p_ is a description of string _x_ if _p_ , run on (some particular)\n> universal Turing machine _U_ , outputs _x_ ; we write this as _U(p) = x_.\n> (The string _x_ here represents a possible world.) The Kolmogorov complexity\n> of _x_ is then _K_(_x_):=min _p_ {_l_(_p_): _U_(_p_) = _x_}, where _l_(_p_)\n> is the length of _p_ in bits. The “Solomonoff” probability of _x_ is then\n> defined as ![Image](images/00041.jpg) where the sum is defined over all\n> (“minimal,” i.e. not necessarily halting) programs _p_ for which _U_ outputs\n> a string starting with _x_ (Hutter 2005).\n\n> [31](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos66243).\n> Bayesian conditioning on evidence _E_ gives\n\n![Image](images/00042.jpg)\n\n> (The probability of a proposition [like _E_] is the sum of the probability\n> of the possible worlds in which it is true.)\n\n> [32](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos68269). Or\n> randomly picks one of the possible actions with the highest expected\n> utility, in case there is a tie.\n\n> [33](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos68884). More\n> concisely, the expected utility of an action can be written as\n> ![Image](images/00043.jpg) where the sum is over all possible worlds.\n\n> [34](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos69220). See,\n> e.g., Howson and Urbach (1993); Bernardo and Smith (1994); Russell and\n> Norvig (2010).\n\n> [35](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos64650).\n> Wainwright and Jordan (2008). The application areas of Bayes nets are\n> myriad; see, e.g., Pourret et al. (2008).\n\n> [36](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos70986). One\n> might wonder why so much detail is given to game AI here, which to some\n> might seem like an unimportant application area. The answer is that game-\n> playing offers some of the clearest measures of human vs. AI performance.\n\n> [37](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos72211). Samuel\n> (1959); Schaeffer (1997, ch. 6).\n\n> [38](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos72680).\n> Schaeffer et al. (2007).\n\n> [39](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos73121).\n> Berliner (1980a, b).\n\n> [40](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos73495). Tesauro\n> (1995).\n\n> [41](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos73744). Such\n> programs include GNU (see Silver [2006]) and Snowie (see Gammoned.net\n> [2012]).\n\n> [42](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos73938). Lenat\n> himself had a hand in guiding the fleet-design process. He wrote: “Thus the\n> final crediting of the win should be about 60/40% Lenat/Eurisko, though the\n> significant point here is that neither party could have won alone” (Lenat\n> 1983, 80).\n\n> [43](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos74216). Lenat\n> (1982, 1983).\n\n> [44](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos74615).\n> Cirasella and Kopec (2006).\n\n> [45](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos74978).\n> Kasparov (1996, 55).\n\n> [46](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos75088). Newborn\n> (2011).\n\n> [47](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos75367). Keim et\n> al. (1999).\n\n> [48](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos75949). See\n> Armstrong (2012).\n\n> [49](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos76210).\n> Sheppard (2002).\n\n> [50](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos76497).\n> Wikipedia (2012a).\n\n> [51](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos76821). Markoff\n> (2011).\n\n> [52](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos77383). Rubin\n> and Watson (2011).\n\n> [53](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos77761). Elyasaf\n> et al. (2011).\n\n> [54](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos78154). KGS\n> (2012).\n\n> [55](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos71490). Newell\n> et al. (1958, 320).\n\n> [56](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos71685).\n> Attributed in Vardi (2012).\n\n> [57](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos78812). In\n> 1976, I. J. Good wrote: “A computer program of Grandmaster strength would\n> bring us within an ace of [machine ultra-intelligence]” (Good 1976). In\n> 1979, Douglas Hofstadter opined in his Pulitzer-winning _Gödel, Escher,\n> Bach_ : “Question: Will there be chess programs that can beat anyone?\n> Speculation: No. There may be programs that can beat anyone at chess, but\n> they will not be exclusively chess programs. They will be programs of\n> general intelligence, and they will be just as temperamental as people. ‘Do\n> you want to play chess?’ ‘No, I’m bored with chess. Let’s talk about\n> poetry’” (Hofstadter [1979] 1999, 678).\n\n> [58](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos79257). The\n> algorithm is minimax search with alpha-beta pruning, used with a chess-\n> specific heuristic evaluation function of board states. Combined with a good\n> library of openings and endgames, and various other tricks, this can make\n> for a capable chess engine.\n\n> [59](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos79521). Though\n> especially with recent progress in learning the evaluation heuristic from\n> simulated games, many of the underlying algorithms would probably also work\n> well for many other games.\n\n> [60](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos80002). Nilsson\n> (2009, 318). Knuth was certainly overstating his point. There are many\n> “thinking tasks” that AI has not succeeded in doing—inventing a new subfield\n> of pure mathematics, doing any kind of philosophy, writing a great detective\n> novel, engineering a coup d’état, or designing a major new consumer product.\n\n> [61](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos80720). Shapiro\n> (1992).\n\n> [62](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos81128). One\n> might speculate that one reason it has been difficult to match human\n> abilities in perception, motor control, common sense, and language\n> understanding is that our brains have dedicated wetware for these\n> functions—neural structures that have been optimized over evolutionary\n> timescales. By contrast, logical thinking and skills like chess playing are\n> not natural to us; so perhaps we are forced to rely on a limited pool of\n> general-purpose cognitive resources to perform these tasks. Maybe what our\n> brains do when we engage in explicit logical reasoning or calculation is in\n> some ways analogous to running a “virtual machine,” a slow and cumbersome\n> mental simulation of a general-purpose computer. One might then say\n> (somewhat fancifully) that a classical AI program is not so much emulating\n> human thinking as the other way around: a human who is thinking logically is\n> emulating an AI program.\n\n> [63](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos82288). This\n> example is controversial: a minority view, represented by approximately 20%\n> of adults in the USA and similar numbers in many other developed nations,\n> holds that the Sun revolves around the Earth (Crabtree 1999; Dean 2005).\n\n> [64](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos83196). World\n> Robotics (2011).\n\n> [65](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos83304).\n> Estimated from data in Guizzo (2010).\n\n> [66](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos83904). Holley\n> (2009).\n\n> [67](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos84556). Hybrid\n> rule-based statistical approaches are also used, but they are currently a\n> small part of the picture.\n\n> [68](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos86244). Cross\n> and Walker (1994); Hedberg (2002).\n\n> [69](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos89492). Based\n> on the statistics from TABB Group, a New York- and London-based capital\n> markets research firm (personal communication).\n\n> [70](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos91913). CFTC\n> and SEC (2010). For a different perspective on the events of 6 May 2010, see\n> CME Group (2010).\n\n> [71](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos92753). Nothing\n> in the text should be construed as an argument against algorithmic high-\n> frequency trading, which might normally perform a beneficial function by\n> increasing liquidity and market efficiency.\n\n> [72](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos94203). A\n> smaller market scare occurred on August, 1, 2012, in part because the\n> “circuit breaker” was not also programmed to halt trading if there were\n> extreme changes in the _number_ of shares being traded (Popper 2012). This\n> again foreshadows another later theme: the difficulty of anticipating all\n> specific ways in which some particular plausible-seeming rule might go\n> wrong.\n\n> [73](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos95820). Nilsson\n> (2009, 319).\n\n> [74](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos96102). Minsky\n> (2006); McCarthy (2007); Beal and Winston (2009).\n\n> [75](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos97015). Peter\n> Norvig, personal communication. Machine-learning classes are also very\n> popular, reflecting a somewhat orthogonal hype-wave of “big data” (inspired\n> by e.g. Google and the Netflix Prize).\n\n> [76](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos97381).\n> Armstrong and Sotala (2012).\n\n> [77](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos97946). Müller\n> and Bostrom (forthcoming).\n\n> [78](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos98606). See\n> Baum et al. (2011), another survey cited therein, and Sandberg and Bostrom\n> (2011).\n\n> [79](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos99100). Nilsson\n> (2009).\n\n> [80](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos99232). This is\n> again conditional on no civilization-disrupting catastrophe occurring. The\n> definition of HLMI used by Nilsson is “AI able to perform around 80% of jobs\n> as well or better than humans perform” (Kruel 2012).\n\n> [81](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos99531). The\n> table shows the results of four different polls as well as the combined\n> results. The first two were polls taken at academic conferences: _PT-AI_ ,\n> participants of the conference _Philosophy and Theory of AI_ in Thessaloniki\n> 2011 (respondents were asked in November 2012), with a response rate of 43\n> out of 88; and _AGI_ , participants of the conferences _Artificial General\n> Intelligence_ and _Impacts and Risks of Artificial General Intelligence_ ,\n> both in Oxford, December 2012 (response rate: 72/111). The _EETN_ poll\n> sampled the members of the Greek Association for Artificial Intelligence, a\n> professional organization of published researchers in the field, in April\n> 2013 (response rate: 26/250). The _TOP100_ poll elicited the opinions among\n> the 100 top authors in artificial intelligence as measured by a citation\n> index, in May 2013 (response rate: 29/100).\n\n> [82](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos100199).\n> Interviews with some 28 (at the time of writing) AI practitioners and\n> related experts have been posted by Kruel (2011).\n\n> [83](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos103335). The\n> diagram shows renormalized median estimates. Means are significantly\n> different. For example, the mean estimates for the “Extremely bad” outcome\n> were 7.6% (for _TOP100_) and 17.2% (for the combined pool of expert\n> assessors).\n\n> [84](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos104254). There\n> is a substantial literature documenting the unreliability of expert\n> forecasts in many domains, and there is every reason to think that many of\n> the findings in this body of research apply to the field of artificial\n> intelligence too. In particular, forecasters tend to be overconfident in\n> their predictions, believing themselves to be more accurate than they really\n> are, and therefore assigning too little probability to the possibility that\n> their most-favored hypothesis is wrong (Tetlock 2005). (Various other biases\n> have also been documented; see, e.g., Gilovich et al. [2002].) However,\n> uncertainty is an inescapable fact of the human condition, and many of our\n> actions unavoidably rely on expectations about which long-term consequences\n> are more or less plausible: in other words, on probabilistic predictions.\n> Refusing to offer explicit probabilistic predictions would not make the\n> epistemic problem go away; it would just hide it from view (Bostrom 2007).\n> Instead, we should respond to evidence of overconfidence by broadening our\n> confidence intervals (or “credible intervals”)—i.e. by smearing out our\n> credence functions—and in general we must struggle as best we can with our\n> biases, by considering different perspectives and aiming for intellectual\n> honesty. In the longer run, we can also work to develop techniques, training\n> methods, and institutions that can help us achieve better calibration. See\n> also Armstrong and Sotala (2012).\n\n##### CHAPTER 2: PATHS TO SUPERINTELLIGENCE\n\n> [1](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos105519). This\n> resembles the definition in Bostrom (2003c) and Bostrom (2006a). It can also\n> be compared with Shane Legg’s definition (“Intelligence measures an agent’s\n> ability to achieve goals in a wide range of environments”) and its\n> formalizations (Legg 2008). It is also very similar to Good’s definition of\n> ultraintelligence in [Chapter\n> 1](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos33916) (“a\n> machine that can far surpass all the intellectual activities of any man\n> however clever”).\n\n> [2](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos106258). For the\n> same reason, we make no assumption regarding whether a superintelligent\n> machine could have “true intentionality” (_pace_ Searle, it could; but this\n> seems irrelevant to the concerns of this book). And we take no position in\n> the internalism/externalism debate about mental content that has been raging\n> in the philosophical literature, or on the related issue of the extended\n> mind thesis (Clark and Chalmers 1998).\n\n> [3](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos109231). Turing\n> (1950, 456).\n\n> [4](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos110222). Turing\n> (1950, 456).\n\n> [5](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos110942).\n> Chalmers (2010); Moravec (1976, 1988, 1998, 1999).\n\n> [6](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos111846). See\n> Moravec (1976). A similar argument is advanced by David Chalmers (2010).\n\n> [7](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos116197). See\n> also Shulman and Bostrom (2012), where these matters are elaborated in more\n> detail.\n\n> [8](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos116423). Legg\n> (2008) offers this reason in support of the claim that humans will be able\n> to recapitulate the progress of evolution over much shorter timescales and\n> with reduced computational resources (while noting that evolution’s\n> unadjusted computational resources are far out of reach). Baum (2004) argues\n> that some developments relevant to AI occurred earlier, with the\n> organization of the genome itself embodying a valuable representation for\n> evolutionary algorithms.\n\n> [9](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos116824). Whitman\n> et al. (1998); Sabrosky (1952).\n\n> [10](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos117728).\n> Schultz (2000).\n\n> [11](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos118027). Menzel\n> and Giurfa (2001, 62); Truman et al. (1993).\n\n> [12](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos119347).\n> Sandberg and Bostrom (2008).\n\n> [13](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos120440). See\n> Legg (2008) for further discussion of this point and of the promise of\n> functions or environments that determine fitness based on a smooth landscape\n> of pure intelligence tests.\n\n> [14](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos121469). See\n> Bostrom and Sandberg (2009b) for a taxonomy and more detailed discussion of\n> ways in which engineers may outperform historical evolutionary selection.\n\n> [15](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos122600). The\n> analysis has addressed the nervous systems of living creatures, without\n> reference to the cost of simulating bodies or the surrounding virtual\n> environment as part of a fitness function. It is plausible that an adequate\n> fitness function could test the competence of a particular organism in far\n> fewer operations than it would take to simulate all the neuronal computation\n> of that organism’s brain throughout its natural lifespan. AI programs today\n> often develop and operate in very abstract environments (theorem provers in\n> symbolic math worlds, agents in simple game tournament worlds, etc.).\n\n> A skeptic might insist that an abstract environment would be inadequate for\n> the evolution of general intelligence, believing instead that the virtual\n> environment would need to closely resemble the actual biological environment\n> in which our ancestors evolved. Creating a physically realistic virtual\n> world would require a far greater investment of computational resources than\n> the simulation of a simple toy world or abstract problem domain (whereas\n> evolution had access to a physically realistic real world “for free”). In\n> the limiting case, if complete micro-physical accuracy were insisted upon,\n> the computational requirements would balloon to ridiculous proportions.\n> However, such extreme pessimism is almost certainly unwarranted; it seems\n> unlikely that the best environment for evolving intelligence is one that\n> mimics nature as closely as possible. It is, on the contrary, plausible that\n> it would be more efficient to use an artificial selection environment, one\n> quite unlike that of our ancestors, an environment specifically designed to\n> promote adaptations that increase the type of intelligence we are seeking to\n> evolve (abstract reasoning and general problem-solving skills, for instance,\n> as opposed to maximally fast instinctual reactions or a highly optimized\n> visual system).\n\n> [16](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos123571).\n> Wikipedia (2012b).\n\n> [17](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos125678). For a\n> general treatment of observation selection theory, see Bostrom (2002a). For\n> the specific application to the current issue, see Shulman and Bostrom\n> (2012). For a short popular introduction, see Bostrom (2008b).\n\n> [18](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos127060). Sutton\n> and Barto (1998, 21f); Schultz et al. (1997).\n\n> [19](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos129264). This\n> term was introduced by Eliezer Yudkowsky; see, e.g., Yudkowsky (2007).\n\n> [20](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos130616). This\n> is the scenario described by Good (1965) and Yudkowsky (2007). However, one\n> could also consider an alternative in which the iterative sequence has some\n> steps that do not involve intelligence enhancement but instead design\n> simplification. That is, at some stages, the seed AI might rewrite itself so\n> as make subsequent improvements easier to find.\n\n> [21](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos136717).\n> Helmstaedter et al. (2011).\n\n> [22](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos137020). Andres\n> et al. (2012).\n\n> [23](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos136199).\n> Adequate for enabling instrumentally useful forms of cognitive functioning\n> and communication, that is; but still radically impoverished relative to the\n> interface provided by the muscles and sensory organs of a normal human body.\n\n> [24](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos138001).\n> Sandberg (2013).\n\n> [25](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos139732). See\n> the “Computer requirements” section of Sandberg and Bostrom (2008, 79–81).\n\n> [26](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos142149). A\n> lower level of success might be a brain simulation that has biologically\n> suggestive micro-dynamics and displays a substantial range of emergent\n> species-typical activity such as a slow-wave sleep state or activity-\n> dependent plasticity. Whereas such a simulation could be a useful testbed\n> for neuroscientific research (though one which might come close to raising\n> serious ethical issues), it would not count as a whole brain emulation\n> unless the simulation were sufficiently accurate to be able to perform a\n> substantial fraction of the intellectual work that the simulated brain was\n> capable of. As a rule of thumb, we might say that in order for a simulation\n> of a human brain to count as a whole brain emulation, it would need to be\n> able to express coherent verbal thoughts or have the capacity to learn to do\n> so.\n\n> [27](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos143381).\n> Sandberg and Bostrom (2008).\n\n> [28](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos145024).\n> Sandberg and Bostrom (2008). Further explanation can be found in the\n> original report.\n\n> [29](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos144059). The\n> first map is described in Albertson and Thomson (1976) and White et al.\n> (1986). The combined (and in some cases corrected) network is available from\n> the “WormAtlas” website (<http://www.wormatlas.org/>).\n\n> [30](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos144590). For a\n> review of past attempts of emulating _C. elegans_ and their fates, see\n> Kaufman (2011). Kaufman quotes one ambitious doctoral student working in the\n> area, David Dalrymple, as saying, “With optogenetic techniques, we are just\n> at the point where it’s not an outrageous proposal to reach for the\n> capability to read and write to anywhere in a living _C. elegans_ nervous\n> system, using a high-throughput automated system…. I expect to be finished\n> with _C. elegans_ in 2–3 years. I would be extremely surprised, for whatever\n> that’s worth, if this is still an open problem in 2020” (Dalrymple 2011).\n> Brain models aiming for biological realism that were hand-coded (rather than\n> generated automatically) have achieved some basic functionality; see, e.g.,\n> Eliasmith et al. (2012).\n\n> [31](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos146114).\n> _Caenorhabditis elegans_ does have some convenient special properties. For\n> example, the organism is transparent, and the wiring pattern of its nervous\n> system does not change between individuals.\n\n> [32](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos146331). If\n> neuromorphic AI rather than whole brain emulation is the end product, then\n> it might or might not be the case that the relevant insights would be\n> derived through attempts to simulate _human_ brains. It is conceivable that\n> the important cortical tricks would be discovered during the study of\n> (nonhuman) animal brains. Some animal brains might be easier to work with\n> than human brains, and smaller brains would require fewer resources to scan\n> and model. Research on animal brains would also be subject to less\n> regulation. It is even conceivable that the first human-level machine\n> intelligence will be created by completing a whole brain emulation of some\n> suitable animal and then finding ways to enhance the resultant digital mind.\n> Thus humanity could get its comeuppance from an uplifted lab mouse or\n> macaque.\n\n> [33](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos150265). Uauy\n> and Dangour (2006); Georgieff (2007); Stewart et al. (2008); Eppig et al.\n> (2010); Cotman and Berchtold (2002).\n\n> [34](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos150997).\n> According to the World Health Organization in 2007, nearly 2 billion\n> individuals have insufficient iodine intake (_The Lancet_ 2008). _Severe_\n> iodine deficiency hinders neurological development and leads to cretinism,\n> which involves an average loss of about 12.5 IQ points (Qian et al. 2005).\n> The condition can be easily and inexpensively prevented though salt\n> fortification (Horton et al. 2008).\n\n> [35](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos151240).\n> Bostrom and Sandberg (2009a).\n\n> [36](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos151541).\n> Bostrom and Sandberg (2009b). A typical _putative_ performance increase from\n> pharmacological and nutritional enhancement is in the range of 10–20% on\n> test tasks measuring working memory, attention, etc. But it is generally\n> dubious whether such reported gains are real, sustainable over a longer\n> term, and indicative of correspondingly improved results in real-world\n> problem situations (Repantis et al. 2010). For instance, in some cases there\n> might be a compensating deterioration on some performance dimensions that\n> are not measured by the test tasks (Sandberg and Bostrom 2006).\n\n> [37](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos151794). If\n> there were an easy way to enhance cognition, one would expect evolution\n> already to have taken advantage of it. Consequently, the most promising kind\n> of nootropic to investigate may be one that promises to boost intelligence\n> in some manner that we can see would have lowered fitness in the ancestral\n> environment—for example, by increasing head size at birth or amping up the\n> brain’s glucose metabolism. For a more detailed discussion of this idea\n> (along with several important qualifications), see Bostrom (2009b).\n\n> [38](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos152532). Sperm\n> are harder to screen because, in contrast to embryos, they consist of only\n> one cell—and one cell needs to be destroyed in order to do the sequencing.\n> Oocytes also consist of only one cell; however, the first and second cell\n> divisions are asymmetric and produce one daughter cell with very little\n> cytoplasm, the polar body. Since polar bodies contain the same genome as the\n> main cell and are redundant (they eventually degenerate) they can be\n> biopsied and used for screening (Gianaroli 2000).\n\n> [39](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos153047). Each\n> of these practices was subject to some ethical controversy when it was\n> introduced, but there seems to be a trend toward increasing acceptance.\n> Attitudes toward human genetic engineering and embryo selection vary\n> significantly across cultures, suggesting that development and application\n> of new techniques will probably take place even if some countries initially\n> adopt a cautious stance, although the rate at which this happens will be\n> influenced by moral, religious, and political pressures.\n\n> [40](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos153555). Davies\n> et al. (2011); Benyamin et al. (2013); Plomin et al. (2013). See also Mardis\n> (2011); Hsu (2012).\n\n> [41](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos153742). Broad-\n> sense heritability of adult IQ is usually estimated in the range of 0.5–0.8\n> within middle-class strata of developed nations (Bouchard 2004, 148).\n> Narrow-sense heritability, which measures the portion of variance that is\n> attributable to additive genetic factors, is lower (in the range 0.3–0.5)\n> but still substantial (Devlin et al. 1997; Davies et al. 2011; Visscher et\n> al. 2008). These estimates could change for different populations and\n> environments, as heritabilities vary depending on the population and\n> environment being studied. For example, lower heritabilities have been found\n> among children and those from deprived environments (Benyamin et al. 2013;\n> Turkheimer et al. 2003). Nisbett et al. (2012) review numerous environmental\n> influences on variation in cognitive ability.\n\n> [42](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos154185). The\n> following several paragraphs draw heavily on joint work with Carl Shulman\n> (Shulman and Bostrom 2014).\n\n> [43](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos155170). This\n> table is taken from Shulman and Bostrom (2014). It is based on a toy model\n> that assumes a Gaussian distribution of predicted IQs among the embryos with\n> a standard deviation of 7.5 points. The amount of cognitive enhancement that\n> would be delivered with different numbers of embryos depends on how\n> different the embryos are from one another in the additive genetic variants\n> whose effects we know. Siblings have a coefficient of relatedness of ½, and\n> common additive genetic variants account for half or less of variance in\n> adult fluid intelligence (Davies et al. 2011). These two facts suggest that\n> where the observed population standard deviation in developed countries is\n> 15 points, the standard deviation of genetic influences within a batch of\n> embryos would be 7.5 points or less.\n\n> [44](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos154658). With\n> imperfect information about the additive genetic effects on cognitive\n> ability, effect sizes would be reduced. However, even a small amount of\n> knowledge would go a relatively long way, because the gains from selection\n> do not scale linearly with the portion of variance that we can predict.\n> Instead, the effectiveness of our selection depends on the standard\n> deviation of predicted mean IQ, which scales as the _square root_ of\n> variance. For example, if one could account for 12.5% of the variance, this\n> could deliver effects half as great as those in [Table\n> 1](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos71800), which\n> assume 50%. For comparison, a recent study (Rietveld et al. 2013) claims to\n> have already identified 2.5% of the variance.\n\n> [45](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos154993). For\n> comparison, standard practice today involves the creation of fewer than ten\n> embryos.\n\n> [46](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos157268). Adult\n> and embryonic stem cells can be coaxed to develop into sperm cells and\n> oocytes, which can then be fused to produce an embryo (Nagy et al. 2008;\n> Nagy and Chang 2007). Egg cell precursors can also form parthenogenetic\n> blastocysts, unfertilized and non-viable embryos, able to produce embryonic\n> stem cell lines for the process (Mai et al. 2007).\n\n> [47](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos157729). The\n> opinion is that of Katsuhiko Hayashi, as reported in Cyranoski (2013). The\n> Hinxton Group, an international consortium of scientists that discusses stem\n> cell ethics and challenges, predicted in 2008 that human stem cell-derived\n> gametes would be available within ten years (Hinxton Group 2008), and\n> developments thus far are broadly consistent with this.\n\n> [48](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos158768).\n> Sparrow (2013); Miller (2012); The Uncertain Future (2012).\n\n> [49](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos159165).\n> Sparrow (2013).\n\n> [50](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos160943).\n> Secular concerns might focus on anticipated impacts on social inequality,\n> the medical safety of the procedure, fears of an enhancement “rat race,”\n> rights and responsibilities of parents vis-à-vis their prospective\n> offspring, the shadow of twentieth-century eugenics, the concept of human\n> dignity, and the proper limits of states’ involvement in the reproductive\n> choices of their citizens. (For a discussion of the ethics of cognitive\n> enhancement see Bostrom and Ord [2006], Bostrom and Roache [2011], and\n> Sandberg and Savulescu [2011].) Some religious traditions may offer\n> additional concerns, including ones centering on the moral status of embryos\n> or the proper limits of human agency within the scheme of creation.\n\n> [51](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos163007). To\n> stave off the negative effects of inbreeding, iterated embryo selection\n> would require either a large starting supply of donors or the expenditure of\n> substantial selective power to reduce harmful recessive alleles. Either\n> alternative would tend to push toward offspring being less closely\n> genetically related to their parents (and more related to one another).\n\n> [52](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos164099).\n> Adapted from Shulman and Bostrom (2014).\n\n> [53](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos164165).\n> Bostrom (2008b).\n\n> [54](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos163509). Just\n> how difficult an obstacle epigenetics will be is not yet known (Chason et\n> al. 2011; Iliadou et al. 2011).\n\n> [55](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos163914). While\n> cognitive ability is a fairly heritable trait, there may be few or no\n> _common_ alleles or polymorphisms that individually have a large positive\n> effect on intelligence (Davis et al. 2010; Davies et al. 2011; Rietveld et\n> al. 2013). As sequencing methods improve, the mapping out of low-frequency\n> alleles and their cognitive and behavioral correlates will become\n> increasingly feasible. There is some theoretical evidence suggesting that\n> some alleles that cause genetic disorders in homozygotes may provide\n> sizeable cognitive advantages in heterozygote carriers, leading to a\n> prediction that Gaucher, Tay-Sachs, and Niemann-Pick heterozygotes would be\n> about 5 IQ points higher than control groups (Cochran et al. 2006). Time\n> will tell whether this holds.\n\n> [56](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos164667). One\n> paper (Nachman and Crowell 2000) estimates 175 mutations per genome per\n> generation. Another (Lynch 2010), using different methods, estimates that\n> the average newborn has between 50 and 100 new mutations, and Kong et al.\n> (2012) implies a figure of around 77 new mutations per generation. Most of\n> these mutations do not affect functioning, or do so only to an imperceptibly\n> slight degree; but the combined effects of many very slightly deleterious\n> mutations could be a significant loss of fitness. See also Crow (2000).\n\n> [57](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos164913). Crow\n> (2000); Lynch (2010).\n\n> [58](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos165903). There\n> are some potentially important caveats to this idea. It is possible that the\n> modal genome would need some adjustments in order to avoid problems. For\n> example, parts of the genome might be adapted to interacting with other\n> parts under the assumption that all parts function with a certain level of\n> efficiency. Increasing the efficiency of those parts might then lead to\n> overshooting along some metabolic pathways.\n\n> [59](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos166951). These\n> composites were created by Mike Mike from individual photographs taken by\n> Virtual Flavius (Mike 2013).\n\n> [60](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos169591). They\n> can, of course, have some effects sooner—for instance, by changing people’s\n> expectations of what is to come.\n\n> [61](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos172619). Louis\n> Harris & Associates (1969); Mason (2003).\n\n> [62](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos172917).\n> Kalfoglou et al. (2004).\n\n> [63](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos176416). The\n> data is obviously limited, but individuals selected for 1-in-10,000 results\n> on childhood ability tests have been shown, in longitudinal studies, to be\n> substantially more likely to become tenured professors, earn patents, and\n> succeed in business than those with slightly less exceptional scores (Kell\n> et al. 2013). Roe (1953) studied sixty-four eminent scientists and found\n> median cognitive ability three to four standard deviations above the\n> population norm and strikingly higher than is typical for scientists in\n> general. (Cognitive ability is also correlated with lifetime earnings and\n> with non-financial outcomes such as life expectancy, divorce rates, and\n> probability of dropping out of school [Deary 2012].) An upward shift of the\n> distribution of cognitive ability would have disproportionately large\n> effects at the tails, especially increasing the number of highly gifted and\n> reducing the number of people with retardation and learning disabilities.\n> See also Bostrom and Ord (2006) and Sandberg and Savulescu (2011).\n\n> [64](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos178002). E.g.\n> Warwick (2002). Stephen Hawking even suggested that taking this step might\n> be necessary in order to keep up with advances in machine intelligence: “We\n> must develop as quickly as possible technologies that make possible a direct\n> connection between brain and computer, so that artificial brains contribute\n> to human intelligence rather than opposing it” (reported in Walsh [2001]).\n> Ray Kurzweil concurs: “As far as Hawking’s … recommendation is concerned,\n> namely direct connection between the brain and computers, I agree that this\n> is both reasonable, desirable and inevitable. [_sic_] It’s been my\n> recommendation for years” (Kurzweil 2001).\n\n> [65](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos178258). See\n> Lebedev and Nicolelis (2006); Birbaumer et al. (2008); Mak and Wolpaw\n> (2009); and Nicolelis and Lebedev (2009). A more personal outlook on the\n> problem of enhancement through implants can be found in Chorost (2005, Chap.\n> 11).\n\n> [66](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos179498).\n> Smeding et al. (2006).\n\n> [67](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos180177). Degnan\n> et al. (2002).\n\n> [68](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos180317).\n> Dagnelie (2012); Shannon (2012).\n\n> [69](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos180541).\n> Perlmutter and Mink (2006); Lyons (2011).\n\n> [70](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos181498). Koch\n> et al. (2006).\n\n> [71](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos182442). Schalk\n> (2008). For a general review of the current state of the art, see Berger et\n> al. (2008). For the case that this would help lead to enhanced intelligence,\n> see Warwick (2002).\n\n> [72](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos182653). Some\n> examples: Bartels et al. (2008); Simeral et al. (2011); Krusienski and Shih\n> (2011); and Pasqualotto et al. (2012).\n\n> [73](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos183092). E.g.\n> Hinke et al. (1993).\n\n> [74](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos185378). There\n> are partial exceptions to this, especially in early sensory processing. For\n> example, the primary visual cortex uses a retinotopic mapping, which means\n> roughly that adjacent neural assemblies receive inputs from adjacent areas\n> of the retinas (though ocular dominance columns somewhat complicate the\n> mapping).\n\n> [75](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos187135). Berger\n> et al. (2012); Hampson et al. (2012).\n\n> [76](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos189835). Some\n> brain implants require two forms of learning: the device learning to\n> interpret the organism’s neural representations and the organism learning to\n> use the system by generating appropriate neural firing patterns (Carmena et\n> al. 2003).\n\n> [77](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos191199). It has\n> been suggested that we should regard corporate entities (corporations,\n> unions, governments, churches, and so forth) as artificial intelligent\n> agents, entities with sensors and effectors, able to represent knowledge and\n> perform inference and take action (e.g. Kuipers [2012]; cf. Huebner [2008]\n> for a discussion on whether collective representations can exist). They are\n> clearly powerful and ecologically successful, although their capabilities\n> and internal states are different from those of humans.\n\n> [78](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos192877). Hanson\n> (1995, 2000); Berg and Rietz (2003).\n\n> [79](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos193077). In the\n> workplace, for instance, employers might use lie detectors to crack down on\n> employee theft and shirking, by asking the employee at the end of each\n> business day whether she has stolen anything and whether she has worked as\n> hard as she could. Political and business leaders could likewise be asked\n> whether they were wholeheartedly pursuing the interests of their\n> shareholders or constituents. Dictators could use them to target seditious\n> generals within the regime or suspected troublemakers in the wider\n> population.\n\n> [80](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos193188). One\n> could imagine neuroimaging techniques making it possible to detect neural\n> signatures of motivated cognition. Without self-deception detection, lie\n> detection would favor individuals who believe their own propaganda. Better\n> tests for self-deception tests could also be used to train rationality and\n> to study the effectiveness of interventions aimed at reducing biases.\n\n> [81](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos193975). Bell\n> and Gemmel (2009). An early example is found in the work of MIT’s Deb Roy,\n> who recorded every moment of his son’s first three years of life. Analysis\n> of this audiovisual data is yielding information on language development;\n> see Roy (2012).\n\n> [82](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos194311). Growth\n> in total world population of biological human beings will contribute only a\n> small factor. Scenarios involving machine intelligence could see the world\n> population (including digital minds) explode by many orders of magnitude in\n> a brief period of time. But that road to superintelligence involves\n> artificial intelligence or whole brain emulation, so we need not consider it\n> in this subsection.\n\n> [83](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos195282). Vinge\n> (1993).\n\n##### CHAPTER 3: FORMS OF SUPERINTELLIGENCE\n\n> [1](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_010.html#filepos203722). Vernor\n> Vinge has used the term “weak superintelligence” to refer to such sped-up\n> human minds (Vinge 1993).\n\n> [2](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_010.html#filepos204344). For\n> example, if a very fast system could do everything that any human could do\n> except dance a mazurka, we should still call it a speed superintelligence.\n> Our interest lies in those core cognitive capabilities that have economic or\n> strategic significance.\n\n> [3](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_010.html#filepos204525). At\n> least a millionfold speedup compared to human brains is physically possible,\n> as can been seen by considering the difference in speed and energy of\n> relevant brain processes in comparison to more efficient information\n> processing. The speed of light is more than a million times greater than\n> that of neural transmission, synaptic spikes dissipate more than a million\n> times more heat than is thermodynamically necessary, and current transistor\n> frequencies are more than a million times faster than neuron spiking\n> frequencies (Yudkowsky [2008a]; see also Drexler [1992]). The ultimate\n> limits of speed superintelligence are bounded by light-speed communications\n> delays, quantum limits on the speed of state transitions, and the volume\n> needed to contain the mind (Lloyd 2000). The “ultimate laptop” described by\n> Lloyd (2000) would run a 1.4×1021 FLOPS brain emulation at speedup of\n> 3.8×1029× (assuming the emulation could be sufficiently parallelized).\n> Lloyd’s construction, however, is not intended to be technologically\n> plausible; it is only meant to illustrate those constraints on computation\n> that are readily derivable from basic physical laws.\n\n> [4](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_010.html#filepos204877). With\n> emulations, there is also an issue of how long a human-like mind can keep\n> working on something before going mad or falling into a rut. Even with task\n> variety and regular holidays, it is not certain that a human-like mind could\n> live for thousands of subjective years without developing psychological\n> problems. Furthermore, if total memory capacity is limited—a consequence of\n> having a limited neuron population—then cumulative learning cannot continue\n> indefinitely: beyond some point, the mind must start forgetting one thing\n> for each new thing it learns. (Artificial intelligence could be designed\n> such as to ameliorate these potential problems.)\n\n> [5](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_010.html#filepos206188).\n> Accordingly, nanomechanisms moving at a modest 1 m/s have typical timescales\n> of nanoseconds. See section 2.3.2 of Drexler (1992). Robin Hanson mentions\n> 7-mm “tinkerbell” robot bodies moving at 260 times normal speed (Hanson\n> 1994).\n\n> [6](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_010.html#filepos206581). Hanson\n> (2012).\n\n> [7](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_010.html#filepos208193).\n> “Collective intelligence” does not refer to low-level parallelization of\n> computing hardware but to parallelization at the level of intelligent\n> autonomous agents such as human beings. Implementing a single emulation on a\n> massively parallel machine might result in speed superintelligence if the\n> parallel computer is sufficiently fast: it would not produce a collective\n> intelligence.\n\n> [8](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_010.html#filepos210126).\n> Improvements to the speed or the quality of the individual components could\n> also indirectly affect the performance of collective intelligence, but here\n> we mainly consider such improvements under the other two forms of\n> superintelligence in our classification.\n\n> [9](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_010.html#filepos211574). It has\n> been argued that a higher population density triggered the Upper Paleolithic\n> Revolution and that beyond a certain threshold accumulation of cultural\n> complexity became much easier (Powell et al. 2009).\n\n> [10](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_010.html#filepos212203). What\n> about the Internet? It seems not yet to have amounted to a super-sized\n> boost. Maybe it will do so eventually. It took centuries or millennia for\n> the other examples listed here to reveal their full potential.\n\n> [11](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_010.html#filepos215643). This\n> is, obviously, not meant to be a realistic thought experiment. A planet\n> large enough to sustain seven quadrillion human organisms with present\n> technology would implode, unless it were made of very light matter or were\n> hollow and held up by pressure or other artificial means. (A Dyson sphere or\n> a Shellworld might be a better solution.) History would have unfolded\n> differently on such a vast surface. Set all this aside.\n\n> [12](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_010.html#filepos215969). Our\n> focus here is on the functional properties of a unified intellect, not on\n> the question of whether such an intellect would have qualia or whether it\n> would be a mind in the sense of having subjective conscious experience. (One\n> might ponder, though, what kinds of conscious experience might arise from\n> intellects that are more or less integrated than those of human brains. On\n> some views of consciousness, such as the global workspace theory, it seems\n> one might expect more integrated brains to have more capacious\n> consciousness. Cf. Baars (1997), Shanahan (2010), and Schwitzgebel (2013).)\n\n> [13](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_010.html#filepos218543). Even\n> small groups of humans that have remained isolated for some time might still\n> benefit from the intellectual outputs of a larger collective intelligence.\n> For example, the language they use might have been developed by a much\n> larger linguistic community, and the tools they use might have been invented\n> in a much larger population before the small group became isolated. But even\n> if a small group had always been isolated, it might still be part of a\n> larger collective intelligence than meets the eye—namely, the collective\n> intelligence consisting of not only the present but all ancestral\n> generations as well, an aggregate that can function as a feed-forward\n> information processing system.\n\n> [14](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_010.html#filepos223396). By the\n> Church–Turing thesis, all computable functions are computable by a Turing\n> machine. Since any of the three forms of superintelligence could simulate a\n> Turing machine (if given access to unlimited memory and allowed to operate\n> indefinitely), they are by this formal criterion computationally equivalent.\n> Indeed, an average human being (provided with unlimited scrap paper and\n> unlimited time) could also implement a Turing machine, and thus is also\n> equivalent by the same criterion. What matters for our purposes, however, is\n> what these different systems can achieve _in practice_ , with finite memory\n> and in reasonable time. And the efficiency variations are so great that one\n> can readily make some distinctions. For example, a typical individual with\n> an IQ of 85 could be taught to implement a Turing machine. (Conceivably, it\n> might even be possible to train some particularly gifted and docile\n> chimpanzee to do this.) Yet, for all practical intents and purposes, such an\n> individual is presumably incapable of, say, independently developing general\n> relativity theory or of winning a Fields medal.\n\n> [15](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_010.html#filepos223732). Oral\n> storytelling traditions can produce great works (such as the Homeric epics)\n> but perhaps some of the contributing authors possessed uncommon gifts.\n\n> [16](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_010.html#filepos224499). Unless\n> it contains as components intellects that have speed or quality\n> superintelligence.\n\n> [17](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_010.html#filepos224654). Our\n> inability to specify what all these problems are may in part be due to a\n> lack of trying: there is little point in spending time detailing\n> intellectual jobs that no individual and no currently feasible organization\n> can perform. But it is also possible that even conceptualizing some of these\n> jobs is itself one of those jobs that we currently lack the brains to\n> perform.\n\n> [18](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_010.html#filepos225717). Cf.\n> Boswell (1917); see also Walker (2002).\n\n> [19](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_010.html#filepos226841). This\n> mainly occurs in short bursts in a subset of neurons—most have more sedate\n> firing rates (Gray and McCormick 1996; Steriade et al. 1998). There are some\n> neurons (“chattering neurons,” also known as “fast rhythmically bursting”\n> cells) that may reach firing frequencies as high as 750 Hz, but these seem\n> to be extreme outliers.\n\n> [20](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_010.html#filepos227087).\n> Feldman and Ballard (1982).\n\n> [21](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_010.html#filepos227856). The\n> conduction velocity depends on axon diameter (thicker axons are faster) and\n> whether the axon is myelinated. Within the central nervous system,\n> transmission delays can range from less than a millisecond to up to 100 ms\n> (Kandel et al. 2000). Transmission in optical fibers is around 68% _c_\n> (because of the refractive index of the material). Electrical cables are\n> roughly the same speed, 59–77% _c_.\n\n> [22](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_010.html#filepos228358). This\n> assumes a signal velocity of 70% _c_. Assuming 100% _c_ ups the estimate to\n> 1.8×1018 m3.\n\n> [23](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_010.html#filepos228547). The\n> number of neurons in an adult human male brain has been estimated at 86.1 ±\n> 8.1 billion, a number arrived at by dissolving brains and fractionating out\n> the cell nuclei, counting the ones stained with a neuron-specific marker. In\n> the past, estimates in the neighborhood of 75–125 billion neurons were\n> common. These were typically based on manual counting of cell densities in\n> representative small regions (Azevedo et al. 2009).\n\n> [24](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_010.html#filepos228731).\n> Whitehead (2003).\n\n> [25](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_010.html#filepos229152).\n> Information processing systems can very likely use molecular-scale processes\n> for computing and data storage and reach at least planetary size in extent.\n> The ultimate physical limits to computation set by quantum mechanics,\n> general relativity, and thermodynamics are, however, far beyond this\n> “Jupiter brain” level (Sandberg 1999; Lloyd 2000).\n\n> [26](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_010.html#filepos229319).\n> Stansberry and Kudritzki (2012). Electricity used in data centers worldwide\n> amounted to 1.1–1.5% of total electricity use (Koomey 2011). See also\n> Muehlhauser and Salamon (2012).\n\n> [27](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_010.html#filepos229570). This\n> is an oversimplification. The number of chunks working memory can maintain\n> is both information- and task-dependent; however, it is clearly limited to a\n> small number of chunks. See Miller (1956) and Cowan (2001).\n\n> [28](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_010.html#filepos230022). An\n> example might be that the difficulty of learning Boolean concepts\n> (categories defined by logical rules) is proportional to the length of the\n> shortest logically equivalent propositional formula. Typically, even\n> formulae just 3–4 literals long are very difficult to learn. See Feldman\n> (2000).\n\n> [29](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_010.html#filepos230434). See\n> Landauer (1986). This study is based on experimental estimates of learning\n> and forgetting rates in humans. Taking into account implicit learning might\n> push the estimate up a little. If one assumes a storage capacity ~1 bit per\n> synapse, one gets an _upper bound_ on human memory capacity of about 1015\n> bits. For an overview of different estimates, see Appendix A of Sandberg and\n> Bostrom (2008).\n\n> [30](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_010.html#filepos230860).\n> Channel noise can trigger action potentials, and synaptic noise produces\n> significant variability in the strength of transmitted signals. Nervous\n> systems appear to have evolved to make numerous trade-offs between noise\n> tolerance and costs (mass, size, time delays); see Faisal et al. (2008). For\n> example, axons cannot be thinner than 0.1 μm lest random opening of ion\n> channels create spontaneous action potentials (Faisal et al. 2005).\n\n> [31](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_010.html#filepos231737).\n> Trachtenberg et al. (2002).\n\n> [32](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_010.html#filepos232105). In\n> terms of memory and computational power, though not in terms of energy\n> efficiency. The fastest computer in the world at the time of writing was\n> China’s “Tianhe-2,” which displaced Cray Inc. Titan in June 2013 with a\n> performance of 33.86 petaFLOPS. It uses 17.6 MW of power, almost six orders\n> of magnitude more than the brain’s ~20 W.\n\n> [33](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_010.html#filepos235902). Note\n> that this survey of sources of machine advantage is _disjunctive_ : our\n> argument succeeds even if some of the items listed are illusory, so long as\n> there is at least one source that can provide a sufficiently large\n> advantage.\n\n##### CHAPTER 4: THE KINETICS OF AN INTELLIGENCE EXPLOSION\n\n> [1](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_011.html#filepos240023). The\n> system may not reach one of these baselines at any sharply defined point.\n> There may instead be an interval during which the system gradually becomes\n> able to outperform the external research team on an increasing number of\n> system-improving development tasks.\n\n> [2](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_011.html#filepos246918). In the\n> past half-century, at least one scenario has been widely recognized in which\n> the existing world order would come to an end in the course of minutes or\n> hours: global thermonuclear war.\n\n> [3](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_011.html#filepos250548). This\n> would be consistent with the observation that the Flynn effect—the secular\n> increase in measured IQ scores within most populations at a rate of some 3\n> IQ points per decade over the past 60 years or so—appears to have ceased or\n> even reversed in recent years in some highly developed countries such as the\n> United Kingdom, Denmark, and Norway (Teasdale and Owen 2008; Sundet et al.\n> 2004). The cause of the Flynn effect in the past—and whether and to what\n> extent it represents any genuine gain in general intelligence or merely\n> improved skill at solving IQ test-style puzzles—has been the subject of wide\n> debate and is still not known. Even if the Flynn effect (at least partially)\n> reflects real cognitive gains, and even if the effect is now diminishing or\n> even reversing, this does not prove that we have yet hit diminishing returns\n> in whatever underlying cause was responsible for the observed Flynn effect\n> in the past. The decline or reversal could instead be due to some\n> independent detrimental factor that would otherwise have produced an even\n> bigger observed decline.\n\n> [4](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_011.html#filepos252065). Bostrom\n> and Roache (2011).\n\n> [5](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_011.html#filepos253285). Somatic\n> gene therapy could eliminate the maturational lag, but is technically much\n> more challenging than germline interventions and has a lower ultimate\n> potential.\n\n> [6](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_011.html#filepos254309). Average\n> global economic productivity growth per year over the period 1960–2000 was\n> 4.3% (Isaksson 2007). Only part of this productivity growth is due to gains\n> in organizational efficiency. Some _particular_ networks or organizational\n> processes of course are improving at much faster rates.\n\n> [7](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_011.html#filepos258223).\n> Biological brain evolution was subject to many constraints and trade-offs\n> that are drastically relaxed when the mind moves to a digital medium. For\n> example, brain size is limited by head size, and a head that is too big has\n> trouble passing through the birth canal. A large brain also guzzles\n> metabolic resources and is a dead weight that impedes movement. The\n> connectivity between certain brain regions might be limited by steric\n> constraints—the volume of white matter is significantly larger than the\n> volume of the gray matter it connects. Heat dissipation is limited by blood\n> flow, and might be close to the upper limit for acceptable functioning.\n> Furthermore, biological neurons are noisy, slow, and in need of constant\n> protection, maintenance, and resupply by glial cells and blood vessels\n> (contributing to the intracranial crowding). See Bostrom and Sandberg\n> (2009b).\n\n> [8](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_011.html#filepos264276).\n> Yudkowsky (2008a, 326). For a more recent discussion, see Yudkowsky (2013).\n\n> [9](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_011.html#filepos266270). The\n> picture shows cognitive ability as a one-dimensional parameter, to keep the\n> drawing simple. But this is not essential to the point being made here. One\n> could, for example, instead represent a cognitive ability profile as a\n> hypersurface in a multidimensional space.\n\n> [10](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_011.html#filepos267581). Lin et\n> al. (2012).\n\n> [11](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_011.html#filepos270383). One\n> gets a certain increase in collective intelligence simply by increasing the\n> number of its constituent intellects. Doing so should at least enable better\n> overall performance on tasks that can be easily parallelized. To reap the\n> full returns from such a population explosion, however, one would also need\n> to achieve some (more than minimal) level of coordination between the\n> constituents.\n\n> [12](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_011.html#filepos271026). The\n> distinction between speed and quality of intelligence is anyhow blurred in\n> the case of non-neuromorphic AI systems.\n\n> [13](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_011.html#filepos272055). Rajab\n> et al. (2006, 41–52).\n\n> [14](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_011.html#filepos273524). It has\n> been suggested that using configurable integrated circuits (FPGAs) rather\n> than general-purpose processors could increase computational speeds in\n> neural network simulations by up to two orders of magnitude (Markram 2006).\n> A study of high-resolution climate modeling in the petaFLOP-range found a\n> twenty-four to thirty-four-fold reduction of cost and about two orders of\n> magnitude reduction in power requirements using a custom variant of embedded\n> processor chips (Wehner et al. 2008).\n\n> [15](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_011.html#filepos273947).\n> Nordhaus (2007). There are many overviews of the different meanings of\n> Moore’s law; see, e.g., Tuomi (2002) and Mack (2011).\n\n> [16](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_011.html#filepos277519). If the\n> development is slow enough, the project can avail itself of progress being\n> made in the interim by the outside world, such as advances in computer\n> science made by university researchers and improvements in hardware made by\n> the semiconductor industry.\n\n> [17](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_011.html#filepos279255).\n> Algorithmic overhang is perhaps less likely, but one exception would be if\n> exotic hardware such as quantum computing becomes available to run\n> algorithms that were previously infeasible. One might also argue that neural\n> networks and deep machine learning are cases of algorithm overhang: too\n> computationally expensive to work well when first invented, they were\n> shelved for a while, then dusted off when fast graphics processing units\n> made them cheap to run. Now they win contests.\n\n> [18](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_011.html#filepos280057). And\n> even if progress on the way toward the human baseline were slow.\n\n> [19](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_011.html#filepos281741).\n> ![Image](images/00020.jpg) is that part of the world’s optimization power\n> that is applied to improving the system in question. For a project operating\n> in complete isolation, one that receives no significant ongoing support from\n> the external world, we have ![Image](images/00020.jpg) ≈ 0, even though the\n> project must have started with a resource endowment (computers, scientific\n> concepts, educated personnel, etc.) that is derived from the entire world\n> economy and many centuries of development.\n\n> [20](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_011.html#filepos282057). The\n> most relevant of the seed AI’s cognitive abilities here is its ability to\n> perform intelligent design work to improve itself, i.e. its intelligence\n> amplification capability. (If the seed AI is good at enhancing another\n> system, which is good at enhancing the seed AI, then we could view these as\n> subsystems of a larger system and focus our analysis on the greater whole.)\n\n> [21](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_011.html#filepos283509). This\n> assumes that recalcitrance is not known to be so high as to discourage\n> investment altogether or divert it to some alternative project.\n\n> [22](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_011.html#filepos284845). A\n> similar example is discussed in Yudkowsky (2008b).\n\n> [23](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_011.html#filepos286041). Since\n> inputs have risen (e.g. amounts invested in building new foundries, and\n> number of people working in the semiconductor industry), Moore’s law itself\n> has not given such a rapid growth if we control for this increase in inputs.\n> Combined with advances in software, however, an 18-month doubling time in\n> performance per unit of input may be more historically plausible.\n\n> [24](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_011.html#filepos280665). Some\n> tentative attempts have been made to develop the idea of an intelligence\n> explosion within the framework of economic growth theory; see, e.g., Hanson\n> (1998b); Jones (2009); Salamon (2009). These studies have pointed to the\n> potential of extremely rapid growth given the arrival of digital minds, but\n> since endogenous growth theory is relatively poorly developed even for\n> historical and contemporary applications, any application to a potentially\n> discontinuous future context is better viewed at this stage as a source of\n> potentially useful concepts and considerations than as an exercise likely to\n> deliver authoritative forecasts. For an overview of attempts to\n> mathematically model a technological singularity, see Sandberg (2010).\n\n> [25](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_011.html#filepos288759). It is\n> of course also possible that there will be no takeoff at all. But since, as\n> argued earlier, superintelligence looks technically feasible, the absence of\n> a takeoff would likely be due to the intervention of some defeater, such as\n> an existential catastrophe. If strong superintelligence arrived not in the\n> shape of artificial intelligence or whole brain emulation but through one of\n> other paths we considered above, then a slower takeoff would be more likely.\n\n##### CHAPTER 5: DECISIVE STRATEGIC ADVANTAGE\n\n> [1](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_012.html#filepos290739). A\n> software mind might run on a single machine as opposed to a worldwide\n> network of computers; but this is not what we mean by “concentration.”\n> Instead, what we are interested in here is the extent to which power,\n> specifically power derived from technological ability, will be concentrated\n> in the advanced stages of, or immediately following, the machine\n> intelligence revolution.\n\n> [2](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_012.html#filepos292769).\n> Technology diffusion of consumer products, for example, tends to be slower\n> in developing countries (Talukdar et al. 2002). See also Keller (2004) and\n> The World Bank (2008).\n\n> [3](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_012.html#filepos293625). The\n> economic literature dealing with the theory of the firm is relevant as a\n> comparison point for the present discussion. The _locus classicus_ is Coase\n> (1937). See also, e.g., Canbäck et al. (2006); Milgrom and Roberts (1990);\n> Hart (2008); Simester and Knez (2002).\n\n> [4](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_012.html#filepos294394). On the\n> other hand, it could be especially easy to steal a seed AI, since it\n> consists of software that could be transmitted electronically or carried on\n> a portable memory device.\n\n> [5](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_012.html#filepos295462). Barber\n> (1991) suggests that the Yangshao culture (5000–3000 BC) might have used\n> silk. Sun et al. (2012) estimate, based on genetic studies, domestication of\n> the silkworm to have occurred about 4,100 years ago.\n\n> [6](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_012.html#filepos295983). Cook\n> (1984, 144). This story might be too good to withstand historical scrutiny,\n> rather like Procopius’ (_Wars_ VIII.xvii.1–7) story of how the silkworms\n> were supposedly brought to Byzantium by wandering monks, hidden in their\n> hollow bamboo staves (Hunt 2011).\n\n> [7](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_012.html#filepos296370). Wood\n> (2007); Temple (1986).\n\n> [8](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_012.html#filepos296583). Pre-\n> Columbian cultures did have the wheel but used it only for toys (probably\n> due to a lack of good draft animals).\n\n> [9](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_012.html#filepos297276). Koubi\n> (1999); Lerner (1997); Koubi and Lalman (2007); Zeira (2011); Judd et al.\n> (2012).\n\n> [10](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_012.html#filepos298005).\n> Estimated from a variety of sources. The time gap is often somewhat\n> arbitrary, depending on how exactly “equivalent” capabilities are defined.\n> Radar was used by at least two countries within a couple of years of its\n> introduction, but exact figures in months are hard to come by.\n\n> [11](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_012.html#filepos299554). The\n> RDS-6 in 1953 was the first test of a bomb with fusion reactions, but the\n> RDS-37 in 1955 was the first “true” fusion bomb, where most power came from\n> the fusion reaction.\n\n> [12](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_012.html#filepos299554).\n> Unconfirmed.\n\n> [13](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_012.html#filepos299554). Tests\n> in 1989, project cancelled in 1994.\n\n> [14](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_012.html#filepos299554).\n> Deployed system, capable of a range greater than 5,000 km.\n\n> [15](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_012.html#filepos299554).\n> Polaris missiles bought from the USA.\n\n> [16](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_012.html#filepos299554).\n> Current work is underway on the Taimur missile, likely based on Chinese\n> missiles.\n\n> [17](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_012.html#filepos299554). The\n> RSA-3 rocket tested 1989–90 was intended for satellite launches and/or as an\n> ICBM.\n\n> [18](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_012.html#filepos299554). MIRV =\n> multiple independently targetable re-entry vehicle, a technology that\n> enables a single ballistic missile to carry multiple warheads that can be\n> programmed to hit different targets.\n\n> [19](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_012.html#filepos299554). The\n> Agni V system is not yet in service.\n\n> [20](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_012.html#filepos299071). Ellis\n> (1999).\n\n> [21](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_012.html#filepos299983). If we\n> model the situation as one where the lag time between projects is drawn from\n> a normal distribution, then the likely distance between the leading project\n> and its closest follower will also depend on how many projects there are. If\n> there are a vast number of projects, then the distance between the first two\n> is likely small even if the variance of the distribution is moderately high\n> (though the expected gap between the lead and the second project declines\n> very slowly with the number of competitors if completion times are normally\n> distributed). However, it is unlikely that there will be a vast number of\n> projects that are each well enough resourced to be serious contenders.\n> (There might be a greater number of projects if there are a large number of\n> different basic approaches that could be pursued, but in that case many of\n> those approaches are likely to prove dead ends.) As suggested, empirically\n> we seem to find that there is usually no more than a handful of serious\n> competitors pursuing any one specific technological goal. The situation is\n> somewhat different in a consumer market where there are many niches for\n> slightly different products and where barriers to entry are low. There are\n> lots of one-person projects designing T-shirts, but only a few firms in the\n> world developing the next generation of graphics cards. (Two firms, AMD and\n> NVIDIA, enjoy a near duopoly at the moment, though Intel is also competing\n> at the lower-performance end of the market.)\n\n> [22](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_012.html#filepos302942).\n> Bostrom (2006c). One could imagine a singleton whose existence is invisible\n> (e.g. a superintelligence with such advanced technology or insight that it\n> could subtly control world events without any human noticing its\n> interventions); or a singleton that voluntarily imposes very strict\n> limitations on its own exercise of power (e.g. punctiliously confining\n> itself to ensuring that certain treaty-specified international rules—or\n> libertarian principles—are respected). How likely any particular kind of\n> singleton is to arise is of course an empirical question; but _conceptually_\n> , at least, it is possible to have a good singleton, a bad singleton, a\n> rambunctiously diverse singleton, a blandly monolithic singleton, a\n> crampingly oppressive singleton, or a singleton more akin to an extra law of\n> nature than to a yelling despot.\n\n> [23](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_012.html#filepos306030). Jones\n> (1985, 344).\n\n> [24](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_012.html#filepos306337). It\n> might be significant that the Manhattan Project was carried out during\n> wartime. Many of the scientists who participated claimed to be primarily\n> motivated by the wartime situation and the fear that Nazi Germany might\n> develop atomic weapons ahead of the Allies. It might be difficult for many\n> governments to mobilize a similarly intensive and secretive effort in\n> peacetime. The Apollo program, another iconic science/engineering\n> megaproject, received a strong impetus from the Cold War rivalry.\n\n> [25](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_012.html#filepos307567). Though\n> even if they _were_ looking hard, it is not clear that they would appear\n> (publicly) to be doing so.\n\n> [26](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_012.html#filepos309856).\n> Cryptographic techniques could enable the collaborating team to be\n> physically dispersed. The only weak link in the communication chain might be\n> the input stage, where the physical act of typing could potentially be\n> observed. But if indoor surveillance became common (by means of microscopic\n> recording devices), those keen on protecting their privacy might develop\n> countermeasures (e.g. special closets that could be sealed off from would-be\n> eavesdropping devices). Whereas physical space might become transparent in a\n> coming surveillance age, cyberspace might possibly become more protected\n> through wider adoption of stronger cryptographic protocols.\n\n> [27](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_012.html#filepos310333). A\n> totalitarian state might take recourse to even more coercive measures.\n> Scientists in relevant fields might be swept up and put into work camps,\n> akin to the “academic villages” in Stalinist Russia.\n\n> [28](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_012.html#filepos311815). When\n> the level of public concern is relatively low, some researchers might\n> welcome a little bit of public fear-mongering because it draws attention to\n> their work and makes the area they work in seem important and exciting. When\n> the level of concern becomes greater, the relevant research communities\n> might change their tune as they begin to worry about funding cuts,\n> regulation, and public backlash. Researchers in neighboring disciplines—such\n> as those parts of computer science and robotics that are not very relevant\n> to artificial general intelligence—might resent the drift of funding and\n> attention away from their own research areas. These researchers might also\n> correctly observe that _their_ work carries no risk whatever of leading to a\n> dangerous intelligence explosion. (Some historical parallels might be drawn\n> with the career of the idea of nanotechnology; see Drexler [2013].)\n\n> [29](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_012.html#filepos314541). These\n> have been successful in that they have achieved at least some of what they\n> set out to do. How successful they have been in a broader sense (taking into\n> account cost-effectiveness and so forth) is harder to determine. In the case\n> of the International Space Station, for example, there have been huge cost\n> overruns and delays. For details of the problems encountered by the project,\n> see NASA (2013). The Large Hadron Collider project has had some major\n> setbacks, but this might be due to the inherent difficulty of the task. The\n> Human Genome Project achieved success in the end, but seems to have received\n> a speed boost from being forced to compete with Craig Venter’s private\n> corporate effort. Internationally sponsored projects to achieve controlled\n> fusion energy have failed to deliver on expectations, despite massive\n> investment; but again, this might be attributable to the task turning out to\n> be more difficult than anticipated.\n\n> [30](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_012.html#filepos314828). US\n> Congress, Office of Technology Assessment (1995).\n\n> [31](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_012.html#filepos316727).\n> Hoffman (2009); Rhodes (2008).\n\n> [32](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_012.html#filepos317275). Rhodes\n> (1986).\n\n> [33](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_012.html#filepos317520). The US\n> Navy’s code-breaking organization, OP-20-G, apparently ignored an invitation\n> to gain full knowledge of Britain’s anti-Enigma methods, and failed to\n> inform higher-level US decision makers of Britain’s offer to share its\n> cryptographic secrets (Burke 2001). This gave American leaders the\n> impression that Britain was withholding important information, a cause of\n> friction throughout the war. Britain did share with the Soviet government\n> some of the intelligence they had gleaned from decrypted German\n> communications. In particular, Russia was warned about the German\n> preparations for Operation Barbarossa. But Stalin refused to believe the\n> warning, partly because the British did not disclose how they had obtained\n> the information.\n\n> [34](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_012.html#filepos319962). For a\n> few years, Russell seems to have advocated the threat of nuclear war to\n> persuade Russia to accept the Baruch plan; later, he was a strong proponent\n> of mutual nuclear disarmament (Russell and Griffin 2001). John von Neumann\n> is reported to have believed that a war between the United States and Russia\n> was inevitable, and to have said, “If you say why not bomb them [the\n> Russians] tomorrow, I say why not bomb them today? If you say today at five\n> o’clock, I say why not one o’clock?” (It is possible that he made this\n> notorious statement to burnish his anti-communist credentials with US\n> Defense hawks in the McCarthy era. Whether von Neumann, had he been in\n> charge of US policy, would actually have launched a first strike is\n> impossible to ascertain. See Blair [1957], 96.)\n\n> [35](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_012.html#filepos320776).\n> Baratta (2004).\n\n> [36](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_012.html#filepos324941). If the\n> AI is controlled by a group of humans, the problem may apply to this human\n> group, though it is possible that new ways of reliably committing to an\n> agreement will be available by this time, in which case even human groups\n> could avoid this problem of potential internal unraveling and overthrow by a\n> sub-coalition.\n\n##### CHAPTER 6: COGNITIVE SUPERPOWERS\n\n> [1](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_013.html#filepos328466). In what\n> sense is humanity a dominant species on Earth? Ecologically speaking, humans\n> are the most common large (~50 kg) animal, but the total human dry biomass\n> (~100 billion kg) is not so impressive compared with that of ants, the\n> family Formicidae (300 billion–3,000 billion kg). Humans and human utility\n> organisms form a very small part (<0.001) of total global biomass. However,\n> croplands and pastures are now among the largest ecosystems on the planet,\n> covering about 35% of the ice-free land surface (Foley et al. 2007). And we\n> appropriate nearly a quarter of net primary productivity according to a\n> typical assessment (Haberl et al. 2007), though estimates range from 3 to\n> over 50% depending mainly on varying definitions of the relevant terms\n> (Haberl et al. 2013). Humans also have the largest geographic coverage of\n> any animal species and top the largest number of different food chains.\n\n> [2](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_013.html#filepos329092).\n> Zalasiewicz et al. (2008).\n\n> [3](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_013.html#filepos329242). See\n> first note to this chapter.\n\n> [4](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_013.html#filepos333397).\n> Strictly speaking, this may not be quite correct. Intelligence in the human\n> species ranges all the way down to approximately zero (e.g. in the case of\n> embryos or patients in permanent vegetative state). In qualitative terms,\n> the maximum difference in cognitive ability within the human species is\n> therefore perhaps greater than the difference between any human and a\n> superintelligence. But the point in the text stands if we read “human” as\n> “normally functioning adult.”\n\n> [5](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_013.html#filepos334559).\n> Gottfredson (2002). See also Carroll (1993) and Deary (2001).\n\n> [6](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_013.html#filepos335454). See\n> Legg (2008). Roughly, Legg proposes to measure a reinforcement-learning\n> agent as its expected performance in all reward-summable environments, where\n> each such environment receives a weight determined by its Kolmogorov\n> complexity. We will explain what is meant by reinforcement learning in\n> [Chapter\n> 12](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos645293). See\n> also Dowe and Hernández-Orallo (2012) and Hibbard (2011).\n\n> [7](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_013.html#filepos337757). With\n> regard to technology research in areas like biotechnology and\n> nanotechnology, what a superintelligence would excel at is the design and\n> modeling of new structures. To the extent that design ingenuity and modeling\n> cannot substitute for physical experimentation, the superintelligence’s\n> performance advantage may be qualified by its level of access to the\n> requisite experimental apparatus.\n\n> [8](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_013.html#filepos338153). E.g.,\n> Drexler (1992, 2013).\n\n> [9](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_013.html#filepos338713). A\n> narrow-domain AI could of course have significant commercial applications,\n> but this does not mean that it would have the economic productivity\n> superpower. For example, even if a narrow-domain AI earned its owners\n> several billions of dollars a year, this would still be four orders of\n> magnitude less than the rest of the world economy. In order for the system\n> directly and substantially to increase world product, an AI would need to be\n> able to perform many kinds of work; that is, it would need competence in\n> many domains.\n\n> [10](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_013.html#filepos345777). The\n> criterion does not rule out all scenarios in which the AI fails. For\n> example, the AI might rationally take a gamble that has a high chance of\n> failing. In this case, however, the criterion could take the form that (a)\n> the AI should make an unbiased estimate of the gamble’s low chance of\n> success and (b) there should be no better gamble available to the AI that we\n> present-day humans can think of but that the AI overlooks.\n\n> [11](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_013.html#filepos348623). Cf.\n> Freitas (2000) and Vassar and Freitas (2006).\n\n> [12](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_013.html#filepos351534).\n> Yudkowsky (2008a).\n\n> [13](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_013.html#filepos357604).\n> Freitas (1980); Freitas and Merkle (2004, Chap. 3); Armstrong and Sandberg\n> (2013).\n\n> [14](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_013.html#filepos358842). See,\n> e.g., Huffman and Pless (2003), Knill et al. (2000), Drexler (1986).\n\n> [15](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_013.html#filepos361802). That\n> is to say, the distance would be small on some “natural” metric, such as the\n> logarithm of the size of the population that could be sustainably supported\n> at subsistence level by a given level of capability if all resources were\n> devoted to that end.\n\n> [16](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_013.html#filepos362526). This\n> estimate is based on the WMAP estimate of a cosmological baryon density of\n> 9.9×10–30 g/cm3 and assumes that 90% of the mass is intergalactic gas, that\n> some 15% of the galactic mass is stars (about 80% of baryonic matter), and\n> that the average star weighs in at 0.7 solar masses (Read and Trentham 2005;\n> Carroll and Ostlie 2007).\n\n> [17](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_013.html#filepos362703).\n> Armstrong and Sandberg (2013).\n\n> [18](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_013.html#filepos363003). Even\n> at 100% of _c_ (which is unattainable for objects with nonzero rest mass)\n> the number of reachable galaxies is only about 6×109. (Cf. Gott et al.\n> [2005] and Heyl [2005].) We are assuming that our current understanding of\n> the relevant physics is correct. It is hard to be very confident in any\n> upper bound, since it is at least conceivable that a superintelligent\n> civilization might extend its reach in some way that we take to be\n> physically impossible (for instance, by building time machines, by spawning\n> new inflationary universes, or by some other, as yet unimagined means).\n\n> [19](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_013.html#filepos363505). The\n> number of habitable planets per star is currently uncertain, so this is\n> merely a crude estimate. Traub (2012) predicts that one-third of stars in\n> spectral classes F, G, or K have at least one terrestrial planet in the\n> habitable zone; see also Clavin (2012). FGK stars form about 22.7% of the\n> stars in the solar neighborhood, suggesting that 7.6% of stars have\n> potentially suitable planets. In addition, there might be habitable planets\n> around the more numerous M stars (Gilster 2012). See also Robles et al.\n> (2008).\n\n> It would not be necessary to subject human bodies to the rigors of\n> intergalactic travels. AIs could oversee the colonization process. _Homo\n> sapiens_ could be brought along as information, which the AIs could later\n> use to instantiate specimens of our species. For example, genetic\n> information could be synthesized into DNA, and a first generation of humans\n> could be incubated, raised, and educated by AI guardians taking an\n> anthropomorphic guise.\n\n> [20](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_013.html#filepos364459).\n> O’Neill (1974).\n\n> [21](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_013.html#filepos365208). Dyson\n> (1960) claims to have gotten the basic idea from science fiction writer Olaf\n> Stapledon (1937), who in turn might have been inspired by similar thoughts\n> by J. D. Bernal (Dyson 1979, 211).\n\n> [22](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_013.html#filepos365798).\n> Landauer’s principle states that there is a minimum amount of energy\n> required to change one bit of information, known as the Landauer limit,\n> equal to _kT_ ln 2, where _k_ is the Boltzmann constant (1.38×10–23 J/K) and\n> _T_ is the temperature. If we assume the circuitry is maintained at around\n> 300 K, then 1026 watts allows us to erase approximately 1047 bits per\n> second. (On the achievable efficiency of nanomechanical computational\n> devices, see Drexler [1992]. See also Bradbury [1999]; Sandberg [1999];\n> Ćirković [2004]. The foundations of Landauer’s principle are still somewhat\n> in dispute; see, e.g., Norton [2011].)\n\n> [23](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_013.html#filepos366123). Stars\n> vary in their power output, but the Sun is a fairly typical main-sequence\n> star.\n\n> [24](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_013.html#filepos366711). A more\n> detailed analysis might consider more closely what types of computation we\n> are interested in. The number of _serial_ computations that can be performed\n> is quite limited, since a fast serial computer must be small in order to\n> minimize communications lags within the different parts of the computer.\n> There are also limits on the number of bits that can be stored, and, as we\n> saw, on the number of irreversible computational steps (involving the\n> erasure of information) that can be performed.\n\n> [25](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_013.html#filepos369916). We are\n> assuming here that there are no extraterrestrial civilizations that might\n> get in the way. We are also assuming that the simulation hypothesis is\n> false. See Bostrom (2003a). If either of these assumptions is incorrect,\n> there may be important non-anthropogenic risks—ones that involve intelligent\n> agency of a nonhuman sort. See also Bostrom (2003b, 2009c).\n\n> [26](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_013.html#filepos371094). At\n> least a wise singleton that grasped the idea of evolution could, in\n> principle, have embarked on a eugenics program by means of which it could\n> slowly have raised its level of collective intelligence.\n\n> [27](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_013.html#filepos371501).\n> Tetlock and Belkin (1996).\n\n> [28](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_013.html#filepos371705). To be\n> clear: colonizing and re-engineering a large part of the accessible universe\n> is not currently within our _direct_ reach. Intergalactic colonization is\n> far beyond today’s technology. The point is that we could in principle use\n> our present capabilities to develop the additional capabilities that would\n> be needed, thus placing the accomplishment within our _indirect_ reach. It\n> is of course also true that humanity is not currently a singleton and that\n> we do not know that we would never face intelligent opposition from some\n> external power if we began to re-engineer the accessible universe. To meet\n> the wise-singleton sustainability threshold, however, it suffices that one\n> possesses a capability set such that if a wise singleton facing no\n> intelligent opposition had possessed this capability set then the\n> colonization and reengineering of a large part of the accessible universe\n> would be within its indirect reach.\n\n> [29](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_013.html#filepos372949).\n> Sometimes it might be useful to speak of two AIs as each having a given\n> superpower. In an extended sense of the word, one could thus conceive of a\n> superpower as something that an agent has relative to some field of\n> action—in this example, perhaps a field that includes all of human\n> civilization but excludes the other AI.\n\n\n##### CHAPTER 7: THE SUPERINTELLIGENT WILL\n\n> [1](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_014.html#filepos376778). This is\n> of course not to deny that differences that appear small visually can be\n> functionally profound.\n\n> [2](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_014.html#filepos377968).\n> Yudkowsky (2008a, 310).\n\n> [3](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_014.html#filepos382502). David\n> Hume, the Scottish Enlightenment philosopher, thought that beliefs alone\n> (say, about what is a good thing to do) cannot motivate action: some desire\n> is required. This would support the orthogonality thesis by undercutting one\n> possible objection to it, namely that sufficient intelligence might entail\n> the acquisition of certain beliefs which would then necessarily produce\n> certain motivations. However, although the orthogonality thesis can draw\n> support from the Humean theory of motivation, it does not presuppose it. In\n> particular, one need not maintain that beliefs alone can never motivate\n> action. It would suffice to assume, for example, that an agent—be it ever so\n> intelligent—can be motivated to pursue any course of action if the agent\n> happens to have certain desires of some sufficient, overriding strength.\n> Another way in which the orthogonality thesis could be true even if the\n> Humean theory of motivation is false is if arbitrarily high intelligence\n> does not entail the acquisition of any such beliefs as are (putatively)\n> motivating on their own. A third way in which it might be possible for the\n> orthogonality thesis to be true even if the Humean theory were false is if\n> it is possible to build an agent (or more neutrally, an “optimization\n> process”) with arbitrarily high intelligence but with constitution so alien\n> as to contain no clear functional analogs to what in humans we call\n> “beliefs” and “desires.” (For some recent attempts to defend the Humean\n> theory of motivation see Smith [1987], Lewis [1988], and Sinhababu [2009].)\n\n> [4](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_014.html#filepos382625). For\n> instance, Derek Parfit has argued that certain basic preferences would be\n> irrational, such as that of an otherwise normal agent who has “Future-\n> Tuesday-Indifference”:\n\n> A certain hedonist cares greatly about the quality of his future\n> experiences. With one exception, he cares equally about all the parts of his\n> future. The exception is that he has Future-Tuesday-Indifference. Throughout\n> every Tuesday he cares in the normal way about what is happening to him. But\n> he never cares about possible pains or pleasures on a future Tuesday…. This\n> indifference is a bare fact. When he is planning his future, it is simply\n> true that he always prefers the prospect of great suffering on a Tuesday to\n> the mildest pain on any other day. (Parfit [1986, 123–4]; see also Parfit\n> [2011])\n\n> For our purposes, we need take no stand on whether Parfit is right that this\n> agent is irrational, so long as we grant that it is not necessarily\n> unintelligent in the instrumental sense explained in the text. Parfit’s\n> agent could have impeccable instrumental rationality, and therefore great\n> intelligence, even if he falls short on some kind of sensitivity to\n> “objective reason” that might be required of a fully rational agent.\n> Therefore, this kind of example does not undermine the orthogonality thesis.\n\n> [5](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_014.html#filepos382952). Even if\n> there are objective moral facts that any fully rational agent would\n> comprehend, and even if these moral facts are somehow intrinsically\n> motivating (such that anybody who fully comprehends them is necessarily\n> motivated to act in accordance with them), this need not undermine the\n> orthogonality thesis. The thesis could still be true if an agent could have\n> impeccable _instrumental_ rationality even whilst lacking some other faculty\n> constitutive of rationality proper, or some faculty required for the full\n> comprehension of the objective moral facts. (An agent could also be\n> extremely intelligent, even superintelligent, without having full\n> instrumental rationality in every domain.)\n\n> [6](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_014.html#filepos383575). For\n> more on the orthogonality thesis, see Bostrom (2012) and Armstrong (2013).\n\n> [7](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_014.html#filepos385097).\n> Sandberg and Bostrom (2008).\n\n> [8](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_014.html#filepos388252). Stephen\n> Omohundro has written two pioneering papers on this topic (Omohundro 2007,\n> 2008). Omohundro argues that all advanced AI systems are likely to exhibit a\n> number of “basic drives,” by which he means “tendencies which will be\n> present unless explicitly counteracted.” The term “AI drive” has the\n> advantage of being short and evocative, but it has the disadvantage of\n> suggesting that the instrumental goals to which it refers influence the AI’s\n> decision-making in the same way as psychological drives influence human\n> decision-making, i.e. via a kind of phenomenological tug on our ego which\n> our willpower may occasionally succeed in resisting. That connotation is\n> unhelpful. One would not normally say that a typical human being has a\n> “drive” to fill out their tax return, even though filing taxes may be a\n> fairly convergent instrumental goal for humans in contemporary societies (a\n> goal whose realization averts trouble that would prevent us from realizing\n> many of our final goals). Our treatment here also differs from that of\n> Omohundro in some other more substantial ways, although the underlying idea\n> is the same. (See also Chalmers [2010] and Omohundro [2012].)\n\n> [9](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_014.html#filepos391419).\n> Chislenko (1997).\n\n> [10](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_014.html#filepos392750). See\n> also Shulman (2010b).\n\n> [11](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_014.html#filepos393760). An\n> agent might also change its goal _representation_ if it changes its\n> ontology, in order to transpose its old representation into the new\n> ontology; cf. de Blanc (2011).\n\n> Another type of factor that might make an _evidential decision theorist_\n> undertake various actions, including changing its final goals, is the\n> evidential import of deciding to do so. For example, an agent that follows\n> evidential decision theory might believe that there exist other agents like\n> it in the universe, and that its own actions will provide some evidence\n> about how those other agents will act. The agent might therefore choose to\n> adopt a final goal that is altruistic towards those other evidentially\n> linked agents, on grounds that this will give the agent evidence that those\n> other agents will have chosen to act in like manner. An equivalent outcome\n> might be obtained, however, without changing one’s final goals, by choosing\n> in each instant to act _as if_ one had those final goals.\n\n> [12](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_014.html#filepos394646). An\n> extensive psychological literature explores adaptive preference formation.\n> See, e.g., Forgas et al. (2010).\n\n> [13](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_014.html#filepos395769). In\n> formal models, the value of information is quantified as the difference\n> between the expected value realized by optimal decisions made with that\n> information and the expected value realized by optimal decisions made\n> without it. (See, e.g., Russell and Norvig [2010].) It follows that the\n> value of information is never negative. It also follows that any information\n> you know will never affect any decision you will ever make has zero value\n> for you. However, this kind of model assumes several idealizations which are\n> often invalid in the real world—such as that knowledge has no final value\n> (meaning that knowledge has only instrumental value and is not valuable for\n> its own sake) and that agents are not transparent to other agents.\n\n> [14](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_014.html#filepos396408). E.g.,\n> Hájek (2009).\n\n> [15](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_014.html#filepos397521). This\n> strategy is exemplified by the sea squirt larva, which swims about until it\n> finds a suitable rock, to which it then permanently affixes itself. Cemented\n> in place, the larva has less need for complex information processing, whence\n> it proceeds to digest part of its own brain (its cerebral ganglion). One can\n> observe the same phenomenon in some academics when they have been granted\n> tenure.\n\n> [16](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_014.html#filepos397783).\n> Bostrom (2012).\n\n> [17](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_014.html#filepos401712).\n> Bostrom (2006c).\n\n> [18](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_014.html#filepos402049). One\n> could reverse the question and look instead at possible reasons for a\n> superintelligent singleton _not_ to develop some technological capabilities.\n> These include the following: (a) the singleton foresees that it will have no\n> use for the capability; (b) the development cost is too large relative to\n> its anticipated utility (e.g. if the technology will never be suitable for\n> achieving any of the singleton’s ends, or if the singleton has a very high\n> discount rate that strongly discourages investment); (c) the singleton has\n> some final value that requires abstention from particular avenues of\n> technology development; (d) if the singleton is not certain it will remain\n> stable, it might prefer to refrain from developing technologies that could\n> threaten its internal stability or that would make the consequences of\n> dissolution worse (for instance, a world government may not wish to develop\n> technologies that would facilitate rebellion, even if they have some good\n> uses, nor develop technologies for the easy production of weapons of mass\n> destruction which could wreak havoc if the world government were to\n> dissolve); (e) similarly, the singleton might have made some kind of binding\n> strategic commitment not to develop some technology, a commitment that\n> remains operative even if it would now be convenient to develop it. (Note,\n> however, that some _current_ reasons for technology development would _not_\n> apply to a singleton: for instance, reasons arising from arms races.)\n\n> [19](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_014.html#filepos405553).\n> Suppose that an agent discounts resources obtained in the future at an\n> exponential rate, and that because of the light speed limitation the agent\n> can only increase its resource endowment at a polynomial rate. Would this\n> mean that there will be some time after which the agent would not find it\n> worthwhile to continue acquisitive expansion? No, because although the\n> present value of the resources obtained at future times would asymptote to\n> zero the further into the future we look, _so would the present cost of\n> obtaining them_. The present cost of sending out one more von Neumann probe\n> a 100 million years from now (possibly using some resource acquired some\n> short time earlier) would be diminished by the same discount factor that\n> would diminish the present value of the future resources that the extra\n> probe would acquire (modulo a constant factor).\n\n> [20](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_014.html#filepos406458). While\n> the volume reached by colonization probes at a given time might be roughly\n> spherical and expanding with a rate proportional to the square of time\n> elapsed since the first probe was launched (~_t_ 2), the amount of resources\n> contained within this volume will follow a less regular growth pattern,\n> since the distribution of resources is inhomogeneous and varies over several\n> scales. Initially, the growth rate might be ~_t_ 2 as the home planet is\n> colonized; then the growth rate might become spiky as nearby planets and\n> solar systems are colonized; then, as the roughly disc-shaped volume of the\n> Milky Way gets filled out, the growth rate might even out, to be\n> approximately proportional to _t_ ; then the growth rate might again become\n> spiky as nearby galaxies are colonized; then the growth rate might again\n> approximate ~_t_ 2 as expansion proceeds on a scale over which the\n> distribution of galaxies is roughly homogeneous; then another period of\n> spiky growth followed by smooth ~_t_ 2 growth as galactic superclusters are\n> colonized; until ultimately the growth rate starts a final decline,\n> eventually reaching zero as the expansion speed of the universe increases to\n> such an extent as to make further colonization impossible.\n\n> [21](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_014.html#filepos407388). The\n> simulation argument may be of particular importance in this context. A\n> superintelligent agent may assign a significant probability to hypotheses\n> according to which it lives in a computer simulation and its percept\n> sequence is generated by another superintelligence, and this might generate\n> various convergent instrumental reasons depending on the agent’s guesses\n> about what types of simulations it is most likely to be in. Cf. Bostrom\n> (2003a).\n\n> [22](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_014.html#filepos408119).\n> Discovering the basic laws of physics and other fundamental facts about the\n> world is a convergent instrumental goal. We may place it under the rubric\n> “cognitive enhancement” here, though it could also be derived from the\n> “technology perfection” goal (since novel physical phenomena might enable\n> novel technologies).\n\n##### CHAPTER 8: IS THE DEFAULT OUTCOME DOOM?\n\n> [1](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_015.html#filepos412632). Some\n> additional existential risk resides in scenarios in which humanity survives\n> in some highly suboptimal state or in which a large portion of our potential\n> for desirable development is irreversibly squandered. On top of this, there\n> may be existential risks associated with the lead-up to a potential\n> intelligence explosion, arising, for example, from war between countries\n> competing to develop superintelligence first.\n\n> [2](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_015.html#filepos415674). There\n> is an important moment of vulnerability when the AI first realizes the need\n> for such concealment (an event which we may term _the conception of\n> deception_). This initial realization would not itself be deliberately\n> concealed when it occurs. But having had this realization, the AI might move\n> swiftly to hide the fact that the realization has occurred, while setting up\n> some covert internal dynamic (perhaps disguised as some innocuous process\n> that blends in with all the other complicated processes taking place in its\n> mind) that will enable it to continue to plan its long-term strategy in\n> privacy.\n\n> [3](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_015.html#filepos415825). Even\n> human hackers can write small and seemingly innocuous programs that do\n> completely unexpected things. (For examples, see some the winning entries in\n> the International Obfuscated C Code Contest.)\n\n> [4](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_015.html#filepos423773). The\n> point that some AI control measures could appear to work within a fixed\n> context yet fail catastrophically when the context changes is also\n> emphasized by Eliezer Yudkowsky; see, e.g., Yudkowsky (2008a).\n\n> [5](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_015.html#filepos432806). The\n> term seems to have been coined by science-fiction writer Larry Niven (1973),\n> but is based on real-world brain stimulation reward experiments; cf. Olds\n> and Milner (1954) and Oshima and Katayama (2010). See also Ring and Orseau\n> (2011).\n\n> [6](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_015.html#filepos433385). Bostrom\n> (1997).\n\n> [7](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_015.html#filepos436660). There\n> might be some possible implementations of a reinforcement learning mechanism\n> that would, when the AI discovers the wireheading solution, lead to a safe\n> incapacitation rather than to infrastructure profusion. The point is that\n> this could easily go wrong and fail for unexpected reasons.\n\n> [8](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_015.html#filepos437636). This\n> was suggested by Marvin Minsky (_vide_ Russell and Norvig [2010, 1039]).\n\n> [9](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_015.html#filepos448463). The\n> issue of which kinds of digital mind would be conscious, in the sense of\n> having subjective phenomenal experience, or “qualia” in philosopher-speak,\n> is important in relation to this point (though it is irrelevant to many\n> other parts of this book). One open question is how hard it would be to\n> accurately estimate how a human-like being would behave in various\n> circumstances without simulating its brain in enough detail that the\n> simulation is conscious. Another question is whether there are generally\n> useful algorithms for a superintelligence, for instance reinforcement-\n> learning techniques, such that the implementation of these algorithms would\n> generate qualia. Even if we judge the probability that any such subroutines\n> would be conscious to be fairly small, the number of instantiations might be\n> so large that even a small risk that they might experience suffering ought\n> to be accorded significant weight in our moral calculation. See also\n> Metzinger (2003, Chap. 8).\n\n> [10](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_015.html#filepos448981).\n> Bostrom (2002a, 2003a); Elga (2004).\n\n##### CHAPTER 9: THE CONTROL PROBLEM\n\n> [1](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_016.html#filepos451785). E.g.,\n> Laffont and Martimort (2002).\n\n> [2](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_016.html#filepos452247). Suppose\n> a majority of voters want their country to build some particular kind of\n> superintelligence. They elect a candidate who promises to do their bidding,\n> but they might find it difficult to ensure that the candidate, once in\n> power, will follow through on her campaign promise and pursue the project in\n> the way that the voters intended. Supposing she is true to her word, she\n> instructs her government to contract with an academic or industry consortium\n> to carry out the work; but again there are agency problems: the bureaucrats\n> in the government department might have their own views about what should be\n> done and may implement the project in a way that respects the letter but not\n> the spirit of the leader’s instructions. Even if the government department\n> does its job faithfully, the contracted scientific partners might have their\n> own separate agendas. The problem recurs on many levels. The director of one\n> of the participating laboratories might lie awake worrying about a\n> technician introducing an unsanctioned element into the design—imagining Dr.\n> T. R. Eason sneaking into his office late one night, logging into the\n> project code base, rewriting a part of the seed AI’s goal system. Where it\n> was supposed to say “serve humanity,” it now says “serve Dr. T. R. Eason.”\n\n> [3](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_016.html#filepos455925). Even\n> for superintelligence development, though, there could be a role for\n> behavioral testing—as one auxiliary element within a wider battery of safety\n> measures. Should an AI misbehave in its developmental phase, something is\n> clearly awry—though, importantly, the converse does not hold.\n\n> [4](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_016.html#filepos459312). In a\n> classic exploit from 1975, Steven Dompier wrote a program for the Altair\n> 8800 that took advantage of this effect (and the absence of shielding around\n> the microcomputer’s case). Running the program caused the emission of\n> electromagnetic waves that would produce music when one held a transistor\n> radio close to the computer (Driscoll 2012). The young Bill Gates, who\n> attended a demo, reported that he was impressed and mystified by the hack\n> (Gates 1975). There are in any case plans to design future chips with built-\n> in Wi-Fi capabilities (Greene 2012).\n\n> [5](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_016.html#filepos459793). It is\n> no light matter to have held a conviction, which, had we had an opportunity\n> to act upon it, could have resulted in the ruination of all our cosmic\n> endowment. Perhaps one could argue for the following principle: if somebody\n> has in the past been certain on _N_ occasions that a system has been\n> improved sufficiently to make it safe, and each time it was revealed that\n> they were wrong, then on the next occasion they are not entitled to assign a\n> credence greater than 1/(_N_ \\+ 1) to the system being safe.\n\n> [6](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_016.html#filepos461872). In one\n> informal experiment, the role of the AI was played by an intelligent human.\n> Another individual played the role of gatekeeper and was tasked with not\n> letting the AI out of the box. The AI could communicate with the gatekeeper\n> only by text and was given two hours to persuade the gatekeeper to let it\n> out. In three cases out of five, with different individuals playing the\n> gatekeeper, the AI escaped (Yudkowsky 2002). What a human can do, a\n> superintelligence can do too. (The reverse, of course, does not hold. Even\n> if the task for a real superintelligence were harder—maybe the gatekeepers\n> would be more strongly motivated to refrain from releasing the AI than the\n> individuals playing gatekeeper in the experiment—the superintelligence might\n> still succeed where a human would fail.)\n\n> [7](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_016.html#filepos462496). One\n> should not overstate the marginal amount of safety that could be gained in\n> this way. Mental imagery can substitute for graphical display. Consider the\n> impact books can have on people—and books are not even interactive.\n\n> [8](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_016.html#filepos463490). See\n> also Chalmers (2010). It would be a mistake to infer from this that there is\n> _no_ possible use in building a system that will never be observed by any\n> outside entity. One might place a final value on what goes on inside such a\n> system. Also, other people might have preferences about what goes on inside\n> such a system, and might therefore be influenced by its creation or the\n> promise of its creation. Knowledge of the existence of certain kinds of\n> isolated systems (ones containing observers) can also induce anthropic\n> uncertainty in outside observers, which may influence their behavior.\n\n> [9](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_016.html#filepos466704). One\n> might wonder why social integration is considered a form of capability\n> control. Should it not instead be classified as a motivation selection\n> method on the ground that it involves seeking to influence a system’s\n> behavior by means of incentives? We will look closely at motivation\n> selection presently; but, in answer to this question, we are construing\n> motivation selection as a cluster of control methods that work by selecting\n> or shaping a system’s final goals—goals sought for their own sakes rather\n> than for instrumental reasons. Social integration does not target a system’s\n> final goals, so it is not motivation selection. Rather, social integration\n> aims to limit the system’s effective capabilities: it seeks to render the\n> system incapable of achieving a certain set of outcomes—outcomes in which\n> the system attains the benefits of defection without suffering the\n> associated penalties (retribution, and loss of the gains from\n> collaboration). The hope is that by limiting which outcomes the system is\n> able to attain, the system will find that the most effective remaining means\n> of realizing its final goals is to behave cooperatively.\n\n> [10](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_016.html#filepos469598). This\n> approach may be somewhat more promising in the case of an emulation believed\n> to have anthropomorphic motivations.\n\n> [11](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_016.html#filepos470693). I owe\n> this idea to Carl Shulman.\n\n> [12](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_016.html#filepos470929).\n> Creating a cipher certain to withstand a superintelligent code-breaker is a\n> nontrivial challenge. For example, traces of random numbers might be left in\n> some observer’s brain or in the microstructure of the random generator, from\n> whence the superintelligence can retrieve them; or, if pseudorandom numbers\n> are used, the superintelligence might guess or discover the seed from which\n> they were generated. Further, the superintelligence could build large\n> quantum computers, or even discover unknown physical phenomena that could be\n> used to construct new kinds of computers.\n\n> [13](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_016.html#filepos471150). The AI\n> could wire itself to _believe_ that it had received a reward tokens, but\n> this should not make it wirehead if it is designed to want the reward tokens\n> (as opposed to wanting to be in a state in which it has certain beliefs\n> about the reward tokens).\n\n> [14](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_016.html#filepos475450). For\n> the original article, see Bostrom (2003a). See also Elga (2004).\n\n> [15](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_016.html#filepos476404).\n> Shulman (2010a).\n\n> [16](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_016.html#filepos477138).\n> Basement-level reality presumably contains more computational resources than\n> simulated reality, since any computational processes occurring in a\n> simulation are also occurring on the computer running the simulation.\n> Basement-level reality might also contain a wealth of other physical\n> resources which could be hard for simulated agents to access—agents that\n> exist only at the indulgence of powerful simulators who may have other uses\n> in mind for those resources. (Of course, the inference here is not strictly\n> deductively valid: in principle, it could be the case that universes in\n> which simulations are run contain so much more resources that simulated\n> civilizations on average have access to more resources than non-simulated\n> civilizations, even though each non-simulated civilization that runs\n> simulations has more resources than all the civilizations it simulates do\n> combined.)\n\n> [17](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_016.html#filepos479632). There\n> are various further esoteric considerations that might bear on this matter,\n> the implications of which have not yet been fully analyzed. These\n> considerations may ultimately be crucially important in developing an all-\n> things-considered approach to dealing with the prospect of an intelligence\n> explosion. However, it seems unlikely that we will succeed in figuring out\n> the practical import of such esoteric arguments unless we have first made\n> some progress on the more mundane kinds of consideration that are the topic\n> of most of this book.\n\n> [18](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_016.html#filepos481535). Cf.,\n> e.g., Quine and Ullian (1978).\n\n> [19](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_016.html#filepos482650). Which\n> an AI might investigate by considering the performance characteristics of\n> various basic computational functionalities, such as the size and capacity\n> of various data buses, the time it takes to access different parts of\n> memory, the incidence of random bit flips, and so forth.\n\n> [20](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_016.html#filepos483298).\n> Perhaps the prior could be (a computable approximation of) the Solomonoff\n> prior, which assigns probability to possible worlds on the basis of their\n> algorithmic complexity. See Li and Vitányi (2008).\n\n> [21](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_016.html#filepos487411). The\n> moment _after_ the conception of deception, the AI might contrive to erase\n> the trace of its mutinous thought. It is therefore important that this\n> tripwire operate continuously. It would also be good practice to use a\n> “flight recorder” that stores a complete trace of all the AI’s activity\n> (including exact timing of keyboard input from the programmers), so that its\n> trajectory can be retraced or analyzed following an automatic shutdown. The\n> information could be stored on a write-once-read-many medium.\n\n> [22](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_016.html#filepos491850). Asimov\n> (1942). To the three laws were later added a “Zeroth Law”: “(0) A robot may\n> not harm humanity, or, by inaction, allow humanity to come to harm” (Asimov\n> 1985).\n\n> [23](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_016.html#filepos492651). Cf.\n> Gunn (1982).\n\n> [24](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_016.html#filepos492933).\n> Russell (1986, 161f).\n\n> [25](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_016.html#filepos494847).\n> Similarly, although some philosophers have spent entire careers trying to\n> carefully formulate deontological systems, new cases and consequences\n> occasionally come to light that necessitate revisions. For example,\n> deontological moral philosophy has in recent years been reinvigorated\n> through the discovery of a fertile new class of philosophical thought\n> experiments, “trolley problems,” which reveal many subtle interactions among\n> our intuitions about the moral significance of the acts/omissions\n> distinction, the distinction between intended and unintended consequences,\n> and other such matters; see, e.g., Kamm (2007).\n\n> [26](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_016.html#filepos499763).\n> Armstrong (2010).\n\n> [27](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_016.html#filepos500976). As a\n> rule of thumb, if one plans to use multiple safety mechanisms to contain an\n> AI, it may be wise to work on each one _as if_ it were intended to be the\n> sole safety mechanism and _as if_ it were therefore required to be\n> individually sufficient. If one puts a leaky bucket inside another leaky\n> bucket, the water still comes out.\n\n> [28](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_016.html#filepos501555). A\n> variation of the same idea is to build the AI so that it is continuously\n> motivated to act on its best guesses about what the implicitly defined\n> standard is. In this setup, the AI’s final goal is always to act on the\n> implicitly defined standard, and it pursues an investigation into what this\n> standard is only for instrumental reasons.\n\n##### CHAPTER 10: ORACLES, GENIES, SOVEREIGNS, TOOLS\n\n> [1](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_017.html#filepos510438). These\n> names are, of course, anthropomorphic and should not be taken seriously as\n> analogies. They are just meant as labels for some _prima facie_ different\n> concepts of possible system types that one might consider trying to build.\n\n> [2](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_017.html#filepos511223). In\n> response to a question about the outcome of the next election, one would not\n> wish to be served with a comprehensive list of the projected position and\n> momentum vectors of nearby particles.\n\n> [3](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_017.html#filepos513500). Indexed\n> to a particular instruction set on a particular machine.\n\n> [4](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_017.html#filepos514643). Kuhn\n> (1962); de Blanc (2011).\n\n> [5](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_017.html#filepos517281). It\n> would be harder to apply such a “consensus method” to genies or sovereigns,\n> because there may often be numerous sequences of basic actions (such as\n> sending particular patterns of electrical signals to the system’s actuators)\n> that would be almost exactly equally effective at achieving a given\n> objective; whence slightly different agents may legitimately choose slightly\n> different actions, resulting in a failure to reach consensus. By contrast,\n> with appropriately formulated questions, there would usually be a small\n> number of suitable answer options (such as “yes” and “no”). (On the concept\n> of a Schelling point, also referred to as a “focal point,” see Schelling\n> [1980].)\n\n> [6](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_017.html#filepos521338). Is not\n> the world economy in some respects analogous to a weak genie, albeit one\n> that charges for its services? A vastly bigger economy, such as might\n> develop in the future, might then approximate a genie with collective\n> superintelligence.\n\n> One important respect in which the current economy is _unlike_ a genie is\n> that although I can (for a fee) command the economy to deliver a pizza to my\n> door, I cannot command it to deliver peace. The reason is not that the\n> economy is insufficiently powerful, but that it is insufficiently\n> coordinated. In this respect, the economy resembles an _assembly_ of genies\n> serving different masters (with competing agendas) more than it resembles a\n> single genie or any other type of unified agent. Increasing the total power\n> of the economy by making each constituent genie more powerful, or by adding\n> more genies, would not necessarily render the economy more capable of\n> delivering peace. In order to function like a superintelligent genie, the\n> economy would not only need to grow in its ability to inexpensively produce\n> goods and services (including ones that require radically new technology),\n> it would also need to become better able to solve global coordination\n> problems.\n\n> [7](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_017.html#filepos525327). If the\n> genie were somehow incapable of not obeying a subsequent command—and somehow\n> incapable of reprogramming itself to get rid of this susceptibility—then it\n> could act to prevent any new command from being issued.\n\n> [8](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_017.html#filepos526703). Even an\n> oracle that is limited to giving yes/no answers could be used to facilitate\n> the search for a genie or sovereign AI, or indeed could be used directly as\n> a component in such an AI. The oracle could also be used to produce the\n> actual code for such an AI if a sufficiently large number of questions can\n> be asked. A series of such questions might take roughly the following form:\n> “In the binary version of the code of the first AI that you thought of that\n> would constitute a genie, is the _n_ th symbol a zero?”\n\n> [9](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_017.html#filepos528127). One\n> could imagine a slightly more complicated oracle or genie that accepts\n> questions or commands only if they are issued by a designated authority,\n> though this would still leave open the possibility of that authority\n> becoming corrupted or being blackmailed by a third party.\n\n> [10](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_017.html#filepos528863). John\n> Rawls, a leading political philosopher of the twentieth century, famously\n> employed the expository device of a veil of ignorance as a way of\n> characterizing the kinds of preference that should be taken into account in\n> the formulation of a social contract. Rawls suggested that we should imagine\n> we were choosing a social contract from behind a veil of ignorance that\n> prevents us from knowing which person we will be and which social role we\n> will occupy, the idea being that in such a situation we would have to think\n> about which society would be generally fairest and most desirable without\n> regard to our egoistic interests and self-serving biases that might\n> otherwise make us prefer a social order in which we ourselves enjoy unjust\n> privileges. See Rawls (1971).\n\n> [11](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_017.html#filepos530620).\n> Karnofsky (2012).\n\n> [12](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_017.html#filepos532322). A\n> possible exception would be software hooked up to sufficiently powerful\n> actuators, such as software in early warning systems if connected directly\n> to nuclear warheads or to human officers authorized to launch a nuclear\n> strike. Malfunctions in such software can result in high-risk situations.\n> This has happened at least twice within living memory. On November 9, 1979,\n> a computer problem led NORAD (North American Aerospace Defense Command) to\n> make a false report of an incoming full-scale Soviet attack on the United\n> States. The USA made emergency retaliation preparations before data from\n> early-warning radar systems showed that no attack had been launched (McLean\n> and Stewart 1979). On September 26, 1983, the malfunctioning Soviet Oko\n> nuclear early-warning system reported an incoming US missile strike. The\n> report was correctly identified as a false alarm by the duty officer at the\n> command center, Stanislav Petrov: a decision that has been credited with\n> preventing thermonuclear war (Lebedev 2004). It appears that a war would\n> probably have fallen short of causing human extinction, even if it had been\n> fought with the combined arsenals held by all the nuclear powers at the\n> height of the Cold War, though it would have ruined civilization and caused\n> unimaginable death and suffering (Gaddis 1982; Parrington 1997). But bigger\n> stockpiles might be accumulated in future arms races, or even deadlier\n> weapons might be invented, or our models of the impacts of a nuclear\n> Armageddon (particularly of the severity of the consequent nuclear winter)\n> might be wrong.\n\n> [13](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_017.html#filepos535178). This\n> approach could fit the category of a direct-specification rule-based control\n> method.\n\n> [14](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_017.html#filepos538738). The\n> situation is essentially the same if the solution criterion specifies a\n> goodness _measure_ rather than a sharp cutoff for what counts as a solution.\n\n> [15](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_017.html#filepos539213). An\n> advocate for the oracle approach could insist that there is at least a\n> possibility that the user would spot the flaw in the proffered\n> solution—recognize that it fails to match the user’s intent even while\n> satisfying the formally specified success criteria. The likelihood of\n> catching the error at this stage would depend on various factors, including\n> how humanly understandable the oracle’s outputs are and how charitable it is\n> in selecting which features of the potential outcome to bring to the user’s\n> attention.\n\n> Alternatively, instead of relying on the oracle itself to provide these\n> functionalities, one might try to build a separate tool to do this, a tool\n> that could inspect the pronouncements of the oracle and show us in a helpful\n> way what would happen if we acted upon them. But to do to this in full\n> generality would require another superintelligent oracle whose divinations\n> we would then have to trust; so the reliability problem would not have been\n> solved, only displaced. One might seek to gain an increment of safety\n> through the use of multiple oracles to perform peer review, but this does\n> not protect in cases where all the oracles fail in the same way—as may\n> happen if, for instance, they have all been given the same formal\n> specification of what counts as a satisfactory solution.\n\n> [16](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_017.html#filepos544353). Bird\n> and Layzell (2002) and Thompson (1997); also Yaeger (1994, 13–14).\n\n> [17](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_017.html#filepos545859).\n> Williams (1966).\n\n> [18](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_017.html#filepos546401). Leigh\n> (2010).\n\n> [19](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_017.html#filepos546688). This\n> example is borrowed from Yudkowsky (2011).\n\n> [20](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_017.html#filepos547022). Wade\n> (1976). Computer experiments have also been conducted with simulated\n> evolution designed to resemble aspects of biological evolution—again with\n> sometimes strange results (see, e.g., Yaeger [1994]).\n\n> [21](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_017.html#filepos542047). With\n> sufficiently great—finite but physically implausible—amounts of computing\n> power, it _would_ probably be possible to achieve general superintelligence\n> with currently available algorithms. (Cf., e.g., the AIXI _tl_ system;\n> Hutter [2001].) But even the continuation of Moore’s law for another hundred\n> years would not suffice to attain the required levels of computing power to\n> achieve this.\n\n##### CHAPTER 11: MULTIPOLAR SCENARIOS\n\n> [1](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_018.html#filepos557988). Not\n> because this is necessarily the most likely or the most desirable type of\n> scenario, but because it is the one easiest to analyze with the toolkit of\n> standard economic theory, and thus a convenient starting point for our\n> discussion.\n\n> [2](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_018.html#filepos563744).\n> American Horse Council (2005). See also Salem and Rowan (2001).\n\n> [3](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_018.html#filepos564222).\n> Acemoglu (2003); Mankiw (2009); Zuleta (2008).\n\n> [4](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_018.html#filepos566293).\n> Fredriksen (2012, 8); Salverda et al. (2009, 133).\n\n> [5](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_018.html#filepos566503). It is\n> also essential for at least some of the capital to be invested in assets\n> that rise with the general tide. A diversified asset portfolio, such as\n> shares in an index tracker fund, would increase the chances of not entirely\n> missing out.\n\n> [6](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_018.html#filepos566860). Many of\n> the European welfare systems are _unfunded_ , meaning that pensions are paid\n> from ongoing current workers’ contributions and taxes rather than from a\n> pool of savings. Such schemes would not automatically meet the\n> requirement—in case of sudden massive unemployment, the revenues from which\n> the benefits are paid could dry up. However, governments may choose to make\n> up the shortfall from other sources.\n\n> [7](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_018.html#filepos568108).\n> American Horse Council (2005).\n\n> [8](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_018.html#filepos569180).\n> Providing 7 billion people an annual pension of $90,000 would cost $630\n> trillion a year, which is ten times the current world GDP. Over the last\n> hundred years, world GDP has increased about nineteenfold from around $2\n> trillion in 1900 to 37 trillion in 2000 (in 1990 int. dollars) according to\n> Maddison (2007). So if the growth rates we have seen over the past hundred\n> years continued for the next two hundred years, while population remained\n> constant, then providing everybody with an annual $90,000 pension would cost\n> about 3% of world GDP. An intelligence explosion might make this amount of\n> growth happen in a much shorter time span. See also Hanson (1998a, 1998b,\n> 2008).\n\n> [9](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_018.html#filepos569784). And\n> perhaps as much as a millionfold over the past 70,000 years if there was a\n> severe population bottleneck around that time, as has been speculated. See\n> Kremer (1993) and Huff et al. (2010) for more data.\n\n> [10](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_018.html#filepos570222).\n> Cochran and Harpending (2009). See also Clark (2007) and, for a critique,\n> Allen (2008).\n\n> [11](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_018.html#filepos571683). Kremer\n> (1993).\n\n> [12](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_018.html#filepos572653). Basten\n> et al. (2013). Scenarios in which there is a continued rise are also\n> possible. In general, the uncertainty of such projections increases greatly\n> beyond one or two generations into the future.\n\n> [13](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_018.html#filepos572820). Taken\n> globally, the total fertility rate at replacement was 2.33 children per\n> woman in 2003. This number comes from the fact that it takes two children\n> per woman to replace the parents, plus a “third of a child” to make up for\n> (1) the higher probability of boys being born, and (2) early mortality prior\n> to the end of their fertile life. For developed nations, the number is\n> smaller, around 2.1, because of lower mortality rates. (See Espenshade et\n> al. [2003, Introduction, Table 1, 580].) The population in most developed\n> countries would decline if it were not for immigration. A few notable\n> examples of countries with sub-replacement fertility rates are: Singapore at\n> 0.79 (lowest in the world), Japan at 1.39, People’s Republic of China at\n> 1.55, European Union at 1.58, Russia at 1.61, Brazil at 1.81, Iran at 1.86,\n> Vietnam at 1.87, and the United Kingdom at 1.90. Even the U.S. population\n> would probably decrease slightly with a fertility rate of 2.05. (See CIA\n> [2013].)\n\n> [14](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_018.html#filepos580060). The\n> fullness of time might occur many billions of years from now.\n\n> [15](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_018.html#filepos580700). Carl\n> Shulman points out that if biological humans count on living out their\n> natural lifespans alongside the digital economy, they need to assume not\n> only that the political order in the digital sphere would be protective of\n> human interests but that it would remain so over very long periods of time\n> (Shulman 2012). For example, if events in the digital sphere unfolds a\n> thousand times faster than on the outside, then a biological human would\n> have to rely on the digital body politic holding steady for 50,000 years of\n> internal change and churn. Yet if the digital political world were anything\n> like ours, there would be a great many revolutions, wars, and catastrophic\n> upheavals during those millennia that would probably inconvenience\n> biological humans on the outside. Even a 0.01% risk per year of a global\n> thermonuclear war or similar cataclysm would entail a near certain loss for\n> the biological humans living out their lives in slowmo sidereal time. To\n> overcome this problem, a more stable order in the digital realm would be\n> required: perhaps a singleton that gradually improves its own stability.\n\n> [16](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_018.html#filepos581239). One\n> might think that even if machines were far more efficient than humans, there\n> would still be _some_ wage level at which it would be profitable to employ a\n> human worker; say at 1 cent an hour. If this were the only source of income\n> for humans, our species would go extinct since human beings cannot survive\n> on 1 cent an hour. But humans also get income from capital. Now, if we are\n> assuming that population grows until total income is at subsistence level,\n> one might think this would be a state in which humans would be working hard.\n> For example, suppose subsistence level income is $1/day. Then, it might\n> seem, population would grow until per person capital provided only a 90\n> cents per day income, which people would have to supplement with ten hours\n> of hard labor to make up the remaining 10 cents. However, this need not be\n> so, because the subsistence level income depends on the amount of work that\n> is done: harder-working humans burn more calories. Suppose that each hour of\n> work increases food costs by 2 cents. We then have a model in which humans\n> are idle in equilibrium.\n\n> [17](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_018.html#filepos582232). It\n> might be thought that a caucus as enfeebled as this would be unable to vote\n> and to otherwise defend its entitlements. But the pod-dwellers could give\n> power of attorney to AI fiduciaries to manage their affairs and represent\n> their political interests. (This part of the discussion in this section is\n> premised on the assumption that property rights are respected.)\n\n> [18](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_018.html#filepos587967). It is\n> unclear what is the best term. “Kill” may suggest more active brutality than\n> is warranted. “End” may be too euphemistic. One complication is that there\n> are two potentially separate events: ceasing to actively run a process, and\n> erasing the information template. A human death normally involves both\n> events, but for an emulation they can come apart. That a program\n> _temporarily_ ceases to run may be no more consequential than that a human\n> sleeps: but to _permanently_ cease running may be the equivalent of entering\n> a permanent coma. Still further complications arise from the fact that\n> emulations can be copied and that they can run at different speeds:\n> possibilities with no direct analogs in human experience. (Cf. Bostrom\n> [2006b]; Bostrom and Yudkowsky [forthcoming].)\n\n> [19](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_018.html#filepos590708). There\n> will be a trade-off between total parallel computing power and computational\n> speed, as the highest computational speeds will be attainable only at the\n> expense of a reduction in power efficiency. This will be especially true\n> after one enters the era of reversible computing.\n\n> [20](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_018.html#filepos591553). An\n> emulation could be tested by leading it into temptation. By repeatedly\n> testing how an emulation started from a certain prepared state reacts to\n> various sequences of stimuli, one could obtain high confidence in the\n> reliability of that emulation. But the further the mental state is\n> subsequently allowed to develop away from its validated starting point, the\n> less certain one could be that it would remain reliable. (In particular,\n> since a clever emulation might surmise it is sometimes in a simulation, one\n> would need to be cautious about extrapolating its behavior into situations\n> where its simulation hypothesis would weigh less heavily in its decision-\n> making.)\n\n> [21](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_018.html#filepos592398). Some\n> emulations might identify with their clan—i.e. all of their copies and\n> variations derived from the same template—rather with any one particular\n> instantiation. Such an emulation might not regard its own termination as a\n> death event, if it knew that other clan members would survive. Emulations\n> may know that they will get reverted to a particular stored state at the end\n> of the day and lose that day’s memories, but be as little put off by this as\n> the partygoer who knows she will awake the next morning without any\n> recollection of the previous night: regarding this as retrograde amnesia,\n> not death.\n\n> [22](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_018.html#filepos592790). An\n> ethical evaluation might take into account many other factors as well. Even\n> if all the workers were constantly well pleased with their condition, the\n> outcome might still be deeply morally objectionable on other grounds—though\n> _which_ other grounds is a matter of dispute between rival moral theories.\n> But any plausible assessment would consider subjective well-being to be one\n> important factor. See also Bostrom and Yudkowsky (forthcoming).\n\n> [23](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_018.html#filepos593606). World\n> Values Survey (2008).\n\n> [24](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_018.html#filepos594009).\n> Helliwell and Sachs (2012).\n\n> [25](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_018.html#filepos604735). Cf.\n> Bostrom (2004). See also Chislenko (1996) and Moravec (1988).\n\n> [26](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_018.html#filepos606354). It is\n> hard to say whether the information-processing structures that would emerge\n> in this kind of scenario would be conscious (in the sense of having qualia,\n> phenomenal experience). The reason this is hard is partly our empirical\n> ignorance about which cognitive entities would arise and partly our\n> philosophical ignorance about which types of structure have consciousness.\n> One could try to reframe the question, and instead of asking whether the\n> future entities would be conscious, one could ask whether the future\n> entities would have moral status; or one could ask whether they would be\n> such that we have preferences about their “well-being.” But these questions\n> may be no easier to answer than the question about consciousness—in fact,\n> they might require an answer to the consciousness question inasmuch as moral\n> status or our preferences depend on whether the entity in question can\n> subjectively experience its condition.\n\n> [27](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_018.html#filepos608781). For an\n> argument that both geological and human history manifest such a trend toward\n> greater complexity, see Wright (2001). For an opposing argument (criticized\n> in [Chapter\n> 9](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_016.html#filepos449483) of\n> Wright’s book), see Gould (1990). See also Pinker (2011) for an argument\n> that we are witnessing a robust long-term trend toward decreasing violence\n> and brutality.\n\n> [28](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_018.html#filepos609715). For\n> more on observation selection theory, see Bostrom (2002a).\n\n> [29](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_018.html#filepos610691).\n> Bostrom (2008a). A much more careful examination of the details of our\n> evolutionary history would be needed to circumvent the selection effect.\n> See, e.g., Carter (1983, 1993); Hanson (1998d); Ćirković et al. (2010).\n\n> [30](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_018.html#filepos613283). Kansa\n> (2003).\n\n> [31](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_018.html#filepos614603). E.g.,\n> Zahavi and Zahavi (1997).\n\n> [32](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_018.html#filepos615543). See\n> Miller (2000).\n\n> [33](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_018.html#filepos617083). Kansa\n> (2003). For a provocative take, see also Frank (1999).\n\n> [34](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_018.html#filepos618423). It is\n> not obvious how best to measure the degree of global political integration.\n> One perspective would be that whereas a hunter–gatherer tribe might have\n> integrated a hundred individuals into a decision-making entity, the largest\n> political entities today contain more than a billion individuals. This would\n> amount to a difference of seven orders of magnitude, with only one\n> additional magnitude to go before the entire world population is contained\n> within a single political entity. However, at the time when the tribe was\n> the largest scale of integration, the world population was much smaller. The\n> tribe might have contained as much as a thousandth of the individuals then\n> living. This would make the increase in the scale of political integration\n> as little as two orders of magnitude. Looking at the fraction of world\n> population that is politically integrated, rather than at absolute numbers,\n> seems appropriate in the present context (particularly as the transition to\n> machine intelligence may cause a population explosion, of emulations or\n> other digital minds). But there have also been developments in global\n> institutions and networks of collaboration outside of formal state\n> structures, which should also be taken into account.\n\n> [35](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_018.html#filepos619403). One of\n> the reasons for supposing that the first machine intelligence revolution\n> will be swift—the possible existence of a hardware overhang—does not apply\n> here. However, there could be other sources of rapid gain, such as a\n> dramatic breakthrough in software associated with transitioning from\n> emulation to purely synthetic machine intelligence.\n\n> [36](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_018.html#filepos624272).\n> Shulman (2010b).\n\n> [37](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_018.html#filepos626776). How\n> the _pro et contra_ would balance out might depend on what kind of work the\n> superorganism is trying to do, and how generally capable the most generally\n> capable available emulation template is. Part of the reason many different\n> types of human beings are needed in large organizations today is that humans\n> who are very talented in many domains are rare.\n\n> [38](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_018.html#filepos628100). It is\n> of course very easy to make multiple copies of a software agent. But note\n> that copying is not in general sufficient to ensure that the copies have the\n> same final goals. In order for two agents to have the same final goals (in\n> the relevant sense of “same”), the goals must coincide in their _indexical_\n> elements. If Bob is selfish, a copy of Bob will likewise be selfish. Yet\n> their goals do not coincide: Bob cares about Bob whereas Bob-copy cares\n> about Bob-copy.\n\n> [39](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_018.html#filepos630059).\n> Shulman (2010b, 6).\n\n> [40](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_018.html#filepos634112). This\n> might be more feasible for biological humans and whole brain emulations than\n> for arbitrary artificial intelligences, which might be constructed so as to\n> have hidden compartments or functional dynamics that may be very hard to\n> discover. On the other hand, _AIs specifically built to be transparent_\n> should allow for more thoroughgoing inspection and verification than is\n> possible with brain-like architectures. Social pressures may encourage AIs\n> to expose their source code, and to modify themselves to render themselves\n> transparent—especially if being transparent is a precondition to being\n> trusted and thus to being given the opportunity to partake in beneficial\n> transactions. Cf. Hall (2007).\n\n> [41](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_018.html#filepos638270). Some\n> other issues that seem relatively minor, especially in cases where the\n> stakes are enormous (as they are for the key global coordination failures),\n> include the search cost of finding policies that could be of mutual\n> interest, and the possibility that some agents might have a basic preference\n> for “autonomy” in a form that would be reduced by entering into\n> comprehensive global treaties that have monitoring and enforcement\n> mechanisms attached.\n\n> [42](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_018.html#filepos639998). An AI\n> might perhaps achieve this by modifying itself appropriately and then giving\n> observers read-only access to its source code. A machine intelligence with a\n> more opaque architecture (such as an emulation) might perhaps achieve it by\n> publicly applying to itself some motivation selection method. Alternatively,\n> an external coercive agency, such as a superorganism police force, might\n> perhaps be used not only to enforce the implementation of a treaty reached\n> between different parties, but also internally by a single party to commit\n> itself to a particular course of action.\n\n> [43](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_018.html#filepos642017).\n> Evolutionary selection might have favored threat-ignorers and even\n> characters visibly so highly strung that they would rather fight to the\n> death than suffer the slightest discomfiture. Such a disposition might bring\n> its bearer valuable signaling benefits. (Any such instrumental rewards of\n> having the disposition need of course play no part in the agent’s conscious\n> motivation: he may value justice or honor as ends in themselves.)\n\n> [44](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_018.html#filepos643309). A\n> definitive verdict on these matters, however, must await further analysis.\n> There are various other potential complications which we cannot explore\n> here.\n\n##### CHAPTER 12: ACQUIRING VALUES\n\n> [1](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos648191). Various\n> complications and modulations of this basic idea could be introduced. We\n> discussed one variation in [Chapter\n> 8](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_015.html#filepos408438)—that of a\n> satisficing, as opposed to maximizing, agent—and in the next chapter we\n> briefly touch on the issue of alternative decision theories. However, such\n> issues are not essential to the thrust of this subsection, so we will keep\n> things simple by focusing here on the case of an expected utility-maximizing\n> agent.\n\n> [2](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos648386).\n> Assuming the AI is to have a non-trivial utility function. It would be very\n> easy to build an agent that always chooses an action that maximizes expected\n> utility if its utility function is, e.g., the constant function _U_(_w_) =\n> 0. Every action would equally well maximize expected utility relative to\n> that utility function.\n\n> [3](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos650477). Also\n> because we have forgotten the blooming buzzing confusion of our early\n> infancy, a time when we could not yet see very well because our brain had\n> not yet learned to interpret its visual input.\n\n> [4](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos651855). See\n> also Yudkowsky (2011) and the review in section 5 of Muehlhauser and Helm\n> (2012).\n\n> [5](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos652964). It is\n> perhaps just about conceivable that advances in software engineering could\n> eventually overcome these difficulties. Using modern tools, a single\n> programmer can produce software that would have been beyond the pale of a\n> sizeable team of developers forced to write directly in machine code.\n> Today’s AI programmers gain expressiveness from the wide availability of\n> high-quality machine learning and scientific calculation libraries, enabling\n> someone to hack up, for instance, a unique-face-counting webcam application\n> by chaining libraries together that they never could have written on their\n> own. The accumulation of reusable software, produced by specialists but\n> useable by non-specialists, will give future programmers an expressiveness\n> advantage. For example, a future robotics programmer might have ready access\n> to standard facial imprinting libraries, typical-office-building-object\n> collections, specialized trajectory libraries, and many other\n> functionalities that are currently unavailable.\n\n> [6](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos656298). Dawkins\n> (1995, 132). The claim here is not necessarily that the amount of suffering\n> in the natural world _outweighs_ the amount of positive well-being.\n\n> [7](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos656563).\n> Required population sizes might be much larger or much smaller than those\n> that existed in our own ancestry. See Shulman and Bostrom (2012).\n\n> [8](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos657112). If it\n> were easy to get an equivalent result without harming large numbers of\n> innocents, it would seem morally better to do so. If, nevertheless, digital\n> persons are created and made to suffer unjust harm, it may be possible to\n> compensate them for their suffering by saving them to file and later (when\n> humanity’s future is secured) rerunning them under more favorable\n> conditions. Such restitution could be compared in some ways to religious\n> conceptions of an afterlife in the context of theological attempts to\n> address the evidential problem of evil.\n\n> [9](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos658942). One of\n> the field’s leading figures, Richard Sutton, defines reinforcement learning\n> not in terms of a learning method but in terms of a learning problem: any\n> method that is well suited to solving that problem is considered a\n> reinforcement learning method (Sutton and Barto 1998, 4). The present\n> discussion, in contrast, pertains to methods where the agent can be\n> conceived of as having the final goal of maximizing (some notion of)\n> cumulative reward. Since an agent with some very different kind of final\n> goal might be skilled at mimicking a reward-seeking agent in a wide range of\n> situations, and could thus be well suited to solving reinforcement learning\n> problems, there could be methods that would count as “reinforcement learning\n> methods” on Sutton’s definition that would not result in a wireheading\n> syndrome. The remarks in the text, however, apply to most of the methods\n> actually used in the reinforcement learning community.\n\n> [10](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos663331). Even\n> if, somehow, a human-like mechanism could be set up within a human-like\n> machine intellect, the final goals acquired by this intellect need not\n> resemble those of a well-adjusted human, unless the rearing environment for\n> this digital baby also closely matched that of an ordinary child: something\n> that would be difficult to arrange. And even with a human-like rearing\n> environment, a satisfactory result would not be guaranteed, since even a\n> subtle difference in innate dispositions can result in very different\n> reactions to a life event. It may, however, be possible to create a more\n> reliable value-accretion mechanism for human-like minds in the future\n> (perhaps using novel drugs or brain implants, or their digital equivalents).\n\n> [11](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos665352). One\n> might wonder why it appears _we humans_ are not trying to disable the\n> mechanism that leads us to acquire new final values. Several factors might\n> be at play. First, the human motivation system is poorly described as a\n> coldly calculating utility-maximizing algorithm. Second, we might not have\n> any convenient means of altering the ways we acquire values. Third, we may\n> have instrumental reasons (arising, e.g., from social signaling needs) for\n> sometimes acquiring new final values—instrumental values might not be as\n> useful if our minds are partially transparent to other people, or if the\n> cognitive complexity of pretending to have a different set of final values\n> than we actually do is too taxing. Fourth, there are cases where we _do_\n> actively resist tendencies that produce changes in our final values, for\n> instance when we seek to resist the corrupting influence of bad company.\n> Fifth, there is the interesting possibility that we place some final value\n> on being the kind of agent that can acquire new final values in normal human\n> ways.\n\n> [12](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos667776). Or one\n> might try to design the motivation system so that the AI is indifferent to\n> such replacement; see Armstrong (2010).\n\n> [13](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos677820). We\n> will here draw on some elucidations made by Daniel Dewey (2011). Other\n> background ideas contributing to this framework have been developed by\n> Marcus Hutter (2005) and Shane Legg (2008), Eliezer Yudkowsky (2001), Nick\n> Hay (2005), Moshe Looks, and Peter de Blanc.\n\n> [14](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos678593). To\n> avoid unnecessary complications, we confine our attention to deterministic\n> agents that do not discount future rewards.\n\n> [15](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos680834).\n> Mathematically, an agent’s behavior can be formalized as an _agent function_\n> , which maps each possible interaction history to an action. Except for the\n> very simplest agents, it is infeasible to represent the agent function\n> explicitly as a lookup table. Instead, the agent is given some way of\n> computing which action to perform. Since there are many ways of computing\n> the same agent function, this leads to a finer individuation of an agent as\n> an _agent program_. An agent program is a specific program or algorithm that\n> computes an action for any given interaction history. While it is often\n> mathematically convenient and useful to think of an agent program that\n> interacts with some formally specified environment, it is important to\n> remember that this is an idealization. Real agents are physically\n> instantiated. This means not only that the agent interacts with the\n> environment via its sensors and effectors, but also that the agent’s “brain”\n> or controller _is itself part of physical reality_. Its operations can\n> therefore in principle be affected by external physical interferences (and\n> not only by receiving percepts from its sensors). At some point, therefore,\n> it becomes necessary to view an agent as an _agent implementation_. An agent\n> implementation is a physical structure that, in the absence of interference\n> from its environment, implements an agent function. (This definition follows\n> Dewey [2011].)\n\n> [16](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos683111). Dewey\n> proposes the following optimality notion for a value learning agent:\n\n![Image](images/00044.jpg)\n\n> Here, _P_ 1 and _P_ 2 are two probability functions. The second summation\n> ranges over some suitable class of utility functions over possible\n> interaction histories. In the version presented in the text, we have made\n> explicit some dependencies as well as availed ourselves of the simplifying\n> possible worlds notation.\n\n> [17](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos683551). It\n> should be noted that the set of utility functions ![Image](images/00038.jpg)\n> should be such that utilities can be compared and averaged. In general, this\n> is problematic, and it is not always obvious how to represent different\n> moral theories of the good in terms of cardinal utility functions. See,\n> e.g., MacAskill 2010).\n\n> [18](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos684779). Or more generally, since ν might not be such as to directly imply for any given pair of a possible world and a utility function (_w, U_) whether the proposition ν(_U_) is true in _w_ , what needs to be done is to give the AI an adequate representation of the conditional probability distribution _P_(ν(_U_) | _w_).\n\n> [19](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos685664).\n> Consider first ![Image](images/00036.jpg), the class of actions that are\n> possible for an agent. One issue here is what exactly should count as an\n> action: only basic motor commands (e.g. “send an electric pulse along output\n> channel #00101100”), or higher-level actions (e.g. “keep camera centered on\n> face”)? Since we are trying to develop an optimality notion rather than a\n> practical implementation plan, we may take the domain to be basic motor\n> commands (and since the set of possible motor commands might change over\n> time, we may need to index ![Image](images/00036.jpg) to time). However, in\n> order to move toward implementation it will presumably be necessary to\n> introduce some kind of hierarchical planning process, and one may then need\n> to consider how to apply the formula to some class of higher-level actions.\n> Another issue is how to analyze internal actions (such as writing strings to\n> working memory). Since internal actions can have important consequences, one\n> would ideally want ![Image](images/00036.jpg) to include such basic internal\n> actions as well as motor commands. But there are limits to how far one can\n> go in this direction: the computation of the expected utility of any action\n> in ![Image](images/00036.jpg) requires multiple computational operations,\n> and if each such operation were also regarded as an action in\n> ![Image](images/00036.jpg) that needed to be evaluated according to AI-VL,\n> we would face an infinite regress that would make it impossible to ever get\n> started. To avoid the infinite regress, one must restrict any explicit\n> attempt to estimate the expected utility to a limited number of significant\n> action possibilities. The system will then need some heuristic process that\n> identifies some significant action possibilities for further consideration.\n> (Eventually the system might also get around to making explicit decisions\n> regarding some possible actions to make modifications to this heuristic\n> process, actions that might have been flagged for explicit attention by this\n> self-same process; so that in the long run the system might become\n> increasingly effective at approximating the ideal identified by AI-VL.)\n\n> Consider next ![Image](images/00037.jpg), which is a class of possible\n> worlds. One difficulty here is to specify ![Image](images/00037.jpg) so that\n> it is sufficiently inclusive. Failure to include some relevant _w_ in\n> ![Image](images/00037.jpg) could render the AI incapable of representing a\n> situation that actually occurs, resulting in the AI making bad decisions.\n> Suppose, for example, that we use some ontological theory to determine the\n> makeup of ![Image](images/00037.jpg). For instance, we include in\n> ![Image](images/00037.jpg) all possible worlds that consist of a certain\n> kind of spacetime manifold populated by elementary particles found in the\n> standard model in particle physics. This could distort the AI’s epistemology\n> if the standard model is incomplete or incorrect. One could try to use a\n> bigger ![Image](images/00037.jpg)-class to cover more possibilities; but\n> even if one could ensure that every possible physical universe is included\n> one might still worry that some other possibility would be left out. For\n> example, what about the possibility of dualistic possible worlds in which\n> facts about consciousness do not supervene on facts about physics? What\n> about indexical facts? Normative facts? Facts of higher mathematics? What\n> about other kinds of fact that we fallible humans might have overlooked but\n> that could turn out to be important to making things go as well as possible?\n> Some people have strong convictions that some particular ontological theory\n> is correct. (Among people writing on the future of AI, a belief in a\n> materialistic ontology, in which the mental supervenes on the physical, is\n> often taken for granted.) Yet a moment’s reflection on the history of ideas\n> should help us realize that there is a significant possibility that our\n> favorite ontology is wrong. Had nineteenth-century scientists attempted a\n> physics-inspired definition of ![Image](images/00037.jpg), they would\n> probably have neglected to include the possibility of a non-Euclidian\n> spacetime or an Everettian (“many-worlds”) quantum theory or a cosmological\n> multiverse or the simulation hypothesis—possibilities that now appear to\n> have a substantial probability of obtaining in the actual world. It is\n> plausible that there are other possibilities to which we in the present\n> generation are similarly oblivious. (On the other hand, if\n> ![Image](images/00037.jpg) is too big, there arise technical difficulties\n> related to having to assign measures over transfinite sets.) The ideal might\n> be if we could somehow arrange things such that the AI could use some kind\n> of open-ended ontology, one that the AI itself could subsequently extend\n> using the same principles that we would use when deciding whether to\n> recognize a new type of metaphysical possibility.\n\n> Consider _P_(_w_ |_Ey_). Specifying this conditional probability is not strictly part of the value-loading problem. In order to be intelligent, the AI must already have some way of deriving reasonably accurate probabilities over many relevant factual possibilities. A system that falls too far short on this score will not pose the kind of danger that concerns us here. However, there may be a risk that the AI will end up with an epistemology that is good enough to make the AI instrumentally effective yet not good enough to enable it to think correctly about some possibilities that are of great normative importance. (The problem of specifying _P_(_w_ |_Ey_) is in this way related to the problem of specifying ![Image](images/00037.jpg).) Specifying _P_(_w_ | _Ey_) also requires confronting other issues, such as how to represent uncertainty over logical impossibilities.\n\n> The aforementioned issues—how to define a class of possible actions, a class of possible worlds, and a likelihood distribution connecting evidence to classes of possible worlds—are quite generic: similar issues arise for a wide range of formally specified agents. It remains to examine a set of issues more peculiar to the value learning approach; namely, how to define ![Image](images/00038.jpg), _V_(_U_), and _P_(_V_(_U_) | _w_).\n\n> ![Image](images/00038.jpg) is a class of utility functions. There is a\n> connection between ![Image](images/00038.jpg) and ![Image](images/00037.jpg)\n> inasmuch as each utility function _U_ (_w_) in ![Image](images/00038.jpg)\n> should ideally assign utilities to each possible world _w_ in\n> ![Image](images/00037.jpg). But ![Image](images/00038.jpg) also needs to be\n> wide in the sense of containing sufficiently many and diverse utility\n> functions for us to have justified confidence that at least one of them does\n> a good job of representing the intended values.\n\n> The reason for writing _P_(_V_(_U_) | _w_) rather than simply _P_(_U_ |_w_) is to emphasize the fact that probabilities are assigned to propositions. A utility function, per se, is not a proposition, but we can transform a utility function into a proposition by making some claim about it. For example, we may claim of a particular utility function _U_(.) that it describes the preferences of a particular person, or that it represents the prescriptions implied by some ethical theory, or that it is the utility function that the principal would have wished to have implemented if she had thought things through. The “value criterion” _V_(.) can thus be construed as a function that takes as its argument a utility function _U_ and gives as its value a proposition to the effect that _U_ satisfies the criterion _V_. Once we have defined a proposition _V_(_U_), we can hopefully obtain the conditional probability _P_(_V_(_U_)|_w_) from whatever source we used to obtain the other probability distributions in the AI. (If we are certain that all normatively relevant facts are taken into account in individuating the possible worlds ![Image](images/00037.jpg), then _P_(_V_(_U_)|_w_) should equal zero or one in each possible world.) The question remains how to define _V_. This is discussed further in the text.\n\n> [20](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos688316). These\n> are not the only challenges for the value learning approach. Another issue,\n> for instance, is how to get the AI to have sufficiently sensible initial\n> beliefs—at least by the time it becomes strong enough to subvert the\n> programmers’ attempts to correct it.\n\n> [21](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos690099).\n> Yudkowsky (2001).\n\n> [22](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos694010). The\n> term is taken from American football, where a “Hail Mary” is a very long\n> forward pass made in desperation, typically when the time is nearly up, on\n> the off chance that a friendly player might catch the ball near the end zone\n> and score a touchdown.\n\n> [23](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos695539). The\n> Hail Mary approach relies on the idea that a superintelligence could\n> articulate its preferences with greater exactitude than we humans can\n> articulate ours. For example, a superintelligence could specify its\n> preference as code. So if our AI is representing other superintelligences as\n> computational processes that are perceiving their environment, then our AI\n> should be able to reason about how those alien superintelligences would\n> respond to some hypothetical stimulus, such as a “window” popping up in\n> their visual field presenting them with the source code of our own AI and\n> asking them to specify their instructions to us in some convenient pre-\n> specified format. Our AI could then read off these imaginary instructions\n> (from its own model of this counterfactual scenario wherein these alien\n> superintelligences are represented), and we would have built our AI so that\n> it would be motivated to follow those instructions.\n\n> [24](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos696047). An\n> alternative would be to create a detector that looks (within our AI’s world\n> model) for (representations of) physical structures created by a\n> superintelligent civilization. We could then bypass the step of identifying\n> the hypothesized superintelligences’ preference functions, and give our own\n> AI the final value of trying to copy whatever physical structures it\n> believes superintelligent civilizations tend to produce.\n\n> There are technical challenges with this version, too, however. For\n> instance, since our own AI, even after it has attained superintelligence,\n> may not be able to know with great precision what physical structures other\n> superintelligences build, our AI may need to resort to trying to approximate\n> those structures. To do this, it would seem our AI would need a similarity\n> metric by which to judge how closely one physical artifact approximates\n> another. But similarity metrics based on crude physical measures may be\n> inadequate—it being no good, for example, to judge that a brain is more\n> similar to a Camembert cheese than to a computer running an emulation.\n\n> A more feasible approach might be to look for “beacons”: messages about\n> utility functions encoded in some suitable simple format. We would build our\n> AI to want to follow whatever such messages about utility functions it\n> hypothesizes might exist out there in the universe; and we would hope that\n> friendly extraterrestrial AIs would create a variety of beacons of the types\n> that they (with their superintelligence) reckon that simple civilizations\n> like ours are most likely to build our AI to look for.\n\n> [25](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos697543). If\n> _every_ civilization tried to solve the value-loading problem through a Hail\n> Mary, the pass would fail. Somebody has to do it the hard way.\n\n> [26](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos698143).\n> Christiano (2012).\n\n> [27](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos701829). The AI\n> we build need not be able to find the model either. Like us, it could reason\n> about what such a complex implicit definition would entail (perhaps by\n> looking at its environment and following much the same kind of reasoning\n> that we would follow).\n\n> [28](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos702559). Cf.\n> [Chapters 9](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_016.html#filepos449483)\n> and [11](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_018.html#filepos555025).\n\n> [29](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos703048). For\n> instance, MDMA may temporarily increase empathy; oxytocin may temporarily\n> increase trust (Vollenweider et al. 1998; Bartz et al. 2011). However, the\n> effects seem quite variable and context dependent.\n\n> [30](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos709477). The\n> enhanced agents might be killed off or placed in suspended animation\n> (paused), reset to an earlier state, or disempowered and prevented from\n> receiving any further enhancements, until the overall system has reached a\n> more mature and secure state where these earlier rogue elements no longer\n> pose a system-wide threat.\n\n> [31](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos713105). The\n> issue might also be less obvious in a future society of biological humans,\n> one that has access to advanced surveillance or biomedical techniques for\n> psychological manipulation, or that is wealthy enough to afford an extremely\n> high ratio of security professionals to invigilate the regular citizenry\n> (and each other).\n\n> [32](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos714905). Cf.\n> Armstrong (2007) and Shulman (2010b).\n\n> [33](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos715344). One\n> open question is to what degree a level _n_ supervisor would need to monitor\n> not only their level (_n – 1_) supervisees, but also _their_ level (_n – 2_)\n> supervisees, in order to know that the level (_n – 1_) agents are doing\n> their jobs properly. And to know that the level (_n – 1_) agents have\n> successfully managed the level (_n – 2_) agents, is it further necessary for\n> the level _n_ agent to also monitor the level (_n – 3_) agents?\n\n> [34](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos715549). This\n> approach straddles the line between motivation selection and capability\n> control. Technically, the part of the arrangement that consists of human\n> beings controlling a set of software supervisors counts as capability\n> control, whereas the part of the arrangement that consists of layers of\n> software agents within the system controlling other layers is motivation\n> selection (insofar as it is an arrangement that shapes the system’s\n> motivational tendencies).\n\n> [35](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos716867). In\n> fact, many other costs deserve consideration but cannot be given it here.\n> For example, whatever agents are charged with ruling over such a hierarchy\n> might become corrupted or debased by their power.\n\n> [36](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos717352). For\n> this guarantee to be effective, it must be implemented in good faith. This\n> would rule out certain kinds of manipulation of the emulation’s emotional\n> and decision-making faculties which might otherwise be used (for instance)\n> to install a fear of being halted or to prevent the emulation from\n> rationally considering its options.\n\n> [37](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos718585). See,\n> e.g., Brinton (1965); Goldstone (1980, 2001). (Social science progress on\n> these questions could make a nice gift to the world’s despots, who might use\n> more accurate predictive models of social unrest to optimize their\n> population control strategies and to gently nip insurgencies in the bud with\n> less-lethal force.)\n\n> [38](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos720757). Cf.\n> Bostrom (2011a, 2009b).\n\n> [39](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos721924). In the\n> case of an entirely artificial system, it might be possible to obtain some\n> of the advantages of an institutional structure without actually creating\n> distinct subagents. A system might incorporate multiple perspectives into\n> its decision process without endowing each of those perspectives with its\n> own panoply of cognitive faculties required for independent agency. It could\n> be tricky, however, to fully implement the “observe the behavioral\n> consequences of a proposed change, and revert back to an earlier version if\n> the consequences appear undesirable from the _ex ante_ standpoint” feature\n> described in the text in a system that is not composed of subagents.\n\n##### CHAPTER 13: CHOOSING THE CRITERIA FOR CHOOSING\n\n> [1](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_020.html#filepos730129). A\n> recent canvass of professional philosophers found the percentage of\n> respondents who “accept or leans toward” various positions. On normative\n> ethics, the results were _deontology_ 25.9%; _consequentialism_ 23.6%;\n> _virtue ethics_ 18.2%. On metaethics, results were _moral realism_ 56.4%;\n> _moral anti-realism_ 27.7%. On moral judgment: _cognitivism_ 65.7%; _non-\n> cognitivism_ 17.0% (Bourget and Chalmers 2009).\n\n> [2](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_020.html#filepos730524). Pinker\n> (2011).\n\n> [3](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_020.html#filepos731596). For a\n> discussion of this issue, see Shulman et al. (2009).\n\n> [4](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_020.html#filepos731837). Moore\n> (2011).\n\n> [5](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_020.html#filepos732433). Bostrom\n> (2006b).\n\n> [6](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_020.html#filepos732604). Bostrom\n> (2009b).\n\n> [7](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_020.html#filepos732711). Bostrom\n> (2011a).\n\n> [8](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_020.html#filepos734886). More\n> precisely, we should defer to its opinion except on those topics where we\n> have good reason to suppose that our beliefs are more accurate. For example,\n> we might know more about what we are thinking at a particular moment than\n> the superintelligence does if it is not able to scan our brains. However, we\n> could omit this qualification if we assume that the superintelligence has\n> access to our opinions; we could then also defer to the superintelligence\n> the task of judging when our opinions should be trusted. (There might remain\n> some special cases, involving indexical information, that need to be handled\n> separately—by, for example, having the superintelligence explain to us what\n> it would be rational to believe from our perspective.) For an entry into the\n> burgeoning philosophical literature on testimony and epistemic authority,\n> see, e.g., Elga (2007).\n\n> [9](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_020.html#filepos736618).\n> Yudkowsky (2004). See also Mijic (2010).\n\n> [10](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_020.html#filepos737603). For\n> example, David Lewis proposed a _dispositional theory of value_ , which\n> holds, roughly, that some thing _X_ is a value for _A_ if and only if _A_\n> would want to want _X_ if _A_ were perfectly rational and ideally acquainted\n> with _X_ (Smith et al. 1989). Kindred ideas had been put forward earlier;\n> see, e.g., Sen and Williams (1982), Railton (1986), and Sidgwick and Jones\n> (2010). Along somewhat similar lines, one common account of philosophical\n> justification, the _method of reflective equilibrium_ , proposes a process\n> of iterative mutual adjustment between our intuitions about particular\n> cases, the general rules which we think govern these cases, and the\n> principles according to which we think these elements should be revised, to\n> achieve a more coherent system; see, e.g., Rawls (1971) and Goodman (1954).\n\n> [11](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_020.html#filepos739683).\n> Presumably the intention here is that when the AI acts to prevent such\n> disasters, it should do it with _as light a touch as possible_ , i.e. in\n> such a manner that it averts the disaster but without exerting too much\n> influence over how things turn out for humanity in other respects.\n\n> [12](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_020.html#filepos740475).\n> Yudkowsky (2004).\n\n> [13](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_020.html#filepos743354).\n> Rebecca Roache, personal communication.\n\n> [14](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_020.html#filepos744743). The\n> three principles are “Defend humans, the future of humanity, and humane\n> nature” (_humane_ here being that which we wish we were, as distinct from\n> _human_ , which is what we are); “Humankind should not spend the rest of\n> eternity desperately wishing that the programmers had done something\n> differently”; and “Help people.”\n\n> [15](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_020.html#filepos749697). Some\n> religious groups place a strong emphasis on faith in contradistinction to\n> reason, the latter of which they may regard—even in its hypothetically most\n> idealized form and even after it would have ardently and open-mindedly\n> studied every scripture, revelation, and exegesis—to be insufficient for the\n> attainment of essential spiritual insights. Those holding such views might\n> not regard CEV as an optimal guide to decision-making (though they might\n> still prefer it to various other imperfect guides that might in actuality be\n> followed if the CEV approach were eschewed).\n\n> [16](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_020.html#filepos751792). An AI\n> acting like a latent force of nature to regulate human interactions has been\n> referred to as a “Sysop,” a kind of “operating system” for the matter\n> occupied by human civilization. See Yudkowsky (2001).\n\n> [17](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_020.html#filepos754849). “\n> _Might_ ,” because _conditional_ on humanity’s coherent extrapolated\n> volition wishing not to extend moral consideration to these entities, it is\n> perhaps doubtful whether those entities actually have moral status (despite\n> it seeming very plausible now that they do). “ _Potentially_ ,” because even\n> if a blocking vote prevents the CEV dynamic from directly protecting these\n> outsiders, there is still a possibility that, within whatever ground rules\n> are left over once the initial dynamic has run, individuals whose wishes\n> were respected and who want some outsiders’ welfare to be protected may\n> successfully bargain to attain this outcome (at the expense of giving up\n> some of their own resources). Whether this would be possible might depend\n> on, among other things, whether the outcome of the CEV dynamic is a set of\n> ground rules that makes it feasible to reach negotiated resolutions to\n> issues of this kind (which might require provisions to overcome strategic\n> bargaining problems).\n\n> [18](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_020.html#filepos755941).\n> Individuals who contribute positively to realizing a safe and beneficial\n> superintelligence might merit _some_ special reward for their labour, albeit\n> something short of a near-exclusive mandate to determine the disposition of\n> humanity’s cosmic endowment. However, the notion of everybody getting an\n> equal share in our extrapolation base is such a nice Schelling point that it\n> should not be lightly tossed away. There is, in any case, an indirect way in\n> which virtue could be rewarded: namely, the CEV itself might turn out to\n> specify that good people who exerted themselves on behalf of humanity should\n> be suitably recognized. This could happen without such people being given\n> any special weight in the extrapolation base if—as is easily imaginable—our\n> CEV would endorse (in the sense of giving at least some nonzero weight to) a\n> principle of just desert.\n\n> [19](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_020.html#filepos757607).\n> Bostrom et al. (2013).\n\n> [20](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_020.html#filepos758507). To the\n> extent that there is some (sufficiently definite) shared meaning that is\n> being expressed when we make moral assertions, a superintelligence should be\n> able to figure out what that meaning is. And to the extent that moral\n> assertions are “truth-apt” (i.e. have an underlying propositional character\n> that enables them to be true or false), the superintelligence should be able\n> to figure out which assertions of the form “Agent _X_ ought now to _Φ_ ” are\n> true. At least, it should outperform us on this task.\n\n> An AI that initially lacks such a capacity for moral cognition should be\n> able to acquire it if it has the intelligence amplification superpower. One\n> way the AI could do this is by reverse-engineering the human brain’s moral\n> thinking and then implement a similar process but run it faster, feed it\n> more accurate factual information, and so forth.\n\n> [21](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_020.html#filepos759108). Since\n> we are uncertain about metaethics, there is a question of what the AI is to\n> do if the preconditions for MR fail to obtain. One option is to stipulate\n> that the AI shut itself off if it assigns a sufficiently high probability to\n> moral cognitivism being false or to there being no suitable non-relative\n> moral truths. Alternatively, we could have the AI revert to some alternative\n> approach, such as CEV.\n\n> We could refine the MR proposal to make it clearer what is to be done in\n> various ambiguous or degenerate cases. For instance, if error theory is true\n> (and hence all positive moral assertions of the form “I ought now to _τ_ ”\n> are false), then the fallback strategy (e.g. shutting down) would be\n> invoked. We could also specify what should happen if there are multiple\n> feasible actions, each of which would be morally right. For example, we\n> might say that in such cases the AI should perform (one of) the permissible\n> actions that humanity’s collective extrapolation would have favored. We\n> might also stipulate what should happen if the true moral theory does not\n> employ terms like “morally right” in its basic vocabulary. For instance, a\n> consequentialist theory might hold that some actions are better than others\n> but that there is no particular threshold corresponding to the notion of an\n> action being “morally right.” We could then say that if such a theory is\n> correct, MR should perform one of the morally best feasible actions, if\n> there is one; or, if there is an infinite number of feasible actions such\n> that for any feasible action there is a better one, then maybe MR could pick\n> any that is at least astronomically better than the best action that any\n> human would have selected in a similar situation, if such an action is\n> feasible—or if not, then an action that is at least as good as the best\n> action a human would have performed.\n\n> A couple of general points should be borne in mind when thinking about how\n> the MR proposal could be refined. First, we might start conservatively,\n> using the fallback option to cover almost all contingencies and only use the\n> “morally right” option in those that we feel we fully understand. Second, we\n> might add the general modulator to the MR proposal that it is to be\n> “interpreted charitably, and revised as we would have revised it if we had\n> thought more carefully about it before we wrote it down, etc.”\n\n> [22](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_020.html#filepos761129). Of\n> these terms, “knowledge” might seem the one most readily susceptible to a\n> formal analysis (in information-theoretic terms). However, to represent what\n> it is for a human to know something, the AI may need a sophisticated set of\n> representations relating to complex psychological properties. A human being\n> does not “know” all the information that is stored somewhere in her brain.\n\n> [23](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_020.html#filepos761364). One\n> indicator that the terms in CEV are (marginally) less opaque is that it\n> would count as philosophical progress if we could analyze moral rightness in\n> terms like those used in CEV. In fact, one of the main strands in\n> metaethics—ideal observer theory—purports to do just that. See, e.g., Smith\n> et al. (1989).\n\n> [24](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_020.html#filepos762034). This\n> requires confronting the problem of fundamental normative uncertainty. It\n> can be shown that it is not always appropriate to act according to the moral\n> theory that has the highest probability of being true. It can also be shown\n> that it is not always appropriate to perform the action that has the highest\n> probability of being right. Some way of trading probabilities against\n> “degrees of wrongness” or severity of issues at stake seems to be needed.\n> For some ideas in this direction, see Bostrom (2009a).\n\n> [25](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_020.html#filepos762251). It\n> could possibly even be argued that it is an adequacy condition for any\n> explication of the notion of moral rightness that it account for how Joe\n> Sixpack is able to have some idea of right and wrong.\n\n> [26](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_020.html#filepos762636). It is\n> not obvious that the morally right thing _for us_ to do is to build an AI\n> that implements MR, even if we assume that _the AI itself_ would always act\n> morally. Perhaps it would be objectionably hubristic or arrogant of us to\n> build such an AI (especially since many people may disapprove of that\n> project). This issue can be partially finessed by tweaking the MR proposal.\n> Suppose that we stipulate that the AI should act (to do what it would be\n> morally right for it to do) only if it was morally right for its creators to\n> have built the AI in the first place; otherwise it should shut itself down.\n> It is hard to see how we would be committing any grave moral wrong in\n> creating _that_ kind of AI, since if it were wrong for us to create it, the\n> only consequence would be that an AI was created that immediately shuts\n> itself down, assuming that the AI has committed no mind crime up to that\n> point. (We might nevertheless have acted wrongly—for instance, by having\n> failed to seize the opportunity to build some other AI instead.)\n\n> A second issue is supererogation. Suppose there are many actions the AI\n> could take, each of which would be morally right—in the sense of being\n> _morally permissible_ —yet some of which are morally better than the others.\n> One option is to have the AI aim to select the morally best action in any\n> such a situation (or one of the best actions, in case there are several that\n> are equally good). Another option is to have the AI select from among the\n> morally permissible actions one that maximally satisfies some other (non-\n> moral) desideratum. For example, the AI could select, from among the actions\n> that are morally permissible, the action that our CEV would prefer it to\n> take. Such an AI, while never doing anything that is morally impermissible,\n> might protect our interests more than an AI that does what is morally best.\n\n> [27](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_020.html#filepos763521). When\n> the AI evaluates the moral permissibility of our act of creating the AI, it\n> should interpret permissibility in its objective sense. In one ordinary\n> sense of “morally permissible,” a doctor acts morally permissibly when she\n> prescribes a drug she believes will cure her patient—even if the patient,\n> unbeknownst to the doctor, is allergic to the drug and dies as a result.\n> Focusing on objective moral permissibility takes advantage of the presumably\n> superior epistemic position of the AI.\n\n> [28](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_020.html#filepos763939). More\n> directly, it depends on the AI’s _beliefs_ about which ethical theory is\n> true (or, more precisely, on its probability distribution over ethical\n> theories).\n\n> [29](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_020.html#filepos766485). It can\n> be difficult to imagine how superlatively wonderful these physically\n> possible lives might be. See Bostrom (2008c) for a poetic attempt to convey\n> some sense of this. See Bostrom (2008b) for an argument that some of these\n> possibilities could be good _for us_ , good for existing human beings.\n\n> [30](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_020.html#filepos767283). It\n> might seem deceptive or manipulative to promote one proposal if one thinks\n> that some other proposal would be better. But one could promote it in ways\n> that avoid insincerity. For example, one could freely acknowledge the\n> superiority of the ideal while still promoting the non-ideal as the best\n> attainable compromise.\n\n> [31](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_020.html#filepos769428). Or\n> some other positively evaluative term, such as “good,” “great,” or\n> “wonderful.”\n\n> [32](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_020.html#filepos770387). This\n> echoes a principle in software design known as “Do What I Mean,” or DWIM.\n> See Teitelman (1966).\n\n> [33](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_020.html#filepos773603). Goal\n> content, decision theory, and epistemology are three aspects that should be\n> elucidated; but we do not intend to beg the question of whether there must\n> be a neat decomposition into these three separate components.\n\n> [34](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_020.html#filepos776885). An\n> ethical project ought presumably to allocate at most a modest portion of the\n> eventual benefits that the superintelligence produces as special rewards to\n> those who contributed in morally permissible ways to the project’s success.\n> Allocating a great portion to the incentive wrapping scheme would be\n> unseemly. It would be analogous to a charity that spends 90% of its income\n> on performance bonuses for its fundraisers and on advertising campaigns to\n> increase donations.\n\n> [35](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_020.html#filepos778263). How\n> could the dead be rewarded? One can think of several possibilities. At the\n> low end, there could be memorial services and monuments, which would be a\n> reward insofar as people desired posthumous fame. The deceased might also\n> have other preferences about the future that could be honored, for instance\n> concerning cultures, arts, buildings, or natural environments. Furthermore,\n> most people care about their descendants, and special privileges could be\n> granted to the children and grandchildren of contributors. More\n> speculatively, the superintelligence might be able to create relatively\n> faithful simulations of some past people—simulations that would be conscious\n> and that would resemble the original sufficiently to count as a form of\n> survival (according to at least some people’s criteria). This would\n> presumably be easier for people who have been placed in cryonic suspension;\n> but perhaps for a superintelligence it would not be impossible to recreate\n> something quite similar to the original person from other preserved records\n> such as correspondence, publications, audiovisual materials and digital\n> records, or the personal memories of other survivors. A superintelligence\n> might also think of some possibilities that do not readily occur to us.\n\n> [36](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_020.html#filepos780105). On\n> Pascalian mugging, see Bostrom (2009b). For an analysis of issues related to\n> infinite utilities, see Bostrom (2011a). On fundamental normative\n> uncertainty, see, e.g., Bostrom (2009a).\n\n> [37](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_020.html#filepos780452). E.g.,\n> Price (1991); Joyce (1999); Drescher (2006); Yudkowsky (2010); Dai (2009).\n\n> [38](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_020.html#filepos782032). E.g.,\n> Bostrom (2009a).\n\n> [39](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_020.html#filepos782834). It is\n> also conceivable that using indirect normativity to specify the AI’s goal\n> content would mitigate the problems that might arise from an incorrectly\n> specified decision theory. Consider, for example, the CEV approach. If it\n> were implemented well, it might be able to compensate for at least some\n> errors in the specification of the AI’s decision theory. The implementation\n> could allow the values that our coherent extrapolated volition would want\n> the AI to pursue to depend on the AI’s decision theory. If our idealized\n> selves knew they were making value specifications for an AI that was using a\n> particular kind of decision theory, they could adjust their value\n> specifications such as to make the AI behave benignly despite its warped\n> decision theory—much like one can cancel out the distorting effects of one\n> lens by placing another lens in front of it that distorts oppositely.\n\n> [40](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_020.html#filepos783773). Some\n> epistemological systems may, in a holistic manner, have no distinct\n> foundation. In that case, the constitutional inheritance is not a distinct\n> set of principles, but rather, as it were, an epistemic starting point that\n> embodies certain propensities to respond to incoming streams of evidence.\n\n> [41](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_020.html#filepos785213). See,\n> e.g., the problem of distortion discussed in Bostrom (2011a).\n\n> [42](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_020.html#filepos786545). For\n> instance, one disputed issue in anthropic reasoning is whether the so-called\n> self-indication assumption should be accepted. The self-indication\n> assumption states, roughly, that from the fact that you exist you should\n> infer that hypotheses according to which larger numbers _N_ of observers\n> exist should receive a probability boost proportional to _N_. For an\n> argument against this principle, see the “Presumptuous Philosopher” gedanken\n> experiment in Bostrom (2002a). For a defense of the principle, see Olum\n> (2002); and for a critique of that defense, see Bostrom and Ćirković (2003).\n> Beliefs about the self-indication assumption might affect various empirical\n> hypotheses of potentially crucial strategic relevance, for example,\n> considerations such as the Carter–Leslie doomsday argument, the simulation\n> argument, and “great filter” arguments. See Bostrom (2002a, 2003a, 2008a);\n> Carter (1983); Ćirković et al. (2010); Hanson (1998d); Leslie (1996);\n> Tegmark and Bostrom (2005). A similar point could be made with regard to\n> other fraught issues in observation selection theory, such as whether the\n> choice of reference class can be relativized to observer-moments, and if so\n> how.\n\n> [43](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_020.html#filepos787361). See,\n> e.g., Howson and Urbach (1993). There are also some interesting results that\n> narrow the range of situations in which two Bayesian agents can rationally\n> disagree when their opinions are common knowledge; see Aumann (1976) and\n> Hanson (2006).\n\n> [44](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_020.html#filepos792463). Cf.\n> the concept of a “last judge” in Yudkowsky (2004).\n\n> [45](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_020.html#filepos793934). There\n> are many important issues outstanding in epistemology, some mentioned\n> earlier in the text. The point here is that we may not need to get all the\n> solutions exactly right in order to achieve an outcome that is practically\n> indiscernible from the best outcome. A mixture model (which throws together\n> a wide range of diverse priors) might work.\n\n##### CHAPTER 14: THE STRATEGIC PICTURE\n\n> [1](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos801753). This\n> principle is introduced in Bostrom (2009b, 190), where it is also noted that\n> it is not tautological. For a visual analogy, picture a box with large but\n> finite volume, representing the space of basic capabilities that could be\n> obtained through some possible technology. Imagine sand being poured into\n> this box, representing research effort. How you pour the sand determines\n> where it piles up in the box. But if you keep on pouring, the entire space\n> eventually gets filled.\n\n> [2](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos803040). Bostrom\n> (2002b).\n\n> [3](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos803794). This is\n> not the perspective from which science policy has traditionally been viewed.\n> Harvey Averch describes science and technology policy in the United States\n> between 1945 and 1984 as having been centered on debates about the optimum\n> level of public investment in the S&T enterprise and on the extent to which\n> the government should attempt to “pick winners” in order to achieve the\n> greatest increase in the nation’s economic prosperity and military strength.\n> In these calculations, technological progress is always assumed to be good.\n> But Averch also describes the rise of critical perspectives, which question\n> the “progress is always good” premiss (Averch 1985). See also Graham (1997).\n\n> [4](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos806804). Bostrom\n> (2002b).\n\n> [5](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos807131). This is\n> of course by no means tautological. One could imagine a case being made for\n> a different order of development. It could be argued that it would be better\n> for humanity to confront some less difficult challenge first, say the\n> development of nanotechnology, on grounds that this would force us to\n> develop better institutions, become more internationally coordinated, and\n> mature in our thinking about global strategy. Perhaps we would be more\n> likely to rise to a challenge that presents a less metaphysically confusing\n> threat than machine superintelligence. Nanotechnology (or synthetic biology,\n> or whatever the lesser challenge we confront first) might then serve as a\n> footstool that would help us ascend to the capability level required to deal\n> with the higher-level challenge of superintelligence.\n\n> Such an argument would have to be assessed on a case-by-case basis. For\n> example, in the case of nanotechnology, one would have to consider various\n> possible consequences such as the boost in hardware performance from\n> nanofabricated computational substrates; the effects of cheap physical\n> capital for manufacturing on economic growth; the proliferation of\n> sophisticated surveillance technology; the possibility that a singleton\n> might emerge though the direct or indirect effects of a nanotechnology\n> breakthrough; and the greater feasibility of neuromorphic and whole brain\n> emulation approaches to machine intelligence. It is beyond the scope of our\n> investigation to consider all these issues (or the parallel issues that\n> might arise for other existential risk-causing technologies). Here we just\n> point out the prima facie case for favoring a superintelligence-first\n> sequence of development—while stressing that there are complications that\n> might alter this preliminary assessment in some cases.\n\n> [6](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos809878). Pinker\n> (2011); Wright (2001).\n\n> [7](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos813616). It\n> might be tempting to suppose the hypothesis that everything has accelerated\n> to be meaningless on grounds that it does not (at first glance) seem to have\n> any observational consequences; but see, e.g., Shoemaker (1969).\n\n> [8](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos819625). The\n> level of preparedness is not measured by the amount of effort expended on\n> preparedness activities, but by how propitiously configured conditions\n> actually are and how well-poised key decision makers are to take appropriate\n> action.\n\n> [9](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos822874). The\n> degree of international trust during the lead-up to the intelligence\n> explosion may also be a factor. We consider this in the section\n> “Collaboration” later in the chapter.\n\n> [10](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos825428).\n> Anecdotally, it appears those currently seriously interested in the control\n> problem are disproportionately sampled from one extreme end of the\n> intelligence distribution, though there could be alternative explanations\n> for this impression. If the field becomes fashionable, it will undoubtedly\n> be flooded with mediocrities and cranks.\n\n> [11](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos828419). I owe\n> this term to Carl Shulman.\n\n> [12](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos830094). How\n> similar to a brain does a machine intelligence have to be to count as a\n> whole brain emulation rather than a neuromorphic AI? The relevant\n> determinant might be whether the system reproduces either the values or the\n> full panoply of cognitive and evaluative tendencies of either a particular\n> individual or a generic human being, because this would plausibly make a\n> difference to the control problem. Capturing these properties may require a\n> rather high degree of emulation fidelity.\n\n> [13](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos832376). The\n> magnitude of the boost would of course depend on how big the push was, and\n> also where resources for the push came from. There might be no net boost for\n> neuroscience if all the extra resources invested in whole brain emulation\n> research were deducted from regular neuroscience research—unless a keener\n> focus on emulation research just happened to be a more effective way of\n> advancing neuroscience than the default portfolio of neuroscience research.\n\n> [14](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos834166). See\n> Drexler (1986, 242). Drexler (private communication) confirms that this\n> reconstruction corresponds to the reasoning he was seeking to present.\n> Obviously, a number of implicit premisses would have to be added if one\n> wished to cast the argument in the form of a deductively valid chain of\n> reasoning.\n\n> [15](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos836099).\n> Perhaps we ought not to welcome _small_ catastrophes in case they increase\n> our vigilance to the point of making us prevent the _medium-scale_\n> catastrophes that would have been needed to make us take the strong\n> precautions necessary to prevent existential catastrophes? (And of course,\n> just as with biological immune systems, we also need to be concerned with\n> over-reactions, analogous to allergies and autoimmune disorders.)\n\n> [16](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos837401). Cf.\n> Lenman (2000); Burch-Brown (2014).\n\n> [17](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos838424). Cf.\n> Bostrom (2007).\n\n> [18](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos839807). Note\n> that this argument focuses on the ordering rather than the timing of the\n> relevant events. Making superintelligence happen earlier would help preempt\n> other existential transition risks only if the intervention changes the\n> sequence of the key developments: for example, by making superintelligence\n> happen before various milestones are reached in nanotechnology or synthetic\n> biology.\n\n> [19](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos842790). If\n> solving the control problem is _extremely_ difficult compared to solving the\n> performance problem in machine intelligence, and if project ability\n> correlates only weakly with project size, then it is possible that it would\n> be better that a small project gets there first, namely if the variance in\n> capability is greater among smaller projects. In such a situation, even if\n> smaller projects are on average less competent than larger projects, it\n> could be less unlikely that a given small project would happen to have the\n> freakishly high level of competence needed to solve the control problem.\n\n> [20](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos844167). This\n> is not to deny that one can imagine tools that could promote global\n> deliberation and which would benefit from, or even require, further progress\n> in hardware—for example, high-quality translation, better search, ubiquitous\n> access to smart phones, attractive virtual reality environments for social\n> intercourse, and so forth.\n\n> [21](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos845880).\n> Investment in emulation technology could speed progress toward whole brain\n> emulation not only directly (through any technical deliverables produced)\n> but also indirectly by creating a constituency that will push for more\n> funding and boost the visibility and credibility of the whole brain\n> emulation (WBE) vision.\n\n> [22](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos848544). How\n> much expected value would be lost if the future were shaped by the desires\n> of one random human rather than by (some suitable superposition of) the\n> desires of all of humanity? This might depend sensitively on what evaluation\n> standard we use, and also on whether the desires in question are idealized\n> or raw.\n\n> [23](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos849901). For\n> example, whereas human minds communicate slowly via language, AIs can be\n> designed so that instances of the same program are able easily and quickly\n> to transfer both skills and information amongst one another. Machine minds\n> designed _ab initio_ could do away with cumbersome legacy systems that\n> helped our ancestors deal with aspects of the natural environment that are\n> unimportant in cyberspace. Digital minds might also be designed to take\n> advantage of fast serial processing unavailable to biological brains, and to\n> make it easy to install new modules with highly optimized functionality\n> (e.g. symbolic processing, pattern recognition, simulators, data mining, and\n> planning). Artificial intelligence might also have significant non-technical\n> advantages, such as being more easily patentable or less entangled in the\n> moral complexities of using human uploads.\n\n> [24](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos850580). If _p_\n> 1 and _p_ 2 are the probabilities of failure at each step, the total\n> probability of failure is _p_ 1 \\+ (1 – _p_ 1)_p_ 2 since one can fail\n> terminally only once.\n\n> [25](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos853154). It is\n> possible, of course, that the frontrunner will not have such a large lead\n> and will not be able to form a singleton. It is also possible that a\n> singleton would arise before AI even without the intervention of WBE, in\n> which case this reason for favoring a WBE-first scenario falls away.\n\n> [26](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos854177). Is\n> there a way for a promoter of WBE to increase the specificity of her support\n> so that it accelerates WBE while minimizing the spillover to AI development?\n> Promoting scanning technology is probably a better bet than promoting\n> neurocomputational modeling. (Promoting computer hardware is unlikely to\n> make much difference one way or the other, given the large commercial\n> interests that are anyway incentivizing progress in that field.)\n\n> Promoting scanning technology may increase the likelihood of a multipolar\n> outcome by making scanning less likely to be a bottleneck, thus increasing\n> the chance that the early emulation population will be stamped from many\n> different human templates rather than consisting of gazillions of copies of\n> a tiny number of templates. Progress in scanning technology also makes it\n> more likely that the bottleneck will instead be computing hardware, which\n> would tend to slow the takeoff.\n\n> [27](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos855350).\n> Neuromorphic AI may also lack other safety-promoting attributes of whole\n> brain emulation, such as having a profile of cognitive strengths and\n> weaknesses similar to that of a biological human being (which would let us\n> use our experience of humans to form some expectations of the system’s\n> capabilities at different stages of its development).\n\n> [28](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos856581). If\n> somebody’s motive for promoting WBE is to make WBE happen before AI, they\n> should bear in mind that accelerating WBE will alter the order of arrival\n> only if the default timing of the two paths toward machine intelligence is\n> close and with a slight edge to AI. Otherwise, _either_ investment in WBE\n> will simply make WBE happen earlier than it otherwise would (reducing\n> hardware overhang and preparation time) but without affecting the sequence\n> of development; _or else_ such investment in WBE will have little effect\n> (other than perhaps making AI happen even sooner by stimulating progress on\n> neuromorphic AI).\n\n> [29](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos857681).\n> Comment on Hanson (2009).\n\n> [30](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos859093). There\n> would of course be _some_ magnitude and imminence of existential risk for\n> which it would be preferable even from the person-affecting perspective to\n> postpone the risk—whether to enable existing people to eke out a bit more\n> life before the curtain drops or to provide more time for mitigation efforts\n> that might reduce the danger.\n\n> [31](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos859894).\n> Suppose we could take some action that would bring the intelligence\n> explosion closer by one year. Let us say that the people currently\n> inhabiting the Earth are dying off at a rate of 1% per year, and that the\n> default risk of human extinction from the intelligence explosion is 20% (to\n> pick an arbitrary number for the purposes of illustration only). Then\n> hastening the arrival of the intelligence explosion by 1 year might be worth\n> (from a person-affecting standpoint) an increase in the risk from 20% to\n> 21%, i.e. a 5% increase in risk level. However, the vast majority of people\n> alive one year before the start of the intelligence explosion would at that\n> point have an interest in postponing it if they could thereby reduce the\n> risk of the explosion by one percentage point (since most individuals would\n> reckon their own risk of dying in the next year to be much smaller than\n> 1%—given that most mortality occurs in relatively narrow demographics such\n> as the frail and the elderly). One could thus have a model in which each\n> year the population votes to postpone the intelligence explosion by another\n> year, so that the intelligence explosion never happens, although everybody\n> who ever lives agrees that it would be better if the intelligence explosion\n> happened at some point. In reality, of course, coordination failures,\n> limited predictability, or preferences for things other than personal\n> survival are likely to prevent such an unending pause.\n\n> If one uses the standard economic discount factor instead of the person-\n> affecting standard, the magnitude of the potential upside is diminished,\n> since the value of existing people getting to enjoy astronomically long\n> lives is then steeply discounted. This effect is especially strong if the\n> discount factor is applied to each individual’s subjective time rather than\n> to sidereal time. If future benefits are discounted at a rate of _x_ % per\n> year, and the background level of existential risk from other sources is _y_\n> % per year, then the optimum point for the intelligence explosion would be\n> when delaying the explosion for another year would produce less than _x_ \\+\n> _y_ percentage points of reduction of the existential risk associated with\n> an intelligence explosion.\n\n> [32](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos863274). I am\n> indebted to Carl Shulman and Stuart Armstrong for help with this model. See\n> also Shulman (2010a, 3): “Chalmers (2010) reports a consensus among cadets\n> and staff at the U.S. West Point military academy that the U.S. government\n> would not restrain AI research even in the face of potential catastrophe,\n> for fear that rival powers would gain decisive advantage.”\n\n> [33](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos868426). That\n> is, information in the model is always bad _ex ante_. Of course, depending\n> on what the information actually is, it will in some cases turn out to be\n> good that the information became known, notably if the gap between leader\n> and runner-up is much greater than one would reasonably have guessed in\n> advance.\n\n> [34](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos862610). It\n> might even present an existential risk, especially if preceded by the\n> introduction of novel military technologies of destruction or unprecedented\n> arms buildups.\n\n> [35](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos862864). A\n> project could have its workers distributed over a large number of locations\n> and collaborating via encrypted communications channels. But this tactic\n> involves a security trade-off: while geographical dispersion may offer some\n> protection against military attacks, it would impede operational security,\n> since it is harder to prevent personnel from defecting, leaking information,\n> or being abducted by a rival power if they are spread out over many\n> locations.\n\n> [36](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos869867). Note\n> that a large temporal discount factor could make a project behave in some\n> ways as though it were in a race, even if it knows it has no real\n> competitor. The large discount factor means it would care little about the\n> far future. Depending on the situation, this would discourage blue-sky R&D,\n> which would tend to delay the machine intelligence revolution (though\n> perhaps making it more abrupt when it does occur, because of hardware\n> overhang). But the large discount factor—or a low level of caring for future\n> generations—would also make existential risks seem to matter less. This\n> would encourage gambles that involve the possibility of an immediate gain at\n> the expense of an increased risk of existential catastrophe, thus\n> disincentivizing safety investment and incentivizing an early\n> launch—mimicking the effects of the race dynamic. By contrast to the race\n> dynamic, however, a large discount factor (or disregard for future\n> generations) would have no particular tendency to incite conflict.\n\n> Reducing the race dynamic is a main benefit of collaboration. That\n> collaboration would facilitate sharing of ideas for how to solve the control\n> problem is also a benefit, although this is to some extent counterbalanced\n> by the fact that collaboration would also facilitate sharing of ideas for\n> how to solve the competence problem. The net effect of this facilitation of\n> idea-sharing may be to slightly increase the collective intelligence of the\n> relevant research community.\n\n> [37](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos871807). On the\n> other hand, public oversight by a single government would risk producing an\n> outcome in which one nation monopolizes the gains. This outcome seems\n> inferior to one in which unaccountable altruists ensure that everybody\n> stands to gain. Furthermore, oversight by a national government would not\n> necessarily mean that even all the citizens of that country receive a share\n> of the benefit: depending on the country in question, there is a greater or\n> smaller risk that all the benefits would be captured by a political elite or\n> a few self-serving agency personnel.\n\n> [38](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos873945). One\n> qualification is that the use of incentive wrapping (as discussed in\n> [Chapter\n> 12](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos645293)) might\n> in some circumstances encourage people to join a project as active\n> collaborators rather than passive free-riders.\n\n> [39](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos874591).\n> Diminishing returns would seem to set in at a much smaller scale. Most\n> people would rather have one star than a one-in-a-billion chance of a galaxy\n> with a billion stars. Indeed, most people would rather have a billionth of\n> the resources on Earth than a one-in-a-billion chance of owning the entire\n> planet.\n\n> [40](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos875114). Cf.\n> Shulman (2010a).\n\n> [41](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos875577).\n> Aggregative ethical theories run into trouble when the idea that the cosmos\n> might be infinite is taken seriously; see Bostrom (2011b). There may also be\n> trouble when the idea of ridiculously large but finite values is taken\n> seriously; see Bostrom (2009b).\n\n> [42](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos877607). If one\n> makes a computer larger, one eventually faces relativistic constraints\n> arising from communication latencies between the different parts of the\n> computer—signals do not propagate faster than light. If one shrinks the\n> computer, one encounters quantum limits to miniaturization. If one increases\n> the density of the computer, one slams into the black hole limit.\n> Admittedly, we cannot be completely certain that new physics will not one\n> day be discovered offering some way around these limitations.\n\n> [43](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos878286). The\n> number of copies of a person would scale linearly with resources with no\n> upper bound. Yet it is not clear how much the average human being would\n> value having multiple copies of herself. Even those people who would prefer\n> to be multiply instantiated may not have a utility function that is linear\n> with increasing number of copies. Copy numbers, like life years, might have\n> diminishing returns in the typical person’s utility function.\n\n> [44](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos880492). A\n> singleton is highly internally collaborative at the highest level of\n> decision-making. A singleton _could_ have a lot of non-collaboration and\n> conflict at lower levels, if the higher-level agency that constitutes the\n> singleton chooses to have things that way.\n\n> [45](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos883781). If\n> each rival AI team is convinced that the other teams are so misguided as to\n> have no chance of producing an intelligence explosion, then one reason for\n> collaboration—avoiding the race dynamic—is obviated: each team should\n> independently choose to go slower in the confident belief that it lacks any\n> serious competition.\n\n> [46](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos885272). A PhD\n> student.\n\n> [47](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos887097). This\n> formulation is intended to be read so as to include a prescription that the\n> well-being of nonhuman animals and other sentient beings (including digital\n> minds) that exist or may come to exist be given due consideration. It is not\n> meant to be read as a license for one AI developer to substitute his or her\n> own moral intuitions for those of the wider moral community. The principle\n> is consistent with the “coherent extrapolated volition” approach discussed\n> in [Chapter\n> 12](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos645293), with an\n> extrapolation base encompassing all humans.\n\n> A further clarification: The formulation is not intended to necessarily\n> exclude the possibly of post-transition property rights in artificial\n> superintelligences or their constituent algorithms and data structures. The\n> formulation is meant to be agnostic about what legal or political systems\n> would best serve to organize transactions within a hypothetical future\n> posthuman society. What the formulation _is_ meant to assert is that the\n> choice of such a system, insofar as its selection is causally determined by\n> how superintelligence is initially developed, should to be made on the basis\n> of the stated criterion; that is, the post-transition constitutional system\n> should be chosen for the benefit of all of humanity and in the service of\n> widely shared ethical ideals—as opposed to, for instance, for the benefit\n> merely of whoever happened to be the first to develop superintelligence.\n\n> [48](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos888811).\n> Refinements of the windfall clause are obviously possible. For example,\n> perhaps the threshold should be expressed in _per capita_ terms, or maybe\n> the winner should be allowed to keep a somewhat larger than equal share of\n> the overshoot in order to more strongly incentivize further production (some\n> version of Rawls’s maximin principle might be attractive here). Other\n> refinements would refocus the clause away from dollar amounts and restate it\n> in terms of “influence on humanity’s future” or “degree to which different\n> parties’ interests are weighed in a future singleton’s utility function” or\n> some such.\n\n##### CHAPTER 15: CRUNCH TIME\n\n> [1](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_022.html#filepos891598). Some\n> research is worthwhile not because of what it discovers but for other\n> reasons, such as by entertaining, educating, accrediting, or uplifting those\n> who engage in it.\n\n> [2](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_022.html#filepos894018). I am\n> not suggesting that _nobody_ should work on pure mathematics or philosophy.\n> I am also not suggesting that these endeavors are especially wasteful\n> compared to all the other dissipations of academia or society at large. It\n> is probably very good that some people can devote themselves to the life of\n> the mind and follow their intellectual curiosity wherever it leads,\n> independent of any thought of utility or impact. The suggestion is that at\n> the margin, some of the best minds might, upon realizing that their\n> cognitive performances may become obsolete in the foreseeable future, want\n> to shift their attention to those theoretical problems for which it makes a\n> difference whether we get the solution a little sooner.\n\n> [3](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_022.html#filepos897425). Though\n> one should be cautious in cases where this uncertainty may be\n> protective—recall, for instance, the risk-race model in [Box\n> 13](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos863063), where\n> we found that additional strategic information could be harmful. More\n> generally, we need to worry about information hazards (see Bostrom [2011b]).\n> It is tempting to say that we need more analysis of information hazards.\n> This is probably true, although we might still worry that such analysis\n> itself may produce dangerous information.\n\n> [4](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_022.html#filepos898418). Cf.\n> Bostrom (2007).\n\n> [5](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_022.html#filepos901507). I am\n> grateful to Carl Shulman for emphasizing this point.\n\n\n##\n[BIBLIOGRAPHY](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_006.html#filepos29453)\n\nAcemoglu, Daron. 2003. “Labor- and Capital-Augmenting Technical Change.”\n_Journal of the European Economic Association_ 1 (1): 1–37.\n\nAlbertson, D. G., and Thomson, J. N. 1976. “The Pharynx of _Caenorhabditis\nElegans_.” _Philosophical Transactions of the Royal Society B: Biological\nSciences_ 275 (938): 299–325.\n\nAllen, Robert C. 2008. “A Review of Gregory Clark’s _A Farewell to Alms: A\nBrief Economic History of the World_.” _Journal of Economic Literature_ 46\n(4): 946–73.\n\nAmerican Horse Council. 2005. “National Economic Impact of the US Horse\nIndustry.” Retrieved July 30, 2013. Available at\n<http://www.horsecouncil.org/national-economic-impact-us-horse-industry>.\n\nAnand, Paul, Pattanaik, Prasanta, and Puppe, Clemens, eds. 2009. _The Oxford\nHandbook of Rational and Social Choice_. New York: Oxford University Press.\n\nAndres, B., Koethe, U., Kroeger, T., Helmstaedter, M., Briggman, K. L., Denk,\nW., and Hamprecht, F. A. 2012. “3D Segmentation of SBFSEM Images of Neuropil\nby a Graphical Model over Supervoxel Boundaries.” _Medical Image Analysis_ 16\n(4): 796–805.\n\nArmstrong, Alex. 2012. “Computer Competes in Crossword Tournament.” _I\nProgrammer_ , March 19.\n\nArmstrong, Stuart. 2007. “Chaining God: A Qualitative Approach to AI, Trust\nand Moral Systems.” Unpublished manuscript, October 20. Retrieved December 31,\n2012. Available at <http://www.neweuropeancentury.org/GodAI.pdf>.\n\nArmstrong, Stuart. 2010. _Utility Indifference_ , Technical Report 2010-1.\nOxford: Future of Humanity Institute, University of Oxford.\n\nArmstrong, Stuart. 2013. “General Purpose Intelligence: Arguing the\nOrthogonality Thesis.” _Analysis and Metaphysics_ 12: 68–84.\n\nArmstrong, Stuart, and Sandberg, Anders. 2013. “Eternity in Six Hours:\nIntergalactic Spreading of Intelligent Life and Sharpening the Fermi Paradox.”\n_Acta Astronautica_ 89: 1–13.\n\nArmstrong, Stuart, and Sotala, Kaj. 2012. “How We’re Predicting AI—or Failing\nTo.” In _Beyond AI: Artificial Dreams_ , edited by Jan Romportl, Pavel Ircing,\nEva Zackova, Michal Polak, and Radek Schuster, 52–75. Pilsen: University of\nWest Bohemia. Retrieved February 2, 2013.\n\nAsimov, Isaac. 1942. “Runaround.” _Astounding Science-Fiction_ , March,\n94–103.\n\nAsimov, Isaac. 1985. _Robots and Empire_. New York: Doubleday.\n\nAumann, Robert J. 1976. “Agreeing to Disagree.” _Annals of Statistics_ 4 (6):\n1236–9.\n\nAverch, Harvey Allen. 1985. _A Strategic Analysis of Science and Technology\nPolicy_. Baltimore: Johns Hopkins University Press.\n\nAzevedo, F. A. C., Carvalho, L. R. B., Grinberg, L. T., Farfel, J. M.,\nFerretti, R. E. L., Leite, R. E. P., Jacob, W., Lent, R., and Herculano-\nHouzel, S. 2009. “Equal Numbers of Neuronal and Nonneuronal Cells Make the\nHuman Brain an Isometrically Scaled-up Primate Brain.” _Journal of Comparative\nNeurology_ 513 (5): 532–41.\n\nBaars, Bernard J. 1997. _In the Theater of Consciousness: The Workspace of the\nMind_. New York: Oxford University Press.\n\nBaratta, Joseph Preston. 2004. _The Politics of World Federation: United\nNations, UN Reform, Atomic Control_. Westport, CT: Praeger.\n\nBarber, E. J. W. 1991. _Prehistoric Textiles: The Development of Cloth in the\nNeolithic and Bronze Ages with Special Reference to the Aegean_. Princeton,\nNJ: Princeton University Press.\n\nBartels, J., Andreasen, D., Ehirim, P., Mao, H., Seibert, S., Wright, E. J.,\nand Kennedy, P. 2008. “Neurotrophic Electrode: Method of Assembly and\nImplantation into Human Motor Speech Cortex.” _Journal of Neuroscience\nMethods_ 174 (2): 168–76.\n\nBartz, Jennifer A., Zaki, Jamil, Bolger, Niall, and Ochsner, Kevin N. 2011.\n“Social Effects of Oxytocin in Humans: Context and Person Matter.” _Trends in\nCognitive Science_ 15 (7): 301–9.\n\nBasten, Stuart, Lutz, Wolfgang, and Scherbov, Sergei. 2013. “Very Long Range\nGlobal Population Scenarios to 2300 and the Implications of Sustained Low\nFertility.” _Demographic Research_ 28: 1145–66.\n\nBaum, Eric B. 2004. _What Is Thought?_ Bradford Books. Cambridge, MA: MIT\nPress.\n\nBaum, Seth D., Goertzel, Ben, and Goertzel, Ted G. 2011. “How Long Until\nHuman-Level AI? Results from an Expert Assessment.” _Technological Forecasting\nand Social Change_ 78 (1): 185–95.\n\nBeal, J., and Winston, P. 2009. “Guest Editors’ Introduction: The New Frontier\nof Human-Level Artificial Intelligence.” _IEEE Intelligent Systems_ 24 (4):\n21–3.\n\nBell, C. Gordon, and Gemmell, Jim. 2009. _Total Recall: How the E-Memory\nRevolution Will Change Everything_. New York: Dutton.\n\nBenyamin, B., Pourcain, B. St., Davis, O. S., Davies, G., Hansell, M. K.,\nBrion, M.-J. A., Kirkpatrick, R. M., et al. 2013. “Childhood Intelligence is\nHeritable, Highly Polygenic and Associated With FNBP1L.” _Molecular\nPsychiatry_ (January 23).\n\nBerg, Joyce E., and Rietz, Thomas A. 2003. “Prediction Markets as Decision\nSupport Systems.” _Information Systems Frontiers_ 5 (1): 79–93.\n\nBerger, Theodore W., Chapin, J. K., Gerhardt, G. A., Soussou, W. V., Taylor,\nD. M., and Tresco, P. A., eds. 2008. _Brain–Computer Interfaces: An\nInternational Assessment of Research and Development Trends_. Springer.\n\nBerger, T. W., Song, D., Chan, R. H., Marmarelis, V. Z., LaCoss, J., Wills,\nJ., Hampson, R. E., Deadwyler, S. A., and Granacki, J. J. 2012. “A Hippocampal\nCognitive Prosthesis: Multi-Input, Multi-Output Nonlinear Modeling and VLSI\nImplementation.” _IEEE Transactions on Neural Systems and Rehabilitation\nEngineering_ 20 (2): 198–211.\n\nBerliner, Hans J. 1980a. “Backgammon Computer-Program Beats World Champion.”\n_Artificial Intelligence_ 14 (2): 205–220.\n\nBerliner, Hans J. 1980b. “Backgammon Program Beats World Champ.” _SIGART\nNewsletter_ 69: 6–9.\n\nBernardo, José M., and Smith, Adrian F. M. 1994. _Bayesian Theory_ , 1st ed.\nWiley Series in Probability & Statistics. New York: Wiley.\n\nBirbaumer, N., Murguialday, A. R., and Cohen, L. 2008. “Brain–Computer\nInterface in Paralysis.” _Current Opinion in Neurology_ 21 (6): 634–8.\n\nBird, Jon, and Layzell, Paul. 2002. “The Evolved Radio and Its Implications\nfor Modelling the Evolution of Novel Sensors.” In _Proceedings of the 2002\nCongress on Evolutionary Computation_ , 2: 1836–41.\n\nBlair, Clay, Jr. 1957. “Passing of a Great Mind: John von Neumann, a\nBrilliant, Jovial Mathematician, was a Prodigious Servant of Science and His\nCountry.” _Life_ , February 25, 89–104.\n\nBobrow, Daniel G. 1968. “Natural Language Input for a Computer Problem Solving\nSystem.” In _Semantic Information Processing_ , edited by Marvin Minsky,\n146–227. Cambridge, MA: MIT Press.\n\nBostrom, Nick. 1997. “Predictions from Philosophy? How Philosophers Could Make\nThemselves Useful.” Unpublished manuscript. Last revised September 19, 1998.\n\nBostrom, Nick. 2002a. _Anthropic Bias: Observation Selection Effects in\nScience and Philosophy_. New York: Routledge.\n\nBostrom, Nick. 2002b. “Existential Risks: Analyzing Human Extinction Scenarios\nand Related Hazards.” _Journal of Evolution and Technology_ 9.\n\nBostrom, Nick. 2003a. “Are We Living in a Computer Simulation?” _Philosophical\nQuarterly_ 53 (211): 243–55.\n\nBostrom, Nick. 2003b. “Astronomical Waste: The Opportunity Cost of Delayed\nTechnological Development.” _Utilitas_ 15 (3): 308–314.\n\nBostrom, Nick. 2003c. “Ethical Issues in Advanced Artificial Intelligence.” In\n_Cognitive, Emotive and Ethical Aspects of Decision Making in Humans and in\nArtificial Intelligence_ , edited by Iva Smit and George E. Lasker, 2: 12–17.\nWindsor, ON: International Institute for Advanced Studies in Systems Research\n/ Cybernetics.\n\nBostrom, Nick. 2004. “The Future of Human Evolution.” In _Two Hundred Years\nAfter Kant, Fifty Years After Turing_ , edited by Charles Tandy, 2: 339–371.\nDeath and Anti-Death. Palo Alto, CA: Ria University Press.\n\nBostrom, Nick. 2006a. “How Long Before Superintelligence?” _Linguistic and\nPhilosophical Investigations_ 5(1): 11–30.\n\nBostrom, Nick. 2006b. “Quantity of Experience: Brain-Duplication and Degrees\nof Consciousness.” _Minds and Machines_ 16 (2): 185–200.\n\nBostrom, Nick. 2006c. “What is a Singleton?” _Linguistic and Philosophical\nInvestigations_ 5 (2): 48–54.\n\nBostrom, Nick. 2007. “Technological Revolutions: Ethics and Policy in the\nDark.” In _Nanoscale: Issues and Perspectives for the Nano Century_ , edited\nby Nigel M. de S. Cameron and M. Ellen Mitchell, 129–52. Hoboken, NJ: Wiley.\n\nBostrom, Nick. 2008a. “Where Are They? Why I Hope the Search for\nExtraterrestrial Life Finds Nothing.” _MIT Technology Review_ , May/June\nissue, 72–7.\n\nBostrom, Nick. 2008b. “Why I Want to Be a Posthuman When I Grow Up.” In\n_Medical Enhancement and Posthumanity_ , edited by Bert Gordijn and Ruth\nChadwick, 107–37. New York: Springer.\n\nBostrom, Nick. 2008c. “Letter from Utopia.” _Studies in Ethics, Law, and\nTechnology_ 2 (1): 1–7.\n\nBostrom, Nick. 2009a. “Moral Uncertainty – Towards a Solution?” _Overcoming\nBias_ (blog), January 1.\n\nBostrom, Nick. 2009b. “Pascal’s Mugging.” _Analysis_ 69 (3): 443–5.\n\nBostrom, Nick. 2009c. “The Future of Humanity.” In _New Waves in Philosophy of\nTechnology_ , edited by Jan Kyrre Berg Olsen, Evan Selinger, and Søren Riis,\n186–215. New York: Palgrave Macmillan.\n\nBostrom, Nick. 2011a. “Information Hazards: A Typology of Potential Harms from\nKnowledge.” _Review of Contemporary Philosophy_ 10: 44–79.\n\nBostrom, Nick. 2011b. “Infinite Ethics.” _Analysis and Metaphysics_ 10: 9–59.\n\nBostrom, Nick. 2012. “The Superintelligent Will: Motivation and Instrumental\nRationality in Advanced Artificial Agents.” In “Theory and Philosophy of AI,”\nedited by Vincent C. Müller, special issue, _Minds and Machines_ 22 (2):\n71–85.\n\nBostrom, Nick, and Ćirković, Milan M. 2003. “The Doomsday Argument and the\nSelf-Indication Assumption: Reply to Olum.” _Philosophical Quarterly_ 53\n(210): 83–91.\n\nBostrom, Nick, and Ord, Toby. 2006. “The Reversal Test: Eliminating the Status\nQuo Bias in Applied Ethics.” _Ethics_ 116 (4): 656–79.\n\nBostrom, Nick, and Roache, Rebecca. 2011. “Smart Policy: Cognitive Enhancement\nand the Public Interest.” In _Enhancing Human Capacities_ , edited by Julian\nSavulescu, Ruud ter Meulen, and Guy Kahane, 138–49. Malden, MA: Wiley-\nBlackwell.\n\nBostrom, Nick and Sandberg, Anders. 2009a. “Cognitive Enhancement: Methods,\nEthics, Regulatory Challenges.” _Science and Engineering Ethics_ 15 (3):\n311–41.\n\nBostrom, Nick and Sandberg, Anders. 2009b. “The Wisdom of Nature: An\nEvolutionary Heuristic for Human Enhancement.” In _Human Enhancement_ , 1st\ned., edited by Julian Savulescu and Nick Bostrom, 375–416. New York: Oxford\nUniversity Press.\n\nBostrom, Nick, Sandberg, Anders, and Douglas, Tom. 2013. “The Unilateralist’s\nCurse: The Case for a Principle of Conformity.” Working Paper. Retrieved\nFebruary 28, 2013. Available at\n<http://www.nickbostrom.com/papers/unilateralist.pdf>.\n\nBostrom, Nick, and Yudkowsky, Eliezer. Forthcoming. “The Ethics of Artificial\nIntelligence.” In _Cambridge Handbook of Artificial Intelligence_ , edited by\nKeith Frankish and William Ramsey. New York: Cambridge University Press.\n\nBoswell, James. 1917. _Boswell’s Life of Johnson_. New York: Oxford University\nPress.\n\nBouchard, T. J. 2004. “Genetic Influence on Human Psychological Traits: A\nSurvey.” _Current Directions in Psychological Science_ 13 (4): 148–51.\n\nBourget, David, and Chalmers, David. 2009. “The PhilPapers Surveys.” November.\nAvailable at <http://philpapers.org/surveys/>.\n\nBradbury, Robert J. 1999. “Matrioshka Brains.” Archived version. As revised\nAugust 16, 2004. Available at\n<http://web.archive.org/web/20090615040912/http://www.aeiveos.com/~bradbury/MatrioshkaBrains/MatrioshkaBrainsPaper.html>.\n\nBrinton, Crane. 1965. _The Anatomy of Revolution_. Revised ed. New York:\nVintage Books.\n\nBryson, Arthur E., Jr., and Ho, Yu-Chi. 1969. _Applied Optimal Control:\nOptimization, Estimation, and Control_. Waltham, MA: Blaisdell.\n\nBuehler, Martin, Iagnemma, Karl, and Singh, Sanjiv, eds. 2009. _The DARPA\nUrban Challenge: Autonomous Vehicles in City Traffic_. Springer Tracts in\nAdvanced Robotics 56. Berlin: Springer.\n\nBurch-Brown, J. 2014. “Clues for Consequentialists.” _Utilitas_ 26 (1):\n105–19.\n\nBurke, Colin. 2001. “Agnes Meyer Driscoll vs. the Enigma and the Bombe.”\nUnpublished manuscript. Retrieved February 22, 2013. Available at\n<http://userpages.umbc.edu/~burke/driscoll1-2011.pdf>.\n\nCanbäck, S., Samouel, P., and Price, D. 2006. “Do Diseconomies of Scale Impact\nFirm Size and Performance? A Theoretical and Empirical Overview.” _Journal of\nManagerial Economics_ 4 (1): 27–70.\n\nCarmena, J. M., Lebedev, M. A., Crist, R. E., O’Doherty, J. E., Santucci, D.\nM., Dimitrov, D. F., Patil, P. G., Henriquez, C. S., and Nicolelis, M. A.\n2003. “Learning to Control a Brain–Machine Interface for Reaching and Grasping\nby Primates.” _Public Library of Science Biology_ 1 (2): 193–208.\n\nCarroll, Bradley W., and Ostlie, Dale A. 2007. An Introduction to Modern\nAstrophysics. 2nd ed. San Francisco: Pearson Addison Wesley.\n\nCarroll, John B. 1993. _Human Cognitive Abilities: A Survey of Factor-Analytic\nStudies_. New York: Cambridge University Press.\n\nCarter, Brandon. 1983. “The Anthropic Principle and its Implications for\nBiological Evolution.” _Philosophical Transactions of the Royal Society A:\nMathematical, Physical and Engineering Sciences_ 310 (1512): 347–63.\n\nCarter, Brandon. 1993. “The Anthropic Selection Principle and the Ultra-\nDarwinian Synthesis.” In _The Anthropic Principle: Proceedings of the Second\nVenice Conference on Cosmology and Philosophy_ , edited by F. Bertola and U.\nCuri, 33–66. Cambridge: Cambridge University Press.\n\nCFTC & SEC (Commodity Futures Trading Commission and Securities & Exchange\nCommission). 2010. _Findings Regarding the Market Events of May 6, 2010:\nReport of the Staffs of the CFTC and SEC to the Joint Advisory Committee on\nEmerging Regulatory Issues_. Washington, DC.\n\nChalmers, David John. 2010. “The Singularity: A Philosophical Analysis.”\n_Journal of Consciousness Studies_ 17 (9–10): 7–65.\n\nChason, R. J., Csokmay, J., Segars, J. H., DeCherney, A. H., and Armant, D. R.\n2011. “Environmental and Epigenetic Effects Upon Preimplantation Embryo\nMetabolism and Development.” _Trends in Endocrinology and Metabolism_ 22 (10):\n412–20.\n\nChen, S., and Ravallion, M. 2010. “The Developing World Is Poorer Than We\nThought, But No Less Successful in the Fight Against Poverty.” _Quarterly\nJournal of Economics_ 125 (4): 1577–1625.\n\nChislenko, Alexander. 1996. “Networking in the Mind Age: Some Thoughts on\nEvolution of Robotics and Distributed Systems.” Unpublished manuscript.\n\nChislenko, Alexander. 1997. “Technology as Extension of Human Functional\nArchitecture.” _Extropy Online_.\n\nChorost, Michael. 2005. _Rebuilt: How Becoming Part Computer Made Me More\nHuman_. Boston: Houghton Mifflin.\n\nChristiano, Paul F. 2012. “‘Indirect Normativity’ Write-up.” _Ordinary Ideas_\n(blog), April 21.\n\nCIA. 2013. “The World Factbook.” Central Intelligence Agency. Retrieved August\n3. Available at [https://www.cia.gov/library/publications/the-world-\nfactbook/rankorder/2127rank.html?countryname=United%20States&countrycode=us&regionCode=noa&rank=121#us](https://www.cia.gov/library/publications/the-\nworld-\nfactbook/rankorder/2127rank.html?countryname=United%20States&countrycode=us&regionCode=noa&rank=121#us).\n\nCicero. 1923. “On Divination.” In _On Old Age, on Friendship, on Divination_ ,\ntranslated by W. A. Falconer. Loeb Classical Library. Cambridge, MA: Harvard\nUniversity Press.\n\nCirasella, Jill, and Kopec, Danny. 2006. “The History of Computer Games.”\nExhibit at Dartmouth Artificial Intelligence Conference: The Next Fifty Years\n(AI@50), Dartmouth College, July 13–15.\n\nĆirković, Milan M. 2004. “Forecast for the Next Eon: Applied Cosmology and the\nLong-Term Fate of Intelligent Beings.” _Foundations of Physics_ 34 (2):\n239–61.\n\nĆirković, Milan M., Sandberg, Anders, and Bostrom, Nick. 2010. “Anthropic\nShadow: Observation Selection Effects and Human Extinction Risks.” _Risk\nAnalysis_ 30 (10): 1495–1506.\n\nClark, Andy, and Chalmers, David J. 1998. “The Extended Mind.” _Analysis_ 58\n(1): 7–19.\n\nClark, Gregory. 2007. _A Farewell to Alms: A Brief Economic History of the\nWorld_. 1st ed. Princeton, NJ: Princeton University Press.\n\nClavin, Whitney. 2012. “Study Shows Our Galaxy Has at Least 100 Billion\nPlanets.” _Jet Propulsion Laboratory_ , January 11.\n\nCME Group. 2010. _What Happened on May 6th?_ Chicago, May 10.\n\nCoase, R. H. 1937. “The Nature of the Firm.” _Economica_ 4 (16): 386–405.\n\nCochran, Gregory, and Harpending, Henry. 2009. _The 10,000 Year Explosion: How\nCivilization Accelerated Human Evolution_. New York: Basic Books.\n\nCochran, G., Hardy, J., and Harpending, H. 2006. “Natural History of Ashkenazi\nIntelligence.” _Journal of Biosocial Science_ 38 (5): 659–93.\n\nCook, James Gordon. 1984. _Handbook of Textile Fibres: Natural Fibres_.\nCambridge: Woodhead.\n\nCope, David. 1996. _Experiments in Musical Intelligence_. Computer Music and\nDigital Audio Series. Madison, WI: A-R Editions.\n\nCotman, Carl W., and Berchtold, Nicole C. 2002. “Exercise: A Behavioral\nIntervention to Enhance Brain Health and Plasticity.” _Trends in\nNeurosciences_ 25 (6): 295–301.\n\nCowan, Nelson. 2001. “The Magical Number 4 in Short-Term Memory: A\nReconsideration of Mental Storage Capacity.” _Behavioral and Brain Sciences_\n24 (1): 87–114.\n\nCrabtree, Steve. 1999. “New Poll Gauges Americans’ General Knowledge Levels.”\n_Gallup News_ , July 6.\n\nCross, Stephen E., and Walker, Edward. 1994. “Dart: Applying Knowledge Based\nPlanning and Scheduling to Crisis Action Planning.” In _Intelligent\nScheduling_ , edited by Monte Zweben and Mark Fox, 711–29. San Francisco, CA:\nMorgan Kaufmann.\n\nCrow, James F. 2000. “The Origins, Patterns and Implications of Human\nSpontaneous Mutation.” _Nature Reviews Genetics_ 1 (1): 40–7.\n\nCyranoski, David. 2013. “Stem Cells: Egg Engineers.” _Nature_ 500 (7463):\n392–4.\n\nDagnelie, Gislin. 2012. “Retinal Implants: Emergence of a Multidisciplinary\nField.” _Current Opinion in Neurology_ 25 (1): 67–75.\n\nDai, Wei. 2009. “Towards a New Decision Theory.” _Less Wrong_ (blog), August\n13.\n\nDalrymple, David. 2011. “Comment on Kaufman, J. ‘Whole Brain Emulation:\nLooking at Progress on _C. Elegans_.’” _Less Wrong_ (blog), October 29.\n\nDavies, G., Tenesa, A., Payton, A., Yang, J., Harris, S. E., Liewald, D., Ke,\nX., et al. 2011. “Genome-Wide Association Studies Establish That Human\nIntelligence Is Highly Heritable and Polygenic.” _Molecular Psychiatry_ 16\n(10): 996–1005.\n\nDavis, Oliver S. P., Butcher, Lee M., Docherty, Sophia J., Meaburn, Emma L.,\nCurtis, Charles J. C., Simpson, Michael A., Schalkwyk, Leonard C., and Plomin,\nRobert. 2010. “A Three-Stage Genome-Wide Association Study of General\nCognitive Ability: Hunting the Small Effects.” _Behavior Genetics_ 40 (6):\n759–767.\n\nDawkins, Richard. 1995. _River Out of Eden: A Darwinian View of Life_. Science\nMasters Series. New York: Basic Books.\n\nDe Blanc, Peter. 2011. _Ontological Crises in Artificial Agents’ Value\nSystems_. Machine Intelligence Research Institute, San Francisco, CA, May 19.\n\nDe Long, J. Bradford. 1998. “Estimates of World GDP, One Million\nB.C.–Present.” Unpublished manuscript.\n\nDe Raedt, Luc, and Flach, Peter, eds. 2001. _Machine Learning: ECML 2001: 12th\nEuropean Conference on Machine Learning, Freiburg, Germany, September 5–7,\n2001_. _Proceedings_. Lecture Notes in Computer Science 2167. New York:\nSpringer.\n\nDean, Cornelia. 2005. “Scientific Savvy? In U.S., Not Much.” _New York Times_\n, August 30.\n\nDeary, Ian J. 2001. “Human Intelligence Differences: A Recent History.”\n_Trends in Cognitive Sciences_ 5 (3): 127–30.\n\nDeary, Ian J. 2012. “Intelligence.” _Annual Review of Psychology_ 63: 453–82.\n\nDeary, Ian J., Penke, L., and Johnson, W. 2010. “The Neuroscience of Human\nIntelligence Differences.” _Nature Reviews Neuroscience_ 11 (3): 201–11.\n\nDegnan, G. G., Wind, T. C., Jones, E. V., and Edlich, R. F. 2002. “Functional\nElectrical Stimulation in Tetraplegic Patients to Restore Hand Function.”\n_Journal of Long-Term Effects of Medical Implants_ 12 (3): 175–88.\n\nDevlin, B., Daniels, M., and Roeder, K. 1997. “The Heritability of IQ.”\n_Nature_ 388 (6641): 468–71.\n\nDewey, Daniel. 2011. “Learning What to Value.” In _Artificial General\nIntelligence: 4th International Conference, AGI 2011, Mountain View, CA, USA,\nAugust 3–6, 2011. Proceedings_ , edited by Jürgen Schmidhuber, Kristinn R.\nThórisson, and Moshe Looks, 309–14. Lecture Notes in Computer Science 6830.\nBerlin: Springer.\n\nDowe, D. L., and Hernández-Orallo, J. 2012. “IQ Tests Are Not for Machines,\nYet.” _Intelligence_ 40 (2): 77–81.\n\nDrescher, Gary L. 2006. _Good and Real: Demystifying Paradoxes from Physics to\nEthics_. Bradford Books. Cambridge, MA: MIT Press.\n\nDrexler, K. Eric. 1986. _Engines of Creation_. Garden City, NY: Anchor.\n\nDrexler, K. Eric. 1992. _Nanosystems: Molecular Machinery, Manufacturing, and\nComputation_. New York: Wiley.\n\nDrexler, K. Eric. 2013. _Radical Abundance: How a Revolution in Nanotechnology\nWill Change Civilization_. New York: PublicAffairs.\n\nDriscoll, Kevin. 2012. “Code Critique: ‘Altair Music of a Sort.’” Paper\npresented at Critical Code Studies Working Group Online Conference, 2012,\nFebruary 6.\n\nDyson, Freeman J. 1960. “Search for Artificial Stellar Sources of Infrared\nRadiation.” _Science_ 131 (3414): 1667–1668.\n\nDyson, Freeman J. 1979. _Disturbing the Universe_. 1st ed. Sloan Foundation\nScience Series. New York: Harper & Row.\n\nElga, Adam. 2004. “Defeating Dr. Evil with Self-Locating Belief.” _Philosophy\nand Phenomenological Research_ 69 (2): 383–96.\n\nElga, Adam. 2007. “Reflection and Disagreement.” _Noûs_ 41 (3): 478–502.\n\nEliasmith, Chris, Stewart, Terrence C., Choo, Xuan, Bekolay, Trevor, DeWolf,\nTravis, Tang, Yichuan, and Rasmussen, Daniel. 2012. “A Large-Scale Model of\nthe Functioning Brain.” _Science_ 338(6111): 1202–5.\n\nEllis, J. H. 1999. “The History of Non-Secret Encryption.” _Cryptologia_ 23\n(3): 267–73.\n\nElyasaf, Achiya, Hauptmann, Ami, and Sipper, Moche. 2011. “Ga-Freecell:\nEvolving Solvers for the Game of Freecell.” In _Proceedings of the 13th Annual\nGenetic and Evolutionary Computation Conference_ , 1931–1938. GECCO’ 11. New\nYork: ACM.\n\nEppig, C., Fincher, C. L., and Thornhill, R. 2010. “Parasite Prevalence and\nthe Worldwide Distribution of Cognitive Ability.” _Proceedings of the Royal\nSociety B: Biological Sciences_ 277 (1701): 3801–8.\n\nEspenshade, T. J., Guzman, J. C., and Westoff, C. F. 2003. “The Surprising\nGlobal Variation in Replacement Fertility.” _Population Research and Policy\nReview_ 22 (5–6): 575–83.\n\nEvans, Thomas G. 1964. “A Heuristic Program to Solve Geometric-Analogy\nProblems.” In _Proceedings of the April 21–23, 1964, Spring Joint Computer\nConference_ , 327–338. AFIPS ’64. New York: ACM.\n\nEvans, Thomas G. 1968. “A Program for the Solution of a Class of Geometric-\nAnalogy Intelligence-Test Questions.” In _Semantic Information Processing_ ,\nedited by Marvin Minsky, 271–353. Cambridge, MA: MIT Press.\n\nFaisal, A. A., Selen, L. P., and Wolpert, D. M. 2008. “Noise in the Nervous\nSystem.” _Nature Reviews Neuroscience_ 9 (4): 292–303.\n\nFaisal, A. A., White, J. A., and Laughlin, S. B. 2005. “Ion-Channel Noise\nPlaces Limits on the Miniaturization of the Brain’s Wiring.” _Current Biology_\n15 (12): 1143–9.\n\nFeldman, Jacob. 2000. “Minimization of Boolean Complexity in Human Concept\nLearning.” _Nature_ 407 (6804): 630–3.\n\nFeldman, J. A., and Ballard, Dana H. 1982. “Connectionist Models and Their\nProperties.” _Cognitive Science_ 6 (3): 205–254.\n\nFoley, J. A., Monfreda, C., Ramankutty, N., and Zaks, D. 2007. “Our Share of\nthe Planetary Pie.” _Proceedings of the National Academy of Sciences of the\nUnited States of America_ 104 (31): 12585–6.\n\nForgas, Joseph P., Cooper, Joel, and Crano, William D., eds. 2010. _The\nPsychology of Attitudes and Attitude Change_. Sydney Symposium of Social\nPsychology. New York: Psychology Press.\n\nFrank, Robert H. 1999. _Luxury Fever: Why Money Fails to Satisfy in an Era of\nExcess_. New York: Free Press.\n\nFredriksen, Kaja Bonesmo. 2012. _Less Income Inequality and More Growth – Are\nThey Compatible?: Part 6. The Distribution of Wealth_. Technical report, OECD\nEconomics Department Working Papers 929. OECD Publishing.\n\nFreitas, Robert A., Jr. 1980. “A Self-Replicating Interstellar Probe.”\n_Journal of the British Interplanetary Society_ 33: 251–64.\n\nFreitas, Robert A., Jr. 2000. “Some Limits to Global Ecophagy by Biovorous\nNanoreplicators, with Public Policy Recommendations.” Foresight Institute.\nApril. Retrieved July 28, 2013. Available at\n<http://www.foresight.org/nano/Ecophagy.html>.\n\nFreitas, Robert A., Jr., and Merkle, Ralph C. 2004. _Kinematic Self-\nReplicating Machines_. Georgetown, TX: Landes Bioscience.\n\nGaddis, John Lewis. 1982. _Strategies of Containment: A Critical Appraisal of\nPostwar American National Security Policy_. New York: Oxford University Press.\n\nGammoned.net. 2012. “Snowie.” Archived version. Retrieved June 30. Available\nat\n<http://web.archive.org/web/20070920191840/http://www.gammoned.com/snowie.html>.\n\nGates, Bill. 1975. “Software Contest Winners Announced.” _Computer Notes_ 1\n(2): 1.\n\nGeorgieff, Michael K. 2007. “Nutrition and the Developing Brain: Nutrient\nPriorities and Measurement.” _American Journal of Clinical Nutrition_ 85 (2):\n614S–620S.\n\nGianaroli, Luca. 2000. “Preimplantation Genetic Diagnosis: Polar Body and\nEmbryo Biopsy.” Supplement, _Human Reproduction_ 15 (4): 69–75.\n\nGilovich, Thomas, Griffin, Dale, and Kahneman, Daniel, eds. 2002. _Heuristics\nand Biases: The Psychology of Intuitive Judgment_. New York: Cambridge\nUniversity Press.\n\nGilster, Paul. 2012. “ESO: Habitable Red Dwarf Planets Abundant.” _Centauri\nDreams_ (blog), March 29.\n\nGoldstone, Jack A. 1980. “Theories of Revolution: The Third Generation.”\n_World Politics_ 32 (3): 425–53.\n\nGoldstone, Jack A. 2001. “Towards a Fourth Generation of Revolutionary\nTheory.” _Annual Review of Political Science_ 4: 139–87.\n\nGood, Irving John. 1965. “Speculations Concerning the First Ultraintelligent\nMachine.” In _Advances in Computers_ , edited by Franz L. Alt and Morris\nRubinoff, 6: 31–88. New York: Academic Press.\n\nGood, Irving John. 1970. “Some Future Social Repercussions of Computers.”\n_International Journal of Environmental Studies_ 1 (1–4): 67–79.\n\nGood, Irving John. 1976. “Book review of ‘The Thinking Computer: Mind Inside\nMatter’” In _International Journal of Man-Machine Studies_ 8: 617–20.\n\nGood, Irving John. 1982. “Ethical Machines.” In _Intelligent Systems: Practice\nand Perspective_ , edited by J. E. Hayes, Donald Michie, and Y.-H. Pao,\n555–60. Machine Intelligence 10. Chichester: Ellis Horwood.\n\nGoodman, Nelson. 1954. _Fact, Fiction, and Forecast_. 1st ed. London: Athlone\nPress.\n\nGott, J. R., Juric, M., Schlegel, D., Hoyle, F., Vogeley, M., Tegmark, M.,\nBahcall, N., and Brinkmann, J. 2005. “A Map of the Universe.” _Astrophysical\nJournal_ 624 (2): 463–83.\n\nGottfredson, Linda S. 2002. “G: Highly General and Highly Practical.” In _The\nGeneral Factor of Intelligence: How General Is It?_ , edited by Robert J.\nSternberg and Elena L. Grigorenko, 331–80. Mahwah, NJ: Lawrence Erlbaum.\n\nGould, S. J. 1990. _Wonderful Life: The Burgess Shale and the Nature of\nHistory_. New York: Norton.\n\nGraham, Gordon. 1997. _The Shape of the Past: A Philosophical Approach to\nHistory_. New York: Oxford University Press.\n\nGray, C. M., and McCormick, D. A. 1996. “Chattering Cells: Superficial\nPyramidal Neurons Contributing to the Generation of Synchronous Oscillations\nin the Visual Cortex.” _Science_ 274 (5284): 109–13.\n\nGreene, Kate. 2012. “Intel’s Tiny Wi-Fi Chip Could Have a Big Impact.” _MIT\nTechnology Review_ , September 21.\n\nGuizzo, Erico. 2010. “World Robot Population Reaches 8.6 Million.” _IEEE\nSpectrum_ , April 14.\n\nGunn, James E. 1982. _Isaac Asimov: The Foundations of Science Fiction_.\nScience-Fiction Writers. New York: Oxford University Press.\n\nHaberl, Helmut, Erb, Karl-Heinz, and Krausmann, Fridolin. 2013. “Global Human\nAppropriation of Net Primary Production (HANPP).” _Encyclopedia of Earth_ ,\nSeptember 3.\n\nHaberl, H., Erb, K. H., Krausmann, F., Gaube, V., Bondeau, A., Plutzar, C.,\nGingrich, S., Lucht, W., and Fischer-Kowalski, M. 2007. “Quantifying and\nMapping the Human Appropriation of Net Primary Production in Earth’s\nTerrestrial Ecosystems.” _Proceedings of the National Academy of Sciences of\nthe United States of America_ 104 (31): 12942–7.\n\nHájek, Alan. 2009. “Dutch Book Arguments.” In Anand, Pattanaik, and Puppe\n2009, 173–95.\n\nHall, John Storrs. 2007. _Beyond AI: Creating the Conscience of the Machine_.\nAmherst, NY: Prometheus Books.\n\nHampson, R. E., Song, D., Chan, R. H., Sweatt, A. J., Riley, M. R., Gerhardt,\nG. A., Shin, D. C., Marmarelis, V. Z., Berger, T. W., and Deadwyler, S. A.\n2012. “A Nonlinear Model for Hippocampal Cognitive Prosthesis: Memory\nFacilitation by Hippocampal Ensemble Stimulation.” _IEEE Transactions on\nNeural Systems and Rehabilitation Engineering_ 20 (2): 184–97.\n\nHanson, Robin. 1994. “If Uploads Come First: The Crack of a Future Dawn.”\n_Extropy_ 6 (2).\n\nHanson, Robin. 1995. “Could Gambling Save Science? Encouraging an Honest\nConsensus.” _Social Epistemology_ 9 (1): 3–33.\n\nHanson, Robin. 1998a. “Burning the Cosmic Commons: Evolutionary Strategies for\nInterstellar Colonization.” Unpublished manuscript, July 1. Retrieved April\n26, 2012. <http://hanson.gmu.edu/filluniv.pdf>.\n\nHanson, Robin. 1998b. “Economic Growth Given Machine Intelligence.”\nUnpublished manuscript. Retrieved May 15, 2013. Available at\n<http://hanson.gmu.edu/aigrow.pdf>.\n\nHanson, Robin. 1998c. “Long-Term Growth as a Sequence of Exponential Modes.”\nUnpublished manuscript. Last revised December 2000. Available at\n<http://hanson.gmu.edu/longgrow.pdf>.\n\nHanson, Robin. 1998d. “Must Early Life Be Easy? The Rhythm of Major\nEvolutionary Transitions.” Unpublished manuscript, September 23. Retrieved\nAugust 12, 2012. Available at <http://hanson.gmu.edu/hardstep.pdf>.\n\nHanson, Robin. 2000. “Shall We Vote on Values, But Bet on Beliefs?”\nUnpublished manuscript, September. Last revised October 2007. Available at\n<http://hanson.gmu.edu/futarchy.pdf>.\n\nHanson, Robin. 2006. “Uncommon Priors Require Origin Disputes.” _Theory and\nDecision_ 61 (4): 319–328.\n\nHanson, Robin. 2008. “Economics of the Singularity.” _IEEE Spectrum_ 45 (6):\n45–50.\n\nHanson, Robin. 2009. “Tiptoe or Dash to Future?” _Overcoming Bias_ (blog),\nDecember 23.\n\nHanson, Robin. 2012. “Envisioning the Economy, and Society, of Whole Brain\nEmulations.” Paper presented at the AGI Impacts conference 2012.\n\nHart, Oliver. 2008. “Economica Coase Lecture Reference Points and the Theory\nof the Firm.” _Economica_ 75 (299): 404–11.\n\nHay, Nicholas James. 2005. “Optimal Agents.” B.Sc. thesis, University of\nAuckland.\n\nHedberg, Sara Reese. 2002. “Dart: Revolutionizing Logistics Planning.” _IEEE\nIntelligent Systems_ 17 (3): 81–3.\n\nHelliwell, John, Layard, Richard, and Sachs, Jeffrey. 2012. _World Happiness\nReport_. The Earth Institute.\n\nHelmstaedter, M., Briggman, K. L., and Denk, W. 2011. “High-Accuracy Neurite\nReconstruction for High-Throughput Neuroanatomy.” _Nature Neuroscience_ 14\n(8): 1081–8.\n\nHeyl, Jeremy S. 2005. “The Long-Term Future of Space Travel.” _Physical Review\nD_ 72 (10): 1–4.\n\nHibbard, Bill. 2011. “Measuring Agent Intelligence via Hierarchies of\nEnvironments.” In _Artificial General Intelligence: 4th International\nConference, AGI 2011, Mountain View, CA, USA, August 3–6, 2011. Proceedings_ ,\nedited by Jürgen Schmidhuber, Kristinn R. Thórisson, and Moshe Looks, 303–8.\nLecture Notes in Computer Science 6830. Berlin: Springer.\n\nHinke, R. M., Hu, X., Stillman, A. E., Herkle, H., Salmi, R., and Ugurbil, K.\n1993. “Functional Magnetic Resonance Imaging of Broca’s Area During Internal\nSpeech.” _Neuroreport_ 4 (6): 675–8.\n\nHinxton Group. 2008. _Consensus Statement: Science, Ethics and Policy\nChallenges of Pluripotent Stem Cell-Derived Gametes_. Hinxton, Cambridgeshire,\nUK, April 11. Available at\n<http://www.hinxtongroup.org/Consensus_HG08_FINAL.pdf>.\n\nHoffman, David E. 2009. _The Dead Hand: The Untold Story of the Cold War Arms\nRace and Its Dangerous Legacy_. New York: Doubleday.\n\nHofstadter, Douglas R. (1979) 1999. _Goödel, Escher, Bach: An Eternal Golden\nBraid_. New York: Basic Books.\n\nHolley, Rose. 2009. “How Good Can It Get? Analysing and Improving OCR Accuracy\nin Large Scale Historic Newspaper Digitisation Programs.” _D-Lib Magazine_ 15\n(3–4).\n\nHorton, Sue, Alderman, Harold, and Rivera, Juan A. 2008. _Copenhagen Consensus\n2008 Challenge Paper: Hunger and Malnutrition_. Technical report. Copenhagen\nConsensus Center, May 11.\n\nHowson, Colin, and Urbach, Peter. 1993. _Scientific Reasoning: The Bayesian\nApproach_. 2nd ed. Chicago: Open Court.\n\nHsu, Stephen. 2012. “Investigating the Genetic Basis for Intelligence and\nOther Quantitative Traits.” Lecture given at UC Davis Department of Physics\nColloquium, Davis, CA, February 13.\n\nHuebner, Bryce. 2008. “Do You See What We See? An Investigation of an Argument\nAgainst Collective Representation.” _Philosophical Psychology_ 21 (1): 91–112.\n\nHuff, C. D., Xing, J., Rogers, A. R., Witherspoon, D., and Jorde, L. B. 2010.\n“Mobile Elements Reveal Small Population Size in the Ancient Ancestors of\n_Homo Sapiens_.” _Proceedings of the National Academy of Sciences of the\nUnited States of America_ 107 (5): 2147–52.\n\nHuffman, W. Cary, and Pless, Vera. 2003. _Fundamentals of Error-Correcting\nCodes_. New York: Cambridge University Press.\n\nHunt, Patrick. 2011. “Late Roman Silk: Smuggling and Espionage in the 6th\nCentury CE.” _Philolog, Stanford University_ (blog), August 2.\n\nHutter, Marcus. 2001. “Towards a Universal Theory of Artificial Intelligence\nBased on Algorithmic Probability and Sequential Decisions.” In De Raedt and\nFlach 2001, 226–38.\n\nHutter, Marcus. 2005. _Universal Artificial Intelligencet: Sequential\nDecisions Based On Algorithmic Probability_. Texts in Theoretical Computer\nScience. Berlin: Springer.\n\nIliadou, A. N., Janson, P. C., and Cnattingius, S. 2011. “Epigenetics and\nAssisted Reproductive Technology.” _Journal of Internal Medicine_ 270 (5):\n414–20.\n\nIsaksson, Anders. 2007. _Productivity and Aggregate Growth: A Global Picture_.\nTechnical report 05/2007. Vienna, Austria: UNIDO (United Nations Industrial\nDevelopment Organization) Research and Statistics Branch.\n\nJones, Garret. 2009. “Artificial Intelligence and Economic Growth: A Few\nFinger-Exercises.” Unpublished manuscript, January. Retrieved November 5,\n2012. Available at <http://mason.gmu.edu/~gjonesb/AIandGrowth>.\n\nJones, Vincent C. 1985. _Manhattan: The Army and the Atomic Bomb_. United\nStates Army in World War II. Washington, DC: Center of Military History.\n\nJoyce, James M. 1999. _The Foundations of Causal Decision Theory_. Cambridge\nStudies in Probability, Induction and Decision Theory. New York: Cambridge\nUniversity Press.\n\nJudd, K. L., Schmedders, K., and Yeltekin, S. 2012. “Optimal Rules for Patent\nRaces.” _International Economic Review_ 53 (1): 23–52.\n\nKalfoglou, A., Suthers, K., Scott, J., and Hudson, K. 2004. _Reproductive\nGenetic Testing: What America Thinks_. Genetics and Public Policy Center.\n\nKamm, Frances M. 2007. _Intricate Ethics: Rights, Responsibilities, and\nPermissible Harm_. Oxford Ethics Series. New York: Oxford University Press.\n\nKandel, Eric R., Schwartz, James H., and Jessell, Thomas M., eds. 2000.\n_Principles of Neural Science_. 4th ed. New York: McGraw-Hill.\n\nKansa, Eric. 2003. “Social Complexity and Flamboyant Display in Competition:\nMore Thoughts on the Fermi Paradox.” Unpublished manuscript, archived version.\n\nKarnofsky, Holden. 2012. “Comment on ‘Reply to Holden on Tool AI.’” _Less\nWrong_ (blog), August 1.\n\nKasparov, Garry. 1996. “The Day That I Sensed a New Kind of Intelligence.”\n_Time_ , March 25, no. 13.\n\nKaufman, Jeff. 2011. “Whole Brain Emulation and Nematodes.” _Jeff Kaufman’s\nBlog_ (blog), November 2.\n\nKeim, G. A., Shazeer, N. M., Littman, M. L., Agarwal, S., Cheves, C. M.,\nFitzgerald, J., Grosland, J., Jiang, F., Pollard, S., and Weinmeister, K.\n1999. “Proverb: The Probabilistic Cruciverbalist.” In _Proceedings of the\nSixteenth National Conference on Artificial Intelligence_ , 710–17. Menlo\nPark, CA: AAAI Press.\n\nKell, Harrison J., Lubinski, David, and Benbow, Camilla P. 2013. “Who Rises to\nthe Top? Early Indicators.” _Psychological Science_ 24 (5): 648–59.\n\nKeller, Wolfgang. 2004. “International Technology Diffusion.” _Journal of\nEconomic Literature_ 42 (3): 752–82.\n\nKGS Go Server. 2012. “KGS Game Archives: Games of KGS player zen19.” Retrieved\nJuly 22, 2013. Available at\n[http://www.gokgs.com/gameArchives.jsp?user=zen19d&oldAccounts=t&year=2012&month=3](http://www.gokgs.com/gameArchives.jsp?user=zen19d&oldAccounts=t&year=2012&month=3).\n\nKnill, Emanuel, Laflamme, Raymond, and Viola, Lorenza. 2000. “Theory of\nQuantum Error Correction for General Noise.” _Physical Review Letters_ 84\n(11): 2525–8.\n\nKoch, K., McLean, J., Segev, R., Freed, M. A., Berry, M. J., Balasubramanian,\nV., and Sterling, P. 2006. “How _Much_ the Eye Tells the Brain.” _Current\nBiology_ 16 (14): 1428–34.\n\nKong, A., Frigge, M. L., Masson, G., Besenbacher, S., Sulem, P., Magnusson,\nG., Gudjonsson, S. A., Sigurdsson, A., et al. 2012. “Rate of De Novo Mutations\nand the Importance of Father’s Age to Disease Risk.” _Nature_ 488: 471–5.\n\nKoomey, Jonathan G. 2011. _Growth in Data Center Electricity Use 2005 to\n2010_. Technical report, 08/01/2011. Oakland, CA: Analytics Press.\n\nKoubi, Vally. 1999. “Military Technology Races.” _International Organization_\n53 (3): 537–65.\n\nKoubi, Vally, and Lalman, David. 2007. “Distribution of Power and Military\nR&D.” _Journal of Theoretical Politics_ 19 (2): 133–52.\n\nKoza, J. R., Keane, M. A., Streeter, M. J., Mydlowec, W., Yu, J., and Lanza,\nG. 2003. _Genetic Programming IV: Routine Human-Competitive Machine\nIntelligence_. 2nd ed. Genetic Programming. Norwell, MA: Kluwer Academic.\n\nKremer, Michael. 1993. “Population Growth and Technological Change: One\nMillion B.C. to 1990.” _Quarterly Journal of Economics_ 108 (3): 681–716.\n\nKruel, Alexander. 2011. “Interview Series on Risks from AI.” _Less Wrong Wiki_\n(blog). Retrieved Oct 26, 2013. Available at\n<http://wiki.lesswrong.com/wiki/Interview_series_on_risks_from_AI>.\n\nKruel, Alexander. 2012. “Q&A with Experts on Risks From AI #2.” _Less Wrong_\n(blog), January 9.\n\nKrusienski, D. J., and Shih, J. J. 2011. “Control of a Visual Keyboard Using\nan Electrocorticographic Brain–Computer Interface.” _Neurorehabilitation and\nNeural Repair_ 25 (4): 323–31.\n\nKuhn, Thomas S. 1962. _The Structure of Scientific Revolutions_. 1st ed.\nChicago: University of Chicago Press.\n\nKuipers, Benjamin. 2012. “An Existing, Ecologically-Successful Genus of\nCollectively Intelligent Artificial Creatures.” Paper presented at the 4th\nInternational Conference, ICCCI 2012, Ho Chi Minh City, Vietnam, November\n28–30.\n\nKurzweil, Ray. 2001. “Response to Stephen Hawking.” Kurzweil Accelerating\nIntelligence. September 5. Retrieved December 31, 2012. Available at\n<http://www.kurzweilai.net/response-to-stephen-hawking>.\n\nKurzweil, Ray. 2005. _The Singularity Is Near: When Humans Transcend Biology_.\nNew York: Viking.\n\nLaffont, Jean-Jacques, and Martimort, David. 2002. _The Theory of Incentives:\nThe Principal-Agent Model_. Princeton, NJ: Princeton University Press.\n\n_Lancet, The_. 2008. “Iodine Deficiency—Way to Go Yet.” _The Lancet_ 372\n(9633): 88.\n\nLandauer, Thomas K. 1986. “How Much Do People Remember? Some Estimates of the\nQuantity of Learned Information in Long-Term Memory.” _Cognitive Science_ 10\n(4): 477–93.\n\nLebedev, Anastasiya. 2004. “The Man Who Saved the World Finally Recognized.”\n_MosNews_ , May 21.\n\nLebedev, M. A., and Nicolelis, M. A. 2006. “Brain–Machine Interfaces: Past,\nPresent and Future.” _Trends in Neuroscience_ 29 (9): 536–46.\n\nLegg, Shane. 2008. “Machine Super Intelligence.” PhD diss., University of\nLugano.\n\nLeigh, E. G., Jr. 2010. “The Group Selection Controversy.” _Journal of\nEvolutionary Biology_ 23(1): 6–19.\n\nLenat, Douglas B. 1982. “Learning Program Helps Win National Fleet Wargame\nTournament.” _SIGART Newsletter_ 79: 16–17.\n\nLenat, Douglas B. 1983. “EURISKO: A Program that Learns New Heuristics and\nDomain Concepts.” _Artificial Intelligence_ 21 (1–2): 61–98.\n\nLenman, James. 2000. “Consequentialism and Cluelessness.” _Philosophy & Public\nAffairs_ 29 (4): 342–70.\n\nLerner, Josh. 1997. “An Empirical Exploration of a Technology Race.” _RAND\nJournal of Economics_ 28 (2): 228–47.\n\nLeslie, John. 1996. _The End of the World: The Science and Ethics of Human\nExtinction_. London: Routledge.\n\nLewis, David. 1988. “Desire as Belief.” _Mind: A Quarterly Review of\nPhilosophy_ 97 (387): 323–32.\n\nLi, Ming, and Vitaányi, Paul M. B. 2008. _An Introduction to Kolmogorov\nComplexity and Its Applications_. Texts in Computer Science. New York:\nSpringer.\n\nLin, Thomas, Mausam, and Etzioni, Oren. 2012. “Entity Linking at Web Scale.”\nIn _Proceedings of the Joint Workshop on Automatic Knowledge Base Construction\nand Web-scale Knowledge Extraction(AKBC-WEKEX ’12)_, edited by James Fan,\nRaphael Hoffman, Aditya Kalyanpur, Sebastian Riedel, Fabian Suchanek, and\nPartha Pratim Talukdar, 84–88. Madison, WI: Omnipress.\n\nLloyd, Seth. 2000. “Ultimate Physical Limits to Computation.” _Nature_ 406\n(6799): 1047–54.\n\nLouis Harris & Associates. 1969. “Science, Sex, and Morality Survey, study no.\n1927.” _Life Magazine_ (New York) 4.\n\nLynch, Michael. 2010. “Rate, Molecular Spectrum, and Consequences of Human\nMutation.” _Proceedings of the National Academy of Sciences of the United\nStates of America_ 107 (3): 961–8.\n\nLyons, Mark K. 2011. “Deep Brain Stimulation: Current and Future Clinical\nApplications.” _Mayo Clinic Proceedings_ 86 (7): 662–72.\n\nMacAskill, William. 2010. “Moral Uncertainty and Intertheoretic Comparisons of\nValue.” BPhil thesis, University of Oxford.\n\nMcCarthy, John. 2007. “From Here to Human-Level AI.” _Artificial Intelligence_\n171 (18): 1174–82.\n\nMcCorduck, Pamela. 1979. _Machines Who Think: A Personal Inquiry into the\nHistory and Prospects of Artificial Intelligence_. San Francisco: W. H.\nFreeman.\n\nMack, C. A. 2011. “Fifty Years of Moore’s Law.” _IEEE Transactions on\nSemiconductor Manufacturing_ 24 (2): 202–7.\n\nMacKay, David J. C. 2003. _Information Theory, Inference, and Learning\nAlgorithms_. New York: Cambridge University Press.\n\nMcLean, George, and Stewart, Brian. 1979. “Norad False Alarm Causes Uproar.”\n_The National_. Aired November 10. Ottawa, ON: CBC, 2012. News Broadcast.\n\nMaddison, Angus. 1999. “Economic Progress: The Last Half Century in Historical\nPerspective.” In _Facts and Fancies of Human Development: Annual Symposium and\nCunningham Lecture, 1999_ , edited by Ian Castles. Occasional Paper Series,\n1/2000. Academy of the Social Sciences in Australia.\n\nMaddison, Angus. 2001. _The World Economy: A Millennial Perspective_.\nDevelopment Centre Studies. Paris: Development Centre of the Organisation for\nEconomic Co-operation / Development.\n\nMaddison, Angus. 2005. _Growth and Interaction in the World Economy: The Roots\nof Modernity_. Washington, DC: AEI Press.\n\nMaddison, Angus. 2007. _Contours of the World Economy, 1–2030 AD: Essays in\nMacro-Economic History_. New York: Oxford University Press.\n\nMaddison, Angus. 2010. “Statistics of World Population, GDP and Per Capita GDP\n1–2008 AD.” Retrieved October 26, 2013. Available at\n<http://www.ggdc.net/maddison/Historical_Statistics/vertical-\nfile_02-2010.xls>.\n\nMai, Q., Yu, Y., Li, T., Wang, L., Chen, M. J., Huang, S. Z., Zhou, C., and\nZhou, Q. 2007. “Derivation of Human Embryonic Stem Cell Lines from\nParthenogenetic Blastocysts.” _Cell Research_ 17 (12): 1008–19.\n\nMak, J. N., and Wolpaw, J. R. 2009. “Clinical Applications of Brain-Computer\nInterfaces: Current State and Future Prospects.” _IEEE Reviews in Biomedical\nEngineering_ 2: 187–99.\n\nMankiw, N. Gregory. 2009. _Macroeconomics_. 7th ed. New York, NY: Worth.\n\nMardis, Elaine R. 2011. “A Decade’s Perspective on DNA Sequencing Technology.”\n_Nature_ 470 (7333): 198–203.\n\nMarkoff, John. 2011. “Computer Wins on ‘Jeopardy!’: Trivial, It’s Not.” _New\nYork Times_ , February 16.\n\nMarkram, Henry. 2006. “The Blue Brain Project.” _Nature Reviews Neuroscience_\n7 (2): 153–160.\n\nMason, Heather. 2003. “Gallup Brain: The Birth of In Vitro Fertilization.”\n_Gallup_ , August 5.\n\nMenzel, Randolf, and Giurfa, Martin. 2001. “Cognitive Architecture of a Mini-\nBrain: The Honeybee.” _Trends in Cognitive Sciences_ 5 (2): 62–71.\n\nMetzinger, Thomas. 2003. _Being No One: The Self-Model Theory of\nSubjectivity_. Cambridge, MA: MIT Press.\n\nMijic, Roko. 2010. “Bootstrapping Safe AGI Goal Systems.” Paper presented at\nthe Roadmaps to AGI and the Future of AGI Workshop, Lugano, Switzerland, March\n8.\n\nMike, Mike. 2013. “Face of Tomorrow.” Retrieved June 30, 2012. Available at\n<http://faceoftomorrow.org>.\n\nMilgrom, Paul, and Roberts, John. 1990. “Bargaining Costs, Influence Costs,\nand the Organization of Economic Activity.” In _Perspectives on Positive\nPolitical Economy_ , edited by James E. Alt and Kenneth A. Shepsle, 57–89. New\nYork: Cambridge University Press.\n\nMiller, George A. 1956. “The Magical Number Seven, Plus or Minus Two: Some\nLimits on Our Capacity for Processing Information.” _Psychological Review_ 63\n(2): 81–97.\n\nMiller, Geoffrey. 2000. _The Mating Mind: How Sexual Choice Shaped the\nEvolution of Human Nature_. New York: Doubleday.\n\nMiller, James D. 2012. _Singularity Rising: Surviving and Thriving in a\nSmarter, Richer, and More Dangerous World_. Dallas, TX: BenBella Books.\n\nMinsky, Marvin. 1967. _Computation: Finite and Infinite Machines_. Englewood\nCliffs, NJ: Prentice-Hall.\n\nMinsky, Marvin, ed. 1968. _Semantic Information Processing_. Cambridge, MA:\nMIT Press.\n\nMinsky, Marvin. 1984. “Afterword to Vernor Vinge’s novel, ‘True Names.’”\nUnpublished manuscript, October 1. Retrieved December 31, 2012. Available at\n<http://web.media.mit.edu/~minsky/papers/TrueNames.Afterword.html>.\n\nMinsky, Marvin. 2006. _The Emotion Machine: Commonsense Thinking, Artificial\nIntelligence, and the Future of the Human Mind_. New York: Simon & Schuster.\n\nMinsky, Marvin, and Papert, Seymour. 1969. _Perceptrons: An Introduction to\nComputational Geometry_. 1st ed. Cambridge, MA: MIT Press.\n\nMoore, Andrew. 2011. “Hedonism.” In _The Stanford Encyclopedia of Philosophy_\n, Winter 2011, edited by Edward N. Zalta. Stanford, CA: Stanford University.\n\nMoravec, Hans P. 1976. “The Role of Raw Power in Intelligence.” Unpublished\nmanuscript, May 12. Retrieved August 12, 2012. Available at\n<http://www.frc.ri.cmu.edu/users/hpm/project.archive/general.articles/l975/Raw.Power.html>.\n\nMoravec, Hans P. 1980. “Obstacle Avoidance and Navigation in the Real World by\na Seeing Robot Rover.” PhD diss., Stanford University.\n\nMoravec, Hans P. 1988. _Mind Children: The Future of Robot and Human\nIntelligence_. Cambridge, MA: Harvard University Press.\n\nMoravec, Hans P. 1998. “When Will Computer Hardware Match the Human Brain?”\n_Journal of Evolution and Technology_ 1.\n\nMoravec, Hans P. 1999. “Rise of the Robots.” _Scientific American_ , December,\n124–35.\n\nMuehlhauser, Luke, and Helm, Louie. 2012. “The Singularity and Machine\nEthics.” In _Singularity Hypotheses: A Scientific and Philosophical\nAssessment_ , edited by Amnon Eden, Johnny Søraker, James H. Moor, and Eric\nSteinhart. The Frontiers Collection. Berlin: Springer.\n\nMuehlhauser, Luke, and Salamon, Anna. 2012. “Intelligence Explosion: Evidence\nand Import.” In _Singularity Hypotheses: A Scientific and Philosophical\nAssessment_ , edited by Amnon Eden, Johnny Søraker, James H. Moor, and Eric\nSteinhart. The Frontiers Collection. Berlin: Springer.\n\nMüller, Vincent C., and Bostrom, Nick. Forthcoming. “Future Progress in\nArtificial Intelligence: A Poll Among Experts.” In “Impacts and Risks of\nArtificial General Intelligence,” edited by Vincent C. Müller, special issue,\n_Journal of Experimental and Theoretical Artificial Intelligence_.\n\nMurphy, Kevin P. 2012. _Machine Learning: A Probabilistic Perspective_.\nAdaptive Computation and Machine Learning. Cambridge, MA: MIT Press.\n\nNachman, Michael W., and Crowell, Susan L. 2000. “Estimate of the Mutation\nRate per Nucleotide in Humans.” _Genetics_ 156 (1): 297–304.\n\nNagy, Z. P., and Chang, C. C. 2007. “Artificial Gametes.” _Theriogenology_ 67\n(1): 99–104.\n\nNagy, Z. P., Kerkis, I., and Chang, C. C. 2008. “Development of Artificial\nGametes.” _Reproductive BioMedicine Online_ 16 (4): 539–44.\n\nNASA. 2013. “International Space Station: Facts and Figures.” Available at\n<http://www.nasa.gov/worldbook/intspacestation_worldbook.html>.\n\nNewborn, Monty. 2011. _Beyond Deep Blue: Chess in the Stratosphere_. New York:\nSpringer.\n\nNewell, Allen, Shaw, J. C., and Simon, Herbert A. 1958. “Chess-Playing\nPrograms and the Problem of Complexity.” _IBM Journal of Research and\nDevelopment_ 2 (4): 320–35.\n\nNewell, Allen, Shaw, J. C., and Simon, Herbert A. 1959. “Report on a General\nProblem-Solving Program: Proceedings of the International Conference on\nInformation Processing.” In _Information Processing_ , 256–64. Paris: UNESCO.\n\nNicolelis, Miguel A. L., and Lebedev, Mikhail A. 2009. “Principles of Neural\nEnsemble Physiology Underlying the Operation of Brain–Machine Interfaces.”\n_Nature Reviews Neuroscience_ 10 (7): 530–40.\n\nNilsson, Nils J. 1984. _Shakey the Robot_ , Technical Note 323. Menlo Park,\nCA: AI Center, SRI International, April.\n\nNilsson, Nils J. 2009. _The Quest for Artificial Intelligence: A History of\nIdeas and Achievements_. New York: Cambridge University Press.\n\nNisbett, R. E., Aronson, J., Blair, C., Dickens, W., Flynn, J., Halpern, D.\nF., and Turkheimer, E. 2012. “Intelligence: New Findings and Theoretical\nDevelopments.” _American Psychologist_ 67 (2): 130–59.\n\nNiven, Larry. 1973. “The Defenseless Dead.” In _Ten Tomorrows_ , edited by\nRoger Elwood, 91–142. New York: Fawcett.\n\nNordhaus, William D. 2007. “Two Centuries of Productivity Growth in\nComputing.” _Journal of Economic History_ 67 (1): 128–59.\n\nNorton, John D. 2011. “Waiting for Landauer.” _Studies in History and\nPhilosophy of Science Part B: Studies in History and Philosophy of Modern\nPhysics_ 42 (3): 184–98.\n\nOlds, James, and Milner, Peter. 1954. “Positive Reinforcement Produced by\nElectrical Stimulation of Septal Area and Other Regions of Rat Brain.”\n_Journal of Comparative and Physiological Psychology_ 47 (6): 419–27.\n\nOlum, Ken D. 2002. “The Doomsday Argument and the Number of Possible\nObservers.” _Philosophical Quarterly_ 52 (207): 164–84.\n\nOmohundro, Stephen M. 2007. “The Nature of Self-Improving Artificial\nIntelligence.” Paper presented at Singularity Summit 2007, San Francisco, CA,\nSeptember 8–9.\n\nOmohundro, Stephen M. 2008. “The Basic AI Drives.” In _Artificial General\nIntelligence 2008: Proceedings of the First AGI Conference_ , edited by Pei\nWang, Ben Goertzel, and Stan Franklin, 483–92. Frontiers in Artificial\nIntelligence and Applications 171. Amsterdam: IOS.\n\nOmohundro, Stephen M. 2012. “Rational Artificial Intelligence for the Greater\nGood.” In _Singularity Hypotheses: A Scientific and Philosophical Assessment_\n, edited by Amnon Eden, Johnny Søraker, James H. Moor, and Eric Steinhart. The\nFrontiers Collection. Berlin: Springer.\n\nO’Neill, Gerard K. 1974. “The Colonization of Space.” _Physics Today_ 27 (9):\n32–40.\n\nOshima, Hideki, and Katayama, Yoichi. 2010. “Neuroethics of Deep Brain\nStimulation for Mental Disorders: Brain Stimulation Reward in Humans.”\n_Neurologia medico-chirurgica_ 50 (9): 845–52.\n\nParfit, Derek. 1986. _Reasons and Persons_. New York: Oxford University Press.\n\nParfit, Derek. 2011. _On What Matters_. 2 vols. The Berkeley Tanner Lectures.\nNew York: Oxford University Press.\n\nParrington, Alan J. 1997. “Mutually Assured Destruction Revisited.” _Airpower\nJournal_ 11 (4).\n\nPasqualotto, Emanuele, Federici, Stefano, and Belardinelli, Marta Olivetti.\n2012. “Toward Functioning and Usable Brain–Computer Interfaces (BCIs): A\nLiterature Review.” _Disability and Rehabilitation: Assistive Technology_ 7\n(2): 89–103.\n\nPearl, Judea. 2009. _Causality: Models, Reasoning, and Inference_. 2nd ed. New\nYork: Cambridge University Press.\n\nPerlmutter, J. S., and Mink, J. W. 2006. “Deep Brain Stimulation.” _Annual\nReview of Neuroscience_ 29: 229–57.\n\nPinker, Steven. 2011. _The Better Angels of Our Nature: Why Violence Has\nDeclined_. New York: Viking.\n\nPlomin, R., Haworth, C. M., Meaburn, E. L., Price, T. S., Wellcome Trust Case\nControl Consortium 2, and Davis, O. S. 2013. “Common DNA Markers Can Account\nfor More than Half of the Genetic Influence on Cognitive Abilities.”\n_Psychological Science_ 24 (2): 562–8.\n\nPopper, Nathaniel. 2012. “Flood of Errant Trades Is a Black Eye for Wall\nStreet.” _New York Times_ , August 1.\n\nPourret, Olivier, Naim, Patrick, and Marcot, Bruce, eds. 2008. _Bayesian\nNetworks: A Practical Guide to Applications_. Chichester, West Sussex, UK:\nWiley.\n\nPowell, A., Shennan, S., and Thomas, M. G. 2009. “Late Pleistocene Demography\nand the Appearance of Modern Human Behavior.” _Science_ 324 (5932): 1298–1301.\n\nPrice, Huw. 1991. “Agency and Probabilistic Causality.” _British Journal for\nthe Philosophy of Science_ 42 (2): 157–76.\n\nQian, M., Wang, D., Watkins, W. E., Gebski, V., Yan, Y. Q., Li, M., and Chen,\nZ. P. 2005. “The Effects of Iodine on Intelligence in Children: A Meta-\nAnalysis of Studies Conducted in China.” _Asia Pacific Journal of Clinical\nNutrition_ 14 (1): 32–42.\n\nQuine, Willard Van Orman, and Ullian, Joseph Silbert. 1978. _The Web of\nBelief_ , ed. Richard Malin Ohmann, vol. 2. New York: Random House.\n\nRailton, Peter. 1986. “Facts and Values.” _Philosophical Topics_ 14 (2): 5–31.\n\nRajab, Moheeb Abu, Zarfoss, Jay, Monrose, Fabian, and Terzis, Andreas. 2006.\n“A Multifaceted Approach to Understanding the Botnet Phenomenon.” In\n_Proceedings of the 6th ACM SIGCOMM Conference on Internet Measurement_ ,\n41–52. New York: ACM.\n\nRawls, John. 1971. _A Theory of Justice_. Cambridge, MA: Belknap.\n\nRead, J. I., and Trentham, Neil. 2005. “The Baryonic Mass Function of\nGalaxies.” _Philosophical Transactions of the Royal Society A: Mathematical,\nPhysical and Engineering Sciences_ 363 (1837): 2693–710.\n\nRepantis, D., Schlattmann, P., Laisney, O., and Heuser, I. 2010. “Modafinil\nand Methylphenidate for Neuroenhancement in Healthy Individuals: A Systematic\nReview.” _Pharmacological Research_ 62 (3): 187–206.\n\nRhodes, Richard. 1986. _The Making of the Atomic Bomb_. New York: Simon &\nSchuster.\n\nRhodes, Richard. 2008. _Arsenals of Folly: The Making of the Nuclear Arms\nRace_. New York: Vintage.\n\nRietveld, Cornelius A., Medland, Sarah E., Derringer, Jaime, Yang, Jian, Esko,\nTonu, Martin, Nicolas W., Westra, Harm-Jan, Shakhbazov, Konstantin,\nAbdellaoui, Abdel, et al. 2013. “GWAS of 126,559 Individuals Identifies\nGenetic Variants Associated with Educational Attainment.” _Science_ 340\n(6139): 1467–71.\n\nRing, Mark, and Orseau, Laurent. 2011. “Delusion, Survival, and Intelligent\nAgents.” In _Artificial General Intelligence: 4th International Conference,\nAGI 2011, Mountain View, CA, USA, August 3–6, 2011_. _Proceedings_ , edited by\nJürgen Schmidhuber, Kristinn R. Thórisson, and Moshe Looks, 11–20. Lecture\nNotes in Computer Science 6830. Berlin: Springer.\n\nRitchie, Graeme, Manurung, Ruli, and Waller, Annalu. 2007. “A Practical\nApplication of Computational Humour.” In _Proceedings of the 4th International\nJoint Workshop on Computational Creativity_ , edited by Amilcar Cardoso and\nGeraint A. Wiggins, 91–8. London: Goldsmiths, University of London.\n\nRoache, Rebecca. 2008. “Ethics, Speculation, and Values.” _NanoEthics_ 2 (3):\n317–27.\n\nRobles, J. A., Lineweaver, C. H., Grether, D., Flynn, C., Egan, C. A., Pracy,\nM. B., Holmberg, J., and Gardner, E. 2008. “A Comprehensive Comparison of the\nSun to Other Stars: Searching for Self-Selection Effects.” _Astrophysical\nJournal_ 684 (1): 691–706.\n\nRoe, Anne. 1953. _The Making of a Scientist_. New York: Dodd, Mead.\n\nRoy, Deb. 2012. “About.” Retrieved October 14. Available at\n<http://web.media.mit.edu/~dkroy/>.\n\nRubin, Jonathan, and Watson, Ian. 2011. “Computer Poker: A Review.”\n_Artificial Intelligence_ 175 (5–6): 958–87.\n\nRumelhart, D. E., Hinton, G. E., and Williams, R. J. 1986. “Learning\nRepresentations by Back-Propagating Errors.” _Nature_ 323 (6088): 533–6.\n\nRussell, Bertrand. 1986. “The Philosophy of Logical Atomism.” In _The\nPhilosophy of Logical Atomism and Other Essays 1914–1919_ , edited by John G.\nSlater, 8: 157–244. The Collected Papers of Bertrand Russell. Boston: Allen &\nUnwin.\n\nRussell, Bertrand, and Griffin, Nicholas. 2001. _The Selected Letters of\nBertrand Russell: The Public Years, 1914–1970_. New York: Routledge.\n\nRussell, Stuart J., and Norvig, Peter. 2010. _Artificial Intelligence: A\nModern Approach_. 3rd ed. Upper Saddle River, NJ: Prentice-Hall.\n\nSabrosky, Curtis W. 1952. “How Many Insects Are There?” In _Insects_ , edited\nby United States Department of Agriculture, 1–7. Yearbook of Agriculture.\nWashington, DC: United States Government Printing Office.\n\nSalamon, Anna. 2009. “When Software Goes Mental: Why Artificial Minds Mean\nFast Endogenous Growth.” Working Paper, December 27.\n\nSalem, D. J., and Rowan, A. N. 2001. _The State of the Animals: 2001_. Public\nPolicy Series. Washington, DC: Humane Society Press.\n\nSalverda, W., Nolan, B., and Smeeding, T. M. 2009. _The Oxford Handbook of\nEconomic Inequality_. Oxford: Oxford University Press.\n\nSamuel, A. L. 1959. “Some Studies in Machine Learning Using the Game of\nCheckers.” _IBM Journal of Research and Development_ 3 (3): 210–19.\n\nSandberg, Anders. 1999. “The Physics of Information Processing Superobjects:\nDaily Life Among the Jupiter Brains.” _Journal of Evolution and Technology_ 5.\n\nSandberg, Anders. 2010. “An Overview of Models of Technological Singularity.”\nPaper presented at the Roadmaps to AGI and the Future of AGI Workshop, Lugano,\nSwitzerland, March 8.\n\nSandberg, Anders. 2013. “Feasibility of Whole Brain Emulation.” In _Philosophy\nand Theory of Artificial Intelligence_ , edited by Vincent C. Müller, 5:\n251–64. Studies in Applied Philosophy, Epistemology and Rational Ethics. New\nYork: Springer.\n\nSandberg, Anders, and Bostrom, Nick. 2006. “Converging Cognitive\nEnhancements.” _Annals of the New York Academy of Sciences_ 1093: 201–27.\n\nSandberg, Anders, and Bostrom, Nick. 2008. _Whole Brain Emulation: A Roadmap_.\nTechnical Report 2008-3. Future of Humanity Institute, University of Oxford.\n\nSandberg, Anders, and Bostrom, Nick. 2011. _Machine Intelligence Survey_.\nTechnical Report 2011-1. Future of Humanity Institute, University of Oxford.\n\nSandberg, Anders, and Savulescu, Julian. 2011. “The Social and Economic\nImpacts of Cognitive Enhancement.” In _Enhancing Human Capacities_ , edited by\nJulian Savulescu, Ruud ter Meulen, and Guy Kahane, 92–112. Malden, MA: Wiley-\nBlackwell.\n\nSchaeffer, Jonathan. 1997. _One Jump Ahead: Challenging Human Supremacy in\nCheckers_. New York: Springer.\n\nSchaeffer, J., Burch, N., Bjornsson, Y., Kishimoto, A., Muller, M., Lake, R.,\nLu, P., and Sutphen, S. 2007. “Checkers Is Solved.” _Science_ 317 (5844):\n1518–22.\n\nSchalk, Gerwin. 2008. “Brain–Computer Symbiosis.” _Journal of Neural\nEngineering_ 5 (1): P1–P15.\n\nSchelling, Thomas C. 1980. _The Strategy of Conflict_. 2nd ed. Cambridge, MA:\nHarvard University Press.\n\nSchultz, T. R. 2000. “In Search of Ant Ancestors.” _Proceedings of the\nNational Academy of Sciences of the United States of America_ 97 (26):\n14028–9.\n\nSchultz, W., Dayan, P., and Montague, P. R. 1997. “A Neural Substrate of\nPrediction and Reward.” _Science_ 275 (5306): 1593–9.\n\nSchwartz, Jacob T. 1987. “Limits of Artificial Intelligence.” In _Encyclopedia\nof Artificial Intelligence_ , edited by Stuart C. Shapiro and David Eckroth,\n1: 488–503. New York: Wiley.\n\nSchwitzgebel, Eric. 2013. “If Materialism is True, the United States is\nProbably Conscious.” Working Paper, February 8.\n\nSen, Amartya, and Williams, Bernard, eds. 1982. _Utilitarianism and Beyond_.\nNew York: Cambridge University Press.\n\nShanahan, Murray. 2010. _Embodiment and the Inner Life: Cognition and\nConsciousness in the Space of Possible Minds_. New York: Oxford University\nPress.\n\nShannon, Robert V. 2012. “Advances in Auditory Prostheses.” _Current Opinion\nin Neurology_ 25 (1): 61–6.\n\nShapiro, Stuart C. 1992. “Artificial Intelligence.” In _Encyclopedia of\nArtificial Intelligence_ , 2nd ed., 1: 54–7. New York: Wiley.\n\nSheppard, Brian. 2002. “World-Championship-Caliber Scrabble.” _Artificial\nIntelligence_ 134 (1–2): 241–75.\n\nShoemaker, Sydney. 1969. “Time Without Change.” _Journal of Philosophy_ 66\n(12): 363–81.\n\nShulman, Carl. 2010a. _Omohundro’s “Basic AI Drives” and Catastrophic Risks_.\nSan Francisco, CA: Machine Intelligence Research Institute.\n\nShulman, Carl. 2010b. _Whole Brain Emulation and the Evolution of\nSuperorganisms_. San Francisco, CA: Machine Intelligence Research Institute.\n\nShulman, Carl. 2012. “Could We Use Untrustworthy Human Brain Emulations to\nMake Trustworthy Ones?” Paper presented at the AGI Impacts conference 2012.\n\nShulman, Carl, and Bostrom, Nick. 2012. “How Hard is Artificial Intelligence?\nEvolutionary Arguments and Selection Effects.” _Journal of Consciousness\nStudies_ 19 (7–8): 103–30.\n\nShulman, Carl, and Bostrom, Nick. 2014. “Embryo Selection for Cognitive\nEnhancement: Curiosity or Game-Changer?” _Global Policy_ 5 (1): 85–92.\n\nShulman, Carl, Jonsson, Henrik, and Tarleton, Nick. 2009. “Which\nConsequentialism? Machine Ethics and Moral Divergence.” In _AP-CAP 2009: The\nFifth Asia-Pacific Computing and Philosophy Conference, October 1st-2nd,\nUniversity of Tokyo, Japan_. _Proceedings_ , edited by Carson Reynolds and\nAlvaro Cassinelli, 23–25. AP-CAP 2009.\n\nSidgwick, Henry, and Jones, Emily Elizabeth Constance. 2010. _The Methods of\nEthics_. Charleston, SC: Nabu Press.\n\nSilver, Albert. 2006. “How Strong Is GNU Backgammon?” Backgammon Galore!\nSeptember 16. Retrieved October 26, 2013. Available at\n<http://www.bkgm.com/gnu/AllAboutGNU.html#how_strong_is_gnu>.\n\nSimeral, J. D., Kim, S. P., Black, M. J., Donoghue, J. P., and Hochberg, L. R.\n2011. “Neural Control of Cursor Trajectory and Click by a Human with\nTetraplegia 1000 Days after Implant of an Intracortical Microelectrode Array.”\n_Journal of Neural Engineering_ 8 (2): 025027.\n\nSimester, Duncan, and Knez, Marc. 2002. “Direct and Indirect Bargaining Costs\nand the Scope of the Firm.” _Journal of Business_ 75 (2): 283–304.\n\nSimon, Herbert Alexander. 1965. _The Shape of Automation for Men and\nManagement_. New York: Harper & Row.\n\nSinhababu, Neil. 2009. “The Humean Theory of Motivation Reformulated and\nDefended.” _Philosophical Review_ 118 (4): 465–500.\n\nSlagle, James R. 1963. “A Heuristic Program That Solves Symbolic Integration\nProblems in Freshman Calculus.” _Journal of the ACM_ 10 (4): 507–20.\n\nSmeding, H. M., Speelman, J. D., Koning-Haanstra, M., Schuurman, P. R.,\nNijssen, P., van Laar, T., and Schmand, B. 2006. “Neuropsychological Effects\nof Bilateral STN Stimulation in Parkinson Disease: A Controlled Study.”\n_Neurology_ 66 (12): 1830–6.\n\nSmith, Michael. 1987. “The Humean Theory of Motivation.” _Mind: A Quarterly\nReview of Philosophy_ 96 (381): 36–61.\n\nSmith, Michael, Lewis, David, and Johnston, Mark. 1989. “Dispositional\nTheories of Value.” _Proceedings of the Aristotelian Society_ 63: 89–174.\n\nSparrow, Robert. 2013. “In Vitro Eugenics.” _Journal of Medical Ethics_.\ndoi:10.1136/medethics-2012-101200. Published online April 4, 2013. Available\nat <http://jme.bmj.com/content/early/2013/02/13/medethics-2012-101200.full>.\n\nStansberry, Matt, and Kudritzki, Julian. 2012. _Uptime Institute 2012 Data\nCenter Industry Survey_. Uptime Institute.\n\nStapledon, Olaf. 1937. _Star Maker_. London: Methuen.\n\nSteriade, M., Timofeev, I., Durmuller, N., and Grenier, F. 1998. “Dynamic\nProperties of Corticothalamic Neurons and Local Cortical Interneurons\nGenerating Fast Rhythmic (30–40 Hz) Spike Bursts.” _Journal of\nNeurophysiology_ 79 (1): 483–90.\n\nStewart, P. W., Lonky, E., Reihman, J., Pagano, J., Gump, B. B., and Darvill,\nT. 2008. “The Relationship Between Prenatal PCB Exposure and Intelligence (IQ)\nin 9-Year-Old Children.” _Environmental Health Perspectives_ 116 (10):\n1416–22.\n\nSun, W., Yu, H., Shen, Y., Banno, Y., Xiang, Z., and Zhang, Z. 2012.\n“Phylogeny and Evolutionary History of the Silkworm.” _Science China Life\nSciences_ 55 (6): 483–96.\n\nSundet, J., Barlaug, D., and Torjussen, T. 2004. “The End of the Flynn Effect?\nA Study of Secular Trends in Mean Intelligence Scores of Norwegian Conscripts\nDuring Half a Century.” _Intelligence_ 32 (4): 349–62.\n\nSutton, Richard S., and Barto, Andrew G. 1998. _Reinforcement Learning: An\nIntroduction_. Adaptive Computation and Machine Learning. Cambridge, MA: MIT\nPress.\n\nTalukdar, D., Sudhir, K., and Ainslie, A. 2002. “Investigating New Product\nDiffusion Across Products and Countries.” _Marketing Science_ 21 (1): 97–114.\n\nTeasdale, Thomas W., and Owen, David R. 2008. “Secular Declines in Cognitive\nTest Scores: A Reversal of the Flynn Effect.” _Intelligence_ 36 (2): 121–6.\n\nTegmark, Max, and Bostrom, Nick. 2005. “Is a Doomsday Catastrophe Likely?”\n_Nature_ 438: 754.\n\nTeitelman, Warren. 1966. “Pilot: A Step Towards Man–Computer Symbiosis.” PhD\ndiss., Massachusetts Institute of Technology.\n\nTemple, Robert K. G. 1986. _The Genius of China: 3000 Years of Science,\nDiscovery, and Invention_. 1st ed. New York: Simon & Schuster.\n\nTesauro, Gerald. 1995. “Temporal Difference Learning and TD-Gammon.”\n_Communications of the ACM_ 38 (3): 58–68.\n\nTetlock, Philip E. 2005. _Expert Political Judgment: How Good is it? How Can\nWe Know?_ Princeton, NJ: Princeton University Press.\n\nTetlock, Philip E., and Belkin, Aaron. 1996. “Counterfactual Thought\nExperiments in World Politics: Logical, Methodological, and Psychological\nPerspectives.” In _Counterfactual Thought Experiments in World Politics:\nLogical, Methodological, and Psychological Perspectives_ , edited by Philip E.\nTetlock and Aaron Belkin, 1–38. Princeton, NJ: Princeton University Press.\n\nThompson, Adrian. 1997. “Artificial Evolution in the Physical World.” In\n_Evolutionary Robotics: From Intelligent Robots to Artificial Life_ , edited\nby Takashi Gomi, 101–25. Er ’97. Carp, ON: Applied AI Systems.\n\nThrun, S., Montemerlo, M., Dahlkamp, H., Stavens, D., Aron, A., Diebel, J.,\nFong, P., et al. 2006. “Stanley: The Robot That Won the DARPA Grand\nChallenge.” _Journal of Field Robotics_ 23 (9): 661–92.\n\nTrachtenberg, J. T., Chen, B. E., Knott, G. W., Feng, G., Sanes, J. R.,\nWelker, E., and Svoboda, K. 2002. “Long-Term In Vivo Imaging of Experience-\nDependent Synaptic Plasticity in Adult Cortex.” _Nature_ 420 (6917): 788–94.\n\nTraub, Wesley A. 2012. “Terrestrial, Habitable-Zone Exoplanet Frequency from\n_Kepler_.” _Astrophysical Journal_ 745 (1): 1–10.\n\nTruman, James W., Taylor, Barbara J., and Awad, Timothy A. 1993. “Formation of\nthe Adult Nervous System.” In _The Development of_ Drosophila Melanogaster,\nedited by Michael Bate and Alfonso Martinez Arias. Plainview, NY: Cold Spring\nHarbor Laboratory.\n\nTuomi, Ilkka. 2002. “The Lives and the Death of Moore’s Law.” _First Monday_ 7\n(11).\n\nTuring, A. M. 1950. “Computing Machinery and Intelligence.” _Mind_ 59 (236):\n433–60.\n\nTurkheimer, Eric, Haley, Andreana, Waldron, Mary, D’Onofrio, Brian, and\nGottesman, Irving I. 2003. “Socioeconomic Status Modifies Heritability of IQ\nin Young Children.” _Psychological Science_ 14 (6): 623–8.\n\nUauy, Ricardo, and Dangour, Alan D. 2006. “Nutrition in Brain Development and\nAging: Role of Essential Fatty Acids.” Supplement, _Nutrition Reviews_ 64 (5):\nS24–S33.\n\nUlam, Stanislaw M. 1958. “John von Neumann.” _Bulletin of the American\nMathematical Society_ 64 (3): 1–49.\n\nUncertain Future, The. 2012. “Frequently Asked Questions” The Uncertain\nFuture. Retrieved March 25, 2012. Available at\n<http://www.theuncertainfuture.com/faq.html>.\n\nU.S. Congress, Office of Technology Assessment. 1995. _U.S.–Russian\nCooperation in Space_ ISS-618. Washington, DC: U.S. Government Printing\nOffice, April.\n\nVan Zanden, Jan Luiten. 2003. _On Global Economic History: A Personal View on\nan Agenda for Future Research_. International Institute for Social History,\nJuly 23.\n\nVardi, Moshe Y. 2012. “Artificial Intelligence: Past and Future.”\n_Communications of the ACM_ 55 (1): 5.\n\nVassar, Michael, and Freitas, Robert A., Jr. 2006. “Lifeboat Foundation\nNanoshield.” Lifeboat Foundation. Retrieved May 12, 2012. Available at\n<http://lifeboat.com/ex/nanoshield>.\n\nVinge, Vernor. 1993. “The Coming Technological Singularity: How to Survive in\nthe Post-Human Era.” In _Vision-21: Interdisciplinary Science and Engineering\nin the Era of Cyberspace_ , 11–22. NASA Conference Publication 10129. NASA\nLewis Research Center.\n\nVisscher, P. M., Hill, W. G., and Wray, N. R. 2008. “Heritability in the\nGenomics Era: Concepts and Misconceptions.” _Nature Reviews Genetics_ 9 (4):\n255–66.\n\nVollenweider, Franz, Gamma, Alex, Liechti, Matthias, and Huber, Theo. 1998.\n“Psychological and Cardiovascular Effects and Short-Term Sequelae of MDMA\n(‘Ecstasy’) in MDMA-Naïve Healthy Volunteers.” _Neuropsychopharmachology_ 19\n(4): 241–51.\n\nWade, Michael J. 1976. “Group Selections Among Laboratory Populations of\nTribolium.” _Proceedings of the National Academy of Sciences of the United\nStates of America_ 73 (12): 4604–7.\n\nWainwright, Martin J., and Jordan, Michael I. 2008. “Graphical Models,\nExponential Families, and Variational Inference.” _Foundations and Trends in\nMachine Learning_ 1 (1–2): 1–305.\n\nWalker, Mark. 2002. “Prolegomena to Any Future Philosophy.” _Journal of\nEvolution and Technology_ 10 (1).\n\nWalsh, Nick Paton. 2001. “Alter our DNA or robots will take over, warns\nHawking.” _The Observer_ , September 1.\n<http://www.theguardian.com/uk/2001/sep/02/medicalscience.genetics>.\n\nWarwick, Kevin. 2002. _I, Cyborg_. London: Century.\n\nWehner, M., Oliker, L., and Shalf, J. 2008. “Towards Ultra-High Resolution\nModels of Climate and Weather.” _International Journal of High Performance\nComputing Applications_ 22 (2): 149–65.\n\nWeizenbaum, Joseph. 1966. “Eliza: A Computer Program for the Study of Natural\nLanguage Communication Between Man And Machine.” _Communications of the ACM_ 9\n(1): 36–45.\n\nWeizenbaum, Joseph. 1976. _Computer Power and Human Reason: From Judgment to\nCalculation_. San FrancYork, CA: W. H. Freeman.\n\nWerbos, Paul John. 1994. _The Roots of Backpropagation: From Ordered\nDerivatives to Neural Networks and Political Forecasting_. New York: Wiley.\n\nWhite, J. G., Southgate, E., Thomson, J. N., and Brenner, S. 1986. “The\nStructure of the Nervous System of the Nematode _Caenorhabditis Elegans_.”\n_Philosophical Transactions of the Royal Society of London_. _Series B,\nBiological Sciences_ 314 (1165): 1–340.\n\nWhitehead, Hal. 2003. _Sperm Whales: Social Evolution in the Ocean_. Chicago:\nUniversity of Chicago Press.\n\nWhitman, William B., Coleman, David C., and Wiebe, William J. 1998.\n“Prokaryotes: The Unseen Majority.” _Proceedings of the National Academy of\nSciences of the United States of America_ 95 (12): 6578–83.\n\nWiener, Norbert. 1960. “Some Moral and Technical Consequences of Automation.”\n_Science_ 131 (3410): 1355–8.\n\n_Wikipedia_. 2012a, s.v. “Computer Bridge.” Retrieved June 30, 2013. Available\nat <http://en.wikipedia.org/wiki/Computer_bridge>.\n\n_Wikipedia_. 2012b, s.v. “Supercomputer.” Retrieved June 30, 2013. Available\nat <http://et.wikipedia.org/wiki/Superarvuti>.\n\nWilliams, George C. 1966. _Adaptation and Natural Selection: A Critique of\nSome Current Evolutionary Thought_. Princeton Science Library. Princeton, NJ:\nPrinceton University Press.\n\nWinograd, Terry. 1972. _Understanding Natural Language_. New York: Academic\nPress.\n\nWood, Nigel. 2007. _Chinese Glazes: Their Origins, Chemistry and Re-creation_.\nLondon: A. & C. Black.\n\nWorld Bank. 2008. _Global Economic Prospects: Technology Diffusion in the\nDeveloping World_ 42097\\. Washington, DC.\n\nWorld Robotics. 2011. _Executive Summary of 1_. _World Robotics 2011\nIndustrial Robots; 2. World Robotics 2011 Service Robots_. Retrieved June 30,\n2012. Available at <http://www.bara.org.uk/pdf/2012/world-\nrobotics/Executive_Summary_WR_2012.pdf>.\n\nWorld Values Survey. 2008. _WVS 2005-2008_. Retrieved 29 October, 2013.\nAvailable at <http://www.wvsevsdb.com/wvs/WVSAnalizeStudy.jsp>.\n\nWright, Robert. 2001. _Nonzero: The Logic of Human Destiny_. New York:\nVintage.\n\nYaeger, Larry. 1994. “Computational Genetics, Physiology, Metabolism, Neural\nSystems, Learning, Vision, and Behavior or PolyWorld: Life in a New Context.”\nIn _Proceedings of the Artificial Life III Conference_ , edited by C. G.\nLangton, 263–98. Santa Fe Institute Studies in the Sciences of Complexity.\nReading, MA: Addison-Wesley.\n\nYudkowsky, Eliezer. 2001. _Creating Friendly AI 1.0: The Analysis and Design\nof Benevolent Goal Architectures_. Machine Intelligence Research Institute,\nSan Francisco, CA, June 15.\n\nYudkowsky, Eliezer. 2002. “The AI-Box Experiment.” Retrieved January 15, 2012.\nAvailable at <http://yudkowsky.net/singularity/aibox>.\n\nYudkowsky, Eliezer. 2004. _Coherent Extrapolated Volition_. Machine\nIntelligence Research Institute, San Francisco, CA, May.\n\nYudkowsky, Eliezer. 2007. “Levels of Organization in General Intelligence.” In\n_Artificial General Intelligence_ , edited by Ben Goertzel and Cassio\nPennachin, 389–501. Cognitive Technologies. Berlin: Springer.\n\nYudkowsky, Eliezer. 2008a. “Artificial Intelligence as a Positive and Negative\nFactor in Global Risk.” In _Global Catastrophic Risks_ , edited by Nick\nBostrom and Milan M. Ćirković, 308–45. New York: Oxford University Press.\n\nYudkowsky, Eliezer. 2008b. “Sustained Strong Recursion.” _Less Wrong_ (blog),\nDecember 5.\n\nYudkowsky, Eliezer. 2010. _Timeless Decision Theory_. Machine Intelligence\nResearch Institute, San Francisco, CA.\n\nYudkowsky, Eliezer. 2011. _Complex Value Systems are Required to Realize\nValuable Futures_. San Francisco, CA: Machine Intelligence Research Institute.\n\nYudkowsky, Eliezer. 2013. _Intelligence Explosion Microeconomics_ , Technical\nReport 2013–1. Berkeley, CA: Machine Intelligence Research Institute.\n\nZahavi, Amotz, and Zahavi, Avishag. 1997. _The Handicap Principle: A Missing\nPiece of Darwin’s Puzzle_. Translated by N. Zahavi-Ely and M. P. Ely. New\nYork: Oxford University Press.\n\nZalasiewicz, J., Williams, M., Smith, A., Barry, T. L., Coe, A. L., Bown, P.\nR., Brenchley, P., et al. 2008. “Are We Now Living in the Anthropocene?” _GSA\nToday_ 18 (2): 4–8.\n\nZeira, Joseph. 2011. “Innovations, Patent Races and Endogenous Growth.”\n_Journal of Economic Growth_ 16 (2): 135–56.\n\nZuleta, Hernando. 2008. “An Empirical Note on Factor Shares.” _Journal of\nInternational Trade and Economic Development_ 17 (3): 379–90.\n\n\n## [INDEX](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_006.html#filepos29544)\n\n**A**\n\nAfghan Taliban\n[215](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_020.html#filepos748079)\n\nAgricultural Revolution\n[2](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos36289),\n[80](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_012.html#filepos294863),\n[261](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos906855)\n\nAI-complete problem\n[14](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos78606),\n[47](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos185584),\n[71](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_011.html#filepos265356),\n[93](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_013.html#filepos333467),\n[145](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_017.html#filepos509760),\n[186](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos647860)\n\nAI-OUM, _see_ optimality notions\n\nAI-RL, _see_ optimality notions\n\nAI-VL, _see_ optimality notions\n\nalgorithmic soup\n[172](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_018.html#filepos600322)\n\nalgorithmic trading\n[16](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos86180)–[17](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos89764)\n\nanthropics\n[27](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos123667)–[28](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos125045),\n[126](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_015.html#filepos446809),\n[134](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_016.html#filepos473657)–[135](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_016.html#filepos477270),\n[174](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_018.html#filepos607319),\n[222](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_020.html#filepos774973)–[225](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_020.html#filepos784988)\n\ndefinition\n[225](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_020.html#filepos784988)\n\nArendt, Hannah\n[105](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_014.html#filepos374036)\n\nArmstrong, Stuart\n[280](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1016838),\n[291](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1079986),\n[294](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1097891),\n[302](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1144032)\n\nartificial agent\n[10](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos65097),\n[88](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_012.html#filepos319906),\n[105](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_014.html#filepos374036)–[109](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_014.html#filepos386956),\n[172](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_018.html#filepos600322)–[176](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_018.html#filepos614830),\n[185](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos645352)–[206](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos718456);\n_see also_ Bayesian agent\n\nartificial intelligence\n\narms race\n[64](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_011.html#filepos241525),\n[88](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_012.html#filepos319906),\n[247](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos863033)\n\nfuture of [19](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos96736),\n[292](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1085779)\n\ngreater-than-human, _see_ superintelligence\n\nhistory of\n[5](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos45699)–[18](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos93297)\n\noverprediction of\n[4](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos41948)\n\npioneers\n[4](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos41948)–[5](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos45699),\n[18](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos93297)\n\nAsimov, Isaac\n[139](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_016.html#filepos490909)\n\naugmentation\n[142](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_016.html#filepos501999)–[143](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_016.html#filepos505499),\n[201](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos688701)–[203](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos707772)\n\nautism [57](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_010.html#filepos217912)\n\nautomata theory\n[5](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos45699)\n\nautomatic circuit breaker\n[17](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos89764)\n\nautomation\n[17](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos89764),\n[98](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_013.html#filepos351316),\n[117](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_015.html#filepos414086),\n[160](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_018.html#filepos557644)–[176](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_018.html#filepos614830)\n\n**B**\n\nbackgammon [12](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos71770)\n\nbackpropagation algorithm\n[8](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos57019)\n\nbargaining costs\n[182](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_018.html#filepos636212)\n\nBayesian agent\n[9](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos60697)–[11](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos68753),\n[123](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_015.html#filepos436044),\n[130](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_016.html#filepos458938); _see\nalso_ artificial agent _and_ optimality notions\n\nBayesian networks\n[9](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos60697)\n\nBerliner, Hans\n[12](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos71770)\n\nbiological cognition\n[22](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos104499),\n[36](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos148183)–[48](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos189266),\n[50](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos196673)–[51](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos200099),\n[232](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos809163)\n\nbiological enhancement\n[36](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos148183)–[48](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos189266),\n[50](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos196673)–[51](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos200099),\n[142](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_016.html#filepos501999)–[143](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_016.html#filepos505499),\n[232](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos809163); _see\nalso_ cognitive enhancement\n\nboxing\n[129](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_016.html#filepos455594)–[131](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_016.html#filepos462705),\n[143](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_016.html#filepos505499),\n[156](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_017.html#filepos548381)–[157](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_017.html#filepos551221)\n\ninformational\n[130](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_016.html#filepos458938)\n\nphysical\n[129](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_016.html#filepos455594)–[130](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_016.html#filepos458938)\n\nbrain implant, _see_ cyborg\n\nbrain plasticity\n[48](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos189266)\n\nbrain–computer interfaces\n[44](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos174322)–[48](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos189266),\n[51](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos200099),\n[83](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_012.html#filepos302276),\n[142](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_016.html#filepos501999)–[143](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_016.html#filepos505499);\n_see also_ cyborg\n\nBrown, Louise\n[43](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos170641)\n\n**C**\n\n_C. elegans_\n34–[35](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos143997),\n[266](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos935384),\n[267](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos941273)\n\ncapability control\n[129](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_016.html#filepos455594)–[144](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_016.html#filepos508821),\n[156](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_017.html#filepos548381)–[157](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_017.html#filepos551221)\n\ncapital [39](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos159649),\n[48](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos189266),\n[68](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_011.html#filepos255348),\n[84](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_012.html#filepos305731)–[88](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_012.html#filepos319906),\n[99](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_013.html#filepos354764),\n[113](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_014.html#filepos401500)–[114](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_014.html#filepos405024),\n[159](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_018.html#filepos555084)–[184](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_018.html#filepos643382),\n[251](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos875747),\n[287](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1056226),\n[288](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1062268),\n[289](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1068220)\n\ncausal validity semantics\n[197](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos687167)\n\nCEV, _see_ coherent extrapolated volition\n\nChalmers, David\n[24](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos110767),\n[265](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos929414),\n[283](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1033480),\n[295](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1103863),\n[302](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1144032)\n\ncharacter recognition\n[15](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos82621)\n\ncheckers [12](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos71770)\n\nchess\n[11](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos68753)–[22](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos104499),\n[52](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_010.html#filepos201197),\n[93](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_013.html#filepos333467),\n[134](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_016.html#filepos473657),\n[263](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos917214),\n[264](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos923469)\n\nchild machine\n[23](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos107115),\n[29](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos128738); _see\nalso_ seed AI\n\nCHINOOK [12](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos71770)\n\nChristiano, Paul\n[198](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos688611),\n[207](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos722040)\n\ncivilization baseline\n[63](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_011.html#filepos238595)\n\ncloning [42](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos167047)\n\ncognitive enhancement\n[42](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos167047)–[51](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos200099),\n[67](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_011.html#filepos251591),\n[94](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_013.html#filepos338796),\n[111](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_014.html#filepos394262)–[112](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_014.html#filepos398105),\n[193](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos672716),\n[204](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos711076),\n[232](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos809163)–[238](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos830602),\n[244](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos852127),\n[259](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_022.html#filepos902418)\n\ncoherent extrapolated volition (CEV)\n[198](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos688611),\n[211](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_020.html#filepos733469)–[227](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_020.html#filepos792259),\n[296](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1109694),\n[298](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1120946),\n[303](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1149612)\n\ndefinition\n[211](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_020.html#filepos733469)\n\ncollaboration (benefits of)\n[249](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos869165)\n\ncollective intelligence\n[48](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos189266)–[51](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos200099),\n[52](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_010.html#filepos201197)–[57](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_010.html#filepos217912),\n[67](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_011.html#filepos251591),\n[72](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_011.html#filepos269580),\n[142](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_016.html#filepos501999),\n[163](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_018.html#filepos568204),\n[203](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos707772),\n[259](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_022.html#filepos902418),\n[271](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos964280),\n[273](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos976118),\n[279](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos1011417)\n\ncollective superintelligence\n[39](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos159649),\n[48](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos189266)–[49](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos192786),\n[52](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_010.html#filepos201197)–[59](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_010.html#filepos225304),\n[83](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_012.html#filepos302276),\n[93](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_013.html#filepos333467),\n[99](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_013.html#filepos354764),\n[285](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1045058)\n\ndefinition\n[54](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_010.html#filepos207010)\n\ncombinatorial explosion\n[6](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos49264),\n[9](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos60697),\n[10](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos65097),\n[47](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos185584),\n[155](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_017.html#filepos541561)\n\nCommon Good Principle\n[254](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos886604)–[259](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_022.html#filepos902418)\n\ncommon sense\n[14](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos78606)\n\ncomputer vision\n[9](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos60697)\n\ncomputing power\n[7](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos53377)–[9](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos60697),\n[24](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos110767),\n[25](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos115224)–[35](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos143997),\n[47](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos185584),\n[53](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_010.html#filepos203414)–[60](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_010.html#filepos229392),\n[68](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_011.html#filepos255348)–[77](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_011.html#filepos287200),\n[101](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_013.html#filepos361993),\n[134](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_016.html#filepos473657),\n[155](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_017.html#filepos541561),\n[198](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos688611),\n[240](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos837779)–[244](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos852127),\n[251](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos875747),\n[286](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1050502),\n[288](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1062268); _see\nalso_ computronium _and_ hardware overhang\n\ncomputronium\n[101](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_013.html#filepos361993),\n[123](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_015.html#filepos436044)–[124](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_015.html#filepos439665),\n[140](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_016.html#filepos494574),\n[193](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos672716),\n[219](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_020.html#filepos762868); _see\nalso_ computing power\n\nconnectionism\n[8](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos57019)\n\nconsciousness\n[22](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos104499),\n[106](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_014.html#filepos376407),\n[126](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_015.html#filepos446809),\n[139](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_016.html#filepos490909),\n[173](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_018.html#filepos603698)–[176](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_018.html#filepos614830),\n[216](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_020.html#filepos751628),\n[226](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_020.html#filepos788669),\n[271](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos964280),\n[282](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1028082),\n[288](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1062268),\n[292](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1085779),\n[303](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1149612); _see\nalso_ mind crime\n\ncontrol methods\n[127](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_016.html#filepos449542)–[144](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_016.html#filepos508821),\n[145](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_017.html#filepos509760)–[158](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_017.html#filepos553359),\n[202](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos704501),\n[236](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos823424)–[238](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos830602),\n[286](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1050502); _see\nalso_ capability control _and_ motivation selection\n\nCopernicus, Nicolaus\n[14](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos78606)\n\ncosmic endowment\n[101](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_013.html#filepos361993)–[104](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_013.html#filepos370694),\n[115](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_015.html#filepos408497),\n[134](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_016.html#filepos473657),\n[209](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_020.html#filepos727334),\n[214](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_020.html#filepos744522)–[217](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_020.html#filepos755237),\n[227](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_020.html#filepos792259),\n[250](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos872074),\n[260](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_022.html#filepos905696),\n[283](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1033480),\n[296](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1109694)\n\ncrosswords (solving)\n[12](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos71770)\n\ncryptographic reward tokens\n[134](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_016.html#filepos473657),\n[276](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos994080)\n\ncryptography\n[80](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_012.html#filepos294863)\n\ncyborg\n[44](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos174322)–[48](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos189266),\n[67](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_011.html#filepos251591),\n[270](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos958624)\n\n**D**\n\nDARPA, _see_ Defense Advanced Research Projects Agency\n\nDART (tool)\n[15](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos82621)\n\nDartmouth Summer Project\n[5](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos45699)\n\ndata mining\n[15](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos82621)–[16](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos86180),\n[232](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos809163),\n[301](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1138295)\n\ndecision support systems\n[15](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos82621),\n[98](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_013.html#filepos351316); _see\nalso_ tool-AI\n\ndecision theory\n[10](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos65097)–[11](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos68753),\n[88](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_012.html#filepos319906),\n[185](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos645352)–[186](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos647860),\n[221](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_020.html#filepos770021)–[227](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_020.html#filepos792259),\n[280](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1016838),\n[298](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1120946); _see\nalso_ optimality notions\n\ndecisive strategic advantage\n[78](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_012.html#filepos288933)–[89](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_012.html#filepos323404),\n[95](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_013.html#filepos341571),\n[104](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_013.html#filepos370694)–[112](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_014.html#filepos398105),\n[115](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_015.html#filepos408497)–[126](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_015.html#filepos446809),\n[129](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_016.html#filepos455594)–[138](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_016.html#filepos484729),\n[148](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_017.html#filepos519506)–[149](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_017.html#filepos522885),\n[156](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_017.html#filepos548381)–[159](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_018.html#filepos555084),\n[177](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_018.html#filepos618561),\n[190](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos662079),\n[209](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_020.html#filepos727334)–[214](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_020.html#filepos744522),\n[225](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_020.html#filepos784988),\n[252](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos879537)\n\nDeep Blue [12](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos71770)\n\nDeep Fritz\n[22](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos104499)\n\nDefense Advanced Research Projects Agency (DARPA)\n[15](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos82621)\n\ndesign effort, _see_ optimization power\n\nDewey, Daniel\n[291](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1079986)\n\nDifferential Technological Development (Principle of)\n[230](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos801881)–[237](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos826942)\n\nDiffie–Hellman key exchange protocol\n[80](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_012.html#filepos294863)\n\ndiminishing returns\n[37](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos151873)–[38](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos154616),\n[66](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_011.html#filepos248292),\n[88](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_012.html#filepos319906),\n[114](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_014.html#filepos405024),\n[273](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos976118),\n[303](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1149612)\n\ndirect reach\n[58](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_010.html#filepos221532)\n\ndirect specification\n[139](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_016.html#filepos490909)–[143](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_016.html#filepos505499)\n\nDNA synthesis\n[39](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos159649),\n[98](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_013.html#filepos351316)\n\nDo What I Mean (DWIM)\n[220](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_020.html#filepos766558)–[221](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_020.html#filepos770021)\n\ndomesticity\n[140](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_016.html#filepos494574)–[143](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_016.html#filepos505499),\n[146](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_017.html#filepos512113)–[156](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_017.html#filepos548381),\n[187](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos651630),\n[191](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos665747),\n[207](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos722040),\n[222](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_020.html#filepos774973)\n\nDrexler, Eric\n[239](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos834026),\n[270](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos958624),\n[276](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos994080),\n[278](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos1005344),\n[300](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1132561)\n\ndrones [15](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos82621),\n[98](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_013.html#filepos351316)\n\nDutch book\n[111](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_014.html#filepos394262)\n\nDyson, Freeman\n[101](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_013.html#filepos361993),\n[278](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos1005344)\n\n**E**\n\neconomic growth\n[3](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos40151),\n[160](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_018.html#filepos557644)–[166](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_018.html#filepos578860),\n[179](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_018.html#filepos625490),\n[261](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos906855),\n[274](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos981754),\n[299](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1126981)\n\nEinstein, Albert\n[56](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_010.html#filepos214335),\n[70](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_011.html#filepos262469),\n[85](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_012.html#filepos309319)\n\nELIZA (program)\n[6](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos49264)\n\nembryo selection\n[36](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos148183)–[44](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos174322),\n[67](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_011.html#filepos251591),\n[268](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos946782)\n\nemulation modulation\n[207](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos722040)\n\nEnigma code\n[87](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_012.html#filepos316500)\n\nenvironment of evolutionary adaptedness\n[164](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_018.html#filepos571859),\n[171](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_018.html#filepos596704)\n\nepistemology\n[222](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_020.html#filepos774973)–[224](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_020.html#filepos781205)\n\nequation solvers\n[15](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos82621)\n\neugenics\n[36](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos148183)–[44](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos174322),\n[268](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos946782),\n[279](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos1011417)\n\nEurisko [12](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos71770)\n\nevolution\n[8](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos57019)–[9](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos60697),\n[23](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos107115)–[27](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos123667),\n[44](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos174322),\n[154](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_017.html#filepos541531),\n[173](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_018.html#filepos603698)–[176](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_018.html#filepos614830),\n[187](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos651630),\n[198](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos688611),\n[207](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos722040),\n[265](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos929414),\n[266](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos935384),\n[267](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos941273),\n[273](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos976118)\n\nevolutionary selection\n[187](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos651630),\n[207](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos722040),\n[290](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1074309)\n\nevolvable hardware\n[154](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_017.html#filepos541531)\n\nexhaustive search\n[6](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos49264)\n\nexistential risk\n[4](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos41948),\n[21](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos103150),\n[55](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_010.html#filepos210573),\n[100](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_013.html#filepos356950)–[104](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_013.html#filepos370694),\n[115](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_015.html#filepos408497)–[126](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_015.html#filepos446809),\n[175](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_018.html#filepos611093),\n[183](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_018.html#filepos639789),\n[230](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos801881)–[236](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos823424),\n[239](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos834026)–[254](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos886604),\n[256](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_022.html#filepos892007)–[259](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_022.html#filepos902418),\n[286](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1050502),\n[301](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1138295)–[302](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1144032)\n\nstate risks\n[233](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos812749)–[234](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos816367)\n\nstep risks\n[233](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos812749)\n\nexpert system\n[7](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos53377)\n\nexplicit representation\n[207](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos722040)\n\nexponential growth, _see_ growth\n\nexternal reference semantics\n[197](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos687167)\n\n**F**\n\nface recognition\n[15](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos82621)\n\nfailure modes\n[117](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_015.html#filepos414086)–[120](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_015.html#filepos424758)\n\nFaraday cage\n[130](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_016.html#filepos458938)\n\nFields Medal\n[255](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_022.html#filepos889517)–[256](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_022.html#filepos892007),\n[272](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos969926)\n\nFifth-Generation Computer Systems Project\n[7](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos53377)\n\nfitness function\n[25](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos115224); _see\nalso_ evolution\n\nFlash Crash (2010)\n[16](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos86180)–[17](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos89764)\n\nformal language\n[7](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos53377),\n[145](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_017.html#filepos509760)\n\nFreeCell (game)\n[13](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos75444)\n\n**G**\n\ngame theory\n[87](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_012.html#filepos316500),\n[159](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_018.html#filepos555084)\n\ngame-playing AI\n[12](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos71770)–[14](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos78606)\n\nGeneral Problem Solver\n[6](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos49264)\n\ngenetic algorithms\n[7](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos53377)–[13](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos75444),\n[24](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos110767)–[27](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos123667),\n[237](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos826942)–[240](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos837779);\n_see also_ evolution\n\ngenetic selection\n[37](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos151873)–[50](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos196673),\n[61](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_010.html#filepos233475),\n[232](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos809163)–[238](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos830602);\n_see also_ evolution\n\ngenie AI\n[148](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_017.html#filepos519506)–[158](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_017.html#filepos553359),\n[285](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1045058)\n\ndefinition\n[148](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_017.html#filepos519506)\n\ngenotyping\n[37](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos151873)\n\ngermline interventions\n[37](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos151873)–[44](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos174322),\n[67](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_011.html#filepos251591),\n[273](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos976118); _see\nalso_ embryo selection\n\nGinsberg, Matt\n[12](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos71770)\n\nGo (game) [13](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos75444)\n\ngoal-content\n[109](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_014.html#filepos386956)–[110](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_014.html#filepos390344),\n[146](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_017.html#filepos512113),\n[207](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos722040),\n[222](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_020.html#filepos774973)–[227](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_020.html#filepos792259)\n\nGood Old-Fashioned Artificial Intelligence (GOFAI)\n[7](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos53377)–[15](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos82621),\n[23](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos107115)\n\nGood, I. J. [4](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos41948)\n\nGorbachev, Mikhail\n[86](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_012.html#filepos313010)\n\ngraceful degradation\n[7](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos53377)\n\ngraphical models\n[11](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos68753)\n\ngrowth\n[1](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos33975)–[7](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos53377),\n[48](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos189266)–[55](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_010.html#filepos210573),\n[69](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_011.html#filepos258937)–[75](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_011.html#filepos280832),\n[83](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_012.html#filepos302276),\n[163](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_018.html#filepos568204),\n[261](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos906855),\n[281](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1022605)\n\n**H**\n\nHail Mary approach\n[198](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos688611)–[200](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos688671),\n[207](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos722040),\n[293](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1091662),\n[294](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1097891)\n\nHanson, Robin\n[2](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos36289),\n[160](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_018.html#filepos557644),\n[261](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos906855),\n[270](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos958624),\n[271](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos964280),\n[287](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1056226)\n\nhardware overhang\n[73](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_011.html#filepos273122),\n[240](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos837779)–[243](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos848268),\n[274](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos981754),\n[289](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1068220),\n[301](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1138295),\n[302](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1144032)\n\nhedonism\n[140](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_016.html#filepos494574),\n[210](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_020.html#filepos729641)\n\nhedonium\n[140](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_016.html#filepos494574),\n[219](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_020.html#filepos762868)\n\nHelsinki Declaration\n[188](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos655116)\n\nheuristic search\n[6](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos49264)\n\nHill, Benny\n[105](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_014.html#filepos374036)\n\nhippocampus\n[47](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos185584)\n\nHLMI, _see_ machine intelligence, human-level\n\nHodgkin–Huxley model\n[25](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos115224)\n\nhuman baseline\n[62](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_011.html#filepos236181)–[77](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_011.html#filepos287200),\n[82](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_012.html#filepos299697)\n\nhuman extinction, _see_ existential risk\n\nHuman Genome Project\n[86](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_012.html#filepos313010),\n[253](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos883031),\n[276](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos994080)\n\nhuman intelligence\n[5](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos45699)–[14](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos78606),\n[24](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos110767)–[58](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_010.html#filepos221532),\n[98](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_013.html#filepos351316)–[99](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_013.html#filepos354764),\n[159](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_018.html#filepos555084)–[184](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_018.html#filepos643382),\n[242](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos844785)–[254](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos886604),\n[255](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_022.html#filepos889517)–[257](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_022.html#filepos895429)\n\nhuman–machine interface, _see_ cyborg\n\n**I**\n\nimpersonal perspective\n[228](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos795792)–[246](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos858744)\n\n_in vitro_ fertilization, _see_ embryo selection\n\nincentive methods\n[131](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_016.html#filepos462705)–[143](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_016.html#filepos505499)\n\ncryptographic reward tokens\n[133](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_016.html#filepos469907)\n\nsocial integration\n[131](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_016.html#filepos462705)–[132](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_016.html#filepos466290),\n[156](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_017.html#filepos548381)–[158](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_017.html#filepos553359),\n[159](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_018.html#filepos555084),\n[202](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos704501),\n[283](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1033480)\n\nindirect normativity\n[141](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_016.html#filepos498147)–[150](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_017.html#filepos526513),\n[209](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_020.html#filepos727334)–[227](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_020.html#filepos792259),\n[262](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos911174),\n[298](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1120946)\n\nindirect reach\n[58](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_010.html#filepos221532)\n\ninductive bias\n[9](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos60697)–[10](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos65097)\n\nIndustrial Revolution\n[2](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos36289),\n[80](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_012.html#filepos294863),\n[161](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_018.html#filepos560856)–[163](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_018.html#filepos568204)\n\ninfrastructure profusion\n[123](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_015.html#filepos436044)–[125](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_015.html#filepos443281),\n[153](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_017.html#filepos537661),\n[187](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos651630),\n[226](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_020.html#filepos788669),\n[282](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1028082)\n\ninstitution design\n[202](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos704501)–[208](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos725271)\n\ninstrumental convergence thesis\n[105](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_014.html#filepos374036)–[116](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_015.html#filepos410668)\n\nintelligence explosion\n[2](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos36289)–[5](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos45699),\n[22](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos104499)–[51](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos200099),\n[62](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_011.html#filepos236181)–[77](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_011.html#filepos287200),\n[78](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_012.html#filepos288933)–[90](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_012.html#filepos326953),\n[95](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_013.html#filepos341571)–[104](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_013.html#filepos370694),\n[108](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_014.html#filepos383193),\n[115](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_015.html#filepos408497)–[126](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_015.html#filepos446809),\n[127](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_016.html#filepos449542)–[128](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_016.html#filepos452137),\n[136](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_016.html#filepos480353),\n[151](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_017.html#filepos530369),\n[160](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_018.html#filepos557644),\n[165](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_018.html#filepos575412),\n[178](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_018.html#filepos622135),\n[198](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos688611),\n[205](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos714629),\n[227](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_020.html#filepos792259),\n[228](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos795792)–[254](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos886604),\n[256](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_022.html#filepos892007)–[260](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_022.html#filepos905696),\n[274](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos981754),\n[276](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos994080),\n[282](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1028082),\n[284](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1039071),\n[289](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1068220),\n[300](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1132561),\n[301](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1138295)–[302](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1144032)\n\nInternet [16](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos86180),\n[45](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos177789)–[49](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos192786),\n[67](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_011.html#filepos251591)–[77](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_011.html#filepos287200),\n[85](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_012.html#filepos309319),\n[94](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_013.html#filepos338796)–[98](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_013.html#filepos351316),\n[130](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_016.html#filepos458938),\n[146](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_017.html#filepos512113),\n[241](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos841162),\n[271](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos964280)\n\ninventory control systems\n[16](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos86180)\n\nIVF, _see_ embryo selection\n\n**J**\n\nJeopardy!13\n\n**K**\n\nKasparov, Garry\n[12](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos71770)\n\nKepler, Johannes\n[14](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos78606)\n\nKnuth, Donald\n[14](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos78606),\n[264](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos923469)\n\nKurzweil, Ray\n[2](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos36289),\n[261](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos906855),\n[269](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos952683)\n\n**L**\n\nLenat, Douglas\n[12](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos71770),\n[263](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos917214)\n\nLogic Theorist (system)\n[6](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos49264)\n\nlogicist paradigm, _see_ Good Old-Fashioned Artificial Intelligence (GOFAI)\n\nLogistello [12](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos71770)\n\n**M**\n\nmachine intelligence; _see also_ artificial intelligence\n\nhuman-level (HLMI)\n[4](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos41948),\n[19](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos96736)–[21](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos103150),\n[27](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos123667)–[35](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos143997),\n[73](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_011.html#filepos273122)–[74](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_011.html#filepos276640),\n[207](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos722040),\n[243](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos848268),\n[264](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos923469),\n[267](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos941273)\n\nrevolution, _see_ intelligence explosion\n\nmachine learning\n[8](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos57019)–[18](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos93297),\n[28](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos125045),\n[121](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_015.html#filepos428523),\n[152](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_017.html#filepos534074),\n[188](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos655116),\n[274](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos981754),\n[290](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1074309)\n\nmachine translation\n[15](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos82621)\n\nmacro-structural development accelerator\n[233](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos812749)–[235](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos819927)\n\nmalignant failure\n[123](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_015.html#filepos436044)–[126](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_015.html#filepos446809),\n[149](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_017.html#filepos522885),\n[196](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos676451)\n\nMalthusian condition\n[163](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_018.html#filepos568204)–[165](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_018.html#filepos575412),\n[252](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos879537)\n\nManhattan Project\n[75](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_011.html#filepos280832),\n[80](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_012.html#filepos294863)–[87](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_012.html#filepos316500),\n[276](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos994080)\n\nMcCarthy, John\n[5](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos45699)–[18](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos93297)\n\nMcCulloch–Pitts neuron\n[237](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos826942)\n\nMegaEarth [56](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_010.html#filepos214335)\n\nmemory capacity\n[7](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos53377)–[9](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos60697),\n[60](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_010.html#filepos229392),\n[71](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_011.html#filepos265356)\n\nmemory sharing\n[61](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_010.html#filepos233475)\n\nMill, John Stuart\n[210](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_020.html#filepos729641)\n\nmind crime\n[125](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_015.html#filepos443281)–[126](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_015.html#filepos446809),\n[153](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_017.html#filepos537661),\n[201](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos688701)–[208](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos725271),\n[213](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_020.html#filepos740883),\n[226](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_020.html#filepos788669),\n[297](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1115198)\n\nMinsky, Marvin\n[18](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos93297),\n[261](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos906855),\n[262](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos911174),\n[282](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1028082)\n\nMonte Carlo method\n[9](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos60697)–[13](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos75444)\n\nMoore’s law\n[24](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos110767)–[25](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos115224),\n[73](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_011.html#filepos273122)–[77](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_011.html#filepos287200),\n[274](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos981754),\n[286](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1050502); _see\nalso_ computing power\n\nmoral growth\n[214](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_020.html#filepos744522)\n\nmoral permissibility\n(MP)218–[220](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_020.html#filepos766558),\n[297](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1115198)\n\nmoral rightness\n(MR)217–[220](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_020.html#filepos766558).[296](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1109694),\n[297](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1115198)\n\nmoral status\n[125](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_015.html#filepos443281)–[126](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_015.html#filepos446809),\n[166](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_018.html#filepos578860)–[169](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_018.html#filepos589401),\n[173](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_018.html#filepos603698),\n[202](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos704501)–[205](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos714629),\n[268](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos946782),\n[288](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1062268),\n[296](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1109694)\n\nMoravec, Hans\n[24](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos110767),\n[265](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos929414),\n[288](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1062268)\n\nmotivation selection\n[29](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos128738),\n[127](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_016.html#filepos449542)–[129](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_016.html#filepos455594),\n[138](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_016.html#filepos484729)–[144](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_016.html#filepos508821),\n[147](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_017.html#filepos515864),\n[158](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_017.html#filepos553359),\n[168](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_018.html#filepos585937),\n[180](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_018.html#filepos629083)–[191](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos665747),\n[222](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_020.html#filepos774973)\n\ndefinition\n[138](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_016.html#filepos484729)\n\nmotivational scaffolding\n[191](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos665747),\n[207](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos722040)\n\nmultipolar scenarios\n[90](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_012.html#filepos326953),\n[132](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_016.html#filepos466290),\n[159](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_018.html#filepos555084)–[184](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_018.html#filepos643382),\n[243](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos848268)–[254](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos886604),\n[301](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1138295)\n\nmutational load\n[41](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos164290)\n\n**N**\n\nnanotechnology\n[53](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_010.html#filepos203414),\n[94](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_013.html#filepos338796)–[98](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_013.html#filepos351316),\n[103](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_013.html#filepos367439),\n[113](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_014.html#filepos401500),\n[177](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_018.html#filepos618561),\n[231](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos805508),\n[239](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos834026),\n[276](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos994080),\n[277](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos999790),\n[299](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1126981),\n[300](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1132561)\n\nnatural language\n[14](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos78606)\n\nneural networks\n[5](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos45699)–[9](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos60697),\n[28](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos125045),\n[46](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos181760),\n[173](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_018.html#filepos603698),\n[237](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos826942),\n[262](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos911174),\n[274](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos981754)\n\nneurocomputational modeling\n[25](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos115224)–[30](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos132462),\n[35](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos143997),\n[61](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_010.html#filepos233475),\n[301](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1138295); _see\nalso_ whole brain emulation (WBE) _and_ neuromorphic AI\n\nneuromorphic AI\n[28](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos125045),\n[34](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos142221),\n[47](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos185584),\n[237](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos826942)–[245](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos855020),\n[267](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos941273),\n[300](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1132561),\n[301](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1138295)\n\nNewton, Isaac\n[56](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_010.html#filepos214335)\n\nNilsson, Nils\n[18](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos93297)–[20](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos99704),\n[264](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos923469)\n\nnootropics\n[36](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos148183)–[44](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos174322),\n[66](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_011.html#filepos248292)–[67](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_011.html#filepos251591),\n[201](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos688701),\n[267](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos941273)\n\nNorvig, Peter\n[19](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos96736),\n[264](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos923469),\n[282](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1028082)\n\n**O**\n\nobservation selection theory, _see_ anthropics\n\nOliphant, Mark\n[85](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_012.html#filepos309319)\n\nO’Neill, Gerard\n[101](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_013.html#filepos361993)\n\nontological crisis\n[146](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_017.html#filepos512113),\n[197](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos687167)\n\noptimality notions\n[10](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos65097),\n[186](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos647860),\n[194](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos676391),\n[291](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1079986)–[293](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1091662)\n\nBayesian agent\n[9](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos60697)–[11](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos68753)\n\nvalue learner (AI-VL)\n[194](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos676391)\n\nobservation-utility-maximizer (AI-OUM)\n[194](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos676391)\n\nreinforcement learner (AI-RL)\n[194](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos676391)\n\noptimization power\n[24](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos110767),\n[62](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_011.html#filepos236181)–[75](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_011.html#filepos280832),\n[83](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_012.html#filepos302276),\n[92](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_013.html#filepos329935)–[96](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_013.html#filepos344390),\n[227](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_020.html#filepos792259),\n[274](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos981754)\n\ndefinition\n[65](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_011.html#filepos244846)\n\noracle AI\n[141](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_016.html#filepos498147)–[158](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_017.html#filepos553359),\n[222](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_020.html#filepos774973)–[226](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_020.html#filepos788669),\n[285](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1045058),\n[286](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1050502)\n\ndefinition\n[146](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_017.html#filepos512113)\n\northogonality thesis\n[105](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_014.html#filepos374036)–[109](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_014.html#filepos386956),\n[115](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_015.html#filepos408497),\n[279](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos1011417),\n[280](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1016838)\n\n**P**\n\npaperclip AI\n[107](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_014.html#filepos379402)–[108](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_014.html#filepos383193),\n[123](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_015.html#filepos436044)–[125](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_015.html#filepos443281),\n[132](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_016.html#filepos466290)–[135](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_016.html#filepos477270),\n[153](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_017.html#filepos537661),\n[212](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_020.html#filepos737044),\n[243](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos848268)\n\nParfit, Derek\n[279](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos1011417)\n\nPascal’s mugging\n[223](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_020.html#filepos777439),\n[298](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1120946)\n\nPascal’s wager\n[223](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_020.html#filepos777439)\n\nperson-affecting perspective\n[228](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos795792),\n[245](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos855020)–[246](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos858744),\n[301](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1138295)\n\nperverse instantiation\n[120](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_015.html#filepos424758)–[124](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_015.html#filepos439665),\n[153](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_017.html#filepos537661),\n[190](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos662079)–[196](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos676451)\n\npoker [13](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos75444)\n\nprincipal–agent problem\n[127](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_016.html#filepos449542)–[128](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_016.html#filepos452137),\n[184](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_018.html#filepos643382)\n\nPrinciple of Epistemic Deference\n[211](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_020.html#filepos733469),\n[221](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_020.html#filepos770021)\n\nProverb (program)\n[12](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos71770)\n\n**Q**\n\nqualia, _see_ consciousness\n\nquality superintelligence\n[51](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos200099)–[58](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_010.html#filepos221532),\n[72](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_011.html#filepos269580),\n[243](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos848268),\n[272](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos969926)\n\ndefinition\n[56](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_010.html#filepos214335)\n\n**R**\n\nrace dynamic, _see_ technology race\n\nrate of growth, _see_ growth\n\nratification\n[222](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_020.html#filepos774973)–[225](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_020.html#filepos784988)\n\nRawls, John\n[150](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_017.html#filepos526513)\n\nReagan, Ronald\n[86](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_012.html#filepos313010)–[87](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_012.html#filepos316500)\n\nreasons-based goal\n[220](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_020.html#filepos766558)\n\nrecalcitrance\n[62](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_011.html#filepos236181)–[77](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_011.html#filepos287200),\n[92](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_013.html#filepos329935),\n[241](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos841162),\n[274](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos981754)\n\ndefinition\n[65](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_011.html#filepos244846)\n\nrecursive self-improvement\n[29](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos128738),\n[75](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_011.html#filepos280832),\n[96](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_013.html#filepos344390),\n[142](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_016.html#filepos501999),\n[259](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_022.html#filepos902418); _see\nalso_ seed AI\n\nreinforcement learning\n[12](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos71770),\n[28](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos125045),\n[188](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos655116)–[189](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos658717),\n[194](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos676391)–[196](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos676451),\n[207](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos722040),\n[237](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos826942),\n[277](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos999790),\n[282](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1028082),\n[290](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1074309)\n\nresource acquisition\n[113](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_014.html#filepos401500)–[116](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_015.html#filepos410668),\n[123](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_015.html#filepos436044),\n[193](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos672716)\n\nreward signal\n[71](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_011.html#filepos265356),\n[121](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_015.html#filepos428523)–[122](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_015.html#filepos432338),\n[188](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos655116),\n[194](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos676391),\n[207](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos722040)\n\nRiemann hypothesis catastrophe\n[123](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_015.html#filepos436044),\n[141](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_016.html#filepos498147)\n\nrobotics\n[9](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos60697)–[19](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos96736),\n[94](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_013.html#filepos338796)–[97](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_013.html#filepos346695),\n[117](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_015.html#filepos414086)–[118](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_015.html#filepos417792),\n[139](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_016.html#filepos490909),\n[238](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos830602),\n[276](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos994080),\n[290](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1074309)\n\nRoosevelt, Franklin D.85\n\nRSA encryption scheme\n[80](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_012.html#filepos294863)\n\nRussell, Bertrand\n[6](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos49264),\n[87](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_012.html#filepos316500),\n[139](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_016.html#filepos490909),\n[277](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos999790)\n\n**S**\n\nSamuel, Arthur\n[12](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos71770)\n\nSandberg, Anders\n[265](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos929414),\n[267](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos941273),\n[272](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos969926),\n[274](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos981754)\n\nscanning, _see_ whole brain emulation (WBE)\n\nSchaeffer, Jonathan\n[12](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos71770)\n\nscheduling [15](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos82621)\n\nSchelling point\n[147](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_017.html#filepos515864),\n[183](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_018.html#filepos639789),\n[296](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1109694)\n\nScrabble [13](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos75444)\n\nsecond transition\n[176](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_018.html#filepos614830)–[178](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_018.html#filepos622135),\n[238](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos830602),\n[243](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos848268)–[245](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos855020),\n[252](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos879537)\n\nsecond-guessing (arguments)\n[238](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos830602)–[239](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos834026)\n\nseed AI\n[23](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos107115)–[29](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos128738),\n[36](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos148183),\n[75](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_011.html#filepos280832),\n[83](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_012.html#filepos302276),\n[92](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_013.html#filepos329935)–[96](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_013.html#filepos344390),\n[107](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_014.html#filepos379402),\n[116](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_015.html#filepos410668)–[120](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_015.html#filepos424758),\n[142](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_016.html#filepos501999),\n[151](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_017.html#filepos530369),\n[189](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos658717)–[198](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos688611),\n[201](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos688701)–[217](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_020.html#filepos755237),\n[224](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_020.html#filepos781205)–[225](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_020.html#filepos784988),\n[240](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos837779)–[241](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos841162),\n[266](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos935384),\n[274](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos981754),\n[275](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos987666),\n[282](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1028082)\n\nself-limiting goal\n[123](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_015.html#filepos436044)\n\nShakey (robot)\n[6](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos49264)\n\nSHRDLU (program)\n[6](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos49264)\n\nShulman, Carl\n[178](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_018.html#filepos622135)–[180](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_018.html#filepos629083),\n[265](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos929414),\n[287](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1056226),\n[300](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1132561),\n[302](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1144032),\n[304](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1155503)\n\nsimulation hypothesis\n[134](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_016.html#filepos473657)–[135](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_016.html#filepos477270),\n[143](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_016.html#filepos505499),\n[278](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos1005344),\n[288](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1062268),\n[292](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1085779)\n\nsingleton\n[78](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_012.html#filepos288933)–[90](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_012.html#filepos326953),\n[95](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_013.html#filepos341571)–[104](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_013.html#filepos370694),\n[112](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_014.html#filepos398105)–[114](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_014.html#filepos405024),\n[115](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_015.html#filepos408497)–[126](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_015.html#filepos446809),\n[136](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_016.html#filepos480353),\n[159](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_018.html#filepos555084),\n[176](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_018.html#filepos614830)–[184](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_018.html#filepos643382),\n[242](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos844785),\n[275](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos987666),\n[276](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos994080),\n[279](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos1011417),\n[281](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1022605),\n[287](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1056226),\n[299](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1126981),\n[301](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1138295),\n[303](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1149612)\n\ndefinition\n[78](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_012.html#filepos288933),\n[100](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_013.html#filepos356950)\n\nsingularity\n[1](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos33975),\n[2](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos36289),\n[49](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos192786),\n[75](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_011.html#filepos280832),\n[261](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos906855),\n[274](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos981754); _see\nalso_ intelligence explosion\n\nsocial signaling\n[110](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_014.html#filepos390344)\n\nsomatic gene therapy\n[42](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos167047)\n\nsovereign AI\n[148](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_017.html#filepos519506)–[158](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_017.html#filepos553359),\n[187](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos651630),\n[226](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_020.html#filepos788669),\n[285](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1045058)\n\nspeech recognition\n[15](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos82621)–[16](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos86180),\n[46](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos181760)\n\nspeed superintelligence\n[52](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_010.html#filepos201197)–[58](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_010.html#filepos221532),\n[75](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_011.html#filepos280832),\n[270](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos958624),\n[271](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos964280)\n\ndefinition\n[53](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_010.html#filepos203414)\n\nStrategic Defense Initiative (“Star Wars”)\n[86](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_012.html#filepos313010)\n\nstrong AI [18](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos93297)\n\nstunting\n[135](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_016.html#filepos477270)–[137](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_016.html#filepos483968),\n[143](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_016.html#filepos505499)\n\nsub-symbolic processing, _see_ connectionism\n\nsuperintelligence; _see also_ collective superintelligence, quality\nsuperintelligence _and_ speed superintelligence\n\ndefinition\n[22](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos104499),\n[52](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_010.html#filepos201197)\n\nforms [52](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_010.html#filepos201197),\n[59](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_010.html#filepos225304)\n\npaths to [22](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos104499),\n[50](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos196673)\n\npredicting the behavior of\n[108](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_014.html#filepos383193),\n[155](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_017.html#filepos541561),\n[302](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1144032)\n\nsuperorganisms\n[178](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_018.html#filepos622135)–[180](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_018.html#filepos629083)\n\nsuperpowers\n[52](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_010.html#filepos201197)–[56](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_010.html#filepos214335),\n[80](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_012.html#filepos294863),\n[86](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_012.html#filepos313010)–[87](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_012.html#filepos316500),\n[91](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_013.html#filepos327541)–[104](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_013.html#filepos370694),\n[119](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_015.html#filepos421149),\n[133](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_016.html#filepos469907),\n[148](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_017.html#filepos519506),\n[277](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos999790),\n[279](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos1011417),\n[296](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1109694)\n\ntypes [94](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_013.html#filepos338796)\n\nsurveillance\n[15](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos82621),\n[49](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos192786),\n[64](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_011.html#filepos241525),\n[82](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_012.html#filepos299697)–[85](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_012.html#filepos309319),\n[94](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_013.html#filepos338796),\n[117](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_015.html#filepos414086),\n[132](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_016.html#filepos466290),\n[181](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_018.html#filepos632632),\n[232](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos809163),\n[253](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos883031),\n[276](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos994080),\n[294](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1097891),\n[299](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1126981)\n\nSzilárd, Leó\n[85](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_012.html#filepos309319)\n\n**T**\n\nTD-Gammon [12](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos71770)\n\nTechnological Completion Conjecture\n[112](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_014.html#filepos398105)–[113](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_014.html#filepos401500),\n[229](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos798254)\n\ntechnology race\n[80](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_012.html#filepos294863)–[82](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_012.html#filepos299697),\n[86](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_012.html#filepos313010)–[90](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_012.html#filepos326953)\n[203](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos707772)–[205](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos714629),\n[231](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos805508),\n[246](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos858744)–[252](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos879537),\n[302](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1144032)\n\nteleological threads\n[110](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_014.html#filepos390344)\n\nTesauro, Gerry\n[12](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos71770)\n\nTextRunner (system)\n[71](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_011.html#filepos265356)\n\ntheorem prover\n[15](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos82621),\n[266](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos935384)\n\nthree laws of robotics\n[139](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_016.html#filepos490909),\n[284](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1039071)\n\nThrun, Sebastian\n[19](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos96736)\n\ntool-AI\n[151](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_017.html#filepos530369)–[158](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_017.html#filepos553359)\n\ndefinition\n[151](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_017.html#filepos530369)\n\ntreacherous turn\n[116](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_015.html#filepos410668)–[119](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_015.html#filepos421149),\n[128](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_016.html#filepos452137)\n\nTribolium castaneum\n[154](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_017.html#filepos541531)\n\ntripwires\n[137](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_016.html#filepos483968)–[143](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_016.html#filepos505499)\n\nTruman, Harry\n[85](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_012.html#filepos309319)\n\nTuring, Alan\n[4](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos41948),\n[23](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos107115),\n[29](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos128738),\n[44](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos174322),\n[225](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_020.html#filepos784988),\n[265](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos929414),\n[271](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos964280),\n[272](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos969926)\n\n**U**\n\nunemployment\n[65](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_011.html#filepos244846),\n[159](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_018.html#filepos555084)–[180](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_018.html#filepos629083),\n[287](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1056226)\n\nUnited Nations\n[87](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_012.html#filepos316500)–[89](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_012.html#filepos323404),\n[252](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos879537)–[253](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos883031)\n\nuniversal accelerator\n[233](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos812749)\n\nunmanned vehicle, _see_ drone\n\nuploading, _see_ whole brain emulation (WBE)\n\nutility function\n[10](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos65097)–[11](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos68753),\n[88](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_012.html#filepos319906),\n[100](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_013.html#filepos356950),\n[110](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_014.html#filepos390344),\n[119](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_015.html#filepos421149),\n[124](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_015.html#filepos439665)–[125](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_015.html#filepos443281),\n[133](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_016.html#filepos469907)–[134](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_016.html#filepos473657),\n[172](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_018.html#filepos600322),\n[185](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos645352)–[187](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos651630),\n[192](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos669343)–[208](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos725271),\n[290](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1074309),\n[292](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1085779),\n[293](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1091662),\n[303](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1149612)\n\n**V**\n\nvalue learning\n[191](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos665747)–[198](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos688611),\n[208](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos725271),\n[293](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1091662)\n\nvalue-accretion\n[189](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos658717)–[190](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos662079),\n[207](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos722040)\n\nvalue-loading\n[185](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos645352)–[208](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos725271),\n[293](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1091662),\n[294](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1097891)\n\nveil of ignorance\n[150](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_017.html#filepos526513),\n[156](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_017.html#filepos548381),\n[253](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos883031),\n[285](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1045058)\n\nVinge, Vernor\n[2](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos36289),\n[49](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos192786),\n[270](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos958624)\n\nvirtual reality\n[30](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos132462),\n[31](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos135923),\n[53](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_010.html#filepos203414),\n[113](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_014.html#filepos401500),\n[166](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_018.html#filepos578860),\n[171](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_018.html#filepos596704),\n[198](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos688611),\n[204](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos711076),\n[300](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1132561)\n\nvon Neumann probe\n[100](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_013.html#filepos356950)–[101](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_013.html#filepos361993),\n[113](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_014.html#filepos401500)\n\nvon Neumann, John\n[44](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos174322),\n[87](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_012.html#filepos316500),\n[114](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_014.html#filepos405024),\n[261](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos906855),\n[277](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos999790),\n[281](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1022605)\n\n**W**\n\nwages [65](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_011.html#filepos244846),\n[69](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_011.html#filepos258937),\n[160](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_018.html#filepos557644)–[169](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_018.html#filepos589401)\n\nWatson (IBM)\n[13](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos75444),\n[71](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_011.html#filepos265356)\n\nWBE, _see_ whole brain emulation (WBE)\n\nWhitehead, Alfred N.6\n\nwhole brain emulation (WBE)\n[28](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos125045)–[36](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos148183),\n[50](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_009.html#filepos196673),\n[60](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_010.html#filepos229392),\n[68](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_011.html#filepos255348)–[73](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_011.html#filepos273122),\n[77](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_011.html#filepos287200),\n[84](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_012.html#filepos305731)–[85](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_012.html#filepos309319),\n[108](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_014.html#filepos383193),\n[172](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_018.html#filepos600322),\n[198](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos688611),\n[201](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos688701)–[202](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos704501),\n[236](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos823424)–[245](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos855020),\n[252](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos879537),\n[266](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos935384),\n[267](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos941273),\n[274](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos981754),\n[299](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1126981),\n[300](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1132561),\n[301](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1138295)\n\nWigner, Eugene\n[85](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_012.html#filepos309319)\n\nwindfall clause\n[254](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_021.html#filepos886604),\n[303](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1149612)\n\nWinston, Patrick\n[18](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos93297)\n\nwire-heading\n[122](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_015.html#filepos432338)–[123](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_015.html#filepos436044),\n[133](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_016.html#filepos469907),\n[189](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos658717),\n[194](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos676391),\n[207](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos722040),\n[282](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1028082),\n[291](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1079986)\n\nwise-singleton sustainability threshold\n[100](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_013.html#filepos356950)–[104](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_013.html#filepos370694),\n[279](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos1011417)\n\nworld economy\n[2](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos36289)–[3](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_008.html#filepos40151),\n[63](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_011.html#filepos238595),\n[74](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_011.html#filepos276640),\n[83](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_012.html#filepos302276),\n[159](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_018.html#filepos555084)–[184](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_018.html#filepos643382),\n[274](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos981754),\n[277](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos999790),\n[285](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1045058)\n\n**Y**\n\nYudkowsky, Eliezer\n[70](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_011.html#filepos262469),\n[92](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_013.html#filepos329935),\n[98](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_013.html#filepos351316),\n[106](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_014.html#filepos376407),\n[197](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_019.html#filepos687167),\n[211](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_020.html#filepos733469)–[216](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_020.html#filepos751628),\n[266](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos935384),\n[273](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_023.html#filepos976118),\n[282](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1028082),\n[286](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1050502),\n[291](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1079986),\n[299](CR%21WRXXYATSYN7Q733PS9VMQYQC3QBZ_split_024.html#filepos1126981)\n\n\n\n\n",
    "book_id": "superintelligence",
    "book_title": "Superintelligence: Paths, Dangers, Strategies",
    "book_author": "Nick Bostrom",
    "topic_id": "ai_theory",
    "topic_label": "theory",
    "chunk_index": 5
  },
  {
    "chunk_full": "\n\n\nAtlas of AI\n\n\n# Atlas of AI\n\n_Power, Politics, and the Planetary Costs of Artificial Intelligence_\n\nKATE CRAWFORD\n\n![Images](../images/pub.jpg)\n\n\nCopyright © 2021 by Kate Crawford.  \nAll rights reserved.  \nThis book may not be reproduced, in whole or in part, including illustrations,\nin any form (beyond that copying permitted by Sections 107 and 108 of the U.S.\nCopyright Law and except by reviewers for the public press), without written\npermission from the publishers.\n\nYale University Press books may be purchased in quantity for educational,\nbusiness, or promotional use. For information, please e-mail\n[sales.press@yale.edu](mailto:sales.press@yale.edu) (U.S. office) or\n[sales@yaleup.co.uk](mailto:sales@yaleup.co.uk) (U.K. office).\n\nCover design and chapter opening illustrations by Vladan Joler.  \nSet in Minion by Tseng Information Systems, Inc.\n\nLibrary of Congress Control Number: 2020947842  \nISBN 978-0-300-20957-0 (hardcover : alk. paper)\n\nA catalogue record for this book is available from the British Library.\n\nThis paper meets the requirements of ANSI/NISO Z39.48-1992 (Permanence of\nPaper).\n\n\n_For Elliott and Margaret_\n\n\n## Contents\n\n[Introduction](intro.html)\n\n[ONE. Earth](ch01.html)\n\n[TWO. Labor](ch02.html)\n\n[THREE. Data](ch03.html)\n\n[FOUR. Classification](ch04.html)\n\n[FIVE. Affect](ch05.html)\n\n[SIX. State](ch06.html)\n\n[CONCLUSION. Power](concl.html)\n\n[CODA. Space](code.html)\n\n[_Acknowledgments_](ack.html)\n\n[_Notes_](notes.html)\n\n[_Bibliography_](bib.html)\n\n[_Index_](index.html)\n\n\n![Images](../images/f0viii-01.jpg)\n\n\n## Introduction\n\n### The Smartest Horse in the World\n\nAt the end of the nineteenth century, Europe was captivated by a horse called\nHans. “Clever Hans” was nothing less than a marvel: he could solve math\nproblems, tell time, identify days on a calendar, differentiate musical tones,\nand spell out words and sentences. People flocked to watch the German stallion\ntap out answers to complex problems with his hoof and consistently arrive at\nthe right answer. “What is two plus three?” Hans would diligently tap his hoof\non the ground five times. “What day of the week is it?” The horse would then\ntap his hoof to indicate each letter on a purpose-built letter board and spell\nout the correct answer. Hans even mastered more complex questions, such as, “I\nhave a number in mind. I subtract nine and have three as a remainder. What is\nthe number?” By 1904, Clever Hans was an international celebrity, with the\n_New York Times_ championing him as “Berlin’s Wonderful Horse; He Can Do\nAlmost Everything but Talk.”[1](notes.html#iintronotes1)\n\nHans’s trainer, a retired math teacher named Wilhelm von Osten, had long been\nfascinated by animal intelligence. Von Osten had tried and failed to teach\nkittens and bear cubs cardinal numbers, but it wasn’t until he started working\nwith his own horse that he had success. He first taught Hans to count by\nholding the animal’s leg, showing him a number, and then tapping on the hoof\nthe correct number of times. Soon Hans responded by accurately tapping out\nsimple sums. Next von Osten introduced a chalkboard with the alphabet spelled\nout, so Hans could tap a number for each letter on the board. After two years\nof training, von Osten was astounded by the animal’s strong grasp of advanced\nintellectual concepts. So he took Hans on the road as proof that animals could\nreason. Hans became the viral sensation of the belle époque.\n\nBut many people were skeptical, and the German board of education launched an\ninvestigative commission to test Von Osten’s scientific claims. The Hans\nCommission was led by the psychologist and philosopher Carl Stumpf and his\nassistant Oskar Pfungst, and it included a circus manager, a retired\nschoolteacher, a zoologist, a veterinarian, and a cavalry officer. Yet after\nextensive questioning of Hans, both with his trainer present and without, the\nhorse maintained his record of correct answers, and the commission could find\nno evidence of deception. As Pfungst later wrote, Hans performed in front of\n“thousands of spectators, horse-fanciers, trick-trainers of first rank, and\nnot one of them during the course of many months’ observations are able to\ndiscover any kind of regular signal” between the questioner and the\nhorse.[2](notes.html#iintronotes2)\n\nThe commission found that the methods Hans had been taught were more like\n“teaching children in elementary schools” than animal training and were\n“worthy of scientific examination.”[3](notes.html#iintronotes3) But Strumpf\nand Pfungst still had doubts. One finding in particular troubled them: when\nthe questioner did not know the answer or was standing far away, Hans rarely\ngave the correct answer. This led Pfungst and Strumpf to consider whether some\nsort of unintentional signal had been providing Hans with the answers.\n\n![Images](../images/f0003-01.jpg)\n\nWilhelm von Osten and Clever Hans\n\nAs Pfungst would describe in his 1911 book, their intuition was right: the\nquestioner’s posture, breathing, and facial expression would subtly change\naround the moment Hans reached the right answer, prompting Hans to stop\nthere.[4](notes.html#iintronotes4) Pfungst later tested this hypothesis on\nhuman subjects and confirmed his result. What fascinated him most about this\ndiscovery was that questioners were generally unaware that they were providing\npointers to the horse. The solution to the Clever Hans riddle, Pfungst wrote,\nwas the unconscious direction from the horse’s\nquestioners.[5](notes.html#iintronotes5) The horse was trained to produce the\nresults his owner wanted to see, but audiences felt that this was not the\nextraordinary intelligence they had imagined.\n\nThe story of Clever Hans is compelling from many angles: the relationship\nbetween desire, illusion, and action, the business of spectacles, how we\nanthropomorphize the nonhuman, how biases emerge, and the politics of\nintelligence. Hans inspired a term in psychology for a particular type of\nconceptual trap, the Clever Hans Effect or observer-expectancy effect, to\ndescribe the influence of experimenters’ unintentional cues on their subjects.\nThe relationship between Hans and von Osten points to the complex mechanisms\nby which biases find their ways into systems and how people become entangled\nwith the phenomena they study. The story of Hans is now used in machine\nlearning as a cautionary reminder that you can’t always be sure of what a\nmodel has learned from the data it has been given.[6](notes.html#iintronotes6)\nEven a system that appears to perform spectacularly in training can make\nterrible predictions when presented with novel data in the world.\n\nThis opens a central question of this book: How is intelligence “made,” and\nwhat traps can that create? At first glance, the story of Clever Hans is a\nstory of how one man constructed intelligence by training a horse to follow\ncues and emulate humanlike cognition. But at another level, we see that the\npractice of making intelligence was considerably broader. The endeavor\nrequired validation from multiple institutions, including academia, schools,\nscience, the public, and the military. Then there was the market for von Osten\nand his remarkable horse—emotional and economic investments that drove the\ntours, the newspaper stories, and the lectures. Bureaucratic authorities were\nassembled to measure and test the horse’s abilities. A constellation of\nfinancial, cultural, and scientific interests had a part to play in the\nconstruction of Hans’s intelligence and a stake in whether it was truly\nremarkable.\n\nWe can see two distinct mythologies at work. The first myth is that nonhuman\nsystems (be it computers or horses) are analogues for human minds. This\nperspective assumes that with sufficient training, or enough resources,\nhumanlike intelligence can be created from scratch, without addressing the\nfundamental ways in which humans are embodied, relational, and set within\nwider ecologies. The second myth is that intelligence is something that exists\nindependently, as though it were natural and distinct from social, cultural,\nhistorical, and political forces. In fact, the concept of intelligence has\ndone inordinate harm over centuries and has been used to justify relations of\ndomination from slavery to eugenics.[7](notes.html#iintronotes7)\n\nThese mythologies are particularly strong in the field of artificial\nintelligence, where the belief that human intelligence can be formalized and\nreproduced by machines has been axiomatic since the mid-twentieth century.\nJust as Hans’s intelligence was considered to be like that of a human,\nfostered carefully like a child in elementary school, so AI systems have\nrepeatedly been described as simple but humanlike forms of intelligence. In\n1950, Alan Turing predicted that “at the end of the century the use of words\nand general educated opinion will have altered so much that one will be able\nto speak of machines thinking without expecting to be\ncontradicted.”[8](notes.html#iintronotes8) The mathematician John von Neumann\nclaimed in 1958 that the human nervous system is “prima facie\ndigital.”[9](notes.html#iintronotes9) MIT professor Marvin Minsky once\nresponded to the question of whether machines could think by saying, “Of\ncourse machines can think; we can think and we are ‘meat\nmachines.’”[10](notes.html#iintronotes10) But not everyone was convinced.\nJoseph Weizenbaum, early AI inventor and creator of the first chatbot program,\nknown as ELIZA, believed that the idea of humans as mere information\nprocessing systems is far too simplistic a notion of intelligence and that it\ndrove the “perverse grand fantasy” that AI scientists could create a machine\nthat learns “as a child does.”[11](notes.html#iintronotes11)\n\nThis has been one of the core disputes in the history of artificial\nintelligence. In 1961, MIT hosted a landmark lecture series titled “Management\nand the Computer of the Future.” A stellar lineup of computer scientists\nparticipated, including Grace Hopper, J. C. R. Licklider, Marvin Minsky, Allen\nNewell, Herbert Simon, and Norbert Wiener, to discuss the rapid advances being\nmade in digital computing. At its conclusion, John McCarthy boldly argued that\nthe differences between human and machine tasks were illusory. There were\nsimply some complicated human tasks that would take more time to be formalized\nand solved by machines.[12](notes.html#iintronotes12)\n\nBut philosophy professor Hubert Dreyfus argued back, concerned that the\nassembled engineers “do not even consider the possibility that the brain might\nprocess information in an entirely different way than a\ncomputer.”[13](notes.html#iintronotes13) In his later work _What Computers\nCan’t Do_ , Dreyfus pointed out that human intelligence and expertise rely\nheavily on many unconscious and subconscious processes, while computers\nrequire all processes and data to be explicit and\nformalized.[14](notes.html#iintronotes14) As a result, less formal aspects of\nintelligence must be abstracted, eliminated, or approximated for computers,\nleaving them unable to process information about situations as humans do.\n\nMuch in AI has changed since the 1960s, including a shift from symbolic\nsystems to the more recent wave of hype about machine learning techniques. In\nmany ways, the early fights over what AI can do have been forgotten and the\nskepticism has melted away. Since the mid-2000s, AI has rapidly expanded as a\nfield in academia and as an industry. Now a small number of powerful\ntechnology corporations deploy AI systems at a planetary scale, and their\nsystems are once again hailed as comparable or even superior to human\nintelligence.\n\nYet the story of Clever Hans also reminds us how narrowly we consider or\nrecognize intelligence. Hans was taught to mimic tasks within a very\nconstrained range: add, subtract, and spell words. This reflects a limited\nperspective of what horses or humans can do. Hans was already performing\nremarkable feats of interspecies communication, public performance, and\nconsiderable patience, yet these were not recognized as intelligence. As\nauthor and engineer Ellen Ullman puts it, this belief that the mind is like a\ncomputer, and vice versa, has “infected decades of thinking in the computer\nand cognitive sciences,” creating a kind of original sin for the\nfield.[15](notes.html#iintronotes15) It is the ideology of Cartesian dualism\nin artificial intelligence: where AI is narrowly understood as disembodied\nintelligence, removed from any relation to the material world.\n\n### What Is AI? Neither Artificial nor Intelligent\n\nLet’s ask the deceptively simple question, What is artificial intelligence? If\nyou ask someone in the street, they might mention Apple’s Siri, Amazon’s cloud\nservice, Tesla’s cars, or Google’s search algorithm. If you ask experts in\ndeep learning, they might give you a technical response about how neural nets\nare organized into dozens of layers that receive labeled data, are assigned\nweights and thresholds, and can classify data in ways that cannot yet be fully\nexplained.[16](notes.html#iintronotes16) In 1978, when discussing expert\nsystems, Professor Donald Michie described AI as knowledge refining, where “a\nreliability and competence of codification can be produced which far surpasses\nthe highest level that the unaided human expert has ever, perhaps even could\never, attain.”[17](notes.html#iintronotes17) In one of the most popular\ntextbooks on the subject, Stuart Russell and Peter Norvig state that AI is the\nattempt to understand and build intelligent entities. “Intelligence is\nconcerned mainly with rational action,” they claim. “Ideally, an intelligent\nagent takes the best possible action in a\nsituation.”[18](notes.html#iintronotes18)\n\nEach way of defining artificial intelligence is doing work, setting a frame\nfor how it will be understood, measured, valued, and governed. If AI is\ndefined by consumer brands for corporate infrastructure, then marketing and\nadvertising have predetermined the horizon. If AI systems are seen as more\nreliable or rational than any human expert, able to take the “best possible\naction,” then it suggests that they should be trusted to make high-stakes\ndecisions in health, education, and criminal justice. When specific\nalgorithmic techniques are the sole focus, it suggests that only continual\ntechnical progress matters, with no consideration of the computational cost of\nthose approaches and their far-reaching impacts on a planet under strain.\n\nIn contrast, in this book I argue that AI is neither _artificial_ nor\n_intelligent._ Rather, artificial intelligence is both embodied and material,\nmade from natural resources, fuel, human labor, infrastructures, logistics,\nhistories, and classifications. AI systems are not autonomous, rational, or\nable to discern anything without extensive, computationally intensive training\nwith large datasets or predefined rules and rewards. In fact, artificial\nintelligence as we know it depends entirely on a much wider set of political\nand social structures. And due to the capital required to build AI at scale\nand the ways of seeing that it optimizes AI systems are ultimately designed to\nserve existing dominant interests. In this sense, artificial intelligence is a\nregistry of power.\n\nIn this book we’ll explore how artificial intelligence is made, in the widest\nsense, and the economic, political, cultural, and historical forces that shape\nit. Once we connect AI within these broader structures and social systems, we\ncan escape the notion that artificial intelligence is a purely technical\ndomain. At a fundamental level, AI is technical and social practices,\ninstitutions and infrastructures, politics and culture. Computational reason\nand embodied work are deeply interlinked: AI systems both reflect and produce\nsocial relations and understandings of the world.\n\nIt’s worth noting that the term “artificial intelligence” can create\ndiscomfort in the computer science community. The phrase has moved in and out\nof fashion over the decades and is used more in marketing than by researchers.\n“Machine learning” is more commonly used in the technical literature. Yet the\nnomenclature of AI is often embraced during funding application season, when\nventure capitalists come bearing checkbooks, or when researchers are seeking\npress attention for a new scientific result. As a result, the term is both\nused and rejected in ways that keep its meaning in flux. For my purposes, I\nuse AI to talk about the massive industrial formation that includes politics,\nlabor, culture, and capital. When I refer to machine learning, I’m speaking of\na range of technical approaches (which are, in fact, social and\ninfrastructural as well, although rarely spoken about as such).\n\nBut there are significant reasons _why_ the field has been focused so much on\nthe technical—algorithmic breakthroughs, incremental product improvements, and\ngreater convenience. The structures of power at the intersection of\ntechnology, capital, and governance are well served by this narrow, abstracted\nanalysis. To understand how AI is fundamentally political, we need to go\nbeyond neural nets and statistical pattern recognition to instead ask _what_\nis being optimized, and _for whom_ , and _who_ gets to decide. Then we can\ntrace the implications of those choices.\n\n### Seeing AI Like an Atlas\n\nHow can an atlas help us to understand how artificial intelligence is made? An\natlas is an unusual type of book. It is a collection of disparate parts, with\nmaps that vary in resolution from a satellite view of the planet to a zoomed-\nin detail of an archipelago. When you open an atlas, you may be seeking\nspecific information about a particular place—or perhaps you are wandering,\nfollowing your curiosity, and finding unexpected pathways and new\nperspectives. As historian of science Lorraine Daston observes, all scientific\natlases seek to school the eye, to focus the observer’s attention on\nparticular telling details and significant\ncharacteristics.[19](notes.html#iintronotes19) An atlas presents you with a\nparticular viewpoint of the world, with the imprimatur of science—scales and\nratios, latitudes and longitudes—and a sense of form and consistency.\n\nYet an atlas is as much an act of creativity—a subjective, political, and\naesthetic intervention—as it is a scientific collection. The French\nphilosopher Georges Didi-Huberman thinks of the atlas as something that\ninhabits the aesthetic paradigm of the visual and the epistemic paradigm of\nknowledge. By implicating both, it undermines the idea that science and art\nare ever completely separate.[20](notes.html#iintronotes20) Instead, an atlas\noffers us the possibility of rereading the world, linking disparate pieces\ndifferently and “reediting and piecing it together again without thinking we\nare summarizing or exhausting it.”[21](notes.html#iintronotes21)\n\nPerhaps my favorite account of how a cartographic approach can be helpful\ncomes from the physicist and technology critic Ursula Franklin: “Maps\nrepresent purposeful endeavors: they are meant to be useful, to assist the\ntraveler and bridge the gap between the known and the as yet unknown; they are\ntestaments of collective knowledge and insight.”[22](notes.html#iintronotes22)\n\nMaps, at their best, offer us a compendium of open pathways—shared ways of\nknowing—that can be mixed and combined to make new interconnections. But there\nare also maps of domination, those national maps where territory is carved\nalong the fault lines of power: from the direct interventions of drawing\nborders across contested spaces to revealing the colonial paths of empires. By\ninvoking an atlas, I’m suggesting that we need new ways to understand the\nempires of artificial intelligence. We need a theory of AI that accounts for\nthe states and corporations that drive and dominate it, the extractive mining\nthat leaves an imprint on the planet, the mass capture of data, and the\nprofoundly unequal and increasingly exploitative labor practices that sustain\nit. These are the shifting tectonics of power in AI. A topographical approach\noffers different perspectives and scales, beyond the abstract promises of\nartificial intelligence or the latest machine learning models. The aim is to\nunderstand AI in a wider context by walking through the many different\nlandscapes of computation and seeing how they\nconnect.[23](notes.html#iintronotes23)\n\nThere’s another way in which atlases are relevant here. The field of AI is\nexplicitly attempting to capture the planet in a computationally legible form.\nThis is not a metaphor so much as the industry’s direct ambition. The AI\nindustry is making and normalizing its own proprietary maps, as a centralized\nGod’s-eye view of human movement, communication, and labor. Some AI scientists\nhave stated their desire to capture the world and to supersede other forms of\nknowing. AI professor Fei-Fei Li describes her ImageNet project as aiming to\n“map out the entire world of objects.”[24](notes.html#iintronotes24) In their\ntextbook, Russell and Norvig describe artificial intelligence as “relevant to\nany intellectual task; it is truly a universal\nfield.”[25](notes.html#iintronotes25) One of the founders of artificial\nintelligence and early experimenter in facial recognition, Woody Bledsoe, put\nit most bluntly: “in the long run, AI is the _only_\nscience.”[26](notes.html#iintronotes26) This is a desire not to create an\natlas of the world but to be _the_ atlas—the dominant way of seeing. This\ncolonizing impulse centralizes power in the AI field: it determines how the\nworld is measured and defined while simultaneously denying that this is an\ninherently political activity.\n\nInstead of claiming universality, this book is a partial account, and by\nbringing you along on my investigations, I hope to show you how my views were\nformed. We will encounter well-visited and lesser-known landscapes of\ncomputation: the pits of mines, the long corridors of energy-devouring data\ncenters, skull archives, image databases, and the fluorescent-lit hangars of\ndelivery warehouses. These sites are included not just to illustrate the\nmaterial construction of AI and its ideologies but also to “illuminate the\nunavoidably subjective and political aspects of mapping, and to provide\nalternatives to hegemonic, authoritative—and often naturalized and\nreified—approaches,” as media scholar Shannon Mattern\nwrites.[27](notes.html#iintronotes27)\n\nModels for understanding and holding systems accountable have long rested on\nideals of transparency. As I’ve written with the media scholar Mike Ananny,\nbeing able to _see_ a system is sometimes equated with being able to know how\nit works and how to govern it.[28](notes.html#iintronotes28) But this tendency\nhas serious limitations. In the case of AI, there is no singular black box to\nopen, no secret to expose, but a multitude of interlaced systems of power.\nComplete transparency, then, is an impossible goal. Rather, we gain a better\nunderstanding of AI’s role in the world by engaging with its material\narchitectures, contextual environments, and prevailing politics and by tracing\nhow they are connected.\n\nMy thinking in this book has been informed by the disciplines of science and\ntechnology studies, law, and political philosophy and from my experience\nworking in both academia and an industrial AI research lab for almost a\ndecade. Over those years, many generous colleagues and communities have\nchanged the way I see the world: mapping is always a collective exercise, and\nthis is no exception.[29](notes.html#iintronotes29) I’m grateful to the\nscholars who created new ways to understand sociotechnical systems, including\nGeoffrey Bowker, Benjamin Bratton, Wendy Chun, Lorraine Daston, Peter Galison,\nIan Hacking, Stuart Hall, Donald MacKenzie, Achille Mbembé, Alondra Nelson,\nSusan Leigh Star, and Lucy Suchman, among many others. This book benefited\nfrom many in-person conversations and reading the recent work by authors\nstudying the politics of technology, including Mark Andrejevic, Ruha Benjamin,\nMeredith Broussard, Simone Browne, Julie Cohen, Sasha Costanza-Chock, Virginia\nEubanks, Tarleton Gillespie, Mar Hicks, Tung-Hui Hu, Yuk Hui, Safiya Umoja\nNoble, and Astra Taylor.\n\nAs with any book, this one emerges from a specific lived experience that\nimposes limitations. As someone who has lived and worked in the United States\nfor the past decade, my focus skews toward the AI industry in Western centers\nof power. But my aim is not to create a complete global atlas—the very idea\ninvokes capture and colonial control. Instead, any author’s view can be only\npartial, based on local observations and interpretations, in what\nenvironmental geographer Samantha Saville calls a “humble geography” that\nacknowledges one’s specific perspectives rather than claiming objectivity or\nmastery.[30](notes.html#iintronotes30)\n\nJust as there are many ways to make an atlas, so there are many possible\nfutures for how AI will be used in the world. The expanding reach of AI\nsystems may seem inevitable, but this is contestable and incomplete. The\nunderlying visions of the AI field do not come into being autonomously but\ninstead have been constructed from a particular set of beliefs and\nperspectives. The chief designers of the contemporary atlas of AI are a small\nand homogenous group of people, based in a handful of cities, working in an\nindustry that is currently the wealthiest in the world. Like medieval European\n_mappae mundi_ , which illustrated religious and classical concepts as much as\ncoordinates, the maps made by the AI industry are political interventions, as\nopposed to neutral reflections of the world. This book is made against the\nspirit of colonial mapping logics, and it embraces different stories,\nlocations, and knowledge bases to better understand the role of AI in the\nworld.\n\n![Images](../images/f0014-01.jpg)\n\nHeinrich Bünting’s _mappa mundi_ , known as _The Bünting Clover Leaf Map_ ,\nwhich symbolizes the Christian Trinity, with the city of Jerusalem at the\ncenter of the world. From _Itinerarium Sacrae Scripturae_ (Magdeburg, 1581)\n\n### Topographies of Computation\n\nHow, at this moment in the twenty-first century, is AI conceptualized and\nconstructed? What is at stake in the turn to artificial intelligence, and what\nkinds of politics are contained in the way these systems map and interpret the\nworld? What are the social and material consequences of including AI and\nrelated algorithmic systems into the decision-making systems of social\ninstitutions like education and health care, finance, government operations,\nworkplace interactions and hiring, communication systems, and the justice\nsystem? This book is not a story about code and algorithms or the latest\nthinking in computer vision or natural language processing or reinforcement\nlearning. Many other books do that. Neither is it an ethnographic account of a\nsingle community and the effects of AI on their experience of work or housing\nor medicine—although we certainly need more of those.\n\nInstead, this is an expanded view of artificial intelligence as an _extractive\nindustry._ The creation of contemporary AI systems depends on exploiting\nenergy and mineral resources from the planet, cheap labor, and data at scale.\nTo observe this in action, we will go on a series of journeys to places that\nreveal the makings of AI.\n\nIn [chapter 1](ch01.html), we begin in the lithium mines of Nevada, one of the\nmany sites of mineral extraction needed to power contemporary computation.\nMining is where we see the extractive politics of AI at their most literal.\nThe tech sector’s demand for rare earth minerals, oil, and coal is vast, but\nthe true costs of this extraction is never borne by the industry itself. On\nthe software side, building models for natural language processing and\ncomputer vision is enormously energy hungry, and the competition to produce\nfaster and more efficient models has driven computationally greedy methods\nthat expand AI’s carbon footprint. From the last trees in Malaysia that were\nharvested to produce latex for the first transatlantic undersea cables to the\ngiant artificial lake of toxic residues in Inner Mongolia, we trace the\nenvironmental and human birthplaces of planetary computation networks and see\nhow they continue to terraform the planet.\n\n[Chapter 2](ch02.html) shows how artificial intelligence is made of human\nlabor. We look at the digital pieceworkers paid pennies on the dollar clicking\non microtasks so that data systems can seem more intelligent than they\nare.[31](notes.html#iintronotes31) Our journey will take us inside the Amazon\nwarehouses where employees must keep in time with the algorithmic cadences of\na vast logistical empire, and we will visit the Chicago meat laborers on the\ndisassembly lines where animal carcasses are vivisected and prepared for\nconsumption. And we’ll hear from the workers who are protesting against the\nway that AI systems are increasing surveillance and control for their bosses.\n\nLabor is also a story about time. Coordinating the actions of humans with the\nrepetitive motions of robots and line machinery has always involved a\ncontrolling of bodies in space and time.[32](notes.html#iintronotes32) From\nthe invention of the stopwatch to Google’s TrueTime, the process of time\ncoordination is at the heart of workplace management. AI technologies both\nrequire and create the conditions for ever more granular and precise\nmechanisms of temporal management. Coordinating time demands increasingly\ndetailed information about what people are doing and how and when they do it.\n\n[Chapter 3](ch03.html) focuses on the role of data. All publicly accessible\ndigital material—including data that is personal or potentially damaging—is\nopen to being harvested for training datasets that are used to produce AI\nmodels. There are gigantic datasets full of people’s selfies, of hand\ngestures, of people driving cars, of babies crying, of newsgroup conversations\nfrom the 1990s, all to improve algorithms that perform such functions as\nfacial recognition, language prediction, and object detection. When these\ncollections of data are no longer seen as people’s personal material but\nmerely as _infrastructure_ , the specific meaning or context of an image or a\nvideo is assumed to be irrelevant. Beyond the serious issues of privacy and\nongoing surveillance capitalism, the current practices of working with data in\nAI raise profound ethical, methodological, and epistemological\nconcerns.[33](notes.html#iintronotes33)\n\nAnd how is all this data used? In chapter 4, we look at the practices of\nclassification in artificial intelligence systems, what sociologist Karin\nKnorr Cetina calls the “epistemic machinery.”[34](notes.html#iintronotes34) We\nsee how contemporary systems use labels to predict human identity, commonly\nusing binary gender, essentialized racial categories, and problematic\nassessments of character and credit worthiness. A sign will stand in for a\nsystem, a proxy will stand for the real, and a toy model will be asked to\nsubstitute for the infinite complexity of human subjectivity. By looking at\nhow classifications are made, we see how technical schemas enforce hierarchies\nand magnify inequity. Machine learning presents us with a regime of normative\nreasoning that, when in the ascendant, takes shape as a powerful governing\nrationality.\n\nFrom here, we travel to the hill towns of Papua New Guinea to explore the\nhistory of affect recognition, the idea that facial expressions hold the key\nto revealing a person’s inner emotional state. Chapter 5 considers the claim\nof the psychologist Paul Ekman that there are a small set of universal\nemotional states which can be read directly from the face. Tech companies are\nnow deploying this idea in affect recognition systems, as part of an industry\npredicted to be worth more than seventeen billion\ndollars.[35](notes.html#iintronotes35) But there is considerable scientific\ncontroversy around emotion detection, which is at best incomplete and at worst\nmisleading. Despite the unstable premise, these tools are being rapidly\nimplemented into hiring, education, and policing systems.\n\nIn chapter 6 we look at the ways in which AI systems are used as a tool of\nstate power. The military past and present of artificial intelligence have\nshaped the practices of surveillance, data extraction, and risk assessment we\nsee today. The deep interconnections between the tech sector and the military\nare now being reined in to fit a strong nationalist agenda. Meanwhile,\nextralegal tools used by the intelligence community have now dispersed, moving\nfrom the military world into the commercial technology sector, to be used in\nclassrooms, police stations, workplaces, and unemployment offices. The\nmilitary logics that have shaped AI systems are now part of the workings of\nmunicipal government, and they are further skewing the relation between states\nand subjects.\n\nThe concluding chapter assesses how artificial intelligence functions as a\nstructure of power that combines infrastructure, capital, and labor. From the\nUber driver being nudged to the undocumented immigrant being tracked to the\npublic housing tenants contending with facial recognition systems in their\nhomes, AI systems are built with the logics of capital, policing, and\nmilitarization—and this combination further widens the existing asymmetries of\npower. These ways of seeing depend on the twin moves of abstraction and\nextraction: abstracting away the material conditions of their making while\nextracting more information and resources from those least able to resist.\n\nBut these logics can be challenged, just as systems that perpetuate oppression\ncan be rejected. As conditions on Earth change, calls for data protection,\nlabor rights, climate justice, and racial equity should be heard together.\nWhen these interconnected movements for justice inform how we understand\nartificial intelligence, different conceptions of planetary politics become\npossible.\n\n### Extraction, Power, and Politics\n\nArtificial intelligence, then, is an idea, an infrastructure, an industry, a\nform of exercising power, and a way of seeing; it’s also a manifestation of\nhighly organized capital backed by vast systems of extraction and logistics,\nwith supply chains that wrap around the entire planet. All these things are\npart of what artificial intelligence is—a two-word phrase onto which is mapped\na complex set of expectations, ideologies, desires, and fears.\n\nAI can seem like a spectral force—as disembodied computation—but these systems\nare anything but abstract. They are physical infrastructures that are\nreshaping the Earth, while simultaneously shifting how the world is seen and\nunderstood.\n\nIt’s important for us to contend with these many aspects of artificial\nintelligence—its malleability, its messiness, and its spatial and temporal\nreach. The promiscuity of AI as a term, its openness to being reconfigured,\nalso means that it can be put to use in a range of ways: it can refer to\neverything from consumer devices like the Amazon Echo to nameless back-end\nprocessing systems, from narrow technical papers to the biggest industrial\ncompanies in the world. But this has its usefulness, too. The breadth of the\nterm “artificial intelligence” gives us license to consider all these elements\nand how they are deeply imbricated: from the politics of intelligence to the\nmass harvesting of data; from the industrial concentration of the tech sector\nto geopolitical military power; from the deracinated environment to ongoing\nforms of discrimination.\n\nThe task is to remain sensitive to the terrain and to watch the shifting and\nplastic meanings of the term “artificial intelligence”—like a container into\nwhich various things are placed and then removed—because that, too, is part of\nthe story.\n\nSimply put, artificial intelligence is now a player in the shaping of\nknowledge, communication, and power. These reconfigurations are occurring at\nthe level of epistemology, principles of justice, social organization,\npolitical expression, culture, understandings of human bodies, subjectivities,\nand identities: what we are and what we can be. But we can go further.\nArtificial intelligence, in the process of remapping and intervening in the\nworld, is politics by other means—although rarely acknowledged as such. These\npolitics are driven by the Great Houses of AI, which consist of the half-dozen\nor so companies that dominate large-scale planetary computation.\n\nMany social institutions are now influenced by these tools and methods, which\nshape what they value and how decisions are made while creating a complex\nseries of downstream effects. The intensification of technocratic power has\nbeen under way for a long time, but the process has now accelerated. In part\nthis is due to the concentration of industrial capital at a time of economic\nausterity and outsourcing, including the defunding of social welfare systems\nand institutions that once acted as a check on market power. This is why we\nmust contend with AI as a political, economic, cultural, and scientific force.\nAs Alondra Nelson, Thuy Linh Tu, and Alicia Headlam Hines observe, “Contests\naround technology are always linked to larger struggles for economic mobility,\npolitical maneuvering, and community building.”[36](notes.html#iintronotes36)\n\nWe are at a critical juncture, one that requires us to ask hard questions\nabout the way AI is produced and adopted. We need to ask: What is AI? What\nforms of politics does it propagate? Whose interests does it serve, and who\nbears the greatest risk of harm? And where should the use of AI be\nconstrained? These questions will not have easy answers. But neither is this\nan irresolvable situation or a point of no return—dystopian forms of thinking\ncan paralyze us from taking action and prevent urgently needed\ninterventions.[37](notes.html#iintronotes37) As Ursula Franklin writes, “The\nviability of technology, like democracy, depends in the end on the practice of\njustice and on the enforcement of limits to\npower.”[38](notes.html#iintronotes38)\n\nThis book argues that addressing the foundational problems of AI and planetary\ncomputation requires connecting issues of power and justice: from epistemology\nto labor rights, resource extraction to data protections, racial inequity to\nclimate change. To do that, we need to expand our understanding of what is\nunder way in the empires of AI, to see what is at stake, and to make better\ncollective decisions about what should come next.\n\n\n![Images](../images/f0022-01.jpg)\n\n\n## 1  \nEarth\n\nThe Boeing 757 banks right over San Jose on its final approach to San\nFrancisco International Airport. The left wing drops as the plane lines up\nwith the runway, revealing an aerial view of the tech sector’s most iconic\nlocation. Below are the great empires of Silicon Valley. The gigantic black\ncircle of Apple’s headquarters is laid out like an uncapped camera lens,\nglistening in the sun. Then there’s Google’s head office, nestled close to\nNASA’s Moffett Federal Airfield. This was once a key site for the U.S. Navy\nduring World War II and the Korean War, but now Google has a sixty-year lease\non it, and senior executives park their private planes here. Arrayed near\nGoogle are the large manufacturing sheds of Lockheed Martin, where the\naerospace and weapons manufacturing company builds hundreds of orbital\nsatellites destined to look down on the activities of Earth. Next, by the\nDumbarton Bridge, appears a collection of squat buildings that are home to\nFacebook, ringed with massive parking lots close to the sulfuric salt ponds of\nthe Ravenswood Slough. From this vantage point, the nondescript suburban cul-\nde-sacs and industrial midrise skyline of Palo Alto betray little of its true\nwealth, power, and influence. There are only a few hints of its centrality in\nthe global economy and in the computational infrastructure of the planet.\n\nI’m here to learn about artificial intelligence and what it is made from. To\nsee that, I will need to leave Silicon Valley altogether.\n\nFrom the airport, I jump into a van and drive east. I cross the San\nMateo–Hayward Bridge and pass by the Lawrence Livermore National Laboratory,\nwhere Edward Teller directed his research into thermonuclear weapons in the\nyears after World War II. Soon the Sierra Nevada foothills rise beyond the\nCentral Valley towns of Stockton and Manteca. Here the roads start winding up\nthrough the tall granite cliffs of the Sonora Pass and down the eastern side\nof the mountains toward grassy valleys dotted with golden poppies. Pine\nforests give way to the alkaline waters of Mono Lake and the parched desert\nlandforms of the Basin and Range. To refuel, I pull into Hawthorne, Nevada,\nsite of the world’s biggest ammunition depot, where the U.S. Army stores\narmaments in dozens of dirt-covered ziggurats that populate the valley in neat\nrows. Driving along Nevada State Route 265 I see a lone VORTAC in the\ndistance, a large bowling pin–shaped radio tower that was designed for the era\nbefore GPS. It has a single function: it broadcasts “I am here” to all passing\naircraft, a fixed point of reference in a lonely terrain.\n\nMy destination is the unincorporated community of Silver Peak in Nevada’s\nClayton Valley, where about 125 people live, depending on how you count. The\nmining town, one of the oldest in Nevada, was almost abandoned in 1917 after\nthe ground was stripped bare of silver and gold. A few gold rush buildings\nstill stand, eroding under the desert sun. The town may be small, with more\njunked cars than people, but it harbors something exceedingly rare. Silver\nPeak is perched on the edge of a massive underground lake of lithium. The\nvaluable lithium brine under the surface is pumped out of the ground and left\nin open, iridescent green ponds to evaporate. From miles away, the ponds can\nbe seen when they catch the light and shimmer. Up close, it’s a different\nview. Alien-looking black pipes erupt from the ground and snake along the\nsalt-encrusted earth, moving in and out of shallow trenches, ferrying the\nsalty cocktail to its drying pans.\n\n![Images](../images/f0025-01.jpg)\n\nSilver Peak Lithium Mine. Photograph by Kate Crawford\n\nHere, in a remote pocket of Nevada, is a place where the stuff of AI is made.\n\n### Mining for AI\n\nClayton Valley is connected to Silicon Valley in much the way that the\nnineteenth-century goldfields were to early San Francisco. The history of\nmining, like the devastation it leaves in its wake, is commonly overlooked in\nthe strategic amnesia that accompanies stories of technological progress. As\nhistorical geographer Gray Brechin points out, San Francisco was built from\nthe gains of pulling gold and silver out of the lands of California and Nevada\nin the 1800s.[1](notes.html#ich01notes1) The city is made from mining. Those\nsame lands had been taken from Mexico under the Treaty of Guadalupe Hidalgo in\n1848 at the end of the Mexican-American War, when it was already clear to the\nsettlers that these would be highly valuable goldfields. It was a textbook\nexample, Brechin observes, of the old adage that “commerce follows the flag,\nbut the flag follows the pick.”[2](notes.html#ich01notes2) Thousands of people\nwere forced from their homes during this substantial territorial expansion of\nthe United States. After America’s imperial invasion, the miners moved in. The\nland was stripped until the waterways were contaminated and the surrounding\nforests destroyed.\n\nSince antiquity, the business of mining has only been profitable because it\ndoes not have to account for its true costs: including environmental damage,\nthe illness and death of miners, and the loss to the communities it displaces.\nIn 1555, Georgius Agricola, known as the father of mineralogy, observed that\n“it is clear to all that there is greater detriment from mining than the value\nof the metals which the mining produces.”[3](notes.html#ich01notes3) In other\nwords, those who profit from mining do so only because the costs must be\nsustained by others, those living and those not yet born. It is easy to put a\nprice on precious metals, but what is the exact value of a wilderness, a clean\nstream, breathable air, the health of local communities? It was never\nestimated, and thus an easy calculus emerged: extract everything as rapidly as\npossible. It was the “move fast and break things” of a different time. The\nresult was that the Central Valley was decimated, and as one tourist observed\nin 1869, “Tornado, flood, earthquake and volcano combined could hardly make\ngreater havoc, spread wider ruin and wreck than [the] gold-washing operations.\n. . . There are no rights which mining respects in California. It is the one\nsupreme interest.”[4](notes.html#ich01notes4)\n\nAs San Francisco drew enormous wealth from the mines, it was easy for its\npopulace to forget where it all came from. The mines were located far from the\ncity they enriched, and this remoteness allowed city dwellers to remain\nignorant of what was happening to the mountains, rivers, and laborers that fed\ntheir fortunes. But small reminders of the mines are all around. The city’s\nnew buildings used the same technology that came from deep within the Central\nValley for transport and life support. The pulley systems that carried miners\ndown into the mine shafts were adapted and turned upside down to transport\npeople in elevators to the top of the city’s high-\nrises.[5](notes.html#ich01notes5) Brechin suggests that we should think of the\nskyscrapers of San Francisco as inverted minescapes. The ores extracted from\nholes in the ground were sold to create the stories in the air; the deeper the\nextractions went, the higher the great towers of office work stretched into\nthe sky.\n\nSan Francisco is enriched once more. Once it was gold ore that underwrote\nfortunes; now it is the extraction of substances like white lithium crystal.\nIt’s known in mineral markets as “gray gold.”[6](notes.html#ich01notes6) The\ntechnology industry has become a new supreme interest, and the five biggest\ncompanies in the world by market capitalization have offices in this city:\nApple, Microsoft, Amazon, Facebook, and Google. Walking past the start-up\nwarehouses in the SoMa district where miners in tents once lived, you can see\nluxury cars, venture capital–backed coffee chains, and sumptuous buses with\ntinted windows running along private routes, carrying workers to their offices\nin Mountain View or Menlo Park.[7](notes.html#ich01notes7) But only a short\nwalk away is Division Street, a multilane thoroughfare between SoMa and the\nMission district, where rows of tents have returned to shelter people who have\nnowhere to go. In the wake of the tech boom, San Francisco now has one of the\nhighest rates of street homelessness in the United\nStates.[8](notes.html#ich01notes8) The United Nations special rapporteur on\nadequate housing called it an “unacceptable” human rights violation, due to\nthe thousands of homeless residents denied basic necessities of water,\nsanitation, and health services in contrast to the record number of\nbillionaires who live nearby.[9](notes.html#ich01notes9) The greatest benefits\nof extraction have been captured by the few.\n\nIn this chapter we’ll traverse across Nevada, San Jose, and San Francisco, as\nwell as Indonesia, Malaysia, China, and Mongolia: from deserts to oceans.\nWe’ll also walk the spans of historical time, from conflict in the Congo and\nartificial black lakes in the present day to the Victorian passion for white\nlatex. The scale will shift, telescoping from rocks to cities, trees to\nmegacorporations, transoceanic shipping lanes to the atomic bomb. But across\nthis planetary supersystem we will see the logics of extraction, a constant\ndrawdown of minerals, water, and fossil fuels, undergirded by the violence of\nwars, pollution, extinction, and depletion. The effects of large-scale\ncomputation can be found in the atmosphere, the oceans, the earth’s crust, the\ndeep time of the planet, and the brutal impacts on disadvantaged populations\naround the world. To understand it all, we need a panoramic view of the\nplanetary scale of computational extraction.\n\n### Landscapes of Computation\n\nI’m driving through the desert valley on a summer afternoon to see the\nworkings of this latest mining boom. I ask my phone to direct me to the\nperimeter of the lithium ponds, and it replies from its awkward perch on the\ndashboard, tethered by a white USB cable. Silver Peak’s large, dry lake bed\nwas formed millions of years ago during the late Tertiary Period. It’s\nsurrounded by crusted stratifications pushing up into ridgelines containing\ndark limestones, green quartzites, and gray and red\nslate.[10](notes.html#ich01notes10) Lithium was discovered here after the area\nwas scoped for strategic minerals like potash during World War II. This soft,\nsilvery metal was mined in only modest quantities for the next fifty years,\nuntil it became highly valuable material for the technology sector.\n\nIn 2014, Rockwood Holdings, Inc., a lithium mining operation, was acquired by\nthe chemical manufacturing company Albemarle Corporation for $6.2 billion. It\nis the only operating lithium mine in the United States. This makes Silver\nPeak a site of intense interest to Elon Musk and the many other tech tycoons\nfor one reason: rechargeable batteries. Lithium is a crucial element for their\nproduction. Smartphone batteries, for example, usually contain about three-\ntenths of an ounce of it. Each Tesla Model S electric car needs about one\nhundred thirty-eight pounds of lithium for its battery\npack.[11](notes.html#ich01notes11) These kinds of batteries were never\nintended to supply a machine as power hungry as a car, but lithium batteries\nare currently the only mass-market option\navailable.[12](notes.html#ich01notes12) All of these batteries have a limited\nlifespan; once degraded, they are discarded as waste.\n\nAbout two hundred miles north of Silver Peak is the Tesla Gigafactory. This is\nthe world’s largest lithium battery plant. Tesla is the number-one lithium-ion\nbattery consumer in the world, purchasing them in high volumes from Panasonic\nand Samsung and repackaging them in its cars and home chargers. Tesla is\nestimated to use more than twenty-eight thousand tons of lithium hydroxide\nannually—half of the planet’s total consumption.[13](notes.html#ich01notes13)\nIn fact, Tesla could more accurately be described as a battery business than a\ncar company.[14](notes.html#ich01notes14) The imminent shortage of such\ncritical minerals as nickel, copper, and lithium poses a risk for the company,\nmaking the lithium lake at Silver Peak highly\ndesirable.[15](notes.html#ich01notes15) Securing control of the mine would\nmean controlling the U.S. domestic supply.\n\nAs many have shown, the electric car is far from a perfect solution to carbon\ndioxide emissions.[16](notes.html#ich01notes16) The mining, smelting, export,\nassemblage, and transport of the battery supply chain has a significant\nnegative impact on the environment and, in turn, on the communities affected\nby its degradation. A small number of home solar systems produce their own\nenergy. But for the majority of cases, charging an electric car necessitates\ntaking power from the grid, where currently less than a fifth of all\nelectricity in the United States comes from renewable energy\nsources.[17](notes.html#ich01notes17) So far none of this has dampened the\ndetermination of auto manufacturers to compete with Tesla, putting increasing\npressure on the battery market and accelerating the removal of diminishing\nstores of the necessary minerals.\n\nGlobal computation and commerce rely on batteries. The term “artificial\nintelligence” may invoke ideas of algorithms, data, and cloud architectures,\nbut none of that can function without the minerals and resources that build\ncomputing’s core components. Rechargeable lithium-ion batteries are essential\nfor mobile devices and laptops, in-home digital assistants, and data center\nbackup power. They undergird the internet and every commerce platform that\nruns on it, from banking to retail to stock market trades. Many aspects of\nmodern life have been moved to “the cloud” with little consideration of these\nmaterial costs. Our work and personal lives, our medical histories, our\nleisure time, our entertainment, our political interests—all of this takes\nplace in the world of networked computing architectures that we tap into from\ndevices we hold in one hand, with lithium at their core.\n\nThe mining that makes AI is both literal and metaphorical. The new\nextractivism of data mining also encompasses and propels the old extractivism\nof traditional mining. The stack required to power artificial intelligence\nsystems goes well beyond the multilayered technical stack of data modeling,\nhardware, servers, and networks. The full-stack supply chain of AI reaches\ninto capital, labor, and Earth’s resources—and from each, it demands an\nenormous amount.[18](notes.html#ich01notes18) The cloud is the backbone of the\nartificial intelligence industry, and it’s made of rocks and lithium brine and\ncrude oil.\n\nIn his book _A Geology of Media_ , theorist Jussi Parikka suggests we think of\nmedia not from Marshall McLuhan’s point of view—in which media are extensions\nof the human senses—but rather as extensions of\nEarth.[19](notes.html#ich01notes19) Computational media now participate in\ngeological (and climatological) processes, from the transformation of the\nearth’s materials into infrastructures and devices to the powering of these\nnew systems with oil and gas reserves. Reflecting on media and technology as\ngeological processes enables us to consider the radical depletion of\nnonrenewable resources required to drive the technologies of the present\nmoment. Each object in the extended network of an AI system, from network\nrouters to batteries to data centers, is built using elements that required\nbillions of years to form inside the earth.\n\nFrom the perspective of deep time, we are extracting Earth’s geological\nhistory to serve a split second of contemporary technological time, building\ndevices like the Amazon Echo and the iPhone that are often designed to last\nfor only a few years. The Consumer Technology Association notes that the\naverage smartphone life span is a mere 4.7 years.[20](notes.html#ich01notes20)\nThis obsolescence cycle fuels the purchase of more devices, drives up profits,\nand increases incentives for the use of unsustainable extraction practices.\nAfter a slow process of development, these minerals, elements, and materials\nthen go through an extraordinarily rapid period of excavation, processing,\nmixing, smelting, and logistical transport—crossing thousands of miles in\ntheir transformation. What begins as ore removed from the ground, after the\nspoil and the tailings are discarded, is then made into devices that are used\nand discarded. They ultimately end up buried in e-waste dumping grounds in\nplaces like Ghana and Pakistan. The lifecycle of an AI system from birth to\ndeath has many fractal supply chains: forms of exploitation of human labor and\nnatural resources and massive concentrations of corporate and geopolitical\npower. And all along the chain, a continual, large-scale consumption of energy\nkeeps the cycle going.\n\nThe extractivism on which San Francisco was built is echoed in the practices\nof the tech sector based there today.[21](notes.html#ich01notes21) The massive\necosystem of AI relies on many kinds of extraction: from harvesting the data\nmade from our daily activities and expressions, to depleting natural\nresources, and to exploiting labor around the globe so that this vast\nplanetary network can be built and maintained. And AI extracts far more from\nus and the planet than is widely known. The Bay Area is a central node in the\nmythos of AI, but we’ll need to traverse far beyond the United States to see\nthe many-layered legacies of human and environmental damage that have powered\nthe tech industry.\n\n### The Mineralogical Layer\n\nThe lithium mines in Nevada are just one of the places where the materials are\nextracted from the earth’s crust to make AI. There are many such sites,\nincluding the Salar in southwest Bolivia—the richest site of lithium in the\nworld and thus a site of ongoing political tension—as well as places in\ncentral Congo, Mongolia, Indonesia, and the Western Australia deserts. These\nare the other birthplaces of AI in the greater geography of industrial\nextraction. Without the minerals from these locations, contemporary\ncomputation simply does not work. But these materials are in increasingly\nshort supply.\n\nIn 2020, scientists at the U.S. Geological Survey published a short list of\ntwenty-three minerals that are a high “supply risk” to manufacturers, meaning\nthat if they became unavailable, entire industries—including the tech\nsector—would grind to a halt.[22](notes.html#ich01notes22) The critical\nminerals include the rare earth elements dysprosium and neodymium, which are\nused inside iPhone speakers and electric vehicle motors; germanium, which is\nused in infrared military devices for soldiers and in drones; and cobalt,\nwhich improves performance for lithium-ion batteries.\n\nThere are seventeen rare earth elements: lanthanum, cerium, praseodymium,\nneodymium, promethium, samarium, europium, gadolinium, terbium, dysprosium,\nholmium, erbium, thulium, ytterbium, lutetium, scandium, and yttrium. They are\nprocessed and embedded in laptops and smartphones, making those devices\nsmaller and lighter. The elements can be found in color displays, speakers,\ncamera lenses, rechargeable batteries, hard drives, and many other components.\nThey are key elements in communication systems, from fiber-optic cables and\nsignal amplification in mobile communication towers to satellites and GPS\ntechnology. But extracting these minerals from the ground often comes with\nlocal and geopolitical violence. Mining is and always has been a brutal\nundertaking. As Lewis Mumford writes, “Mining was the key industry that\nfurnished the sinews of war and increased the metallic contents of the\noriginal capital hoard, the war chest: on the other hand, it furthered the\nindustrialization of arms, and enriched the financier by both\nprocesses.”[23](notes.html#ich01notes23) To understand the business of AI, we\nmust reckon with the war, famine, and death that mining brings with it.\n\nRecent U.S. legislation that regulates some of those seventeen rare earth\nelements only hints at the devastation associated with their extraction. The\n2010 Dodd-Frank Act focused on reforming the financial sector in the wake of\nthe 2008 financial crisis. It included a specific provision about so-called\n_conflict minerals_ , or natural resources extracted in a conflict zone and\nthen sold to fund the conflict. Companies using gold, tin, tungsten, and\ntantalum from the region around the Democratic Republic of the Congo now had a\nreporting requirement to track where those minerals came from and whether the\nsale was funding armed militia in the region.[24](notes.html#ich01notes24)\nLike “conflict diamonds,” the term “conflict minerals” masks the profound\nsuffering and prolific killing in the mining sector. Mining profits have\nfinanced military operations in the decades-long Congo-area conflict, fueling\nthe deaths of thousands and the displacement of\nmillions.[25](notes.html#ich01notes25) Furthermore, working conditions inside\nthe mines have often amounted to modern slavery.[26](notes.html#ich01notes26)\n\nIt took Intel more than four years of sustained effort to develop basic\ninsight into its own supply chain.[27](notes.html#ich01notes27) Intel’s supply\nchain is complex, with more than sixteen thousand suppliers in over a hundred\ncountries providing direct materials for the company’s production processes,\ntools, and machines for their factories, as well as their logistics and\npackaging services.[28](notes.html#ich01notes28) In addition, Intel and Apple\nhave been criticized for auditing only smelters—not the actual mines—to\ndetermine the conflict-free status of minerals. The tech giants were assessing\nsmelting plants outside of Congo, and the audits were often performed by\nlocals. So even the conflict-free certifications of the tech industry are now\nunder question.[29](notes.html#ich01notes29)\n\nDutch-based technology company Philips has also claimed that it was working to\nmake its supply chain “conflict-free.” Like Intel, Philips has tens of\nthousands of suppliers, each of which provides component parts for the\ncompany’s manufacturing processes.[30](notes.html#ich01notes30) Those\nsuppliers are themselves linked downstream to thousands of component\nmanufacturers acquiring treated materials from dozens of smelters. The\nsmelters in turn buy their materials from an unknown number of traders who\ndeal directly with both legal and illegal mining operations to source the\nvarious minerals that end up in computer\ncomponents.[31](notes.html#ich01notes31)\n\nAccording to the computer manufacturer Dell, the complexities of the metals\nand mineral supply chains pose almost insurmountable challenges to the\nproduction of conflict-free electronics components. The elements are laundered\nthrough such a vast number of entities along the chain that sourcing their\nprovenance proves impossible—or so the end-product manufacturers claim,\nallowing them a measure of plausible deniability for any exploitative\npractices that drive their profits.[32](notes.html#ich01notes32)\n\nJust like the mines that served San Francisco in the nineteenth century,\nextraction for the technology sector is done by keeping the real costs out of\nsight. Ignorance of the supply chain is baked into capitalism, from the way\nbusinesses protect themselves through third-party contractors and suppliers to\nthe way goods are marketed and advertised to consumers. More than plausible\ndeniability, it has become a well-practiced form of bad faith: the left hand\ncannot know what the right hand is doing, which requires increasingly lavish,\nbaroque, and complex forms of distancing.\n\nWhile mining to finance war is one of the most extreme cases of harmful\nextraction, most minerals are not sourced from direct war zones. This doesn’t\nmean, however, that they are free from human suffering and environmental\ndestruction. The focus on conflict minerals, though important, has also been\nused to avert focus from the harms of mining writ large. If we visit the\nprimary sites of mineral extraction for computational systems, we find the\nrepressed stories of acid-bleached rivers and deracinated landscapes and the\nextinction of plant and animal species that were once vital to the local\necology.\n\n### Black Lakes and White Latex\n\nIn Baotou, the largest city in Inner Mongolia, there is an artificial lake\nfilled with toxic black mud. It reeks of sulfur and stretches as far as the\neye can see, covering more than five and a half miles in diameter. The black\nlake contains more than 180 million tons of waste powder from ore\nprocessing.[33](notes.html#ich01notes33) It was created by the waste runoff\nfrom the nearby Bayan Obo mines, which is estimated to contain almost 70\npercent of the world’s reserves of rare earth minerals. It is the largest\ndeposit of rare earth elements on the planet.[34](notes.html#ich01notes34)\n\nChina supplies 95 percent of the world’s rare earth minerals. China’s market\ndomination, as the writer Tim Maughan observes, owes far less to geology than\nto the country’s willingness to take on the environmental damage of\nextraction.[35](notes.html#ich01notes35) Although rare earth minerals like\nneodymium and cerium are relatively common, making them usable requires the\nhazardous process of dissolving them in large volumes of sulfuric and nitric\nacid. These acid baths yield reservoirs of poisonous waste that fill the dead\nlake in Baotou. This is just one of the places that are brimming with what\nenvironmental studies scholar Myra Hird calls “the waste we want to\nforget.”[36](notes.html#ich01notes36)\n\nTo date, the unique electronic, optical, and magnetic uses of rare earth\nelements cannot be matched by any other metals, but the ratio of usable\nminerals to waste toxins is extreme. Natural resource strategist David Abraham\ndescribes the mining in Jiangxi, China, of dysprosium and terbium, which are\nused in a variety of high-tech devices. He writes, “Only 0.2 percent of the\nmined clay contains the valuable rare earth elements. This means that 99.8\npercent of earth removed in rare earth mining is discarded as waste, called\n‘tailings,’ that are dumped back into the hills and streams,” creating new\npollutants like ammonium.[37](notes.html#ich01notes37) In order to refine one\nton of these rare earth elements, “the Chinese Society of Rare Earths\nestimates that the process produces 75,000 liters of acidic water and one ton\nof radioactive residue.”[38](notes.html#ich01notes38)\n\nAbout three thousand miles south of Baotou are the small Indonesian islands of\nBangka and Belitung, off the coast of Sumatra. Bangka and Belitung produce 90\npercent of Indonesia’s tin, used in semiconductors. Indonesia is the world’s\nsecond-largest producer of the metal, behind China. Indonesia’s national tin\ncorporation, PT Timah, supplies companies such as Samsung directly, as well as\nsolder makers Chernan and Shenmao, which in turn supply Sony, LG, and\nFoxconn—all suppliers for Apple, Tesla, and\nAmazon.[39](notes.html#ich01notes39)\n\nOn these small islands, gray-market miners who are not officially employed sit\non makeshift pontoons, using bamboo poles to scrape the seabed before diving\nunderwater to suck tin from the surface by drawing their breath through giant,\nvacuumlike tubes. The miners sell the tin they find to middlemen, who also\ncollect ore from miners working in authorized mines, and they mix it together\nto sell to companies like Timah.[40](notes.html#ich01notes40) Completely\nunregulated, the process unfolds beyond any formal worker or environmental\nprotections. As investigative journalist Kate Hodal reports, “Tin mining is a\nlucrative but destructive trade that has scarred the island’s landscape,\nbulldozed its farms and forests, killed off its fish stocks and coral reefs,\nand dented tourism to its pretty palm-lined beaches. The damage is best seen\nfrom the air, as pockets of lush forest huddle amid huge swaths of barren\norange earth. Where not dominated by mines, this is pockmarked with graves,\nmany holding the bodies of miners who have died over the centuries digging for\ntin.”[41](notes.html#ich01notes41) The mines are everywhere: in backyards, in\nthe forest, by the side of the road, on the beaches. It is a landscape of\nruin.\n\nIt is a common practice of life to focus on the world immediately before us,\nthe one we see and smell and touch every day. It grounds us where we are, with\nour communities and our known corners and concerns. But to see the full supply\nchains of AI requires looking for patterns in a global sweep, a sensitivity to\nthe ways in which the histories and specific harms are different from place to\nplace and yet are deeply interconnected by the multiple forces of extraction.\n\nWe can see these patterns across space, but we can also find them across time.\nTransatlantic telegraph cables are the essential infrastructure that ferries\ndata between the continents, an emblem of global communication and capital.\nThey are also a material product of colonialism, with its patterns of\nextraction, conflict, and environmental destruction. At the end of the\nnineteenth century, a particular Southeast Asian tree called _Palaquium gutta_\nbecame the center of a cable boom. These trees, found mainly in Malaysia,\nproduce a milky white natural latex called gutta-percha. After English\nscientist Michael Faraday published a study in the _Philosophical Magazine_ in\n1848 about the use of this material as an electrical insulator, gutta-percha\nrapidly became the darling of the engineering world. Engineers saw gutta-\npercha as the solution to the problem of insulating telegraphic cables to\nwithstand harsh and varying conditions on the ocean floor. The twisted strands\nof copper wire needed four layers of the soft, organic tree sap to protect\nthem from water incursion and carry their electrical currents.\n\nAs the global submarine telegraphy business grew, so did demand for _Palaquium\ngutta_ tree trunks. The historian John Tully describes how local Malay,\nChinese, and Dayak workers were paid little for the dangerous work of felling\nthe trees and slowly collecting the latex.[42](notes.html#ich01notes42) The\nlatex was processed and then sold through Singapore’s trade markets into the\nBritish market, where it was transformed into, among other things, lengths\nupon lengths of submarine cable sheaths that wrapped around the globe. As\nmedia scholar Nicole Starosielski writes, “Military strategists saw cables as\nthe most efficient and secure mode of communication with the colonies—and, by\nimplication, of control over them.”[43](notes.html#ich01notes43) The routes of\nsubmarine cables today still mark out the early colonial networks between the\ncenters and the peripheries of empire.[44](notes.html#ich01notes44)\n\nA mature _Palaquium gutta_ could yield around eleven ounces of latex. But in\n1857, the first transatlantic cable was around eighteen hundred miles long and\nweighed two thousand tons—requiring about 250 tons of gutta-percha. To produce\njust one ton of this material required around nine hundred thousand tree\ntrunks. The jungles of Malaysia and Singapore were stripped; by the early\n1880s, the _Palaquium gutta_ had vanished. In a last-ditch effort to save\ntheir supply chain, the British passed a ban in 1883 to halt harvesting the\nlatex, but the tree was all but extinct.[45](notes.html#ich01notes45)\n\nThe Victorian environmental disaster of gutta-percha, at the dawn of the\nglobal information society, shows how the relations between technology and its\nmaterials, environments, and labor practices are\ninterwoven.[46](notes.html#ich01notes46) Just as Victorians precipitated\necological disaster for their early cables, so do contemporary mining and\nglobal supply chains further imperil the delicate ecological balance of our\nera.\n\nThere are dark ironies in the prehistories of planetary computation. Currently\nlarge-scale AI systems are driving forms of environmental, data, and human\nextraction, but from the Victorian era onward, algorithmic computation emerged\nout of desires to manage and control war, population, and climate change. The\nhistorian Theodora Dryer describes how the founding figure of mathematical\nstatistics, English scientist Karl Pearson, sought to resolve uncertainties of\nplanning and management by developing new data architectures including\nstandard deviations and techniques of correlation and regression. His methods\nwere, in turn, deeply imbricated with race science, as Pearson—along with his\nmentor, the statistician and founder of eugenics Sir Francis Galton—believed\nthat statistics could be “the first step in an enquiry into the possible\neffect of a selective process upon any character of a\nrace.”[47](notes.html#ich01notes47)\n\n![Images](../images/f0040-01.jpg)\n\n_Palaquium gutta_\n\nAs Dryer writes, “By the end of the 1930s, these data architectures—regression\ntechniques, standard deviation, and correlations—would become dominant tools\nused in interpreting social and state information on the world stage. Tracking\nthe nodes and routes of global trade, the interwar ‘mathematical-statistics\nmovement’ became a vast enterprise.”[48](notes.html#ich01notes48) This\nenterprise kept expanding after World War II, as new computational systems\nwere used in domains such as weather forecasting during periods of drought to\neke out more productivity from large-scale industrial\nfarming.[49](notes.html#ich01notes49) From this perspective, algorithmic\ncomputing, computational statistics, and artificial intelligence were\ndeveloped in the twentieth century to address social and environmental\nchallenges but would later be used to intensify industrial extraction and\nexploitation and further deplete environmental resources.\n\n### The Myth of Clean Tech\n\nMinerals are the backbone of AI, but its lifeblood is still electrical energy.\nAdvanced computation is rarely considered in terms of carbon footprints,\nfossil fuels, and pollution; metaphors like “the cloud” imply something\nfloating and delicate within a natural, green\nindustry.[50](notes.html#ich01notes50) Servers are hidden in nondescript data\ncenters, and their polluting qualities are far less visible than the billowing\nsmokestacks of coal-fired power stations. The tech sector heavily publicizes\nits environmental policies, sustainability initiatives, and plans to address\nclimate-related problems using AI as a problem-solving tool. It is all part of\na highly produced public image of a sustainable tech industry with no carbon\nemissions. In reality, it takes a gargantuan amount of energy to run the\ncomputational infrastructures of Amazon Web Services or Microsoft’s Azure, and\nthe carbon footprint of the AI systems that run on those platforms is\ngrowing.[51](notes.html#ich01notes51)\n\nAs Tung-Hui Hu writes in _A Prehistory of the Cloud_ , “The cloud is a\nresource-intensive, extractive technology that converts water and electricity\ninto computational power, leaving a sizable amount of environmental damage\nthat it then displaces from sight.”[52](notes.html#ich01notes52) Addressing\nthis energy-intensive infrastructure has become a major concern. Certainly,\nthe industry has made significant efforts to make data centers more energy\nefficient and to increase their use of renewable energy. But already, the\ncarbon footprint of the world’s computational infrastructure has matched that\nof the aviation industry at its height, and it is increasing at a faster\nrate.[53](notes.html#ich01notes53) Estimates vary, with researchers like Lotfi\nBelkhir and Ahmed Elmeligi estimating that the tech sector will contribute 14\npercent of global greenhouse emissions by 2040, while a team in Sweden\npredicts that the electricity demands of data centers alone will increase\nabout fifteenfold by 2030.[54](notes.html#ich01notes54)\n\nBy looking closely at the computational capacity needed to build AI models, we\ncan see how the desire for exponential increases in speed and accuracy is\ncoming at a high cost to the planet. The processing demands of training AI\nmodels, and thus their energy consumption, is still an emerging area of\ninvestigation. One of the early papers in this field came from AI researcher\nEmma Strubell and her team at the University of Massachusetts Amherst in 2019.\nWith a focus on trying to understand the carbon footprint of natural language\nprocessing (NLP) models, they began to sketch out potential estimates by\nrunning AI models over hundreds of thousands of computational\nhours.[55](notes.html#ich01notes55) The initial numbers were striking.\nStrubell’s team found that running only a single NLP model produced more than\n660,000 pounds of carbon dioxide emissions, the equivalent of five gas-powered\ncars over their total lifetime (including their manufacturing) or 125 round-\ntrip flights from New York to Beijing.[56](notes.html#ich01notes56)\n\nWorse, the researchers noted that this modeling is, at minimum, a baseline\noptimistic estimate. It does not reflect the true commercial scale at which\ncompanies like Apple and Amazon operate, scraping internet-wide datasets and\nfeeding their own NLP models to make AI systems like Siri and Alexa sound more\nhuman. But the exact amount of energy consumption produced by the tech\nsector’s AI models is unknown; that information is kept as highly guarded\ncorporate secrets. Here, too, the data economy is premised on maintaining\nenvironmental ignorance.\n\nIn the AI field, it is standard practice to maximize computational cycles to\nimprove performance, in accordance with a belief that bigger is better. As\nRich Sutton of DeepMind describes it: “Methods that leverage computation are\nultimately the most effective, and by a large\nmargin.”[57](notes.html#ich01notes57) The computational technique of brute-\nforce testing in AI training runs, or systematically gathering more data and\nusing more computational cycles until a better result is achieved, has driven\na steep increase in energy consumption. OpenAI estimated that since 2012, the\namount of compute used to train a single AI model has increased by a factor of\nten every year. That’s due to developers “repeatedly finding ways to use more\nchips in parallel, and being willing to pay the economic cost of doing\nso.”[58](notes.html#ich01notes58) Thinking only in terms of economic cost\nnarrows the view on the wider local and environmental price of burning\ncomputation cycles as a way to create incremental efficiencies. The tendency\ntoward “compute maximalism” has profound ecological impacts.\n\nData centers are among the world’s largest consumers of\nelectricity.[59](notes.html#ich01notes59) Powering this multilevel machine\nrequires grid electricity in the form of coal, gas, nuclear, or renewable\nenergy. Some corporations are responding to growing alarm about the energy\nconsumption of large-scale computation, with Apple and Google claiming to be\ncarbon neutral (which means they offset their carbon emissions by purchasing\ncredits) and Microsoft promising to become carbon negative by 2030. But\nworkers within the companies have pushed for reductions in emissions across\nthe board, rather than what they see as buying indulgences out of\nenvironmental guilt.[60](notes.html#ich01notes60) Moreover, Microsoft, Google,\nand Amazon all license their AI platforms, engineering workforces, and\ninfrastructures to fossil fuel companies to help them locate and extract fuel\nfrom the ground, which further drives the industry most responsible for\nanthropogenic climate change.\n\nBeyond the United States, more clouds of carbon dioxide are rising. China’s\ndata center industry draws 73 percent of its power from coal, emitting about\n99 million tons of CO2 in 2018.[61](notes.html#ich01notes61) And electricity\nconsumption from China’s data center infrastructure is expected to increase by\ntwo-thirds by 2023.[62](notes.html#ich01notes62) Greenpeace has raised the\nalarm about the colossal energy demands of China’s biggest technology\ncompanies, arguing that “China’s leading tech companies, including Alibaba,\nTencent, and GDS, must dramatically scale up clean energy procurement and\ndisclose energy use data.”[63](notes.html#ich01notes63) But the lasting\nimpacts of coal-fired power are everywhere, exceeding any national boundaries.\nThe planetary nature of resource extraction and its consequences goes well\nbeyond what the nation-state was designed to address.\n\nWater tells another story of computation’s true cost. The history of water use\nin the United States is full of battles and secret deals, and as with\ncomputation, the deals made over water are kept close. One of the biggest U.S.\ndata centers belongs to the National Security Agency (NSA) in Bluffdale, Utah.\nOpen since late 2013, the Intelligence Community Comprehensive National\nCybersecurity Initiative Data Center is impossible to visit directly. But by\ndriving up through the adjacent suburbs, I found a cul-de-sac on a hill thick\nwith sagebrush, and from there I was afforded a closer view of the sprawling\n1.2-million-square-foot facility. The site has a kind of symbolic power of the\nnext era of government data capture, having been featured in films like\n_Citizenfour_ and pictured in thousands of news stories about the NSA. In\nperson, though, it looks nondescript and prosaic, a giant storage container\ncombined with a government office block.\n\nThe struggle over water began even before the data center was officially open,\ngiven its location in drought-parched Utah.[64](notes.html#ich01notes64) Local\njournalists wanted to confirm whether the estimated consumption of 1.7 million\ngallons of water per day was accurate, but the NSA initially refused to share\nusage data, redacted all details from public records, and claimed that its\nwater use was a matter of national security. Antisurveillance activists\ncreated handbooks encouraging the end of material support of water and energy\nto surveillance, and they strategized that legal controls over water usage\ncould help shut down the facility.[65](notes.html#ich01notes65) But the city\nof Bluffdale had already made a multiyear deal with the NSA, in which the city\nwould sell water at rates well below the average in return for the promise of\neconomic growth the facility might bring to the\nregion.[66](notes.html#ich01notes66) The geopolitics of water are now deeply\ncombined with the mechanisms and politics of data centers, computation, and\npower—in every sense. From the dry hillside that overlooks the NSA’s data\nrepository, all the contestation and obfuscation about water makes sense: this\nis a landscape with a limit, and water that is used to cool servers is being\ntaken away from communities and habitats that rely on it to live.\n\nJust as the dirty work of the mining sector was far removed from the companies\nand city dwellers who profited most, so the majority of data centers are far\nremoved from major population hubs, whether in the desert or in semi-\nindustrial exurbs. This contributes to our sense of the cloud being out of\nsight and abstracted away, when in fact it is material, affecting the\nenvironment and climate in ways that are far from being fully recognized and\naccounted for. The cloud is of the earth, and to keep it growing requires\nexpanding resources and layers of logistics and transport that are in constant\nmotion.\n\n### The Logistical Layer\n\nSo far, we have considered the material stuff of AI, from rare earth elements\nto energy. By grounding our analysis in the specific materialities of AI—the\nthings, places, and people—we can better see how the parts are operating\nwithin broader systems of power. Take, for example, the global logistical\nmachines that move minerals, fuel, hardware, workers, and consumer AI devices\naround the planet.[67](notes.html#ich01notes67) The dizzying spectacle of\nlogistics and production displayed by companies like Amazon would not be\npossible without the development and widespread acceptance of a standardized\nmetal object: the cargo container. Like submarine cables, cargo containers\nbind the industries of global communication, transport, and capital, a\nmaterial exercise of what mathematicians call “optimal transport”—in this\ncase, as an optimization of space and resources across the trade routes of the\nworld.\n\nStandardized cargo containers (themselves built from the basic earth elements\nof carbon and iron forged as steel) enabled the explosion of the modern\nshipping industry, which in turn made it possible to envision and model the\nplanet as a single massive factory. The cargo container is the single unit of\nvalue—like a piece of Lego—that can travel thousands of miles before meeting\nits final destination as a modular part of a greater system of delivery. In\n2017, the capacity of container ships in seaborne trade reached nearly 250\nmillion deadweight tons of cargo, dominated by giant shipping companies\nincluding Maersk of Denmark, the Mediterranean Shipping Company of\nSwitzerland, and France’s CMA CGM Group, each owning hundreds of container\nvessels.[68](notes.html#ich01notes68) For these commercial ventures, cargo\nshipping is a relatively cheap way to navigate the vascular system of the\nglobal factory, yet it disguises far larger external costs. Just as they tend\nto neglect the physical realities and costs of AI infrastructure, popular\nculture and media rarely cover the shipping industry. The author Rose George\ncalls this condition “sea blindness.”[69](notes.html#ich01notes69)\n\nIn recent years, shipping vessels produced 3.1 percent of yearly global carbon\ndioxide emissions, more than the total produced by\nGermany.[70](notes.html#ich01notes70) In order to minimize their internal\ncosts, most container shipping companies use low-grade fuel in enormous\nquantities, which leads to increased amounts of airborne sulfur and other\ntoxic substances. One container ship is estimated to emit as much pollution as\nfifty million cars, and sixty thousand deaths every year are attributed\nindirectly to cargo-ship-industry pollution.[71](notes.html#ich01notes71)\n\nEven industry-friendly sources like the World Shipping Council admit that\nthousands of containers are lost each year, sinking to the ocean floor or\ndrifting loose.[72](notes.html#ich01notes72) Some carry toxic substances that\nleak into the oceans; others release thousands of yellow rubber ducks that\nwash ashore around the world over decades.[73](notes.html#ich01notes73)\nTypically, workers spend almost six months at sea, often with long working\nshifts and without access to external communications.\n\nHere, too, the most severe costs of global logistics are borne by the Earth’s\natmosphere, the oceanic ecosystem and low-paid workers. The corporate\nimaginaries of AI fail to depict the lasting costs and long histories of the\nmaterials needed to build computational infrastructures or the energy required\nto power them. The rapid growth of cloud-based computation, portrayed as\nenvironmentally friendly, has paradoxically driven an expansion of the\nfrontiers of resource extraction. It is only by factoring in these hidden\ncosts, these wider collections of actors and systems, that we can understand\nwhat the shift toward increasing automation will mean. This requires working\nagainst the grain of how the technological imaginary usually works, which is\ncompletely untethered from earthly matters. Like running an image search of\n“AI,” which returns dozens of pictures of glowing brains and blue-tinted\nbinary code floating in space, there is a powerful resistance to engaging with\nthe materialities of these technologies. Instead, we begin with the earth,\nwith extraction, and with the histories of industrial power and then consider\nhow these patterns are repeated in systems of labor and data.\n\n### AI as Megamachine\n\nIn the late 1960s, the historian and philosopher of technology Lewis Mumford\ndeveloped the concept of the _megamachine_ to illustrate how all systems, no\nmatter how immense, consist of the work of many individual human\nactors.[74](notes.html#ich01notes74) For Mumford, the Manhattan Project was\nthe defining modern megamachine whose intricacies were kept not only from the\npublic but even from the thousands of people who worked on it at discrete,\nsecured sites across the United States. A total of 130,000 workers operated in\ncomplete secrecy under the direction of the military, developing a weapon that\nwould kill (by conservative estimates) 237,000 people when it hit Hiroshima\nand Nagasaki in 1945. The atomic bomb depended on a complex, secret chain of\nsupply, logistics, and human labor.\n\nArtificial intelligence is another kind of megamachine, a set of technological\napproaches that depend on industrial infrastructures, supply chains, and human\nlabor that stretch around the globe but are kept opaque. We have seen how AI\nis much more than databases and algorithms, machine learning models and linear\nalgebra. It is metamorphic: relying on manufacturing, transportation, and\nphysical work; data centers and the undersea cables that trace lines between\nthe continents; personal devices and their raw components; transmission\nsignals passing through the air; datasets produced by scraping the internet;\nand continual computational cycles. These all come at a cost.\n\nWe have looked at the relations between cities and mines, companies and supply\nchains, and the topographies of extraction that connect them. The\nfundamentally intertwined nature of production, manufacturing, and logistics\nreminds us that the mines that drive AI are everywhere: not only sited in\ndiscrete locations but diffuse and scattered across the geography of the\nearth, in what Mazen Labban has called the “planetary\nmine.”[75](notes.html#ich01notes75) This is not to deny the many specific\nlocations where technologically driven mining is taking place. Rather, Labban\nobserves that the planetary mine expands and reconstitutes extraction into\nnovel arrangements, extending the practices of mines into new spaces and\ninteractions around the world.\n\nFinding fresh methods for understanding the deep material and human roots of\nAI systems is vital at this moment in history, when the impacts of\nanthropogenic climate change are already well under way. But that’s easier\nsaid than done. In part, that’s because many industries that make up the AI\nsystem chain conceal the ongoing costs of what they do. Furthermore, the scale\nrequired to build artificial intelligence systems is too complex, too obscured\nby intellectual property law, and too mired in logistical and technical\ncomplexity for us to see into it all. But the aim here is not to try and make\nthese complex assemblages transparent: rather than trying to see _inside_\nthem, we will be connecting _across_ multiple systems to understand how they\nwork in relation to each other.[76](notes.html#ich01notes76) Thus, our path\nwill follow the stories about the environmental and labor costs of AI and\nplace them in context with the practices of extraction and classification\nbraided throughout everyday life. It is by thinking about these issues\ntogether that we can work toward greater justice.\n\n![Images](../images/f0050-01.jpg)\n\nThe ruins at Blair. Photograph by Kate Crawford\n\nI make one more trip to Silver Peak. Before I reach the town, I pull the van\nover to the side of the road to read a weather-beaten sign. It’s Nevada\nHistorical Marker 174, dedicated to the creation and destruction of a small\ntown called Blair. In 1906, the Pittsburgh Silver Peak Gold Mining Company\nbought up the mines in the area. Anticipating a boom, land speculators\npurchased all of the available plots near Silver Peak along with its water\nrights, driving prices to record artificial highs. So the mining company\nsurveyed a couple of miles north and declared it the site for a new town:\nBlair. They built a hundred-stamp cyanide mill for leach mining, the biggest\nin the state, and laid the Silver Peak railroad that ran from Blair Junction\nto the Tonopah and Goldfield main line. Briefly, the town thrived. Many\nhundreds of people came from all over for the jobs, despite the harsh working\nconditions. But with so much mining activity, the cyanide began to poison the\nground, and the gold and silver seams began to falter and dry up. By 1918,\nBlair was all but deserted. It was all over within twelve years. The ruins are\nmarked on a local map—just a forty-five-minute walk away.\n\nIt’s a blazing hot day in the desert. The only sounds are the metallic\nreverberations of cicadas and the rumble of an occasional passenger jet. I\ndecide to start up the hill. By the time I reach the collection of stone\nbuildings at the top of the long dirt road, I’m exhausted from the heat. I\ntake shelter inside the collapsed remains of what was once a gold miner’s\nhouse. Not much is left: some broken crockery, shards of glass bottles, a few\nrusted tins. Back in Blair’s lively years, multiple saloons thrived nearby and\na two-story hotel welcomed visitors. Now it’s a cluster of broken foundations.\n\nThrough the space where a window used to be, the view stretches all the way\ndown the valley. I’m struck by the realization that Silver Peak will also be a\nghost town soon. The current draw on the lithium mine is aggressive in\nresponse to the high demand, and no one knows how long it will last. The most\noptimistic estimate is forty years, but the end may come much sooner. Then the\nlithium pools under the Clayton Valley will be exsanguinated—extracted for\nbatteries that are destined for landfill. And Silver Peak will return to its\nprevious life as an empty and quiet place, on the edge of an ancient salt\nlake, now drained.\n\n\n![Images](../images/f0052-01.jpg)\n\n\n## 2  \nLabor\n\nWhen I enter Amazon’s vast fulfillment center in Robbinsville, New Jersey, the\nfirst thing I see is a large sign that reads “Time Clock.” It juts out from\none of the bright yellow concrete pylons spanning across the vast factory\nspace of 1.2 million square feet. This is a major distribution warehouse for\nsmaller objects—a central distribution node for the Northeastern United\nStates. It presents a dizzying spectacle of contemporary logistics and\nstandardization, designed to accelerate the delivery of packages. Dozens of\ntime-clock signs appear at regular intervals along the entryway. Every second\nof work is being monitored and tallied. Workers—known as “associates”—must\nscan themselves in as soon as they arrive. The sparse, fluorescent-lit break\nrooms also feature time clocks—with more signs to underscore that all scans in\nand out of the rooms are tracked. Just as packages are scanned in the\nwarehouse, so too are workers monitored for the greatest possible efficiency:\nthey can only be off-task for fifteen minutes per shift, with an unpaid\nthirty-minute meal break. Shifts are ten hours long.\n\nThis is one of the newer fulfillment centers that feature robots to move the\nheavy shelving units laden with products in trays. The bright orange Kiva\nrobots glide smoothly across the concrete floors like vivid water bugs,\nfollowing a programmed logic that causes them to spin in lazy circles and then\nlock onto a path toward the next worker awaiting the trays. Then they move\nforward, carrying on their backs a tower of purchases that can weigh up to\nthree thousand pounds. This shuffling army of ground-hugging robots presents a\nkind of effortless efficiency: they carry, they rotate, they advance, they\nrepeat. They make a low, whirring hum, but it is almost entirely drowned out\nby the deafening sound of fast-moving conveyor belts that act as the factory’s\narteries. There are fourteen miles of conveyor belts moving without pause in\nthis space. The result is a constant roar.\n\nWhile the robots perform their coordinated algorithmic ballet behind bare\nchain-link fences, the workers in the factory are far less serene. The anxiety\nof making the “picking rate”—the number of items they must select and pack\nwithin the allocated time—is clearly taking a toll. Many of the workers I\nencounter on my visits are wearing some kind of support bandage. I see knee\nbraces, elbow bandages, wrist guards. When I observe that many people seem to\nhave injuries, the Amazon worker guiding me through the factory points to the\nvending machines spaced at regular intervals that are “stocked with over-the-\ncounter painkillers for anyone who needs them.”\n\nRobotics has become a key part of Amazon’s logistical armory, and while the\nmachinery seems well tended, the corresponding human bodies seem like an\nafterthought. They are there to complete the specific, fiddly tasks that\nrobots cannot: picking up and visually confirming all of the oddly shaped\nobjects that people want delivered to their homes, from phone cases to\ndishwashing detergent, within the shortest amount of time. Humans are the\nnecessary connective tissue to get ordered items into containers and trucks\nand delivered to consumers. But they aren’t the most valuable or trusted\ncomponent of Amazon’s machine. At the end of the day, all associates must exit\nthrough a row of metal detectors. This is an effective antitheft measure, I am\ntold.\n\n![Images](../images/f0055-01.jpg)\n\nWorkers and time clocks at the Amazon fulfillment center in Robbinsville\nTownship, New Jersey. AP Photo/Julio Cortez\n\nWithin the layers of the internet, one of the most common units of measurement\nis the network packet—a basic unit of data to be sent from one destination and\ndelivered to another. At Amazon, the basic unit of measurement is the brown\ncardboard box, that familiar domestic cargo vessel emblazoned with a curved\narrow simulating a human smile. Network packets each have a timestamp known as\na _time to live._ Data has to reach its destination before the time to live\nexpires. At Amazon, the cardboard box also has a _time to live_ driven by the\ncustomer’s shipping demands. If the box is late, this affects Amazon’s brand\nand ultimately its profits. So enormous attention has been devoted to the\nmachine learning algorithm that is tuned to the data regarding the best size,\nweight, and strength of corrugated boxes and paper mailers. Apparently without\nirony, the algorithm is called “the matrix.”[1](notes.html#ich02notes1)\nWhenever a person reports a broken item, it becomes a data point about what\nsort of box should be used in the future. The next time that product is\nmailed, it will automatically be assigned a new type of box by the matrix,\nwithout human input. This prevents breakages, which saves time, which\nincreases profits. Workers, however, are forced continually to adapt, which\nmakes it harder to put their knowledge into action or habituate to the job.\n\nThe control over time is a consistent theme in the Amazon logistical empire,\nand the bodies of workers are run according to the cadences of computational\nlogics. Amazon is America’s second-largest private employer, and many\ncompanies strive to emulate its approach. Many large corporations are heavily\ninvesting in automated systems in the attempt to extract ever-larger volumes\nof labor from fewer workers. Logics of efficiency, surveillance, and\nautomation are all converging in the current turn to computational approaches\nto managing labor. The hybrid human-robotic distribution warehouses of Amazon\nare a key site to understand the trade-offs being made in this commitment to\nautomated efficiency. From there, we can begin to consider the question of how\nlabor, capital, and time are entwined in AI systems.\n\nRather than debating whether humans will be replaced by robots, in this\nchapter I focus on how the experience of work is shifting in relation to\nincreased surveillance, algorithmic assessment, and the modulation of time.\nPut another way, instead of asking whether robots will replace humans, I’m\ninterested in how humans are increasingly treated like robots and what this\nmeans for the role of labor. Many forms of work are shrouded in the term\n“artificial intelligence,” hiding the fact that people are often performing\nrote tasks to shore up the impression that machines can do the work. But\nlarge-scale computation is deeply rooted in and running on the exploitation of\nhuman bodies.\n\nIf we want to understand the future of work in the context of artificial\nintelligence, we need to begin by understanding the past and present\nexperience of workers. Approaches to maximizing the extraction of value from\nworkers vary from reworkings of the classical techniques used in Henry Ford’s\nfactories to a range of machine learning–assisted tools designed to increase\nthe granularity of tracking, nudging, and assessment. This chapter maps\ngeographies of labor past and present, from Samuel Bentham’s inspection houses\nto Charles Babbage’s theories of time management and to Frederick Winslow\nTaylor’s micromanagement of human bodies. Along the way, we will see how AI is\nbuilt on the very human efforts of (among other things) crowdwork, the\nprivatization of time, and the seemingly never-ending reaching, lifting, and\ntoiling of putting boxes into order. From the lineage of the mechanized\nfactory, a model emerges that values increased conformity, standardization,\nand interoperability—for products, processes, and humans alike.\n\n### Prehistories of Workplace AI\n\nWorkplace automation, though often told as a story of the future, is already a\nlong-established experience of contemporary work. The manufacturing assembly\nline, with its emphasis on consistent and standardized units of production,\nhas analogues in the service industries, from retail to restaurants.\nSecretarial labor has been increasingly automated since the 1980s and now is\nemulated by highly feminized AI assistants such as Siri, Cortana, and\nAlexa.[2](notes.html#ich02notes2) So-called knowledge workers, those white-\ncollar employees assumed to be less threatened by the forces driving\nautomation, find themselves increasingly subjected to workplace surveillance,\nprocess automation, and collapse between the distinction of work and leisure\ntime (although women have rarely experienced such clear distinctions, as\nfeminist theorists of work like Silvia Federici and Melissa Gregg have\nshown).[3](notes.html#ich02notes3) Work of all stripes has had to\nsignificantly adapt itself in order to be interpretable and understood by\nsoftware-based systems.[4](notes.html#ich02notes4)\n\nThe common refrain for the expansion of AI systems and process automation is\nthat we are living in a time of beneficial human-AI collaboration. But this\ncollaboration is not fairly negotiated. The terms are based on a significant\npower asymmetry—is there ever a choice _not_ to collaborate with algorithmic\nsystems? When a company introduces a new AI platform, workers are rarely\nallowed to opt out. This is less of a collaboration than a forced engagement,\nwhere workers are expected to re-skill, keep up, and unquestioningly accept\neach new technical development.\n\nRather than representing a radical shift from established forms of work, the\nencroachment of AI into the workplace should properly be understood as a\nreturn to older practices of industrial labor exploitation that were well\nestablished in the 1890s and the early twentieth century. That was a time when\nfactory labor was already seen in relation to machines and work tasks were\nincreasingly subdivided into smaller actions requiring minimal skill but\nmaximum exertion. Indeed, the current expansion of labor automation continues\nthe broader historical dynamics inherent in industrial capitalism. Since the\nappearance of the earliest factories, workers have encountered ever more\npowerful tools, machines, and electronic systems that play a role in changing\nhow labor is managed while transferring more value to their employers. We are\nwitnessing new refrains on an old theme. The crucial difference is that\nemployers now observe, assess, and modulate intimate parts of the work cycle\nand bodily data—down to the last micromovement—that were previously off-limits\nto them.\n\nThere are many prehistories of workplace AI; one is the Industrial\nRevolution’s widespread automation of common productive activities. In his\n_Wealth of Nations_ , the eighteenth-century political economist Adam Smith\nfirst pointed to the division and subdivision of manufacturing tasks as the\nbasis of both improved productivity and increasing\nmechanization.[5](notes.html#ich02notes5) He observed that by identifying and\nanalyzing the various steps involved in manufacturing any given item, it was\npossible to divide them into ever-smaller steps, so that a product once made\nentirely by expert craftspeople could now be built by a team of lower-skill\nworkers equipped with tools purpose-built for a particular task. Thus, a\nfactory’s output could be scaled up significantly without an equivalent\nincrease in labor cost.\n\nDevelopments in mechanization were important, but it was only when combined\nwith a growing abundance of energy derived from fossil fuels that they could\ndrive a massive increase in the productive capacities of industrial societies.\nThis increase in production occurred in tandem with a major transformation of\nthe role of labor vis-à-vis machinery in the workplace. Initially conceived as\nlabor-saving devices, factory machines were meant to assist workers with their\ndaily activities but quickly became the center of productive activity, shaping\nthe speed and character of work. Steam engines powered by coal and oil could\ndrive continuous mechanical actions that influenced the pace of work in the\nfactory. Work ceased to be primarily a product of human labor and took on an\nincreasingly machinelike character, with workers adapting to the needs of the\nmachine and its particular rhythms and cadences. Building on Smith, Karl Marx\nnoted as early as 1848 that automation abstracts labor from the production of\nfinished objects and turns a worker into “an appendage of the\nmachine.”[6](notes.html#ich02notes6)\n\nThe integration of workers’ bodies with machines was sufficiently thorough\nthat early industrialists could view their employees as a raw material to be\nmanaged and controlled like any other resource. Factory owners, using both\ntheir local political clout and paid muscle, sought to direct and restrict how\ntheir workers moved around within factory towns, sometimes even preventing\nworkers from emigrating to less mechanized regions of the\nworld.[7](notes.html#ich02notes7)\n\nThis also meant increasing control over time. The historian E. P. Thompson’s\nformative essay explores how the Industrial Revolution demanded greater\nsynchronization of work and stricter time\ndisciplines.[8](notes.html#ich02notes8) The transition to industrial\ncapitalism came with new divisions of labor, oversight, clocks, fines, and\ntime sheets—technologies that also influenced the way people experienced time.\nCulture was also a powerful tool. During the eighteenth and nineteenth\ncenturies, the propaganda about hard work came in the forms of pamphlets and\nessays on the importance of discipline and sermons on the virtues of early\nrising and working diligently for as long as\npossible.[9](notes.html#ich02notes9) The use of time came to be seen in both\nmoral and economic terms: understood as a currency, time could be well spent\nor squandered away. But as more rigid time disciplines were imposed in\nworkshops and factories, the more workers began to push back—campaigning over\ntime itself. By the 1800s, labor movements were strongly advocating for\nreducing the working day, which could run as long as sixteen hours. Time\nitself became a key site for struggle.\n\nMaintaining an efficient and disciplined workforce in the early factory\nnecessitated new systems of surveillance and control. One such invention from\nthe early years of industrial manufacturing was the inspection house, a\ncircular arrangement that placed all of a factory’s workers within sight of\ntheir supervisors, who worked from an office placed on a raised platform at\nthe center of the building. Developed in the 1780s in Russia by the English\nnaval engineer Samuel Bentham while under the employ of Prince Potemkin, this\narrangement allowed expert supervisors to keep an eye on their untrained\nsubordinates—mostly Russian peasants loaned to Bentham by Potemkin—for signs\nof poor working habits. It also allowed Bentham himself to keep an eye on the\nsupervisors for signs of ill-discipline. The supervisors, mostly master\nshipbuilders recruited from England, caused Bentham great annoyance due to\ntheir tendency to drink and get into petty disagreements with one another.\n“Morning after morning I am taken up chiefly with disputes amongst my\nOfficers,” Bentham complained.[10](notes.html#ich02notes10) As his\nfrustrations grew, he embarked on a redesign that would maximize his ability\nto keep a watchful eye on them, and on the system as a whole. With a visit\nfrom his elder brother, the utilitarian philosopher Jeremy Bentham, Samuel’s\ninspection house became the inspiration for the famous panopticon, a design\nfor a model prison featuring a central watchtower from which guards could\nsupervise the prisoners in their cells.[11](notes.html#ich02notes11)\n\nSince Michel Foucault’s _Discipline and Punish_ , it has become commonplace to\nconsider the prison as the origin point of today’s surveillance society, with\nthe elder Bentham as its ideological progenitor. In fact, the panoptic prison\nowes its origins to the work of the younger Bentham in the context of the\nearly manufacturing facility.[12](notes.html#ich02notes12) The panopticon\nbegan as a workplace mechanism well before it was conceptualized for prisons.\n\nWhile Samuel Bentham’s work on the inspection house has largely faded from our\ncollective memory, the story behind it remains part of our shared lexicon. The\ninspection house was part of a strategy coordinated by Bentham’s employer,\nPrince Potemkin, who wished to gain favor in Catherine the Great’s court by\ndemonstrating the potential for modernizing rural Russia and transforming the\npeasantry into a modern manufacturing workforce. The inspection house was\nbuilt to serve as a spectacle for visiting dignitaries and financiers, much\nlike the so-called Potemkin villages, which were little more than decorated\nfacades designed to distract observers from the impoverished rural village\nlandscapes discreetly obscured from view.\n\nAnd this is just one genealogy. Many other histories of labor shaped these\npractices of observation and control. The plantation colonies of the Americas\nused forced labor to maintain cash crops like sugar, and slave owners depended\non systems of constant surveillance. As Nicholas Mirzoeff describes in _The\nRight to Look_ , a central role in the plantation economy was the overseer,\nwho watched over the flow of production on the colonial slave plantation, and\ntheir oversight meant ordering the work of the slaves within a system of\nextreme violence.[13](notes.html#ich02notes13) As one planter described in\n1814, the role of the overseer was “to never leave the slave for an instant in\ninaction; he keeps the fabrication of sugar under surveillance, never leaving\nthe sugar-mill for an instant.”[14](notes.html#ich02notes14) This regime of\noversight also relied on bribing some slaves with food and clothing to enlist\nthem as an expanded surveillance network and to maintain discipline and speed\nof work when the overseer was occupied.[15](notes.html#ich02notes15)\n\nNow the role of oversight in the modern workplace is primarily deputized to\nsurveillance technologies. The managerial class employs a wide range of\ntechnologies to surveil employees, including tracking their movements with\napps, analyzing their social media feeds, comparing the patterns of replying\nto emails and booking meetings, and nudging them with suggestions to make them\nwork faster and more efficiently. Employee data is used to make predictions\nabout who is most likely to succeed (according to narrow, quantifiable\nparameters), who might be diverging from company goals, and who might be\norganizing other workers. Some use the techniques of machine learning, and\nothers are more simplistic algorithmic systems. As workplace AI becomes more\nprevalent, many of the more basic monitoring and tracking systems are being\nexpanded with new predictive capacities to become increasingly invasive\nmechanisms of worker management, asset control, and value extraction.\n\n### Potemkin AI and the Mechanical Turks\n\nOne of the less recognized facts of artificial intelligence is how many\nunderpaid workers are required to help build, maintain, and test AI systems.\nThis unseen labor takes many forms—supply-chain work, on-demand crowdwork, and\ntraditional service-industry jobs. Exploitative forms of work exist at all\nstages of the AI pipeline, from the mining sector, where resources are\nextracted and transported to create the core infrastructure of AI systems, to\nthe software side, where distributed workforces are paid pennies per\nmicrotask. Mary Gray and Sid Suri refer to such hidden labor as “ghost\nwork.”[16](notes.html#ich02notes16) Lilly Irani calls it “human-fueled\nautomation.”[17](notes.html#ich02notes17) These scholars have drawn attention\nto the experiences of crowdworkers or microworkers who perform the repetitive\ndigital tasks that underlie AI systems, such as labeling thousands of hours of\ntraining data and reviewing suspicious or harmful content. Workers do the\nrepetitive tasks that backstop claims of AI magic—but they rarely receive\ncredit for making the systems function.[18](notes.html#ich02notes18)\n\nAlthough this labor is essential to sustaining AI systems, it is usually very\npoorly compensated. A study from the United Nations International Labour\nOrganization surveyed 3,500 crowdworkers from seventy-five countries who\nroutinely offered their labor on popular task platforms like Amazon Mechanical\nTurk, Figure Eight, Microworkers, and Clickworker. The report found that a\nsubstantial number of people earned below their local minimum wage even though\nthe majority of respondents were highly educated, often with specializations\nin science and technology.[19](notes.html#ich02notes19) Likewise, those who do\ncontent moderation work—assessing violent videos, hate speech, and forms of\nonline cruelty for deletion—are also paid poorly. As media scholars such as\nSarah Roberts and Tarleton Gillespie have shown, this kind of work can leave\nlasting forms of psychological trauma.[20](notes.html#ich02notes20)\n\nBut without this kind of work, AI systems won’t function. The technical AI\nresearch community relies on cheap, crowd-sourced labor for many tasks that\ncan’t be done by machines. Between 2008 and 2016, the term “ _crowdsourcing_ ”\nwent from appearing in fewer than a thousand scientific articles to more than\ntwenty thousand—which makes sense, given that Mechanical Turk launched in\n2005. But during the same time frame, there was far too little debate about\nwhat ethical questions might be posed by relying on a workforce that is\ncommonly paid far below the minimum wage.[21](notes.html#ich02notes21)\n\nOf course, there are strong incentives to ignore the dependency on underpaid\nlabor from around the world. All the work they do—from tagging images for\ncomputer-vision systems to testing whether an algorithm is producing the right\nresults—refines AI systems much more quickly and cheaply, particularly when\ncompared to paying students to do these tasks (as was the earlier tradition).\nSo the issue has generally been ignored, and as one crowdwork research team\nobserved, clients using these platforms “expect cheap, ‘frictionless’\ncompletion of work without oversight, as if the platform were not an interface\nto human workers but a vast computer without living\nexpenses.”[22](notes.html#ich02notes22) In other words, clients treat human\nemployees as little more than machines, because to recognize their work and\ncompensate it fairly would make AI more expensive and less “efficient.”\n\nSometimes workers are directly asked to pretend to be an AI system. The\ndigital personal assistant start-up x.ai claimed that its AI agent, called\nAmy, could “magically schedule meetings” and handle many mundane daily tasks.\nBut a detailed Bloomberg investigation by journalist Ellen Huet revealed that\nit wasn’t artificial intelligence at all. “Amy” was carefully being checked\nand rewritten by a team of contract workers pulling long shifts. Similarly,\nFacebook’s personal assistant, M, was relying on regular human intervention by\na group of workers paid to review and edit every\nmessage.[23](notes.html#ich02notes23)\n\nFaking AI is an exhausting job. The workers at x.ai were sometimes putting in\nfourteen-hour shifts of annotating emails in order to sustain the illusion\nthat the service was automated and functioning 24/7. They couldn’t leave at\nthe end of the night until the queues of emails were finished. “I left feeling\ntotally numb and absent of any sort of emotion,” one employee told\nHuet.[24](notes.html#ich02notes24)\n\nWe could think of this as a kind of Potemkin AI—little more than facades,\ndesigned to demonstrate to investors and a credulous media what an automated\nsystem would look like while actually relying on human labor in the\nbackground.[25](notes.html#ich02notes25) In a charitable reading, these\nfacades are an illustration of what the system might be capable of when fully\nrealized, or a “minimum viable product” designed to demonstrate a concept. In\na less charitable reading, Potemkin AI systems are a form of deception\nperpetrated by technology vendors eager to stake a claim in the lucrative tech\nspace. But until there is another way to create large-scale AI that doesn’t\nuse extensive behind-the-curtain work by humans, this is a core logic of how\nAI works.\n\nThe writer Astra Taylor has described the kind of overselling of high-tech\nsystems that aren’t actually automated as\n“fauxtomation.”[26](notes.html#ich02notes26) Automated systems appear to do\nwork previously performed by humans, but in fact the system merely coordinates\nhuman work in the background. Taylor cites the examples of self-service kiosks\nin fast-food restaurants and self-checkout systems in supermarkets as places\nwhere an employee’s labor appears to have been replaced by an automated system\nbut where in fact the data-entry labor has simply been relocated from a paid\nemployee to the customer. Meanwhile, many online systems that provide\nseemingly automated decisions, such as removing duplicated entries or deleting\noffensive content, are actually powered by humans working from home on endless\nqueues of mundane tasks.[27](notes.html#ich02notes27) Much like Potemkin’s\ndecorated villages and model workshops, many valuable automated systems\nfeature a combination of underpaid digital pieceworkers and consumers taking\non unpaid tasks to make systems function. Meanwhile, companies seek to\nconvince investors and the general public that intelligent machines are doing\nthe work.\n\nWhat is at stake in this artifice? The true labor costs of AI are being\nconsistently downplayed and glossed over, but the forces driving this\nperformance run deeper than merely marketing trickery. It is part of a\ntradition of exploitation and deskilling, where people must do more tedious\nand repetitive work to back-fill for automated systems, for a result that may\nbe less effective or reliable than what it replaced. But this approach can\n_scale_ —producing cost reductions and profit increases while obscuring how\nmuch it depends on remote workers being paid subsistence wages and off-loading\nadditional tasks of maintenance or error-checking to consumers.\n\nFauxtomation does not directly replace human labor; rather, it relocates and\ndisperses it in space and time. In so doing it increases the disconnection\nbetween labor and value and thereby performs an ideological function. Workers,\nhaving been alienated from the results of their work as well as disconnected\nfrom other workers doing the same job, are liable to be more easily exploited\nby their employers. This is evident from the extremely low rates of\ncompensation crowdworkers receive around the\nworld.[28](notes.html#ich02notes28) They and other kinds of fauxtomation\nlaborers face the very real fact that their labor is interchangeable with any\nof the thousands of other workers who compete with them for work on platforms.\nAt any point they could be replaced by another crowdworker, or possibly by a\nmore automated system.\n\nIn 1770, Hungarian inventor Wolfgang von Kempelen constructed an elaborate\nmechanical chess player. He built a cabinet of wood and clockwork, behind\nwhich was seated a life-size mechanical man who could play chess against human\nopponents and win. This extraordinary contraption was first shown in the court\nof Empress Maria Theresa of Austria, then to visiting dignitaries and\ngovernment ministers, all of whom were utterly convinced that this was an\nintelligent automaton. The lifelike machine was dressed in a turban, wide-\nlegged pants, and a fur-trimmed robe to give the impression of an “oriental\nsorcerer.”[29](notes.html#ich02notes29) This racialized appearance signaled\nexotic otherness, at a time when the elites of Vienna would drink Turkish\ncoffee and dress their servants in Turkish\ncostumes.[30](notes.html#ich02notes30) It came to be known as the Mechanical\nTurk. But the chess-playing automaton was an elaborate illusion: it had a\nhuman chess master hiding inside an internal chamber, operating the machine\nfrom within and completely out of sight.\n\nSome 250 years later, the hoax lives on. Amazon chose to name its\nmicropayment-based crowdsourcing platform “Amazon Mechanical Turk,” despite\nthe association with racism and trickery. On Amazon’s platform, real workers\nremain out of sight in service of an illusion that AI systems are autonomous\nand magically intelligent.[31](notes.html#ich02notes31) Amazon’s initial\nmotivation to build Mechanical Turk emerged from the failures of its own\nartificial intelligence systems that could not adequately detect duplicate\nproduct pages on its retail site. After a series of futile and expensive\nattempts to solve the problem, the project engineers enlisted humans to fill\nthe gaps in its streamlined systems.[32](notes.html#ich02notes32) Now\nMechanical Turk connects businesses with an unseen and anonymous mass of\nworkers who bid against one another for the opportunity to work on a series of\nmicrotasks. Mechanical Turk is a massively distributed workshop where humans\nemulate and improve on AI systems by checking and correcting algorithmic\nprocesses. This is what Amazon chief executive Jeff Bezos brazenly calls “\n_artificial_ artificial intelligence.”[33](notes.html#ich02notes33)\n\nThese examples of Potemkin AI are all around. Some are directly visible: when\nwe see one of the current crop of self-driving cars on the streets, we also\nsee a human operator in the driver’s seat, ready to take control of the\nvehicle at the first sign of trouble. Others are less visible, as when we\ninteract with a web-based chat interface. We engage only with the facades that\nobscure their inner workings, designed to hide the various combinations of\nmachine and human labor in each interaction. We aren’t informed whether we are\nreceiving a response from the system itself or from a human operator paid to\nrespond on its behalf.\n\nIf there is growing uncertainty about whether we are engaging with an AI\nsystem or not, the feeling is mutual. In a paradox that many of us have\nexperienced, and ostensibly in order to prove true human identity when reading\na website, we are required to convince Google’s reCAPTCHA of our humanity. So\nwe dutifully select multiple boxes containing street numbers, or cars, or\nhouses. We are training Google’s image recognition algorithms for free. Again,\nthe myth of AI as affordable and efficient depends on layers of exploitation,\nincluding the extraction of mass unpaid labor to fine-tune the AI systems of\nthe richest companies on earth.\n\nContemporary forms of artificial intelligence are neither artificial nor\nintelligent. We can—and should—speak instead of the hard physical labor of\nmine workers, the repetitive factory labor on the assembly line, the\ncybernetic labor in the cognitive sweatshops of outsourced programmers, the\npoorly paid crowdsourced labor of Mechanical Turk workers, and the unpaid\nimmaterial work of everyday users. These are the places where we can see how\nplanetary computation depends on the exploitation of human labor, all along\nall the supply chain of extraction.\n\n### Visions of Disassembly and Workplace Automation: Babbage, Ford, and Taylor\n\nCharles Babbage is well known as the inventor of the first mechanical\ncomputer. In the 1820s, he developed the idea for the _Difference Engine_ , a\nmechanical calculating machine designed to generate mathematical and\nastronomical tables in a fraction of the time required to calculate them by\nhand. By the 1830s, he had a viable conceptual design for the _Analytical\nEngine_ , a programmable general-purpose mechanical computer, complete with a\nsystem of punch cards for providing it with\ninstructions.[34](notes.html#ich02notes34)\n\nBabbage also had a strong interest in liberal social theory and wrote\nextensively on the nature of labor—the combination of his interests in\ncomputation and worker automation. Following Adam Smith, he noted the division\nof labor as a means of streamlining factory work and generating efficiencies.\nHe went further, however, arguing that the industrial corporation could be\nunderstood as an analogue to a computational system. Just like a computer, it\nincluded multiple specialized units performing particular tasks, all\ncoordinated to produce a given body of work, but with the labor content of the\nfinished product rendered largely invisible by the process as a whole.\n\nIn Babbage’s more speculative writing, he imagined perfect flows of work\nthrough the system that could be visualized as data tables and monitored by\npedometers and repeating clocks.[35](notes.html#ich02notes35) Through a\ncombination of computation, surveillance, and labor discipline, he argued, it\nwould be possible to enforce ever-higher degrees of efficiency and quality\ncontrol.[36](notes.html#ich02notes36) It was a strangely prophetic vision.\nOnly in very recent years, with the adoption of artificial intelligence in the\nworkplace, has Babbage’s unusual twin goals of computation and worker\nautomation become possible at scale.\n\nBabbage’s economic thought extended outward from Smith’s but diverged in one\nimportant way. For Smith, the economic value of an object was understood in\nrelation to the cost of the labor required to produce it. In Babbage’s\nrendering, however, value in a factory was derived from investment in the\ndesign of the manufacturing process rather than from the labor force of its\nemployees. The real innovation was the logistical process, while workers\nsimply enacted the tasks defined for them and operated the machines as\ninstructed.\n\nFor Babbage, labor’s role in the value production chain was largely negative:\nworkers might fail to perform their tasks in the timely manner prescribed by\nthe precision machines they operated, whether through poor discipline, injury,\nabsenteeism, or acts of resistance. As noted by historian Simon Schaffer,\n“Under Babbage’s gaze, factories looked like perfect engines and calculating\nmachines like perfect computers. The workforce might be a source of trouble—it\ncould make tables err or factories fail—but it could not be seen as a source\nof value.”[37](notes.html#ich02notes37) The factory is conceived as a rational\ncalculating machine with only one weakness: its frail and untrustworthy human\nlabor force.\n\nBabbage’s theory was, of course, heavily inflected with a kind of financial\nliberalism, causing him to view labor as a problem that needed to be contained\nby automation. There was little consideration of the human costs of this\nautomation or of how automation might be put to use to improve the working\nlives of factory employees. Instead, Babbage’s idealized machinery aimed\nprimarily to maximize financial returns to the plant owners and their\ninvestors. In a similar vein, today’s proponents of workplace AI present a\nvision of production that prioritizes efficiency, cost-cutting, and higher\nprofits instead of, say, assisting their employees by replacing repetitive\ndrudge work. As Astra Taylor argues, “The kind of efficiency to which techno-\nevangelists aspire emphasizes standardization, simplification, and speed, not\ndiversity, complexity, and interdependence.”[38](notes.html#ich02notes38) This\nshould not surprise us: it is a necessary outcome of the standard business\nmodel of for-profit companies where the highest responsibility is to\nshareholder value. We are living the result of a system in which companies\nmust extract as much value as possible. Meanwhile, 94 percent of all new\nAmerican jobs created between 2005 and 2015 were for “alternative work”—jobs\nthat fall outside of full-time, salaried\nemployment.[39](notes.html#ich02notes39) As companies reap the benefits of\nincreasing automation, people are, on average, working longer hours, in more\njobs, for less pay, in insecure positions.\n\n### The Meat Market\n\nAmong the first industries to implement the type of mechanized production line\nBabbage envisioned was the Chicago meat-packing industry in the 1870s. Trains\nbrought livestock to the stockyard gates; the animals were funneled toward\ntheir slaughter in adjacent plants; and the carcasses were transported to\nvarious butchering and processing stations by means of a mechanized overhead\ntrolley system, forming what came to be known as the _disassembly line._ The\nfinished products could be shipped to faraway markets in specially designed\nrefrigerated rail cars.[40](notes.html#ich02notes40) Labor historian Harry\nBraverman noted that the Chicago stockyards realized Babbage’s vision of\nautomation and division of labor so completely that the human techniques\nrequired at any point on the disassembly line could be performed by just about\nanyone.[41](notes.html#ich02notes41) Low-skill laborers could be paid the bare\nminimum and replaced at the first sign of trouble, themselves becoming as\nthoroughly commoditized as the packaged meats they produced.\n\nWhen Upton Sinclair wrote _The Jungle_ , his harrowing novel about working-\nclass poverty, he set it in the meat-packing plants of Chicago. Although his\nintended point was to highlight the hardships of working immigrants in support\nof a socialist political vision, the book had an entirely different effect.\nThe depictions of diseased and rotting meat prompted a public outcry over food\nsafety and resulted in the passing of the Meat Inspection Act in 1906. But the\nfocus on workers was lost. Powerful institutions from the meat-packing\nindustry to Congress were prepared to intervene to improve the methods of\nproduction, but addressing the more fundamental exploitative labor dynamics\nthat propped up the entire system was off limits. The persistence of this\npattern underscores how power responds to critique: whether the product is cow\ncarcasses or facial recognition, the response is to accept regulation at the\nmargins but to leave untouched the underlying logics of production.\n\n![Images](../images/f0073-01.jpg)\n\nArmour Beef dressing floor, 1952. Courtesy Chicago Historical Society\n\nTwo other figures loom large in the history of workplace automation: Henry\nFord, whose moving assembly line from the early twentieth century was inspired\nby Chicago’s disassembly lines, and Frederick Winslow Taylor, the founder of\nscientific management. Taylor forged his career in the latter years of the\nnineteenth century developing a systematic approach to workplace management,\none that focused on the minute movements of workers’ bodies. Whereas Smith’s\nand Babbage’s notion of the division of labor was intended to provide a way to\ndistribute work between people and tools, Taylor narrowed his focus to include\nmicroscopic subdivisions in the actions of each worker.\n\nAs the latest technology for precisely tracking time, the stopwatch was to\nbecome a key instrument of workplace surveillance for shop-floor supervisors\nand production engineers alike. Taylor used stopwatches to perform studies of\nworkers that included detailed breakdowns of the time taken to perform the\ndiscrete physical motions involved in any given task. His _Principles of\nScientific Management_ established a system to quantify the movements of\nworkers’ bodies, with a view to deriving an optimally efficient layout of\ntools and working processes. The aim was maximum output at minimal\ncost.[42](notes.html#ich02notes42) It exemplified Marx’s description of the\ndomination of clock time, “Time is everything, man is nothing; he is, at most,\ntime’s carcass.”[43](notes.html#ich02notes43)\n\nFoxconn, the largest electronics manufacturing company in the world, which\nmakes Apple iPhones and iPads, is a vivid example of how workers are reduced\nto animal bodies performing tightly controlled tasks. Foxconn became notorious\nfor its rigid and militaristic management protocols after a spate of suicides\nin 2010.[44](notes.html#ich02notes44) Just two years later, the company’s\nchairman, Terry Gou, described his more than one million employees this way:\n“As human beings are also animals, to manage one million animals gives me a\nheadache.”[45](notes.html#ich02notes45)\n\nControlling time becomes another way to manage bodies. In service and fast-\nfood industries, time is measured down to the second. Assembly line workers\ncooking burgers at McDonald’s are assessed for meeting such targets as five\nseconds to process screen-based orders, twenty-two seconds to assemble a\nsandwich, and fourteen seconds to wrap the food.[46](notes.html#ich02notes46)\nStrict adherence to the clock removes margin for error from the system. The\nslightest delay (a customer taking too long to order, a coffee machine\nfailing, an employee calling in sick) can result in a cascading ripple of\ndelays, warning sounds, and management notifications.\n\nEven before McDonald’s workers join the assembly line, their time is being\nmanaged and tracked. An algorithmic scheduling system incorporating historical\ndata analysis and demand-prediction models determines workers’ shift\nallocations, resulting in work schedules that can vary from week to week and\neven day to day. A 2014 class action lawsuit against McDonald’s restaurants in\nCalifornia noted that franchisees are led by software that gives algorithmic\npredictions regarding employee-to-sales ratios and instructs managers to\nreduce staff quickly when demand drops.[47](notes.html#ich02notes47) Employees\nreported being told to delay clocking in to their shifts and instead to hang\naround nearby, ready to return to work if the restaurant started getting busy\nagain. Because employees are paid only for time clocked in, the suit alleged\nthat this amounted to significant wage theft on the part of the company and\nits franchisees.[48](notes.html#ich02notes48)\n\nAlgorithmically determined time allocations will vary from extremely short\nshifts of an hour or less to very long ones during busy times—whatever is most\nprofitable. The algorithm doesn’t factor in the human costs of waiting or\ngetting to work only to be sent home or being unable to predict one’s schedule\nand plan one’s life. This time theft helps the efficiency of the company, but\nit comes at the direct cost of the employees.\n\n### Managing Time, Privatizing Time\n\nFast-food entrepreneur Ray Kroc, who helped turn McDonald’s into a global\nfranchise, joined the lineage of Smith, Babbage, Taylor, and Ford when he\ndesigned the standard sandwich assembly line and made his employees follow it\nunthinkingly. Surveillance, standardization, and the reduction of individual\ncraft were central to Kroc’s method. As labor researchers Clare Mayhew and\nMichael Quinlan argue with regard to the McDonald’s standardized process, “The\nFordist management system documented work and production tasks in minuscule\ndetail. It required on-going documented participation and entailed detailed\ncontrol of each individual’s work process. There was an almost total removal\nof all conceptual work from execution of tasks.”[49](notes.html#ich02notes49)\n\nMinimizing the time spent at each station, or cycle time, became an object of\nintense scrutiny within the Fordist factory, with engineers dividing work\ntasks into ever-smaller pieces so they could be optimized and automated, and\nwith supervisors disciplining workers whenever they fell behind. Supervisors,\neven Henry Ford himself, could often be seen walking the length of the\nfactory, stopwatch in hand, recording cycle times and noting any discrepancies\nin a station’s productivity.[50](notes.html#ich02notes50)\n\nNow employers can passively surveil their workforce without walking out onto\nthe factory floor. Instead, workers clock in to their shifts by swiping access\nbadges or by presenting their fingerprints to readers attached to electronic\ntime clocks. They work in front of timing devices that indicate the minutes or\nseconds left to perform the current task before a manager is notified. They\nsit at workstations fitted with sensors that continuously report on their body\ntemperature, their physical distance from colleagues, the amount of time they\nspend browsing websites instead of performing assigned tasks, and so on.\nWeWork, the coworking behemoth that burned itself out over the course of 2019,\nquietly fitted its work spaces with surveillance devices in an effort to\ncreate new forms of data monetization. Its 2019 acquisition of the spatial\nanalytics startup Euclid raised concerns, with the suggestion that it planned\nto track its paying members as they moved through their\nfacilities.[51](notes.html#ich02notes51) Domino’s Pizza has added to its\nkitchens machine-vision systems that inspect a finished pizza to ensure the\nstaff made it according to prescribed standards.[52](notes.html#ich02notes52)\nSurveillance apparatuses are justified for producing inputs for algorithmic\nscheduling systems that further modulate work time, or to glean behavioral\nsignals that may correlate with signs of high or low performance, or merely\nsold to data brokers as a form of insight.\n\nIn her essay “How Silicon Valley Sets Time,” sociology professor Judy Wajcman\nargues that the aims of time-tracking tools and the demographic makeup of\nSilicon Valley are no coincidence.[53](notes.html#ich02notes53) Silicon\nValley’s elite workforce “is even younger, more masculine and more fully\ncommitted to working all hours,” while also creating productivity tools that\nare premised on a kind of ruthless, winner-takes-all race to maximal\nefficiency.[54](notes.html#ich02notes54) This means that young, mostly male\nengineers, often unencumbered by time-consuming familial or community\nresponsibilities, are building the tools that will police very different\nworkplaces, quantifying the productivity and desirability of employees. The\nworkaholism and round-the-clock hours often glorified by tech start-ups become\nan implicit benchmark against which other workers are measured, producing a\nvision of a standard worker that is masculinized, narrow, and reliant on the\nunpaid or underpaid care work of others.\n\n### Private Time\n\nThe coordination of time has become ever more granular in the technological\nforms of workplace management. For example, General Motors’ Manufacturing\nAutomation Protocol (MAP) was an early attempt to provide standard solutions\nto common manufacturing robot coordination problems, including clock\nsynchronization.[55](notes.html#ich02notes55) In due course, other, more\ngeneric time synchronization protocols that could be delivered over ethernet\nand TCP/IP networks emerged, including the Network Time Protocol (NTP), and,\nlater, the Precision Time Protocol (PTP), each of which spawned a variety of\ncompeting implementations across various operating systems. Both NTP and PTP\nfunction by establishing a hierarchy of clocks across a network, with a\n“master” clock driving the “slave” clocks.\n\nThe master-slave metaphor is riddled throughout engineering and computation.\nOne of the earliest uses of this racist metaphor dates back to 1904 describing\nastronomical clocks in a Cape Town observatory.[56](notes.html#ich02notes56)\nBut it wasn’t until 1960s that the master-slave terminology spread,\nparticularly after it was used in computing, starting with the Dartmouth\ntimesharing system. Mathematicians John Kemeny and Thomas Kurtz developed a\ntime-sharing program for access to computing resources after a suggestion by\none of the early founders of AI, John McCarthy. As they wrote in _Science_ in\n1968, “First, all computing for users takes place in the slave computer, while\nthe executive program (the ‘brains’ of the system) resides in the master\ncomputer. It is thus impossible for an erroneous or runaway user program in\nthe slave computer to ‘damage’ the executive program and thereby bring the\nwhole system to a halt.”[57](notes.html#ich02notes57) The problematic\nimplication that control is equivalent to intelligence would continue to shape\nthe AI field for decades. And as Ron Eglash has argued, the phrasing has a\nstrong echo of the pre–Civil War discourse on runaway\nslaves.[58](notes.html#ich02notes58)\n\nThe master-slave terminology has been seen as offensive by many and has been\nremoved from Python, a coding language common in machine learning, and Github,\na software development platform. But it persists in one of the most expansive\ncomputational infrastructures in the world. Google’s Spanner—named as such\nbecause it spans the entire planet—is a massive, globally distributed,\nsynchronously replicated database. It is the infrastructure that supports\nGmail, Google search, advertising, and all of Google’s distributed services.\n\nAt this scale, functioning across the globe, Spanner synchronizes time across\nmillions of servers in hundreds of data centers. Every data center has a “time\nmaster” unit that is always receiving GPS time. But because servers were\npolling a variety of master clocks, there was slight network latency and clock\ndrift. How to resolve this uncertainty? The answer was to create a new\ndistributed time protocol—a proprietary form of time—so that all servers could\nbe in sync regardless of where they were across the planet. Google called this\nnew protocol, without irony, TrueTime.\n\nGoogle’s TrueTime is a distributed time protocol that functions by\nestablishing trust relationships between the local clocks of data centers so\nthey can decide which peers to synchronize with. Benefiting from a\nsufficiently large number of reliable clocks, including GPS receivers and\natomic clocks that provide an extremely high degree of precision, and from\nsufficiently low levels of network latency, TrueTime allows a distributed set\nof servers to guarantee that events can occur in a determinate sequence across\na wide area network.[59](notes.html#ich02notes59)\n\nWhat’s most remarkable in this system of privatized Google time is how\nTrueTime manages uncertainty when there is clock drift on individual servers.\n“If the uncertainty is large, Spanner slows down to wait out that\nuncertainty,” Google researchers explain.[60](notes.html#ich02notes60) This\nembodies the fantasy of slowing down time, of moving it at will, and of\nbringing the planet under a single proprietary time code. If we think of the\nhuman experience of time as something shifting and subjective, moving faster\nor slower depending on where we are and whom we are with, then this is a\nsocial experience of time. TrueTime is the ability to create a shifting\ntimescale under the control of a centralized master clock. Just as Isaac\nNewton imagined an absolute form of time that exists independently of any\nperceiver, Google has invented its own form of universal time.\n\nProprietary forms of time have long been used to make machines run smoothly.\nRailroad magnates in the nineteenth century had their own forms of time. In\nNew England in 1849, for example, all trains were to adopt “true time at\nBoston as given by William Bond & Son, No. 26 Congress\nStreet.”[61](notes.html#ich02notes61) As Peter Galison has documented,\nrailroad executives weren’t fond of having to switch to other times depending\non which state their trains traveled to, and the general manager of the New\nYork & New England Railroad Company called switching to other times “a\nnuisance and great inconvenience and no use to anybody I can\nsee.”[62](notes.html#ich02notes62) But after a head-on train collision killed\nfourteen people in 1853, there was immense pressure to coordinate all of the\nclocks using the new technology of the telegraph.\n\nLike artificial intelligence, the telegraph was hailed as a unifying\ntechnology that would expand the capabilities of human beings. In 1889 Lord\nSalisbury boasted that the telegraph had “assembled all mankind upon one great\nplane.”[63](notes.html#ich02notes63) Businesses, governments, and the military\nused the telegraph to compile time into a coherent grid, erasing more local\nforms of timekeeping. And the telegraph was dominated by one of the first\ngreat industrial monopolies, Western Union. In addition to altering the\ntemporal and spatial boundaries of human interaction, communications theorist\nJames Carey argues that the telegraph also enabled a new form of monopoly\ncapitalism: “a new body of law, economic theory, political arrangements,\nmanagement techniques, organizational structures, and scientific rationales\nwith which to justify and make effective the development of a privately owned\nand controlled monopolistic corporation.”[64](notes.html#ich02notes64) While\nthis interpretation implies a kind of technological determinism in what was a\ncomplex series of developments, it is fair to say that the telegraph—paired\nwith the transatlantic cable—enabled imperial powers to maintain more\ncentralized control over their colonies.\n\nThe telegraph made time a central focus for commerce. Rather than traders\nexploiting the difference in prices between regions by buying low and selling\nhigh in varying locations, now they traded between time zones: in Carey’s\nterms, a shift from space to time, from arbitrage to\nfutures.[65](notes.html#ich02notes65) The privatized time zones of data\ncenters are just the latest example. The infrastructural ordering of time acts\nas a kind of “macrophysics of power,” determining new logics of information at\na planetary level.[66](notes.html#ich02notes66) Such power is necessarily\ncentralizing, creating orders of meaning that are extremely difficult to see,\nlet alone disrupt.\n\nDefiance of centralized time is a vital part of this history. In the 1930s,\nwhen Ford wanted more control over his global supply chain, he set up a rubber\nplantation and processing facility deep in the Brazilian rain forest, in a\ntown he named Fordlandia. He employed local workers to process rubber for\nshipping back to Detroit, but his attempts to impose his tightly controlled\nmanufacturing process on the local population backfired. Rioting workers tore\napart the factory’s time clocks, smashing the devices used to track the entry\nand exit of each worker in the plant.\n\nOther forms of insurgence have centered on adding friction to the work\nprocess. The French anarchist Émile Pouget used the term “sabotage” to mean\nthe equivalent of a “go slow” on the factory floor, when workers intentionally\nreduce their pace of work.[67](notes.html#ich02notes67) The objective was to\nwithdraw efficiency, to reduce the value of time as a currency. Although there\nwill always be ways to resist the imposed temporality of work, with forms of\nalgorithmic and video monitoring, this becomes much harder—as the relation\nbetween work and time is observed at ever closer range.\n\nFrom the fine modulations of time within factories to the big modulations of\ntime at the scale of planetary computation networks, defining time is an\nestablished strategy for centralizing power. Artificial intelligence systems\nhave allowed for greater exploitation of distributed labor around the world to\ntake advantage of uneven economic topologies. Simultaneously, the tech sector\nis creating for itself a smooth global terrain of time to strengthen and speed\nits business objectives. Controlling time—whether via the clocks for churches,\ntrains, or data centers—has always been a function of controlling the\npolitical order. But this battle for control has never been smooth, and it is\na far-reaching conflict. Workers have found ways to intervene and resist, even\nwhen technological developments were forced on them or presented as desirable\nimprovements, particularly if the only refinements were to increase\nsurveillance and company control.\n\n### Setting the Rate\n\nAmazon goes to great lengths to control what members of the public can see\nwhen they enter a fulfillment center. We are told about the fifteen-dollar-an-\nhour minimum wage and the perks for employees who can last longer than a year,\nand we are shown brightly lit break rooms that have Orwellian corporate\nslogans painted on the walls: “Frugality,” “Earn trust of others,” and “Bias\nfor action.” The official Amazon guide cheerily explains what is happening at\npredetermined stops with rehearsed vignettes. Any questions about labor\nconditions are carefully answered to paint the most positive picture. But\nthere are signs of unhappiness and dysfunction that are much harder to manage.\n\nOut on the picking floor, where associates must pick up gray containers (known\nas “totes”) full of purchases to ship, whiteboards bear the marks of recent\nmeetings. One had multiple complaints that the totes were stacked too high and\nthat constantly reaching up to grab them was causing considerable pain and\ninjuries. When asked about this, the Amazon guide quickly responded that this\nconcern was being addressed by lowering the height of the conveyor belt in key\nsections. This was seen as a success: a complaint had been registered and\naction would be taken. The guide took this opportunity to explain for the\nsecond time that this was why unions were unnecessary here, because\n“associates have many opportunities to interface with their managers,” and\nunionization only interferes with communication.[68](notes.html#ich02notes68)\n\n![Images](../images/f0083-01.jpg)\n\nFordlandia Time Clock, destroyed in the riot of December 1930. From the\nCollections of The Henry Ford\n\nBut on the way out of the facility, I walked past a live feed of messages from\nworkers on a large flat screen, with a sign above it that read, “The Voice of\nthe Associates.” This was far less varnished. Messages scrolled rapidly past\nwith complaints about arbitrary scheduling changes, the inability to book\nvacation time near holidays, and missing family occasions and birthdays. Pat\nresponses from management seemed to be multiple variations on the theme of “We\nvalue your feedback.”\n\n“Enough is enough. Amazon, we want you to treat us like humans, and not like\nrobots.”[69](notes.html#ich02notes69) These are the words of Abdi Muse,\nexecutive director of the Awood Center in Minneapolis, a community\norganization that advocates for the working conditions of Minnesota’s East\nAfrican populations. Muse is a soft-spoken defender of Amazon warehouse\nworkers who are pushing for better working conditions. Many workers in his\nMinnesota community have been hired by Amazon, which actively recruited them\nand added sweeteners to the deal, such as free busing to work.\n\nWhat Amazon didn’t advertise was “the rate”—the worker productivity metric\ndriving the fulfillment centers that quickly became unsustainable and,\naccording to Muse, inhumane. Workers began suffering high stress, injuries,\nand illness. Muse explained that if their rate went down three times they\nwould be fired, no matter how long they had worked at the warehouse. Workers\ntalked about having to skip bathroom breaks for fear that they would\nunderperform.\n\nBut the day we met, Muse was optimistic. Even though Amazon explicitly\ndiscourages unions, informal groups of workers were springing up across the\nUnited States and staging protests. He smiled widely as he reported that the\norganizing was starting to have an impact. “Something incredible is\nhappening,” he told me. “Tomorrow a group of Amazon workers will be walking\noff the job. It’s such a courageous group of women, and they are the real\nheroes.”[70](notes.html#ich02notes70) Indeed, that night, approximately sixty\nwarehouse workers walked out of a delivery center in Eagan, Minnesota, wearing\ntheir mandated yellow vests. They were mostly women of Somali descent, and\nthey held up signs in the rain, demanding such improvements as increased wages\nfor night shifts and weight restrictions on\nboxes.[71](notes.html#ich02notes71) Only a few days earlier, Amazon workers in\nSacramento, California, had protested the firing of an employee who had gone\none hour over her bereavement leave after a family member died. Two weeks\nbefore that, more than a thousand Amazon workers staged the first ever white-\ncollar walkout in the company’s history over its massive carbon footprint.\n\nEventually, Amazon’s representatives in Minnesota came to the table. They were\nhappy to discuss many issues but never “the rate.” “They said forget about\n‘the rate,’” recounted Muse. “We can talk about other issues, but the rate is\nour business model. We cannot change that.”[72](notes.html#ich02notes72) The\nworkers threatened to walk away from the table, and still Amazon would not\nbudge. For both sides, “the rate” was the core issue, but it was also the\nhardest to alter. Unlike other local labor disputes where the on-the-ground\nsupervisors might have been able to make concessions, the rate was set based\non what the executives and tech workers in Seattle—far removed from the\nwarehouse floor—had decided and had programmed Amazon’s computational\ndistribution infrastructure to optimize for. If the local warehouses were out\nof sync, Amazon’s ordering of time was threatened. Workers and organizers\nstarted to see this as the real issue. They are shifting their focus\naccordingly toward building a movement across different factories and sectors\nof Amazon’s workforce to address the core issues of power and centralization\nrepresented by the relentless rhythm of “the rate” itself.\n\nThese fights for time sovereignty, as we’ve seen, have a history. AI and\nalgorithmic monitoring are simply the latest technologies in the long\nhistorical development of factories, timepieces, and surveillance\narchitectures. Now many more sectors—from Uber drivers to Amazon warehouse\nworkers to highly paid Google engineers—perceive themselves in this shared\nfight. This was strongly articulated by the executive director of the New York\nTaxi Workers Alliance, Bhairavi Desai, who put it this way: “Workers always\nknow. They are out there building solidarity with each other, at red lights or\nin restaurants or in hotel queues, because they know that in order to prosper\nthey have to band together.”[73](notes.html#ich02notes73) Technologically\ndriven forms of worker exploitation are a widespread problem in many\nindustries. Workers are fighting against the logics of production and the\norder of time they must work within. The structures of time are never\ncompletely inhumane, but they are maintained right at the outer limit of what\nmost people can tolerate.\n\nCross-sector solidarity in labor organizing is nothing new. Many movements,\nsuch as those led by traditional labor unions, have connected workers in\ndifferent industries to win the victories of paid overtime, workplace safety,\nparental leave, and weekends. But as powerful business lobbies and neoliberal\ngovernments have chipped away at labor rights and protections over the past\nseveral decades and limited the avenues for worker organizing and\ncommunications, cross-sector support has become more\ndifficult.[74](notes.html#ich02notes74) Now AI-driven systems of extraction\nand surveillance have become a shared locus for labor organizers to fight as a\nunified front.[75](notes.html#ich02notes75)\n\n“We are all tech workers” has become a common sign at tech-related protests,\ncarried by programmers, janitors, cafeteria workers, and engineers\nalike.[76](notes.html#ich02notes76) It can be read in multiple ways: it\ndemands that the tech sector recognize the wide labor force it draws on to\nmake its products, infrastructures, and workplaces function. It also reminds\nus that so many workers use laptops and mobile devices for work, engage on\nplatforms like Facebook or Slack, and are subject to forms of workplace AI\nsystems for standardization, tracking, and assessment. This has set the stage\nfor a form of solidarity built around tech work. But there are risks in\ncentering tech workers and technology in what are more generalized and long-\nstanding labor struggles. All kinds of workers are subject to the extractive\ntechnical infrastructures that seek to control and analyze time to its finest\ngrain—many of whom have no identification with the technology sector or tech\nwork at all. The histories of labor and automation remind us that what is at\nstake is producing more just conditions for every worker, and this broader\ngoal should not depend on expanding the definition of tech work in order to\ngain legitimacy. We all have a collective stake in what the future of work\nlooks like.\n\n\n![Images](../images/f0088-01.jpg)\n\n\n## 3  \nData\n\nA young woman gazes upward, eyes focused on something outside the frame, as\nthough she is refusing to acknowledge the camera. In the next photograph, her\neyes are locked on the middle distance. Another image shows her with\ndisheveled hair and a downcast expression. Over the sequence of photos we see\nher aging over time, and the lines around her mouth turn down and deepen. In\nthe final frame she appears injured and dispirited. These are mug shots of a\nwoman across multiple arrests over many years of her life. Her images are\ncontained in a collection known as NIST Special Database 32–Multiple Encounter\nDataset, which is shared on the internet for researchers who would like to\ntest their facial recognition software.[1](notes.html#ich03notes1)\n\nThis dataset is one of several maintained by the National Institute of\nStandards and Technology (NIST), one of the oldest and most respected physical\nscience laboratories in the United States and now part of the Department of\nCommerce. NIST was created in 1901 to bolster the nation’s measurement\ninfrastructure and to create standards that could compete with economic rivals\nin the industrialized world, such as Germany and the United Kingdom.\nEverything from electronic health records to earthquake-resistant skyscrapers\nto atomic clocks is under the purview of NIST. It became the agency of\nmeasurement: of time, of communications protocols, of inorganic crystal\nstructures, of nanotechnology.[2](notes.html#ich03notes2) NIST’s purpose is to\nmake systems interoperable through defining and supporting standards, and this\nnow includes developing standards for artificial intelligence. One of the\ntesting infrastructures it maintains is for biometric data.\n\n![Images](../images/f0090-01.jpg)\n\nImages from NIST Special Database 32—Multiple Encounter Dataset (MEDS).\nNational Institute of Standards and Technology, U.S. Department of Commerce\n\nI first discovered the mug shot databases in 2017 when I was researching\nNIST’s data archives. Their biometric collections are extensive. For more than\nfifty years, NIST has collaborated with the Federal Bureau of Investigation on\nautomated fingerprint recognition and has developed methods to assess the\nquality of fingerprint scanners and imaging\nsystems.[3](notes.html#ich03notes3) After the terrorist attacks of September\n11, 2001, NIST became part of the national response to create biometric\nstandards to verify and track people entering the United\nStates.[4](notes.html#ich03notes4) This was a turning point for research on\nfacial recognition; it widened out from a focus on law enforcement to\ncontrolling people crossing national borders.[5](notes.html#ich03notes5)\n\nThe mug shot images themselves are devastating. Some people have visible\nwounds, bruising, and black eyes; some are distressed and crying. Others stare\nblankly back at the camera. Special Dataset 32 contains thousands of\nphotographs of deceased people with multiple arrests, as they endured repeated\nencounters with the criminal justice system. The people in the mug shot\ndatasets are presented as data points; there are no stories, contexts, or\nnames. Because mug shots are taken at the time of arrest, it’s not clear if\nthese people were charged, acquitted, or imprisoned. They are all presented\nalike.\n\nThe inclusion of these images in the NIST database has shifted their meaning\nfrom being used to identify individuals in systems of law enforcement to\nbecoming the technical baseline to test commercial and academic AI systems for\ndetecting faces. In his account of police photography, Allan Sekula has argued\nthat mug shots are part of a tradition of technical realism that aimed to\n“provide a standard physiognomic gauge of the\ncriminal.”[6](notes.html#ich03notes6) There are two distinct approaches in the\nhistory of the police photograph, Sekula observes. Criminologists like\nAlphonse Bertillon, who invented the mug shot, saw it as a kind of\nbiographical machine of identification, necessary to spot repeat offenders. On\nthe other hand, Francis Galton, the statistician and founding figure of\neugenics, used composite portraiture of prisoners as a way to detect a\nbiologically determined “criminal type.”[7](notes.html#ich03notes7) Galton was\nworking within a physiognomist paradigm in which the goal was to find a\ngeneralized look that could be used to identify deep character traits from\nexternal appearances. When mug shots are used as training data, they function\nno longer as tools of identification but rather to fine-tune an automated form\nof vision. We might think of this as Galtonian formalism. They are used to\ndetect the basic mathematical components of faces, to “reduce nature to its\ngeometrical essence.”[8](notes.html#ich03notes8)\n\nMug shots form part of the archive that is used to test facial-recognition\nalgorithms. The faces in the Multiple Encounter Dataset have become\nstandardized images, a technical substrate for comparing algorithmic accuracy.\nNIST, in collaboration with the Intelligence Advanced Research Projects\nActivity (IARPA), has run competitions with these mug shots in which\nresearchers compete to see whose algorithm is the fastest and most accurate.\nTeams strive to beat one another at tasks like verifying the identity of faces\nor retrieving a face from a frame of surveillance\nvideo.[9](notes.html#ich03notes9) The winners celebrate these victories; they\ncan bring fame, job offers, and industrywide\nrecognition.[10](notes.html#ich03notes10)\n\nNeither the people depicted in the photographs nor their families have any say\nabout how these images are used and likely have no idea that they are part of\nthe test beds of AI. The subjects of the mug shots are rarely considered, and\nfew engineers will ever look at them closely. As the NIST document describes\nthem, they exist purely to “refine tools, techniques, and procedures for face\nrecognition as it supports Next Generation Identification (NGI), forensic\ncomparison, training, analysis, and face image conformance and inter-agency\nexchange standards.”[11](notes.html#ich03notes11) The Multiple Encounter\nDataset description observes that many people show signs of enduring violence,\nsuch as scars, bruises, and bandages. But the document concludes that these\nsigns are “difficult to interpret due to the lack of ground truth for\ncomparison with a ‘clean’ sample.”[12](notes.html#ich03notes12) These people\nare not seen so much as individuals but as part of a shared technical\nresource—just another data component of the Facial Recognition Verification\nTesting program, the gold standard for the field.\n\nI’ve looked at hundreds of datasets over years of research into how AI systems\nare built, but the NIST mug shot databases are particularly disturbing because\nthey represent the model of what was to come. It’s not just the overwhelming\npathos of the images themselves. Nor is it solely the invasion of privacy they\nrepresent, since suspects and prisoners have no right to refuse being\nphotographed. It’s that the NIST databases foreshadow the emergence of a logic\nthat has now thoroughly pervaded the tech sector: the unswerving belief that\neverything is data and is there for the taking. It doesn’t matter where a\nphotograph was taken or whether it reflects a moment of vulnerability or pain\nor if it represents a form of shaming the subject. It has become so normalized\nacross the industry to take and use whatever is available that few stop to\nquestion the underlying politics.\n\nMug shots, in this sense, are the urtext of the current approach to making AI.\nThe context—and exertion of power—that these images represent is considered\nirrelevant because they no longer exist as distinct things unto themselves.\nThey are not seen to carry meanings or ethical weight as images of individual\npeople or as representations of structural power in the carceral system. The\npersonal, the social, and the political meanings are all imagined to be\nneutralized. I argue this represents a shift from _image_ to _infrastructure_\n, where the meaning or care that might be given to the image of an individual\nperson, or the context behind a scene, is presumed to be erased at the moment\nit becomes part of an aggregate mass that will drive a broader system. It is\nall treated as data to be run through functions, material to be ingested to\nimprove technical performance. This is a core premise in the ideology of data\nextraction.\n\nMachine learning systems are trained on images like these every day—images\nthat were taken from the internet or from state institutions without context\nand without consent. They are anything but neutral. They represent personal\nhistories, structural inequities, and all the injustices that have accompanied\nthe legacies of policing and prison systems in the United States. But the\npresumption that somehow these images can serve as apolitical, inert material\ninfluences how and what a machine learning tool “sees.” A computer vision\nsystem can detect a face or a building but not why a person was inside a\npolice station or any of the social and historical context surrounding that\nmoment. Ultimately, the specific instances of data—a picture of a face, for\nexample—aren’t considered to matter for training an AI model. All that matters\nis a sufficiently varied aggregate. Any individual image could easily be\nsubstituted for another and the system would work the same. According to this\nworldview, there is always more data to capture from the constantly growing\nand globally distributed treasure chest of the internet and social media\nplatforms.\n\nA person standing in front of a camera in an orange jumpsuit, then, is\ndehumanized as just more data. The history of these images, how they were\nacquired, and their institutional, personal, and political contexts are not\nconsidered relevant. The mug shot collections are used like any other\npractical resource of free, well-lit images of faces, a benchmark to make\ntools like facial recognition function. And like a tightening ratchet, the\nfaces of deceased persons, suspects, and prisoners are harvested to sharpen\nthe police and border surveillance facial recognition systems that are then\nused to monitor and detain more people.\n\nThe last decade has seen a dramatic capture of digital material for AI\nproduction. This data is the basis for sense-making in AI, not as classical\nrepresentations of the world with individual meaning, but as a mass collection\nof data for machine abstractions and operations. This large-scale capture has\nbecome so fundamental to the AI field that it is unquestioned. So how did we\nget here? What ways of conceiving data have facilitated this stripping of\ncontext, meaning, and specificity? How is training data acquired, understood,\nand used in machine learning? In what ways does training data limit _what_ and\n_how_ AI interprets the world? What forms of power do these approaches enhance\nand enable?\n\nIn this chapter I show how data has become a driving force in the success of\nAI and its mythos and how everything that can be readily captured is being\nacquired. But the deeper implications of this standard approach are rarely\naddressed, even as it propels further asymmetries of power. The AI industry\nhas fostered a kind of ruthless pragmatism, with minimal context, caution, or\nconsent-driven data practices while promoting the idea that the mass\nharvesting of data is necessary and justified for creating systems of\nprofitable computational “intelligence.” This has resulted in a profound\nmetamorphosis, where all forms of image, text, sound, and video are just raw\ndata for AI systems and the ends are thought to justify the means. But we\nshould ask: Who has benefited most from this transformation, and why have\nthese dominant narratives of data persisted? And as we saw in the previous\nchapters, the logic of extraction that has shaped the relationship to the\nearth and to human labor is also a defining feature of how data is used and\nunderstood in AI. By looking closely at training data as a central example in\nthe ensemble of machine learning, we can begin to see what is at stake in this\ntransformation.\n\n### Training Machines to See\n\nIt’s useful to consider why machine learning systems currently demand massive\namounts of data. One example of the problem in action is computer vision, the\nsubfield of artificial intelligence concerned with teaching machines to detect\nand interpret images. For reasons that are rarely acknowledged in the field of\ncomputer science, the project of interpreting images is a profoundly complex\nand relational endeavor. Images are remarkably slippery things, laden with\nmultiple potential meanings, irresolvable questions, and contradictions. Yet\nnow it’s common practice for the first steps of creating a computer vision\nsystem to scrape thousands—or even millions—of images from the internet,\ncreate and order them into a series of classifications, and use this as a\nfoundation for how the system will perceive observable reality. These vast\ncollections are called training datasets, and they constitute what AI\ndevelopers often refer to as “ground truth.”[13](notes.html#ich03notes13)\nTruth, then, is less about a factual representation or an agreed-upon reality\nand more commonly about a jumble of images scraped from whatever various\nonline sources were available.\n\nFor supervised machine learning, human engineers supply labeled training data\nto a computer. Two distinct types of algorithms then come into play:\n_learners_ and _classifiers._ The learner is the algorithm that is trained on\nthese labeled data examples; it then informs the classifier how best to\nanalyze the relation between the new inputs and the desired target output (or\nprediction). It might be predicting whether a face is contained in an image or\nwhether an email is spam. The more examples of correctly labeled data there\nare, the better the algorithm will be at producing accurate predictions. There\nare many kinds of machine learning models, including neural networks, logistic\nregression, and decision trees. Engineers will choose a model based on what\nthey are building—be it a facial recognition system or a means of detecting\nsentiment on social media—and fit it to their computational resources.\n\nConsider the task of building a machine learning system that can detect the\ndifference between pictures of apples and oranges. First, a developer has to\ncollect, label, and train a neural network on thousands of labeled images of\napples and oranges. On the software side, the algorithms conduct a statistical\nsurvey of the images and develop a model to recognize the difference between\nthe two classes. If all goes according to plan, the trained model will be able\nto distinguish the difference between images of apples and oranges that it has\nnever encountered before.\n\nBut if, in our example, all of the training images of apples are red and none\nare green, then a machine learning system might deduce that “all apples are\nred.” This is what is known as an _inductive inference_ , an open hypothesis\nbased on available data, rather than a _deductive inference_ , which follows\nlogically from a premise.[14](notes.html#ich03notes14) Given how this system\nwas trained, a green apple wouldn’t be recognized as an apple at all. Training\ndatasets, then, are at the core of how most machine learning systems make\ninferences. They serve as the primary source material that AI systems use to\nform the basis of their predictions.\n\nTraining data also defines more than just the features of machine learning\nalgorithms. It is used to assess how they perform over time. Like prized\nthoroughbreds, machine learning algorithms are constantly raced against one\nanother in competitions all over the world to see which ones perform the best\nwith a given dataset. These benchmark datasets become the alphabet on which a\n_lingua franca_ is based, with many labs from multiple countries converging\naround canonical sets to try to outperform one another. One of the best-known\ncompetitions is the ImageNet Challenge, where researchers compete to see whose\nmethods can most accurately classify and detect objects and\nscenes.[15](notes.html#ich03notes15)\n\nOnce training sets have been established as useful benchmarks, they are\ncommonly adapted, built upon, and expanded. As we will see in the next\nchapter, a type of genealogy of training sets emerges—they inherit learned\nlogic from earlier examples and then give rise to subsequent ones. For\nexample, ImageNet draws on the taxonomy of words inherited from the\ninfluential 1980s lexical database known as WordNet; and WordNet inherits from\nmany sources, including the Brown Corpus of one million words, published in\n1961. Training datasets stand on the shoulders of older classifications and\ncollections. Like an expanding encyclopedia, the older forms remain and new\nitems are added over decades.\n\nTraining data, then, is the foundation on which contemporary machine learning\nsystems are built.[16](notes.html#ich03notes16) These datasets shape the\nepistemic boundaries governing how AI operates and, in that sense, create the\nlimits of how AI can “see” the world. But training data is a brittle form of\nground truth—and even the largest troves of data cannot escape the fundamental\nslippages that occur when an infinitely complex world is simplified and sliced\ninto categories.\n\n### A Brief History of the Demand for Data\n\n“The world has arrived at an age of cheap complex devices of great\nreliability; and something is bound to come of it.” So said Vannevar Bush, the\ninventor and administrator who oversaw the Manhattan Project as director of\nthe Office of Scientific Research and Development and later was integral to\nthe creation of the National Science Foundation. It was July 1945; the bombs\nwere yet to drop on Hiroshima and Nagasaki, and Bush had a theory about a new\nkind of data-connecting system that was yet to be born. He envisaged the\n“advanced arithmetical machines of the future” that would perform at extremely\nfast speed and “select their own data and manipulate it in accordance with the\ninstructions.” But the machines would need monumental amounts of data: “Such\nmachines will have enormous appetites. One of them will take instructions and\ndata from a whole roomful of girls armed with simple key board punches, and\nwill deliver sheets of computed results every few minutes. There will always\nbe plenty of things to compute in the detailed affairs of millions of people\ndoing complicated things.”[17](notes.html#ich03notes17)\n\nThe “roomful of girls” Bush referred to were the keypunch operators doing the\nday-to-day work of computation. As historians Jennifer Light and Mar Hicks\nhave shown, these women were often dismissed as input devices for intelligible\ndata records. In fact, their role was just as important to crafting data and\nmaking systems work as that of the engineers who designed the wartime-era\ndigital computers.[18](notes.html#ich03notes18) But the relationship between\ndata and processing machinery was already being imagined as one of endless\nconsumption. The machines would be data-hungry, and there would surely be a\nwide horizon of material to extract from millions of people.\n\nIn the 1970s, artificial intelligence researchers were mainly exploring what’s\ncalled an expert systems approach: rules-based programming that aims to reduce\nthe field of possible actions by articulating forms of logical reasoning. But\nit quickly became evident that this approach was fragile and impractical in\nreal-world settings, where a rule set was rarely able to handle uncertainty\nand complexity.[19](notes.html#ich03notes19) New approaches were needed. By\nthe mid-1980s, research labs were turning toward probabilistic or brute force\napproaches. In short, they were using lots of computing cycles to calculate as\nmany options as possible to find the optimal result.\n\nOne significant example was the speech recognition group at IBM Research. The\nproblem of speech recognition had primarily been dealt with using linguistic\nmethods, but then information theorists Fred Jelinek and Lalit Bahl formed a\nnew group, which included Peter Brown and Robert Mercer (long before Mercer\nbecame a billionaire, associated with funding Cambridge Analytica, Breitbart\nNews, and Donald Trump’s 2016 presidential campaign). They tried something\ndifferent. Their techniques ultimately produced precursors for the speech\nrecognition systems underlying Siri and Dragon Dictate, as well as machine\ntranslation systems like Google Translate and Microsoft Translator.\n\nThey started using statistical methods that focused more on how often words\nappeared in relation to one another, rather than trying to teach computers a\nrules-based approach using grammatical principles or linguistic features.\nMaking this statistical approach work required an enormous amount of real\nspeech and text data, or training data. The result, as media scholar Xiaochang\nLi writes, was that it required “a radical reduction of speech to merely data,\nwhich could be modeled and interpreted in the absence of linguistic knowledge\nor understanding. Speech _as such_ ceased to matter.” This shift was\nincredibly significant, and it would become a pattern repeated for decades:\nthe reduction from context to data, from meaning to statistical pattern\nrecognition. Li explains:\n\nThe reliance on data over linguistic principles, however, presented a new set\nof challenges, for it meant that the statistical models were necessarily\ndetermined by the characteristics of training data. As a result, the size of\nthe dataset became a central concern. . . . Larger datasets of observed\noutcomes not only improved the probability estimates for a random process, but\nalso increased the chance that the data would capture more rarely-occurring\noutcomes. Training data size, in fact, was so central to IBM’s approach that\nin 1985, Robert Mercer explained the group’s outlook by simply proclaiming,\n“There’s no data like more data.”[20](notes.html#ich03notes20)\n\nFor several decades, that data was remarkably hard to come by. As Lalit Bahl\ndescribes in an interview with Li, “Back in those days . . . you couldn’t even\nfind a million words in computer-readable text very easily. And we looked all\nover the place for text.”[21](notes.html#ich03notes21) They tried IBM\ntechnical manuals, children’s novels, patents of laser technology, books for\nthe blind, and even the typed correspondence of IBM Fellow Dick Garwin, who\ncreated the first hydrogen bomb design.[22](notes.html#ich03notes22) Their\nmethod strangely echoed a short story by the science fiction author Stanislaw\nLem, in which a man called Trurl decides to build a machine that would write\npoetry. He starts with “eight hundred and twenty tons of books on cybernetics\nand twelve thousand tons of the finest poetry.”[23](notes.html#ich03notes23)\nBut Trurl realizes that to program an autonomous poetry machine, one needs “to\nrepeat the entire Universe from the beginning—or at least a good piece of\nit.”[24](notes.html#ich03notes24)\n\nUltimately, the IBM Continuous Speech Recognition group found their “good\npiece” of the universe from an unlikely source. A major federal antitrust\nlawsuit was filed against IBM in 1969; the proceedings lasted for thirteen\nyears, and almost a thousand witnesses were called. IBM employed a large staff\njust to digitize all of the deposition transcripts onto Hollerith punch cards.\nThis ended up creating a corpus of a hundred million words by the mid-1980s.\nThe notoriously antigovernment Mercer called this a “case of utility\naccidentally created by the government in spite of\nitself.”[25](notes.html#ich03notes25)\n\nIBM wasn’t the only group starting to gather words by the ton. From 1989 to\n1992, a team of linguists and computer scientists at the University of\nPennsylvania worked on the Penn Treebank Project, an annotated database of\ntext. They collected four and a half million words of American English for the\npurpose of training natural language processing systems. Their sources\nincluded Department of Energy abstracts, Dow Jones newswire articles, and\nFederal News Service reports of “terrorist activity” in South\nAmerica.[26](notes.html#ich03notes26) The emerging text collections borrowed\nfrom earlier collections and then contributed new sources. Genealogies of data\ncollections began to emerge, each building on the last—and often importing the\nsame peculiarities, issues, or omissions wholesale.\n\nAnother classic corpus of text came from the fraud investigations of Enron\nCorporation after it declared the largest bankruptcy in American history. The\nFederal Energy Regulatory Commission seized the emails of 158 employees for\nthe purposes of legal discovery.[27](notes.html#ich03notes27) It also decided\nto release these emails online because “the public’s right to disclosure\noutweighs the individual’s right to privacy.”[28](notes.html#ich03notes28)\nThis became an extraordinary collection. Over half a million exchanges in\neveryday speech could now be used as a linguistic mine: one that nonetheless\nrepresented the gender, race, and professional skews of those 158 workers. The\nEnron corpus has been cited in thousands of academic papers. Despite its\npopularity, it is rarely looked at closely: the _New Yorker_ described it as\n“a canonic research text that no one has actually\nread.”[29](notes.html#ich03notes29) This construction of and reliance on\ntraining data anticipated a new way of doing things. It transformed the field\nof natural language processing and laid the foundations of what would become\nnormal practice in machine learning.\n\nThe seeds of later problems were planted here. Text archives were seen as\nneutral collections of language, as though there was a general equivalence\nbetween the words in a technical manual and how people write to colleagues via\nemail. All text was repurposable and swappable, so long as there was enough of\nit that it could train a language model to predict with high levels of success\nwhat word might follow another. Like images, text corpuses work on the\nassumption that all training data is interchangeable. But language isn’t an\ninert substance that works the same way regardless of where it is found.\nSentences taken from Reddit will be different from those composed by\nexecutives at Enron. Skews, gaps, and biases in the collected text are built\ninto the bigger system, and if a language model is based on the kinds of words\nthat are clustered together, it matters where those words come from. There is\nno neutral ground for language, and all text collections are also accounts of\ntime, place, culture, and politics. Further, languages that have less\navailable data are not served by these approaches and so are often left\nbehind.[30](notes.html#ich03notes30)\n\nClearly there are many histories and contexts that combine within IBM’s\ntraining data, the Enron archive, or the Penn Treebank. How do we unpack what\nis and is not meaningful to understand these datasets? How does one\ncommunicate warnings like, “This dataset likely reflects skews related to its\nreliance on news stories about South American terrorists in the 1980s”? The\norigins of the underlying data in a system can be incredibly significant, and\nyet there are still, thirty years later, no standardized practices to note\nwhere all this data came from or how it was acquired—let alone what biases or\nclassificatory politics these datasets contain that will influence all the\nsystems that come to rely on them.[31](notes.html#ich03notes31)\n\n### Capturing the Face\n\nWhile computer-readable text was becoming highly valued for speech\nrecognition, the human face was the core concern for building systems of\nfacial recognition. One central example emerged in the last decade of the\ntwentieth century, funded by the Department of Defense CounterDrug Technology\nDevelopment Program Office. It sponsored the Face Recognition Technology\n(FERET) program to develop automatic face recognition for intelligence and law\nenforcement. Before FERET, little training data of human faces was available,\nonly a few collections of fifty or so faces here and there—not enough to do\nfacial recognition at scale. The U.S. Army Research Laboratory led the\ntechnical project of creating a training set of portraits of more than a\nthousand people, in multiple poses, to make a grand total of 14,126 images.\nLike NIST’s mug shot collections, FERET became a standard benchmark—a shared\nmeasuring tool to compare approaches for detecting faces.\n\nThe tasks that the FERET infrastructure was created to support included, once\nagain, automated searching of mug shots, as well as monitoring airports and\nborder crossings and searching driver’s license databases for “fraud\ndetection” (multiple welfare claims was a particular example mentioned in\nFERET research papers).[32](notes.html#ich03notes32) But there were two\nprimary testing scenarios. In the first, an electronic mug book of known\nindividuals would be presented to an algorithm, which then had to locate the\nclosest matches from a large gallery. The second scenario focused on border\nand airport control: identifying a known individual—“smugglers, terrorists, or\nother criminals”—from a large population of unknown people.\n\nThese photographs are machine-readable by design, and not meant for human\neyes, yet they make for remarkable viewing. The images are surprisingly\nbeautiful—high-resolution photographs captured in the style of formal\nportraiture. Taken with 35 mm cameras at George Mason University, the tightly\nframed headshots depict a wide range of people, some of whom seem to have\ndressed for the occasion with carefully styled hair, jewelry, and makeup. The\nfirst set of photographs, taken between 1993 and 1994, are like a time capsule\nof early nineties haircuts and fashion. The subjects were asked to turn their\nheads to multiple positions; flicking through the images, you can see profile\nshots, frontal images, varying levels of illumination, and sometimes different\noutfits. Some subjects were photographed over several years, in order to begin\nto study how to track people as they age. Each subject was briefed about the\nproject and signed a release form that had been approved by the university’s\nethics review board. Subjects knew what they were participating in and gave\nfull consent.[33](notes.html#ich03notes33) This level of consent would become\na rarity in later years.\n\nFERET was the high-water mark of a formal style of “making data,” before the\ninternet began offering mass extraction without any permissions or careful\ncamera work. Even at this early stage, though, there were problems with the\nlack of diversity of the faces collected. The FERET research paper from 1996\nadmits that “some questions were raised about the age, racial, and sexual\ndistribution of the database” but that “at this stage of the program, the key\nissue was algorithm performance on a database of a large number of\nindividuals.”[34](notes.html#ich03notes34) Indeed, FERET was extraordinarily\nuseful for this. As the interest in terrorist detection intensified and\nfunding for facial recognition dramatically increased after 9/11, FERET became\nthe most commonly used benchmark. From that point onward, biometric tracking\nand automated vision systems would rapidly expand in scale and ambition.\n\n### From the Internet to ImageNet\n\nThe internet, in so many ways, changed everything; it came to be seen in the\nAI research field as something akin to a natural resource, there for the\ntaking. As more people began to upload their images to websites, to photo-\nsharing services, and ultimately to social media platforms, the pillaging\nbegan in earnest. Suddenly, training sets could reach a size that scientists\nin the 1980s could never have imagined. Gone was the need to stage photo\nshoots using multiple lighting conditions, controlled parameters, and devices\nto position the face. Now there were millions of selfies in every possible\nlighting condition, position, and depth of field. People began to share their\nbaby photos, family snaps, and images of how they looked a decade ago, an\nideal resource for tracking genetic similarity and face aging. Trillions of\nlines of text, containing both formal and informal forms of speech, were\npublished every day. It was all grist for the mills of machine learning. And\nit was vast. As an example, on an average day in 2019, approximately 350\nmillion photographs were uploaded to Facebook and 500 million tweets were\nsent.[35](notes.html#ich03notes35) And that’s just two platforms based in the\nUnited States. Anything and everything online was primed to become a training\nset for AI.\n\nThe tech industry titans were now in a powerful position: they had a pipeline\nof endlessly refreshing images and text, and the more people shared their\ncontent, the more the tech industry’s power grew. People would happily label\ntheir photographs with names and locations, free of charge, and that unpaid\nlabor resulted in having more accurate, labeled data for machine vision and\nlanguage models. Within the industry, these collections are highly valuable.\nThey are proprietary troves that are rarely shared, given both the privacy\nissues and the competitive advantage they represent. But those outside the\nindustry, such as the leading computer science labs in academia, wanted the\nsame advantages. How could they afford to harvest people’s data and have it\nhand-labeled by willing human participants? That’s when new ideas began to\nemerge: combining images and text extracted from the internet with the labor\nof low-paid crowdworkers.\n\nOne of the most significant training sets in AI is ImageNet. It was first\nconceptualized in 2006, when Professor Fei-Fei Li decided to build an enormous\ndataset for object recognition. “We decided we wanted to do something that was\ncompletely historically unprecedented,” Li said. “We’re going to map out the\nentire world of objects.”[36](notes.html#ich03notes36) The breakthrough\nresearch poster was published by the ImageNet team at a computer vision\nconference in 2009. It opened with this description:\n\nThe digital era has brought with it an enormous explosion of data. The latest\nestimations put a number of more than 3 billion photos on Flickr, a similar\nnumber of video clips on YouTube and an even larger number for images in the\nGoogle Image Search database. More sophisticated and robust models and\nalgorithms can be proposed by exploiting these images, resulting in better\napplications for users to index, retrieve, organize and interact with these\ndata.[37](notes.html#ich03notes37)\n\nFrom the outset, data was characterized as something voluminous, disorganized,\nimpersonal, and ready to be exploited. According to the authors, “Exactly how\nsuch data can be utilized and organized is a problem yet to be solved.” By\nextracting millions of images from the internet, primarily from search engines\nusing the image-search option, the team produced a “large-scale ontology of\nimages” that was meant to serve as a resource for “providing critical training\nand benchmarking data” for object and image recognition algorithms. Using this\napproach, ImageNet grew enormous. The team mass-harvested more than fourteen\nmillion images from the internet to be organized into more than twenty\nthousand categories. Ethical concerns about taking people’s data were not\nmentioned in any of the team’s research papers, even though many thousands of\nthe images were of a highly personal and compromising nature.\n\nOnce the images had been scraped from the internet, a major concern arose: Who\nwould label them all and put them into intelligible categories? As Li\ndescribes it, the team’s first plan was to hire undergraduate students for ten\ndollars an hour to find images manually and add them to the\ndataset.[38](notes.html#ich03notes38) But she realized that with their budget,\nit would take more than ninety years to complete the project. The answer came\nwhen a student told Li about a new service: Amazon Mechanical Turk. As we saw\nin [chapter 2](ch02.html), this distributed platform meant that it was\nsuddenly possible to access a distributed labor force to do online tasks, like\nlabeling and sorting images, at scale and at low cost. “He showed me the\nwebsite, and I can tell you literally that day I knew the ImageNet project was\ngoing to happen,” Li said. “Suddenly we found a tool that could scale, that we\ncould not possibly dream of by hiring Princeton\nundergrads.”[39](notes.html#ich03notes39) Unsurprisingly, the undergraduates\ndid not get the job.\n\nInstead, ImageNet would become, for a time, the world’s largest academic user\nof Amazon’s Mechanical Turk, deploying an army of piecemeal workers to sort an\naverage of fifty images a minute into thousands of\ncategories.[40](notes.html#ich03notes40) There were categories for apples and\nairplanes, scuba divers and sumo wrestlers. But there were cruel, offensive,\nand racist labels, too: photographs of people were classified into categories\nlike “alcoholic,” “ape-man,” “crazy,” “hooker,” and “slant eye.” All of these\nterms were imported from WordNet’s lexical database and given to crowdworkers\nto pair with images. Over the course of a decade, ImageNet grew into a\ncolossus of object recognition for machine learning and a powerfully important\nbenchmark for the field. The approach of mass data extraction without consent\nand labeling by underpaid crowdworkers would become standard practice, and\nhundreds of new training datasets would follow ImageNet’s lead. As we will see\nin the next chapter, these practices—and the labeled data they\ngenerated—eventually came back to haunt the project.\n\n### The End of Consent\n\nThe early years of the twenty-first century marked a shift away from consent-\ndriven data collection. In addition to dispensing with the need for staged\nphoto shoots, those responsible for assembling datasets presumed that the\ncontents of the internet were theirs for the taking, beyond the need for\nagreements, signed releases, and ethics reviews. Now even more troubling\npractices of extraction began to emerge. For example, at the Colorado Springs\ncampus of the University of Colorado, a professor installed a camera on the\nmain walkway of the campus and secretly captured photos of more than seventeen\nhundred students and faculty—all to train a facial recognition system of his\nown.[41](notes.html#ich03notes41) A similar project at Duke University\nharvested footage of more than two thousand students without their knowledge\nas they went between their classes and then published the results on the\ninternet. The dataset, called DukeMTMC (for multitarget, multicamera facial\nrecognition), was funded by the U.S. Army Research Office and the National\nScience Foundation.[42](notes.html#ich03notes42)\n\nThe DukeMTMC project was roundly criticized after an investigative project by\nartists and researchers Adam Harvey and Jules LaPlace showed that the Chinese\ngovernment was using the images to train systems for the surveillance of\nethnic minorities. This spurred an investigation by Duke’s institutional\nreview board, which determined that this was a “significant deviation” from\nacceptable practices. The dataset was removed from the\ninternet.[43](notes.html#ich03notes43)\n\nBut what happened at the University of Colorado and Duke were by no means\nisolated cases. At Stanford University, researchers commandeered a webcam from\na popular café in San Francisco to extract almost twelve thousand images of\n“everyday life of a busy downtown café” without anyone’s\nconsent.[44](notes.html#ich03notes44) Over and over, data extracted without\npermission or consent would be uploaded for machine learning researchers, who\nwould then use it as an infrastructure for automated imaging systems.\n\nAnother example is Microsoft’s landmark training dataset MS-Celeb, which\nscraped approximately ten million photos of a hundred thousand celebrities\nfrom the internet in 2016. At the time, it was the largest public facial\nrecognition dataset in the world, and the people included were not just famous\nactors and politicians but also journalists, activists, policymakers,\nacademics, and artists.[45](notes.html#ich03notes45) Ironically, several of\nthe people who had been included in the set without consent are known for\ntheir work critiquing surveillance and facial recognition itself, including\ndocumentary filmmaker Laura Poitras; digital rights activist Jillian York;\ncritic Evgeny Morozov; and the author of _Surveillance Capitalism_ , Shoshana\nZuboff.[46](notes.html#ich03notes46)\n\nEven when datasets are scrubbed of personal information and released with\ngreat caution, people have been reidentified or highly sensitive details about\nthem have been revealed. In 2013, for example, the New York City Taxi and\nLimousine Commission released a dataset of 173 million individual cab rides,\nand it included pickup and drop-off times, locations, fares, and tip amounts.\nThe taxi drivers’ medallion numbers were anonymized, but this was quickly\nundone, enabling researchers to infer sensitive information like annual\nincomes and home addresses.[47](notes.html#ich03notes47) Once combined with\npublic information from sources like celebrity blogs, some actors and\npoliticians were identified, and it was possible to deduce the addresses of\npeople who visited strip clubs.[48](notes.html#ich03notes48) But beyond\nindividual harms, such datasets also generate “predictive privacy harms” for\nwhole groups or communities.[49](notes.html#ich03notes49) For instance, the\nsame New York City taxi dataset was used to suggest which taxi drivers were\ndevout Muslims by observing when they stopped at prayer\ntimes.[50](notes.html#ich03notes50)\n\nFrom any seemingly innocuous and anonymized dataset can come many unexpected\nand highly personal forms of information, but this fact has not hampered the\ncollection of images and text. As success in machine learning has come to rely\non ever-larger datasets, more people are seeking to acquire them. But why does\nthe wider AI field accept this practice, despite the ethical, political, and\nepistemological problems and potential harms? What beliefs, justifications,\nand economic incentives normalized this mass extraction and general\nequivalence of data?\n\n### Myths and Metaphors of Data\n\nThe oft-cited history of artificial intelligence written by AI professor Nils\nNilsson outlines several of the founding myths about data in machine learning.\nHe neatly illustrates how data is typically described in the technical\ndisciplines: “The great volume of raw data calls for efficient ‘data-mining’\ntechniques for classifying, quantifying, and extracting useful information.\nMachine learning methods are playing an increasingly important role in data\nanalysis because they can deal with massive amounts of data. In fact, the more\ndata the better.”[51](notes.html#ich03notes51)\n\nEchoing Robert Mercer from decades earlier, Nilsson perceived that data was\neverywhere for the taking, and all the better for mass classification by\nmachine learning algorithms.[52](notes.html#ich03notes52) It was such a common\nbelief as to have become axiomatic: data is there to be acquired, refined, and\nmade valuable.\n\nBut vested interests carefully manufactured and supported this belief over\ntime. As sociologists Marion Fourcade and Kieran Healy note, the injunction\nalways to collect data came not only from the data professions but also from\ntheir institutions and the technologies they deploy:\n\nThe institutional command coming from technology is the most potent of all: we\ndo these things _because we can. . . ._ Professionals recommend, the\ninstitutional environment demands, and technology enables organizations to\nsweep up as much individual data as possible. It does not matter that the\namounts collected may vastly exceed a firm’s imaginative reach or analytic\ngrasp. The assumption is that it will eventually be useful, i.e. valuable. . .\n. Contemporary organizations are both culturally impelled by the data\nimperative and powerfully equipped with new tools to enact\nit.[53](notes.html#ich03notes53)\n\nThis produced a kind of moral imperative to collect data in order to make\nsystems better, regardless of the negative impacts the data collection might\ncause at any future point. Behind the questionable belief that “more is\nbetter” is the idea that individuals can be completely knowable, once enough\ndisparate pieces of data are collected.[54](notes.html#ich03notes54) But what\ncounts as data? Historian Lisa Gitelman notes that every discipline and\ninstitution “has its own norms and standards for the imagination of\ndata.”[55](notes.html#ich03notes55) Data, in the twenty-first century, became\nwhatever could be captured.\n\nTerms like “data mining” and phrases like “data is the new oil” were part of a\nrhetorical move that shifted the notion of data away from something personal,\nintimate, or subject to individual ownership and control toward something more\ninert and nonhuman. Data began to be described as a resource to be consumed, a\nflow to be controlled, or an investment to be\nharnessed.[56](notes.html#ich03notes56) The expression “data as oil” became\ncommonplace, and although it suggested a picture of data as a crude material\nfor extraction, it was rarely used to emphasize the costs of the oil and\nmining industries: indentured labor, geopolitical conflicts, depletion of\nresources, and consequences stretching beyond human timescales.\n\nUltimately, “data” has become a bloodless word; it disguises both its material\norigins and its ends. And if data is seen as abstract and immaterial, then it\nmore easily falls outside of traditional understandings and responsibilities\nof care, consent, or risk. As researchers Luke Stark and Anna Lauren Hoffman\nargue, metaphors of data as a “natural resource” just lying in wait to be\ndiscovered are a well-established rhetorical trick used for centuries by\ncolonial powers.[57](notes.html#ich03notes57) Extraction is justified if it\ncomes from a primitive and “unrefined” source.[58](notes.html#ich03notes58) If\ndata is framed as oil, just waiting to be extracted, then machine learning has\ncome to be seen as its necessary refinement process.\n\nData also started to be viewed as capital, in keeping with the broader\nneoliberal visions of markets as the primary forms of organizing value. Once\nhuman activities are expressed through digital traces and then tallied up and\nranked within scoring metrics, they function as a way to extract value. As\nFourcade and Healy observe, those who have the right data signals gain\nadvantages like discounted insurance and higher standing across\nmarkets.[59](notes.html#ich03notes59) High achievers in the mainstream economy\ntend to do well in a data-scoring economy, too, while those who are poorest\nbecome targets of the most harmful forms of data surveillance and extraction.\nWhen data is considered as a form of capital, then everything is justified if\nit means collecting more. The sociologist Jathan Sadowski similarly argues\nthat data now operates as a form of capital. He suggests that once everything\nis understood as data, it justifies a cycle of ever-increasing data\nextraction: “Data collection is thus driven by the perpetual cycle of capital\naccumulation, which in turn drives capital to construct and rely upon a world\nin which everything is made of data. The supposed universality of data\nreframes everything as falling under the domain of data capitalism. All spaces\nmust be subjected to datafication. If the universe is conceived of as a\npotentially infinite reserve of data, then that means the accumulation and\ncirculation of data can be sustained forever.”[60](notes.html#ich03notes60)\n\nThis drive to accumulate and circulate is the powerful underlying ideology of\ndata. Mass data extraction is the “new frontier of accumulation and next step\nin capitalism,” Sadowski suggests, and it is the foundational layer that makes\nAI function.[61](notes.html#ich03notes61) Thus, there are entire industries,\ninstitutions, and individuals who don’t want this frontier—where data is there\nfor the taking—to be questioned or destabilized.\n\nMachine learning models require ongoing flows of data to become more accurate.\nBut machines are asymptotic, never reaching full precision, which propels the\njustification for more extraction from as many people as possible to fuel the\nrefineries of AI. This has created a shift away from ideas like “human\nsubjects”—a concept that emerged from the ethics debates of the twentieth\ncentury—to the creation of “data subjects,” agglomerations of data points\nwithout subjectivity or context or clearly defined rights.\n\n### Ethics at Arm’s Length\n\nThe great majority of university-based AI research is done without any ethical\nreview process. But if machine learning techniques are being used to inform\ndecisions in sensitive domains like education and health care, then why are\nthey not subject to greater review? To understand that, we need to look at the\nprecursor disciplines of artificial intelligence. Before the emergence of\nmachine learning and data science, the fields of applied mathematics,\nstatistics, and computer science had not historically been considered forms of\nresearch on human subjects.\n\nIn the early decades of AI, research using human data was usually seen to be a\nminimal risk.[62](notes.html#ich03notes62) Even though datasets in machine\nlearning often come from and represent people and their lives, the research\nthat used those datasets was seen more as a form of applied math with few\nconsequences for human subjects. The infrastructures of ethics protections,\nlike university-based institutional review boards (IRBs), had accepted this\nposition for years.[63](notes.html#ich03notes63) This initially made sense;\nIRBs had been overwhelmingly focused on the methods common to biomedical and\npsychological experimentation in which interventions carry clear risks to\nindividual subjects. Computer science was seen as far more abstract.\n\nOnce AI moved out of the laboratory contexts of the 1980s and 1990s and into\nreal-world situations—such as attempting to predict which criminals will\nreoffend or who should receive welfare benefits—the potential harms expanded.\nFurther, those harms affect entire communities as well as individuals. But\nthere is still a strong presumption that publicly available datasets pose\nminimal risks and therefore should be exempt from ethics\nreview.[64](notes.html#ich03notes64) This idea is the product of an earlier\nera, when it was harder to move data between locations and very expensive to\nstore it for long periods. Those earlier assumptions are out of step with what\nis currently going on in machine learning. Now datasets are more easily\nconnectable, indefinitely repurposable, continuously updatable, and frequently\nremoved from the context of collection.\n\nThe risk profile of AI is rapidly changing as its tools become more invasive\nand as researchers are increasingly able to access data without interacting\nwith their subjects. For example, a group of machine learning researchers\npublished a paper in which they claimed to have developed an “automatic system\nfor classifying crimes.”[65](notes.html#ich03notes65) In particular, their\nfocus was on whether a violent crime was gang-related, which they claimed\ntheir neural network could predict with only four pieces of information: the\nweapon, the number of suspects, the neighborhood, and the location. They did\nthis using a crime dataset from the Los Angeles Police Department, which\nincluded thousands of crimes that had been labeled by police as gang-related.\n\nGang data is notoriously skewed and riddled with errors, yet researchers use\nthis database and others like it as a definitive source for training\npredictive AI systems. The CalGang database, for example, which is widely used\nby police in California, has been shown to have major inaccuracies. The state\nauditor discovered that 23 percent of the hundreds of records it reviewed\nlacked adequate support for inclusion. The database also contained forty-two\ninfants, twenty-eight of whom were listed for having “admitting to being gang\nmembers.”[66](notes.html#ich03notes66) Most of the adults on the list had\nnever been charged, but once they were included in the database, there was no\nway to have their name removed. Reasons for being included might be as simple\nas chatting with a neighbor while wearing a red shirt; using these trifling\njustifications, Black and Latinx people have been disproportionately added to\nthe list.[67](notes.html#ich03notes67)\n\nWhen the researchers presented their gang-crime prediction project at a\nconference, some attendees were troubled. As reported by _Science_ , questions\nfrom the audience included, “How could the team be sure the training data were\nnot biased to begin with?” and “What happens when someone is mislabeled as a\ngang member?” Hau Chan, a computer scientist now at Harvard University who\npresented the work, responded that he couldn’t know how the new tool would be\nused. “[These are the] sort of ethical questions that I don’t know how to\nanswer appropriately,” he said, being just “a researcher.” An audience member\nreplied by quoting a lyric from Tom Lehrer’s satiric song about the wartime\nrocket scientist Wernher von Braun: “Once the rockets are up, who cares where\nthey come down?”[68](notes.html#ich03notes68)\n\nThis separation of ethical questions away from the technical reflects a wider\nproblem in the field, where the responsibility for harm is either not\nrecognized or seen as beyond the scope of the research. As Anna Lauren Hoffman\nwrites: “The problem here isn’t only one of biased datasets or unfair\nalgorithms and of unintended consequences. It’s also indicative of a more\npersistent problem of researchers actively reproducing ideas that damage\nvulnerable communities and reinforce current injustices. Even if the Harvard\nteam’s proposed system for identifying gang violence is never implemented,\nhasn’t a kind of damage already been done? Wasn’t their project an act of\ncultural violence in itself?”[69](notes.html#ich03notes69) Sidelining issues\nof ethics is harmful in itself, and it perpetuates the false idea that\nscientific research happens in a vacuum, with no responsibility for the ideas\nit propagates.\n\nThe reproduction of harmful ideas is particularly dangerous now that AI has\nmoved from being an experimental discipline used only in laboratories to being\ntested at scale on millions of people. Technical approaches can move rapidly\nfrom conference papers to being deployed in production systems, where harmful\nassumptions can become ingrained and hard to reverse.\n\nMachine learning and data-science methods can create an abstract relationship\nbetween researchers and subjects, where work is being done at a distance,\nremoved from the communities and individuals at risk of harm. This\narm’s-length relationship of AI researchers to the people whose lives are\nreflected in datasets is a long-established practice. Back in 1976, when AI\nscientist Joseph Weizenbaum wrote his scathing critique of the field, he\nobserved that computer science was already seeking to circumvent all human\ncontexts.[70](notes.html#ich03notes70) He argued that data systems allowed\nscientists during wartime to operate at a psychological distance from the\npeople “who would be maimed and killed by the weapons systems that would\nresult from the ideas they communicated.”[71](notes.html#ich03notes71) The\nanswer, in Weizenbaum’s view, was to directly contend with what data actually\nrepresents: “The lesson, therefore, is that the scientist and technologist\nmust, by acts of will and of the imagination, actively strive to reduce such\npsychological distances, to counter the forces that tend to remove him from\nthe consequences of his actions. He must—it is as simple as this—think of what\nhe is actually doing.”[72](notes.html#ich03notes72)\n\nWeizenbaum hoped that scientists and technologists would think more deeply\nabout the consequences of their work—and of who might be at risk. But this\nwould not become the standard of the AI field. Instead, data is more commonly\nseen as something to be taken at will, used without restriction, and\ninterpreted without context. There is a rapacious international culture of\ndata harvesting that can be exploitative and invasive and can produce lasting\nforms of harm.[73](notes.html#ich03notes73) And there are many industries,\ninstitutions, and individuals who are strongly incentivized to maintain this\ncolonizing attitude—where data is there for the taking—and they do not want it\nquestioned or regulated.\n\n### The Capture of the Commons\n\nThe current widespread culture of data extraction continues to grow despite\nconcerns about privacy, ethics, and safety. By researching the thousands of\ndatasets that are freely available for AI development, I got a glimpse into\nwhat technical systems are built to recognize, of how the world is rendered\nfor computers in ways that humans rarely see. There are gigantic datasets full\nof people’s selfies, tattoos, parents walking with their children, hand\ngestures, people driving their cars, people committing crimes on CCTV, and\nhundreds of everyday human actions like sitting down, waving, raising a glass,\nor crying. Every form of biodata—including forensic, biometric, sociometric,\nand psychometric—is being captured and logged into databases for AI systems to\nfind patterns and make assessments.\n\nTraining sets raise complex questions from ethical, methodological, and\nepistemological perspectives. Many were made without people’s knowledge or\nconsent and were harvested from online sources like Flickr, Google image\nsearch, and YouTube or were donated by government agencies like the FBI. This\ndata is now used to expand facial recognition systems, modulate health\ninsurance rates, penalize distracted drivers, and fuel predictive policing\ntools. But the practices of data extraction are extending even deeper into\nareas of human life that were once off-limits or too expensive to reach. Tech\ncompanies have drawn on a range of approaches to gain new ground. Voice data\nis gathered from devices that sit on kitchen counters or bedroom nightstands;\nphysical data comes from watches on wrists and phones in pockets; data about\nwhat books and newspapers are read comes from tablets and laptops; gestures\nand facial expressions are compiled and assessed in workplaces and classrooms.\n\nThe collection of people’s data to build AI systems raises clear privacy\nconcerns. Take, for example, the deal that Britain’s Royal Free National\nHealth Service Foundation Trust made with Google’s subsidiary DeepMind to\nshare the patient data records of 1.6 million people. The National Health\nService in Britain is a revered institution, entrusted to provide health care\nthat is primarily free to all while keeping patient data secure. But when the\nagreement with DeepMind was investigated, the company was found to have\nviolated data protection laws by not sufficiently informing\npatients.[74](notes.html#ich03notes74) In her findings, the information\ncommissioner observed that “the price of innovation does not need to be the\nerosion of fundamental privacy rights.”[75](notes.html#ich03notes75)\n\nYet there are other serious issues that receive less attention than privacy.\nThe practices of data extraction and training dataset construction are\npremised on a commercialized capture of what was previously part of the\ncommons. This particular form of erosion is a privatization by stealth, an\nextraction of knowledge value from public goods. A dataset may still be\npublicly available, but the metavalue of the data—the model created by it—is\nprivately held. Certainly, many good things can be done with public data. But\nthere has been a social and, to some degree, a technical expectation that the\nvalue of data shared via public institutions and public spaces online should\ncome back to the public good in other forms of the commons. Instead, we see a\nhandful of privately owned companies that now have enormous power to extract\ninsights and profits from those sources. The new AI gold rush consists of\nenclosing different fields of human knowing, feeling, and action—every type of\navailable data—all caught in an expansionist logic of never-ending collection.\nIt has become a pillaging of public space.\n\nFundamentally, the practices of data accumulation over many years have\ncontributed to a powerful extractive logic, a logic that is now a core feature\nof how the AI field works. This logic has enriched the tech companies with the\nlargest data pipelines, while the spaces free from data collection have\ndramatically diminished. As Vannevar Bush foresaw, machines have enormous\nappetites. But how and what they are fed has an enormous impact on how they\nwill interpret the world, and the priorities of their masters will always\nshape how that vision is monetized. By looking at the layers of training data\nthat shape and inform AI models and algorithms, we can see that gathering and\nlabeling data about the world is a social and political intervention, even as\nit masquerades as a purely technical one.\n\nThe way data is understood, captured, classified, and named is fundamentally\nan act of world-making and containment. It has enormous ramifications for the\nway artificial intelligence works in the world and which communities are most\naffected. The myth of data collection as a benevolent practice in computer\nscience has obscured its operations of power, protecting those who profit most\nwhile avoiding responsibility for its consequences.\n\n\n![Images](../images/f0122-01.jpg)\n\n\n## 4  \nClassification\n\nI am surrounded by human skulls. This room contains almost five hundred,\ncollected in the early decades of the 1800s. All are varnished, with numbers\ninscribed in black ink on the frontal bone. Delicate calligraphic circles mark\nout areas of the skull associated in phrenology with particular qualities,\nincluding “Benevolence” and “Veneration.” Some bear descriptions in capital\nletters, with words like “Dutchman,” “Peruvian of the Inca Race,” or\n“Lunatic.” Each was painstakingly weighed, measured, and labeled by the\nAmerican craniologist Samuel Morton. Morton was a physician, natural\nhistorian, and member of the Academy of Natural Sciences of Philadelphia. He\ngathered these human skulls from around the world by trading with a network of\nscientists and skull hunters who brought back specimens for his experiments,\nsometimes by robbing graves.[1](notes.html#ich04notes1) By the end of his life\nin 1851, Morton had amassed more than a thousand skulls, the largest\ncollection in the world at the time.[2](notes.html#ich04notes2) Much of the\narchive is now held in storage at the Physical Anthropology Section of the\nPenn Museum in Philadelphia.\n\nMorton was not a classical phrenologist in that he didn’t believe that human\ncharacter could be read through examining the shape of the head. Rather, his\naim was to classify and rank human races “objectively” by comparing the\nphysical characteristics of skulls. He did this by dividing them into the five\n“races” of the world: African, Native American, Caucasian, Malay, and\nMongolian—a typical taxonomy of the time and a reflection of the colonialist\nmentality that dominated its geopolitics.[3](notes.html#ich04notes3) This was\nthe viewpoint of polygenism—the belief that distinct human races had evolved\nseparately at different times—legitimized by white European and American\nscholars and hailed by colonial explorers as a justification for racist\nviolence and dispossession.[4](notes.html#ich04notes4) Craniometry grew to be\none of their leading methods since it purported to assess human difference and\nmerit accurately.[5](notes.html#ich04notes5)\n\n![Images](../images/f0124-01.jpg)\n\nA skull from the Morton cranial collection marked “Lunatic.” Photograph by\nKate Crawford\n\nMany of the skulls I see belong to people who were born in Africa but who died\nenslaved in the Americas. Morton measured these skulls by filling the cranial\ncavities with lead shot, then pouring the shot back into cylinders and gauging\nthe volume of lead in cubic inches.[6](notes.html#ich04notes6) He published\nhis results, comparing them to skulls he acquired from other locations: for\nexample, he claimed that white people had the largest skulls, while Black\npeople were on the bottom of the scale. Morton’s tables of average skull\nvolume by race were regarded as the cutting edge of science of the time. His\nwork was cited for the rest of the century as objective, hard data that proved\nthe relative intelligence of human races and biological superiority of the\nCaucasian race. This research was instrumented in the United States to\nmaintain the legitimacy of slavery and racial\nsegregation.[7](notes.html#ich04notes7) Considered the scientific state of the\nart at the time, it was used to authorize racial oppression long after the\nstudies were no longer cited.\n\nBut Morton’s work was not the kind of evidence it claimed to be. As Stephen\nJay Gould describes in his landmark book _The Mismeasure of Man:_\n\nIn short, and to put it bluntly, Morton’s summaries are a patchwork of fudging\nand finagling in the clear interest of controlling _a priori_ convictions.\nYet—and this is the most intriguing aspect of his case—I find no evidence of\nconscious fraud. . . . The prevalence of unconscious finagling, on the other\nhand, suggests a general conclusion about the social context of science. For\nif scientists can be honestly self-deluded to Morton’s extent, then prior\nprejudice may be found anywhere, even in the basics of measuring bones and\ntoting sums.[8](notes.html#ich04notes8)\n\nGould, and many others since, has reweighed the skulls and reexamined Morton’s\nevidence.[9](notes.html#ich04notes9) Morton made errors and miscalculations,\nas well as procedural omissions, such as ignoring the basic fact that larger\npeople have larger brains.[10](notes.html#ich04notes10) He selectively chose\nsamples that supported his belief of white supremacy and deleted the\nsubsamples that threw off his group averages. Contemporary assessments of the\nskulls at the Penn Museum show no significant differences among people—even\nwhen using Morton’s data.[11](notes.html#ich04notes11) But prior prejudice—a\nway of seeing the world—had shaped what Morton believed was objective science\nand was a self-reinforcing loop that influenced his findings as much as the\nlead-filled skulls themselves.\n\nCraniometry was, as Gould notes, “the leading numerical science of biological\ndeterminism during the nineteenth century” and was based on “egregious errors”\nin terms of the core underlying assumptions: that brain size equated to\nintelligence, that there are separate human races which are distinct\nbiological species, and that those races could be placed in a hierarchy\naccording to their intellect and innate\ncharacter.[12](notes.html#ich04notes12) Ultimately, this kind of race science\nwas debunked, but as Cornel West has argued, its dominant metaphors, logics,\nand categories not only supported white supremacy but also made specific\npolitical ideas about race possible while closing down\nothers.[13](notes.html#ich04notes13)\n\nMorton’s legacy foreshadows epistemological problems with measurement and\nclassification in artificial intelligence. Correlating cranial morphology with\nintelligence and claims to legal rights acts as a technical alibi for\ncolonialism and slavery.[14](notes.html#ich04notes14) While there is a\ntendency to focus on the errors in skull measurements and how to correct for\nthem, the far greater error is in the underlying worldview that animated this\nmethodology. The aim, then, should be not to call for more accurate or “fair”\nskull measurements to shore up racist models of intelligence but to condemn\nthe approach altogether. The practices of classification that Morton used were\n_inherently_ political, and his invalid assumptions about intelligence, race,\nand biology had far-ranging social and economic effects.\n\nThe politics of classification is a core practice in artificial intelligence.\nThe practices of classification inform how machine intelligence is recognized\nand produced from university labs to the tech industry. As we saw in the\nprevious chapter, artifacts in the world are turned into data through\nextraction, measurement, labeling, and ordering, and this\nbecomes—intentionally or otherwise—a slippery ground truth for technical\nsystems trained on that data. And when AI systems are shown to produce\ndiscriminatory results along the categories of race, class, gender,\ndisability, or age, companies face considerable pressure to reform their tools\nor diversify their data. But the result is often a narrow response, usually an\nattempt to address technical errors and skewed data to make the AI system\nappear more fair. What is often missing is a more fundamental set of\nquestions: How does classification function in machine learning? What is at\nstake when we classify? In what ways do classifications interact with the\nclassified? And what unspoken social and political theories underlie and are\nsupported by these classifications of the world?\n\nIn their landmark study of classification, Geoffrey Bowker and Susan Leigh\nStar write that “classifications are powerful technologies. Embedded in\nworking infrastructures they become relatively invisible without losing any of\ntheir power.”[15](notes.html#ich04notes15) Classification is an act of power,\nbe it labeling images in AI training sets, tracking people with facial\nrecognition, or pouring lead shot into skulls. But classifications can\ndisappear, as Bowker and Star observe, “into infrastructure, into habit, into\nthe taken for granted.”[16](notes.html#ich04notes16) We can easily forget that\nthe classifications that are casually chosen to shape a technical system can\nplay a dynamic role in shaping the social and material world.\n\nThe tendency to focus on the issue of bias in artificial intelligence has\ndrawn us away from assessing the core practices of classification in AI, along\nwith their attendant politics. To see that in action, in this chapter we’ll\nexplore some of the training datasets of the twenty-first century and observe\nhow their schemas of social ordering naturalize hierarchies and magnify\ninequalities. We will also look at the limits of the bias debates in AI, where\nmathematical parity is frequently proposed to produce “fairer systems” instead\nof contending with underlying social, political, and economic structures. In\nshort, we will consider how artificial intelligence uses classification to\nencode power.\n\n### Systems of Circular Logic\n\nA decade ago, the suggestion that there could be a problem of bias in\nartificial intelligence was unorthodox. But now examples of discriminatory AI\nsystems are legion, from gender bias in Apple’s creditworthiness algorithms to\nracism in the COMPAS criminal risk assessment software and to age bias in\nFacebook’s ad targeting.[17](notes.html#ich04notes17) Image recognition tools\nmiscategorize Black faces, chatbots adopt racist and misogynistic language,\nvoice recognition software fails to recognize female-sounding voices, and\nsocial media platforms show more highly paid job advertisements to men than to\nwomen.[18](notes.html#ich04notes18) As scholars like Ruha Benjamin and Safiya\nNoble have shown, there are hundreds of examples throughout the tech\necosystem.[19](notes.html#ich04notes19) Many more have never been detected or\npublicly admitted.\n\nThe typical structure of an episode in the ongoing AI bias narrative begins\nwith an investigative journalist or whistleblower revealing how an AI system\nis producing discriminatory results. The story is widely shared, and the\ncompany in question promises to address the issue. Then either the system is\nsuperseded by something new, or technical interventions are made in the\nattempt to produce results with greater parity. Those results and technical\nfixes remain proprietary and secret, and the public is told to rest assured\nthat the malady of bias has been “cured.”[20](notes.html#ich04notes20) It is\nmuch rarer to have a public debate about _why_ these forms of bias and\ndiscrimination frequently recur and whether more fundamental problems are at\nwork than simply an inadequate underlying dataset or a poorly designed\nalgorithm.\n\nOne of the more vivid examples of bias in action comes from an insider account\nat Amazon. In 2014, the company decided to experiment with automating the\nprocess of recommending and hiring workers. If automation had worked to drive\nprofits in product recommendation and warehouse organization, it could, the\nlogic went, make hiring more efficient. In the words of one engineer, “They\nliterally wanted it to be an engine where I’m going to give you 100 resumes,\nit will spit out the top five, and we’ll hire\nthose.”[21](notes.html#ich04notes21) The machine learning system was designed\nto rank people on a scale of one to five, mirroring Amazon’s system of product\nratings. To build the underlying model, Amazon’s engineers used a dataset of\nten years’ worth of résumés from fellow employees and then trained a\nstatistical model on fifty thousand terms that appeared in those résumés.\nQuickly, the system began to assign less importance to commonly used\nengineering terms, like programming languages, because everyone listed them in\ntheir job histories. Instead, the models began valuing more subtle cues that\nrecurred on successful applications. A strong preference emerged for\nparticular verbs. The examples the engineers mentioned were “executed” and\n“captured.”[22](notes.html#ich04notes22)\n\nRecruiters starting using the system as a supplement to their usual\npractices.[23](notes.html#ich04notes23) Soon enough, a serious problem\nemerged: the system wasn’t recommending women. It was actively downgrading\nrésumés from candidates who attended women’s colleges, along with any résumés\nthat even included the word “women.” Even after editing the system to remove\nthe influence of explicit references to gender, the biases remained. Proxies\nfor hegemonic masculinity continued to emerge in the gendered use of language\nitself. The model was biased against women not just as a category but against\ncommonly gendered forms of speech.\n\nInadvertently, Amazon had created a diagnostic tool. The vast majority of\nengineers hired by Amazon over ten years had been men, so the models they\ncreated, which were trained on the successful résumés of men, had learned to\nrecommend men for future hiring. The employment practices of the past and\npresent were shaping the hiring tools for the future. Amazon’s system\nunexpectedly revealed the ways bias already existed, from the way masculinity\nis encoded in language, in résumés, and in the company itself. The tool was an\nintensification of the existing dynamics of Amazon and highlighted the lack of\ndiversity across the AI industry past and\npresent.[24](notes.html#ich04notes24)\n\nAmazon ultimately shut down its hiring experiment. But the scale of the bias\nproblem goes much deeper than a single system or failed approach. The AI\nindustry has traditionally understood the problem of bias as though it is a\nbug to be fixed rather than a feature of classification itself. The result has\nbeen a focus on adjusting technical systems to produce greater quantitative\nparity across disparate groups, which, as we’ll see, has created its own\nproblems.\n\nUnderstanding the relation between bias and classification requires going\nbeyond an analysis of the production of knowledge—such as determining whether\na dataset is biased or unbiased—and, instead, looking at the mechanics of\nknowledge construction itself, what sociologist Karin Knorr Cetina calls the\n“epistemic machinery.”[25](notes.html#ich04notes25) To see that requires\nobserving how patterns of inequality across history shape access to resources\nand opportunities, which in turn shape data. That data is then extracted for\nuse in technical systems for classification and pattern recognition, which\nproduces results that are perceived to be somehow objective. The result is a\nstatistical ouroboros: a self-reinforcing discrimination machine that\namplifies social inequalities under the guise of technical neutrality.\n\n### The Limits of Debiasing Systems\n\nTo better understand the limitations of analyzing AI bias, we can look to the\nattempts to fix it. In 2019, IBM tried to respond to concerns about bias in\nits AI systems by creating what the company described as a more “inclusive”\ndataset called Diversity in Faces (DiF).[26](notes.html#ich04notes26) DiF was\npart of an industry response to the groundbreaking work released a year\nearlier by researchers Joy Buolamwini and Timnit Gebru that had demonstrated\nthat several facial recognition systems—including those by IBM, Microsoft, and\nAmazon—had far greater error rates for people with darker skin, particularly\nwomen.[27](notes.html#ich04notes27) As a result, efforts were ongoing inside\nall three companies to show progress on rectifying the problem.\n\n“We expect face recognition to work accurately for each of us,” the IBM\nresearchers wrote, but the only way that the “challenge of diversity could be\nsolved” would be to build “a data set comprised from the face of every person\nin the world.”[28](notes.html#ich04notes28) IBM’s researchers decided to draw\non a preexisting dataset of a hundred million images taken from Flickr, the\nlargest publicly available collection on the internet at the\ntime.[29](notes.html#ich04notes29) They then used one million photos as a\nsmall sample and measured the craniofacial distances between landmarks in each\nface: eyes, nasal width, lip height, brow height, and so on. Like Morton\nmeasuring skulls, the IBM researchers sought to assign cranial measures and\ncreate categories of difference.\n\nThe IBM team claimed that their goal was to increase diversity of facial\nrecognition data. Though well intentioned, the classifications they used\nreveal the politics of what diversity meant in this context. For example, to\nlabel the gender and age of a face, the team tasked crowdworkers to make\nsubjective annotations, using the restrictive model of binary gender. Anyone\nwho seemed to fall outside of this binary was removed from the dataset. IBM’s\nvision of diversity emphasized the expansive options for cranial orbit height\nand nose bridges but discounted the existence of trans or gender nonbinary\npeople. “Fairness” was reduced to meaning higher accuracy rates for machine-\nled facial recognition, and “diversity” referred to a wider range of faces to\ntrain the model. Craniometric analysis functions like a bait and switch,\nultimately depoliticizing the idea of diversity and replacing it with a focus\non _variation._ Designers get to decide what the variables are and how people\nare allocated to categories. Again, the practice of classification is\ncentralizing power: the power to decide which differences make a difference.\n\nIBM’s researchers go on to state an even more problematic conclusion: “Aspects\nof our heritage—including race, ethnicity, culture, geography—and our\nindividual identity—age, gender and visible forms of self-expression—are\nreflected in our faces.”[30](notes.html#ich04notes30) This claim goes against\ndecades of research that has challenged the idea that race, gender, and\nidentity are biological categories at all but are better understood as\npolitically, culturally, and socially\nconstructed.[31](notes.html#ich04notes31) Embedding identity claims in\ntechnical systems as though they are facts observable from the face is an\nexample of what Simone Browne calls “digital epidermalization,” the imposition\nof race on the body. Browne defines this as the exercise of power when the\ndisembodied gaze of surveillance technologies “do the work of alienating the\nsubject by producing a ‘truth’ about the body and one’s identity (or\nidentities) despite the subject’s claims.”[32](notes.html#ich04notes32)\n\nThe foundational problems with IBM’s approach to classifying diversity grow\nout of this kind of centralized production of identity, led by the machine\nlearning techniques that were available to the team. Skin color detection is\ndone because it can be, not because it says anything about race or produces a\ndeeper cultural understanding. Similarly, the use of cranial measurement is\ndone because it is a method that _can_ be done with machine learning. The\naffordances of the tools become the horizon of truth. The capacity to deploy\ncranial measurements and digital epidermalization at scale drives a desire to\nfind meaning in these approaches, even if this method has nothing to do with\nculture, heritage, or diversity. They are used to increase a problematic\nunderstanding of accuracy. Technical claims about accuracy and performance are\ncommonly shot through with political choices about categories and norms but\nare rarely acknowledged as such.[33](notes.html#ich04notes33) These approaches\nare grounded in an ideological premise of biology as destiny, where our faces\nbecome our fate.\n\n### The Many Definitions of Bias\n\nSince antiquity, the act of classification has been aligned with power. In\ntheology, the ability to name and divide things was a divine act of God. The\nword “category” comes from the Ancient Greek _katēgoríā_ , formed from two\nroots: _kata_ (against) and _agoreuo_ (speaking in public). In Greek, the word\ncan be either a logical assertion or an accusation in a trial—alluding to both\nscientific and legal methods of categorization.\n\nThe historical lineage of “bias” as a term is much more recent. It first\nappears in fourteenth-century geometry, where it refers to an oblique or\ndiagonal line. By the sixteenth century, it had acquired something like its\ncurrent popular meaning, of “undue prejudice.” By the 1900s, “bias” had\ndeveloped a more technical meaning in statistics, where it refers to\nsystematic differences between a sample and population, when the sample is not\ntruly reflective of the whole.[34](notes.html#ich04notes34) It is from this\nstatistical tradition that the machine learning field draws its understanding\nof bias, where it relates to a set of other concepts: generalization,\nclassification, and variance.\n\nMachine learning systems are designed to be able to generalize from a large\ntraining set of examples and to correctly classify new observations not\nincluded in the training datasets.[35](notes.html#ich04notes35) In other\nwords, machine learning systems can perform a type of induction, learning from\nspecific examples (such as past résumés of job applicants) in order to decide\nwhich data points to look for in new examples (such as word groupings in\nrésumés from new applicants). In such cases, the term “bias” refers to a type\nof error that can occur during this predictive process of\ngeneralization—namely, a systematic or consistently reproduced classification\nerror that the system exhibits when presented with new examples. This type of\nbias is often contrasted with another type of generalization error, variance,\nwhich refers to an algorithm’s sensitivity to differences in training data. A\nmodel with high bias and low variance may be underfitting the data—failing to\ncapture all of its significant features or signals. Alternatively, a model\nwith high variance and low bias may be overfitting the data—building a model\ntoo close to the training data so that it potentially captures “noise” in\naddition to the data’s significant features.[36](notes.html#ich04notes36)\n\nOutside of machine learning, “bias” has many other meanings. For instance, in\nlaw, bias refers to a preconceived notion or opinion, a judgment based on\nprejudices, as opposed to a decision come to from the impartial evaluation of\nthe facts of a case.[37](notes.html#ich04notes37) In psychology, Amos Tversky\nand Daniel Kahneman study “cognitive biases,” or the ways in which human\njudgments deviate systematically from probabilistic\nexpectations.[38](notes.html#ich04notes38) More recent research on implicit\nbiases emphasizes the ways that unconscious attitudes and stereotypes “produce\nbehaviors that diverge from a person’s avowed or endorsed beliefs or\nprinciples.”[39](notes.html#ich04notes39) Here bias is not simply a type of\ntechnical error; it also opens onto human beliefs, stereotypes, or forms of\ndiscrimination. These definitional distinctions limit the utility of “bias” as\na term, especially when used by practitioners from different disciplines.\n\nTechnical designs can certainly be improved to better account for how their\nsystems produce skews and discriminatory results. But the harder questions of\nwhy AI systems perpetuate forms of inequity are commonly skipped over in the\nrush to arrive at narrow technical solutions of statistical bias as though\nthat is a sufficient remedy for deeper structural problems. There has been a\ngeneral failure to address the ways in which the instruments of knowledge in\nAI reflect and serve the incentives of a wider extractive economy. What\nremains is a persistent asymmetry of power, where technical systems maintain\nand extend structural inequality, regardless of the intention of the\ndesigners.\n\nEvery dataset used to train machine learning systems, whether in the context\nof supervised or unsupervised machine learning, whether seen to be technically\nbiased or not, contains a worldview. To create a training set is to take an\nalmost infinitely complex and varied world and fix it into taxonomies composed\nof discrete classifications of individual data points, a process that requires\ninherently political, cultural, and social choices. By paying attention to\nthese classifications, we can glimpse the various forms of power that are\nbuilt into the architectures of AI world-building.\n\n### Training Sets as Classification Engines: The Case of ImageNet\n\nIn the last chapter we looked at the history of ImageNet and how this\nbenchmark training set has influenced computer vision research since its\ncreation in 2009. By taking a closer look at ImageNet’s structure, we can\nbegin to see how the dataset is ordered and its underlying logic for mapping\nthe world of objects. ImageNet’s structure is labyrinthine, vast, and filled\nwith curiosities. The underlying semantic structure of ImageNet was imported\nfrom WordNet, a database of word classifications first developed at Princeton\nUniversity’s Cognitive Science Laboratory in 1985 and funded by the U.S.\nOffice of Naval Research.[40](notes.html#ich04notes40) WordNet was conceived\nas a machine-readable dictionary, where users would search on the basis of\nsemantic rather than alphabetic similarity. It became a vital source for the\nfields of computational linguistics and natural language processing. The\nWordNet team collected as many words as they could, starting with the Brown\nCorpus, a collection of one million words compiled in the\n1960s.[41](notes.html#ich04notes41) The words in the Brown Corpus came from\nnewspapers and a ramshackle collection of books including _New Methods of\nParapsychology_ , _The Family Fallout Shelter_ , and _Who Rules the Marriage\nBed?_[ 42](notes.html#ich04notes42)\n\nWordNet attempts to organize the entire English language into synonym sets, or\nsynsets. The ImageNet researchers selected only nouns, with the idea that\nnouns are things that pictures can represent—and that would be sufficient to\ntrain machines to automatically recognize objects. So ImageNet’s taxonomy is\norganized according to a nested hierarchy derived from WordNet, in which each\nsynset represents a distinct concept, with synonyms grouped together (for\nexample, “auto” and “car” are treated as belonging to the same set). The\nhierarchy moves from more general concepts to more specific ones. For example,\nthe concept “chair” is found under artifact → furnishing → furniture → seat →\nchair. This classification system unsurprisingly evokes many prior taxonomical\nranks, from the Linnaean system of biological classification to the ordering\nof books in libraries.\n\nBut the first indication of the true strangeness of ImageNet’s worldview is\nits nine top-level categories that it drew from WordNet: plant, geological\nformation, natural object, sport, artifact, fungus, person, animal, and\nmiscellaneous. These are curious categories into which all else must be\nordered. Below that, it spawns into thousands of strange and specific nested\nclasses, into which millions of images are housed like Russian dolls. There\nare categories for apples, apple butter, apple dumplings, apple geraniums,\napple jelly, apple juice, apple maggots, apple rust, apple trees, apple\nturnovers, apple carts, and applesauce. There are pictures of hot lines, hot\npants, hot plates, hot pots, hot rods, hot sauce, hot springs, hot toddies,\nhot tubs, hot-air balloons, hot fudge sauce, and hot water bottles. It is a\nriot of words, ordered into strange categories like those from Jorge Luis\nBorges’s mythical encyclopedia.[43](notes.html#ich04notes43) At the level of\nimages, it looks like madness. Some images are high-resolution stock\nphotography, others are blurry phone photographs in poor lighting. Some are\nphotos of children. Others are stills from pornography. Some are cartoons.\nThere are pin-ups, religious icons, famous politicians, Hollywood celebrities,\nand Italian comedians. It veers wildly from the professional to the amateur,\nthe sacred to the profane.\n\nHuman classifications are a good place to see these politics of classification\nat work. In ImageNet the category “human body” falls under the branch Natural\nObject → Body → Human Body. Its subcategories include “male body,” “person,”\n“juvenile body,” “adult body,” and “female body.” The “adult body” category\ncontains the subclasses “adult female body” and “adult male body.” There is an\nimplicit assumption here that only “male” and “female” bodies are recognized\nas “natural.” There is an ImageNet category for the term “Hermaphrodite,” but\nit is situated within the branch Person → Sensualist → Bisexual alongside the\ncategories “Pseudohermaphrodite” and “Switch\nHitter.”[44](notes.html#ich04notes44)\n\nEven before we look at the more controversial categories within ImageNet, we\ncan see the politics of this classificatory scheme. The decisions to classify\ngender in this way are also naturalizing gender as a biological construct,\nwhich is binary, and transgender or gender nonbinary people are either\nnonexistent or placed under categories of\nsexuality.[45](notes.html#ich04notes45) Of course, this is not a novel\napproach. The classification hierarchy of gender and sexuality in ImageNet\nrecalls earlier harmful forms of categorization, such as the classification of\nhomosexuality as a mental disorder in the _Diagnostic and Statistical\nManual._[ 46](notes.html#ich04notes46) This deeply damaging categorization was\nused to justify subjecting people to repressive so-called therapies, and it\ntook years of activism before the American Psychiatric Association removed it\nin 1973.[47](notes.html#ich04notes47)\n\nReducing humans into binary gender categories and rendering transgender people\ninvisible or “deviant” are common features of classification schemes in\nmachine learning. Os Keyes’s study of automatic gender detection in facial\nrecognition shows that almost 95 percent of papers in the field treat gender\nas binary, with the majority describing gender as immutable and\nphysiological.[48](notes.html#ich04notes48) While some might respond that this\ncan be easily remedied by creating more categories, this fails to address the\ndeeper harm of allocating people into gender or race categories without their\ninput or consent. This practice has a long history. Administrative systems for\ncenturies have sought to make humans legible by applying fixed labels and\ndefinite properties. The work of essentializing and ordering on the basis of\nbiology or culture has long been used to justify forms of violence and\noppression.\n\nWhile these classifying logics are treated as though they are natural and\nfixed, they are moving targets: not only do they affect the people being\nclassified, but how they impact people in turn changes the classifications\nthemselves. Hacking calls this the “looping effect,” produced when the\nsciences engage in “making up people.”[49](notes.html#ich04notes49) Bowker and\nStar also underscore that once classifications of people are constructed, they\ncan stabilize a contested political category in ways that are difficult to\nsee.[50](notes.html#ich04notes50) They become taken for granted unless they\nare actively resisted. We see this phenomenon in the AI field when highly\ninfluential infrastructures and training datasets pass as purely technical,\nwhereas in fact they contain political interventions within their taxonomies:\nthey naturalize a particular ordering of the world which produces effects that\nare seen to justify their original ordering.\n\n### The Power to Define “Person”\n\nTo impose order onto an undifferentiated mass, to ascribe phenomena to a\ncategory—that is, to name a thing—is in turn a means of reifying the existence\nof that category.\n\nIn the case of the 21,841 categories that were originally in the ImageNet\nhierarchy, noun classes such as “apple” or “apple butter” might seem\nreasonably uncontroversial, but not all nouns are created equal. To borrow an\nidea from linguist George Lakoff, the concept of an “apple” is a more _nouny_\nnoun than the concept of “light,” which in turn is more nouny than a concept\nsuch as “health.”[51](notes.html#ich04notes51) Nouns occupy various places on\nan axis from the concrete to the abstract, from the descriptive to the\njudgmental. These gradients have been erased in the logic of ImageNet.\nEverything is flattened out and pinned to a label, like taxidermy butterflies\nin a display case. While this approach has the aesthetics of objectivity, it\nis nonetheless a profoundly ideological exercise.\n\nFor a decade, ImageNet contained 2,832 subcategories under the top-level\ncategory “Person.” The subcategory with the most associated pictures was “gal”\n(with 1,664 images) followed by “grandfather” (1,662), “dad” (1,643), and\nchief executive officer (1,614—most of them male). With these highly populated\ncategories, we can already begin to see the outlines of a worldview. ImageNet\ncontains a profusion of classificatory categories, including ones for race,\nage, nationality, profession, economic status, behavior, character, and even\nmorality.\n\nThere are many problems with the way ImageNet’s taxonomy purports to classify\nphotos of people with the logics of object recognition. Even though its\ncreators removed some explicitly offensive synsets in 2009, categories\nremained for racial and national identities including Alaska Native, Anglo-\nAmerican, Black, Black African, Black Woman (but not White Woman), Latin\nAmerican, Mexican American, Nicaraguan, Pakistani, South American Indian,\nSpanish American, Texan, Uzbek, White, and Zulu. To present these as logical\ncategories of organizing people is already troubling, even before they are\nused to classify people based on their appearance. Other people are labeled by\ncareers or hobbies: there are Boy Scouts, cheerleaders, cognitive\nneuroscientists, hairdressers, intelligence analysts, mythologists, retailers,\nretirees, and so on. The existence of these categories suggests that people\ncan be visually ordered according to their profession, in a way that seems\nreminiscent of such children’s books as Richard Scarry’s _What Do People Do\nAll Day?_ ImageNet also contains categories that make no sense whatsoever for\nimage classification such as Debtor, Boss, Acquaintance, Brother, and Color-\nBlind Person. These are all nonvisual concepts that describe a relationship,\nbe it to other people, to a financial system, or to the visual field itself.\nThe dataset reifies these categories and connects them to images, so that\nsimilar images can be “recognized” by future systems.\n\nMany truly offensive and harmful categories hid in the depths of ImageNet’s\nPerson categories. Some classifications were misogynist, racist, ageist, and\nableist. The list includes Bad Person, Call Girl, Closet Queen, Codger,\nConvict, Crazy, Deadeye, Drug Addict, Failure, Flop, Fucker, Hypocrite,\nJezebel, Kleptomaniac, Loser, Melancholic, Nonperson, Pervert, Prima Donna,\nSchizophrenic, Second-Rater, Slut, Spastic, Spinster, Streetwalker, Stud,\nTosser, Unskilled Person, Wanton, Waverer, and Wimp. Insults, racist slurs,\nand moral judgments abound.\n\nThese offensive terms remained in ImageNet for ten years. Because ImageNet was\ntypically used for object recognition—with “object” broadly defined—the\nspecific Person category was rarely discussed at technical conferences, nor\ndid it receive much public attention until the ImageNet Roulette project went\nviral in 2019: led by the artist Trevor Paglen, the project included an app\nthat allowed people to upload images to see how they would be classified based\non ImageNet’s Person categories.[52](notes.html#ich04notes52) This focused\nconsiderable media attention on the influential collection’s longtime\ninclusion of racist and sexist terms. The creators of ImageNet published a\npaper shortly afterward titled “Toward Fairer Datasets” that sought to “remove\nunsafe synsets.” They asked twelve graduate students to flag any categories\nthat seemed unsafe because they were either “inherently offensive” (for\nexample, containing profanity or “racial or gender slurs”) or “sensitive” (not\ninherently offensive but terms that “may cause offense when applied\ninappropriately, such as the classification of people based on sexual\norientation and religion”).[53](notes.html#ich04notes53) While this project\nsought to assess the offensiveness of ImageNet’s categories by asking graduate\nstudents, the authors nonetheless continue to support the automated\nclassification of people based on photographs despite the notable problems.\n\nThe ImageNet team ultimately removed 1,593 of 2,832 of the People\ncategories—roughly 56 percent—deeming them “unsafe,” along with the associated\n600,040 images. The remaining half-million images were “temporarily deemed\nsafe.”[54](notes.html#ich04notes54) But what constitutes _safe_ when it comes\nto classifying people? The focus on the hateful categories is not wrong, but\nit avoids addressing questions about the workings of the larger system. The\nentire taxonomy of ImageNet reveals the complexities and dangers of human\nclassification. While terms like “microeconomist” or “basketball player” may\ninitially seem less concerning than the use of labels like “spastic,”\n“unskilled person,” “mulatto,” or “redneck,” when we look at the people who\nare labeled in these categories we see many assumptions and stereotypes,\nincluding race, gender, age, and ability. In the metaphysics of ImageNet,\nthere are separate image categories for “assistant professor” and “associate\nprofessor”—as though once someone gets a promotion, her or his biometric\nprofile would reflect the change in rank.\n\nIn fact, there are no neutral categories in ImageNet, because the selection of\nimages always interacts with the meaning of words. The politics are baked into\nthe classificatory logic, even when the words aren’t offensive. ImageNet is a\nlesson, in this sense, of what happens when people are categorized like\nobjects. But this practice has only become more common in recent years, often\ninside the tech companies. The classification schemes used in companies like\nFacebook are much harder to investigate and criticize: proprietary systems\noffer few ways for outsiders to probe or audit how images are ordered or\ninterpreted.\n\nThen there is the issue of where the images in ImageNet’s Person categories\ncome from. As we saw in the last chapter, ImageNet’s creators harvested images\nen masse from image search engines like Google, extracted people’s selfies and\nvacation photos without their knowledge, and then paid Mechanical Turk workers\nto label and repackage them. All the skews and biases in how search engines\nreturn results are then informing the subsequent technical systems that scrape\nand label them. Low-paid crowdworkers are given the impossible task of making\nsense of the images at the rate of fifty per minute and fitting them into\ncategories based on WordNet sysnets and Wikipedia\ndefinitions.[55](notes.html#ich04notes55) Perhaps it is no surprise that when\nwe investigate the bedrock layer of these labeled images, we find that they\nare beset with stereotypes, errors, and absurdities. A woman lying on a beach\ntowel is a “kleptomaniac,” a teenager in a sports jersey is labeled a “loser,”\nand an image of the actor Sigourney Weaver appears, classified as a\n“hermaphrodite.”\n\nImages—like all forms of data—are laden with all sorts of potential meanings,\nirresolvable questions, and contradictions. In trying to resolve these\nambiguities, ImageNet’s labels compress and simplify complexity. The focus on\nmaking training sets “fairer” by deleting offensive terms fails to contend\nwith the power dynamics of classification and precludes a more thorough\nassessment of the underlying logics. Even if the worst examples are fixed, the\napproach is still fundamentally built on an extractive relationship with data\nthat is divorced from the people and places from whence it came. Then it is\nrendered through a technical worldview that seeks to fuse together a form of\nsingular objectivity from what are complex and varied cultural materials. The\nworldview of ImageNet is not unusual in this sense. In fact, it is typical of\nmany AI training datasets, and it reveals many of the problems of top-down\nschemes that flatten complex social, cultural, political, and historical\nrelations into quantifiable entities. This phenomenon is perhaps most obvious\nand insidious when it comes to the widespread efforts to classify people by\nrace and gender in technical systems.\n\n### Constructing Race and Gender\n\nBy focusing on classification in AI, we can trace the ways that gender, race,\nand sexuality are falsely assumed to be natural, fixed, and detectable\nbiological categories. Surveillance scholar Simone Browne observes, “There is\na certain assumption with these technologies that categories of gender\nidentity and race are clear cut, that a machine can be programmed to assign\ngender categories or determine what bodies and body parts should\nsignify.”[56](notes.html#ich04notes56) Indeed, the idea that race and gender\ncan be automatically detectable in machine learning is treated as an assumed\nfact and rarely questioned by the technical disciplines, despite the profound\npolitical problems this presents.[57](notes.html#ich04notes57)\n\nThe UTKFace dataset (produced by a group at the University of Tennessee at\nKnoxville), for example, consists of more than twenty thousand images of faces\nwith annotations for age, gender, and race.[58](notes.html#ich04notes58) The\ndataset’s authors state that the dataset can be used for a variety of tasks,\nincluding automated face detection, age estimation, and age progression. The\nannotations for each image include an estimated age for each person, expressed\nin years from zero to 116. Gender is a forced binary: either zero for male or\none for female. Second, race is categorized into five classes: White, Black,\nAsian, Indian, and Others. The politics of gender and race here are as obvious\nas they are harmful. Yet these kinds of dangerously reductive categorizations\nare widely used across many human-classifying training sets and have been part\nof the AI production pipelines for years.\n\nUTKFace’s narrow classificatory schema echoes the problematic racial\nclassifications of the twentieth century, such as South Africa’s apartheid\nsystem. As Bowker and Star have detailed, the South African government passed\nlegislation in the 1950s that created a crude racial classification scheme to\ndivide citizens into the categories of “Europeans, Asiatics, persons of mixed\nrace or coloureds, and ‘natives’ or pure-blooded individuals of the Bantu\nrace.”[59](notes.html#ich04notes59) This racist legal regime governed people’s\nlives, overwhelmingly those of Black South Africans whose movements were\nrestricted and who were forcibly removed from their land. The politics of\nracial classification extended into the most intimate parts of people’s lives.\nInterracial sexuality was forbidden, leading to more than 11,500 convictions\nby 1980, mostly of nonwhite women.[60](notes.html#ich04notes60) The complex\ncentralized database for these classifications was designed and maintained by\nIBM, but the firm often had to rearrange the system and reclassify people,\nbecause in practice there were no singular pure racial\ncategories.[61](notes.html#ich04notes61)\n\nAbove all, these systems of classification have caused enormous harm to\npeople, and the concept of a pure “race” signifier has always been in dispute.\nIn her writing about race, Donna Haraway observes, “In these taxonomies, which\nare, after all, little machines for clarifying and separating categories, the\nentity that always eluded the classifier was simple: race itself. The pure\nType, which animated dreams, sciences, and terrors, kept slipping through, and\nendlessly multiplying, all the typological\ntaxonomies.”[62](notes.html#ich04notes62) Yet in dataset taxonomies, and in\nthe machine learning systems that train on them, the myth of the pure type has\nemerged once more, claiming the authority of science. In an article on the\ndangers of facial recognition, media scholar Luke Stark notes that “by\nintroducing a variety of classifying logics that either reify existing racial\ncategories or produce new ones, the automated pattern-generating logics of\nfacial recognition systems both reproduce systemic inequality and exacerbate\nit.”[63](notes.html#ich04notes63)\n\nSome machine learning methods go beyond predicting age, gender, and race.\nThere have been highly publicized efforts to detect sexuality from photographs\non dating sites and criminality based on headshots from drivers’\nlicenses.[64](notes.html#ich04notes64) These approaches are deeply problematic\nfor many reasons, not least of which is that characteristics such as\n“criminality”—like race and gender—are profoundly relational, socially\ndetermined categories. These are not inherent features that are fixed; they\nare contextual and shifting depending on time and place. To make such\npredictions, machine learning systems are seeking to classify entirely\nrelational things into fixed categories and are rightly critiqued as\nscientifically and ethically problematic.[65](notes.html#ich04notes65)\n\nMachine learning systems are, in a very real way, _constructing_ race and\ngender: they are defining the world within the terms they have set, and this\nhas long-lasting ramifications for the people who are classified. When such\nsystems are hailed as scientific innovations for predicting identities and\nfuture actions, this erases the technical frailties of how the systems were\nbuilt, the priorities of why they were designed, and the many political\nprocesses of categorization that shape them. Disability scholars have long\npointed to the ways in which so-called normal bodies are classified and how\nthat has worked to stigmatize difference.[66](notes.html#ich04notes66) As one\nreport notes, the history of disability itself is a “story of the ways in\nwhich various systems of classification (i.e., medical, scientific, legal)\ninterface with social institutions and their articulations of power and\nknowledge.”[67](notes.html#ich04notes67) At multiple levels, the act of\ndefining categories and ideas of normalcy creates an outside: forms of\nabnormality, difference, and otherness. Technical systems are making political\nand normative interventions when they give names to something as dynamic and\nrelational as personal identity, and they commonly do so using a reductive set\nof possibilities of what it is to be human. That restricts the range of how\npeople are understood and can represent themselves, and it narrows the horizon\nof recognizable identities.\n\nAs Ian Hacking observes, classifying people is an imperial imperative:\nsubjects were classified by empires when they were conquered, and then they\nwere ordered into “a kind of people” by institutions and\nexperts.[68](notes.html#ich04notes68) These acts of naming were assertions of\npower and colonial control, and the negative effects of those classifications\ncan outlast the empires themselves. Classifications are technologies that\nproduce and limit ways of knowing, and they are built into the logics of AI.\n\n### The Limits of Measurement\n\nSo what is to be done? If so much of the classificatory strata in training\ndata and technical systems are forms of power and politics represented as\nobjective measurement, how should we go about redressing this? How should\nsystem designers account for, in some cases, slavery, oppression, and hundreds\nof years of discrimination against some groups to the benefit of others? In\nother words, how should AI systems make representations of the social?\n\nMaking these choices about which information feeds AI systems to produce new\nclassifications is a powerful moment of decision making: but who gets to\nchoose and on what basis? The problem for computer science is that justice in\nAI systems will never be something that can be coded or computed. It requires\na shift to assessing systems beyond optimization metrics and statistical\nparity and an understanding of where the frameworks of mathematics and\nengineering are causing the problems. This also means understanding how AI\nsystems interact with data, workers, the environment, and the individuals\nwhose lives will be affected by its use and deciding where AI should not be\nused.\n\nBowker and Star conclude that the sheer density of the collisions of\nclassification schemes calls for a new kind of approach, a sensitivity to the\n“topography of things such as the distribution of ambiguity; the fluid\ndynamics of how classification systems meet up—a plate tectonics rather than\nstatic geology.”[69](notes.html#ich04notes69) But it also requires attending\nto the uneven allocations of advantage and suffering, for “how these choices\nare made, and how we may think about that invisible matching process, is at\nthe core of the ethical project.”[70](notes.html#ich04notes70) Nonconsensual\nclassifications present serious risks, as do normative assumptions about\nidentity, yet these practices have become standard. That must change.\n\nIn this chapter we’ve seen how classificatory infrastructures contain gaps and\ncontradictions: they necessarily reduce complexity, and they remove\nsignificant context, in order to make the world more computable. But they also\nproliferate in machine learning platforms in what Umberto Eco called “chaotic\nenumeration.”[71](notes.html#ich04notes71) At a certain level of granularity,\nlike and unlike things become sufficiently commensurate so that their\nsimilarities and differences are machine readable, yet in actuality their\ncharacteristics are uncontainable. Here, the issues go far beyond whether\nsomething is classified wrong or classified right. We are seeing strange,\nunpredictable twists as machine categories and people interact and change each\nother, as they try to find legibility in the shifting terrain, to fit the\nright categories and be spiked into the most lucrative feeds. In a machine\nlearning landscape, these questions are no less urgent because they are hard\nto see. What is at stake is not just a historical curiosity or the odd feeling\nof a mismatch between the dotted-outline profiles we may glimpse in our\nplatforms and feeds. Each and every classification has its consequence.\n\nThe histories of classification show us that the most harmful forms of human\ncategorization—from the Apartheid system to the pathologization of\nhomosexuality—did not simply fade away under the light of scientific research\nand ethical critique. Rather, change also required political organizing,\nsustained protest, and public campaigning over many years. Classificatory\nschemas enact and support the structures of power that formed them, and these\ndo not shift without considerable effort. In Frederick Douglass’s words,\n“Power concedes nothing without a demand. It never did and it never\nwill.”[72](notes.html#ich04notes72) Within the invisible regimes of\nclassification in machine learning, it is harder to make demands and oppose\ntheir internal logics.\n\nThe training sets that are made public—such as ImageNet, UTKFace, and DiF—give\nus some insight into the kinds of categorizations that are propagating across\nindustrial AI systems and research practices. But the truly massive engines of\nclassification are the ones being operated at a global scale by private\ntechnology companies, including Facebook, Google, TikTok, and Baidu. These\ncompanies operate with little oversight into how they categorize and target\nusers, and they fail to offer meaningful avenues for public contestation. When\nthe matching processes of AI are truly hidden and people are kept unaware of\nwhy or how they receive forms of advantage or disadvantage, a collective\npolitical response is needed—even as it becomes more difficult.\n\n\n![Images](../images/f0150-01.jpg)\n\n\n## 5  \nAffect\n\nIn a remote outpost in the mountainous highlands of Papua New Guinea, a young\nAmerican psychologist named Paul Ekman arrived with a collection of flashcards\nand a new theory.[1](notes.html#ich05notes1) It was 1967, and Ekman had heard\nthat the Fore people of Okapa were so isolated from the wider world that they\nwould be his ideal test subjects. Like many Western researchers before him,\nEkman had come to Papua New Guinea to extract data from the indigenous\ncommunity. He was gathering evidence to bolster a controversial hypothesis:\nthat all humans exhibit a small number of universal emotions or affects that\nare natural, innate, cross-cultural, and the same all over the world. Although\nthat claim remains tenuous, it has had far-reaching consequences: Ekman’s\npresuppositions about emotions have grown into an expanding industry worth\nwell over seventeen billion dollars.[2](notes.html#ich05notes2) This is the\nstory of how affect recognition came to be part of artificial intelligence and\nthe problems this presents.\n\nIn the tropics of Okapa, guided by medical researcher D. Carleton Gajdusek and\nanthropologist E. Richard Sorenson, Ekman hoped to run experiments that would\nassess how the Fore recognized emotions conveyed by facial expressions.\nBecause the Fore had minimal contact with Westerners or mass media, Ekman\ntheorized that their recognition and display of core expressions would prove\nthat such expressions were universal. His methods were simple. He would show\nthem flashcards of facial expressions and see if they described the emotion as\nhe did. In Ekman’s own words, “All I was doing was showing funny\npictures.”[3](notes.html#ich05notes3)\n\nBut Ekman had no training in Fore history, language, culture, or politics. His\nattempts to conduct his flashcard experiments using translators floundered; he\nand his subjects were exhausted by the process, which he described as like\npulling teeth.[4](notes.html#ich05notes4) Ekman left Papua New Guinea,\nfrustrated by his first attempt at cross-cultural research on emotional\nexpression. But this would just be the beginning.\n\nToday affect recognition tools can be found in national security systems and\nat airports, in education and hiring start-ups, from systems that purport to\ndetect psychiatric illness to policing programs that claim to predict\nviolence. By looking at the history of how computer-based emotion detection\ncame to be, we can understand how its methods have raised both ethical\nconcerns and scientific doubts. As we will see, the claim that a person’s\ninterior state of feeling can be accurately assessed by analyzing their face\nis premised on shaky evidence.[5](notes.html#ich05notes5) In fact, a\ncomprehensive review of the available scientific literature on inferring\nemotions from facial movements published in 2019 was definitive: there is _no\nreliable evidence_ that you can accurately predict someone’s emotional state\nfrom their face.[6](notes.html#ich05notes6)\n\nHow did this collection of contested claims and experimental methodologies\nresolve into an approach that drives many parts of the affect AI industry? Why\ndid the idea that there is a small set of universal emotions, readily\ninterpreted from the face, become so accepted in the AI field, despite\nconsiderable evidence to the contrary? To understand that requires tracing how\nthese ideas developed, long before AI emotion detection tools were built into\nthe infrastructure of everyday life.\n\nEkman is just one of many people who have contributed to the theories behind\naffect recognition. But the rich and surprising history of Ekman’s research\nilluminates some of the complex forces driving the field. His work is\nconnected to U.S. intelligence funding of the human sciences during the Cold\nWar through foundational work in the field of computer vision to the post-9/11\nsecurity programs employed to identify terrorists and right up to the current\nfashion for AI-based emotion recognition. It is a chronicle that combines\nideology, economic policy, fear-based politics, and the desire to extract more\ninformation about people than they are willing to give.\n\n### Emotion Prophets: When Feelings Pay\n\nFor the world’s militaries, corporations, intelligence agencies, and police\nforces, the idea of automated affect recognition is as compelling as it is\nlucrative. It holds the promise of reliably filtering friend from foe,\ndistinguishing lies from truths, and using the instruments of science to see\ninto interior worlds.\n\nTechnology companies have captured immense volumes of surface-level imagery of\nhuman expressions—including billions of Instagram selfies, Pinterest\nportraits, TikTok videos, and Flickr photos. One of the many things made\npossible by this profusion of images is the attempt to extract the so-called\nhidden truth of interior emotional states using machine learning. Affect\nrecognition is being built into several facial recognition platforms, from the\nbiggest tech companies to small start-ups. Whereas facial recognition attempts\nto identify a _particular_ individual, affect detection aims to detect and\nclassify emotions by analyzing _any_ face. These systems may not be doing what\nthey purport to do, but they can nonetheless be powerful agents in influencing\nbehavior and training people to perform in recognizable ways. These systems\nare already playing a role in shaping how people behave and how social\ninstitutions operate, despite a lack of substantial scientific evidence that\nthey work.\n\nAutomated affect detection systems are now widely deployed, particularly in\nhiring. A startup in London called Human uses emotion recognition to analyze\nvideo interviews of job candidates. According to a report in the _Financial\nTimes_ , “The company claims it can spot the emotional expressions of\nprospective candidates and match them with personality traits”; the company\nthen scores subjects on such personality traits as honesty or passion for a\njob.[7](notes.html#ich05notes7) The AI hiring company HireVue, which lists\namong its clients Goldman Sachs, Intel and Unilever, uses machine learning to\nassess facial cues to infer people’s suitability for a job. In 2014, the\ncompany launched its AI system to extract microexpressions, tone of voice, and\nother variables from video job interviews, which they used to compare job\napplicants against the company’s top performers.[8](notes.html#ich05notes8)\n\nIn January 2016, Apple acquired the startup Emotient, which claimed to have\nproduced software capable of detecting emotions from images of\nfaces.[9](notes.html#ich05notes9) Emotient grew out of academic research\nconducted at the University of California San Diego and is one of a number of\nstartups working in this area.[10](notes.html#ich05notes10) Perhaps the\nlargest of these is Affectiva, a company based in Boston that emerged from\nacademic work done at Massachusetts Institute of Technology. At MIT, Rosalind\nPicard and her colleagues were part of an emergent wider field known as\naffective computing, which describes computing that “relates to, arises from,\nor deliberately influences emotion or other affective\nphenomena.”[11](notes.html#ich05notes11)\n\nAffectiva codes a variety of emotion-related applications, primarily using\ndeep learning techniques. These range from detecting distracted and “risky”\ndrivers on roads to measuring the emotional responses of consumers to\nadvertising. The company has built what they call the world’s largest emotion\ndatabase, made up of over ten million people’s expressions from eighty-seven\ncountries.[12](notes.html#ich05notes12) Their monumental collection of videos\nof people emoting was hand labeled by crowdworkers based primarily in\nCairo.[13](notes.html#ich05notes13) Many more companies have now licensed\nAffectiva’s products to develop everything from applications that assess job\ncandidates to analyzing whether students are engaged in class, all by\ncapturing and analyzing their facial expressions and body\nlanguage.[14](notes.html#ich05notes14)\n\nBeyond the start-up sector, AI giants like Amazon, Microsoft, and IBM have all\ndesigned systems for affect and emotion detection. Microsoft offers emotion\ndetection in its Face API, which claims to detect what an individual is\nfeeling across the emotions of “anger, contempt, disgust, fear, happiness,\nneutral, sadness, and surprise” and asserts that “these emotions are\nunderstood to be cross-culturally and universally communicated with particular\nfacial expressions.”[15](notes.html#ich05notes15) Amazon’s Rekognition tool\nsimilarly claims that it can identify “all seven emotions” and “measure how\nthese things change over time, such as constructing a timeline of the emotions\nof an actor.”[16](notes.html#ich05notes16)\n\nBut how do these technologies work? Emotion recognition systems grew from the\ninterstices between AI technologies, military priorities, and the behavioral\nsciences—psychology in particular. They share a similar set of blueprints and\nfounding assumptions: that there is a small number of distinct and universal\nemotional categories, that we involuntarily reveal these emotions on our\nfaces, and that they can be detected by machines. These articles of faith are\nso accepted in some fields that it can seem strange even to notice them, let\nalone question them. They are so ingrained that they have come to constitute\n“the common view.”[17](notes.html#ich05notes17) But if we look at how emotions\ncame to be taxonomized—neatly ordered and labeled—we see that questions are\nlying in wait at every corner. And a leading figure behind this approach is\nPaul Ekman.\n\n### “The World’s Most Famous Face-Reader”\n\nEkman’s research began with a fortunate encounter with Silvan Tomkins, then an\nestablished psychologist based at Princeton who had published the first volume\nof his magnum opus, _Affect Imagery Consciousness_ , in\n1962.[18](notes.html#ich05notes18) Tomkins’s work on affect had a huge\ninfluence on Ekman, who devoted much of his career to studying its\nimplications. One aspect in particular played an outsized role: the idea that\nif affect was an innate set of evolutionary responses, they would be universal\nand so recognizable across cultures. This desire for universality has an\nimportant bearing on why these theories are widely applied in AI emotion\nrecognition systems today: it offered a small set of principles that could be\napplied everywhere, a simplification of complexity that was easily replicable.\n\nIn the introduction to _Affect Imagery Consciousness_ , Tomkins framed his\ntheory of biologically based universal affects as one addressing an acute\ncrisis of human sovereignty. He was challenging the development of behaviorism\nand psychoanalysis, two schools of thought that he believed treated\nconsciousness as a mere by-product of—and in service to—other forces. He noted\nthat human consciousness had “been challenged and reduced again and again,\nfirst by Copernicus”—who displaced man from the center of the universe—“then\nby Darwin”—whose theory of evolution shattered the idea that humans were\ncreated in the image of a Christian God—“and most of all by Freud”—who\ndecentered human consciousness and reason as the driving force behind our\nmotivations.[19](notes.html#ich05notes19) Tomkins continued, “The paradox of\nmaximal control over nature and minimal control over human nature is in part a\nderivative of the neglect of the role of consciousness as a control\nmechanism.”[20](notes.html#ich05notes20) To put it simply, _consciousness\ntells us little about why we feel and act the way we do._ This is a critical\nclaim for all sorts of later applications of affect theory, which stress the\ninability of humans to recognize both the feeling and the expression of\naffects. If we as humans are incapable of truly detecting what we are feeling,\nthen perhaps AI systems can do it for us?\n\nTomkins’s theory of affects was his way to address the problem of human\nmotivation. He argued that motivation was governed by two systems: affects and\ndrives. Tomkins contended that drives tend to be closely associated with\nimmediate biological needs such as hunger and\nthirst.[21](notes.html#ich05notes21) They are instrumental; the pain of hunger\ncan be remedied with food. But the primary system governing human motivation\nand behavior is that of affects, involving positive and negative _feelings._\nAffects, which play the most important role in human motivation, amplify drive\nsignals, but they are much more complex. For example, it is difficult to know\nthe precise reason or causes that lead a baby to cry, expressing the distress-\nanguish affect. The baby might be “hungry or cold or wet or in pain or\n[crying] because of a high temperature.”[22](notes.html#ich05notes22)\nSimilarly, there are a number of ways that this affective feeling can be\nmanaged: “Crying can be stopped by feeding, cuddling, making the room warmer,\nmaking it colder, taking the diaper pin out of his skin and so\non.”[23](notes.html#ich05notes23)\n\nTomkins concludes, “The price that is paid for this flexibility is ambiguity\nand error. The individual may or may not correctly identify the ‘cause’ of his\nfear or joy and may or may not learn to reduce his fear or maintain or\nrecapture his joy. In this respect the affect system is not as simple a signal\nsystem as the drive system.”[24](notes.html#ich05notes24) Affects, unlike\ndrives, are not strictly instrumental; they have a high degree of independence\nfrom stimuli and objects, meaning that we often may not know why we feel\nangry, afraid, or happy.[25](notes.html#ich05notes25)\n\nAll of this ambiguity might suggest that the complexities of affects are\nimpossible to untangle. How can we know anything about a system where the\nconnections between cause and effect, stimulus and response, are so tenuous\nand uncertain? Tomkins proposed an answer: “The primary affects . . . seem to\nbe innately related in a one-to-one fashion with an organ system which is\nextraordinarily visible.” Namely, the face.[26](notes.html#ich05notes26) He\nfound precedents for this emphasis on facial expression in two works published\nin the nineteenth century: Charles Darwin’s _The Expression of the Emotions in\nMan and Animals_ (1872) and an obscure volume by the French neurologist\nGuillaume-Benjamin-Amand Duchenne de Boulogne, _Mécanisme de la physionomie\nhumaine ou Analyse électro-physiologique de l’expression des passions\napplicable à la pratique des arts plastiques_\n(1862).[27](notes.html#ich05notes27)\n\nTomkins assumed that the facial display of affects was a human universal.\n“Affects,” Tomkins believed, “are sets of muscle, vascular, and glandular\nresponses located in the face and also widely distributed through the body,\nwhich generate sensory feedback. . . . These organized sets of responses are\ntriggered at subcortical centers where specific ‘programs’ for each distinct\naffect are stored”—a very early use of a computational metaphor for a human\nsystem.[28](notes.html#ich05notes28)\n\nBut Tomkins acknowledged that the _interpretation_ of affective displays\ndepends on individual, social, and cultural factors. He admitted that there\nwere very different “dialects” of facial language in different\nsocieties.[29](notes.html#ich05notes29) Even the forefather of affect research\nraised the possibility that recognizing affect and emotion depends on social\nand cultural context. The potential conflict between cultural dialects and a\nbiologically based, universal language had enormous implications for the study\nof facial expression and later forms of emotion recognition. Given that facial\nexpressions are culturally variable, using them to train machine learning\nsystems would inevitably mix together all sorts of different contexts,\nsignals, and expectations.\n\nDuring the mid-1960s, opportunity knocked at Ekman’s door in the form of the\nAdvanced Research Projects Agency (ARPA), a research arm of the Department of\nDefense. Looking back on this period, he admitted, “It wasn’t my idea to do\nthis [affect research]. I was asked—pushed. I didn’t even write the research\nproposal. It was written for me by the man who gave me the money to do\nit.”[30](notes.html#ich05notes30) In 1965, he was researching nonverbal\nexpression in clinical settings and seeking funding to develop a research\nprogram at Stanford University. He arranged a meeting in Washington, D.C.,\nwith Lee Hough, head of ARPA’s behavioral sciences\ndivision.[31](notes.html#ich05notes31) Hough was uninterested in how Ekman\ndescribed his research, but he saw potential in understanding cross-cultural\nnonverbal communication.[32](notes.html#ich05notes32)\n\nThe only problem was that, by Ekman’s own admission, he did not know how to do\ncross-cultural research: “I did not even know what the arguments were, the\nliterature, or the methods.”[33](notes.html#ich05notes33) So Ekman\nunderstandably decided to drop pursuit of ARPA funding. But Hough insisted,\nand according to Ekman, he “sat for a day in my office, and wrote the proposal\nhe then funded that allowed me to do the research I am best known for—evidence\nfor the universality of some facial expressions of emotion, and cultural\ndifferences in gestures.”[34](notes.html#ich05notes34) He got a massive\ninjection of funds from ARPA, roughly one million dollars—the equivalent of\nmore than eight million dollars today.[35](notes.html#ich05notes35)\n\nAt the time, Ekman wondered why Hough seemed so eager to fund this research,\neven over his objections and despite his lack of expertise. It turns out that\nHough wanted to distribute his money quickly to avoid suspicion from Senator\nFrank Church, who had caught Hough using social science research as a cover\nfor acquiring information in Chile that could be used to overthrow its left-\nwing government under President Salvador Allende.[36](notes.html#ich05notes36)\nEkman later concluded that he was just a lucky guy, someone “who could do\noverseas research that wouldn’t get him [Hough] into\ntrouble!”[37](notes.html#ich05notes37) ARPA would be the first in a long line\nof agencies from defense, intelligence, and law enforcement that would fund\nboth Ekman’s career and the field of affect recognition more generally.\n\nWith the support of a large grant behind him, Ekman began his first studies to\nprove universality in facial expression. In general, these studies followed a\ndesign that would be copied in early AI labs. He largely duplicated Tomkins’s\nmethods, even using Tomkins’s photographs to test subjects drawn from Chile,\nArgentina, Brazil, the United States, and Japan.[38](notes.html#ich05notes38)\nHe relied on asking research participants to simulate the expressions of an\nemotion, which were then compared with expressions gathered “in the wild,”\nmeaning outside of laboratory conditions.[39](notes.html#ich05notes39)\nSubjects were presented with photographs of posed facial expressions, selected\nby the designers as exemplifying or expressing a particularly “pure” or\nintense affect. Subjects were then asked to choose among these affect\ncategories and to label the posed image. The analysis measured the degree to\nwhich the labels chosen by subjects correlated with those chosen by the\ndesigners.\n\nFrom the start, the methodology had problems. Ekman’s forced choice response\nformat would be later criticized for alerting subjects to the connections that\ndesigners had already made between facial expressions and\nemotions.[40](notes.html#ich05notes40) Further, the fact that these emotions\nwere faked or posed would raise significant concerns about the validity of\nthese results.[41](notes.html#ich05notes41) Ekman found some cross-cultural\nagreements using this approach, but his findings were challenged by the\nanthropologist Ray Birdwhistell, who suggested that this agreement may not\nreflect innate affect states if they were culturally learned through exposure\nto such mass media as films, television, or\nmagazines.[42](notes.html#ich05notes42) It was this dispute that compelled\nEkman to set out for Papua New Guinea, specifically to study indigenous people\nin the highlands region. He figured that if people with little contact to\nWestern culture and media could agree with how he had categorized posed\naffective expressions, then this would provide strong evidence for the\nuniversality of his schema.\n\nAfter Ekman returned from his first attempt to study the Fore people in Papua\nNew Guinea, he devised an alternative approach to prove his theory. He showed\nhis U.S. research subjects a photograph, then asked them to choose one of six\naffect concepts: happy, fear, disgust-contempt, anger, surprise, and\nsadness.[43](notes.html#ich05notes43) The results were close enough to\nsubjects from other countries that Ekman believed he could claim that\n“particular facial behaviors are universally associated with particular\nemotions.”[44](notes.html#ich05notes44)\n\n### Affect: From Physiognomy to Photography\n\nThe idea that interior states can be reliably inferred from external signs\nstems in part from the history of physiognomy, which was premised on studying\na person’s facial features for indications of their character. In the ancient\nGreek world, Aristotle had believed that “it is possible to judge men’s\ncharacter from their physical appearance . . . for it has been assumed that\nbody and soul are affected together.”[45](notes.html#ich05notes45) The Greeks\nalso used physiognomy as an early form of racial classification, applied to\n“the genus man itself, dividing him into races, in so far as they differ in\nappearance and in character (for instance Egyptians, Thracians and\nScythians).”[46](notes.html#ich05notes46) They presumed a link between body\nand soul that justified reading a person’s interior character based on their\nexterior appearance.\n\nPhysiognomy in Western culture reached a high point during the eighteenth and\nnineteenth centuries, when it was seen as part of the anatomical sciences. A\nkey figure in this tradition was the Swiss pastor Johann Kaspar Lavater, who\nwrote _Essays on Physiognomy; For the Promotion of Knowledge and the Love of\nMankind_ , originally published in German in\n1789.[47](notes.html#ich05notes47) Lavater took the approaches of physiognomy\nand blended them with the latest scientific knowledge. He tried to create a\nmore “objective” comparison of faces by using silhouettes instead of artists’\nengravings because they were more mechanical and fixed the position of each\nface into the familiar profile form, allowing for a comparative\nviewpoint.[48](notes.html#ich05notes48) He believed that bone structure was an\nunderlying connection between physical appearance and character type. If\nfacial expressions were fleeting, skulls offered a more solid material for\nphysiognomic inferences.[49](notes.html#ich05notes49) The measurement of\nskulls, as we saw in the last chapter, was used to support an emerging\nnationalism, racism, and xenophobia. This work was infamously elaborated on\nthroughout the nineteenth century by phrenologists like Franz Joseph Gall and\nJohann Gaspar Spurzheim, as well as in scientific criminology through the work\nof Cesare Lombroso—all leading into the types of inferential classifications\nthat recur in contemporary AI systems.\n\nBut it was the French neurologist Duchenne, described by Ekman as a\n“marvelously gifted observer,” who codified the use of photography and other\ntechnical means in the study of human faces.[50](notes.html#ich05notes50) In\n_Mécanisme de la physionomie humaine_ , Duchenne laid important foundations\nfor both Darwin and Ekman, connecting older ideas from physiognomy and\nphrenology with more modern investigations into physiology and psychology. He\nreplaced vague assertions about character with a more limited investigation\ninto expression and interior mental or emotional\nstates.[51](notes.html#ich05notes51)\n\nDuchenne worked in Paris at the Salpetrière asylum, which housed up to five\nthousand people with a wide range of diagnoses of mental illness and\nneurological conditions. Some would become his subjects for distressing\nexperiments, part of the long tradition of medical and technological\nexperimentation on the most vulnerable and those who cannot\nrefuse.[52](notes.html#ich05notes52) Duchenne, who was little known in the\nscientific community, decided to develop techniques of electrical shocks to\nstimulate isolated muscle movements in people’s faces. His aim was to build a\nmore complete anatomical and physiological understanding of the face. Duchenne\nused these methods to bridge the new psychological science and the much older\nstudy of physiognomic signs, or passions.[53](notes.html#ich05notes53) He\nrelied on the latest photographic techniques, like collodion processing, which\nallowed for much shorter exposure times, allowing Duchenne to freeze fleeting\nmuscular movements and facial expressions in\nimages.[54](notes.html#ich05notes54)\n\nEven at these very early stages, the faces were never natural or socially\noccurring human expressions but _simulations_ produced by the brute\napplication of electricity to the muscles. Regardless, Duchenne believed that\nthe use of photography and other technical systems would transform the squishy\nbusiness of representation into something objective and evidentiary, more\nsuitable for scientific study.[55](notes.html#ich05notes55) In his\nintroduction to _The Expression of the Emotions in Man and Animals_ , Darwin\npraised Duchenne’s “magnificent photographs” and included reproductions in his\nown work.[56](notes.html#ich05notes56) Because emotions were temporal, even\nfleeting occurrences, photography offered the ability to fix, compare, and\ncategorize their visible expression on the face. Yet Duchenne’s images of\ntruth were highly manufactured.\n\n![Images](../images/f0164-01.jpg)\n\nPlates from G.-B. Duchenne (de Boulogne), _Mécanisme de la physionomie\nhumaine, ou Analyse électro-physiologique de l’expression des passions._\nCourtesy U.S. National Library of Medicine\n\nEkman would follow Duchenne in placing photography at the center of his\nexperimental practice.[57](notes.html#ich05notes57) He believed that slow\nmotion photography was essential to his approach, because many facial\nexpressions operate at the limits of human perception. The aim was to find so-\ncalled microexpressions—tiny muscle movements in the face. The duration of\nmicroexpressions, in his view, “is so short that they are at the threshold of\nrecognition unless slow motion projection is\nutilized.”[58](notes.html#ich05notes58) In later years Ekman also would insist\nthat anyone could come to learn to recognize microexpressions, with no special\ntraining or slow motion capture, in about an\nhour.[59](notes.html#ich05notes59) But if these expressions are too quick for\nhumans to recognize, how are they to be\nunderstood?[60](notes.html#ich05notes60)\n\nOne of Ekman’s ambitious plans in his early research was to codify a system\nfor detecting and analyzing facial expressions.[61](notes.html#ich05notes61)\nIn 1971, he copublished a description of what he called the Facial Action\nScoring Technique (FAST). Relying on posed photographs, the approach used six\nbasic emotional types largely derived from Ekman’s\nintuitions.[62](notes.html#ich05notes62) But FAST soon ran into problems when\nother scientists were able to produce facial expressions not included in its\ntypology.[63](notes.html#ich05notes63) So Ekman decided to ground his next\nmeasurement tool in facial musculature, harkening back to Duchenne’s original\nelectroshock studies. Ekman identified roughly forty distinct muscular\ncontractions on the face and called the basic components of each facial\nexpression an Action Unit.[64](notes.html#ich05notes64) After some testing and\nvalidation, Ekman and Wallace Friesen published the Facial Action Coding\nSystem (FACS) in 1978; the updated editions continue to be widely\nused.[65](notes.html#ich05notes65) FACS was very labor intensive to use as a\nmeasurement tool. Ekman said that it took from seventy-five to a hundred hours\nto train users in the FACS methodology and an hour to score a minute of facial\nfootage.[66](notes.html#ich05notes66)\n\n![Images](../images/f0166-01.jpg)\n\nElements from the Facial Action Coding System. Source: Paul Ekman and Wallace\nV. Friesen\n\nAt a conference in the early 1980s, Ekman heard a research presentation that\nsuggested a solution to the intense labor demands of FACS: the use of\ncomputers to automate measurement. Although in his memoir Ekman does not\nmention the researcher who gave the paper, he does state that the system was\ncalled Wizard and was developed at Brunel University in\nLondon.[67](notes.html#ich05notes67) This is likely Igor Aleksander’s early\nmachine learning object-recognition system, WISARD, which had used neural\nnetworks at a time when this approach was out of\nfashion.[68](notes.html#ich05notes68) Some sources report that WISARD was\ntrained on a “database of known football hooligans,” anticipating the\nwidespread contemporary use of criminal mug shots to train facial recognition\ntechnologies.[69](notes.html#ich05notes69)\n\nBecause facial recognition emerged as a foundational application for\nartificial intelligence in the 1960s, it is not surprising that early\nresearchers working in this field found common cause with Ekman’s approach to\nanalyzing faces.[70](notes.html#ich05notes70) Ekman himself claims to have\nplayed an active role in driving the automated forms of affect recognition\nthrough his old contacts in defense and intelligence agencies from his ARPA\nfunding days. He helped to set up an informal competition between two teams\nworking with FACS data, and this seems to have had lasting impact. Both of\nthose teams have since gone on to feature prominently in the affective\ncomputing field. One team was composed of Terry Sejnowski and his student\nMarian Bartlett, who herself became an important figure in the computer\nscience of emotion recognition and the lead scientist at Emotient, acquired by\nApple in 2016.[71](notes.html#ich05notes71) The second team, based in\nPittsburgh, was led by the psychologist Jeffrey Cohn of the University of\nPittsburgh and the eminent computer vision researcher Takeo Kanade of Carnegie\nMellon.[72](notes.html#ich05notes72) These two figures pursued affect\nrecognition over the long term and developed the well-known Cohn-Kanade (CK)\nemotional expression dataset and its descendants.\n\nEkman’s FACS system provided two things essential for later machine learning\napplications: a stable, discrete, finite set of labels that humans can use to\ncategorize photographs of faces and a system for producing measurements. It\npromised to remove the difficult work of representing interior lives away from\nthe purview of artists and novelists and bring it under the umbrella of a\nrational, knowable, and measurable rubric suitable to laboratories,\ncorporations, and governments.\n\n### Capturing Feeling: The Artifice of Performing Emotions\n\nAs work into the use of computers in affect recognition began to take shape,\nresearchers recognized the need for a collection of standardized images to\nexperiment with. A 1992 NSF report coauthored by Ekman recommended that “a\nreadily accessible, multimedia database shared by the diverse facial research\ncommunity would be an important resource for the resolution and extension of\nissues concerning facial understanding.”[73](notes.html#ich05notes73) Within a\nyear, the Department of Defense would begin funding the FERET program to\ncollect facial photographs, as we saw in [chapter 3](ch03.html). By the end of\nthe decade, machine learning researchers had begun to assemble, label, and\nmake public the datasets that drive much of today’s machine learning research.\n\nEkman’s FACS guidelines directly shaped the CK\ndataset.[74](notes.html#ich05notes74) Following Ekman’s tradition of posed\nfacial expressions, “subjects were instructed by an experimenter to perform a\nseries of 23 facial displays,” which FACS experts then coded, providing labels\nfor the data. The CK dataset allowed laboratories to benchmark their results\nand compare progress as they built new expression recognition systems.\n\nOther labs and companies worked on parallel projects, creating scores of photo\ndatabases. For example, researchers in a lab in Sweden created Karolinska\nDirected Emotional Faces. This database is composed of images of individuals\nportraying posed emotional expressions corresponding to Ekman’s\ncategories.[75](notes.html#ich05notes75) They make their faces into the shapes\nthat accord with six basic emotional states. When looking at these training\nsets, it is difficult to not be struck by how extreme they are: _Incredible\nsurprise! Abundant joy! Paralyzing fear!_ These subjects are literally making\nmachine-readable emotion.\n\nAs the field grew in scale and complexity, so did the types of photographs\nused in affect recognition. Researchers began using the FACS system to label\ndata generated not from posed expressions but rather from spontaneous facial\nexpressions, sometimes gathered outside of laboratory conditions. For example,\na decade after the hugely successful release of the CK dataset, a group of\nresearchers released a second generation, the Extended Cohn-Kanade (CK+)\nDataset.[76](notes.html#ich05notes76) CK+ included the usual range of posed\nexpressions but also began to include so-called non-posed or spontaneous\nexpressions taken from videos where subjects made unprompted facial\nexpressions.\n\n![Images](../images/f0169-01.jpg)\n\nFacial expressions from the Cohn-Kanade dataset: joy, anger, disgust, sadness,\nsurprise, fear. Posed images from T. Kanade et al., _Yearbook of Physical\nAnthropology_ (2000). © Cohn & Kanade\n\nBy 2009, Affectiva emerged from the MIT Media Lab with the aim of capturing\n“naturalistic and spontaneous facial expressions” in real-life\nsettings.[77](notes.html#ich05notes77) The company collected data by allowing\nusers to opt into a system that would record their faces using a webcam as\nthey watched a series of commercials. These images would then be hand-labeled\nusing custom software by coders trained in Ekman’s\nFACS.[78](notes.html#ich05notes78) But here we find another problem of\ncircularity. FACS was developed from Ekman’s substantial archive of posed\nphotographs.[79](notes.html#ich05notes79) Even when images are gathered in\nnaturalistic settings, they are commonly classified according to a scheme\nderived from posed images.\n\nEkman’s work became a profound and wide-ranging influence on everything from\nlie detection software to computer vision. _The New York Times_ described\nEkman as “the world’s most famous face reader,” and _Time_ named him one of\nthe one hundred most influential people in the world. He would eventually\nconsult with clients as disparate as the Dalai Lama, the FBI, the CIA, the\nSecret Service, and even the animation studio Pixar, which wanted to create\nmore lifelike renderings of cartoon faces.[80](notes.html#ich05notes80) His\nideas became part of popular culture, included in best sellers like Malcolm\nGladwell’s _Blink_ and a television drama, _Lie to Me_ , on which Ekman was a\nconsultant for the lead character’s role, apparently loosely based on\nhim.[81](notes.html#ich05notes81)\n\nHis business also prospered: Ekman sold techniques of deception detection to\nsecurity agencies such as the Transportation Security Administration, which\nused them in the development of the Screening of Passengers by Observation\nTechniques (SPOT) program. SPOT was used to monitor facial expressions of air\ntravelers in the years following the September 11 attacks, attempting to\n“automatically” detect terrorists. The system uses a set of ninety-four\ncriteria, all of which are allegedly signs of stress, fear, or deception. But\nlooking for these responses meant that some groups are immediately\ndisadvantaged. Anyone who was stressed, was uncomfortable under questioning,\nor had had negative experiences with police and border guards could score\nhigher. This produced its own forms of racial profiling. The SPOT program has\nbeen criticized by the Government Accountability Office and civil liberties\ngroups for its lack of scientific methodology and, despite its nine-hundred-\nmillion-dollar price tag, producing no clear\nsuccesses.[82](notes.html#ich05notes82)\n\n### The Many Critiques of Ekman’s Theories\n\nAs Ekman’s fame grew, so did the skepticism of his work, with critiques\nemerging from a number of fields. An early critic was the cultural\nanthropologist Margaret Mead, who debated Ekman on the question of the\nuniversality of emotions in the late 1960s, resulting in fierce exchanges not\nonly between Mead and Ekman but also among other anthropologists critical of\nEkman’s idea of absolute universality.[83](notes.html#ich05notes83) Mead was\nunconvinced by Ekman’s belief in universal, biological determinants of\nbehavior rather than considering cultural\nfactors.[84](notes.html#ich05notes84) In particular, Ekman tended to collapse\nemotions into an oversimplified, mutually exclusive binary: either emotions\nwere universal or they were not. Critics like Mead pointed out that more\nnuanced positions were possible.[85](notes.html#ich05notes85) Mead took a\nmiddle ground, emphasizing that there was no inherent contradiction between\n“the possibility that human beings may share a core of innate behaviors . . .\nand the idea that emotional expressions could, _at the same time_ , be highly-\nconditioned by cultural factors.”[86](notes.html#ich05notes86)\n\nMore scientists from different fields joined the chorus over the decades. In\nmore recent years, the psychologists James Russell and José-Miguel Fernández-\nDols have shown that the most basic aspects of the science remain unsolved:\n“The most fundamental questions, such as whether ‘facial expressions of\nemotion’ in fact express emotions, remain subjects of great\ncontroversy.”[87](notes.html#ich05notes87) Social scientists Maria Gendron and\nLisa Feldman Barrett have pointed to the specific dangers of Ekman’s theories\nbeing used by the AI industry because the automated detection of facial\nexpressions does not reliably indicate an internal mental\nstate.[88](notes.html#ich05notes88) As Barrett observes, “Companies can say\nwhatever they want, but the data are clear. They can detect a scowl, but\nthat’s not the same thing as detecting anger.”[89](notes.html#ich05notes89)\n\nMore troubling still is that in the field of the study of emotions, there is\nno consensus among researchers about what an emotion actually is. What\nemotions are, how they are formulated within us and expressed, what their\nphysiological or neurobiological functions could be, their relation to\nstimuli, even how to define them—all of this in its entirety remains\nstubbornly unsettled.[90](notes.html#ich05notes90)\n\nPerhaps the foremost critic of Ekman’s theory of emotions is the historian of\nscience Ruth Leys. In _The Ascent of Affect_ she thoroughly pulls apart “the\nimplications of the fundamental physiognomic assumption underlying Ekman’s\nwork . . . namely, the idea that a distinction can be strictly maintained\nbetween authentic and artificial expressions of emotion based on differences\nbetween the faces we make when we are alone and those we make when we are with\nothers.”[91](notes.html#ich05notes91) Leys sees a fundamental circularity in\nEkman’s method. First, the posed or simulated photographs he used were assumed\nto express a set of basic affective states, “already free of cultural\ninfluence.”[92](notes.html#ich05notes92) Then, these photographs were used to\nelicit labels from different populations to demonstrate the universality of\nfacial expressions. Leys points out the serious problem: Ekman assumed that\n“the facial expressions in the photographs he employed in his experiments must\nhave been free of cultural taint because they were universally recognized. At\nthe same time, he suggested that those facial expressions were universally\nrecognized because they were free of cultural\ntaint.”[93](notes.html#ich05notes93) The approach is fundamentally\nrecursive.[94](notes.html#ich05notes94)\n\nOther problems became clear as Ekman’s ideas were implemented in technical\nsystems. As we’ve seen, many datasets underlying the field are based on actors\nsimulating emotional states, performing for the camera. That means that AI\nsystems are trained to recognize faked expressions of feeling. Although AI\nsystems claim to have access to ground truth about natural interior states,\nthey are trained on material that is inescapably constructed. Even for images\nthat are captured of people responding to commercials or films, those people\nare aware they are being watched, which can change their responses.\n\nThe difficulty in automating the connection between facial movements and basic\nemotional categories leads to the larger question of whether emotions can be\nadequately grouped into a small number of discrete categories at\nall.[95](notes.html#ich05notes95) This view can be traced back to Tomkins, who\nargued that “each kind of emotion can be identified by a more or less unique\nsignature response within the body.”[96](notes.html#ich05notes96) But there is\nvery little consistent evidence of this. Psychologists have conducted multiple\nreviews of the published evidence, which has failed to find associations among\nmeasurable responses to the emotional states that they assume to\nexist.[97](notes.html#ich05notes97) Finally, there is the stubborn issue that\nfacial expressions may indicate little about our honest interior states, as\nanyone who has smiled without feeling truly happy can\nconfirm.[98](notes.html#ich05notes98)\n\nNone of these serious questions about the basis for Ekman’s claims have\nstopped his work from attaining a privileged role in current AI applications.\nHundreds of papers cite Ekman’s view of interpretable facial expressions as\nthough it were unproblematic fact, despite decades of scientific controversy.\nFew computer scientists have even acknowledged this literature of uncertainty.\nThe affective computing researcher Arvid Kappas, for example, directly names\nthe lack of basic scientific consensus: “We know too little regarding the\ncomplex social modulators of facial and possibly other expressive activity in\nsuch situations to be able to measure emotional state reliably from expressive\nbehavior. _This is not an engineering problem that could be solved with a\nbetter algorithm._ ”[99](notes.html#ich05notes99) Unlike many in the field who\nconfidently support affect recognition, Kappas questions the belief that it’s\na good idea for computers to be trying to sense emotions at\nall.[100](notes.html#ich05notes100)\n\nThe more time researchers from other backgrounds spend examining Ekman’s work,\nthe stronger the evidence against it grows. In 2019, Lisa Feldman Barrett led\na research team that conducted a wide-ranging review of the literature on\ninferring emotions from facial expressions. They concluded firmly that facial\nexpressions are far from indisputable and are “not ‘fingerprints’ or\ndiagnostic displays” that reliably signal emotional states, let alone across\ncultures and contexts. Based on all the current evidence, the team observed,\n“It is not possible to confidently infer happiness from a smile, anger from a\nscowl, or sadness from a frown, as much of current technology tries to do when\napplying what are mistakenly believed to be the scientific\nfacts.”[101](notes.html#ich05notes101)\n\nBarrett’s team was critical of AI companies claiming to be able to automate\nthe inference of emotion: “Technology companies, for example, are spending\nmillions of research dollars to build devices to read emotions from faces,\nerroneously taking the common view as a fact that has strong scientific\nsupport. . . . In fact, our review of the scientific evidence indicates that\nvery little is known about how and why certain facial movements express\ninstances of emotion, particularly at a level of detail sufficient for such\nconclusions to be used in important, real-world\napplications.”[102](notes.html#ich05notes102)\n\nWhy, with so many critiques, has the approach of “reading emotions” from the\nface endured? By analyzing the history of these ideas, we can begin to see how\nmilitary research funding, policing priorities, and profit motives have shaped\nthe field. Since the 1960s, driven by significant Department of Defense\nfunding, multiple systems have been developed that are increasingly accurate\nat measuring movements on faces. Once the theory emerged that it is possible\nto assess internal states by measuring facial movements and the technology was\ndeveloped to measure them, people willingly adopted the underlying premise.\nThe theory fit what the tools could do. Ekman’s theories seemed ideal for the\nemerging field of computer vision because they could be automated at scale.\n\nThere are powerful institutional and corporate investments in the validity of\nEkman’s theories and methodologies. Recognizing that emotions are not easily\nclassified, or that they’re not reliably detectable from facial expressions,\ncould undermine an expanding industry. In the AI field, Ekman is commonly\ncited as though the issue was settled, before directly proceeding into\nengineering challenges. The more complex issues of context, conditioning,\nrelationality, and cultural factors are hard to reconcile with the current\ndisciplinary approaches of computer science or the ambitions of the commercial\ntech sector. So Ekman’s basic emotional categories became standard. More\nsubtle approaches, like Mead’s middle ground, were largely overlooked. The\nfocus has been on increasing the accuracy rates of AI systems rather than on\naddressing the bigger questions about the many ways we experience, show, and\nhide emotion and how we interpret the facial expressions of others.\n\nAs Barrett writes, “Many of the most influential models in our science assume\nthat emotions are biological categories imposed by nature, so that emotion\ncategories are _recognized_ , rather than constructed, by the human\nmind.”[103](notes.html#ich05notes103) AI systems for emotion detection are\npremised on this idea. Recognition might be the wrong framework entirely when\nthinking about emotions because recognition assumes that emotional categories\nare givens, rather than emergent and relational.\n\n![Images](../images/f0176-01.jpg)\n\nColumbia Gaze Dataset. From Brian A. Smith et al., “Gaze Locking: Passive Eye\nContact Detection for Human-Object Interaction,” _ACM Symposium on User\nInterface Software and Technology (UIST)_ , October 2013, 271–80. Courtesy of\nBrian A. Smith\n\n### The Politics of Faces\n\nInstead of trying to build more systems that can group expressions into\nmachine-readable categories, we should question the origins of those\ncategories themselves, as well as their social and political consequences.\nAlready, affect recognition tools are being deployed in political attacks. For\nexample, a conservative blog claimed to create a “virtual polygraph system” to\nassess videos of Congresswoman Ilhan Abdullahi\nOmar.[104](notes.html#ich05notes104) By using face and speech analytics from\nAmazon’s Rekognition, XRVision Sentinel AI, and IBM Watson, the blogger\nclaimed that Omar’s analytic lie score exceeded her “truth baseline” and that\nshe was registering high on stress, contempt, and nervousness. Several\nconservative media outlets ran with the story, claiming that Omar is a\n“pathological liar” and a security threat to the\nnation.[105](notes.html#ich05notes105)\n\nIt’s known that these systems flag the speech affects of women differently\nfrom men, particularly Black women. As we saw in [chapter 3](ch03.html), the\nconstruction of the “average” from unrepresentative training data is\nepistemologically suspect from the outset, with clear racial biases. A study\nconducted at the University of Maryland has shown that some facial recognition\nsoftware interprets Black faces as having more negative emotions than white\nfaces, particularly registering them as angrier and more contemptuous, even\ncontrolling for their degree of smiling.[106](notes.html#ich05notes106)\n\nThis is the danger of affect recognition tools. As we’ve seen, they take us\nback to the phrenological past, where spurious claims were made, allowed to\nstand, and deployed to support existing systems of power. The decades of\nscientific controversies around the idea of inferring distinct emotions from\nhuman faces underscores a central point: the one-size-fits-all recognition\nmodel is not the right metaphor for identifying emotional states. Emotions are\ncomplex, and they develop and change in relation to our families, friends,\ncultures, and histories, all the manifold contexts that live outside of the AI\nframe. In many cases, emotion detection systems do not do what they claim.\nRather than directly measuring people’s interior mental states, they merely\nstatistically optimize correlations of certain physical characteristics among\nfacial images. The scientific foundations of automated emotion detection are\nin question, yet a new generation of affect tools is already making inferences\nacross a growing range of high-stakes contexts from policing to hiring.\n\nEven though evidence now points to the unreliability of affect detection,\ncompanies continue to seek out new sources to mine for facial imagery, vying\nfor the leading market share of a sector that promises billions in profits.\nBarrett’s systemic review of the research behind inferring emotion from\npeople’s faces concludes on a damning note: “More generally, tech companies\nmay well be asking a question that is fundamentally wrong. Efforts to simply\n‘read out’ people’s internal states from an analysis of their facial movements\nalone, without considering various aspects of context, are at best incomplete\nand at worst entirely lack validity, no matter how sophisticated the\ncomputational algorithms. . . . It is premature to use this technology to\nreach conclusions about what people feel on the basis of their facial\nmovements.”[107](notes.html#ich05notes107)\n\nUntil we resist the desire to automate affect recognition, we run the risk of\njob applicants being judged unfairly because their microexpressions do not\nmatch other employees, students receiving poorer grades than their peers\nbecause their faces indicate a lack of enthusiasm, and customers being\ndetained because an AI system flagged them as likely shoplifters based on\ntheir facial cues.[108](notes.html#ich05notes108) These are the people who\nwill bear the costs of systems that are not just technically imperfect but\nbased on questionable methodologies.\n\nThe areas of life in which these systems are operating are expanding as\nrapidly as labs and corporations can create new markets for them. Yet they all\nrely on a narrow understanding of emotions—grown from Ekman’s initial set of\nanger, happiness, surprise, disgust, sadness, and fear—to stand in for the\ninfinite universe of human feeling and expression across space and time. This\ntakes us back to the profound limitations of capturing the complexities of the\nworld in a single classificatory schema. It returns us to the same problem we\nhave seen repeated: the desire to oversimplify what is stubbornly complex so\nthat it can be easily computed, and packaged for the market. AI systems are\nseeking to extract the mutable, private, divergent experiences of our\ncorporeal selves, but the result is a cartoon sketch that cannot capture the\nnuances of emotional experience in the world.\n\n\n![Images](../images/f0180-01.jpg)\n\n\n## 6  \nState\n\nI’m sitting in front of an air-gapped laptop on the tenth floor of a warehouse\nbuilding in New York. On the screen is a software program normally used for\ndigital forensics, a tool for investigating evidence and validating\ninformation held on hard drives. I’m here to research an archive that contains\nsome of the most specific details about how machine learning began to be used\nin the intelligence sector, as led by some of the wealthiest governments in\nthe world. This is the Snowden archive: all the documents, PowerPoint\npresentations, internal memos, newsletters, and technical manuals that former\nNSA contractor and whistleblower Edward Snowden leaked in 2013. Each page is\nmarked with a header noting different forms of classification. TOP SECRET //\nSI // ORCON // NOFORN.[1](notes.html#ich06notes1) Each is a warning and a\ndesignation.\n\nThe filmmaker Laura Poitras first gave me access to this archive in 2014. It\nwas overwhelming to read: the archive held well over a decade of intelligence\nthinking and communication, including internal documents of the National\nSecurity Agency in the United States and the Government Communication\nHeadquarters in the United Kingdom, and the international network of the Five\nEyes.[2](notes.html#ich06notes2) This knowledge was strictly off-limits to\nthose without high-level clearance. It was part of the “classified empire” of\ninformation, once estimated to be growing five times faster than publicly\naccessible knowledge but now is anyone’s guess.[3](notes.html#ich06notes3) The\nSnowden archive captures the years when the collection of data metastasized:\nwhen phones, browsers, social media platforms, and email all became data\nsources for the state. The documents reveal how the intelligence community\ncontributed to the development of many of the techniques we now refer to as\nartificial intelligence.\n\nThe Snowden archive reveals a parallel AI sector, one developed in secrecy.\nThe methods share many similarities, but there are striking differences in\nterms of the reach, the objectives, and the result. Gone are any rhetorical\nconstructs justifying extraction and capture: every software system is simply\ndescribed as something to be owned, to be defeated; all data platforms are\nfair game, and very little is designated as protected. One NSA PowerPoint deck\noutlines TREASUREMAP, a program designed to build a near real-time,\ninteractive map of the internet.[4](notes.html#ich06notes4) It claims to track\nthe location and owner of any connected computer, mobile device, or router:\n“Map the entire internet—any device, anywhere, all the time,” the slide\nboasts. A few slides on “TREASUREMAP as an Enabler” offers up a layer-cake\nimage of signals analysis. Above the geographical layer and the network layer\nis the “cyber persona layer”—quaintly represented on the slide by jellybean-\nera iMacs and Nokia feature phones—and then the “persona layer” of personal\nconnections. This is meant to depict all people who use connected devices\naround the world, in a “300,000-foot view of the internet.” It also looks\nremarkably like the work of social network mapping and manipulation companies\nlike Cambridge Analytica.\n\nThe Snowden documents were released in 2013, but they still read like the AI\nmarketing brochures of today. If TREASUREMAP was a precursor to Facebook’s\nGod’s-eye network view, then the program called FOXACID is reminiscent of\nAmazon Ring for a home computer: recording everyday\nactivity.[5](notes.html#ich06notes5) “If we can get the target to visit us in\nsome sort of browser, we can probably own them,” the slide\nexplains.[6](notes.html#ich06notes6) Once individuals have been tempted to\nclick on a spam email or visit a captured website, the NSA drops files through\na browser that will permanently live on their device, quietly reporting\neverything they do back to base. One slide describes how analysts “deploy very\ntargeted emails” that require “a level of guilty knowledge” about the\ntarget.[7](notes.html#ich06notes7) The restrictions on the NSA gathering that\nguilty knowledge (when it comes to data from American citizens, at least) are\nrarely discussed. One document notes that the agency was working on multiple\nfronts to “aggressively pursue legal authorities and a policy framework mapped\nmore fully to the information age.”[8](notes.html#ich06notes8) In other words,\nchange the laws to fit the tools, not the other way around.\n\n![Images](../images/f0183-01.jpg)\n\ntreasuremap as an Enabler. Snowden archive\n\nThe U.S. intelligence agencies are the old guards of big data. Along with the\nDefense Advanced Research Projects Agency, they have been major drivers of AI\nresearch since the 1950s. As the historian of science Paul Edwards describes\nin _The Closed World_ , military research agencies actively shaped the\nemerging field that would come to be known as AI from its earliest\ndays.[9](notes.html#ich06notes9) The Office of Naval Research, for example,\npartly funded the first Summer Research Project on Artificial Intelligence at\nDartmouth College in 1956.[10](notes.html#ich06notes10) The field of AI has\nalways been strongly guided by military support and often military priorities,\nlong before it was clear that AI could be practical at scale. As Edwards\nnotes:\n\nAs the project with the least immediate utility and the farthest-reaching\nambitions, AI came to rely unusually heavily on ARPA funding. As a result,\nARPA became the primary patron for the first twenty years of AI research.\nFormer director Robert Sproull proudly concluded that “a whole generation of\ncomputer experts got their start from DARPA funding” and that “all the ideas\nthat are going into the fifth-generation [advanced computing] project [of the\nmid-1980s]—artificial intelligence, parallel computing, speech understanding,\nnatural-languages programming—ultimately started from DARPA-funded\nresearch.”[11](notes.html#ich06notes11)\n\nThe military priorities of command and control, automation, and surveillance\nprofoundly shaped what AI was to become. The tools and approaches that came\nout of DARPA funding have marked the field, including computer vision,\nautomatic translation, and autonomous vehicles. But these technical methods\nhave deeper implications. Infused into the overall logics of AI are certain\nkinds of classificatory thinking—from explicitly battlefield-oriented notions\nsuch as target, asset, and anomaly detection to subtler categories of high,\nmedium, and low risk. Concepts of constant situational awareness and targeting\nwould drive AI research for decades, creating epistemological frameworks that\nwould inform both industry and academia.\n\nFrom the point of view of the state, the turn to big data and machine learning\nexpanded the modes of information extraction and informed a social theory of\nhow people can be tracked and understood: _you shall know them by their\nmetadata._ Who is texted, which locations are visited, what is read, when\ndevices spring into action and for what reason—these molecular actions became\na vision of threat identification and assessment, guilt or innocence.\nHarvesting and measuring large aggregates of data at a distance became the\npreferred way to develop alleged insights into groups and communities as well\nas assessments of potential targets for killing. The NSA and GCHQ are not\nunique—China, Russia, Israel, Syria, and many other countries have similar\nagencies. There are many systems of sovereign surveillance and control, a\nmultitude of war machines that never wind down. The Snowden archive\nunderscores how state and corporate actors collaborate in order to produce\nwhat Achille Mbembe calls “infrastructural\nwarfare.”[12](notes.html#ich06notes12)\n\nBut the relationship between national militaries and the AI industry has\nexpanded beyond security contexts. Technologies once only available to\nintelligence agencies—that were _extralegal_ by design—have filtered down to\nthe state’s municipal arms: government and law enforcement agencies. While the\nNSA has been a focus for privacy concerns, less attention is given to the\ngrowing commercial surveillance sector, which aggressively markets its tools\nand platforms to police departments and public agencies. The AI industry is\nsimultaneously challenging and reshaping the traditional role of states while\nalso being used to shore up and expand older forms of geopolitical power.\nAlgorithmic governance is both part of and exceeds traditional state\ngovernance. To paraphrase the theorist Benjamin Bratton, the state is taking\non the armature of a machine because the machines have already taken on the\nroles and register of the state.[13](notes.html#ich06notes13)\n\n### Making the Third Offset\n\nThe story of the internet’s creation has been centered around U.S. military\nand academic innovation and dominance.[14](notes.html#ich06notes14) But in the\nspace of AI, we see that there is no pure national system. Instead, AI systems\noperate within a complex interwoven network of multinational and multilateral\ntools, infrastructures, and labor. Take, for example, a facial recognition\nsystem that was rolled out in the streets of\nBelgrade.[15](notes.html#ich06notes15) The director of police ordered the\ninstallation of two thousand cameras in eight hundred locations around the\ncity to capture faces and license plates. The Serbian government signed an\nagreement with Chinese telecommunications giant Huawei to provide the video\nsurveillance, 4G network support, and unified data and command centers. Such\ndeals are common. Local systems are often hybrids, with infrastructure from\nChina, India, the United States, and elsewhere, with porous boundaries,\ndifferent security protocols, and potential data backdoors.\n\nBut the rhetoric around artificial intelligence is much starker: we are\nrepeatedly told that we are in an AI war. The dominant objects of concern are\nthe supernational efforts of the United States and China, with regular\nreminders that China has stated its commitment to be the global leader in\nAI.[16](notes.html#ich06notes16) The data practices of China’s leading tech\ncompanies, including Alibaba, Huawei, Tencent, and ByteDance, are often framed\nas direct Chinese state policy and thus seen as inherently more threatening\nthan U.S. private actors such as Amazon and Facebook, even though the lines\nbetween state and corporate imperatives and incentives are complexly\nintertwined. Yet the language of war is more than just the usual articulation\nof xenophobia, mutual suspicion, international espionage, and network hacking.\nAs media scholars such as Wendy Chun and Tung-Hui Hu have noted, the liberal\nvision of global digital citizens engaging as equals in the abstract space of\nnetworks has shifted toward a paranoid vision of defending a national cloud\nagainst the racialized enemy.[17](notes.html#ich06notes17) The specter of the\nforeign threat works to assert a kind of sovereign power over AI and to redraw\nthe locus of power of tech companies (which are transnational in\ninfrastructure and influence) back within the bounds of the nation-state.\n\nYet the nationalized race for technological superiority is both rhetorical and\nreal at the same time, creating the dynamics for geopolitical competition\nacross and within commercial and military sectors, increasingly blurring the\nlines between the two. The dual use of AI applications in both civilian and\nmilitary domains has also produced strong incentives for close collaboration\nand funding.[18](notes.html#ich06notes18) In the United States, we can see how\nthis became an explicit strategy: to seek national control and international\ndominance of AI in order to secure military and corporate advantage.\n\nThe latest iteration of this strategy emerged under Ash Carter, who served as\nU.S. secretary of defense from 2015 to 2017. Carter played a significant role\nin bringing Silicon Valley into closer relationship to the military,\nconvincing tech companies that national security and foreign policy depended\non American dominance of AI.[19](notes.html#ich06notes19) He called this the\nThird Offset strategy. An offset is generally understood as a way of\ncompensating for an underlying military disadvantage by changing the\nconditions, or as former secretary of defense Harold Brown stated in 1981,\n“Technology can be a force multiplier, a resource that can be used to help\noffset numerical advantages of an adversary. Superior technology is one very\neffective way to balance military capabilities other than by matching an\nadversary tank-for-tank or soldier-for-soldier.”[20](notes.html#ich06notes20)\n\nThe First Offset is commonly understood as the use of nuclear weapons in the\n1950s.[21](notes.html#ich06notes21) The Second was the expansion of covert,\nlogistical, and conventional weapons in the 1970s and 1980s. The Third,\naccording to Carter, should be a combination of AI, computational warfare, and\nrobots.[22](notes.html#ich06notes22) But unlike the NSA, which already had\nrobust surveillance capabilities, the U.S. military lacked the AI resources,\nexpertise, and infrastructure of America’s leading technology\ncompanies.[23](notes.html#ich06notes23) In 2014, Deputy Defense Secretary\nRobert Work outlined the Third Offset as an attempt to “exploit all the\nadvances in artificial intelligence and\nautonomy.”[24](notes.html#ich06notes24)\n\nTo build AI war machines, the Department of Defense would need gigantic\nextractive infrastructures. Yet in order to gain access to highly paid\nengineering labor and sophisticated development platforms, partnering with\nindustry was necessary. The NSA had paved the way with systems like PRISM,\nboth working with and secretly infiltrating telecommunications and technology\ncompanies.[25](notes.html#ich06notes25) But these more covert approaches faced\nrenewed political pushback after the Snowden disclosures. Congress passed the\nUSA Freedom Act in 2015, which introduced some limitations on the NSA’s access\nto real-time data from Silicon Valley. Yet the possibility for a larger\nmilitary-industrial complex around data and AI remained tantalizingly close.\nSilicon Valley had already built and monetized the logics and infrastructures\nof AI required to drive a new offset. But first the tech sector had to be\nconvinced that partnering on creating the infrastructure of warfare would be\nworth it without alienating their employees and deepening public mistrust.\n\n### Enter Project Maven\n\nIn April 2017, the Department of Defense published a memo announcing the\nAlgorithmic Warfare Cross-Functional Team, code-named Project\nMaven.[26](notes.html#ich06notes26) “The Department of Defense must integrate\nartificial intelligence and machine learning more effectively across\noperations to maintain advantages over increasingly capable adversaries and\ncompetitors,” wrote the deputy defense secretary.[27](notes.html#ich06notes27)\nThe goal of the program was to get the best possible algorithmic systems into\nthe battlefield quickly, even when they were just 80 percent\ncomplete.[28](notes.html#ich06notes28) It was part of a much bigger plan, the\nJoint Enterprise Defense Infrastructure cloud project—or JEDI—an enormous\nredesign of the entire IT infrastructure of the Defense Department, from the\nPentagon to field-level support. Project Maven was a small piece of this\nlarger picture, and the aim was to create an AI system that would allow\nanalysts to select a target and then see every existing clip of drone footage\nthat featured the same person or vehicle.[29](notes.html#ich06notes29)\nUltimately, the Defense Department wanted an automated search engine of drone\nvideos to detect and track enemy combatants.\n\nThe technical platforms and machine learning skills needed for Project Maven\nwere centered in the commercial tech sector. The Defense Department decided to\npay tech companies to analyze military data collected from satellites and\nbattlefield drones in places where U.S. domestic privacy laws did not apply.\nThis would align military and U.S. tech sector financial interests around AI\nwithout directly triggering constitutional privacy tripwires, as the National\nSecurity Agency had done. A bidding war began among the technology companies\nthat wanted the Maven contract, including Amazon, Microsoft, and Google.\n\n![Images](../images/f0190-01.jpg)\n\nThe official seal of the Algorithmic Warfare Cross-Functional Team, code-named\nProject Maven. The Latin motto translates as “Our job is to help.” Produced by\nU.S. Department of Defense\n\nThe first Project Maven contract went to Google. Under the agreement, the\nPentagon would use Google’s TensorFlow AI infrastructure to comb through drone\nfootage and detect objects and individuals as they moved between\nlocations.[30](notes.html#ich06notes30) Fei-Fei Li, then chief scientist of\nAI/ML at Google, was already an expert in building object recognition\ndatasets, given her experience creating ImageNet and using satellite data to\ndetect and analyze cars.[31](notes.html#ich06notes31) But she was adamant that\nthe project should be kept secret. “Avoid at ALL COSTS any mention or\nimplication of AI,” Li wrote in an email to Google colleagues that was later\nleaked. “Weaponized AI is probably one of the most sensitized topics of AI—if\nnot the most. This is red meat to the media to find all ways to damage\nGoogle.”[32](notes.html#ich06notes32)\n\nBut in 2018, Google employees discovered the extent of the company’s role in\nthe project. They were furious that their work was being used for warfare\npurposes, especially after it became known that Project Maven’s image\nidentification goals included objects such as vehicles, buildings, and\nhumans.[33](notes.html#ich06notes33) More than 3,100 employees signed a letter\nof protest stating that Google should not be in the business of war and\ndemanded that the contract be canceled.[34](notes.html#ich06notes34) Under\nincreasing pressure, Google officially ended its work on Project Maven and\nwithdrew from the competition for the Pentagon’s ten-billion-dollar JEDI\ncontract. In October that year, Microsoft’s president, Brad Smith, announced\nin a blog post that “we believe in the strong defense of the United States and\nwe want the people who defend it to have access to the nation’s best\ntechnology, including from Microsoft.”[35](notes.html#ich06notes35) The\ncontract ultimately went to Microsoft, which outbid\nAmazon.[36](notes.html#ich06notes36)\n\nShortly after the internal uprising, Google released its Artificial\nIntelligence Principles, which included a section on “AI applications we will\nnot pursue.”[37](notes.html#ich06notes37) These included making “weapons or\nother technologies whose principal purpose or implementation is to cause or\ndirectly facilitate injury to people,” as well as “technologies that gather or\nuse information for surveillance violating internationally accepted\nnorms.”[38](notes.html#ich06notes38) While the turn to AI ethics quelled some\ninternal and external concerns, the enforceability and parameters of ethical\nrestraint were left unclear.[39](notes.html#ich06notes39)\n\nIn response, former Google CEO Eric Schmidt characterized the pushback over\nProject Maven as “a general concern in the tech community of somehow the\nmilitary-industrial complex using our stuff to kill people incorrectly, if you\nwill.”[40](notes.html#ich06notes40) This shift, from the debate over whether\nto use AI in warfare at all to a debate over whether AI could help to “kill\npeople correctly,” was quite strategic.[41](notes.html#ich06notes41) It moved\nthe focus away from the foundational ethics of AI as a military technology\ntoward questions of precision and technical accuracy. But Lucy Suchman argues\nthat the problems with automated warfare go far beyond whether the killing was\naccurate or “correct.”[42](notes.html#ich06notes42) Particularly in the case\nof object detection, Suchman asks, who is building the training sets and using\nwhat data, and how are things labeled as an imminent threat? What kinds of\nclassificatory taxonomies are used to decide what constitutes sufficiently\nabnormal activity to trigger a legal drone attack? And why should we condone\nattaching life or death consequences to these unstable and inherently\npolitical classifications?[43](notes.html#ich06notes43)\n\nThe Maven episode, as well as the AI principles that emerge, points to the\ndeep schisms in the AI industry about the relationship between the military\nand civilian spheres. The AI war, both real and imagined, instills a politics\nof fear and insecurity that creates a climate that is used to stifle internal\ndissent and promote unquestioning support for a nationalist\nagenda.[44](notes.html#ich06notes44) After the fallout from Maven faded,\nGoogle’s chief legal officer, Kent Walker, said that the company was pursuing\nhigher security certifications in order to work more closely with the Defense\nDepartment. “I want to be clear,” he said. “We are a proud American\ncompany.”[45](notes.html#ich06notes45) Articulating patriotism as policy, tech\ncompanies are increasingly expressing strong alignment with the interests of\nthe nation-state, even as their platforms and capacities exceed traditional\nstate governance.\n\n### The Outsourced State\n\nThe relationship between the state and the AI industry goes well beyond\nnational militaries. The technologies once reserved for war zones and\nespionage are now used at the local level of government, from welfare agencies\nto law enforcement. This shift has been propelled by outsourcing key functions\nof the state to technology contractors. On the surface, this does not seem\nvery different than the usual outsourcing of government functions to the\nprivate sector through companies such as Lockheed Martin or Halliburton. But\nnow militarized forms of pattern detection and threat assessment are moving at\nscale into municipal-level services and\ninstitutions.[46](notes.html#ich06notes46) A significant example of this\nphenomenon is the company named after the magical seeing stones in _Lord of\nthe Rings:_ Palantir.\n\nPalantir was established in 2004, cofounded by PayPal billionaire Peter Thiel,\nwho was also an adviser and financial supporter of President Trump. Thiel\nwould later argue in an opinion piece that AI is first and foremost a military\ntechnology: “Forget the sci-fi fantasy; what is powerful about actually\nexisting AI is its application to relatively mundane tasks like computer\nvision and data analysis. Though less uncanny than Frankenstein’s monster,\nthese tools are nevertheless valuable to any army—to gain an intelligence\nadvantage, for example. . . . No doubt machine learning tools have civilian\nuses, too.”[47](notes.html#ich06notes47)\n\nWhile Thiel recognizes the nonmilitary uses of machine learning, he\nparticularly believes in the _in-between space:_ where commercial companies\nproduce military-styled tools to be provided to anyone who would like to gain\nan intelligence advantage and is willing to pay for it. Both he and Palantir’s\nCEO, Alex Karp, describe Palantir as “patriotic,” with Karp accusing other\ntechnology companies that refuse to work with the military agencies as\n“borderline craven.”[48](notes.html#ich06notes48) In an insightful essay, the\nwriter Moira Weigel studied Karp’s university dissertation, which reveals his\nearly intellectual interest in aggression and a belief that “the desire to\ncommit violence is a constant founding fact of human\nlife.”[49](notes.html#ich06notes49) Karp’s thesis was titled “Aggression in\nthe Life World.”\n\nPalantir’s original clients were federal military and intelligence agencies,\nincluding the Defense Department, National Security Agency, FBI, and\nCIA.[50](notes.html#ich06notes50) As revealed in an investigation by Mijente,\nafter Trump took the presidency, Palantir’s contracts with U.S. agencies\ntotaled more than a billion dollars.[51](notes.html#ich06notes51) But Palantir\ndid not style itself as a typical defense contractor in the mold of Lockheed\nMartin. It adopted the character of the Silicon Valley start-up, based in Palo\nAlto and predominantly staffed by young engineers, and it was backed by In-Q-\nTel, the venture capital firm funded by the CIA. Beyond its initial\nintelligence agency clients, Palantir began to work with hedge funds, banks,\nand corporations like Walmart.[52](notes.html#ich06notes52) But its DNA was\nshaped working for, and within, the defense community. It deployed the same\napproaches seen in the Snowden documents, including extracting data across\ndevices and infiltrating networks in order to track and evaluate people and\nassets. Palantir quickly became a preferred outsourced surveillance provider,\nincluding designing the databases and management software to drive the\nmechanics of deportation for Immigration and Customs Enforcement\n(ICE).[53](notes.html#ich06notes53)\n\nPalantir’s business model is based on a mix of data analysis and pattern\ndetection using machine learning, combined with more generic consulting.\nPalantir sends engineers into a company, who extract a wide variety of\ndata—emails, call logs, social media, when employees enter and leave\nbuildings, when they book plane tickets, everything the company is prepared to\nshare—then look for patterns and give advice on what to do next. One common\napproach is to search for current or potential so-called bad actors,\ndisgruntled employees who may leak information or defraud the company. The\nunderlying worldview built into Palantir’s tools is reminiscent of the NSA:\ncollect everything, then look for anomalies in the data. However, while the\nNSA’s tools are built to surveil and target enemies of the state, in either\nconventional or covert warfare, Palantir’s approach has been directed against\ncivilians. As described in a major investigation by Bloomberg in 2018,\nPalantir is “an intelligence platform designed for the global War on Terror”\nthat is now “weaponized against ordinary Americans at home”: “Palantir cut its\nteeth working for the Pentagon and the CIA in Afghanistan and Iraq. . . . The\nU.S. Department of Health and Human Services uses Palantir to detect Medicare\nfraud. The FBI uses it in criminal probes. The Department of Homeland Security\ndeploys it to screen air travelers and keep tabs on\nimmigrants.”[54](notes.html#ich06notes54)\n\nSoon, keeping tabs on undocumented workers evolved into capturing and\ndeporting people at schools and places of work. In furtherance of this\nobjective, Palantir produced a phone app called falcon, which functions as a\nvast dragnet, gathering data from multiple law enforcement and public\ndatabases that list people’s immigration histories, family relationships,\nemployment information, and school details. In 2018, ICE agents used falcon to\nguide their raid of almost a hundred 7-Elevens across the United States in\nwhat was called “the largest operation against a single employer in the Trump\nera.”[55](notes.html#ich06notes55)\n\nDespite Palantir’s efforts to maintain secrecy about what it builds or how its\nsystems work, its patent applications give us some insight into the company’s\napproach to AI for deportation. In an application innocuously entitled\n_Database systems and user interfaces for dynamic and interactive mobile image\nanalysis and identification_ , Palantir brags about the app’s ability to\nphotograph people in short-time-frame encounters and, regardless of whether\nthey are under suspicion or not, to run their image against all available\ndatabases. In essence, the system uses facial recognition and back-end\nprocessing to create a framework on which to base any arrest or deportation.\n\n![Images](../images/f0196-01.jpg)\n\nAn image from Palantir’s patent US10339416B2. Courtesy U.S. Patent and\nTrademark Office\n\nWhile Palantir’s systems have structural similarities to those at the NSA,\nthey have devolved to a local community level, to be sold to supermarket\nchains and local law enforcement alike. This represents a shift away from\ntraditional policing toward the goals more associated with military\nintelligence infrastructures. As law professor Andrew Ferguson explains, “We\nare moving to a state where prosecutors and police are going to say ‘the\nalgorithm told me to do it, so I did, I had no idea what I was doing.’ And\nthis will be happening at a widespread level with very little\noversight.”[56](notes.html#ich06notes56)\n\nThe sociologist Sarah Brayne was one of the first scholars to observe directly\nhow Palantir’s data platforms are used in situ, specifically by the Los\nAngeles Police Department. After more than two years of riding along with\npolice on patrols, watching them at their desks, and conducting multiple\ninterviews, Brayne concluded that in some domains these tools merely amplify\nprior police practices but that in other ways they are transforming the\nprocess of surveillance entirely. In short, police are turning into\nintelligence agents:\n\nThe shift from traditional to big data surveillance is associated with a\nmigration of law enforcement operations toward intelligence activities. The\nbasic distinction between law enforcement and intelligence is as follows: law\nenforcement typically becomes involved once a criminal incident has occurred.\nLegally, the police cannot undertake a search and gather personal information\nuntil there is probable cause. Intelligence, by contrast, is fundamentally\npredictive. Intelligence activities involve gathering data; identifying\nsuspicious patterns, locations, activity, and individuals; and preemptively\nintervening based on the intelligence acquired.[57](notes.html#ich06notes57)\n\nAlthough everyone is subject to these types of surveillance, some people are\nmore likely to be subjected to it than others: immigrants, the undocumented,\nthe poor, and communities of color. As Brayne observed in her study, the use\nof Palantir’s software reproduces inequality, making those in predominantly\npoor, Black, and Latinx neighborhoods subject to even greater surveillance.\nPalantir’s point system lends an aura of objectivity: it’s “just math,” in the\nwords of one police officer. But it creates a reinforcing loop of\nlogic.[58](notes.html#ich06notes58) Brayne writes:\n\nDespite the stated intent of the point system to avoid legally contestable\nbias in police practices, it hides both intentional and unintentional bias in\npolicing and creates a self-perpetuating cycle: if individuals have a high\npoint value, they are under heightened surveillance and therefore have a\ngreater likelihood of being stopped, further increasing their point value.\nSuch practices hinder the ability of individuals already in the criminal\njustice system from being further drawn into the surveillance net, while\nobscuring the role of enforcement in shaping risk\nscores.[59](notes.html#ich06notes59)\n\nThe machine learning approaches of Palantir and its ilk can lead to a feedback\nloop, where those included in a criminal justice database are more likely to\nbe surveilled and thus more likely to have more information about them\nincluded, which justifies further police\nscrutiny.[60](notes.html#ich06notes60) Inequity is not only deepened but tech-\nwashed, justified by the systems that appear immune to error yet are, in fact,\nintensifying the problems of overpolicing and racially biased\nsurveillance.[61](notes.html#ich06notes61) The intelligence models that began\nin national government agencies have now become part of the policing of local\nneighborhoods. The NSA-ification of police departments exacerbates historical\ninequality and radically transforms and expands the practices of police work.\n\nDespite the massive expansion of government contracts for AI systems, little\nattention has been given to the question of whether private vendors of these\ntechnologies should be legally accountable for the harms produced when\ngovernments use their systems. Given how often governments are turning to\ncontractors to provide the algorithmic architectures for state decision-\nmaking, be it policing or welfare systems, there is a case that technology\ncontractors like Palantir should be liable for discrimination and other\nviolations. Currently most states attempt to disclaim any responsibility for\nproblems created by the AI systems they procure, with the argument that “we\ncannot be responsible for something we don’t understand.” This means that\ncommercial algorithmic systems are contributing to the process of government\ndecision making without meaningful mechanisms of accountability. With the\nlegal scholar Jason Schultz, I’ve argued that developers of AI systems that\ndirectly influence government decisions should be found to be state actors for\npurposes of constitutional liability in certain\ncontexts.[62](notes.html#ich06notes62) That is, they could be found legally\nliable for harms in the same way that states can be. Until then, vendors and\ncontractors have little incentive to ensure that their systems aren’t\nreinforcing historical harms or creating entirely new\nones.[63](notes.html#ich06notes63)\n\nAnother example of this phenomenon is Vigilant Solutions, established in 2005.\nThe company works on the basis of a single premise: take surveillance tools\nthat might require judicial oversight if operated by governments and turn them\ninto a thriving private enterprise outside constitutional privacy limits.\nVigilant began its venture in multiple cities across the United States by\ninstalling automatic license-plate recognition (ALPR) cameras, placing them\neverywhere from cars to light poles, parking lots to apartment buildings. This\narray of networked cameras photographs every passing car, storing license\nplate images in a massive perpetual database. Vigilant then sells access to\nthat database to the police, private investigators, banks, insurance\ncompanies, and others who want access to it. If police officers want to track\na car across the entire state and mark every place it has been, Vigilant can\nshow them. Likewise, if a bank wanted to repossess a car, Vigilant could\nreveal where it was, for a price.\n\nCalifornia-based Vigilant markets itself as “one of those trusted crime\nfighting tools to help law enforcement develop leads and solve crimes faster,”\nand it has partnered with a range of governments in Texas, California, and\nGeorgia to provide their police with a suite of ALPR systems to use on patrol,\nalong with access to Vigilant’s database.[64](notes.html#ich06notes64) In\nreturn, the local governments provide Vigilant with records of outstanding\narrest warrants and overdue court fees. Any license plates flagged to match\nthose associated with outstanding fines in the database are fed into police\nofficers’ mobile systems, altering them to pull these drivers over. Drivers\nare then given two options: pay the outstanding fine on the spot or be\narrested. On top of taking a 25 percent surcharge, Vigilant keeps records of\nevery license plate reading, extracting that data to add to its massive\ndatabases.\n\nVigilant signed a significant contract with ICE that gave the agency access to\nfive billion records of license plates gathered by private businesses, as well\nas 1.5 billion data points contributed by eighty local law enforcement\nagencies across the United States—including information on where people live\nand work. That data can stem from informal arrangements between local police\nand ICE and may already violate state data-sharing laws. ICE’s own privacy\npolicy limits data collection near “sensitive locations” like schools,\nchurches, and protests. But in this case, ICE doesn’t collect the data or\nmaintain the database—the agency simply buys access to Vigilant’s systems,\nwhich has far fewer restrictions. This is a de facto privatization of public\nsurveillance, a blurring between private contractors and state entities, and\nit creates opaque forms of data harvesting that live outside of traditional\nprotective guidelines.[65](notes.html#ich06notes65)\n\nVigilant has since expanded its “crime-solving” toolkit beyond license plate\nreaders to include ones that claim to recognize faces. In doing so, Vigilant\nseeks to render human faces as the equivalent of license plates and then feed\nthem back into the policing ecology.[66](notes.html#ich06notes66) Like a\nnetwork of private detectives, Vigilant creates a God’s-eye view of America’s\ninterlaced roads and highways, along with everyone who travels along them,\nwhile remaining beyond any meaningful form of regulation or\naccountability.[67](notes.html#ich06notes67)\n\nIf we move from the police cruiser to the front porch, we see yet another\nlocation where the differences between public and private sector data\npractices are eroding. A new generation of social media crime-reporting apps\nlike Neighbors, Citizen, and Nextdoor allow users to get alerts about local\nincidents reported in real time, then discuss them, as well as broadcast,\nshare, and tag security camera footage. Neighbors, which is made by Amazon and\nrelies on its Ring doorbell cameras, defines itself as the “new neighborhood\nwatch” and classifies footage into categories like Crime, Suspicious, or\nStranger. Videos are often shared with police.[68](notes.html#ich06notes68) In\nthese residential surveillance ecosystems, the logics of treasuremap and\nfoxacid conjoin, but connected to the home, the street, and every place in\nbetween.\n\nFor Amazon, each new Ring device sold helps build yet more large-scale\ntraining datasets inside and outside the home, with classificatory logics of\nnormal and anomalous behavior aligned with the battlefield logics of allies\nand enemies. One example is a feature where users can report stolen Amazon\npackages. According to one journalistic investigation, many of the posts\nfeatured racist commentary, and video posts disproportionately depicted people\nof color as potential thieves.[69](notes.html#ich06notes69) Beyond reporting\ncrime, Ring is also used to report Amazon employees who are seen as\nunderperforming, such as being insufficiently careful with packages—creating a\nnew layer of worker surveillance and retribution.[70](notes.html#ich06notes70)\n\nTo complete its public-private infrastructure of surveillance, Amazon has been\naggressively marketing the Ring system to police departments, giving them\ndiscounts and offering a portal that allows police to see where Ring cameras\nare located in the local area and to contact homeowners directly to request\nfootage informally without a warrant.[71](notes.html#ich06notes71) Amazon has\nnegotiated Ring video-sharing partnerships with more than six hundred police\ndepartments.[72](notes.html#ich06notes72)\n\nIn one case, Amazon negotiated a memorandum of understanding with a police\ndepartment in Florida, discovered through a public records request filed by\njournalist Caroline Haskins, which showed that police were incentivized to\npromote the Neighbors app and for every qualifying download they would receive\ncredits toward free Ring cameras.[73](notes.html#ich06notes73) The result was\na “self-perpetuating surveillance network: more people download Neighbors,\nmore people get Ring, surveillance footage proliferates, and police can\nrequest whatever they want,” Haskins writes.[74](notes.html#ich06notes74)\nSurveillance capacities that were once ruled over by courts are now on offer\nin Apple’s App Store and promoted by local street cops. As media scholar Tung-\nHui Hu observes, by using such apps, we “become freelancers for the state’s\nsecurity apparatus.”[75](notes.html#ich06notes75)\n\nHu describes how targeting—a quintessential militaristic term—in all its forms\nshould be considered together as one interconnected system of power—from\ntargeted advertising to targeting suspicious neighbors to targeting drones.\n“We cannot merely consider one form of targeting in isolation from the other;\nconjoined in the sovereignty of data, they call on us to understand power in\nthe age of the cloud differently.”[76](notes.html#ich06notes76) The ways of\nseeing that were once the sole province of intelligence agencies have been\ngranulated and dispersed throughout many social systems—embedded in\nworkplaces, homes, and cars—and promoted by technology companies that live in\nthe cross-hatched spaces that overlap the commercial and military AI sectors.\n\n### From Terrorist Credit Scores to Social Credit Scores\n\nUnderlying the military logics of targeting is the idea of the _signature._\nToward the end of President George W. Bush’s second term, the CIA argued that\nit should be able to launch drone attacks based solely on an individual’s\nobserved “pattern of behavior” or “signature.”[77](notes.html#ich06notes77)\nWhereas a “personality strike” involves targeting a specific individual, a\n“signature strike” is when a person is killed due to their metadata signature;\nin other words, their identity is not known but data suggests that they might\nbe a terrorist.[78](notes.html#ich06notes78) As the Snowden documents showed,\nduring the Obama years, the National Security Agency’s global metadata\nsurveillance program would geolocate a SIM card or handset of a suspect, and\nthen the U.S. military would conduct drone strikes to kill the individual in\npossession of the device.[79](notes.html#ich06notes79) “We kill people based\non metadata,” said General Michael Hayden, former director of the NSA and the\nCIA.[80](notes.html#ich06notes80) The NSA’s Geo Cell division was reported to\nuse more colorful language: “We track ’em, you whack\n’em.”[81](notes.html#ich06notes81)\n\nSignature strikes may sound precise and authorized, implying a true mark of\nsomeone’s identity. But in 2014, the legal organization Reprieve published a\nreport showing that drone strikes attempting to kill 41 individuals resulted\nin the deaths of an estimated 1,147 people. “Drone strikes have been sold to\nthe American public on the claim that they are ‘precise.’ But they are only as\nprecise as the intelligence that feeds them,” said Jennifer Gibson, who led\nthe report.[82](notes.html#ich06notes82) But the form of the signature strike\nis not about precision: it is about correlation. Once a pattern is found in\nthe data and it reaches a certain threshold, the suspicion becomes enough to\ntake action even in the absence of definitive proof. This mode of adjudication\nby pattern recognition is found in many domains—most often taking the form of\na score.\n\nConsider an example from the 2015 Syrian refugee crisis. Millions of people\nwere fleeing widespread civil war and enemy occupation in hopes of finding\nasylum in Europe. Refugees were risking their lives on rafts and overcrowded\nboats. On September 2, a three-year-old boy named Alan Kurdi drowned in the\nMediterranean Sea, alongside his five-year-old brother, when their boat\ncapsized. A photograph showing his body washed up on a beach in Turkey made\ninternational headlines as a potent symbol for the extent of the humanitarian\ncrisis: one image standing in for the aggregate horror. But some saw this as a\ngrowing threat. It is around this time that IBM was approached about a new\nproject. Could the company use its machine learning platform to detect the\ndata signature of refugees who might be connected to jihadism? In short, could\nIBM automatically distinguish a terrorist from a refugee?\n\nAndrew Borene, a strategic initiatives executive at IBM, described the\nrationale behind the program to the military publication _Defense One:_ “Our\nworldwide team, some of the folks in Europe, were getting feedback that there\nwere some concerns that within these asylum-seeking populations that had been\nstarved and dejected, there were fighting-age males coming off of boats that\nlooked awfully healthy. Was that a cause for concern in regard to ISIS and, if\nso, could this type of solution be helpful?”[83](notes.html#ich06notes83)\n\nFrom the safe distance of their corporate offices, IBM’s data scientists\nviewed the problem as one best addressed through data extraction and social\nmedia analysis. Setting aside the many variables that existed in the\nconditions of makeshift refugee camps and the dozens of assumptions used to\nclassify terrorist behavior, IBM created an experimental “terrorist credit\nscore” to weed out ISIS fighters from refugees. Analysts harvested a\nmiscellany of unstructured data, from Twitter to the official list of those\nwho had drowned alongside the many capsized boats off the shores of Greece and\nTurkey. They also made up a data set, modeled on the types of metadata\navailable to border guards. From these disparate measures, they developed a\nhypothetical threat score: not an absolute indicator of guilt or innocence,\nthey pointed out, but a deep “insight” into the individual, including past\naddresses, workplaces, and social connections.[84](notes.html#ich06notes84)\nMeanwhile, Syrian refugees had no knowledge that their personal data was being\nharvested to trial a system that might single them out as potential\nterrorists.\n\nThis is just one of many cases where new technical systems of state control\nuse the bodies of refugees as test cases. These military and policing logics\nare now suffused with a form of financialization: socially constructed models\nof creditworthiness have entered into many AI systems, influencing everything\nfrom the ability to get a loan to permission to cross borders. Hundreds of\nsuch platforms are now in use around the world, from China to Venezuela to the\nUnited States, rewarding predetermined forms of social behavior and penalizing\nthose who do not conform.[85](notes.html#ich06notes85) This “new regime of\nmoralized social classification,” in the words of sociologists Marion Fourcade\nand Kieran Healy, benefits the “high achievers” of the traditional economy\nwhile further disadvantaging the least privileged\npopulations.[86](notes.html#ich06notes86) Credit scoring, in the broadest\nsense, has become a place where the military and commercial signatures\ncombine.\n\nThis AI scoring logic is deeply entwined in law enforcement and border\ncontrol, traditional domains of the state, but it also informs another state\nfunction: access to public benefits. As the political scientist Virginia\nEubanks shows in her book _Automating Inequality_ , when AI systems are\ndeployed as part of the welfare state, they are used primarily as a way to\nsurveil, assess, and restrict people’s access to public resources rather than\nas a way to provide for greater support.[87](notes.html#ich06notes87)\n\nA key example of this dynamic emerged when former Republican governor of\nMichigan Rick Snyder, previously the chairman of computer hardware computer\nGateway, decided to implement two algorithmically driven austerity programs in\nan attempt to undermine the economic security of his poorest citizens under\nthe auspices of state budget cuts. First, he directed that a matching\nalgorithm be used to implement the state’s “fugitive felon” policy, which\nsought automatically to disqualify individuals from food assistance based on\noutstanding felony warrants. Between 2012 and 2015, the new system\ninaccurately matched more than nineteen thousand Michigan residents and\nautomatically disqualified each of them from food\nassistance.[88](notes.html#ich06notes88)\n\nThe second scheme was called the Michigan Integrated Data Automated System\n(MiDAS), a system built to “robo-adjudicate” and punish those it determined to\nbe defrauding the state’s unemployment insurance. MiDAS was designed to treat\nalmost any data discrepancies or inconsistencies in an individual’s record as\npotential evidence of illegal conduct. The system inaccurately identified more\nthan forty thousand Michigan residents of suspected fraud. The consequences\nwere severe: seizure of tax refunds, garnishment of wages, and imposition of\ncivil penalties that were four times the amount people were accused of owing.\nUltimately, both systems were giant financial failures, costing Michigan far\nmore money than it saved. Those harmed were able to successfully sue the state\nover the systems, but not before thousands of people were affected, with many\nentering bankruptcy.[89](notes.html#ich06notes89)\n\nWhen viewed in the overall context of state-driven AI systems, one can see the\nconsistent logics between targeting terrorists or undocumented workers and\ntargeting fugitive felons or suspected fraudsters. Even though food assistance\nand unemployment benefits were created to support the poor and to promote\nsocial and economic stability, the use of militaristic systems of command-and-\ncontrol for the purposes of punishment and exclusion undermine the overall\ngoals of the systems. In essence these systems are punitive, designed on a\nthreat-targeting model. The motifs of scoring and risk have permeated deeply\nthrough the structures of state bureaucracy, and the automated decision\nsystems that are imagined in those institutions drive that logic deeply into\nthe way that communities and individuals are imagined, evaluated, scored, and\nserved.\n\n### The Tangled Haystack\n\nI am almost at the end of a long day searching through the Snowden archive\nwhen I run across a slide that describes the planet as a “haystack of\ninformation,” in which desirable intel is a needle lost somewhere among the\nstraw. It includes a cheery clip art image of a giant haystack in a field with\na blue sky overhead. This cliché of information gathering is tactical: hay is\nmown for the good of the farm, gleaned to produce value. This invokes a\ncomforting pastoral imagery of data agriculture—tending the fields to further\norderly extraction and production cycles. Phil Agre once observed that\n“technology at present is covert philosophy; the point is to make it openly\nphilosophical.”[90](notes.html#ich06notes90) The philosophy here is that data\nshould be extracted globally and structured in order to maintain U.S.\nhegemony. But we’ve seen how these stories break down under scrutiny.\n\nThe overlapping grids of planetary computation are complex, cross-breeding\ncorporate and state logics, exceeding traditional state border and governance\nlimits, and they are far messier than the idea of winner takes all might\nimply. As Benjamin Bratton argues, “The armature of planetary-scale\ncomputation has a determining logic that is self-reinforcing if not self-\nfulfilling, and which through the automation of its own infrastructural\noperations, exceeds any national designs even if it is also used on their\nbehalf.”[91](notes.html#ich06notes91) The jingoistic idea of sovereign AI,\nsecurely contained within national borders, is a myth. AI infrastructure is\nalready a hybrid, and as Hu argues, so is the labor force underpinning it,\nfrom factory laborers in China who make electronic components to Russian\nprogrammers providing cloud labor to Moroccan freelancers who screen content\nand label images.[92](notes.html#ich06notes92)\n\nTaken together, the AI and algorithmic systems used by the state, from the\nmilitary to the municipal level, reveal a covert philosophy of _en masse_\ninfrastructural command and control via a combination of extractive data\ntechniques, targeting logics, and surveillance. These goals have been central\nto the intelligence agencies for decades, but now they have spread to many\nother state functions, from local law enforcement to allocating\nbenefits.[93](notes.html#ich06notes93) This is just part of the deep\nintermingling of state, municipal, and corporate logics through extractive\nplanetary computation. But it is an uncomfortable bargain: states are making\ndeals with technology companies they can’t control or even fully understand,\nand technology companies are taking on state and extrastate functions that\nthey are ill-suited to fulfill and for which, at some point in the future,\nthey might be held liable.\n\nThe Snowden archive shows how far these overlapping and contradictory logics\nof surveillance extend. One document notes the symptoms of what an NSA\nemployee described as an addiction to the God’s-eye view that data seems to\noffer: “Mountaineers call this phenomenon ‘summit fever’—when an ‘individual\nbecomes so fixated on reaching the summit that all else fades from\nconsciousness.’ I believe that siginters, like the world-class climbers, are\nnot immune to summit fever. It’s easy enough to lose sight of the bad weather\nand push on relentlessly, especially after pouring lots of money, time, and\nresources into something.”[94](notes.html#ich06notes94)\n\nAll the money and resources spent on relentless surveillance is part of a\nfever dream of centralized control that has come at the cost of other visions\nof social organization. The Snowden disclosures were a watershed moment in\nrevealing how far a culture of extraction can go when the state and the\ncommercial sector collaborate, but the network diagrams and PowerPoint clip\nart can feel quaint compared to all that has happened\nsince.[95](notes.html#ich06notes95) The NSA’s distinctive methods and tools\nhave filtered down to classrooms, police stations, workplaces, and\nunemployment offices. It is the result of enormous investments, of de facto\nforms of privatization, and the securitization of risk and fear. The current\ndeep entanglement of different forms of power was the hope of the Third\nOffset. It has warped far beyond the objective of strategic advantage in\nbattlefield operations to encompass all those parts of everyday life that can\nbe tracked and scored, grounded in normative definitions of how good citizens\nshould communicate, behave, and spend. This shift brings with it a different\nvision of state sovereignty, modulated by corporate algorithmic governance,\nand it furthers the profound imbalance of power between agents of the state\nand the people they are meant to serve.\n\n\n![Images](../images/f0210-01.jpg)\n\n\n## Conclusion  \nPower\n\nArtificial intelligence is not an objective, universal, or neutral\ncomputational technique that makes determinations without human direction. Its\nsystems are embedded in social, political, cultural, and economic worlds,\nshaped by humans, institutions, and imperatives that determine what they do\nand how they do it. They are designed to discriminate, to amplify hierarchies,\nand to encode narrow classifications. When applied in social contexts such as\npolicing, the court system, health care, and education, they can reproduce,\noptimize, and amplify existing structural inequalities. This is no accident:\nAI systems are built to see and intervene in the world in ways that primarily\nbenefit the states, institutions, and corporations that they serve. In this\nsense, AI systems are expressions of power that emerge from wider economic and\npolitical forces, created to increase profits and centralize control for those\nwho wield them. But this is not how the story of artificial intelligence is\ntypically told.\n\nThe standard accounts of AI often center on a kind of algorithmic\nexceptionalism—the idea that because AI systems can perform uncanny feats of\ncomputation, they must be smarter and more objective than their flawed human\ncreators. Consider this diagram of AlphaGo Zero, an AI program designed by\nGoogle’s DeepMind to play strategy games.[1](notes.html#iconclnotes1) The\nimage shows how it “learned” to play the Chinese strategy game Go by\nevaluating more than a thousand options per move. In the paper announcing this\ndevelopment, the authors write: “Starting _tabula rasa_ , our new program\nAlphaGo Zero achieved superhuman performance.”[2](notes.html#iconclnotes2)\nDeepMind cofounder Demis Hassabis has described these game engines as akin to\nan alien intelligence. “It doesn’t play like a human, but it also doesn’t play\nlike computer engines. It plays in a third, almost alien, way. . . . It’s like\nchess from another dimension.”[3](notes.html#iconclnotes3) When the next\niteration mastered Go within three days, Hassabis described it as\n“rediscovering three thousand years of human knowledge in 72\nhours!”[4](notes.html#iconclnotes4)\n\n![Images](../images/f0212-01.jpg)\n\nGo knowledge learned by AlphaGo Zero. Courtesy of DeepMind\n\nThe Go diagram shows no machines, no human workers, no capital investment, no\ncarbon footprint, just an abstract rules-based system endowed with\notherworldly skills. Narratives of magic and mystification recur throughout\nAI’s history, drawing bright circles around spectacular displays of speed,\nefficiency, and computational reasoning.[5](notes.html#iconclnotes5) It’s no\ncoincidence that one of the iconic examples of contemporary AI is a game.\n\n### Games without Frontiers\n\nGames have been a preferred testing ground for AI programs since the\n1950s.[6](notes.html#iconclnotes6) Unlike everyday life, games offer a closed\nworld with defined parameters and clear victory conditions. The historical\nroots of AI in World War II stemmed from military-funded research in signal\nprocessing and optimization that sought to simplify the world, rendering it\nmore like a strategy game. A strong emphasis on rationalization and prediction\nemerged, along with a faith that mathematical formalisms would help us\nunderstand humans and society.[7](notes.html#iconclnotes7) The belief that\naccurate prediction is fundamentally about reducing the complexity of the\nworld gave rise to an implicit theory of the social: find the signal in the\nnoise and make order from disorder.\n\nThis epistemological flattening of complexity into clean signal for the\npurposes of prediction is now a central logic of machine learning. The\nhistorian of technology Alex Campolo and I call this _enchanted determinism:_\nAI systems are seen as enchanted, beyond the known world, yet deterministic in\nthat they discover patterns that can be applied with predictive certainty to\neveryday life.[8](notes.html#iconclnotes8) In discussions of deep learning\nsystems, where machine learning techniques are extended by layering abstract\nrepresentations of data on top of each other, enchanted determinism acquires\nan almost theological quality. That deep learning approaches are often\nuninterpretable, even to the engineers who created them, gives these systems\nan aura of being too complex to regulate and too powerful to refuse. As the\nsocial anthropologist F. G. Bailey observed, the technique of “obscuring by\nmystification” is often employed in public settings to argue for a\nphenomenon’s inevitability.[9](notes.html#iconclnotes9) We are told to focus\non the innovative nature of the method rather than on what is primary: the\npurpose of the thing itself. Above all, enchanted determinism obscures power\nand closes off informed public discussion, critical scrutiny, or outright\nrejection.\n\nEnchanted determinism has two dominant strands, each a mirror image of the\nother. One is a form of tech utopianism that offers computational\ninterventions as universal solutions applicable to any problem. The other is a\ntech dystopian perspective that blames algorithms for their negative outcomes\nas though they are independent agents, without contending with the contexts\nthat shape them and in which they operate. At an extreme, the tech dystopian\nnarrative ends in the singularity, or superintelligence—the theory that a\nmachine intelligence could emerge that will ultimately dominate or destroy\nhumans.[10](notes.html#iconclnotes10) This view rarely contends with the\nreality that so many people around the world are _already_ dominated by\nsystems of extractive planetary computation.\n\nThese dystopian and utopian discourses are metaphysical twins: one places its\nfaith in AI as a solution to every problem, while the other fears AI as the\ngreatest peril. Each offers a profoundly ahistorical view that locates power\nsolely within technology itself. Whether AI is abstracted as an all-purpose\ntool or an all-powerful overlord, the result is technological determinism. AI\ntakes the central position in society’s redemption or ruin, permitting us to\nignore the systemic forces of unfettered neoliberalism, austerity politics,\nracial inequality, and widespread labor exploitation. Both the tech utopians\nand dystopians frame the problem with technology always at the center,\ninevitably expanding into every part of life, decoupled from the forms of\npower that it magnifies and serves.\n\nWhen AlphaGo defeats a human grandmaster, it’s tempting to imagine that some\nkind of otherworldly intelligence has arrived. But there’s a far simpler and\nmore accurate explanation. AI game engines are designed to play millions of\ngames, run statistical analyses to optimize for winning outcomes, and then\nplay millions more. These programs produce surprising moves uncommon in human\ngames for a straightforward reason: they can play and analyze far more games\nat a far greater speed than any human can. This is not magic; it is\nstatistical analysis at scale. Yet the tales of preternatural machine\nintelligence persist.[11](notes.html#iconclnotes11) Over and over, we see the\nideology of Cartesian dualism in AI: the fantasy that AI systems are\ndisembodied brains that absorb and produce knowledge independently from their\ncreators, infrastructures, and the world at large. These illusions distract\nfrom the far more relevant questions: Whom do these systems serve? What are\nthe political economies of their construction? And what are the wider\nplanetary consequences?\n\n### The Pipelines of AI\n\nConsider a different illustration of AI: the blueprint for Google’s first\nowned and operated data center, in The Dalles, Oregon. It depicts three\n68,680-square-foot buildings, an enormous facility that was estimated in 2008\nto use enough energy to power eighty-two thousand homes, or a city the size of\nTacoma, Washington.[12](notes.html#iconclnotes12) The data center now spreads\nalong the shores of the Columbia River, where it draws heavily on some of the\ncheapest electricity in North America. Google’s lobbyists negotiated for six\nmonths with local officials to get a deal that included tax exemptions,\nguarantees of cheap energy, and use of the city-built fiber-optic ring. Unlike\nthe abstract vision of a Go game, the engineering plan reveals how much of\nGoogle’s technical vision depends on public utilities, including gas mains,\nsewer pipes, and the high-voltage lines through which the discount electricity\nwould flow. In the words of the writer Ginger Strand, “Through city\ninfrastructure, state givebacks, and federally subsidized power, YouTube is\nbankrolled by us.”[13](notes.html#iconclnotes13)\n\n![Images](../images/f0216-01.jpg)\n\nBlueprint of Google Data Center. Courtesy of _Harper’s_\n\nThe blueprint reminds us of how much the artificial intelligence industry’s\nexpansion has been publicly subsidized: from defense funding and federal\nresearch agencies to public utilities and tax breaks to the data and unpaid\nlabor taken from all who use search engines or post images online. AI began as\na major public project of the twentieth century and was relentlessly\nprivatized to produce enormous financial gains for the tiny minority at the\ntop of the extraction pyramid.\n\nThese diagrams present two different ways of understanding how AI works. I’ve\nargued that there is much at stake in how we define AI, what its boundaries\nare, and who determines them: it shapes what can be seen and contested. The Go\ndiagram speaks to the industry narratives of an abstract computational cloud,\nfar removed from the earthly resources needed to produce it, a paradigm where\ntechnical innovation is lionized, regulation is rejected, and true costs are\nnever revealed. The blueprint points us to the physical infrastructure, but it\nleaves out the full environmental implications and the political deals that\nmade it possible. These partial accounts of AI represent what philosophers\nMichael Hardt and Antonio Negri call the “dual operation of _abstraction_ and\n_extraction_ ” in information capitalism: abstracting away the material\nconditions of production while extracting more information and\nresources.[14](notes.html#iconclnotes14) The description of AI as\nfundamentally abstract distances it from the energy, labor, and capital needed\nto produce it and the many different kinds of mining that enable it.\n\nThis book has explored the planetary infrastructure of AI as an extractive\nindustry: from its material genesis to the political economy of its operations\nto the discourses that support its aura of immateriality and inevitability. We\nhave seen the politics inherent in how AI systems are trained to recognize the\nworld. And we’ve observed the systemic forms of inequity that make AI what it\nis today. The core issue is the deep entanglement of technology, capital, and\npower, of which AI is the latest manifestation. Rather than being inscrutable\nand alien, these systems are products of larger social and economic structures\nwith profound material consequences.\n\n### The Map Is Not the Territory\n\nHow do we see the full life cycle of artificial intelligence and the dynamics\nof power that drive it? We have to go beyond the conventional maps of AI to\nlocate it in a wider landscape. Atlases can provoke a shift in scale, to see\nhow spaces are joined in relation to one another. This book proposes that the\nreal stakes of AI are the global interconnected systems of extraction and\npower, not the technocratic imaginaries of artificiality, abstraction, and\nautomation. To understand AI for what it is, we need to see the structures of\npower it serves.\n\nAI is born from salt lakes in Bolivia and mines in Congo, constructed from\ncrowdworker-labeled datasets that seek to classify human actions, emotions,\nand identities. It is used to navigate drones over Yemen, direct immigration\npolice in the United States, and modulate credit scores of human value and\nrisk across the world. A wide-angle, multiscalar perspective on AI is needed\nto contend with these overlapping regimes.\n\nThis book began below the ground, where the extractive politics of artificial\nintelligence can be seen at their most literal. Rare earth minerals, water,\ncoal, and oil: the tech sector carves out the earth to fuel its highly energy-\nintensive infrastructures. AI’s carbon footprint is never fully admitted or\naccounted for by the tech sector, which is simultaneously expanding the\nnetworks of data centers while helping the oil and gas industry locate and\nstrip remaining reserves of fossil fuels. The opacity of the larger supply\nchain for computation in general, and AI in particular, is part of a long-\nestablished business model of extracting value from the commons and avoiding\nrestitution for the lasting damage.\n\nLabor represents another form of extraction. In chapter 2, we ventured beyond\nthe highly paid machine learning engineers to consider the other forms of work\nneeded to make artificial intelligence systems function. From the miners\nextracting tin in Indonesia to crowdworkers in India completing tasks on\nAmazon Mechanical Turk to iPhone factory workers at Foxconn in China, the\nlabor force of AI is far greater than we normally imagine. Even within the\ntech companies there is a large shadow workforce of contract laborers, who\nsignificantly outnumber full-time employees but have fewer benefits and no job\nsecurity.[15](notes.html#iconclnotes15)\n\nIn the logistical nodes of the tech sector, we find humans completing the\ntasks that machines cannot. Thousands of people are needed to support the\nillusion of automation: tagging, correcting, evaluating, and editing AI\nsystems to make them appear seamless. Others lift packages, drive for ride-\nhailing apps, and deliver food. AI systems surveil them all while squeezing\nthe most output from the bare functionality of human bodies: the complex\njoints of fingers, eyes, and knee sockets are cheaper and easier to acquire\nthan robots. In those spaces, the future of work looks more like the Taylorist\nfactories of the past, but with wristbands that vibrate when workers make\nerrors and penalties given for taking too many bathroom breaks.\n\nThe uses of workplace AI further skew power imbalances by placing more control\nin employers’ hands. Apps are used to track workers, nudge them to work longer\nhours, and rank them in real time. Amazon provides a canonical example of how\na microphysics of power—disciplining bodies and their movement through\nspace—is connected to a macrophysics of power, a logistics of planetary time\nand information. AI systems exploit differences in time and wages across\nmarkets to speed the circuits of capital. Suddenly, everyone in urban centers\ncan have—and expects—same day delivery. And the system speeds up again, with\nthe material consequences hidden behind the cardboard boxes, delivery trucks,\nand “buy now” buttons.\n\nAt the data layer, we can see a different geography of extraction. “We are\nbuilding a mirror of the real world,” a Google Street View engineer said in\n2012. “Anything that you see in the real world needs to be in our\ndatabases.”[16](notes.html#iconclnotes16) Since then, the harvesting of the\nreal world has only intensified to reach into spaces that were previously hard\nto capture. As we saw in chapter 3, there has been a widespread pillaging of\npublic spaces; the faces of people in the street have been captured to train\nfacial recognition systems; social media feeds have been ingested to build\npredictive models of language; sites where people keep personal photos or have\nonline debates have been scraped in order to train machine vision and natural\nlanguage algorithms. This practice has become so common that few in the AI\nfield even question it. In part, that is because so many careers and market\nvaluations depend on it. The collect-it-all mentality, once the remit of\nintelligence agencies, is not only normalized but moralized—it is seen as\nwasteful not to collect data wherever possible.[17](notes.html#iconclnotes17)\n\nOnce data is extracted and ordered into training sets, it becomes the\nepistemic foundation by which AI systems classify the world. From the\nbenchmark training sets such as ImageNet, MS-Celeb, or NIST’s collections,\nimages are used to represent ideas that are far more relational and contested\nthan the labels may suggest. In chapter 4, we saw how labeling taxonomies\nallocate people into forced gender binaries, simplistic and offensive racial\ngroupings, and highly normative and stereotypical analyses of character,\nmerit, and emotional state. These classifications, unavoidably value-laden,\nforce a way of seeing onto the world while claiming scientific neutrality.\n\nDatasets in AI are never raw materials to feed algorithms: they are inherently\npolitical interventions. The entire practice of harvesting data, categorizing\nand labeling it, and then using it to train systems is a form of politics. It\nhas brought a shift to what are called operational images—representations of\nthe world made solely for machines.[18](notes.html#iconclnotes18) Bias is a\nsymptom of a deeper affliction: a far-ranging and centralizing normative logic\nthat is used to determine how the world should be seen and evaluated.\n\nA central example of this is affect detection, described in chapter 5, which\ndraws on controversial ideas about the relation of faces to emotions and\napplies them with the reductive logic of a lie detector test. The science\nremains deeply contested.[19](notes.html#iconclnotes19) Institutions have\nalways classified people into identity categories, narrowing personhood and\ncutting it down into precisely measured boxes. Machine learning allows that to\nhappen at scale. From the hill towns of Papua New Guinea to military labs in\nMaryland, techniques have been developed to reduce the messiness of feelings,\ninterior states, preferences, and identifications into something quantitative,\ndetectable, and trackable.\n\nWhat epistemological violence is necessary to make the world readable to a\nmachine learning system? AI seeks to systematize the unsystematizable,\nformalize the social, and convert an infinitely complex and changing universe\ninto a Linnaean order of machine-readable tables. Many of AI’s achievements\nhave depended on boiling things down to a terse set of formalisms based on\nproxies: identifying and naming some features while ignoring or obscuring\ncountless others. To adapt a phrase from philosopher Babette Babich, machine\nlearning exploits what it does know to predict what it does not know: a game\nof repeated approximations. Datasets are also _proxies_ —stand-ins for what\nthey claim to measure. Put simply, this is transmuting difference into\ncomputable sameness. This kind of knowledge schema recalls what Friedrich\nNietzsche described as “the falsifying of the multifarious and incalculable\ninto the identical, similar, and calculable.”[20](notes.html#iconclnotes20) AI\nsystems become deterministic when these proxies are taken as ground truth,\nwhen fixed labels are applied to a fluid complexity. We saw this in the cases\nwhere AI is used to predict gender, race, or sexuality from a photograph of a\nface.[21](notes.html#iconclnotes21) These approaches resemble phrenology and\nphysiognomy in their desire to essentialize and impose identities based on\nexternal appearances.\n\nThe problem of ground truth for AI systems is heightened in the context of\nstate power, as we saw in chapter 6. The intelligence agencies led the way on\nthe mass collection of data, where metadata signatures are sufficient for\nlethal drone strikes and a cell phone location becomes a proxy for an unknown\ntarget. Even here, the bloodless language of metadata and surgical strikes is\ndirectly contradicted by the unintended killings from drone\nmissiles.[22](notes.html#iconclnotes22) As Lucy Suchman has asked, how are\n“objects” identified as imminent threats? We know that “ISIS pickup truck” is\na category based on hand-labeled data, but who chose the categories and\nidentified the vehicles?[23](notes.html#iconclnotes23) We saw the\nepistemological confusions and errors of object recognition training sets like\nImageNet; military AI systems and drone attacks are built on the same unstable\nterrain.\n\nThe deep interconnections between the tech sector and the military are now\nframed within a strong nationalist agenda. The rhetoric about the AI war\nbetween the United States and China drives the interests of the largest tech\ncompanies to operate with greater government support and few restrictions.\nMeanwhile, the surveillance armory used by agencies like the NSA and the CIA\nis now deployed domestically at a municipal level in the in-between space of\ncommercial-military contracting by companies like Palantir. Undocumented\nimmigrants are hunted down with logistical systems of total information\ncontrol and capture that were once reserved for extralegal espionage. Welfare\ndecision-making systems are used to track anomalous data patterns in order to\ncut people off from unemployment benefits and accuse them of fraud. License\nplate reader technology is being used by home surveillance systems—a\nwidespread integration of previously separate surveillance\nnetworks.[24](notes.html#iconclnotes24)\n\nThe result is a profound and rapid expansion of surveillance and a blurring\nbetween private contractors, law enforcement, and the tech sector, fueled by\nkickbacks and secret deals. It is a radical redrawing of civic life, where the\ncenters of power are strengthened by tools that see with the logics of\ncapital, policing, and militarization.\n\n### Toward Connected Movements for Justice\n\nIf AI currently serves the existing structures of power, an obvious question\nmight be: Should we not seek to democratize it? Could there not be an AI for\nthe people that is reoriented toward justice and equality rather than\nindustrial extraction and discrimination? This may seem appealing, but as we\nhave seen throughout this book, the infrastructures and forms of power that\nenable and are enabled by AI skew strongly toward the centralization of\ncontrol. To suggest that we democratize AI to reduce asymmetries of power is a\nlittle like arguing for democratizing weapons manufacturing in the service of\npeace. As Audre Lorde reminds us, the master’s tools will never dismantle the\nmaster’s house.[25](notes.html#iconclnotes25)\n\nA reckoning is due for the technology sector. To date, one common industry\nresponse has been to sign AI ethics principles. As European Union\nparliamentarian Marietje Schaake observed, in 2019 there were 128 frameworks\nfor AI ethics in Europe alone.[26](notes.html#iconclnotes26) These documents\nare often presented as products of a “wider consensus” on AI ethics. But they\nare overwhelmingly produced by economically developed countries, with little\nrepresentation from Africa, South and Central America, or Central Asia. The\nvoices of the people most harmed by AI systems are largely missing from the\nprocesses that produce them.[27](notes.html#iconclnotes27) Further, ethical\nprinciples and statements don’t discuss how they should be implemented, and\nthey are rarely enforceable or accountable to a broader public. As Shannon\nMattern has noted, the focus is more commonly on the ethical ends for AI,\nwithout assessing the ethical means of its\napplication.[28](notes.html#iconclnotes28) Unlike medicine or law, AI has no\nformal professional governance structure or norms—no agreed-upon definitions\nand goals for the field or standard protocols for enforcing ethical\npractice.[29](notes.html#iconclnotes29)\n\nSelf-regulating ethical frameworks allow companies to choose how to deploy\ntechnologies and, by extension, to decide what ethical AI means for the rest\nof the world.[30](notes.html#iconclnotes30) Tech companies rarely suffer\nserious financial penalties when their AI systems violate the law and even\nfewer consequences when their ethical principles are violated. Further, public\ncompanies are pressured by shareholders to maximize return on investment over\nethical concerns, commonly making ethics secondary to profits. As a result,\nethics is necessary but not sufficient to address the fundamental concerns\nraised in this book.\n\nTo understand what is at stake, we must focus less on ethics and more on\npower. AI is invariably designed to amplify and reproduce the forms of power\nit has been deployed to optimize. Countering that requires centering the\ninterests of the communities most affected.[31](notes.html#iconclnotes31)\nInstead of glorifying company founders, venture capitalists, and technical\nvisionaries, we should begin with the lived experiences of those who are\ndisempowered, discriminated against, and harmed by AI systems. When someone\nsays, “AI ethics,” we should assess the labor conditions for miners,\ncontractors, and crowdworkers. When we hear “optimization,” we should ask if\nthese are tools for the inhumane treatment of immigrants. When there is\napplause for “large-scale automation,” we should remember the resulting carbon\nfootprint at a time when the planet is already under extreme stress. What\nwould it mean to work toward justice across all these systems?\n\nIn 1986, the political theorist Langdon Winner described a society “committed\nto making artificial realities” with no concern for the harms it could bring\nto the conditions of life: “Vast transformations in the structure of our\ncommon world have been undertaken with little attention to what those\nalterations mean. . . . In the technical realm we repeatedly enter into a\nseries of social contracts, the terms of which are only revealed after\nsigning.”[32](notes.html#iconclnotes32)\n\nIn the four decades since, those transformations are now at a scale that has\nshifted the chemical composition of the atmosphere, the temperature of Earth’s\nsurface, and the contents of the planet’s crust. The gap between how\ntechnology is judged on its release and its lasting consequences has only\nwidened. The social contract, to the extent that there ever was one, has\nbrought a climate crisis, soaring wealth inequality, racial discrimination,\nand widespread surveillance and labor exploitation. But the idea that these\ntransformations occurred in ignorance of their possible results is part of the\nproblem. The philosopher Achille Mbembé sharply critiques the idea that we\ncould not have foreseen what would become of the knowledge systems of the\ntwenty-first century, as they were always “operations of abstraction that\nclaim to rationalize the world on the basis of corporate\nlogic.”[33](notes.html#iconclnotes33) He writes: “It is about extraction,\ncapture, the cult of data, the commodification of human capacity for thought\nand the dismissal of critical reason in favour of programming. . . . Now more\nthan ever before, what we need is a new critique of technology, of the\nexperience of technical life.”[34](notes.html#iconclnotes34)\n\nThe next era of critique will also need to find spaces beyond technical life\nby overturning the dogma of inevitability. When AI’s rapid expansion is seen\nas unstoppable, it is possible only to patch together legal and technical\nrestraints on systems after the fact: to clean up datasets, strengthen privacy\nlaws, or create ethics boards. But these will always be partial and incomplete\nresponses in which technology is assumed and everything else must adapt. But\nwhat happens if we reverse this polarity and begin with the commitment to a\nmore just and sustainable world? How can we intervene to address\ninterdependent issues of social, economic, and climate injustice? Where does\ntechnology serve that vision? And are there places where AI should not be\nused, where it undermines justice?\n\nThis is the basis for a renewed politics of refusal—opposing the narratives of\ntechnological inevitability that says, “If it can be done, it will be.” Rather\nthan asking where AI will be applied, merely because it can, the emphasis\nshould be on _why_ it ought to be applied. By asking, “Why use artificial\nintelligence?” we can question the idea that everything should be subject to\nthe logics of statistical prediction and profit accumulation, what Donna\nHaraway terms the “informatics of domination.”[35](notes.html#iconclnotes35)\nWe see glimpses of this refusal when populations choose to dismantle\npredictive policing, ban facial recognition, or protest algorithmic grading.\nSo far these minor victories have been piecemeal and localized, often centered\nin cities with more resources to organize, such as London, San Francisco, Hong\nKong, and Portland, Oregon. But they point to the need for broader national\nand international movements that refuse technology-first approaches and focus\non addressing underlying inequities and injustices. Refusal requires rejecting\nthe idea that the same tools that serve capital, militaries, and police are\nalso fit to transform schools, hospitals, cities, and ecologies, as though\nthey were value neutral calculators that can be applied everywhere.\n\nThe calls for labor, climate, and data justice are at their most powerful when\nthey are united. Above all, I see the greatest hope in the growing justice\nmovements that address the interrelatedness of capitalism, computation, and\ncontrol: bringing together issues of climate justice, labor rights, racial\njustice, data protection, and the overreach of police and military power. By\nrejecting systems that further inequity and violence, we challenge the\nstructures of power that AI currently reinforces and create the foundations\nfor a different society.[36](notes.html#iconclnotes36) As Ruha Benjamin notes,\n“Derrick Bell said it like this: ‘To see things as they really are, you must\nimagine them for what they might be.’ We are pattern makers and we must change\nthe content of our existing patterns.”[37](notes.html#iconclnotes37) To do so\nwill require shaking off the enchantments of tech solutionism and embracing\nalternative solidarities—what Mbembé calls “a different politics of inhabiting\nthe Earth, of repairing and sharing the planet.”[38](notes.html#iconclnotes38)\nThere are sustainable collective politics beyond value extraction; there are\ncommons worth keeping, worlds beyond the market, and ways to live beyond\ndiscrimination and brutal modes of optimization. Our task is to chart a course\nthere.\n\n\n![Images](../images/f0228-01.jpg)\n\n\n## Coda  \nSpace\n\nA countdown begins. File footage starts rolling. Engines at the base of a\ntowering Saturn V ignite, and the rocket begins liftoff. We hear the voice of\nJeff Bezos: “Ever since I was five years old—that’s when Neil Armstrong\nstepped onto the surface of the moon—I’ve been passionate about space,\nrockets, rocket engines, space travel.” A parade of inspirational images\nappears: mountain climbers at summits, explorers descending into canyons, an\nocean diver swimming through a shoal of fish.\n\nCut to Bezos in a control room during a launch, adjusting his headset. His\nvoiceover continues: “This is the most important work I’m doing. It’s a simple\nargument, this is the best planet. And so we face a choice. As we move\nforward, we’re gonna have to decide whether we want a civilization of\nstasis—we will have to cap population, we will have to cap energy usage per\ncapita—or we can fix that problem, by moving out into\nspace.”[1](notes.html#icodenotes1)\n\nThe soundtrack soars, and images of deep space are counterposed with shots of\nthe busy freeways of Los Angeles and clogged cloverleaf junctions. “Von Braun\nsaid, after the lunar landing, ‘I have learned to use the word impossible with\ngreat caution.’ And I hope you guys take that attitude about your\nlives.”[2](notes.html#icodenotes2)\n\nThis scene comes from a promotional video for Bezos’s private aerospace\ncompany, Blue Origin. The company motto is _Gradatim Ferociter_ , Latin for\n“Step by Step, Ferociously.” In the near term, Blue Origin is building\nreusable rockets and lunar landers, testing them primarily at its facility and\nsuborbital base in West Texas. By 2024, the company wants to be shuttling\nastronauts and cargo to the Moon.[3](notes.html#icodenotes3) But in the longer\nterm, the company’s mission is far more ambitious: to help bring about a\nfuture in which millions are living and working in space. Specifically, Bezos\nhas outlined his hopes to build giant space colonies, where people would live\nin floating manufactured environments.[4](notes.html#icodenotes4) Heavy\nindustry would move off-planet altogether, the new frontier for extraction.\nMeanwhile, Earth would be zoned for residential building and light industry,\nleft as a “beautiful place to live, a beautiful place to visit”—presumably for\nthose who can afford to be there, rather than working in the off-world\ncolonies.[5](notes.html#icodenotes5)\n\nBezos possesses extraordinary and growing industrial power. Amazon continues\nto capture more of U.S. online commerce, Amazon Web Services represents nearly\nhalf of the cloud-computing industry, and, by some estimates, Amazon’s site\nhas more product searches than Google.[6](notes.html#icodenotes6) Despite all\nthis, Bezos is worried. His fear is that the planet’s growing energy demands\nwill soon outstrip its limited supply. For him, the greatest concern “is not\nnecessarily extinction” but _stasis:_ “We will have to stop growing, which I\nthink is a very bad future.”[7](notes.html#icodenotes7)\n\nBezos is not alone. He is just one of several tech billionaires focused on\nspace. Planetary Resources, led by the founder of the X Prize, Peter\nDiamandis, and backed with investment from Google’s Larry Page and Eric\nSchmidt, aimed to create the first commercial mine in space by drilling\nasteroids.[8](notes.html#icodenotes8) Elon Musk, chief executive of Tesla and\nSpaceX, has announced his intention to colonize Mars within a hundred\nyears—while admitting that, to do so, the first astronauts must “be prepared\nto die.”[9](notes.html#icodenotes9) Musk has also advocated terraforming the\nsurface of Mars for human settlement by exploding nuclear weapons at the\npoles.[10](notes.html#icodenotes10) SpaceX made a T-shirt that reads “nuke\nmars.” Musk also conducted what is arguably the most expensive public\nrelations exercise in history when he launched a Tesla car into heliocentric\norbit on a SpaceX Falcon Heavy rocket. Researchers estimate that the car will\nremain in space for millions of years, until it finally crashes back to\nEarth.[11](notes.html#icodenotes11)\n\nThe ideology of these space spectacles is deeply interconnected with that of\nthe AI industry. Extreme wealth and power generated from technology companies\nnow enables a small group of men to pursue their own private space race. They\ndepend on exploiting the knowledge and infrastructures of the public space\nprograms of the twentieth century and often rely on government funding and tax\nincentives as well.[12](notes.html#icodenotes12) Their aim is not to limit\nextraction and growth but to extend it across the solar system. In actuality,\nthese efforts are as much about an _imaginary_ of space, endless growth, and\nimmortality than they are about the uncertain and unpleasant possibilities of\nactual space colonization.\n\nBezos’s inspiration for conquering space comes, in part, from the physicist\nand science fiction novelist Gerard K. O’Neill. O’Neill wrote _The High\nFrontier: Human Colonies in Space_ , a 1976 fantasy of space colonization,\nwhich includes lush illustrations of moon mining with Rockwellian\nabundance.[13](notes.html#icodenotes13) Bezos’s plan for Blue Origin is\ninspired by this bucolic vision of permanent human settlement, for which no\ncurrent technology exists.[14](notes.html#icodenotes14) O’Neill was driven by\nthe “dismay and shock” he felt when he read the 1972 landmark report by the\nClub of Rome, called _The Limits to Growth._[ 15](notes.html#icodenotes15) The\nreport published extensive data and predictive models about the end of\nnonrenewable resources and the impact on population growth, sustainability,\nand humanity’s future on Earth.[16](notes.html#icodenotes16) As the\narchitecture and planning scholar Fred Scharmen summarizes:\n\nThe Club of Rome models calculate outcomes from different sets of initial\nassumptions. The baseline scenarios, extrapolated from then-current trends,\nshow resource and population collapse before the year 2100. When the models\nassume double the known resource reserves, they collapse again, to a slightly\nhigher level but still before 2100. When they assume that technology will make\navailable “unlimited” resources, population collapses _even more sharply_ than\nbefore due to spikes in pollution. With pollution controls added to the model,\npopulation collapses after running out of food. In models that increase\nagricultural capacity, pollution overruns previous controls and both food and\npopulation collapse.[17](notes.html#icodenotes17)\n\n_Limits to Growth_ suggested that moving to sustainable management and reuse\nof resources was the answer to long-term stability of global society and that\nnarrowing the gap between rich and poor nations was the key to survival. Where\n_Limits to Growth_ fell short was that it did not foresee the larger set of\ninterconnected systems that now make up the global economy and how previously\nuneconomic forms of mining would be incentivized, driving greater\nenvironmental harms, land and water degradation, and accelerated resource\ndepletion.\n\nIn writing _The High Frontier_ , O’Neill wanted to imagine a different way out\nof the no-growth model rather than limiting production and\nconsumption.[18](notes.html#icodenotes18) By positing that space was a\nsolution, O’Neill redirected global anxiety in the 1970s over gasoline\nshortages and oil crises with visions of serene stable space structures that\nwould simultaneously preserve the status quo and offer new opportunities. “If\nEarth doesn’t have enough surface area,” O’Neill urged, “then humans should\nsimply build more.”[19](notes.html#icodenotes19) The science of how it would\nwork and the economics of how we could afford it were details left for another\nday; the dream was all that mattered.[20](notes.html#icodenotes20)\n\nThat space colonization and frontier mining have become the common corporate\nfantasies of tech billionaires underscores a fundamentally troubling\nrelationship to Earth. Their vision of the future does not include minimizing\noil and gas exploration or containing resource consumption or even reducing\nthe exploitative labor practices that have enriched them. Instead, the\nlanguage of the tech elite often echoes settler colonialism, seeking to\ndisplace Earth’s population and capture territory for mineral extraction.\nSilicon Valley’s billionaire space race similarly assumes that the last\ncommons—outer space—can be taken by whichever empire gets there first. This is\ndespite the main convention governing space mining, the 1967 Outer Space\nTreaty, which recognizes that space is the “common interest of all mankind”\nand that any exploration or use “should be carried on for the benefit of all\npeoples.”[21](notes.html#icodenotes21)\n\nIn 2015, Bezos’s Blue Origin and Musk’s SpaceX lobbied Congress and the Obama\nadministration to enact the Commercial Space Launch Competitiveness\nAct.[22](notes.html#icodenotes22) It extends an exemption for commercial space\ncompanies from federal regulation until 2023, allowing them to own any mining\nresources extracted from asteroids and keep the\nprofits.[23](notes.html#icodenotes23) This legislation directly undercuts the\nidea of space as a commons, and creates a commercial incentive to “go forth\nand conquer.”[24](notes.html#icodenotes24)\n\nSpace has become the ultimate imperial ambition, symbolizing an escape from\nthe limits of Earth, bodies, and regulation. It is perhaps no surprise that\nmany of the Silicon Valley tech elite are invested in the vision of abandoning\nthe planet. Space colonization fits well alongside the other fantasies of\nlife-extension dieting, blood transfusions from teenagers, brain-uploading to\nthe cloud, and vitamins for immortality.[25](notes.html#icodenotes25) Blue\nOrigin’s high-gloss advertising is part of this dark utopianism. It is a\nwhispered summons to become the Übermensch, to exceed all boundaries:\nbiological, social, ethical, and ecological. But underneath, these visions of\nbrave new worlds seem driven most of all by fear: fear of death—individually\nand collectively—and fear that time is truly running out.\n\nI’m back in the van for the last leg of my journey. I drive south out of\nAlbuquerque, New Mexico, headed toward the Texas border. On my way, I take a\ndetour past the rocky face of San Augustin Peak and follow the steep drive\ndown to the White Sands Missile Range, where in 1946 the United States\nlaunched the first rocket containing a camera into space. That mission was led\nby Wernher von Braun, who had been the technical director of Germany’s missile\nrocket development program. He defected to the United States after the war,\nand there he began experimenting with confiscated V-2 rockets—the very\nmissiles he had helped design, which had been fired against the Allies across\nEurope. But this time he sent them directly upward, into space. The rocket\nascended to an altitude of 65 miles, capturing images every 1.5 seconds,\nbefore crashing into the New Mexican desert. The film survived inside of a\nsteel cassette, revealing a grainy but distinctly Earthlike\ncurve.[26](notes.html#icodenotes26)\n\n![Images](../images/f0235-01.jpg)\n\nView of Earth from a camera on V-2 #13, launched October 24, 1946. Courtesy\nWhite Sands Missile Range/Applied Physics Laboratory\n\nThat Bezos chose to quote von Braun in his Blue Origin commercial is notable.\nVon Braun was chief rocket engineer of the Third Reich and admitted using\nconcentration camp slave labor to build his V-2 rockets; some consider him a\nwar criminal.[27](notes.html#icodenotes27) More people died in the camps\nbuilding the rockets than were killed by them in\nwar.[28](notes.html#icodenotes28) But it is von Braun’s work as head of NASA’s\nMarshall Space Flight Center, where he was instrumental in the design of the\nSaturn V rocket, that is best known.[29](notes.html#icodenotes29) Bathed in\nthe glow of Apollo 11, washed clean of history, Bezos has found his hero—a man\nwho refused to believe in impossibility.\n\nAfter driving through El Paso, Texas, I take Route 62 toward the Salt Basin\nDunes. It’s late in the afternoon, and colors are starting to bloom in the\ncumulus clouds. There’s a T-junction, and after turning right, the road begins\nto trace along the Sierra Diablos. This is Bezos country. The first indication\nis a large ranch house set back from the road, with a sign in red letters that\nreads “Figure 2” on a white gate. It’s the ranch that Bezos purchased in 2004,\njust part of the three hundred thousand acres he owns in the\narea.[30](notes.html#icodenotes30) The land has a violent colonial history:\none of the final battles between the Texas Rangers and the Apaches occurred\njust west of this site in 1881, and nine years later the ranch was created by\nthe one-time Confederate rider and cattleman James Monroe\nDaugherty.[31](notes.html#icodenotes31)\n\n![Images](../images/f0236-01.jpg)\n\nBlue Origin suborbital launch facility, West Texas. Photograph by Kate\nCrawford\n\nNearby is the turn-off to the Blue Origin suborbital launch facility. The\nprivate road is blocked by a bright blue gate with security notices warning of\nvideo surveillance and a guard station bristling with cameras. I stay on the\nhighway and pull the van over to the side of the road a few minutes away. From\nhere, the views stretch across the valley to the Blue Origin landing site,\nwhere the rockets are being tested for what is expected to be the company’s\nfirst human mission into space. Cars pass through the boom gates as the\nworkers clock out for the day.\n\nLooking back at the clusters of sheds that mark out the rocket base, it feels\nvery provisional and makeshift in this dry expanse of the Permian Basin. The\nvast span of the valley is broken with a hollow circle, the landing pad where\nBlue Origin’s reusable rockets are meant to touch down on a feather logo\npainted in the center. That’s all there is to see. It’s a private\ninfrastructure-in-progress, guarded and gated, a technoscientific imaginary of\npower, extraction, and escape, driven by the wealthiest man on the planet. It\nis a hedge against Earth.\n\nThe light is fading now, and steel-gray clouds are moving against the sky. The\ndesert looks silvery, dotted with white sage bushes and clusters of volcanic\ntuff punctuating what was once the floor of a great inland sea. After taking a\nphotograph, I head back to the van to begin the final drive of the day to the\ntown of Marfa. It’s not until I start driving away that I realize I’m being\nfollowed. Two matching black Chevrolet pickups begin aggressively tailgating\nat close range. I pull over in the hope they will pass. They also pull over.\nNo one moves. After waiting a few minutes, I slowly begin to drive again. They\nmaintain their sinister escort all the way to the edge of the darkening\nvalley.\n\n![Images](../images/f0238-01.jpg)\n\n_The World without Water_ (_Den Aardkloot van water ontbloot_), Thomas\nBurnet’s 1694 map of the world drained of its oceans.\n\n\n## Acknowledgments\n\nAll books are collective projects, and the longer they take to write, the\nbigger the collective. _Atlas of AI_ was many years in the making, and was\nmade possible thanks to the friends, colleagues, collaborators, and\ncoadventurers who came along with me. There were many late-night conversations\nand early-morning coffees, as well as road trips and roundtables, all of which\nhave brought this book to life. I have enough gratitude to merit a separate\nvolume, but these few words will have to suffice for now.\n\nFirst, to the scholars and friends whose work left the deepest imprints on\nthis book: Mike Ananny, Geoffrey Bowker, Benjamin Bratton, Simone Browne,\nWendy Chun, Vladan Joler, Alondra Nelson, Jonathan Sterne, Lucy Suchman, Fred\nTurner, and McKenzie Wark. To Jer Thorp, thank you for the days we wrote side\nby side, and for the commiserations and celebrations (depending on the week).\n\nI’ve been fortunate over the years to be a member of multiple research\ncommunities who have taught me so much. There are many scholars and engineers\nwho make Microsoft Research an exceptional place, and I’m grateful to be a\nmember of both the FATE group and the Social Media Collective. Thanks to\nIfeoma Ajunwa, Peter Bailey, Solon Barocas, Nancy Baym, Christian Borgs,\nMargarita Boyarskaya, danah boyd, Sarah Brayne, Jed Brubaker, Bill Buxton,\nJennifer Chayes, Tressie McMillan Cottom, Hal Daume, Jade Davis, Fernando\nDiaz, Kevin Driscoll, Miro Dudik, Susan Dumais, Megan Finn, Timnit Gebru,\nTarleton Gillespie, Mary L. Gray, Dan Greene, Caroline Jack, Adam Kalai, Tero\nKarppi, Os Keyes, Airi Lampinen, Jessa Lingel, Sonia Livingstone, Michael\nMadaio, Alice Marwick, J. Nathan Matias, Josh McVeigh-Schultz, Andrés Monroy-\nHernández, Dylan Mulvin, Laura Norén, Alexandra Olteanu, Aaron Plasek, Nick\nSeaver, Aaron Shapiro, Luke Stark, Lana Swartz, TL Taylor, Jenn Wortman\nVaughan, Hanna Wallach, and Glen Weyl. I could not ask to learn from a more\nluminous constellation of scholars.\n\nParticular thanks to everyone who has been a part of creating the AI Now\nInstitute at NYU: Alejandro Calcaño Bertorelli, Alex Butzbach, Roel Dobbe,\nTheodora Dryer, Genevieve Fried, Casey Gollan, Ben Green, Joan Greenbaum, Amba\nKak, Elizabeth Kaziunas, Varoon Mathur, Erin McElroy, Andrea Nill Sánchez,\nMariah Peebles, Deb Raji, Joy Lisi Rankin, Noopur Raval, Dillon Reisman,\nRashida Richardson, Julia Bloch Thibaud, Nantina Vgontzas, Sarah Myers West,\nand Meredith Whittaker.\n\nAnd I’m always grateful to the extraordinary Australian scholars who grounded\nme from the beginning, including Kath Albury, Mark Andrejevic, Genevieve Bell,\nJean Burgess, Chris Chesher, Anne Dunn, Gerard Goggin, Melissa Gregg, Larissa\nHjorth, Catharine Lumby, Elspeth Probyn, Jo Tacchi, and Graeme Turner. The\nroad is long, but it always leads back home.\n\nThis book greatly benefited from several research assistants, readers, and\narchivists over the years, all of whom are amazing scholars in their own\nright. Thanks to Sally Collings, Sarah Hamid, Rebecca Hoffman, Caren\nLitherland, Kate Miltner, Léa Saint-Raymond, and Kiran Samuel for helping me\nthink harder, track down sources, access archives, and complete endnotes.\nParticular thanks to Alex Campolo for his depth of expertise on the history of\nscience in the twentieth century—it is a joy to work with you. Elmo Keep was a\nbrilliant interlocutor, and Joy Lisi Rankin was an insightful editor. Several\narchivists generously helped this project, but particularly Janet Monge at the\nSamuel Morton skull archive and Henrik Moltke with the Snowden archive.\n\nTo Joseph Calamia, I owe you so much. Thank you for believing in this project\nand for your patience while I completed the many journeys it required. Thanks\nalso to Bill Frucht and Karen Olson at Yale University Press for bringing it\nover the line.\n\nI’m deeply indebted to the institutions that invited me to visit and gave me\ntime to write. Thanks to the École Normale Supérieure in Paris, where I was\nthe inaugural chair in AI and Justice, to the Robert Bosch Academy in Berlin,\nwhere I was a Richard von Weizsäcker Fellow, and to the University of\nMelbourne for the Miengunyah Distinguished Visiting Fellowship. The\ncommunities at each one of these institutions have been so welcoming and\nexpanded the contexts of this atlas. For making all that possible, thanks to\nAnne Bouverot, Tanya Perelmuter, Mark Mezard, Fondation Abeona, Sandra Breka,\nJannik Rust, and Jeannie Paterson.\n\nI developed the ideas in this book in conference presentations, exhibitions,\nand lectures over a decade, across the fields of architecture, art, critical\ngeography, computer science, cultural studies, law, media studies, philosophy,\nand science and technology studies. Audiences at the Australian National\nUniversity, California Institute of Technology, Columbia University, Haus der\nKulturen der Welt, MIT, National Academy of Science, New York University,\nRoyal Society of London, Smithsonian Museum, University of New South Wales,\nYale University, École Normale Supérieure, and at conferences like NeurIPS,\nAoIR, and ICML gave vital feedback as I developed this project.\n\nSome material in various chapters has been drawn from previously published\njournal articles and substantially altered for this context, and I’d like to\nacknowledge all the coauthors and journals with whom it’s been my honor to\ncollaborate:\n\n“Enchanted Determinism: Power without Responsibility in Artificial\nIntelligence,” _Engaging Science, Technology, and Society_ 6 (2020): 1–19\n(with Alex Campolo); “‘Excavating AI: The Politics of Images in Machine\nLearning Training Sets,” _AI and Society_ 2020 (with Trevor Paglen); “Alexa,\nTell Me about Your Mother: The History of the Secretary and the End of\nSecrecy,” _Catalyst: Feminism, Theory, Technoscience_ 6, no. 1 (2020) (with\nJessa Lingel); “AI Systems as State Actors,” _Columbia Law Review_ 119 (2019):\n1941–72 (with Jason Schultz); “Halt the Use of Facial-Recognition Technology\nuntil It Is Regulated,” _Nature_ 572 (2019): 565; “Dirty Data, Bad\nPredictions: How Civil Rights Violations Impact Police Data, Predictive\nPolicing Systems, and Justice,” _NYU Law Review Online_ 94, no. 15 (2019):\n15–55 (with Rashida Richardson and Jason Schultz); “Anatomy of an AI System:\nThe Amazon Echo as an Anatomical Map of Human Labor, Data and Planetary\nResources,” _AI Now Institute_ and _Share Lab_ , September 7, 2018 (with\nVladan Joler); “Datasheets for Datasets,” Proceedings of the Fifth Workshop on\nFairness, Accountability, and Transparency in Machine Learning, Stockholm,\n2018 (with Timnit Gebru, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman\nVaughan, Hanna Wallach, and Hal Daumeé III); “The Problem with Bias:\nAllocative Versus Representational Harms in Machine Learning,” SIGCIS\nConference 2017 (with Solon Barocas, Aaron Shapiro, and Hanna Wallach);\n“Limitless Worker Surveillance,” _California Law Review_ 105, no. 3 (2017):\n735–76 (with Ifeoma Ajunwa and Jason Schultz); “Can an Algorithm Be Agonistic?\nTen Scenes from Life in Calculated Publics,” _Science, Technology and Human\nValues_ 41 (2016): 77–92; “Asking the Oracle,” in _Astro Noise_ , ed. Laura\nPoitras (New Haven: Yale University Press, 2016), 128–41; “Seeing without\nKnowing: Limitations of the Transparency Ideal and Its Application to\nAlgorithmic Accountability,” _New Media and Society_ 20, no. 3 (2018): 973–89\n(with Mike Ananny); “Where Are the Human Subjects in Big Data Research? The\nEmerging Ethics Divide,” _Big Data and Society_ 3, no. 1 (2016) (with Jake\nMetcalf); “Exploring or Exploiting? Social and Ethical Implications of\nAutonomous Experimentation in AI,” Workshop on Fairness, Accountability, and\nTransparency in Machine Learning (FAccT), 2016 (with Sarah Bird, Solon\nBarocas, Fernando Diaz, and Hanna Wallach); “There Is a Blind Spot in AI\nResearch,” _Nature_ 538 (2016): 311–13 (with Ryan Calo); “Circuits of Labour:\nA Labour Theory of the iPhone Era,” _TripleC: Communication, Capitalism and\nCritique_ , 2014 (with Jack Qiu and Melissa Gregg); “Big Data and Due Process:\nToward a Framework to Redress Predictive Privacy Harms,” _Boston College Law\nReview_ 55, no. 1 (2014) (with Jason Schultz); and “Critiquing Big Data:\nPolitics, Ethics, Epistemology,” _International Journal of Communications_ 8\n(2014): 663–72 (with Kate Miltner and Mary Gray).\n\nBeyond articles, I’ve been fortunate to participate on collaborative reports\nwith the team at the AI Now Institute, which have informed this book: _AI Now\n2019 Report_ , AI Now Institute, 2019 (with Roel Dobbe, Theodora Dryer,\nGenevieve Fried, Ben Green, Amba Kak, Elizabeth Kaziunas, Varoon Mathur, Erin\nMcElroy, Andrea Nill Sánchez, Deborah Raji, Joy Lisi Rankin, Rashida\nRichardson, Jason Schultz, Sarah Myers West, and Meredith Whittaker);\n“Discriminating Systems: Gender, Race and Power in AI,” AI Now Institute, 2019\n(with Sarah Myers West and Meredith Whittaker); _AI Now Report 2018_ , AI Now\nInstitute, 2018 (with Meredith Whittaker, Roel Dobbe, Genevieve Fried,\nElizabeth Kaziunas, Varoon Mathur, Sarah Myers West, Rashida Richardson, Jason\nSchultz, and Oscar Schwartz); “Algorithmic Impact Assessments: A Practical\nFramework for Public Agency Accountability,” AI Now Institute, 2018 (with\nDillon Reisman, Jason Schultz, and Meredith Whittaker); _AI Now 2017 Report_ ,\nAI Now Institute, 2017 (with Alex Campolo, Madelyn Sanfilippo, and Meredith\nWhittaker); and _AI Now 2016 Report_ , NYU Information Law Institute, 2016\n(with Madeleine Clare Elish, Solon Barocas, Aaron Plasek, Kadija Ferryman, and\nMeredith Whittaker).\n\nFinally, this book would not exist without these people: Trevor Paglen, a true\ncompass, from desert explorations to archaeological investigations; Vladan\nJoler, a friend in mapmaking, whose designs illuminate this book and my\nthinking; Laura Poitras, who gave me the courage; Karen Murphy, for her\ndesigner’s eye; Adrian Hobbes and Edwina Throsby, for getting me through the\nfires; Bo Daley, who made everything better; and to my family, Margaret,\nJames, Judith, Claudia, Cliff, and Hilary. Eternal thanks are due to Jason and\nElliott, my favorite cartographers.\n\n\n## Notes\n\n### Introduction\n\n[1](intro.html#intronotes1). Heyn, “Berlin’s Wonderful Horse.”\n\n[2](intro.html#intronotes2). Pfungst, _Clever Hans._\n\n[3](intro.html#intronotes3). “‘Clever Hans’ Again.”\n\n[4](intro.html#intronotes4). Pfungst, _Clever Hans._\n\n[5](intro.html#intronotes5). Pfungst.\n\n[6](intro.html#intronotes6). Lapuschkin et al., “Unmasking Clever Hans\nPredictors.”\n\n[7](intro.html#intronotes7). See the work of philosopher Val Plumwood on the\ndualisms of intelligence-stupid, emotional-rational, and master-slave.\nPlumwood, “Politics of Reason.”\n\n[8](intro.html#intronotes8). Turing, “Computing Machinery and Intelligence.”\n\n[9](intro.html#intronotes9). Von Neumann, _The Computer and the Brain_ , 44.\nThis approach was deeply critiqued by Dreyfus, _What Computers Can’t Do._\n\n[10](intro.html#intronotes10). See Weizenbaum, “On the Impact of the Computer\non Society,” 612. After his death, Minsky was implicated in serious\nallegations related to convicted pedophile and rapist Jeffrey Epstein. Minsky\nwas one of several scientists who met with Epstein and visited his island\nretreat where underage girls were forced to have sex with members of Epstein’s\ncoterie. As scholar Meredith Broussard observes, this was part of a broader\nculture of exclusion that became endemic in AI: “As wonderfully creative as\nMinsky and his cohort were, they also solidified the culture of tech as a\nbillionaire boys’ club. Math, physics, and the other ‘hard’ sciences have\nnever been hospitable to women and people of color; tech followed this lead.”\nSee Broussard, _Artificial Unintelligence_ , 174.\n\n[11](intro.html#intronotes11). Weizenbaum, _Computer Power and Human Reason_ ,\n202–3.\n\n[12](intro.html#intronotes12). Greenberger, _Management and the Computer of\nthe Future_ , 315.\n\n[13](intro.html#intronotes13). Dreyfus, _Alchemy and Artificial Intelligence._\n\n[14](intro.html#intronotes14). Dreyfus, _What Computers Can’t Do._\n\n[15](intro.html#intronotes15). Ullman, _Life in Code_ , 136–37.\n\n[16](intro.html#intronotes16). See, as one of many examples, Poggio et al.,\n“Why and When Can Deep—but Not Shallow—Networks Avoid the Curse of\nDimensionality.”\n\n[17](intro.html#intronotes17). Quoted in Gill, _Artificial Intelligence for\nSociety_ , 3.\n\n[18](intro.html#intronotes18). Russell and Norvig, _Artificial Intelligence_ ,\n30.\n\n[19](intro.html#intronotes19). Daston, “Cloud Physiognomy.”\n\n[20](intro.html#intronotes20). Didi-Huberman, _Atlas_ , 5.\n\n[21](intro.html#intronotes21). Didi-Huberman, 11.\n\n[22](intro.html#intronotes22). Franklin and Swenarchuk, _Ursula Franklin\nReader_ , Prelude.\n\n[23](intro.html#intronotes23). For an account of the practices of data\ncolonization, see “Colonized by Data”; and Mbembé, _Critique of Black Reason._\n\n[24](intro.html#intronotes24). Fei-Fei Li quoted in Gershgorn, “Data That\nTransformed AI Research.”\n\n[25](intro.html#intronotes25). Russell and Norvig, _Artificial Intelligence_ ,\n1.\n\n[26](intro.html#intronotes26). Bledsoe quoted in McCorduck, _Machines Who\nThink_ , 136.\n\n[27](intro.html#intronotes27). Mattern, _Code and Clay, Data and Dirt_ ,\nxxxiv–xxxv.\n\n[28](intro.html#intronotes28). Ananny and Crawford, “Seeing without Knowing.”\n\n[29](intro.html#intronotes29). Any list will always be an inadequate account\nof all the people and communities who have inspired and informed this work.\nI’m particularly grateful to these research communities: FATE (Fairness,\nAccountability, Transparency and Ethics) and the Social Media Collective at\nMicrosoft Research, the AI Now Institute at NYU, the Foundations of AI working\ngroup at the École Normale Supérieure, and the Richard von Weizsäcker Visiting\nFellows at the Robert Bosch Academy in Berlin.\n\n[30](intro.html#intronotes30). Saville, “Towards Humble Geographies.”\n\n[31](intro.html#intronotes31). For more on crowdworkers, see Gray and Suri,\n_Ghost Work;_ and Roberts, _Behind the Screen._\n\n[32](intro.html#intronotes32). Canales, _Tenth of a Second._\n\n[33](intro.html#intronotes33). Zuboff, _Age of Surveillance Capitalism._\n\n[34](intro.html#intronotes34). Cetina, _Epistemic Cultures_ , 3.\n\n[35](intro.html#intronotes35). “Emotion Detection and Recognition (EDR) Market\nSize.”\n\n[36](intro.html#intronotes36). Nelson, Tu, and Hines, “Introduction,” 5.\n\n[37](intro.html#intronotes37). Danowski and de Castro, _Ends of the World._\n\n[38](intro.html#intronotes38). Franklin, _Real World of Technology_ , 5.\n\n### 1  \nEarth\n\n[1](ch01.html#ch01notes1). Brechin, _Imperial San Francisco._\n\n[2](ch01.html#ch01notes2). Brechin, 29.\n\n[3](ch01.html#ch01notes3). Agricola quoted in Brechin, 25.\n\n[4](ch01.html#ch01notes4). Quoted in Brechin, 50.\n\n[5](ch01.html#ch01notes5). Brechin, 69.\n\n[6](ch01.html#ch01notes6). See, e.g., Davies and Young, _Tales from the Dark\nSide of the City;_ and “Grey Goldmine.”\n\n[7](ch01.html#ch01notes7). For more on the street-level changes in San\nFrancisco, see Bloomfield, “History of the California Historical Society’s New\nMission Street Neighborhood.”\n\n[8](ch01.html#ch01notes8). “Street Homelessness.” See also “Counterpoints: An\nAtlas of Displacement and Resistance.”\n\n[9](ch01.html#ch01notes9). Gee, “San Francisco or Mumbai?”\n\n[10](ch01.html#ch01notes10). H. W. Turner published a detailed geological\nsurvey of the Silver Peak area in July 1909. In beautiful prose, Turner\nextolled the geological variety within what he described as “slopes of cream\nand pink tuffs, and little hillocks of a bright brick red.” Turner,\n“Contribution to the Geology of the Silver Peak Quadrangle, Nevada,” 228.\n\n[11](ch01.html#ch01notes11). Lambert, “Breakdown of Raw Materials in Tesla’s\nBatteries and Possible Breaknecks.”\n\n[12](ch01.html#ch01notes12). Bullis, “Lithium-Ion Battery.”\n\n[13](ch01.html#ch01notes13). “Chinese Lithium Giant Agrees to Three-Year Pact\nto Supply Tesla.”\n\n[14](ch01.html#ch01notes14). Wald, “Tesla Is a Battery Business.”\n\n[15](ch01.html#ch01notes15). Scheyder, “Tesla Expects Global Shortage.”\n\n[16](ch01.html#ch01notes16). Wade, “Tesla’s Electric Cars Aren’t as Green.”\n\n[17](ch01.html#ch01notes17). Business Council for Sustainable Energy, “2019\nSustainable Energy in America Factbook.” U.S. Energy Information\nAdministration, “What Is U.S. Electricity Generation by Energy Source?”\n\n[18](ch01.html#ch01notes18). Whittaker et al., _AI Now Report 2018._\n\n[19](ch01.html#ch01notes19). Parikka, _Geology of Media_ , vii—viii; McLuhan,\n_Understanding Media._\n\n[20](ch01.html#ch01notes20). Ely, “Life Expectancy of Electronics.”\n\n[21](ch01.html#ch01notes21). Sandro Mezzadra and Brett Neilson use the term\n“extractivism” to name the relation between different forms of extractive\noperations in contemporary capitalism, which we see repeated in the context of\nthe AI industry. Mezzadra and Neilson, “Multiple Frontiers of Extraction.”\n\n[22](ch01.html#ch01notes22). Nassar et al., “Evaluating the Mineral Commodity\nSupply Risk of the US Manufacturing Sector.”\n\n[23](ch01.html#ch01notes23). Mumford, _Technics and Civilization_ , 74.\n\n[24](ch01.html#ch01notes24). See, e.g., Ayogu and Lewis, “Conflict Minerals.”\n\n[25](ch01.html#ch01notes25). Burke, “Congo Violence Fuels Fears of Return to\n90s Bloodbath.”\n\n[26](ch01.html#ch01notes26). “Congo’s Bloody Coltan.”\n\n[27](ch01.html#ch01notes27). “Congo’s Bloody Coltan.”\n\n[28](ch01.html#ch01notes28). “Transforming Intel’s Supply Chain with Real-Time\nAnalytics.”\n\n[29](ch01.html#ch01notes29). See, e.g., an open letter from seventy\nsignatories that criticizes the limitations of the so-called conflict-free\ncertification process: “An Open Letter.”\n\n[30](ch01.html#ch01notes30). “Responsible Minerals Policy and Due Diligence.”\n\n[31](ch01.html#ch01notes31). In _The Elements of Power_ , David S. Abraham\ndescribes the invisible networks of rare metals traders in global electronics\nsupply chains: “The network to get rare metals from the mine to your laptop\ntravels through a murky network of traders, processors, and component\nmanufacturers. Traders are the middlemen who do more than buy and sell rare\nmetals: they help to regulate information and are the hidden link that helps\nin navigating the network between metals plants and the components in our\nlaptops” (89).\n\n[32](ch01.html#ch01notes32). “Responsible Minerals Sourcing.”\n\n[33](ch01.html#ch01notes33). Liu, “Chinese Mining Dump.”\n\n[34](ch01.html#ch01notes34). “Bayan Obo Deposit.”\n\n[35](ch01.html#ch01notes35). Maughan, “Dystopian Lake Filled by the World’s\nTech Lust.”\n\n[36](ch01.html#ch01notes36). Hird, “Waste, Landfills, and an Environmental\nEthics of Vulnerability,” 105.\n\n[37](ch01.html#ch01notes37). Abraham, _Elements of Power_ , 175.\n\n[38](ch01.html#ch01notes38). Abraham, 176.\n\n[39](ch01.html#ch01notes39). Simpson, “Deadly Tin Inside Your Smartphone.”\n\n[40](ch01.html#ch01notes40). Hodal, “Death Metal.”\n\n[41](ch01.html#ch01notes41). Hodal.\n\n[42](ch01.html#ch01notes42). Tully, “Victorian Ecological Disaster.”\n\n[43](ch01.html#ch01notes43). Starosielski, _Undersea Network_ , 34.\n\n[44](ch01.html#ch01notes44). See Couldry and Mejías, _Costs of Connection_ ,\n46.\n\n[45](ch01.html#ch01notes45). Couldry and Mejías, 574.\n\n[46](ch01.html#ch01notes46). For a superb account of the history of undersea\ncables, see Starosielski, _Undersea Network._\n\n[47](ch01.html#ch01notes47). Dryer, “Designing Certainty,” 45.\n\n[48](ch01.html#ch01notes48). Dryer, 46.\n\n[49](ch01.html#ch01notes49). Dryer, 266–68.\n\n[50](ch01.html#ch01notes50). More people are now drawing attention to this\nproblem—including researchers at AI Now. See Dobbe and Whittaker, “AI and\nClimate Change.”\n\n[51](ch01.html#ch01notes51). See, as an example of early scholarship in this\narea, Ensmenger, “Computation, Materiality, and the Global Environment.”\n\n[52](ch01.html#ch01notes52). Hu, _Prehistory of the Cloud_ , 146.\n\n[53](ch01.html#ch01notes53). Jones, “How to Stop Data Centres from Gobbling Up\nthe World’s Electricity.” Some progress has been made toward mitigating these\nconcerns through greater energy efficiency practices, but significant long-\nterm challenges remain. Masanet et al., “Recalibrating Global Data Center\nEnergy-Use Estimates.”\n\n[54](ch01.html#ch01notes54). Belkhir and Elmeligi, “Assessing ICT Global\nEmissions Footprint”; Andrae and Edler, “On Global Electricity Usage.”\n\n[55](ch01.html#ch01notes55). Strubell, Ganesh, and McCallum, “Energy and\nPolicy Considerations for Deep Learning in NLP.”\n\n[56](ch01.html#ch01notes56). Strubell, Ganesh, and McCallum.\n\n[57](ch01.html#ch01notes57). Sutton, “Bitter Lesson.”\n\n[58](ch01.html#ch01notes58). “AI and Compute.”\n\n[59](ch01.html#ch01notes59). Cook et al., _Clicking Clean._\n\n[60](ch01.html#ch01notes60). Ghaffary, “More Than 1,000 Google Employees\nSigned a Letter.” See also “Apple Commits to Be 100 Percent Carbon Neutral”;\nHarrabin, “Google Says Its Carbon Footprint Is Now Zero”; Smith, “Microsoft\nWill Be Carbon Negative by 2030.”\n\n[61](ch01.html#ch01notes61). “Powering the Cloud.”\n\n[62](ch01.html#ch01notes62). “Powering the Cloud.”\n\n[63](ch01.html#ch01notes63). “Powering the Cloud.”\n\n[64](ch01.html#ch01notes64). Hogan, “Data Flows and Water Woes.”\n\n[65](ch01.html#ch01notes65). “Off Now.”\n\n[66](ch01.html#ch01notes66). Carlisle, “Shutting Off NSA’s Water Gains\nSupport.”\n\n[67](ch01.html#ch01notes67). Materiality is a complex concept, and there is a\nlengthy literature that contends with it in such fields as STS, anthropology,\nand media studies. In one sense, materiality refers to what Leah Lievrouw\ndescribes as “the physical character and existence of objects and artifacts\nthat makes them useful and usable for certain purposes under particular\nconditions.” Lievrouw quoted in Gillespie, Boczkowski, and Foot, _Media\nTechnologies_ , 25. But as Diana Coole and Samantha Frost write, “Materiality\nis always something more than ‘mere’ matter: an excess, force, vitality,\nrelationality, or difference that renders matter active, self-creative,\nproductive, unproductive.” Coole and Frost, _New Materialisms_ , 9.\n\n[68](ch01.html#ch01notes68). United Nations Conference on Trade and\nDevelopment, _Review of Maritime Transport, 2017._\n\n[69](ch01.html#ch01notes69). George, _Ninety Percent of Everything_ , 4.\n\n[70](ch01.html#ch01notes70). Schlanger, “If Shipping Were a Country.”\n\n[71](ch01.html#ch01notes71). Vidal, “Health Risks of Shipping Pollution.”\n\n[72](ch01.html#ch01notes72). “Containers Lost at Sea—2017 Update.”\n\n[73](ch01.html#ch01notes73). Adams, “Lost at Sea.”\n\n[74](ch01.html#ch01notes74). Mumford, _Myth of the Machine._\n\n[75](ch01.html#ch01notes75). Labban, “Deterritorializing Extraction.” For an\nexpansion on this idea, see Arboleda, _Planetary Mine._\n\n[76](ch01.html#ch01notes76). Ananny and Crawford, “Seeing without Knowing.”\n\n### 2  \nLabor\n\n[1](ch02.html#ch02notes1). Wilson, “Amazon and Target Race.”\n\n[2](ch02.html#ch02notes2). Lingel and Crawford, “Alexa, Tell Me about Your\nMother.”\n\n[3](ch02.html#ch02notes3). Federici, _Wages against Housework;_ Gregg,\n_Counterproductive._\n\n[4](ch02.html#ch02notes4). In _The Utopia of Rules_ , David Graeber details\nthe sense of loss experienced by white-collar workers who now have to enter\ndata into the decision-making systems that have replaced specialist\nadministrative support staff in most professional workplaces.\n\n[5](ch02.html#ch02notes5). Smith, _Wealth of Nations_ , 4–5.\n\n[6](ch02.html#ch02notes6). Marx and Engels, _Marx-Engels Reader_ , 479. Marx\nexpanded on this notion of the worker as an “appendage” in _Capital_ , vol. 1:\n“In handicrafts and manufacture, the worker makes use of a tool; in the\nfactory, the machine makes use of him. There the movements of the instrument\nof labor proceed from him, here it is the movements of the machine that he\nmust follow. In manufacture the workers are parts of a living mechanism. In\nthe factory we have a lifeless mechanism which is independent of the workers,\nwho are incorporated into it as its living appendages.” Marx, _Das Kapital_ ,\n548–49.\n\n[7](ch02.html#ch02notes7). Luxemburg, “Practical Economies,” 444.\n\n[8](ch02.html#ch02notes8). Thompson, “Time, Work-Discipline, and Industrial\nCapitalism.”\n\n[9](ch02.html#ch02notes9). Thompson, 88–90.\n\n[10](ch02.html#ch02notes10). Werrett, “Potemkin and the Panopticon,” 6.\n\n[11](ch02.html#ch02notes11). See, e.g., Cooper, “Portsmouth System of\nManufacture.”\n\n[12](ch02.html#ch02notes12). Foucault, _Discipline and Punish;_ Horne and\nMaly, _Inspection House._\n\n[13](ch02.html#ch02notes13). Mirzoeff, _Right to Look_ , 58.\n\n[14](ch02.html#ch02notes14). Mirzoeff, 55.\n\n[15](ch02.html#ch02notes15). Mirzoeff, 56.\n\n[16](ch02.html#ch02notes16). Gray and Suri, _Ghost Work._\n\n[17](ch02.html#ch02notes17). Irani, “Hidden Faces of Automation.”\n\n[18](ch02.html#ch02notes18). Yuan, “How Cheap Labor Drives China’s A.I.\nAmbitions”; Gray and Suri, “Humans Working behind the AI Curtain.”\n\n[19](ch02.html#ch02notes19). Berg et al., _Digital Labour Platforms._\n\n[20](ch02.html#ch02notes20). Roberts, _Behind the Screen;_ Gillespie,\n_Custodians of the Internet_ , 111–40.\n\n[21](ch02.html#ch02notes21). Silberman et al., “Responsible Research with\nCrowds.”\n\n[22](ch02.html#ch02notes22). Silberman et al.\n\n[23](ch02.html#ch02notes23). Huet, “Humans Hiding behind the Chatbots.”\n\n[24](ch02.html#ch02notes24). Huet.\n\n[25](ch02.html#ch02notes25). See Sadowski, “Potemkin AI.”\n\n[26](ch02.html#ch02notes26). Taylor, “Automation Charade.”\n\n[27](ch02.html#ch02notes27). Taylor.\n\n[28](ch02.html#ch02notes28). Gray and Suri, _Ghost Work._\n\n[29](ch02.html#ch02notes29). Standage, _Turk_ , 23.\n\n[30](ch02.html#ch02notes30). Standage, 23.\n\n[31](ch02.html#ch02notes31). See, e.g., Aytes, “Return of the Crowds,” 80.\n\n[32](ch02.html#ch02notes32). Irani, “Difference and Dependence among Digital\nWorkers,” 225.\n\n[33](ch02.html#ch02notes33). Pontin, “Artificial Intelligence.”\n\n[34](ch02.html#ch02notes34). Menabrea and Lovelace, “Sketch of the Analytical\nEngine.”\n\n[35](ch02.html#ch02notes35). Babbage, _On the Economy of Machinery and\nManufactures_ , 39–43.\n\n[36](ch02.html#ch02notes36). Babbage evidently acquired an interest in\nquality-control processes while trying (vainly) to establish a reliable supply\nchain for the components of his calculating engines.\n\n[37](ch02.html#ch02notes37). Schaffer, “Babbage’s Calculating Engines and the\nFactory System,” 280.\n\n[38](ch02.html#ch02notes38). Taylor, _People’s Platform_ , 42.\n\n[39](ch02.html#ch02notes39). Katz and Krueger, “Rise and Nature of Alternative\nWork Arrangements.”\n\n[40](ch02.html#ch02notes40). Rehmann, “Taylorism and Fordism in the\nStockyards,” 26.\n\n[41](ch02.html#ch02notes41). Braverman, _Labor and Monopoly Capital_ , 56, 67;\nSpecht, _Red Meat Republic._\n\n[42](ch02.html#ch02notes42). Taylor, _Principles of Scientific Management._\n\n[43](ch02.html#ch02notes43). Marx, _Poverty of Philosophy_ , 22.\n\n[44](ch02.html#ch02notes44). Qiu, Gregg, and Crawford, “Circuits of Labour”;\nQiu, _Goodbye iSlave._\n\n[45](ch02.html#ch02notes45). Markoff, “Skilled Work, without the Worker.”\n\n[46](ch02.html#ch02notes46). Guendelsberger, _On the Clock_ , 22.\n\n[47](ch02.html#ch02notes47). Greenhouse, “McDonald’s Workers File Wage Suits.”\n\n[48](ch02.html#ch02notes48). Greenhouse.\n\n[49](ch02.html#ch02notes49). Mayhew and Quinlan, “Fordism in the Fast Food\nIndustry.”\n\n[50](ch02.html#ch02notes50). Ajunwa, Crawford, and Schultz, “Limitless Worker\nSurveillance.”\n\n[51](ch02.html#ch02notes51). Mikel, “WeWork Just Made a Disturbing\nAcquisition.”\n\n[52](ch02.html#ch02notes52). Mahdawi, “Domino’s ‘Pizza Checker’ Is Just the\nBeginning.”\n\n[53](ch02.html#ch02notes53). Wajcman, “How Silicon Valley Sets Time.”\n\n[54](ch02.html#ch02notes54). Wajcman, 1277.\n\n[55](ch02.html#ch02notes55). Gora, Herzog, and Tripathi, “Clock\nSynchronization.”\n\n[56](ch02.html#ch02notes56). Eglash, “Broken Metaphor,” 361.\n\n[57](ch02.html#ch02notes57). Kemeny and Kurtz, “Dartmouth Timesharing,” 223.\n\n[58](ch02.html#ch02notes58). Eglash, “Broken Metaphor,” 364.\n\n[59](ch02.html#ch02notes59). Brewer, “Spanner, TrueTime.”\n\n[60](ch02.html#ch02notes60). Corbett et al., “Spanner,” 14, cited in House,\n“Synchronizing Uncertainty,” 124.\n\n[61](ch02.html#ch02notes61). Galison, _Einstein’s Clocks, Poincaré’s Maps_ ,\n104.\n\n[62](ch02.html#ch02notes62). Galison, 112.\n\n[63](ch02.html#ch02notes63). Colligan and Linley, “Media, Technology, and\nLiterature,” 246.\n\n[64](ch02.html#ch02notes64). Carey, “Technology and Ideology.”\n\n[65](ch02.html#ch02notes65). Carey, 13.\n\n[66](ch02.html#ch02notes66). This contrasts with what Foucault called the\n“microphysics of power” to describe how institutions and apparatuses create\nparticular logics and forms of validity. Foucault, _Discipline and Punish_ ,\n26.\n\n[67](ch02.html#ch02notes67). Spargo, _Syndicalism, Industrial Unionism, and\nSocialism._\n\n[68](ch02.html#ch02notes68). Personal conversation with the author at an\nAmazon fulfillment center tour, Robbinsville, N.J., October 8, 2019.\n\n[69](ch02.html#ch02notes69). Muse, “Organizing Tech.”\n\n[70](ch02.html#ch02notes70). Abdi Muse, personal conversation with the author,\nOctober 2, 2019.\n\n[71](ch02.html#ch02notes71). Gurley, “60 Amazon Workers Walked Out.”\n\n[72](ch02.html#ch02notes72). Muse quoted in _Organizing Tech._\n\n[73](ch02.html#ch02notes73). Desai quoted in _Organizing Tech._\n\n[74](ch02.html#ch02notes74). Estreicher and Owens, “Labor Board Wrongly\nRejects Employee Access to Company Email.”\n\n[75](ch02.html#ch02notes75). This observation comes from conversations with\nvarious labor organizers, tech workers, and researchers, including Astra\nTaylor, Dan Greene, Bo Daley, and Meredith Whittaker.\n\n[76](ch02.html#ch02notes76). Kerr, “Tech Workers Protest in SF.”\n\n### 3  \nData\n\n[1](ch03.html#ch03notes1). National Institute of Standards and Technology\n(NIST), “Special Database 32—Multiple Encounter Dataset (MEDS).”\n\n[2](ch03.html#ch03notes2). Russell, _Open Standards and the Digital Age._\n\n[3](ch03.html#ch03notes3). Researchers at NIST (then the National Bureau of\nStandards, NBS) began working on the first version of the FBI’s Automated\nFingerprint Identification System in the late 1960s. See Garris and Wilson,\n“NIST Biometrics Evaluations and Developments,” 1.\n\n[4](ch03.html#ch03notes4). Garris and Wilson, 1.\n\n[5](ch03.html#ch03notes5). Garris and Wilson, 12.\n\n[6](ch03.html#ch03notes6). Sekula, “Body and the Archive,” 17.\n\n[7](ch03.html#ch03notes7). Sekula, 18–19.\n\n[8](ch03.html#ch03notes8). Sekula, 17.\n\n[9](ch03.html#ch03notes9). See, e.g., Grother et al., “2017 IARPA Face\nRecognition Prize Challenge (FRPC).”\n\n[10](ch03.html#ch03notes10). See, e.g., Ever AI, “Ever AI Leads All US\nCompanies.”\n\n[11](ch03.html#ch03notes11). Founds et al., “NIST Special Database 32.”\n\n[12](ch03.html#ch03notes12). Curry et al., “NIST Special Database 32 Multiple\nEncounter Dataset I (MEDS-I),” 8.\n\n[13](ch03.html#ch03notes13). See, e.g., Jaton, “We Get the Algorithms of Our\nGround Truths.”\n\n[14](ch03.html#ch03notes14). Nilsson, _Quest for Artificial Intelligence_ ,\n398.\n\n[15](ch03.html#ch03notes15). “ImageNet Large Scale Visual Recognition\nCompetition (ILSVRC).”\n\n[16](ch03.html#ch03notes16). In the late 1970s, Ryszard Michalski wrote an\nalgorithm based on symbolic variables and logical rules. This language was\npopular in the 1980s and 1990s, but as the rules of decision-making and\nqualification became more complex, the language became less usable. At the\nsame moment, the potential of using large training sets triggered a shift from\nthis conceptual clustering to contemporary machine learning approaches.\nMichalski, “Pattern Recognition as Rule-Guided Inductive Inference.”\n\n[17](ch03.html#ch03notes17). Bush, “As We May Think.”\n\n[18](ch03.html#ch03notes18). Light, “When Computers Were Women”; Hicks,\n_Programmed Inequality._\n\n[19](ch03.html#ch03notes19). As described in Russell and Norvig, _Artificial\nIntelligence_ , 546.\n\n[20](ch03.html#ch03notes20). Li, “Divination Engines,” 143.\n\n[21](ch03.html#ch03notes21). Li, 144.\n\n[22](ch03.html#ch03notes22). Brown and Mercer, “Oh, Yes, Everything’s Right on\nSchedule, Fred.”\n\n[23](ch03.html#ch03notes23). Lem, “First Sally (A), or Trurl’s Electronic\nBard,” 199.\n\n[24](ch03.html#ch03notes24). Lem, 199.\n\n[25](ch03.html#ch03notes25). Brown and Mercer, “Oh, Yes, Everything’s Right on\nSchedule, Fred.”\n\n[26](ch03.html#ch03notes26). Marcus, Marcinkiewicz, and Santorini, “Building a\nLarge Annotated Corpus of English.”\n\n[27](ch03.html#ch03notes27). Klimt and Yang, “Enron Corpus.”\n\n[28](ch03.html#ch03notes28). Wood, Massey, and Brownell, “FERC Order Directing\nRelease of Information,” 12.\n\n[29](ch03.html#ch03notes29). Heller, “What the Enron Emails Say about Us.”\n\n[30](ch03.html#ch03notes30). Baker et al., “Research Developments and\nDirections in Speech Recognition.”\n\n[31](ch03.html#ch03notes31). I have participated in early work to address this\ngap. See, e.g., Gebru et al., “Datasheets for Datasets.” Other researchers\nhave also sought to address this problem for AI models; see Mitchell et al.,\n“Model Cards for Model Reporting”; Raji and Buolamwini, “Actionable Auditing.”\n\n[32](ch03.html#ch03notes32). Phillips, Rauss, and Der, “FERET (Face\nRecognition Technology) Recognition Algorithm Development and Test Results,”\n9.\n\n[33](ch03.html#ch03notes33). Phillips, Rauss, and Der, 61.\n\n[34](ch03.html#ch03notes34). Phillips, Rauss, and Der, 12.\n\n[35](ch03.html#ch03notes35). See Aslam, “Facebook by the Numbers (2019)”; and\n“Advertising on Twitter.”\n\n[36](ch03.html#ch03notes36). Fei-Fei Li, as quoted in Gershgorn, “Data That\nTransformed AI Research.”\n\n[37](ch03.html#ch03notes37). Deng et al., “ImageNet.”\n\n[38](ch03.html#ch03notes38). Gershgorn, “Data That Transformed AI Research.”\n\n[39](ch03.html#ch03notes39). Gershgorn.\n\n[40](ch03.html#ch03notes40). Markoff, “Seeking a Better Way to Find Web\nImages.”\n\n[41](ch03.html#ch03notes41). Hernandez, “CU Colorado Springs Students Secretly\nPhotographed.”\n\n[42](ch03.html#ch03notes42). Zhang et al., “Multi-Target, Multi-Camera\nTracking by Hierarchical Clustering.”\n\n[43](ch03.html#ch03notes43). Sheridan, “Duke Study Recorded Thousands of\nStudents’ Faces.”\n\n[44](ch03.html#ch03notes44). Harvey and LaPlace, “Brainwash Dataset.”\n\n[45](ch03.html#ch03notes45). Locker, “Microsoft, Duke, and Stanford Quietly\nDelete Databases.”\n\n[46](ch03.html#ch03notes46). Murgia and Harlow, “Who’s Using Your Face?” When\nthe _Financial Times_ exposed the contents of this dataset, Microsoft removed\nthe set from the internet, and a spokesperson for Microsoft claimed simply\nthat it was removed “because the research challenge is over.” Locker,\n“Microsoft, Duke, and Stanford Quietly Delete Databases.”\n\n[47](ch03.html#ch03notes47). Franceschi-Bicchierai, “Redditor Cracks Anonymous\nData Trove.”\n\n[48](ch03.html#ch03notes48). Tockar, “Riding with the Stars.”\n\n[49](ch03.html#ch03notes49). Crawford and Schultz, “Big Data and Due Process.”\n\n[50](ch03.html#ch03notes50). Franceschi-Bicchierai, “Redditor Cracks Anonymous\nData Trove.”\n\n[51](ch03.html#ch03notes51). Nilsson, _Quest for Artificial Intelligence_ ,\n495.\n\n[52](ch03.html#ch03notes52). And, as Geoff Bowker famously reminds us, “Raw\ndata is both an oxymoron and a bad idea; to the contrary, data should be\ncooked with care.” Bowker, _Memory Practices in the Sciences_ , 184–85.\n\n[53](ch03.html#ch03notes53). Fourcade and Healy, “Seeing Like a Market,” 13,\nemphasis added.\n\n[54](ch03.html#ch03notes54). Meyer and Jepperson, “‘Actors’ of Modern\nSociety.”\n\n[55](ch03.html#ch03notes55). Gitelman, _“Raw Data” Is an Oxymoron_ , 3.\n\n[56](ch03.html#ch03notes56). Many scholars have looked closely at the work\nthese metaphors do. Media studies professors Cornelius Puschmann and Jean\nBurgess analyzed the common data metaphors and noted two widespread\ncategories: data “as a natural force to be controlled and [data] as a resource\nto be consumed.” Puschmann and Burgess, “Big Data, Big Questions,” abstract.\nResearchers Tim Hwang and Karen Levy suggest that describing data as “the new\noil” carries connotations of being costly to acquire but also suggests the\npossibility of “big payoffs for those with the means to extract it.” Hwang and\nLevy, “‘The Cloud’ and Other Dangerous Metaphors.”\n\n[57](ch03.html#ch03notes57). Stark and Hoffmann, “Data Is the New What?”\n\n[58](ch03.html#ch03notes58). Media scholars Nick Couldry and Ulises Mejías\ncall this “data colonialism,” which is steeped in the historical, predatory\npractices of colonialism but married to (and obscured by) contemporary\ncomputing methods. However, as other scholars have shown, this terminology is\ndouble-edged because it can occlude the real and ongoing harms of colonialism.\nCouldry and Mejías, “Data Colonialism”; Couldry and Mejías, _Costs of\nConnection_ _;_ Segura and Waisbord, “Between Data Capitalism and Data\nCitizenship.”\n\n[59](ch03.html#ch03notes59). They refer to this form of capital as\n“ubercapital.” Fourcade and Healy, “Seeing Like a Market,” 19.\n\n[60](ch03.html#ch03notes60). Sadowski, “When Data Is Capital,” 8.\n\n[61](ch03.html#ch03notes61). Sadowski, 9.\n\n[62](ch03.html#ch03notes62). Here I’m drawing from a history of human subjects\nreview and large-scale data studies coauthored with Jake Metcalf. See Metcalf\nand Crawford, “Where Are Human Subjects in Big Data Research?”\n\n[63](ch03.html#ch03notes63). “Federal Policy for the Protection of Human\nSubjects.”\n\n[64](ch03.html#ch03notes64). See Metcalf and Crawford, “Where Are Human\nSubjects in Big Data Research?”\n\n[65](ch03.html#ch03notes65). Seo et al., “Partially Generative Neural\nNetworks.” Jeffrey Brantingham, one of the authors, is also a co-founder of\nthe controversial predictive policing company PredPol. See Winston and\nBurrington, “A Pioneer in Predictive Policing.”\n\n[66](ch03.html#ch03notes66). “CalGang Criminal Intelligence System.”\n\n[67](ch03.html#ch03notes67). Libby, “Scathing Audit Bolsters Critics’ Fears.”\n\n[68](ch03.html#ch03notes68). Hutson, “Artificial Intelligence Could Identify\nGang Crimes.”\n\n[69](ch03.html#ch03notes69). Hoffmann, “Data Violence and How Bad Engineering\nChoices Can Damage Society.”\n\n[70](ch03.html#ch03notes70). Weizenbaum, _Computer Power and Human Reason_ ,\n266.\n\n[71](ch03.html#ch03notes71). Weizenbaum, 275–76.\n\n[72](ch03.html#ch03notes72). Weizenbaum, 276.\n\n[73](ch03.html#ch03notes73). For more on the history of extraction of data and\ninsights from marginalized communities, see Costanza-Chock, _Design Justice;_\nand D’Ignazio and Klein, _Data Feminism._\n\n[74](ch03.html#ch03notes74). Revell, “Google DeepMind’s NHS Data Deal ‘Failed\nto Comply.’”\n\n[75](ch03.html#ch03notes75). “Royal Free–Google DeepMind Trial Failed to\nComply.”\n\n### 4  \nClassification\n\n[1](ch04.html#ch04notes1). Fabian, _Skull Collectors._\n\n[2](ch04.html#ch04notes2). Gould, _Mismeasure of Man_ , 83.\n\n[3](ch04.html#ch04notes3). Kolbert, “There’s No Scientific Basis for Race.”\n\n[4](ch04.html#ch04notes4). Keel, “Religion, Polygenism and the Early Science\nof Human Origins.”\n\n[5](ch04.html#ch04notes5). Thomas, _Skull Wars._\n\n[6](ch04.html#ch04notes6). Thomas, 85.\n\n[7](ch04.html#ch04notes7). Kendi, “History of Race and Racism in America.”\n\n[8](ch04.html#ch04notes8). Gould, _Mismeasure of Man_ , 88.\n\n[9](ch04.html#ch04notes9). Mitchell, “Fault in His Seeds.”\n\n[10](ch04.html#ch04notes10). Horowitz, “Why Brain Size Doesn’t Correlate with\nIntelligence.”\n\n[11](ch04.html#ch04notes11). Mitchell, “Fault in His Seeds.”\n\n[12](ch04.html#ch04notes12). Gould, _Mismeasure of Man_ , 58.\n\n[13](ch04.html#ch04notes13). West, “Genealogy of Modern Racism,” 91.\n\n[14](ch04.html#ch04notes14). Bouche and Rivard, “America’s Hidden History.”\n\n[15](ch04.html#ch04notes15). Bowker and Star, _Sorting Things Out_ , 319.\n\n[16](ch04.html#ch04notes16). Bowker and Star, 319.\n\n[17](ch04.html#ch04notes17). Nedlund, “Apple Card Is Accused of Gender Bias”;\nAngwin et al., “Machine Bias”; Angwin et al., “Dozens of Companies Are Using\nFacebook to Exclude.”\n\n[18](ch04.html#ch04notes18). Dougherty, “Google Photos Mistakenly Labels Black\nPeople ‘Gorillas’”; Perez, “Microsoft Silences Its New A.I. Bot Tay”;\nMcMillan, “It’s Not You, It’s It”; Sloane, “Online Ads for High-Paying Jobs\nAre Targeting Men More Than Women.”\n\n[19](ch04.html#ch04notes19). See Benjamin, _Race after Technology;_ and Noble,\n_Algorithms of Oppression._\n\n[20](ch04.html#ch04notes20). Greene, “Science May Have Cured Biased AI”;\nNatarajan, “Amazon and NSF Collaborate to Accelerate Fairness in AI Research.”\n\n[21](ch04.html#ch04notes21). Dastin, “Amazon Scraps Secret AI Recruiting\nTool.”\n\n[22](ch04.html#ch04notes22). Dastin.\n\n[23](ch04.html#ch04notes23). This is part of a larger trend toward automating\naspects of hiring. For a detailed account, see Ajunwa and Greene, “Platforms\nat Work.”\n\n[24](ch04.html#ch04notes24). There are several superb accounts of the history\nof inequality and discrimination in computation. These are a few that have\ninformed my thinking on these issues: Hicks, _Programmed Inequality;_\nMcIlwain, _Black Software;_ Light, “When Computers Were Women”; and Ensmenger,\n_Computer Boys Take Over._\n\n[25](ch04.html#ch04notes25). Cetina, _Epistemic Cultures_ , 3.\n\n[26](ch04.html#ch04notes26). Merler et al., “Diversity in Faces.”\n\n[27](ch04.html#ch04notes27). Buolamwini and Gebru, “Gender Shades”; Raj et al.\n“Saving Face.”\n\n[28](ch04.html#ch04notes28). Merler et al., “Diversity in Faces.”\n\n[29](ch04.html#ch04notes29). “YFCC100M Core Dataset.”\n\n[30](ch04.html#ch04notes30). Merler et al., “Diversity in Faces,” 1.\n\n[31](ch04.html#ch04notes31). There are many excellent books on these issues,\nbut in particular, see Roberts, _Fatal Invention_ , 18–41; and Nelson, _Social\nLife of DNA_ , 43. See also Tishkoff and Kidd, “Implications of Biogeography.”\n\n[32](ch04.html#ch04notes32). Browne, “Digital Epidermalization,” 135.\n\n[33](ch04.html#ch04notes33). Benthall and Haynes, “Racial Categories in\nMachine Learning.”\n\n[34](ch04.html#ch04notes34). Mitchell, “Need for Biases in Learning\nGeneralizations.”\n\n[35](ch04.html#ch04notes35). Dietterich and Kong, “Machine Learning Bias,\nStatistical Bias.”\n\n[36](ch04.html#ch04notes36). Domingos, “Useful Things to Know about Machine\nLearning.”\n\n[37](ch04.html#ch04notes37). _Maddox v. State_ , 32 Ga. 5S7, 79 Am. Dec. 307;\n_Pierson v. State_ , 18 Tex. App. 55S; _Hinkle v. State_ , 94 Ga. 595, 21 S.\nE. 601.\n\n[38](ch04.html#ch04notes38). Tversky and Kahneman, “Judgment under\nUncertainty.”\n\n[39](ch04.html#ch04notes39). Greenwald and Krieger, “Implicit Bias,” 951.\n\n[40](ch04.html#ch04notes40). Fellbaum, _WordNet_ , xviii. Below I am drawing\non research into ImageNet conducted with Trevor Paglen. See Crawford and\nPaglen, “Excavating AI.”\n\n[41](ch04.html#ch04notes41). Fellbaum, xix.\n\n[42](ch04.html#ch04notes42). Nelson and Kucera, _Brown Corpus Manual._\n\n[43](ch04.html#ch04notes43). Borges, “The Analytical Language of John\nWilkins.”\n\n[44](ch04.html#ch04notes44). These are some of the categories that have now\nbeen deleted entirely from ImageNet as of October 1, 2020.\n\n[45](ch04.html#ch04notes45). See Keyes, “Misgendering Machines.”\n\n[46](ch04.html#ch04notes46). Drescher, “Out of DSM.”\n\n[47](ch04.html#ch04notes47). See Bayer, _Homosexuality and American\nPsychiatry._\n\n[48](ch04.html#ch04notes48). Keyes, “Misgendering Machines.”\n\n[49](ch04.html#ch04notes49). Hacking, “Making Up People,” 23.\n\n[50](ch04.html#ch04notes50). Bowker and Star, _Sorting Things Out_ , 196.\n\n[51](ch04.html#ch04notes51). This is drawn from Lakoff, _Women, Fire, and\nDangerous Things._\n\n[52](ch04.html#ch04notes52). ImageNet Roulette was one of the outputs of a\nmultiyear research collaboration between the artist Trevor Paglen and me, in\nwhich we studied the underlying logic of multiple benchmark training sets in\nAI. ImageNet Roulette, led by Paglen and produced by Leif Ryge, was an app\nthat allowed people to interact with a neural net trained on the “person”\ncategory of ImageNet. People could upload images of themselves—or news images\nor historical photographs—to see how ImageNet would label them. People could\nalso see how many of the labels are bizarre, racist, misogynist, and otherwise\nproblematic. The app was designed to show people these concerning labels while\nwarning them in advance of the potential results. All uploaded image data were\nimmediately deleted on processing. See Crawford and Paglen, “Excavating AI.”\n\n[53](ch04.html#ch04notes53). Yang et al., “Towards Fairer Datasets,” paragraph\n4.2.\n\n[54](ch04.html#ch04notes54). Yang et al., paragraph 4.3.\n\n[55](ch04.html#ch04notes55). Markoff, “Seeking a Better Way to Find Web\nImages.”\n\n[56](ch04.html#ch04notes56). Browne, _Dark Matters_ , 114.\n\n[57](ch04.html#ch04notes57). Scheuerman et al., “How We’ve Taught Algorithms\nto See Identity.”\n\n[58](ch04.html#ch04notes58). UTKFace Large Scale Face Dataset,\n<https://susanqq.github.io/UTKFace>.\n\n[59](ch04.html#ch04notes59). Bowker and Star, _Sorting Things Out_ , 197.\n\n[60](ch04.html#ch04notes60). Bowker and Star, 198.\n\n[61](ch04.html#ch04notes61). Edwards and Hecht, “History and the\nTechnopolitics of Identity,” 627.\n\n[62](ch04.html#ch04notes62). Haraway, _Modest_Witness@Second_Millennium_ ,\n234.\n\n[63](ch04.html#ch04notes63). Stark, “Facial Recognition Is the Plutonium of\nAI,” 53.\n\n[64](ch04.html#ch04notes64). In order of the examples, see Wang and Kosinski,\n“Deep Neural Networks Are More Accurate than Humans”; Wu and Zhang, “Automated\nInference on Criminality Using Face Images”; and Angwin et al., “Machine\nBias.”\n\n[65](ch04.html#ch04notes65). Agüera y Arcas, Mitchell, and Todorov,\n“Physiognomy’s New Clothes.”\n\n[66](ch04.html#ch04notes66). Nielsen, _Disability History of the United\nStates;_ Kafer, _Feminist, Queer, Crip;_ Siebers, _Disability Theory._\n\n[67](ch04.html#ch04notes67). Whittaker et al., “Disability, Bias, and AI.”\n\n[68](ch04.html#ch04notes68). Hacking, “Kinds of People,” 289.\n\n[69](ch04.html#ch04notes69). Bowker and Star, _Sorting Things Out_ , 31.\n\n[70](ch04.html#ch04notes70). Bowker and Star, 6.\n\n[71](ch04.html#ch04notes71). Eco, Infinity of Lists.\n\n[72](ch04.html#ch04notes72). Douglass, “West India Emancipation.”\n\n### 5  \nAffect\n\n[1](ch05.html#ch05notes1). Particular thanks to Alex Campolo, who was my\nresearch assistant and interlocutor for this chapter, and for his research\ninto Ekman and the history of emotions.\n\n[2](ch05.html#ch05notes2). “Emotion Detection and Recognition”; Schwartz,\n“Don’t Look Now.”\n\n[3](ch05.html#ch05notes3). Ohtake, “Psychologist Paul Ekman Delights at\nExploratorium.”\n\n[4](ch05.html#ch05notes4). Ekman, _Emotions Revealed,_ 7.\n\n[5](ch05.html#ch05notes5). For an overview of researchers who have found flaws\nin the claim that emotional expressions are universal and can be predicted by\nAI, see Heaven, “Why Faces Don’t Always Tell the Truth.”\n\n[6](ch05.html#ch05notes6). Barrett et al., “Emotional Expressions\nReconsidered.”\n\n[7](ch05.html#ch05notes7). Nilsson, “How AI Helps Recruiters.”\n\n[8](ch05.html#ch05notes8). Sánchez-Monedero and Dencik, “Datafication of the\nWorkplace,” 48; Harwell, “Face-Scanning Algorithm.”\n\n[9](ch05.html#ch05notes9). Byford, “Apple Buys Emotient.”\n\n[10](ch05.html#ch05notes10). Molnar, Robbins, and Pierson, “Cutting Edge.”\n\n[11](ch05.html#ch05notes11). Picard, “Affective Computing Group.”\n\n[12](ch05.html#ch05notes12). “Affectiva Human Perception AI Analyzes Complex\nHuman States.”\n\n[13](ch05.html#ch05notes13). Schwartz, “Don’t Look Now.”\n\n[14](ch05.html#ch05notes14). See, e.g., Nilsson, “How AI Helps Recruiters.”\n\n[15](ch05.html#ch05notes15). “Face: An AI Service That Analyzes Faces in\nImages.”\n\n[16](ch05.html#ch05notes16). “Amazon Rekognition Improves Face Analysis”;\n“Amazon Rekognition—Video and Image.”\n\n[17](ch05.html#ch05notes17). Barrett et al., “Emotional Expressions\nReconsidered,” 1.\n\n[18](ch05.html#ch05notes18). Sedgwick, Frank, and Alexander, _Shame and Its\nSisters_ , 258.\n\n[19](ch05.html#ch05notes19). Tomkins, _Affect Imagery Consciousness._\n\n[20](ch05.html#ch05notes20). Tomkins.\n\n[21](ch05.html#ch05notes21). Leys, _Ascent of Affect_ , 18.\n\n[22](ch05.html#ch05notes22). Tomkins, _Affect Imagery Consciousness_ , 23.\n\n[23](ch05.html#ch05notes23). Tomkins, 23.\n\n[24](ch05.html#ch05notes24). Tomkins, 23.\n\n[25](ch05.html#ch05notes25). For Ruth Leys, this “radical dissociation between\nfeeling and cognition” is the major reason for its attractiveness to theorists\nin the humanities, most notably Eve Kosofsky Sedgwick, who wants to revalorize\nour experiences of error or confusion into new forms of freedom. Leys, _Ascent\nof Affect_ , 35; Sedgwick, _Touching Feeling._\n\n[26](ch05.html#ch05notes26). Tomkins, _Affect Imagery Consciousness_ , 204.\n\n[27](ch05.html#ch05notes27). Tomkins, 206; Darwin, _Expression of the\nEmotions;_ Duchenne (de Boulogne), _Mécanisme de la physionomie humaine._\n\n[28](ch05.html#ch05notes28). Tomkins, 243, quoted in Leys, _Ascent of Affect_\n, 32.\n\n[29](ch05.html#ch05notes29). Tomkins, _Affect Imagery Consciousness_ , 216.\n\n[30](ch05.html#ch05notes30). Ekman, _Nonverbal Messages_ , 45.\n\n[31](ch05.html#ch05notes31). Tuschling, “Age of Affective Computing,” 186.\n\n[32](ch05.html#ch05notes32). Ekman, _Nonverbal Messages_ , 45.\n\n[33](ch05.html#ch05notes33). Ekman, 46.\n\n[34](ch05.html#ch05notes34). Ekman, 46.\n\n[35](ch05.html#ch05notes35). Ekman, 46.\n\n[36](ch05.html#ch05notes36). Ekman, 46.\n\n[37](ch05.html#ch05notes37). Ekman, 46.\n\n[38](ch05.html#ch05notes38). Ekman and Rosenberg, _What the Face Reveals_ ,\n375.\n\n[39](ch05.html#ch05notes39). Tomkins and McCarter, “What and Where Are the\nPrimary Affects?”\n\n[40](ch05.html#ch05notes40). Russell, “Is There Universal Recognition of\nEmotion from Facial Expression?” 116.\n\n[41](ch05.html#ch05notes41). Leys, _Ascent of Affect_ , 93.\n\n[42](ch05.html#ch05notes42). Ekman and Rosenberg, _What the Face Reveals_ ,\n377.\n\n[43](ch05.html#ch05notes43). Ekman, Sorenson, and Friesen, “Pan-Cultural\nElements in Facial Displays of Emotion,” 86, 87.\n\n[44](ch05.html#ch05notes44). Ekman and Friesen, “Constants across Cultures in\nthe Face and Emotion,” 128.\n\n[45](ch05.html#ch05notes45). Aristotle, _Categories_ , 70b8–13, 527.\n\n[46](ch05.html#ch05notes46). Aristotle, 805a, 27–30, 87.\n\n[47](ch05.html#ch05notes47). It would be difficult to overstate the influence\nof this work, which has since fallen into disrepute: by 1810 it went through\nsixteen German and twenty English editions. Graham, “Lavater’s Physiognomy in\nEngland,” 561.\n\n[48](ch05.html#ch05notes48). Gray, _About Face_ , 342.\n\n[49](ch05.html#ch05notes49). Courtine and Haroche, _Histoire du visage_ , 132.\n\n[50](ch05.html#ch05notes50). Ekman, “Duchenne and Facial Expression of\nEmotion.”\n\n[51](ch05.html#ch05notes51). Duchenne (de Boulogne), _Mécanisme de la\nphysionomie humaine._\n\n[52](ch05.html#ch05notes52). Clarac, Massion, and Smith, “Duchenne, Charcot\nand Babinski,” 362–63.\n\n[53](ch05.html#ch05notes53). Delaporte, _Anatomy of the Passions_ , 33.\n\n[54](ch05.html#ch05notes54). Delaporte, 48–51.\n\n[55](ch05.html#ch05notes55). Daston and Galison, _Objectivity._\n\n[56](ch05.html#ch05notes56). Darwin, _Expression of the Emotions in Man and\nAnimals_ , 12, 307.\n\n[57](ch05.html#ch05notes57). Leys, _Ascent of Affect_ , 85; Russell,\n“Universal Recognition of Emotion,” 114.\n\n[58](ch05.html#ch05notes58). Ekman and Friesen, “Nonverbal Leakage and Clues\nto Deception,” 93.\n\n[59](ch05.html#ch05notes59). Pontin, “Lie Detection.”\n\n[60](ch05.html#ch05notes60). Ekman and Friesen, “Nonverbal Leakage and Clues\nto Deception,” 94. In a footnote, Ekman and Friesen explained: “Our own\nresearch and the evidence from the neurophysiology of visual perception\nstrongly suggest that micro-expressions that are as short as one motion-\npicture frame (1/50 of a second) can be perceived. That these micro-\nexpressions are not usually seen must depend upon their being embedded in\nother expressions which distract attention, their infrequency, or some learned\nperceptual habit of ignoring fast facial expressions.”\n\n[61](ch05.html#ch05notes61). Ekman, Sorenson, and Friesen, “Pan-Cultural\nElements in Facial Displays of Emotion,” 87.\n\n[62](ch05.html#ch05notes62). Ekman, Friesen, and Tomkins, “Facial Affect\nScoring Technique,” 40.\n\n[63](ch05.html#ch05notes63). Ekman, _Nonverbal Messages_ , 97.\n\n[64](ch05.html#ch05notes64). Ekman, 102.\n\n[65](ch05.html#ch05notes65). Ekman and Rosenberg, _What the Face Reveals._\n\n[66](ch05.html#ch05notes66). Ekman, _Nonverbal Messages_ , 105.\n\n[67](ch05.html#ch05notes67). Ekman, 169.\n\n[68](ch05.html#ch05notes68). Eckman, 106; Aleksander, _Artificial Vision for\nRobots._\n\n[69](ch05.html#ch05notes69). “Magic from Invention.”\n\n[70](ch05.html#ch05notes70). Bledsoe, “Model Method in Facial Recognition.”\n\n[71](ch05.html#ch05notes71). Molnar, Robbins, and Pierson, “Cutting Edge.”\n\n[72](ch05.html#ch05notes72). Kanade, _Computer Recognition of Human Faces._\n\n[73](ch05.html#ch05notes73). Kanade, 16.\n\n[74](ch05.html#ch05notes74). Kanade, Cohn, and Tian, “Comprehensive Database\nfor Facial Expression Analysis,” 6.\n\n[75](ch05.html#ch05notes75). See Kanade, Cohn, and Tian; Lyons et al., “Coding\nFacial Expressions with Gabor Wavelets”; and Goeleven et al., “Karolinska\nDirected Emotional Faces.”\n\n[76](ch05.html#ch05notes76). Lucey et al., “Extended Cohn-Kanade Dataset\n(CK+).”\n\n[77](ch05.html#ch05notes77). McDuff et al., “Affectiva-MIT Facial Expression\nDataset (AM-FED).”\n\n[78](ch05.html#ch05notes78). McDuff et al.\n\n[79](ch05.html#ch05notes79). Ekman and Friesen, _Facial Action Coding System\n(FACS)._\n\n[80](ch05.html#ch05notes80). Foreman, “Conversation with: Paul Ekman”; Taylor,\n“2009 Time 100”; Paul Ekman Group.\n\n[81](ch05.html#ch05notes81). Weinberger, “Airport Security,” 413.\n\n[82](ch05.html#ch05notes82). Halsey, “House Member Questions $900 Million TSA\n‘SPOT’ Screening Program.”\n\n[83](ch05.html#ch05notes83). Ekman, “Life’s Pursuit”; Ekman, _Nonverbal\nMessages_ , 79–81.\n\n[84](ch05.html#ch05notes84). Mead, “Review of _Darwin and Facial Expression_\n,” 209.\n\n[85](ch05.html#ch05notes85). Tomkins, _Affect Imagery Consciousness_ , 216.\n\n[86](ch05.html#ch05notes86). Mead, “Review of _Darwin and Facial Expression_\n,” 212. See also Fridlund, “Behavioral Ecology View of Facial Displays.” Ekman\nlater conceded to many of Mead’s points. See Ekman, “Argument for Basic\nEmotions”; Ekman, _Emotions Revealed;_ and Ekman, “What Scientists Who Study\nEmotion Agree About.” Ekman also had his defenders. See Cowen et al., “Mapping\nthe Passions”; and Elfenbein and Ambady, “Universality and Cultural\nSpecificity of Emotion Recognition.”\n\n[87](ch05.html#ch05notes87). Fernández-Dols and Russell, _Science of Facial\nExpression_ , 4.\n\n[88](ch05.html#ch05notes88). Gendron and Barrett, _Facing the Past_ , 30.\n\n[89](ch05.html#ch05notes89). Vincent, “AI ‘Emotion Recognition’ Can’t Be\nTrusted.’” Disability studies scholars have also noted that assumptions about\nhow biology and bodies function can also raise concerns around bias,\nespecially when automated through technology. See Whittaker et al.,\n“Disability, Bias, and AI.”\n\n[90](ch05.html#ch05notes90). Izard, “Many Meanings/Aspects of Emotion.”\n\n[91](ch05.html#ch05notes91). Leys, _Ascent of Affect_ , 22.\n\n[92](ch05.html#ch05notes92). Leys, 92.\n\n[93](ch05.html#ch05notes93). Leys, 94.\n\n[94](ch05.html#ch05notes94). Leys, 94.\n\n[95](ch05.html#ch05notes95). Barrett, “Are Emotions Natural Kinds?” 28.\n\n[96](ch05.html#ch05notes96). Barrett, 30.\n\n[97](ch05.html#ch05notes97). See, e.g., Barrett et al., “Emotional Expressions\nReconsidered.”\n\n[98](ch05.html#ch05notes98). Barrett et al., 40.\n\n[99](ch05.html#ch05notes99). Kappas, “Smile When You Read This,” 39, emphasis\nadded.\n\n[100](ch05.html#ch05notes100). Kappas, 40.\n\n[101](ch05.html#ch05notes101). Barrett et al., 46.\n\n[102](ch05.html#ch05notes102). Barrett et al., 47–48.\n\n[103](ch05.html#ch05notes103). Barrett et al., 47, emphasis added.\n\n[104](ch05.html#ch05notes104). Apelbaum, “One Thousand and One Nights.”\n\n[105](ch05.html#ch05notes105). See, e.g., Hoft, “Facial, Speech and Virtual\nPolygraph Analysis.”\n\n[106](ch05.html#ch05notes106). Rhue, “Racial Influence on Automated\nPerceptions of Emotions.”\n\n[107](ch05.html#ch05notes107). Barrett et al., “Emotional Expressions\nReconsidered,”48.\n\n[108](ch05.html#ch05notes108). See, e.g., Connor, “Chinese School Uses Facial\nRecognition”; and Du and Maki, “AI Cameras That Can Spot Shoplifters.”\n\n### 6  \nState\n\n[1](ch06.html#ch06notes1). NOFORN stands for Not Releasable to Foreign\nNationals. “Use of the ‘Not Releasable to Foreign Nationals’ (NOFORN) Caveat.”\n\n[2](ch06.html#ch06notes2). The Five Eyes is a global intelligence alliance\ncomprising Australia, Canada, New Zealand, the United Kingdom, and the United\nStates. “Five Eyes Intelligence Oversight and Review Council.”\n\n[3](ch06.html#ch06notes3). Galison, “Removing Knowledge,” 229.\n\n[4](ch06.html#ch06notes4). Risen and Poitras, “N.S.A. Report Outlined Goals\nfor More Power”; Müller-Maguhn et al., “The NSA Breach of Telekom and Other\nGerman Firms.”\n\n[5](ch06.html#ch06notes5). FOXACID is software developed by the Office of\nTailored Access Operations, now Computer Network Operations, a cyberwarfare\nintelligence-gathering unit of the NSA.\n\n[6](ch06.html#ch06notes6). Schneier, “Attacking Tor.” Document available at\n“NSA Phishing Tactics and Man in the Middle Attacks.”\n\n[7](ch06.html#ch06notes7). Swinhoe, “What Is Spear Phishing?”\n\n[8](ch06.html#ch06notes8). “Strategy for Surveillance Powers.”\n\n[9](ch06.html#ch06notes9). Edwards, _Closed World._\n\n[10](ch06.html#ch06notes10). Edwards.\n\n[11](ch06.html#ch06notes11). Edwards, 198.\n\n[12](ch06.html#ch06notes12). Mbembé, _Necropolitics,_ 82.\n\n[13](ch06.html#ch06notes13). Bratton, _Stack_ , 151.\n\n[14](ch06.html#ch06notes14). For an excellent account of the history of the\ninternet in the United States, see Abbate, _Inventing the Internet._\n\n[15](ch06.html#ch06notes15). SHARE Foundation, “Serbian Government Is\nImplementing Unlawful Video Surveillance.”\n\n[16](ch06.html#ch06notes16). Department of International Cooperation Ministry\nof Science and Technology, “Next Generation Artificial Intelligence\nDevelopment Plan.”\n\n[17](ch06.html#ch06notes17). Chun, _Control and Freedom;_ Hu, _Prehistory of\nthe Cloud_ , 87–88.\n\n[18](ch06.html#ch06notes18). Cave and ÓhÉigeartaigh, “AI Race for Strategic\nAdvantage.”\n\n[19](ch06.html#ch06notes19). Markoff, “Pentagon Turns to Silicon Valley for\nEdge.”\n\n[20](ch06.html#ch06notes20). Brown, _Department of Defense Annual Report._\n\n[21](ch06.html#ch06notes21). Martinage, “Toward a New Offset Strategy,” 5–16.\n\n[22](ch06.html#ch06notes22). Carter, “Remarks on ‘the Path to an Innovative\nFuture for Defense’”; Pellerin, “Deputy Secretary.”\n\n[23](ch06.html#ch06notes23). The origins of U.S. military offsets can be\ntraced back to December 1952, when the Soviet Union had almost ten times more\nconventional military divisions than the United States. President Dwight\nEisenhower turned to nuclear deterrence as a way to “offset” these odds. The\nstrategy involved not only the threat of the retaliatory power of the U.S.\nnuclear forces but also accelerating the growth of the U.S. weapons stockpile,\nas well as developing long-range jet bombers, the hydrogen bomb, and\neventually intercontinental ballistic missiles. It also included increased\nreliance on espionage, sabotage, and covert operations. In the 1970s and\n1980s, U.S. military strategy turned to computational advances in analytics\nand logistics, building on the influence of such military architects as Robert\nMcNamara in search of military supremacy. This Second Offset could be seen in\nmilitary engagements like Operation Desert Storm during the Gulf War in 1991,\nwhere reconnaissance, suppression of enemy defenses, and precision-guided\nmunitions dominated how the United States not only fought the war but thought\nand spoke about it. Yet as Russia and China began to adopt these capacities\nand deploy digital networks for warfare, anxiety grew to reestablish a new\nkind of strategic advantage. See McNamara and Blight, _Wilson’s Ghost._\n\n[24](ch06.html#ch06notes24). Pellerin, “Deputy Secretary.”\n\n[25](ch06.html#ch06notes25). Gellman and Poitras, “U.S., British Intelligence\nMining Data.”\n\n[26](ch06.html#ch06notes26). Deputy Secretary of Defense to Secretaries of the\nMilitary Departments et al.\n\n[27](ch06.html#ch06notes27). Deputy Secretary of Defense to Secretaries of the\nMilitary Departments et al.\n\n[28](ch06.html#ch06notes28). Michel, _Eyes in the Sky_ , 134.\n\n[29](ch06.html#ch06notes29). Michel, 135.\n\n[30](ch06.html#ch06notes30). Cameron and Conger, “Google Is Helping the\nPentagon Build AI for Drones.”\n\n[31](ch06.html#ch06notes31). For example, Gebru et al., “Fine-Grained Car\nDetection for Visual Census Estimation.”\n\n[32](ch06.html#ch06notes32). Fang, “Leaked Emails Show Google Expected\nLucrative Military Drone AI Work.”\n\n[33](ch06.html#ch06notes33). Bergen, “Pentagon Drone Program Is Using Google\nAI.”\n\n[34](ch06.html#ch06notes34). Shane and Wakabayashi, “‘Business of War.’”\n\n[35](ch06.html#ch06notes35). Smith, “Technology and the US Military.”\n\n[36](ch06.html#ch06notes36). When the JEDI contract was ultimately awarded to\nMicrosoft, Brad Smith, the president of Microsoft, explained that the reason\nthat Microsoft won the contract was that it was seen “not just as a sales\nopportunity, but really, a very large-scale engineering project.” Stewart and\nCarlson, “President of Microsoft Says It Took Its Bid.”\n\n[37](ch06.html#ch06notes37). Pichai, “AI at Google.”\n\n[38](ch06.html#ch06notes38). Pichai. Project Maven was subsequently picked up\nby Anduril Industries, a secretive tech startup founded by Oculus Rift’s\nPalmer Luckey. Fang, “Defense Tech Startup.”\n\n[39](ch06.html#ch06notes39). Whittaker et al., _AI Now Report 2018._\n\n[40](ch06.html#ch06notes40). Schmidt quoted in Scharre et al., “Eric Schmidt\nKeynote Address.”\n\n[41](ch06.html#ch06notes41). As Suchman notes, “‘Killing people correctly’\nunder the laws of war requires adherence to the Principle of Distinction and\nthe identification of an imminent threat.” Suchman, “Algorithmic Warfare and\nthe Reinvention of Accuracy,” n. 18.\n\n[42](ch06.html#ch06notes42). Suchman.\n\n[43](ch06.html#ch06notes43). Suchman.\n\n[44](ch06.html#ch06notes44). Hagendorff, “Ethics of AI Ethics.”\n\n[45](ch06.html#ch06notes45). Brustein and Bergen, “Google Wants to Do Business\nwith the Military.”\n\n[46](ch06.html#ch06notes46). For more on why municipalities should more\ncarefully assess the risks of algorithmic platforms, see Green, _Smart Enough\nCity._\n\n[47](ch06.html#ch06notes47). Thiel, “Good for Google, Bad for America.”\n\n[48](ch06.html#ch06notes48). Steinberger, “Does Palantir See Too Much?”\n\n[49](ch06.html#ch06notes49). Weigel, “Palantir goes to the Frankfurt School.”\n\n[50](ch06.html#ch06notes50). Dilanian, “US Special Operations Forces Are\nClamoring to Use Software.”\n\n[51](ch06.html#ch06notes51). “War against Immigrants.”\n\n[52](ch06.html#ch06notes52). Alden, “Inside Palantir, Silicon Valley’s Most\nSecretive Company.”\n\n[53](ch06.html#ch06notes53). Alden, “Inside Palantir, Silicon Valley’s Most\nSecretive Company.”\n\n[54](ch06.html#ch06notes54). Waldman, Chapman, and Robertson, “Palantir Knows\nEverything about You.”\n\n[55](ch06.html#ch06notes55). Joseph, “Data Company Directly Powers Immigration\nRaids in Workplace”; Anzilotti, “Emails Show That ICE Uses Palantir Technology\nto Detain Undocumented Immigrants.”\n\n[56](ch06.html#ch06notes56). Andrew Ferguson, conversation with author, June\n21, 2019.\n\n[57](ch06.html#ch06notes57). Brayne, “Big Data Surveillance.” Brayne also\nnotes that the migration of law enforcement to intelligence was occurring even\nbefore the shift to predictive analytics, given such court decisions as Terry\nv. Ohio and Whren v. United States that made it easier for law enforcement to\ncircumvent probable cause and produced a proliferation of pretext stops.\n\n[58](ch06.html#ch06notes58). Richardson, Schultz, and Crawford, “Dirty Data,\nBad Predictions.”\n\n[59](ch06.html#ch06notes59). Brayne, “Big Data Surveillance,” 997.\n\n[60](ch06.html#ch06notes60). Brayne, 997.\n\n[61](ch06.html#ch06notes61). See, e.g., French and Browne, “Surveillance as\nSocial Regulation.”\n\n[62](ch06.html#ch06notes62). Crawford and Schultz, “AI Systems as State\nActors.”\n\n[63](ch06.html#ch06notes63). Cohen, _Between Truth and Power;_ Calo and\nCitron, “Automated Administrative State.”\n\n[64](ch06.html#ch06notes64). “Vigilant Solutions”; Maass and Lipton, “What We\nLearned.”\n\n[65](ch06.html#ch06notes65). Newman, “Internal Docs Show How ICE Gets\nSurveillance Help.”\n\n[66](ch06.html#ch06notes66). England, “UK Police’s Facial Recognition System.”\n\n[67](ch06.html#ch06notes67). Scott, _Seeing Like a State._\n\n[68](ch06.html#ch06notes68). Haskins, “How Ring Transmits Fear to American\nSuburbs.”\n\n[69](ch06.html#ch06notes69). Haskins, “Amazon’s Home Security Company.”\n\n[70](ch06.html#ch06notes70). Haskins.\n\n[71](ch06.html#ch06notes71). Haskins. “Amazon Requires Police to Shill\nSurveillance Cameras.”\n\n[72](ch06.html#ch06notes72). Haskins, “Amazon Is Coaching Cops.”\n\n[73](ch06.html#ch06notes73). Haskins.\n\n[74](ch06.html#ch06notes74). Haskins.\n\n[75](ch06.html#ch06notes75). Hu, _Prehistory of the Cloud_ , 115.\n\n[76](ch06.html#ch06notes76). Hu, 115.\n\n[77](ch06.html#ch06notes77). Benson, “‘Kill ’Em and Sort It Out Later,’” 17.\n\n[78](ch06.html#ch06notes78). Hajjar, “Lawfare and Armed Conflicts,” 70.\n\n[79](ch06.html#ch06notes79). Scahill and Greenwald, “NSA’s Secret Role in the\nU.S. Assassination Program.”\n\n[80](ch06.html#ch06notes80). Cole, “‘We Kill People Based on Metadata.’”\n\n[81](ch06.html#ch06notes81). Priest, “NSA Growth Fueled by Need to Target\nTerrorists.”\n\n[82](ch06.html#ch06notes82). Gibson quoted in Ackerman, “41 Men Targeted but\n1,147 People Killed.”\n\n[83](ch06.html#ch06notes83). Tucker, “Refugee or Terrorist?”\n\n[84](ch06.html#ch06notes84). Tucker.\n\n[85](ch06.html#ch06notes85). O’Neil, _Weapons of Math Destruction_ , 288–326.\n\n[86](ch06.html#ch06notes86). Fourcade and Healy, “Seeing Like a Market.”\n\n[87](ch06.html#ch06notes87). Eubanks, _Automating Inequality._\n\n[88](ch06.html#ch06notes88). Richardson, Schultz, and Southerland, “Litigating\nAlgorithms,” 19.\n\n[89](ch06.html#ch06notes89). Richardson, Schultz, and Southerland, 23.\n\n[90](ch06.html#ch06notes90). Agre, _Computation and Human Experience_ , 240.\n\n[91](ch06.html#ch06notes91). Bratton, _Stack_ , 140.\n\n[92](ch06.html#ch06notes92). Hu, _Prehistory of the Cloud_ , 89.\n\n[93](ch06.html#ch06notes93). Nakashima and Warrick, “For NSA Chief, Terrorist\nThreat Drives Passion.”\n\n[94](ch06.html#ch06notes94). Document available at Maass, “Summit Fever.”\n\n[95](ch06.html#ch06notes95). The future of the Snowden archive itself is\nuncertain. In March 2019, it was announced that the _Intercept_ —the\npublication that Glenn Greenwald established with Laura Poitras and Jeremy\nScahill after they shared the Pulitzer Prize for their reporting on the\nSnowden materials—was no longer going to fund the Snowden archive. Tani,\n“Intercept Shuts Down Access to Snowden Trove.”\n\n### Conclusion\n\n[1](concl.html#conclnotes1). Silver et al., “Mastering the Game of Go without\nHuman Knowledge.”\n\n[2](concl.html#conclnotes2). Silver et al., 357.\n\n[3](concl.html#conclnotes3). Full talk at the Artificial Intelligence Channel:\n_Demis Hassabis, DeepMind—Learning from First Principles._ See also Knight,\n“Alpha Zero’s ‘Alien’ Chess Shows the Power.”\n\n[4](concl.html#conclnotes4). _Demis Hassabis, DeepMind—Learning from First\nPrinciples._\n\n[5](concl.html#conclnotes5). For more on the myths of “magic” in AI, see Elish\nand boyd, “Situating Methods in the Magic of Big Data and AI.”\n\n[6](concl.html#conclnotes6). Meredith Broussard notes that playing games has\nbeen dangerously conflated with intelligence. She cites the programmer George\nV. Neville-Neil, who argues: “We have had nearly 50 years of human/computer\ncompetition in the game of chess, but does this mean that any of those\ncomputers are intelligent? No, it does not—for two reasons. The first is that\nchess is not a test of intelligence; it is the test of a particular skill—the\nskill of playing chess. If I could beat a Grandmaster at chess and yet not be\nable to hand you the salt at the table when asked, would I be intelligent? The\nsecond reason is that thinking chess was a test of intelligence was based on a\nfalse cultural premise that brilliant chess players were brilliant minds, more\ngifted than those around them. Yes, many intelligent people excel at chess,\nbut chess, or any other single skill, does not denote intelligence.”\nBroussard, _Artificial Unintelligence_ , 206.\n\n[7](concl.html#conclnotes7). Galison, “Ontology of the Enemy.”\n\n[8](concl.html#conclnotes8). Campolo and Crawford, “Enchanted Determinism.”\n\n[9](concl.html#conclnotes9). Bailey, “Dimensions of Rhetoric in Conditions of\nUncertainty,” 30.\n\n[10](concl.html#conclnotes10). Bostrom, _Superintelligence._\n\n[11](concl.html#conclnotes11). Bostrom.\n\n[12](concl.html#conclnotes12). Strand, “Keyword: Evil,” 64–65.\n\n[13](concl.html#conclnotes13). Strand, 65.\n\n[14](concl.html#conclnotes14). Hardt and Negri, _Assembly_ , 116, emphasis\nadded.\n\n[15](concl.html#conclnotes15). Wakabayashi, “Google’s Shadow Work Force.”\n\n[16](concl.html#conclnotes16). Quoted in McNeil, “Two Eyes See More Than\nNine,” 23.\n\n[17](concl.html#conclnotes17). On the idea of data as capital, see Sadowski,\n“When Data Is Capital.”\n\n[18](concl.html#conclnotes18). Harun Farocki discussed in Paglen, “Operational\nImages.”\n\n[19](concl.html#conclnotes19). For a summary, see Heaven, “Why Faces Don’t\nAlways Tell the Truth.”\n\n[20](concl.html#conclnotes20). Nietzsche, _Sämtliche Werke_ , 11:506.\n\n[21](concl.html#conclnotes21). Wang and Kosinski, “Deep Neural Networks Are\nMore Accurate Than Humans”; Kleinberg et al., “Human Decisions and Machine\nPredictions”; Crosman, “Is AI a Threat to Fair Lending?”; Seo et al.,\n“Partially Generative Neural Networks.”\n\n[22](concl.html#conclnotes22). Pugliese, “Death by Metadata.”\n\n[23](concl.html#conclnotes23). Suchman, “Algorithmic Warfare and the\nReinvention of Accuracy.”\n\n[24](concl.html#conclnotes24). Simmons, “Rekor Software Adds License Plate\nReader Technology.”\n\n[25](concl.html#conclnotes25). Lorde, _Master’s Tools._\n\n[26](concl.html#conclnotes26). Schaake, “What Principles Not to Disrupt.”\n\n[27](concl.html#conclnotes27). Jobin, Ienca, and Vayena, “Global Landscape of\nAI Ethics Guidelines.”\n\n[28](concl.html#conclnotes28). Mattern, “Calculative Composition,” 572.\n\n[29](concl.html#conclnotes29). For more on why AI ethics frameworks are\nlimited in effectiveness, see Crawford et al., _AI Now 2019 Report._\n\n[30](concl.html#conclnotes30). Mittelstadt, “Principles Alone Cannot Guarantee\nEthical AI.” See also Metcalf, Moss, and boyd, “Owning Ethics.”\n\n[31](concl.html#conclnotes31). For recent scholarship that addresses important\npractical steps to do this without replicating forms of extraction and harm,\nsee Costanza-Chock, _Design Justice._\n\n[32](concl.html#conclnotes32). Winner, _The Whale and the Reactor_ , 9.\n\n[33](concl.html#conclnotes33). Mbembé, _Critique of Black Reason_ , 3.\n\n[34](concl.html#conclnotes34). Bangstad et al., “Thoughts on the Planetary.”\n\n[35](concl.html#conclnotes35). Haraway, _Simians, Cyborgs, and Women_ , 161.\n\n[36](concl.html#conclnotes36). Mohamed, Png, and Isaac, “Decolonial AI,” 405.\n\n[37](concl.html#conclnotes37). “Race after Technology, Ruha Benjamin.”\n\n[38](concl.html#conclnotes38). Bangstad et al., “Thoughts on the Planetary.”\n\n### Coda\n\n[1](code.html#codenotes1). _Blue Origin’s Mission._\n\n[2](code.html#codenotes2). _Blue Origin’s Mission._\n\n[3](code.html#codenotes3). Powell, “Jeff Bezos Foresees a Trillion People.”\n\n[4](code.html#codenotes4). Bezos, _Going to Space to Benefit Earth._\n\n[5](code.html#codenotes5). Bezos.\n\n[6](code.html#codenotes6). Foer, “Jeff Bezos’s Master Plan.”\n\n[7](code.html#codenotes7). Foer.\n\n[8](code.html#codenotes8). “Why Asteroids.”\n\n[9](code.html#codenotes9). Welch, “Elon Musk.”\n\n[10](code.html#codenotes10). Cuthbertson, “Elon Musk Really Wants to ‘Nuke\nMars.’”\n\n[11](code.html#codenotes11). Rein, Tamayo, and Vokrouhlicky, “Random Walk of\nCars.”\n\n[12](code.html#codenotes12). Gates, “Bezos’ Blue Origin Seeks Tax Incentives.”\n\n[13](code.html#codenotes13). Marx, “Instead of Throwing Money at the Moon”;\nO’Neill, _High Frontier._\n\n[14](code.html#codenotes14). “Our Mission.”\n\n[15](code.html#codenotes15). Davis, “Gerard K. O’Neill on Space Colonies.”\n\n[16](code.html#codenotes16). Meadows et al., _Limits to Growth._\n\n[17](code.html#codenotes17). Scharmen, _Space Settlements_ , 216. In recent\nyears, scholars have suggested that the Club of Rome’s models were overly\noptimistic, underestimating the rapid rate of extraction and resource\nconsumption worldwide and the climate implications of greenhouse gases and\nindustrial waste heat. See Turner, “Is Global Collapse Imminent?”\n\n[18](code.html#codenotes18). The case for a no-growth model that involves\nstaying on the planet has been made by many academics in the limits to growth\nmovement. See, e.g., Trainer, _Renewable Energy Cannot Sustain a Consumer\nSociety._\n\n[19](code.html#codenotes19). Scharmen, _Space Settlements_ , 91.\n\n[20](code.html#codenotes20). One wonders how the Bezos mission would differ\nhad he been inspired instead by the science fiction author Philip K. Dick, who\nwrote the short story “Autofac” in 1955. In it, human survivors of an\napocalyptic war are left on Earth with the “autofacs”—autonomous, self-\nreplicating factory machines. The autofacs had been tasked with producing\nconsumer goods in prewar society but could no longer stop, consuming the\nplanet’s resources and threatening the survival of the last people left. The\nonly way to survive was to trick the artificial intelligence machines to fight\nagainst each other over a critical element they need for manufacturing: the\nrare earth element tungsten. It seems to succeed, and wild vines begin to grow\nthroughout the factories, and farmers can return to the land. Only later do\nthey realize that the autofacs had sought more resources deep in Earth’s core\nand would soon launch thousands of self-replicating “seeds” to mine the rest\nof the galaxy. Dick, “Autofac.”\n\n[21](code.html#codenotes21). NASA, “Outer Space Treaty of 1967.”\n\n[22](code.html#codenotes22). U.S. Commercial Space Launch Competitiveness\nAct.”\n\n[23](code.html#codenotes23). Wilson, “Top Lobbying Victories of 2015.”\n\n[24](code.html#codenotes24). Shaer, “Asteroid Miner’s Guide to the Galaxy.”\n\n[25](code.html#codenotes25). As Mark Andrejevic writes, “The promise of\ntechnological immortality is inseparable from automation, which offers to\nsupplant human limitations at every turn.” Andrejevic, _Automated Media_ , 1.\n\n[26](code.html#codenotes26). Reichhardt, “First Photo from Space.”\n\n[27](code.html#codenotes27). See, e.g., Pulitzer Prize–winning journalist\nWayne Biddle’s account of von Braun as a war criminal who participated in the\nbrutal treatment of slave laborers under the Nazi regime. Biddle, _Dark Side\nof the Moon._\n\n[28](code.html#codenotes28). Grigorieff, “Mittelwerk/Mittelbau/Camp Dora.”\n\n[29](code.html#codenotes29). Ward, _Dr. Space._\n\n[30](code.html#codenotes30). Keates, “Many Places Amazon CEO Jeff Bezos Calls\nHome.”\n\n[31](code.html#codenotes31). Center for Land Use Interpretation, “Figure 2\nRanch, Texas.”\n\n\n## Bibliography\n\nAbbate, Janet. _Inventing the Internet._ Cambridge, Mass.: MIT Press, 1999.\n\nAbraham, David S. _The Elements of Power: Gadgets, Guns, and the Struggle for\na Sustainable Future in the Rare Metal Age._ New Haven: Yale University Press,\n2017.\n\nAchtenberg, Emily. “Bolivia Bets on State-Run Lithium Industry.” NACLA,\nNovember 15, 2010. <https://nacla.org/news/bolivia-bets-state-run-lithium-\nindustry>.\n\nAckerman, Spencer. “41 Men Targeted but 1,147 People Killed: US Drone\nStrikes—the Facts on the Ground.” _Guardian_ , November 24, 2014.\n<https://www.theguardian.com/us-news/2014/nov/24/-sp-us-drone-strikes-\nkill-1147>.\n\nAdams, Guy. “Lost at Sea: On the Trail of Moby-Duck.” _Independent_ , February\n27, 2011. <https://www.independent.co.uk/environment/nature/lost-at-sea-on-\nthe-trail-of-moby-duck-2226788.html>.\n\n“Advertising on Twitter.” Twitter for Business.\n<https://business.twitter.com/en/Twitter-ads-signup.html>.\n\n“Affectiva Human Perception AI Analyzes Complex Human States.” Affectiva.\n<https://www.affectiva.com/>.\n\nAgre, Philip E. _Computation and Human Experience._ Cambridge: Cambridge\nUniversity Press, 1997.\n\nAgüera y Arcas, Blaise, Margaret Mitchell, and Alexander Todorov.\n“Physiognomy’s New Clothes.” _Medium: Artificial Intelligence_ (blog), May 7,\n2017. <https://medium.com/@blaisea/physiognomys-new-clothes-f2d4b59fdd6a>.\n\n“AI and Compute.” Open AI, May 16, 2018. <https://openai.com/blog/ai-and-\ncompute/>.\n\nAlden, William. “Inside Palantir, Silicon Valley’s Most Secretive Company.”\nBuzzfeed News, May 6, 2016,\n<https://www.buzzfeednews.com/article/williamalden/inside-palantir-silicon-\nvalleys-most-secretive-company>.\n\nAjunwa, Ifeoma, Kate Crawford, and Jason Schultz. “Limitless Worker\nSurveillance.” _California Law Review_ 105, no. 3 (2017): 735–76.\n<https://doi.org/10.15779/z38br8mf94>.\n\nAjunwa, Ifeoma, and Daniel Greene. “Platforms at Work: Automated Hiring\nPlatforms and other new Intermediaries in the Organization of Work.” In _Work\nand Labor in the Digital Age_ , edited by Steven P. Vallas and Anne\nKovalainen, 66–91. Bingley, U.K.: Emerald, 2019.\n\n“Albemarle (NYSE:ALB) Could Be Targeting These Nevada Lithium Juniors.”\nSmallCapPower, September 9, 2016. <https://smallcappower.com/top-\nstories/albemarle-nysealb-targeting-nevada-lithium-juniors/>.\n\nAlden, William. “Inside Palantir, Silicon Valley’s Most Secretive Company.”\n_Buzzfeed News_ , May 6, 2016.\n<https://www.buzzfeednews.com/article/williamalden/inside-palantir-silicon-\nvalleys-most-secretive-company>.\n\nAleksander, Igor, ed. _Artificial Vision for Robots._ Boston: Springer US,\n1983.\n\n“Amazon.Com Market Cap | AMZN.” YCharts. <https://ycharts.com/companies/AMZN/market_cap>.\n\n“Amazon Rekognition Improves Face Analysis.” Amazon Web Services, August 12,\n2019. <https://aws.amazon.com/about-aws/whats-new/2019/08/amazon-rekognition-\nimproves-face-analysis/>.\n\n“Amazon Rekognition—Video and Image—AWS.” Amazon Web Services.\n<https://aws.amazon.com/rekognition/>.\n\nAnanny, Mike, and Kate Crawford. “Seeing without Knowing: Limitations of the\nTransparency Ideal and Its Application to Algorithmic Accountability.” _New\nMedia and Society_ 20, no. 3 (2018): 973–89.\n<https://doi.org/10.1177/1461444816676645>.\n\nAnderson, Warwick. _The Collectors of Lost Souls: Turning Kuru Scientists into\nWhitemen._ Updated ed. Baltimore: Johns Hopkins University Press, 2019.\n\nAndrae, Anders A. E., and Tomas Edler. “On Global Electricity Usage of\nCommunication Technology: Trends to 2030.” _Challenges_ 6, no. 1 (2015):\n117–57. <https://www.doi.org/10.3390/challe6010117>.\n\nAndrejevic, Mark. _Automated Media._ New York: Routledge, 2020.\n\nAngwin, Julia, et al. “Dozens of Companies Are Using Facebook to Exclude Older\nWorkers from Job Ads.” ProPublica, December 20, 2017.\n<https://www.propublica.org/article/facebook-ads-age-discrimination-\ntargeting>.\n\nAngwin, Julia, et al. “Machine Bias.” _ProPublica_ , May 23, 2016.\n<https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-\nsentencing>.\n\nAnzilotti, Eillie. “Emails Show That ICE Uses Palantir Technology to Detain\nUndocumented Immigrants,” _FastCompany_ (blog), July 16, 2019.\n<https://www.fastcompany.com/90377603/ice-uses-palantir-tech-to-detain-\nimmigrants-wnyc-report>.\n\nApelbaum, Yaacov. “One Thousand and One Nights and Ilhan Omar’s Biographical\nEngineering.” _The Illustrated Primer_ (blog), August 13, 2019.\n<https://apelbaum.wordpress.com/2019/08/13/one-thousand-and-one-nights-and-\nilhan-omars-biographical-engineering/>.\n\nApple. “Apple Commits to Be 100 Percent Carbon Neutral for Its Supply Chain\nand Products by 2030,” July 21, 2020.\n<https://www.apple.com/au/newsroom/2020/07/apple-commits-to-be-100-percent-\ncarbon-neutral-for-its-supply-chain-and-products-by-2030/>.\n\nApple. _Supplier Responsibility: 2018 Progress Report._ Cupertino, Calif.:\nApple, n.d. <https://www.apple.com/supplier-\nresponsibility/pdf/Apple_SR_2018_Progress_Report.pdf>.\n\nArboleda, Martin. _Planetary Mine: Territories of Extraction under Late\nCapitalism._ London: Verso, 2020.\n\nAristotle. _The Categories: On Interpretation._ Translated by Harold Percy\nCooke and Hugh Tredennick. Loeb Classical Library 325. Cambridge, Mass.:\nHarvard University Press, 1938.\n\nAslam, Salman. “Facebook by the Numbers (2019): Stats, Demographics & Fun\nFacts.” Omnicore, January 6, 2020. <https://www.omnicoreagency.com/facebook-\nstatistics/>.\n\nAyogu, Melvin, and Zenia Lewis. “Conflict Minerals: An Assessment of the Dodd-\nFrank Act.” Brookings Institution, October 3,2011.\n<https://www.brookings.edu/opinions/conflict-minerals-an-assessment-of-the-\ndodd-frank-act/>.\n\nAytes, Ayhan. “Return of the Crowds: Mechanical Turk and Neoliberal States of\nException.” In _Digital Labor: The Internet as Playground and Factory_ ,\nedited by Trebor Scholz. New York: Routledge, 2013.\n\nBabbage, Charles. _On the Economy of Machinery and Manufactures_ [1832].\nCambridge: Cambridge University Press, 2010.\n\nBabich, Babette E. _Nietzsche’s Philosophy of Science: Reflecting Science on\nthe Ground of Art and Life._ Albany: State University of New York Press, 1994.\n\nBailey, F. G. “Dimensions of Rhetoric in Conditions of Uncertainty.” In\n_Politically Speaking: Cross-Cultural Studies of Rhetoric_ , edited by Robert\nPaine, 25–38. Philadelphia: ISHI Press, 1981.\n\nBaker, Janet M., et al. “Research Developments and Directions in Speech\nRecognition and Understanding, Part 1.” _IEEE_ , April 2009.\n<https://dspace.mit.edu/handle/1721.1/51891>.\n\nBangstad, Sindre, et al. “Thoughts on the Planetary: An Interview with Achille\nMbembé.” _New Frame_ , September 5, 2019. <https://www.newframe.com/thoughts-\non-the-planetary-an-interview-with-achille-mbembe/>.\n\nBarrett, Lisa Feldman. “Are Emotions Natural Kinds?” _Perspectives on\nPsychological Science_ 1, no. 1 (2006): 28–58.\n<https://doi.org/10.1111/j.1745-6916.2006.00003.x>.\n\nBarrett, Lisa Feldman, et al. “Emotional Expressions Reconsidered: Challenges\nto Inferring Emotion from Human Facial Movements.” _Psychological Science in\nthe Public Interest_ 20, no. 1 (2019): 1–68.\n<https://doi.org/10.1177/1529100619832930>.\n\n“Bayan Obo Deposit, . . . Inner Mongolia, China.” Mindat.org.\n<https://www.mindat.org/loc-720.html>.\n\nBayer, Ronald. _Homosexuality and American Psychiatry: The Politics of\nDiagnosis._ Princeton, N.J.: Princeton University Press, 1987.\n\nBechmann, Anja, and Geoffrey C. Bowker. “Unsupervised by Any Other Name:\nHidden Layers of Knowledge Production in Artificial Intelligence on Social\nMedia.” _Big Data and Society_ 6, no. 1 (2019): 205395171881956.\n<https://doi.org/10.1177/2053951718819569>.\n\nBeck, Julie. “Hard Feelings: Science’s Struggle to Define Emotions.”\n_Atlantic_ , February 24, 2015.\n<https://www.theatlantic.com/health/archive/2015/02/hard-feelings-sciences-\nstruggle-to-define-emotions/385711/>.\n\nBehrmann, Elisabeth, Jack Farchy, and Sam Dodge. “Hype Meets Reality as\nElectric Car Dreams Run into Metal Crunch.” _Bloomberg_ , January 11, 2018.\n<https://www.bloomberg.com/graphics/2018-cobalt-batteries/>.\n\nBelkhir, L., and A. Elmeligi. “Assessing ICT Global Emissions Footprint:\nTrends to 2040 and Recommendations.” _Journal of Cleaner Production_ 177\n(2018): 448–63.\n\nBenjamin, Ruha. _Race after Technology: Abolitionist Tools for the New Jim\nCode._ Cambridge: Polity, 2019.\n\nBenson, Kristina. “‘Kill ’Em and Sort It Out Later’: Signature Drone Strikes\nand International Humanitarian Law.” _Pacific McGeorge Global Business and\nDevelopment Law Journal_ 27, no. 1 (2014): 17–51.\n<https://www.mcgeorge.edu/documents/Publications/02_Benson_27_1.pdf>.\n\nBenthall, Sebastian, and Bruce D. Haynes. “Racial Categories in Machine\nLearning.” In _FAT* ’19: Proceedings of the Conference on Fairness,\nAccountability, and Transparency_ , 289–98. New York: ACM Press, 2019.\n<https://dl.acm.org/doi/10.1145/3287560.3287575>.\n\nBerg, Janine, et al. _Digital Labour Platforms and the Future of Work: Towards\nDecent Work in the Online World._ Geneva: International Labor Organization,\n2018.\n<https://www.ilo.org/wcmsp5/groups/public/---dgreports/---dcomm/---publ/documents/publication/wcms_645337.pdf>.\n\nBergen, Mark.“Pentagon Drone Program Is Using Google AI.” _Bloomberg_ , March\n6, 2018. <https://www.bloomberg.com/news/articles/2018-03-06/google-ai-used-\nby-pentagon-drone-program-in-rare-military-pilot>.\n\nBerman, Sanford. _Prejudices and Antipathies: A Tract on the LC Subject Heads\nconcerning People._ Metuchen, N.J.: Scarecrow Press, 1971.\n\nBezos, Jeff. _Going to Space to Benefit Earth._ Video, May 9, 2019.\n[https://www.youtube.com/watch?v=GQ98hGUe6FM&](https://www.youtube.com/watch?v=GQ98hGUe6FM&).\n\nBiddle, Wayne. _Dark Side of the Moon: Wernher von Braun, the Third Reich, and\nthe Space Race._ New York: W. W. Norton, 2012.\n\nBlack, Edwin. _IBM and the Holocaust: The Strategic Alliance between Nazi\nGermany and America’s Most Powerful Corporation._ Expanded ed. Washington,\nD.C.: Dialog Press, 2012.\n\nBledsoe, W. W. “The Model Method in Facial Recognition.” Technical report, PRI\n15. Palo Alto, Calif.: Panoramic Research, 1964.\n\nBloomfield, Anne B. “A History of the California Historical Society’s New\nMission Street Neighborhood.” _California History_ 74, no. 4 (1995–96):\n372–93.\n\nBlue, Violet. “Facebook Patents Tech to Determine Social Class.” _Engadget_ ,\nFebruary 9, 2018. <https://www.engadget.com/2018-02-09-facebook-patents-tech-\nto-determine-social-class.html>.\n\n_Blue Origin’s Mission._ Blue Origin. Video, February 1, 2019.\n<https://www.youtube.com/watch?v=1YOL89kY8Og>.\n\nBond, Charles F., Jr. “Commentary: A Few Can Catch a Liar, Sometimes: Comments\non Ekman and O’Sullivan (1991), as Well as Ekman, O’Sullivan, and Frank\n(1999).” _Applied Cognitive Psychology_ 22, no. 9 (2008): 1298–1300.\n<https://doi.org/10.1002/acp.1475>.\n\nBorges, Jorge Luis. _Collected Fictions._ Translated by Andrew Hurley. New\nYork: Penguin Books, 1998.\n\n———. “John Wilkins’ Analytical Language.” In _Borges: Selected Non-Fictions_ ,\nedited by Eliot Weinberger. New York: Penguin Books, 2000.\n\n———. _The Library of Babel._ Translated by Andrew Hurley. Boston: David R.\nGodine, 2000.\n\nBostrom, Nick. _Superintelligence: Paths, Dangers, Strategies._ Oxford: Oxford\nUniversity Press, 2014.\n\nBouche, Teryn, and Laura Rivard. “America’s Hidden History: The Eugenics\nMovement.” Scitable, September 18, 2014.\n<https://www.nature.com/scitable/forums/genetics-generation/america-s-hidden-\nhistory-the-eugenics-movement-123919444/>.\n\nBowker, Geoffrey C. _Memory Practices in the Sciences._ Cambridge, Mass.: MIT\nPress, 2005.\n\nBowker, Geoffrey C., and Susan Leigh Star. _Sorting Things Out: Classification\nand Its Consequences._ Cambridge, Mass.: MIT Press, 1999.\n\nBratton, Benjamin H. _The Stack: On Software and Sovereignty._ Cambridge,\nMass.: MIT Press, 2015.\n\nBraverman, Harry. _Labor and Monopoly Capital: The Degradation of Work in the\nTwentieth Century._ 25th anniversary ed. New York: Monthly Review Press, 1998.\n\nBrayne, Sarah. “Big Data Surveillance: The Case of Policing.” _American\nSociological Review_ 82, no. 5 (2017): 977–1008.\n<https://doi.org/10.1177/0003122417725865>.\n\nBrechin, Gray. _Imperial San Francisco: Urban Power, Earthly Ruin._ Berkeley:\nUniversity of California Press, 2007.\n\nBrewer, Eric. “Spanner, TrueTime and the CAP Theorem.” Infrastructure: Google,\nFebruary 14, 2017. <https://storage.googleapis.com/pub-tools-public-\npublication-data/pdf/45855.pdf>.\n\nBridle, James. “Something Is Wrong on the Internet.” _Medium_ (blog), November\n6, 2017. <https://medium.com/@jamesbridle/something-is-wrong-on-the-\ninternet-c39c471271d2>.\n\nBroussard, Meredith. _Artificial Unintelligence: How Computers Misunderstand\nthe World._ Cambridge, Mass.: MIT Press, 2018.\n\nBrown, Harold. _Department of Defense Annual Report: Fiscal Year 1982._ Report\nAD-A-096066/6. Washington, D.C., January 19, 1982.\n<https://history.defense.gov/Portals/70/Documents/annual_reports/1982_DoD_AR.pdf?ver=2014-06-24-150904-113>.\n\nBrown, Peter, and Robert Mercer. “Oh, Yes, Everything’s Right on Schedule,\nFred.” Lecture, Twenty Years of Bitext Workshop, Empirical Methods in Natural\nLanguage Processing Conference, Seattle, Wash., October 2013.\n<http://cs.jhu.edu/~post/bitext>.\n\nBrowne, Simone. _Dark Matters: On the Surveillance of Blackness._ Durham,\nN.C.: Duke University Press, 2015.\n\n———. “Digital Epidermalization: Race, Identity and Biometrics.” _Critical\nSociology_ 36, no. 1 (January 2010): 131–50.\n\nBrustein, Joshua, and Mark Bergen. “Google Wants to Do Business with the\nMilitary—Many of Its Employees Don’t.” _Bloomberg News_ , November 21, 2019.\n<https://www.bloomberg.com/features/2019-google-military-contract-dilemma/>.\n\nBullis, Kevin. “Lithium-Ion Battery.” _MIT Technology Review_ , June 19, 2012.\n<https://www.technologyreview.com/s/428155/lithium-ion-battery/>.\n\nBuolamwini, Joy, and Timnit Gebru. “Gender Shades: Intersectional Accuracy\nDisparities in Commercial Gender Classification.” _Proceedings of the First\nConference on Fairness, Accountability and Transparency_ , _PLMR_ 81 (2018):\n77–91. <http://proceedings.mlr.press/v81/buolamwini18a.html>.\n\nBurke, Jason. “Congo Violence Fuels Fears of Return to 90s Bloodbath.”\n_Guardian_ , June 30,2017.\n<https://www.theguardian.com/world/2017/jun/30/congo-violence-fuels-fears-of-\nreturn-to-90s-bloodbath>.\n\nBush, Vannevar. “As We May Think.” _Atlantic_ , July 1945.\n<https://www.theatlantic.com/magazine/archive/1945/07/as-we-may-\nthink/303881/>.\n\nBusiness Council for Sustainable Energy. “2019 Sustainable Energy in America\nFactbook.” BCSE, February 11, 2019. <https://www.bcse.org/wp-\ncontent/uploads/2019-Sustainable-Energy-in-America-Factbook.pdf>.\n\nByford, Sam. “Apple Buys Emotient, a Company That Uses AI to Read Emotions.”\n_The Verge_ , January 7, 2016.\n<https://www.theverge.com/2016/1/7/10731232/apple-emotient-ai-startup-\nacquisition>.\n\n“The CalGang Criminal Intelligence System.” Sacramento: California State\nAuditor, Report 2015-130, August 2016.\n<https://www.auditor.ca.gov/pdfs/reports/2015-130.pdf>.\n\nCalo, Ryan, and Danielle Citron. “The Automated Administrative State: A Crisis\nof Legitimacy” (March 9, 2020). _Emory Law Journal_ (forthcoming). Available\nat SSRN: <https://ssrn.com/abstract=3553590>.\n\nCameron, Dell, and Kate Conger. “Google Is Helping the Pentagon Build AI for\nDrones.” _Gizmodo_ , March 6, 2018. <https://gizmodo.com/google-is-helping-\nthe-pentagon-build-ai-for-drones-1823464533>.\n\nCampolo, Alexander, and Kate Crawford. “Enchanted Determinism: Power without\nResponsibility in Artificial Intelligence.” _Engaging Science, Technology, and\nSociety_ 6 (2020): 1–19. <https://doi.org/10.17351/ests2020.277>.\n\nCanales, Jimena. _A Tenth of a Second: A History._ Chicago: University of\nChicago Press, 2010.\n\nCarey, James W. “Technology and Ideology: The Case of the Telegraph.”\n_Prospects_ 8 (1983): 303–25. <https://doi.org/10.1017/S0361233300003793>.\n\nCarlisle, Nate. “NSA Utah Data Center Using More Water.” _Salt Lake Tribune_ ,\nFebruary 2, 2015.\n[https://archive.sltrib.com/article.php?id=2118801&itype=CMSID](https://archive.sltrib.com/article.php?id=2118801&itype=CMSID).\n\n———. “Shutting Off NSA’s Water Gains Support in Utah Legislature.” _Salt Lake\nTribune_ , November 20, 2014.\n[https://archive.sltrib.com/article.php?id=1845843&itype=CMSID](https://archive.sltrib.com/article.php?id=1845843&itype=CMSID).\n\nCarter, Ash. _“Remarks on ‘the Path to an Innovative Future for Defense’ (CSIS\nThird Offset Strategy Conference).”_ Washington, D.C.: U.S. Department of\nDefense, October 28, 2016.\n<https://www.defense.gov/Newsroom/Speeches/Speech/Article/990315/remarks-on-\nthe-path-to-an-innovative-future-for-defense-csis-third-offset-strat/>.\n\nCave, Stephen, and Seán S. ÓhÉigeartaigh. “An AI Race for Strategic Advantage:\nRhetoric and Risks.” In _Proceedings of the 2018 AAAI/ACM Conference on AI,\nEthics, and Society_ , 36–40.\n<https://dl.acm.org/doi/10.1145/3278721.3278780>.\n\nCenter for Land Use Interpretation, “Figure 2 Ranch, Texas,”\n<http://www.clui.org/ludb/site/figure-2-ranch>.\n\nCetina, Karin Knorr. _Epistemic Cultures: How the Sciences Make Knowledge._\nCambridge, Mass.: Harvard University Press, 1999.\n\nChamps, Emmanuelle de. “The Place of Jeremy Bentham’s Theory of Fictions in\nEighteenth-Century Linguistic Thought.” _Journal of Bentham Studies_ 2 (1999).\n<https://doi.org/10.14324/111.2045-757X.011>.\n\n“Chinese Lithium Giant Agrees to Three-Year Pact to Supply Tesla.” Bloomberg,\nSeptember 21, 2018.\n<https://www.industryweek.com/leadership/article/22026386/chinese-lithium-\ngiant-agrees-to-threeyear-pact-to-supply-tesla>.\n\nChinoy, Sahil. “Opinion: The Racist History behind Facial Recognition.” _New\nYork Times_ , July 10, 2019.\n<https://www.nytimes.com/2019/07/10/opinion/facial-recognition-race.html>.\n\nChun, Wendy Hui Kyong. _Control and Freedom: Power and Paranoia in the Age of\nFiber Optics_ , Cambridge, Mass: MIT Press, 2005.\n\nCitton, Yves. _The Ecology of Attention._ Cambridge: Polity, 2017.\n\nClarac, François, Jean Massion, and Allan M. Smith. “Duchenne, Charcot and\nBabinski, Three Neurologists of La Salpetrière Hospital, and Their\nContribution to Concepts of the Central Organization of Motor Synergy.”\n_Journal of Physiology–Paris_ 103, no. 6 (2009): 361–76.\n<https://doi.org/10.1016/j.jphysparis.2009.09.001>.\n\nClark, Nicola, and Simon Wallis. “Flamingos, Salt Lakes and Volcanoes: Hunting\nfor Evidence of Past Climate Change on the High Altiplano of Bolivia.”\n_Geology Today_ 33, no. 3 (2017): 101–7. <https://doi.org/10.1111/gto.12186>.\n\nClauss, Sidonie. “John Wilkins’ Essay toward a Real Character: Its Place in\nthe Seventeenth-Century Episteme.” _Journal of the History of Ideas_ 43, no. 4\n(1982): 531–53. <https://doi.org/10.2307/2709342>.\n\n“‘Clever Hans’ Again: Expert Commission Decides That the Horse Actually\nReasons.’” _New York Times_ , October 2, 1904.\n<https://timesmachine.nytimes.com/timesmachine/1904/10/02/120289067.pdf>.\n\nCochran, Susan D., et al. “Proposed Declassification of Disease Categories\nRelated to Sexual Orientation in the International Statistical Classification\nof Diseases and Related Health Problems (ICD-11).” _Bulletin of the World\nHealth Organization_ 92, no. 9 (2014): 672–79.\n<https://doi.org/10.2471/BLT.14.135541>.\n\nCohen, Julie E. _Between Truth and Power: The Legal Constructions of\nInformational Capitalism._ New York: Oxford University Press, 2019.\n\nCole, David. “‘We Kill People Based on Metadata.’” _New York Review of Books_\n, May 10, 2014. <https://www.nybooks.com/daily/2014/05/10/we-kill-people-\nbased-metadata/>.\n\nColligan, Colette, and Margaret Linley, eds. _Media, Technology, and Litera_\n_ture in the Nineteenth Century: Image, Sound, Touch._ Burlington, VT:\nAshgate, 2011.\n\n“Colonized by Data: The Costs of Connection with Nick Couldry and Ulises\nMejías.” Book talk, September 19, 2019, Berkman Klein Center for Internet and\nSociety at Harvard University. <https://cyber.harvard.edu/events/colonized-\ndata-costs-connection-nick-couldry-and-ulises-mejias>.\n\n_“Congo’s Bloody Coltan.”_ Pulitzer Center on Crisis Reporting, January 6,\n2011. <https://pulitzercenter.org/reporting/congos-bloody-coltan>.\n\nConnolly, William E. _Climate Machines, Fascist Drives, and Truth._ Durham,\nN.C.: Duke University Press, 2019.\n\nConnor, Neil. “Chinese School Uses Facial Recognition to Monitor Student\nAttention in Class.” _Telegraph_ , May 17, 2018.\n<https://www.telegraph.co.uk/news/2018/05/17/chinese-school-uses-facial-\nrecognition-monitor-student-attention/>.\n\n“Containers Lost at Sea—2017 Update.” World Shipping Council, July 10, 2017.\n<http://www.worldshipping.org/industry-\nissues/safety/Containers_Lost_at_Sea_-_2017_Update_FINAL_July_10.pdf>.\n\nCook, Gary, et al. _Clicking Clean: Who Is Winning the Race to Build a Green\nInternet?_ Washington, D.C.: Greenpeace, 2017.\n<http://www.clickclean.org/international/en/>.\n\nCook, James. “Amazon Patents New Alexa Feature That Knows When You’re Ill and\nOffers You Medicine.” _Telegraph_ , October 9, 2018.\n<https://www.telegraph.co.uk/technology/2018/10/09/amazon-patents-new-alexa-\nfeature-knows-offers-medicine/>.\n\nCoole, Diana, and Samantha Frost, eds. _New Materialisms: Ontology, Agency,\nand Politics._ Durham, N.C.: Duke University Press, 2012.\n\nCooper, Carolyn C. “The Portsmouth System of Manufacture.” _Technology and\nCulture_ 25, no. 2 (1984): 182–225. <https://doi.org/10.2307/3104712>.\n\nCorbett, James C., et al. “Spanner: Google’s Globally-Distributed Database.”\n_Proceedings of OSDI 2012_ (2012): 14.\n\nCostanza-Chock, Sasha. _Design Justice: Community-Led Practices to Build the\nWorlds We Need._ Cambridge, Mass.: MIT Press, 2020.\n\nCouldry, Nick, and Ulises A. Mejías. _The Costs of Connection: How Data Is\nColonizing Human Life and Appropriating It for Capitalism._ Stanford, Calif.:\nStanford University Press, 2019.\n\n———. “Data Colonialism: Rethinking Big Data’s Relation to the Contemporary\nSubject.” _Television and New Media_ 20, no. 4 (2019): 336–49.\n<https://doi.org/10.1177/1527476418796632>.\n\n“Counterpoints: An Atlas of Displacement and Resistance.” _Anti-Eviction\nMapping Project_ (blog), September 3, 2020.\n<https://antievictionmap.com/blog/2020/9/3/counterpoints-an-atlas-of-\ndisplacement-and-resistance>.\n\nCourtine, Jean-Jacques, and Claudine Haroche. _Histoire du visage: Exprimer et\ntaire ses émotions (du XVIe siècle au début du XIXe siècle)._ Paris: Payot et\nRivages, 2007.\n\nCowen, Alan, et al. “Mapping the Passions: Toward a High-Dimensional Taxonomy\nof Emotional Experience and Expression.” _Psychological Science in the Public\nInterest_ 20, no. 1 (2019): 61–90. <https://doi.org/10.1177/1529100619850176>.\n\nCrawford, Kate. “Halt the Use of Facial-Recognition Technology until It Is\nRegulated.” _Nature_ 572 (2019): 565.\n<https://doi.org/10.1038/d41586-019-02514-7>.\n\nCrawford, Kate, and Vladan Joler. “Anatomy of an AI System.” Anatomy of an AI\nSystem, 2018. <http://www.anatomyof.ai>.\n\nCrawford, Kate, and Jason Schultz. “AI Systems as State Actors.” _Columbia Law\nReview_ 119, no. 7 (2019). <https://columbialawreview.org/content/ai-systems-\nas-state-actors/>.\n\n———. “Big Data and Due Process: Toward a Framework to Redress Predictive\nPrivacy Harms.” _Boston College Law Review_ 55, no. 1 (2014).\n<https://lawdigitalcommons.bc.edu/bclr/vol55/iss1/4>.\n\nCrawford, Kate, et al. _AI Now 2019 Report._ New York: AI Now Institute, 2019.\n<https://ainowinstitute.org/AI_Now_2019_Report.html>.\n\nCrevier, Daniel. _AI: The Tumultuous History of the Search for Artificial\nIntelligence._ New York: Basic Books, 1993.\n\nCrosman, Penny. “Is AI a Threat to Fair Lending?” _American Banker_ ,\nSeptember 7, 2017. <https://www.americanbanker.com/news/is-artificial-\nintelligence-a-threat-to-fair-lending>.\n\nCurrier, Cora, Glenn Greenwald, and Andrew Fishman. “U.S. Government\nDesignated Prominent Al Jazeera Journalist as ‘Member of Al Qaeda.’” _The\nIntercept_ (blog), May 8, 2015. <https://theintercept.com/2015/05/08/u-s-\ngovernment-designated-prominent-al-jazeera-journalist-al-qaeda-member-put-\nwatch-list/>.\n\nCurry, Steven, et al. “NIST Special Database 32: Multiple Encounter Dataset I\n(MEDS-I).” National Institute of Standards and Technology, NISTIR 7679,\nDecember 2009. <https://nvlpubs.nist.gov/nistpubs/Legacy/IR/nistir7679.pdf>.\n\nCuthbertson, Anthony. “Elon Musk Really Wants to ‘Nuke Mars.’” _Independent_ ,\nAugust 19, 2019. <https://www.independent.co.uk/life-style/gadgets-and-\ntech/news/elon-musk-mars-nuke-spacex-t-shirt-nuclear-weapons-\nspace-a9069141.html>.\n\nDanowski, Déborah, and Eduardo Batalha Viveiros de Castro. _The Ends of the\nWorld._ Translated by Rodrigo Guimaraes Nunes. Malden, Mass.: Polity, 2017.\n\nDanziger, Shai, Jonathan Levav, and Liora Avnaim-Pesso. “Extraneous Factors in\nJudicial Decisions.” _Proceedings of the National Academy of Sciences_ 108,\nno. 17 (2011): 6889–92. <https://doi.org/10.1073/pnas.1018033108>.\n\nDarwin, Charles. _The Expression of the Emotions in Man and Animals_ , edited\nby Joe Cain and Sharon Messenger. London: Penguin, 2009.\n\nDastin, Jeffrey. “Amazon Scraps Secret AI Recruiting Tool That Showed Bias\nagainst Women.” _Reuters_ , October 10, 2018.\n<https://www.reuters.com/article/us-amazon-com-jobs-automation-insight-\nidUSKCN1MK08G>.\n\nDaston, Lorraine. “Cloud Physiognomy.” _Representations_ 135, no. 1 (2016):\n45–71. <https://doi.org/10.1525/rep.2016.135.1.45>.\n\nDaston, Lorraine, and Peter Galison. _Objectivity._ Paperback ed. New York:\nZone Books, 2010.\n\nDavies, Kate, and Liam Young. _Tales from the Dark Side of the City: The\nBreastmilk of the Volcano, Bolivia and the Atacama Desert Expedition._ London:\nUnknown Fields, 2016.\n\nDavis, F. James. _Who Is Black? One Nation’s Definition._ 10th anniversary ed.\nUniversity Park: Pennsylvania State University Press, 2001.\n\nDavis, Monte. “Gerard K. O’Neill on Space Colonies.” _Omni Magazine_ , October\n12, 2017. <https://omnimagazine.com/interview-gerard-k-oneill-space-\ncolonies/>.\n\nDelaporte, François. _Anatomy of the Passions._ Translated by Susan Emanuel.\nStanford, Calif.: Stanford University Press, 2008.\n\n_Demis Hassabis, DeepMind—Learning from First Principles—Artificial\nIntelligence NIPS2017._ Video, December 9, 2017.\n[https://www.youtube.com/watch?v=DXNqYSNvnjA&feature=emb_title](https://www.youtube.com/watch?v=DXNqYSNvnjA&feature=emb_title).\n\nDeng, Jia, et al. “ImageNet: A Large-Scale Hierarchical Image Database.” In\n_2009 IEEE Conference on Computer Vision and Pattern Recognition_ , 248–55.\n<https://doi.org/10.1109/CVPR.2009.5206848>.\n\nDepartment of International Cooperation, Ministry of Science and Technology.\n“Next Generation Artificial Intelligence Development Plan.” _China Science and\nTechnology Newsletter_ , no. 17, September 15, 2017. <http://fi.china-\nembassy.org/eng/kxjs/P020171025789108009001.pdf>.\n\nDeputy Secretary of Defense to Secretaries of the Military Departments et al.,\nApril 26, 2017. Memorandum: “Establishment of an Algorithmic Warfare Cross-\nFunctional Team (Project Maven).”\n<https://www.govexec.com/media/gbc/docs/pdfs_edit/establishment_of_the_awcft_project_maven.pdf>.\n\nDerrida, Jacques, and Eric Prenowitz. “Archive Fever: A Freudian Impression.”\n_Diacritics_ 25, no. 2 (1995): 9. <https://doi.org/10.2307/465144>.\n\nDick, Philip K. “Autofac.” _Galaxy Magazine_ , November 1955.\n<http://archive.org/details/galaxymagazine-1955-11>.\n\nDidi-Huberman, Georges. _Atlas, or the Anxious Gay Science: How to Carry the\nWorld on One’s Back?_ Chicago: University of Chicago Press, 2018.\n\nDietterich, Thomas, and Eun Bae Kong. “Machine Learning Bias, Statistical\nBias, and Statistical Variance of Decision Tree Algorithms.” Unpublished\npaper, Oregon State University, 1995.\n<http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.38.2702>.\n\nD’Ignazio, Catherine, and Lauren F. Klein. _Data Feminism._ Cambridge, Mass.:\nMIT Press, 2020.\n\nDilanian, Ken. “US Special Operations Forces Are Clamoring to Use Software\nfrom Silicon Valley Company Palantir.” _Business Insider_ , March 26, 2015.\n<https://www.businessinsider.com/us-special-operations-forces-are-clamoring-\nto-use-software-from-silicon-valley-company-palantir-2015-3>.\n\nDobbe, Roel, and Meredith Whittaker. “AI and Climate Change: How They’re\nConnected, and What We Can Do about It.” _Medium_ (blog), October 17, 2019.\n<https://medium.com/@AINowInstitute/ai-and-climate-change-how-theyre-\nconnected-and-what-we-can-do-about-it-6aa8d0f5b32c>.\n\nDomingos, Pedro. “A Few Useful Things to Know about Machine Learning.”\n_Communications of the ACM_ 55, no. 10 (2012): 78.\n<https://doi.org/10.1145/2347736.2347755>.\n\nDooley, Ben, Eimi Yamamitsu, and Makiko Inoue. “Fukushima Nuclear Disaster\nTrial Ends with Acquittals of 3 Executives.” _New York Times_ , September 19,\n2019. <https://www.nytimes.com/2019/09/19/business/japan-tepco-fukushima-\nnuclear-acquitted.html>.\n\nDougherty, Conor. “Google Photos Mistakenly Labels Black People ‘Gorillas.’”\n_Bits Blog_ (blog), July 1, 2015.\n<https://bits.blogs.nytimes.com/2015/07/01/google-photos-mistakenly-labels-\nblack-people-gorillas/>.\n\nDouglass, Frederick. “West India Emancipation.” Speech delivered at\nCanandaigua, N.Y., August 4, 1857. <https://rbscp.lib.rochester.edu/4398>.\n\nDrescher, Jack. “Out of DSM: Depathologizing Homosexuality.” _Behavioral\nSciences_ 5, no. 4 (2015): 565–75. <https://doi.org/10.3390/bs5040565>.\n\nDreyfus, Hubert L. _Alchemy and Artificial Intelligence._ Santa Monica,\nCalif.: RAND, 1965.\n\n———. _What Computers Can’t Do: A Critique of Artificial Reason._ New York:\nHarper and Row, 1972.\n\nDryer, Theodora. “Designing Certainty: The Rise of Algorithmic Computing in an\nAge of Anxiety 1920–1970.” Ph.D. diss., University of California, San Diego,\n2019.\n\nDu, Lisa, and Ayaka Maki. “AI Cameras That Can Spot Shoplifters Even before\nThey Steal.” _Bloomberg_ , March 4, 2019.\n<https://www.bloomberg.com/news/articles/2019-03-04/the-ai-cameras-that-can-\nspot-shoplifters-even-before-they-steal>.\n\nDuchenne (de Boulogne), G.-B. _Mécanisme de la physionomie humaine ou Analyse\nélectro-physiologique de l’expression des passions applicable à la_ _pratique\ndes arts plastiques._ 2nd ed. Paris: Librairie J.-B. Baillière et Fils, 1876.\n\nEco, Umberto. _The Infinity of Lists: An Illustrated Essay._ Translated by\nAlastair McEwen. New York: Rizzoli, 2009.\n\nEdwards, Paul N. _The Closed World: Computers and the Politics of Discourse in\nCold War America._ Cambridge, Mass.: MIT Press, 1996.\n\nEdwards, Paul N., and Gabrielle Hecht. “History and the Technopolitics of\nIdentity: The Case of Apartheid South Africa.” _Journal of Southern African\nStudies_ 36, no. 3 (2010): 619–39.\n<https://doi.org/10.1080/03057070.2010.507568>.\n\nEglash, Ron. “Broken Metaphor: The Master-Slave Analogy in Technical\nLiterature.” _Technology and Culture_ 48, no. 2 (2007): 360–69.\n<https://doi.org/10.1353/tech.2007.0066>.\n\nEkman, Paul. “An Argument for Basic Emotions.” _Cognition and Emotion_ 6, no.\n3–4 (1992): 169–200.\n\n———. “Duchenne and Facial Expression of Emotion.” In G.-B. Duchenne de\nBoulogne, _The Mechanism of Human Facial Expression_ , 270–84. Edited and\ntranslated by R. A. Cuthbertson. Cambridge: Cambridge University Press, 1990.\n\n———. _Emotions Revealed: Recognizing Faces and Feelings to Improve\nCommunication and Emotional Life._ New York: Times Books, 2003.\n\n———. “A Life’s Pursuit.” In _The Semiotic Web ’86: An International Yearbook_\n, edited by Thomas A. Sebeok and Jean Umiker-Sebeok, 4–46. Berlin: Mouton de\nGruyter, 1987.\n\n———. _Nonverbal Messages: Cracking the Code: My Life’s Pursuit._ San\nFrancisco: PEG, 2016.\n\n———. _Telling Lies: Clues to Deceit in the Marketplace, Politics, and\nMarriage._ 4th ed. New York: W. W. Norton, 2009.\n\n———. “Universal Facial Expressions of Emotion.” _California Mental Health\nResearch Digest_ 8, no. 4 (1970): 151–58.\n\n———. “What Scientists Who Study Emotion Agree About.” _Perspectives on\nPsychological Science_ 11, no. 1 (2016): 81–88.\n<https://doi.org/10.1177/1745691615596992>.\n\nEkman, Paul, and Wallace V. Friesen. “Constants across Cultures in the Face\nand Emotion.” _Journal of Personality and Social Psychology_ 17, no. 2 (1971):\n124–29. <https://doi.org/10.1037/h0030377>.\n\n———. _Facial Action Coding System (FACS): A Technique for the Measurement of\nFacial Action._ Palo Alto, Calif.: Consulting Psychologists Press, 1978.\n\n———. “Nonverbal Leakage and Clues to Deception.” _Psychiatry_ 31, no. 1\n(1969): 88–106.\n\n———. _Unmasking the Face._ Cambridge, Mass.: Malor Books, 2003.\n\nEkman, Paul, and Harriet Oster. “Facial Expressions of Emotion.” _Annual\nReview of Psychology_ 30 (1979): 527–54.\n\nEkman, Paul, and Maureen O’Sullivan. “Who Can Catch a Liar?” _American\nPsychologist_ 46, no. 9 (1991): 913–20.\n<https://doi.org/10.1037/0003-066X.46.9.913>.\n\nEkman, Paul, Maureen O’Sullivan, and Mark G. Frank. “A Few Can Catch a Liar.”\n_Psychological Science_ 10, no. 3 (1999): 263–66.\n<https://doi.org/10.1111/1467-9280.00147>.\n\nEkman, Paul, and Erika L. Rosenberg, eds. _What the Face Reveals: Basic and\nApplied Studies of Spontaneous Expression Using the Facial Action Coding\nSystem (FACS)._ New York: Oxford University Press, 1997.\n\nEkman, Paul, E. Richard Sorenson, and Wallace V. Friesen. “Pan-Cultural\nElements in Facial Displays of Emotion.” _Science_ 164 (1969): 86–88.\n<https://doi.org/10.1126/science.164.3875.86>.\n\nEkman, Paul, et al. “Universals and Cultural Differences in the Judgments of\nFacial Expressions of Emotion.” _Journal of Personality and Social Psychology_\n53, no. 4 (1987): 712–17.\n\nElfenbein, Hillary Anger, and Nalini Ambady. “On the Universality and Cultural\nSpecificity of Emotion Recognition: A Meta-Analysis.” _Psychological Bulletin_\n128, no. 2 (2002): 203–35. <https://doi.org/10.1037/0033-2909.128.2.203>.\n\nElish, Madeline Clare, and danah boyd. “Situating Methods in the Magic of Big\nData and AI.” _Communication Monographs_ 85, no. 1 (2018): 57–80.\n<https://doi.org/10.1080/03637751.2017.1375130>.\n\nEly, Chris. “The Life Expectancy of Electronics.” Consumer Technology\nAssociation, September 16, 2014.\n<https://www.cta.tech/News/Blog/Articles/2014/September/The-Life-Expectancy-\nof-Electronics.aspx>.\n\n“Emotion Detection and Recognition (EDR) Market Size to surpass 18%+ CAGR 2020\nto 2027.” _MarketWatch_ , October 5, 2020. <https://www.marketwatch.com/press-\nrelease/emotion-detection-and-recognition-edr-market-size-to-\nsurpass-18-cagr-2020-to-2027-2020-10-05>.\n\nEngland, Rachel. “UK Police’s Facial Recognition System Has an 81 Percent\nError Rate.” _Engadget_ , July 4, 2019.\n<https://www.engadget.com/2019/07/04/uk-met-facial-recognition-failure-rate/>.\n\nEnsmenger, Nathan. “Computation, Materiality, and the Global Environment.”\n_IEEE Annals of the History of Computing_ 35, no. 3 (2013): 80.\n<https://www.doi.org/10.1109/MAHC.2013.33>.\n\n———. _The Computer Boys Take Over: Computers, Programmers, and the Politics of\nTechnical Expertise._ Cambridge, Mass.: MIT Press, 2010.\n\nEschner, Kat. “Lie Detectors Don’t Work as Advertised and They Never Did.”\n_Smithsonian_ , February 2, 2017. <https://www.smithsonianmag.com/smart-\nnews/lie-detectors-dont-work-advertised-and-they-never-did-180961956/>.\n\nEstreicher, Sam, and Christopher Owens. “Labor Board Wrongly Rejects Employee\nAccess to Company Email for Organizational Purposes.” _Verdict_ , February 19,\n2020. <https://verdict.justia.com/2020/02/19/labor-board-wrongly-rejects-\nemployee-access-to-company-email-for-organizational-purposes>.\n\nEubanks, Virginia. _Automating Inequality: How High-Tech Tools Profile,\nPolice, and Punish the Poor._ New York: St. Martin’s, 2017.\n\nEver AI. “Ever AI Leads All US Companies on NIST’s Prestigious Facial\nRecognition Vendor Test.” _GlobeNewswire_ , November 27, 2018.\n<http://www.globenewswire.com/news-release/2018/11/27/1657221/0/en/Ever-AI-\nLeads-All-US-Companies-on-NIST-s-Prestigious-Facial-Recognition-Vendor-\nTest.html>.\n\nFabian, Ann. _The Skull Collectors: Race, Science, and America’s Unburied\nDead._ Chicago: University of Chicago Press, 2010.\n\n“Face: An AI Service That Analyzes Faces in Images.” Microsoft Azure.\n<https://azure.microsoft.com/en-us/services/cognitive-services/face/>.\n\nFadell, Anthony M., et al. Smart-home automation system that suggests or\nautomatically implements selected household policies based on sensed\nobservations. US10114351B2, filed March 5, 2015, and issued October 30, 2018.\n\nFang, Lee. “Defense Tech Startup Founded by Trump’s Most Prominent Silicon\nValley Supporters Wins Secretive Military AI Contract.” _The Intercept_\n(blog), March 9, 2019. <https://theintercept.com/2019/03/09/anduril-\nindustries-project-maven-palmer-luckey/>.\n\n———. “Leaked Emails Show Google Expected Lucrative Military Drone AI Work to\nGrow Exponentially.” _The Intercept_ (blog), May 31, 2018.\n<https://theintercept.com/2018/05/31/google-leaked-emails-drone-ai-pentagon-\nlucrative/>.\n\n“Federal Policy for the Protection of Human Subjects.” _Federal Register_ ,\nSeptember 8, 2015.\n<https://www.federalregister.gov/documents/2015/09/08/2015-21756/federal-\npolicy-for-the-protection-of-human-subjects>.\n\nFederici, Silvia. _Wages against Housework._ 6th ed. London: Power of Women\nCollective and Falling Walls Press, 1975.\n\nFellbaum, Christiane, ed. _WordNet: An Electronic Lexical Database._\nCambridge, Mass.: MIT Press, 1998.\n\nFernández-Dols, José-Miguel, and James A. Russell, eds. _The Science of Facial\nExpression._ New York: Oxford University Press, 2017.\n\nFeuer, William. “Palantir CEO Alex Karp Defends His Company’s Relationship\nwith Government Agencies.” _CNBC_ , January 23, 2020.\n<https://www.cnbc.com/2020/01/23/palantir-ceo-alex-karp-defends-his-companys-\nwork-for-the-government.html>.\n\n“Five Eyes Intelligence Oversight and Review Council.” U.S. Office of the\nDirector of National Intelligence. <https://www.dni.gov/index.php/who-we-\nare/organizations/enterprise-capacity/chco/chco-related-menus/chco-related-\nlinks/recruitment-and-outreach/217-about/organization/icig-pages/2660-icig-\nfiorc>.\n\nFoer, Franklin. “Jeff Bezos’s Master Plan.” _Atlantic_ , November 2019.\n<https://www.theatlantic.com/magazine/archive/2019/11/what-jeff-bezos-\nwants/598363/>.\n\nForeman, Judy. “A Conversation with: Paul Ekman; The 43 Facial Muscles That\nReveal Even the Most Fleeting Emotions.” _New York Times_ , August 5, 2003.\n<https://www.nytimes.com/2003/08/05/health/conversation-with-paul-\nekman-43-facial-muscles-that-reveal-even-most-fleeting.html>.\n\nForsythe, Diana E. “Engineering Knowledge: The Construction of Knowledge in\nArtificial Intelligence.” _Social Studies of Science_ 23, no. 3 (1993):\n445–77. <https://doi.org/10.1177/0306312793023003002>.\n\nFortunati, Leopoldina. “Robotization and the Domestic Sphere.” _New Media and\nSociety_ 20, no. 8 (2018): 2673–90.\n<https://doi.org/10.1177/1461444817729366>.\n\nFoucault, Michel. _Discipline and Punish: The Birth of the Prison._ 2nd ed.\nNew York: Vintage Books, 1995.\n\nFounds, Andrew P., et al. “NIST Special Database 32: Multiple Encounter\nDataset II (MEDS-II).” National Institute of Standards and Technology, NISTIR\n7807, February 2011.\n<https://tsapps.nist.gov/publication/get_pdf.cfm?pub_id=908383>.\n\nFourcade, Marion, and Kieran Healy. “Seeing Like a Market.” _Socio-Economic\nReview_ 15, no. 1 (2016): 9–29. <https://doi.org/10.1093/ser/mww033>.\n\nFranceschi-Bicchierai, Lorenzo. “Redditor Cracks Anonymous Data Trove to\nPinpoint Muslim Cab Drivers.” _Mashable_ , January 28, 2015.\n<https://mashable.com/2015/01/28/redditor-muslim-cab-drivers/>.\n\nFranklin, Ursula M. _The Real World of Technology._ Rev. ed. Toronto, Ont.:\nHouse of Anansi Press, 2004.\n\nFranklin, Ursula M., and Michelle Swenarchuk. _The Ursula Franklin Reader:\nPacifism as a Map._ Toronto, Ont.: Between the Lines, 2006.\n\nFrench, Martin A., and Simone A. Browne. “Surveillance as Social Regulation:\nProfiles and Profiling Technology.” In _Criminalization, Representation,\nRegulation: Thinking Differently about Crime_ , edited by Deborah R. Brock,\nAmanda Glasbeek, and Carmela Murdocca, 251–84. North York, Ont.: University of\nToronto Press, 2014.\n\nFridlund, Alan. “A Behavioral Ecology View of Facial Displays, 25 Years\nLater.” _Emotion Researcher_ , August 2015.\n<https://emotionresearcher.com/the-behavioral-ecology-view-of-facial-\ndisplays-25-years-later/>.\n\nFussell, Sidney. “The Next Data Mine Is Your Bedroom.” _Atlantic_ , November\n17, 2018. <https://www.theatlantic.com/technology/archive/2018/11/google-\npatent-bedroom-privacy-smart-home/576022/>.\n\nGalison, Peter. _Einstein’s Clocks, Poincaré’s Maps: Empires of Time._ New\nYork: W. W. Norton, 2003.\n\n———. “The Ontology of the Enemy: Norbert Wiener and the Cybernetic Vision.”\n_Critical Inquiry_ 21, no. 1 (1994): 228–66.\n\n———. “Removing Knowledge.” _Critical Inquiry_ 31, no. 1 (2004): 229–43.\n<https://doi.org/10.1086/427309>.\n\nGarris, Michael D., and Charles L. Wilson. “NIST Biometrics Evaluations and\nDevelopments.” National Institute of Standards and Technology, NISTIR 7204,\nFebruary 2005.\n<https://www.govinfo.gov/content/pkg/GOVPUB-C13-1ba4778e3b87bdd6ce660349317d3263/pdf/GOVPUB-C13-1ba4778e3b87bdd6ce660349317d3263.pdf>.\n\nGates, Dominic. “Bezos’s Blue Origin Seeks Tax Incentives to Build Rocket\nEngines Here.” _Seattle Times_ , January 14, 2016.\n<https://www.seattletimes.com/business/boeing-aerospace/bezoss-blue-origin-\nseeks-tax-incentives-to-build-rocket-engines-here/>.\n\nGebru, Timnit, et al. “Datasheets for Datasets.” _ArXiv:1803.09010 [Cs]_ ,\nMarch 23, 2018. <http://arxiv.org/abs/1803.09010>.\n\n———. “Fine-Grained Car Detection for Visual Census Estimation.” In\n_Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence_ ,\n_AAAI ’17_ , 4502–8.\n\nGee, Alastair. “San Francisco or Mumbai? UN Envoy Encounters Homeless Life in\nCalifornia.” _Guardian_ , January 22, 2018. <https://www.theguardian.com/us-\nnews/2018/jan/22/un-rapporteur-homeless-san-francisco-california>.\n\nGellman, Barton, and Laura Poitras. “U.S., British Intelligence Mining Data\nfrom Nine U.S. Internet Companies in Broad Secret Program.” _Washington Post_\n, June 7, 2013. <https://www.washingtonpost.com/investigations/us-\nintelligence-mining-data-from-nine-us-internet-companies-in-broad-secret-\nprogram/2013/06/06/3a0c0da8-cebf-11e2-8845-d970ccb04497_story.html>.\n\nGendron, Maria, and Lisa Feldman Barrett. _Facing the Past._ Vol. 1. New York:\nOxford University Press, 2017.\n\nGeorge, Rose. _Ninety Percent of Everything: Inside Shipping, the Invisible\nIndustry That Puts Clothes on Your Back, Gas in Your Car, and Food on Your\nPlate._ New York: Metropolitan Books, 2013.\n\nGershgorn, Dave. “The Data That Transformed AI Research—and Possibly the\nWorld.” _Quartz_ , July 26, 2017. <https://qz.com/1034972/the-data-that-\nchanged-the-direction-of-ai-research-and-possibly-the-world/>.\n\nGhaffary, Shirin. “More Than 1,000 Google Employees Signed a Letter Demanding\nthe Company Reduce Its Carbon Emissions.” _Recode_ , November 4, 2019.\n<https://www.vox.com/recode/2019/11/4/20948200/google-employees-letter-demand-\nclimate-change-fossil-fuels-carbon-emissions>.\n\nGill, Karamjit S. _Artificial Intelligence for Society._ New York: John Wiley\nand Sons, 1986.\n\nGillespie, Tarleton. _Custodians of the Internet: Platforms, Content\nModeration, and the Hidden Decisions That Shape Social Media._ New Haven: Yale\nUniversity Press, 2018.\n\nGillespie, Tarleton, Pablo J. Boczkowski, and Kirsten A. Foot, eds. _Media\nTechnologies: Essays on Communication, Materiality, and Society._ Cambridge.\nMass.: MIT Press, 2014.\n\nGitelman, Lisa, ed. _“Raw Data” Is an Oxymoron._ Cambridge, Mass.: MIT Press,\n2013.\n\nGoeleven, Ellen, et al. “The Karolinska Directed Emotional Faces: A Validation\nStudy.” _Cognition and Emotion_ 22, no. 6 (2008): 1094–18.\n<https://doi.org/10.1080/02699930701626582>.\n\nGoenka, Aakash, et al. Database systems and user interfaces for dynamic and\ninteractive mobile image analysis and identification. US10339416B2, filed July\n5, 2018, and issued July 2, 2019.\n\n“Google Outrage at ‘NSA Hacking.’” _BBC News_ , October 31, 2013.\n<https://www.bbc.com/news/world-us-canada-24751821>.\n\nGora, Walter, Ulrich Herzog, and Satish Tripathi. “Clock Synchronization on\nthe Factory Floor (FMS).” _IEEE Transactions on Industrial Electronics_ 35,\nno. 3 (1988): 372–80. <https://doi.org/10.1109/41.3109>.\n\nGould, Stephen Jay. _The Mismeasure of Man._ Rev. and expanded ed. New York:\nW. W. Norton, 1996.\n\nGraeber, David. _The Utopia of Rules: On Technology, Stupidity, and the Secret\nJoys of Bureaucracy._ Brooklyn, N.Y.: Melville House, 2015.\n\nGraham, John. “Lavater’s Physiognomy in England.” _Journal of the History of\nIdeas_ 22, no. 4 (1961): 561. <https://doi.org/10.2307/2708032>.\n\nGraham, Mark, and Håvard Haarstad. “Transparency and Development: Ethical\nConsumption through Web 2.0 and the Internet of Things.” _Information\nTechnologies and International Development_ 7, no. 1 (2011): 1–18.\n\nGray, Mary L., and Siddharth Suri. _Ghost Work: How to Stop Silicon Valley\nfrom Building a New Global Underclass._ Boston: Houghton Mifflin Harcourt,\n2019.\n\n———. “The Humans Working behind the AI Curtain.” _Harvard Business Review_ ,\nJanuary 9, 2017. <https://hbr.org/2017/01/the-humans-working-behind-the-ai-\ncurtain>.\n\nGray, Richard T. _About Face: German Physiognomic Thought from Lavater to\nAuschwitz._ Detroit, Mich.: Wayne State University Press, 2004.\n\nGreen, Ben. _Smart Enough City: Taking Off Our Tech Goggles and Reclaiming the\nFuture of Cities._ Cambridge, Mass.: MIT Press, 2019.\n\nGreenberger, Martin, ed. _Management and the Computer of the Future._ New\nYork: Wiley, 1962.\n\nGreene, Tristan. “Science May Have Cured Biased AI.” The Next Web, October 26,\n2017. <https://thenextweb.com/artificial-intelligence/2017/10/26/scientists-\nmay-have-just-created-the-cure-for-biased-ai/>.\n\nGreenhouse, Steven. “McDonald’s Workers File Wage Suits in 3 States.” _New\nYork Times_ , March 13, 2014.\n<https://www.nytimes.com/2014/03/14/business/mcdonalds-workers-in-three-\nstates-file-suits-claiming-underpayment.html>.\n\nGreenwald, Anthony G., and Linda Hamilton Krieger. “Implicit Bias: Scientific\nFoundations.” _California Law Review_ 94, no. 4 (2006): 945.\n<https://doi.org/10.2307/20439056>.\n\nGregg, Melissa. _Counterproductive: Time Management in the Knowledge Economy._\nDurham, N.C.: Duke University Press, 2018.\n\n“A Grey Goldmine: Recent Developments in Lithium Extraction in Bolivia and\nAlternative Energy Projects.” Council on Hemispheric Affairs, November 17,\n2009. <http://www.coha.org/a-grey-goldmine-recent-developments-in-lithium-\nextraction-in-bolivia-and-alternative-energy-projects/>.\n\nGrigorieff, Paul. “The Mittelwerk/Mittelbau/Camp Dora.” V2rocket.com.\n<http://www.v2rocket.com/start/chapters/mittel.html>.\n\nGrother, Patrick, et al. “The 2017 IARPA Face Recognition Prize Challenge\n(FRPC).” National Institute of Standards and Technology, NISTIR 8197, November\n2017. <https://nvlpubs.nist.gov/nistpubs/ir/2017/NIST.IR.8197.pdf>.\n\nGrothoff, Christian, and J. M. Porup. “The NSA’s SKYNET Program May Be Killing\nThousands of Innocent People.” Ars Technica, February 16, 2016.\n<https://arstechnica.com/information-technology/2016/02/the-nsas-skynet-\nprogram-may-be-killing-thousands-of-innocent-people/>.\n\nGuendelsberger, Emily. _On the Clock: What Low-Wage Work Did to Me and How It\nDrives America Insane._ New York: Little, Brown, 2019.\n\nGurley, Lauren Kaori. “60 Amazon Workers Walked Out over Warehouse Working\nConditions.” _Vice_ (blog), October 3, 2019.\n<https://www.vice.com/en_us/article/pa7qny/60-amazon-workers-walked-out-over-\nwarehouse-working-conditions>.\n\nHacking, Ian. “Kinds of People: Moving Targets.” _Proceedings of the British\nAcademy_ 151 (2007): 285–318.\n\n———. “Making Up People.” _London Review of Books_ , August 17, 2006, 23–26.\n\nHagendorff, Thilo. “The Ethics of AI Ethics: An Evaluation of Guidelines.”\n_Minds and Machines_ 30 (2020): 99–120.\n<https://doi.org/10.1007/s11023-020-09517-8>.\n\nHaggerty, Kevin D., and Richard V. Ericson. “The Surveillant Assemblage.”\n_British Journal of Sociology_ 51, no. 4 (2000): 605–22.\n<https://doi.org/10.1080/00071310020015280>.\n\nHajjar, Lisa. “Lawfare and Armed Conflicts: A Comparative Analysis of Israeli\nand U.S. Targeted Killing Policies.” In _Life in the Age of Drone Warfare_ ,\nedited by Lisa Parks and Caren Kaplan, 59–88. Durham, N.C.: Duke University\nPress, 2017.\n\nHalsey III, Ashley. “House Member Questions $900 Million TSA ‘SPOT’ Screening\nProgram.” _Washington Post_ , November 14, 2013.\n<https://www.washingtonpost.com/local/trafficandcommuting/house-member-\nquestions-900-million-tsa-spot-screening-\nprogram/2013/11/14/ad194cfe-4d5c-11e3-be6b-d3d28122e6d4_story.html>.\n\nHao, Karen. “AI Is Sending People to Jail—and Getting It Wrong.” _MIT\nTechnology Review_ , January 21, 2019.\n<https://www.technologyreview.com/s/612775/algorithms-criminal-justice-ai/>.\n\n———. “The Technology behind OpenAI’s Fiction-Writing, Fake-News-Spewing AI,\nExplained.” _MIT Technology Review_ , February 16, 2019.\n<https://www.technologyreview.com/s/612975/ai-natural-language-processing-\nexplained/>.\n\n———. “Three Charts Show How China’s AI Industry Is Propped Up by Three\nCompanies.” _MIT Technology Review_ , January 22, 2019.\n<https://www.technologyreview.com/s/612813/the-future-of-chinas-ai-industry-\nis-in-the-hands-of-just-three-companies/>.\n\nHaraway, Donna J. _Modest_Witness@Second_Millennium.FemaleMan_Meets_OncoMouse:\nFeminism and Technoscience._ New York: Routledge, 1997.\n\n———. _Simians, Cyborgs, and Women: The Reinvention of Nature._ New York:\nRoutledge, 1990.\n\n———. _When Species Meet._ Minneapolis: University of Minnesota Press, 2008.\n\nHardt, Michael, and Antonio Negri. _Assembly._ New York: Oxford University\nPress, 2017.\n\nHarrabin, Roger. “Google Says Its Carbon Footprint Is Now Zero.” BBC News,\nSeptember 14, 2020. <https://www.bbc.com/news/technology-54141899>.\n\nHarvey, Adam R. “MegaPixels.” MegaPixels. <https://megapixels.cc/>.\n\nHarvey, Adam, and Jules LaPlace. “Brainwash Dataset.” MegaPixels.\n<https://megapixels.cc/brainwash/>.\n\nHarwell, Drew. “A Face-Scanning Algorithm Increasingly Decides Whether You\nDeserve the Job.” _Washington Post_ , November 7, 2019.\n<https://www.washingtonpost.com/technology/2019/10/22/ai-hiring-face-scanning-\nalgorithm-increasingly-decides-whether-you-deserve-job/>.\n\nHaskins, Caroline. “Amazon Is Coaching Cops on How to Obtain Surveillance\nFootage without a Warrant.” _Vice_ (blog), August 5, 2019\\.\n<https://www.vice.com/en_us/article/43kga3/amazon-is-coaching-cops-on-how-to-\nobtain-surveillance-footage-without-a-warrant>.\n\n———. “Amazon’s Home Security Company Is Turning Everyone into Cops.” _Vice_\n(blog), February 7, 2019. <https://www.vice.com/en_us/article/qvyvzd/amazons-\nhome-security-company-is-turning-everyone-into-cops>.\n\n———. “How Ring Transmits Fear to American Suburbs.” _Vice_ (blog), July 12,\n2019. <https://www.vice.com/en/article/ywaa57/how-ring-transmits-fear-to-\namerican-suburbs>.\n\nHeaven, Douglas. “Why Faces Don’t Always Tell the Truth about Feelings.”\n_Nature_ , February 26, 2020.\n<https://www.nature.com/articles/d41586-020-00507-5>.\n\nHeller, Nathan. “What the Enron Emails Say about Us.” _New Yorker_ , July 17,\n2017. <https://www.newyorker.com/magazine/2017/07/24/what-the-enron-e-mails-\nsay-about-us>.\n\nHernandez, Elizabeth. “CU Colorado Springs Students Secretly Photographed for\nGovernment-Backed Facial-Recognition Research.” _Denver Post_ , May 27, 2019.\n<https://www.denverpost.com/2019/05/27/cu-colorado-springs-facial-recognition-\nresearch/>.\n\nHeyn, Edward T. “Berlin’s Wonderful Horse; He Can Do Almost Everything but\nTalk—How He Was Taught.” _New York Times_ , Sept. 4, 1904.\n<https://timesmachine.nytimes.com/timesmachine/1904/09/04/101396572.pdf>.\n\nHicks, Mar. _Programmed Inequality: How Britain Discarded Women Technologists\nand Lost Its Edge in Computing._ Cambridge, Mass.: MIT Press, 2017.\n\nHird, M. J. “Waste, Landfills, and an Environmental Ethics of Vulnerability.”\n_Ethics and the Environment_ 18, no. 1 (2013): 105–24.\n<https://www.doi.org/10.2979/ethicsenviro.18.1.105>.\n\nHodal, Kate. “Death Metal: Tin Mining in Indonesia.” _Guardian_ , November 23,\n2012. <https://www.theguardian.com/environment/2012/nov/23/tin-mining-\nindonesia-bangka>.\n\nHoffmann, Anna Lauren. “Data Violence and How Bad Engineering Choices Can\nDamage Society.” _Medium_ (blog), April 30, 2018.\n<https://medium.com/s/story/data-violence-and-how-bad-engineering-choices-can-\ndamage-society-39e44150e1d4>.\n\nHoffower, Hillary. “We Did the Math to Calculate How Much Money Jeff Bezos\nMakes in a Year, Month, Week, Day, Hour, Minute, and Second.” _Business\nInsider_ , January 9, 2019. <https://www.businessinsider.com/what-amazon-ceo-\njeff-bezos-makes-every-day-hour-minute-2018-10>.\n\nHoft, Joe. “Facial, Speech and Virtual Polygraph Analysis Shows Ilhan Omar\nExhibits Many Indications of a Compulsive Fibber!!!” The Gateway Pundit, July\n21, 2019. <https://www.thegatewaypundit.com/2019/07/facial-speech-and-virtual-\npolygraph-analysis-shows-ilhan-omar-exhibits-many-indications-of-a-compulsive-\nfibber/>.\n\nHogan, Mél. “Data Flows and Water Woes: The Utah Data Center.” _Big Data and\nSociety_ (December 2015). <https://www.doi.org/10.1177/2053951715592429>.\n\nHolmqvist, Caroline. _Policing Wars: On Military Intervention in the Twenty-\nFirst Century._ London: Palgrave Macmillan, 2014.\n\nHorne, Emily, and Tim Maly. _The Inspection House: An Impertinent Field Guide\nto Modern Surveillance._ Toronto: Coach House Books, 2014.\n\nHorowitz, Alexandra. “Why Brain Size Doesn’t Correlate with Intelligence.”\n_Smithsonian_ , December 2013. <https://www.smithsonianmag.com/science-\nnature/why-brain-size-doesnt-correlate-with-intelligence-180947627/>.\n\nHouse, Brian. “Synchronizing Uncertainty: Google’s Spanner and Cartographic\nTime.” In _Executing Practices_ , edited by Helen Pritchard, Eric Snodgrass,\nand Magda Tyżlik-Carver, 117–26. London: Open Humanities Press, 2018.\n\n“How Does a Lithium-Ion Battery Work?” Energy.gov, September 14, 2017.\n<https://www.energy.gov/eere/articles/how-does-lithium-ion-battery-work>.\n\nHu, Tung-Hui. _A Prehistory of the Cloud._ Cambridge, Mass.: MIT Press, 2015.\n\nHuet, Ellen. “The Humans Hiding behind the Chatbots.” _Bloomberg_ , April 18,\n2016. <https://www.bloomberg.com/news/articles/2016-04-18/the-humans-hiding-\nbehind-the-chatbots>.\n\nHutson, Matthew. “Artificial Intelligence Could Identify Gang Crimes—and\nIgnite an Ethical Firestorm.” _Science_ , February 28, 2018.\n<https://www.sciencemag.org/news/2018/02/artificial-intelligence-could-\nidentify-gang-crimes-and-ignite-ethical-firestorm>.\n\nHwang, Tim, and Karen Levy. “‘The Cloud’ and Other Dangerous Metaphors.”\n_Atlantic_ , January 20, 2015.\n<https://www.theatlantic.com/technology/archive/2015/01/the-cloud-and-other-\ndangerous-metaphors/384518/>.\n\n“ImageNet Large Scale Visual Recognition Competition (ILSVRC).” <http://image-\nnet.org/challenges/LSVRC/>.\n\n“Intel’s Efforts to Achieve a Responsible Minerals Supply Chain.” Intel, May\n2019. <https://www.intel.com/content/www/us/en/corporate-\nresponsibility/conflict-minerals-white-paper.html>.\n\nIrani, Lilly. “Difference and Dependence among Digital Workers: The Case of\nAmazon Mechanical Turk.” _South Atlantic Quarterly_ 114, no. 1 (2015): 225–34.\n<https://doi.org/10.1215/00382876-2831665>.\n\n———. “The Hidden Faces of Automation.” _XRDS_ 23, no. 2 (2016): 34–37.\n<https://doi.org/10.1145/3014390>.\n\nIzard, Carroll E. “The Many Meanings/Aspects of Emotion: Definitions,\nFunctions, Activation, and Regulation.” _Emotion Review_ 2, no. 4 (2010):\n363–70. <https://doi.org/10.1177/1754073910374661>.\n\nJaton, Florian. “We Get the Algorithms of Our Ground Truths: Designing\nReferential Databases in Digital Image Processing.” _Social Studies of\nScience_ 47, no. 6 (2017): 811–40. <https://doi.org/10.1177/0306312717730428>.\n\nJin, Huafeng, and Shuo Wang. Voice-based determination of physical and\nemotional characteristics of users. US10096319B1, n.d.\n\nJobin, Anna, Marcello Ienca, and Effy Vayena. “The Global Landscape of AI\nEthics Guidelines.” _Nature Machine Intelligence_ 1 (2019): 389–99.\n<https://doi.org/10.1038/s42256-019-0088-2>.\n\nJones, Nicola. “How to Stop Data Centres from Gobbling Up the World’s\nElectricity.” _Nature_ , September 12, 2018.\n<https://www.nature.com/articles/d41586-018-06610-y>.\n\nJoseph, George. “Data Company Directly Powers Immigration Raids in Workplace.”\n_WNYC_ , July 16, 2019. <https://www.wnyc.org/story/palantir-directly-powers-\nice-workplace-raids-emails-show/>.\n\nJune, Laura. “YouTube Has a Fake Peppa Pig Problem.” _The Outline_ , March 16,\n2017. <https://theoutline.com/post/1239/youtube-has-a-fake-peppa-pig-problem>.\n\nKafer, Alison. _Feminist, Queer, Crip._ Bloomington: Indiana University Press,\n2013.\n\nKak, Amba, ed. “Regulating Biometrics: Global Approaches and Urgent\nQuestions.” AI Now Institute, September 1, 2020.\n<https://ainowinstitute.org/regulatingbiometrics.html>.\n\nKanade, Takeo. _Computer Recognition of Human Faces._ Basel: Birkhäuser\nBoston, 2013.\n\nKanade, T., J. F. Cohn, and Yingli Tian. “Comprehensive Database for Facial\nExpression Analysis.” In _Proceedings Fourth IEEE International Conference on\nAutomatic Face and Gesture Recognition_ , 46–53. 2000.\n<https://doi.org/10.1109/AFGR.2000.840611>.\n\nKappas, A. “Smile When You Read This, Whether You Like It or Not: Conceptual\nChallenges to Affect Detection.” _IEEE Transactions on Affective Computing_ 1,\nno. 1 (2010): 38–41. <https://doi.org/10.1109/T-AFFC.2010.6>.\n\nKatz, Lawrence F., and Alan B. Krueger. “The Rise and Nature of Alternative\nWork Arrangements in the United States, 1995–2015.” _ILR Review_ 72, no. 2\n(2019): 382–416.\n\nKeates, Nancy. “The Many Places Amazon CEO Jeff Bezos Calls Home.” _Wall\nStreet Journal_ , January 9, 2019. <https://www.wsj.com/articles/the-many-\nplaces-amazon-ceo-jeff-bezos-calls-home-1507204462>.\n\nKeel, Terence D. “Religion, Polygenism and the Early Science of Human\nOrigins.” _History of the Human Sciences_ 26, no. 2 (2013): 3–32.\n<https://doi.org/10.1177/0952695113482916>.\n\nKelly, Kevin. _What Technology Wants._ New York: Penguin Books, 2011.\n\nKemeny, John, and Thomas Kurtz. “Dartmouth Timesharing.” _Science_ 162 (1968):\n223–68.\n\nKendi, Ibram X. “A History of Race and Racism in America, in 24 Chapters.”\n_New York Times_ , February 22, 2017.\n<https://www.nytimes.com/2017/02/22/books/review/a-history-of-race-and-racism-\nin-america-in-24-chapters.html>.\n\nKerr, Dara. “Tech Workers Protest in SF to Keep Attention on Travel Ban.”\n_CNET_ , February 13, 2017. <https://www.cnet.com/news/trump-immigration-ban-\ntech-workers-protest-no-ban-no-wall/>.\n\nKeyes, Os. “The Misgendering Machines: Trans/HCI Implications of Automatic\nGender Recognition.” In _Proceedings of the ACM on Human-Computer Interaction_\n2, Issue CSCW (2018): art. 88. <https://doi.org/10.1145/3274357>.\n\nKleinberg, Jon, et al. “Human Decisions and Machine Predictions.” _Quarterly\nJournal of Economics_ 133, no. 1 (2018): 237–93.\n<https://doi.org/10.1093/qje/qjx032>.\n\nKlimt, Bryan, and Yiming Yang. “The Enron Corpus: A New Dataset for Email\nClassification Research.” In _Machine Learning: ECML 2004_ , edited by Jean-\nFrançois Boulicat et al., 217–26. Berlin: Springer, 2004.\n\nKlose, Alexander. _The Container Principle: How a Box Changes the Way We\nThink._ Translated by Charles Marcrum. Cambridge, Mass.: MIT Press, 2015.\n\nKnight, Will. “Alpha Zero’s ‘Alien’ Chess Shows the Power, and the\nPeculiarity, of AI.” _MIT Technology Review_ , December 8, 2017.\n<https://www.technologyreview.com/s/609736/alpha-zeros-alien-chess-shows-the-\npower-and-the-peculiarity-of-ai/>.\n\nKolbert, Elizabeth. “There’s No Scientific Basis for Race—It’s a Made-Up\nLabel.” _National Geographic_ , March 12, 2018.\n<https://www.nationalgeographic.com/magazine/2018/04/race-genetics-science-\nafrica/>.\n\nKrizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. “ImageNet\nClassification with Deep Convolutional Neural Networks.” _Communications of\nthe ACM_ 60, no. 6 (2017): 84–90. <https://doi.org/10.1145/3065386>.\n\nLabban, Mazen. “Deterritorializing Extraction: Bioaccumulation and the\nPlanetary Mine.” _Annals of the Association of American Geographers_ 104, no.\n3 (2014): 560–76. <https://www.jstor.org/stable/24537757>.\n\nLakoff, George. _Women, Fire, and Dangerous Things: What Categories Reveal\nabout the Mind._ Chicago: University of Chicago Press, 1987.\n\nLambert, Fred. “Breakdown of Raw Materials in Tesla’s Batteries and Possible\nBreaknecks,” electrek, November 1, 2016.\n<https://electrek.co/2016/11/01/breakdown-raw-materials-tesla-batteries-\npossible-bottleneck/>.\n\nLapuschkin, Sebastian, et al. “Unmasking Clever Hans Predictors and Assessing\nWhat Machines Really Learn.” _Nature Communications_ 10, no. 1 (2019): 1–8.\n<https://doi.org/10.1038/s41467-019-08987-4>.\n\nLatour, Bruno. “Tarde’s Idea of Quantification.” In _The Social after Gabriel\nTarde: Debates and Assessments_ , edited by Matei Candea, 147–64. New York:\nRoutledge, 2010.\n\nLem, Stainslaw. “The First Sally (A), or Trurl’s Electronic Bard.” In _From\nHere to Forever_ , vol. 4, _The Road to Science Fiction_ , edited by James\nGunn. Lanham, Md.: Scarecrow, 2003.\n\nLeys, Ruth. _The Ascent of Affect: Genealogy and Critique._ Chicago:\nUniversity of Chicago Press, 2017.\n\nLi, Xiaochang. “Divination Engines: A Media History of Text Prediction.” Ph.D.\ndiss., New York University, 2017.\n\nLibby, Sara. “Scathing Audit Bolsters Critics’ Fears about Secretive State\nGang Database.” _Voice of San Diego_ , August 11, 2016.\n<https://www.voiceofsandiego.org/topics/public-safety/scathing-audit-bolsters-\ncritics-fears-secretive-state-gang-database/>.\n\nLight, Jennifer S. “When Computers Were Women.” _Technology and Culture_ 40,\nno. 3 (1999): 455–83. <https://www.jstor.org/stable/25147356>.\n\nLingel, Jessa, and Kate Crawford. “Alexa, Tell Me about Your Mother: The\nHistory of the Secretary and the End of Secrecy.” _Catalyst: Feminism, Theory,\nTechnoscience_ 6, no. 1 (2020).\n<https://catalystjournal.org/index.php/catalyst/article/view/29949>.\n\nLiu, Zhiyi. “Chinese Mining Dump Could Hold Trillion-Dollar Rare Earth\nDeposit.” China Dialogue, December 14, 2012.\n<https://www.chinadialogue.net/article/show/single/en/5495-Chinese-mining-\ndump-could-hold-trillion-dollar-rare-earth-deposit>.\n\nLloyd, G. E. R. “The Development of Aristotle’s Theory of the Classification\nof Animals.” _Phronesis_ 6, no. 1–2 (1961): 59–81.\n<https://doi.org/10.1163/156852861X00080>.\n\nLo, Chris. “The False Monopoly: China and the Rare Earths Trade.” _Mining\nTechnology, Mining News and Views Updated Daily_ (blog), August 19, 2015.\n<https://www.mining-technology.com/features/featurethe-false-monopoly-china-\nand-the-rare-earths-trade-4646712/>.\n\nLocker, Melissa. “Microsoft, Duke, and Stanford Quietly Delete Databases with\nMillions of Faces.” _Fast Company_ , June 6, 2019.\n<https://www.fastcompany.com/90360490/ms-celeb-microsoft-deletes-10m-faces-\nfrom-face-database>.\n\nLorde, Audre. _The Master’s Tools Will Never Dismantle the Master’s House._\nLondon: Penguin Classics, 2018.\n\nLucey, Patrick, et al. “The Extended Cohn-Kanade Dataset (CK+): A Complete\nDataset for Action Unit and Emotion-Specified Expression.” In _2010 IEEE\nComputer Society Conference on Computer Vision and Pat_ _tern\nRecognition—Workshops_ , 94–101. <https://doi.org/10.1109/CVPRW.2010.5543262>.\n\nLuxemburg, Rosa. “Practical Economies: Volume 2 of Marx’s Capital.” In _The\nComplete Works of Rosa Luxemburg_ , edited by Peter Hudis, 421–60. London:\nVerso, 2013.\n\nLyons, M., et al. “Coding Facial Expressions with Gabor Wavelets.” In\n_Proceedings Third IEEE International Conference on Automatic Face and Gesture\nRecognition_ , 200–205. 1998. <https://doi.org/10.1109/AFGR.1998.670949>.\n\nLyotard, Jean François. “Presenting the Unpresentable: The Sublime.”\n_Artforum_ , April 1982.\n\nMaass, Peter. “Summit Fever.” _The Intercept_ (blog), June 25, 2012.\n<https://www.documentcloud.org/documents/2088979-summit-fever.html>.\n\nMaass, Peter, and Beryl Lipton. “What We Learned.” _MuckRock_ , November 15,\n2018. <https://www.muckrock.com/news/archives/2018/nov/15/alpr-what-we-\nlearned/>.\n\nMacKenzie, Donald A. _Inventing Accuracy: A Historical Sociology of Nuclear\nMissile Guidance._ Cambridge, Mass.: MIT Press, 2001.\n\n“Magic from Invention.” Brunel University London.\n<https://www.brunel.ac.uk/research/Brunel-Innovations/Magic-from-invention>.\n\nMahdawi, Arwa. “The Domino’s ‘Pizza Checker’ Is Just the Beginning—Workplace\nSurveillance Is Coming for You.” _Guardian_ , October 15, 2019.\n<https://www.theguardian.com/commentisfree/2019/oct/15/the-dominos-pizza-\nchecker-is-just-the-beginning-workplace-surveillance-is-coming-for-you>.\n\nMarcus, Mitchell P., Mary Ann Marcinkiewicz, and Beatrice Santorini. “Building\na Large Annotated Corpus of English: The Penn Treebank.” _Computational\nLinguistics_ 19, no. 2 (1993): 313–30.\n<https://dl.acm.org/doi/abs/10.5555/972470.972475>.\n\nMarkoff, John. “Pentagon Turns to Silicon Valley for Edge in Artificial\nIntelligence.” _New York Times_ , May 11, 2016.\n<https://www.nytimes.com/2016/05/12/technology/artificial-intelligence-as-the-\npentagons-latest-weapon.html>.\n\n———. “Seeking a Better Way to Find Web Images.” _New York Times_ , November\n19, 2012. <https://www.nytimes.com/2012/11/20/science/for-web-images-creating-\nnew-technology-to-seek-and-find.html>.\n\n———. “Skilled Work, without the Worker.” _New York Times_ , August 18, 2012.\n<https://www.nytimes.com/2012/08/19/business/new-wave-of-adept-robots-is-\nchanging-global-industry.html>.\n\nMartinage, Robert. “Toward a New Offset Strategy: Exploiting U.S. Long-Term\nAdvantages to Restore U.S. Global Power Projection Capability.” Washington,\nD.C.: Center for Strategic and Budgetary Assessments, 2014.\n<https://csbaonline.org/uploads/documents/Offset-Strategy-Web.pdf>.\n\nMarx, Karl. _Das Kapital: A Critique of Political Economy._ Chicago: H.\nRegnery, 1959.\n\n———. _The Poverty of Philosophy._ New York: Progress, 1955.\n\nMarx, Karl, and Friedrich Engels. _The Marx-Engels Reader_ , edited by Robert\nC. Tucker. 2nd ed. New York: W. W. Norton, 1978.\n\nMarx, Paris. “Instead of Throwing Money at the Moon, Jeff Bezos Should Try\nHelping Earth.” _NBC News_ , May 15, 2019.\n<https://www.nbcnews.com/think/opinion/jeff-bezos-blue-origin-space-colony-\ndreams-ignore-plight-millions-ncna1006026>.\n\nMasanet, Eric, Arman Shehabi, Nuoa Lei, Sarah Smith, and Jonathan Koomey.\n“Recalibrating Global Data Center Energy-Use Estimates.” _Science_ 367, no.\n6481 (2020): 984–86.\n\nMatney, Lucas. “More than 100 Million Alexa Devices Have Been Sold.”\n_TechCrunch_ (blog), January 4, 2019.\n<http://social.techcrunch.com/2019/01/04/more-than-100-million-alexa-devices-\nhave-been-sold/>.\n\nMattern, Shannon. “Calculative Composition: The Ethics of Automating Design.”\nIn _The Oxford Handbook of Ethics of AI_ , edited by Markus D. Dubber, Frank\nPasquale, and Sunit Das, 572–92. Oxford: Oxford University Press, 2020.\n\n———. _Code and Clay, Data and Dirt: Five Thousand Years of Urban Media._\nMinneapolis: University of Minnesota Press, 2017.\n\nMaughan, Tim. “The Dystopian Lake Filled by the World’s Tech Lust.” BBC\nFuture, April 2, 2015. <https://www.bbc.com/future/article/20150402-the-worst-\nplace-on-earth>.\n\nMayhew, Claire, and Michael Quinlan. “Fordism in the Fast Food Industry:\nPervasive Management Control and Occupational Health and Safety Risks for\nYoung Temporary Workers.” _Sociology of Health and Illness_ 24, no. 3 (2002):\n261–84. <https://doi.org/10.1111/1467-9566.00294>.\n\nMayr, Ernst. _The Growth of Biological Thought: Diversity, Evolution, and\nInheritance._ Cambridge, Mass.: Harvard University Press, 1982.\n\nMbembé, Achille. _Critique of Black Reason._ Durham, N.C.: Duke University\nPress, 2017.\n\n———. _Necropolitics._ Durham, N.C.: Duke University Press, 2019.\n\nMbembé, Achille, and Libby Meintjes. “Necropolitics.” _Public Culture_ 15, no.\n1 (2003): 11–40. <https://www.muse.jhu.edu/article/39984>.\n\nMcCorduck, Pamela. _Machines Who Think: A Personal Inquiry into the History\nand Prospects of Artificial Intelligence._ Natick, Mass.: A. K. Peters, 2004.\n\nMcCurry, Justin. “Fukushima Disaster: Japanese Power Company Chiefs Cleared of\nNegligence.” _Guardian_ , September 19, 2019.\n<https://www.theguardian.com/environment/2019/sep/19/fukushima-disaster-\njapanese-power-company-chiefs-cleared-of-negligence>.\n\n———. “Fukushima Nuclear Disaster: Former Tepco Executives Go on Trial.”\n_Guardian_ , June 30, 2017.\n<https://www.theguardian.com/environment/2017/jun/30/fukushima-nuclear-crisis-\ntepco-criminal-trial-japan>.\n\nMcDuff, Daniel, et al. “Affectiva-MIT Facial Expression Dataset (AM-FED):\nNaturalistic and Spontaneous Facial Expressions Collected ‘In-the-Wild.’” In\n_2013 IEEE Conference on Computer Vision and Pattern Recognition Workshops_ ,\n881–88. <https://doi.org/10.1109/CVPRW.2013.130>.\n\nMcIlwain, Charlton. _Black Software: The Internet and Racial Justice, from the\nAfroNet to Black Lives Matter._ New York: Oxford University Press, 2019.\n\nMcLuhan, Marshall. _Understanding Media: The Extensions of Man._ Reprint ed.\nCambridge, Mass.: MIT Press, 1994.\n\nMcMillan, Graeme. “It’s Not You, It’s It: Voice Recognition Doesn’t Recognize\nWomen.” _Time_ , June 1, 2011. <http://techland.time.com/2011/06/01/its-not-\nyou-its-it-voice-recognition-doesnt-recognize-women/>.\n\nMcNamara, Robert S., and James G. Blight. _Wilson’s Ghost: Reducing the Risk\nof Conflict, Killing, and Catastrophe in the 21st Century._ New York: Public\nAffairs, 2001.\n\nMcNeil, Joanne. “Two Eyes See More Than Nine.” In _Jon Rafman: Nine Eyes_ ,\nedited by Kate Steinmann. Los Angeles: New Documents, 2016.\n\nMead, Margaret. Review of _Darwin and Facial Expression: A Century of Research\nin Review_ , edited by Paul Ekman. _Journal of Communication_ 25, no. 1\n(1975): 209–40. <https://doi.org/10.1111/j.1460-2466.1975.tb00574.x>.\n\nMeadows, Donella H., et al. _The Limits to Growth._ New York: Signet, 1972.\n\nMenabrea, Luigi Federico, and Ada Lovelace. “Sketch of the Analytical Engine\nInvented by Charles Babbage.” The Analytical Engine.\n<https://www.fourmilab.ch/babbage/sketch.html>.\n\nMerler, Michele, et al. “Diversity in Faces.” _ArXiv:1901.10436 [Cs]_ , April\n8, 2019. <http://arxiv.org/abs/1901.10436>.\n\nMetcalf, Jacob, and Kate Crawford. “Where Are Human Subjects in Big Data\nResearch? The Emerging Ethics Divide.” _Big Data and Society_ 3, no. 1 (2016):\n1–14. <https://doi.org/10.1177/2053951716650211>.\n\nMetcalf, Jacob, Emanuel Moss, and danah boyd. “Owning Ethics: Corporate\nLogics, Silicon Valley, and the Institutionalization of Ethics.”\n_International Quarterly_ 82, no. 2 (2019): 449–76.\n\nMeulen, Rob van der. “Gartner Says 8.4 Billion Connected ‘Things’ Will Be in\nUse in 2017, Up 31 Percent from 2016.” _Gartner_ , February 7, 2017.\n<https://www.gartner.com/en/newsroom/press-releases/2017-02-07-gartner-\nsays-8-billion-connected-things-will-be-in-use-in-2017-up-31-percent-\nfrom-2016>.\n\nMeyer, John W., and Ronald L. Jepperson. “The ‘Actors’ of Modern Society: The\nCultural Construction of Social Agency.” _Sociological Theory_ 18, no. 1\n(2000): 100–120. <https://doi.org/10.1111/0735-2751.00090>.\n\nMezzadra, Sandro, and Brett Neilson. “On the Multiple Frontiers of Extraction:\nExcavating Contemporary Capitalism.” _Cultural Studies_ 31, no. 2–3 (2017):\n185–204. <https://doi.org/10.1080/09502386.2017.1303425>.\n\nMichalski, Ryszard S. “Pattern Recognition as Rule-Guided Inductive\nInference.” _IEEE Transactions on Pattern Analysis Machine Intelligence_ 2,\nno. 4 (1980): 349–61. <https://doi.org/10.1109/TPAMI.1980.4767034>.\n\nMichel, Arthur Holland. _Eyes in the Sky: The Secret Rise of Gorgon Stare and\nHow It Will Watch Us All._ Boston: Houghton Mifflin Harcourt, 2019.\n\nMikel, Betsy. “WeWork Just Made a Disturbing Acquisition; It Raises a Lot of\nFlags about Workers’ Privacy.” Inc.com, February 17, 2019.\n<https://www.inc.com/betsy-mikel/wework-is-trying-a-creepy-new-strategy-it-\njust-might-signal-end-of-workplace-as-we-know-it.html>.\n\nMirzoeff, Nicholas. _The Right to Look: A Counterhistory of Visuality._\nDurham, N.C.: Duke University Press, 2011.\n\nMitchell, Margaret, et al. “Model Cards for Model Reporting.” In _FAT* ’19:\nProceedings of the Conference on Fairness, Accountability, and Transparency_ ,\n220–29. Atlanta: ACM Press, 2019. <https://doi.org/10.1145/3287560.3287596>.\n\nMitchell, Paul Wolff. “The Fault in His Seeds: Lost Notes to the Case of Bias\nin Samuel George Morton’s Cranial Race Science.” _PLOS Biology_ 16, no. 10\n(2018): e2007008. <https://doi.org/10.1371/journal.pbio.2007008>.\n\nMitchell, Tom M. “The Need for Biases in Learning Generalizations.” Working\npaper, Rutgers University, May 1980.\n\nMitchell, W. J. T. _Picture Theory: Essays on Verbal and Visual\nRepresentation._ Chicago.: University of Chicago Press, 1994.\n\nMittelstadt, Brent. “Principles Alone Cannot Guarantee Ethical AI.” _Nature\nMachine Intelligence_ 1, no. 11 (2019): 501–7.\n<https://doi.org/10.1038/s42256-019-0114-4>.\n\nMohamed, Shakir, Marie-Therese Png, and William Isaac. “Decolonial AI:\nDecolonial Theory as Sociotechnical Foresight in Artificial Intelligence.”\n_Philosophy and Technology_ (2020): 405.\n<https://doi.org/10.1007/s13347-020-00405-8>.\n\nMoll, Joana. “CO2GLE.” <http://www.janavirgin.com/CO2/>.\n\nMolnar, Phillip, Gary Robbins, and David Pierson. “Cutting Edge: Apple’s\nPurchase of Emotient Fuels Artificial Intelligence Boom in Silicon Valley.”\n_Los Angeles Times_ , January 17, 2016.\n<https://www.latimes.com/business/technology/la-fi-cutting-edge-facial-\nrecognition-20160117-story.html>.\n\nMorris, David Z. “Major Advertisers Flee YouTube over Videos Exploiting\nChildren.” _Fortune_ , November 26, 2017.\n<https://fortune.com/2017/11/26/advertisers-flee-youtube-child-exploitation/>.\n\nMorton, Timothy. _Hyperobjects: Philosophy and Ecology after the End of the\nWorld._ Minneapolis: University of Minnesota Press, 2013.\n\nMosco, Vincent. _To the Cloud: Big Data in a Turbulent World._ Boulder, Colo.:\nParadigm, 2014.\n\nMüller-Maguhn, Andy, et al. “The NSA Breach of Telekom and Other German\nFirms.” _Spiegel_ , September 14, 2014.\n<https://www.spiegel.de/international/world/snowden-documents-indicate-nsa-\nhas-breached-deutsche-telekom-a-991503.html>.\n\nMumford, Lewis. “The First Megamachine.” _Diogenes_ 14, no. 55 (1966): 1–15.\n<https://doi.org/10.1177/039219216601405501>.\n\n———. _The Myth of the Machine._ Vol. 1: _Technics and Human Development._ New\nYork: Harcourt Brace Jovanovich, 1967.\n\n———. _Technics and Civilization._ Chicago: University of Chicago Press, 2010.\n\nMurgia, Madhumita, and Max Harlow. “Who’s Using Your Face? The Ugly Truth\nabout Facial Recognition.” _Financial Times_ , April 19, 2019.\n<https://www.ft.com/content/cf19b956-60a2-11e9-b285-3acd5d43599e>.\n\nMuse, Abdi. “Organizing Tech.” AI Now 2019 Symposium, AI Now Institute, 2019.\n<https://ainowinstitute.org/symposia/2019-symposium.html>.\n\nNakashima, Ellen, and Joby Warrick. “For NSA Chief, Terrorist Threat Drives\nPassion to ‘Collect It All.’” _Washington Post_ , July 14, 2013.\n<https://www.washingtonpost.com/world/national-security/for-nsa-chief-\nterrorist-threat-drives-passion-to-collect-it-\nall/2013/07/14/3d26ef80-ea49-11e2-a301-ea5a8116d211_story.html>.\n\nNASA. “Outer Space Treaty of 1967.” NASA History, 1967.\n<https://history.nasa.gov/1967treaty.html>.\n\nNassar, Nedal, et al. “Evaluating the Mineral Commodity Supply Risk of the US\nManufacturing Sector.” _Science Advances_ 6, no. 8 (2020): eaa8647.\n<https://www.doi.org/10.1126/sciadv.aay8647>.\n\nNatarajan, Prem. “Amazon and NSF Collaborate to Accelerate Fairness in AI\nResearch.” _Alexa Blogs_ (blog), March 25, 2019.\n<https://developer.amazon.com/blogs/alexa/post/1786ea03-2e55-4a93-9029-5df88c200ac1/amazon-\nand-nsf-collaborate-to-accelerate-fairness-in-ai-research>.\n\nNational Institute of Standards and Technology (NIST). “Special Database\n32—Multiple Encounter Dataset (MEDS).” <https://www.nist.gov/itl/iad/image-\ngroup/special-database-32-multiple-encounter-dataset-meds>.\n\nNedlund, Evelina. “Apple Card Is Accused of Gender Bias; Here’s How That Can\nHappen.” _CNN_ , November 12, 2019.\n<https://edition.cnn.com/2019/11/12/business/apple-card-gender-\nbias/index.html>.\n\nNegroni, Christine. “How to Determine the Power Rating of Your Gadget’s\nBatteries.” _New York Times_ , December 26, 2016.\n<https://www.nytimes.com/2016/12/26/business/lithium-ion-battery-airline-\nsafety.html>.\n\n“Neighbors by Ring: Appstore for Android.” Amazon.\n<https://www.amazon.com/Ring-Neighbors-by/dp/B07V7K49QT>.\n\nNelson, Alondra. _The Social Life of DNA: Race, Reparations, and\nReconciliation after the Genome._ Boston: Beacon, 2016.\n\nNelson, Alondra, Thuy Linh N. Tu, and Alicia Headlam Hines. “Introduction:\nHidden Circuits.” In _Technicolor: Race, Technology, and Everyday Life_ ,\nedited by Alondra Nelson, Thuy Linh N. Tu, and Alicia Headlam Hines, 1–12. New\nYork: New York University Press 2001.\n\nNelson, Francis W., and Henry Kucera. _Brown Corpus Manual: Manual of\nInformation to Accompany a Standard Corpus of Present-Day Edited American\nEnglish for Use with Digital Computers._ Providence, R.I.: Brown University,\n1979. <http://icame.uib.no/brown/bcm.html>.\n\nNelson, Robin. “Racism in Science: The Taint That Lingers.” _Nature_ 570\n(2019): 440–41. <https://doi.org/10.1038/d41586-019-01968-z>.\n\nNewman, Lily Hay. “Internal Docs Show How ICE Gets Surveillance Help From\nLocal Cops.” _Wired_ , March 13, 2019. <https://www.wired.com/story/ice-\nlicense-plate-surveillance-vigilant-solutions/>.\n\nNielsen, Kim E. _A Disability History of the United States._ Boston: Beacon,\n2012.\n\nNietzsche, Friedrich. _Sämtliche Werke._ Vol. 11. Berlin: de Gruyter, 1980.\n\nNilsson, Nils J. _The Quest for Artificial Intelligence: A History of Ideas\nand Achievements._ New York: Cambridge University Press, 2009.\n\nNilsson, Patricia. “How AI Helps Recruiters Track Jobseekers’ Emotions.”\n_Financial Times_ , February 28, 2018.\n<https://www.ft.com/content/e2e85644-05be-11e8-9650-9c0ad2d7c5b5>.\n\nNoble, Safiya Umoja. _Algorithms of Oppression: How Search Engines Reinforce\nRacism._ New York: NYU Press, 2018.\n\n“NSA Phishing Tactics and Man in the Middle Attacks.” _The Intercept_ (blog),\nMarch 12, 2014. <https://theintercept.com/document/2014/03/12/nsa-phishing-\ntactics-man-middle-attacks/>.\n\n“Off Now: How Your State Can Help Support the Fourth Amendment.” OffNow.org.\n_<https://s3.amazonaws.com/TAChandbooks/OffNow-Handbook.pdf>._\n\nOhm, Paul. “Don’t Build a Database of Ruin.” _Harvard Business Review_ ,\nAugust 23, 2012. <https://hbr.org/2012/08/dont-build-a-database-of-ruin>.\n\nOhtake, Miyoko. “Psychologist Paul Ekman Delights at Exploratorium.” _WIRED_ ,\nJanuary 28, 2008. <https://www.wired.com/2008/01/psychologist-pa/>.\n\nO’Neil, Cathy. _Weapons of Math Destruction: How Big Data Increases Inequality\nand Threatens Democracy._ New York: Crown, 2016.\n\nO’Neill, Gerard K. _The High Frontier: Human Colonies in Space._ 3rd ed.\nBurlington, Ont.: Apogee Books, 2000.\n\n“One-Year Limited Warranty for Amazon Devices or Accessories.” Amazon.\n<https://www.amazon.com/gp/help/customer/display.html?nodeId=201014520>.\n\n“An Open Letter.” <https://ethuin.files.wordpress.com/2014/09/09092014-open-\nletter-final-and-list.pdf>.\n\n_Organizing Tech._ Video, AI Now Institute, 2019.\n[https://www.youtube.com/watch?v=jLeOyIS1jwc&feature=emb_title](https://www.youtube.com/watch?v=jLeOyIS1jwc&feature=emb_title).\n\nOsumi, Magdalena. “Former Tepco Executives Found Not Guilty of Criminal\nNegligence in Fukushima Nuclear Disaster.” _Japan Times Online_ , September\n19, 2019. <https://www.japantimes.co.jp/news/2019/09/19/national/crime-\nlegal/tepco-trio-face-tokyo-court-ruling-criminal-case-stemming-fukushima-\nnuclear-disaster/>.\n\n“Our Mission.” Blue Origin. <https://www-dev.blueorigin.com/our-mission>.\n\nPaglen, Trevor. “Operational Images.” _e-flux_ , November 2014.\n<https://www.e-flux.com/journal/59/61130/operational-images/>.\n\nPalantir. “Palantir Gotham.” <https://palantir.com/palantir-\ngotham/index.html>.\n\n“Palantir and Cambridge Analytica: What Do We Know?” _WikiTribune_ , March 27,\n2018. <https://www.wikitribune.com/wt/news/article/58386/>.\n\nPande, Vijay. “Artificial Intelligence’s ‘Black Box’ Is Nothing to Fear.” _New\nYork Times_ , January 25, 2018.\n<https://www.nytimes.com/2018/01/25/opinion/artificial-intelligence-black-\nbox.html>.\n\nPapert, Seymour A. “The Summer Vision Project.” July 1, 1966.\n<https://dspace.mit.edu/handle/1721.1/6125>.\n\nParikka, Jussi. _A Geology of Media._ Minneapolis: University of Minnesota\nPress, 2015.\n\nPasquale, Frank. _The Black Box Society: The Secret Algorithms That Control\nMoney and Information._ Cambridge, Mass.: Harvard University Press, 2015.\n\nPatterson, Scott, and Alexandra Wexler. “Despite Cleanup Vows, Smartphones and\nElectric Cars Still Keep Miners Digging by Hand in Congo.” _Wall Street\nJournal_ , September 13, 2018. <https://www.wsj.com/articles/smartphones-\nelectric-cars-keep-miners-digging-by-hand-in-congo-1536835334>.\n\nPaul Ekman Group. <https://www.paulekman.com/>.\n\nPellerin, Cheryl. “Deputy Secretary: Third Offset Strategy Bolsters America’s\nMilitary Deterrence.” Washington, D.C.: U.S. Department of Defense, October\n31, 2016. <https://www.defense.gov/Explore/News/Article/Article/991434/deputy-\nsecretary-third-offset-strategy-bolsters-americas-military-deterrence/>.\n\nPerez, Sarah. “Microsoft Silences Its New A.I. Bot Tay, after Twitter Users\nTeach It Racism [Updated].” _TechCrunch_ (blog), March 24, 2016.\n<http://social.techcrunch.com/2016/03/24/microsoft-silences-its-new-a-i-bot-\ntay-after-twitter-users-teach-it-racism/>.\n\nPfungst, Oskar. _Clever Hans (The Horse of Mr. von Osten): A Contribution to\nExperimental Animal and Human Psychology._ Translated by Carl L. Rahn. New\nYork: Henry Holt, 1911.\n\nPhillips, P. Jonathon, Patrick J. Rauss, and Sandor Z. Der. “FERET (Face\nRecognition Technology) Recognition Algorithm Development and Test Results.”\nAdelphi, Md.: Army Research Laboratory, October 1996.\n<https://apps.dtic.mil/dtic/tr/fulltext/u2/a315841.pdf>.\n\nPicard, Rosalind. “Affective Computing Group.” MIT Media Lab.\n<https://affect.media.mit.edu/>.\n\nPichai, Sundar. “AI at Google: Our Principles.” Google, June 7, 2018.\n<https://blog.google/technology/ai/ai-principles/>.\n\nPlumwood, Val. “The Politics of Reason: Towards a Feminist Logic.”\n_Australasian Journal of Philosophy_ 71, no. 4 (1993): 436–62.\n<https://doi.org/10.1080/00048409312345432>.\n\nPoggio, Tomaso, et al. “Why and When Can Deep—but not Shallow—Networks Avoid\nthe Curse of Dimensionality: A Review.” _International Journal of Automation\nand Computing_ 14, no. 5 (2017): 503–19.\n<https://link.springer.com/article/10.1007/s11633-017-1054-2>.\n\nPontin, Jason. “Artificial Intelligence, with Help from the Humans.” _New York\nTimes_ , March 25, 2007.\n<https://www.nytimes.com/2007/03/25/business/yourmoney/25Stream.html>.\n\nPontin, Mark Williams. “Lie Detection.” _MIT Technology Review_ , April 21,\n2009. <https://www.technologyreview.com/s/413133/lie-detection/>.\n\nPowell, Corey S. “Jeff Bezos Foresees a Trillion People Living in Millions of\nSpace Colonies.” _NBC News_ , May 15, 2019.\n<https://www.nbcnews.com/mach/science/jeff-bezos-foresees-trillion-people-\nliving-millions-space-colonies-here-ncna1006036>.\n\n“Powering the Cloud: How China’s Internet Industry Can Shift to Renewable\nEnergy.” Greenpeace, September 9, 2019.\n<https://storage.googleapis.com/planet4-eastasia-\nstateless/2019/11/7bfe9069-7bfe9069-powering-the-cloud-_-english-\nbriefing.pdf>.\n\nPratt, Mary Louise. “Arts of the Contact Zone.” _Profession, Ofession_ (1991):\n33–40.\n\n———. _Imperial Eyes: Travel Writing and Transculturation._ 2nd ed. London:\nRoutledge, 2008.\n\nPriest, Dana. “NSA Growth Fueled by Need to Target Terrorists.” _Washington\nPost_ , July 21, 2013. <https://www.washingtonpost.com/world/national-\nsecurity/nsa-growth-fueled-by-need-to-target-\nterrorists/2013/07/21/24c93cf4-f0b1-11e2-bed3-b9b6fe264871_story.html>.\n\nPryzbylski, David J. “Changes Coming to NLRB’s Stance on Company E-Mail\nPolicies?” _National Law Review_ , August 2, 2018.\n<https://www.natlawreview.com/article/changes-coming-to-nlrb-s-stance-company-\ne-mail-policies>.\n\nPuar, Jasbir K. _Terrorist Assemblages: Homonationalism in Queer Times._ 2nd\ned. Durham, N.C.: Duke University Press, 2017.\n\nPugliese, Joseph. “Death by Metadata: The Bioinformationalisation of Life and\nthe Transliteration of Algorithms to Flesh.” In _Security, Race, Biopower:\nEssays on Technology and Corporeality_ , edited by Holly Randell-Moon and Ryan\nTippet, 3–20. London: Palgrave Macmillan, 2016.\n\nPuschmann, Cornelius, and Jean Burgess. “Big Data, Big Questions: Metaphors of\nBig Data.” _International Journal of Communication_ 8 (2014): 1690–1709.\n\nQiu, Jack. _Goodbye iSlave: A Manifesto for Digital Abolition._ Urbana:\nUniversity of Illinois Press, 2016.\n\nQiu, Jack, Melissa Gregg, and Kate Crawford. “Circuits of Labour: A Labour\nTheory of the iPhone Era.” _TripleC: Communication, Capitalism and Critique_\n12, no. 2 (2014). <https://doi.org/10.31269/triplec.v12i2.540>.\n\n“Race after Technology, Ruha Benjamin.” Meeting minutes, Old Guard of\nPrinceton, N.J., November 14, 2018.\n<https://www.theoldguardofprinceton.org/11-14-2018.html>.\n\nRaji, Inioluwa Deborah, and Joy Buolamwini. “Actionable Auditing:\nInvestigating the Impact of Publicly Naming Biased Performance Results of\nCommercial AI Products.” In _Proceedings of the 2019 AAAI/ACM Conference on\nAI, Ethics, and Society,_ 429–35. 2019.\n\nRaji, Inioluwa Deborah, Timnit Gebru, Margaret Mitchell, Joy Buolamwini,\nJoonseok Lee, and Emily Denton. “Saving Face: Investigating the Ethical\nConcerns of Facial Recognition Auditing.” In _Proceedings of the AAAI/ACM\nConference on AI, Ethics, and Society,_ 145–51. 2020.\n\nRamachandran, Vilayanur S., and Diane Rogers-Ramachandran. “Aristotle’s\nError.” _Scientific American_ , March 1, 2010.\n<https://doi.org/10.1038/scientificamericanmind0310-20>.\n\nRankin, Joy Lisi. _A People’s History of Computing in the United States._\nCambridge, Mass.: Harvard University Press, 2018.\n\n———. “Remembering the Women of the Mathematical Tables Project.” _The New\nInquiry_ (blog), March 14, 2019. <https://thenewinquiry.com/blog/remembering-\nthe-women-of-the-mathematical-tables-project/>.\n\nRehmann, Jan. “Taylorism and Fordism in the Stockyards.” In _Max Weber:\nModernisation as Passive Revolution_ , 24–29. Leiden, Netherlands: Brill,\n2015.\n\nReichhardt, Tony. “First Photo from Space.” _Air and Space Magazine_ , October\n24, 2006. <https://www.airspacemag.com/space/the-first-photo-from-\nspace-13721411/>.\n\nRein, Hanno, Daniel Tamayo, and David Vokrouhlicky. “The Random Walk of Cars\nand Their Collision Probabilities with Planets.” _Aerospace_ 5, no. 2 (2018):\n57. <https://doi.org/10.3390/aerospace5020057>.\n\n“Responsible Minerals Policy and Due Diligence.” Philips.\n<https://www.philips.com/a-w/about/company/suppliers/supplier-\nsustainability/our-programs/responsible-sourcing-of-minerals.html>.\n\n“Responsible Minerals Sourcing.” Dell.\n<https://www.dell.com/learn/us/en/uscorp1/conflict-minerals?s=corp>.\n\nRevell, Timothy. “Google DeepMind’s NHS Data Deal ‘Failed to Comply’ with\nLaw.” _New Scientist_ , July 3, 2017.\n<https://www.newscientist.com/article/2139395-google-deepminds-nhs-data-deal-\nfailed-to-comply-with-law/>.\n\nRhue, Lauren. “Racial Influence on Automated Perceptions of Emotions.”\nNovember 9, 2018. <https://dx.doi.org/10.2139/ssrn.3281765>.\n\nRichardson, Rashida, Jason M. Schultz, and Kate Crawford. “Dirty Data, Bad\nPredictions: How Civil Rights Violations Impact Police Data, Predictive\nPolicing Systems, and Justice.” _NYU Law Review Online_ 94, no. 15 (2019):\n15–55. <https://www.nyulawreview.org/wp-\ncontent/uploads/2019/04/NYULawReview-94-Richardson-Schultz-Crawford.pdf>.\n\nRichardson, Rashida, Jason M. Schultz, and Vincent M. Southerland. “Litigating\nAlgorithms: 2019 US Report.” AI Now Institute, September 2019.\n<https://ainowinstitute.org/litigatingalgorithms-2019-us.pdf>.\n\nRisen, James, and Laura Poitras. “N.S.A. Report Outlined Goals for More\nPower.” _New York Times_ , November 22, 2013.\n<https://www.nytimes.com/2013/11/23/us/politics/nsa-report-outlined-goals-for-\nmore-power.html>.\n\nRobbins, Martin. “How Can Our Future Mars Colonies Be Free of Sexism and\nRacism?” _Guardian_ , May 6, 2015. <https://www.theguardian.com/science/the-\nlay-scientist/2015/may/06/how-can-our-future-mars-colonies-be-free-of-sexism-\nand-racism>.\n\nRoberts, Dorothy. _Fatal Invention: How Science, Politics, and Big Business\nRe-Create Race in the Twenty-First Century._ New York: New Press, 2011.\n\nRoberts, Sarah T. _Behind the Screen: Content Moderation in the Shadows of\nSocial Media._ New Haven: Yale University Press, 2019.\n\nRomano, Benjamin. “Suits Allege Amazon’s Alexa Violates Laws by Recording\nChildren’s Voices without Consent.” _Seattle Times_ , June 12, 2019.\n<https://www.seattletimes.com/business/amazon/suit-alleges-amazons-alexa-\nviolates-laws-by-recording-childrens-voices-without-consent/>.\n\nRomm, Tony. “U.S. Government Begins Asking Foreign Travelers about Social\nMedia.” _Politico_ , December 22, 2016.\n<https://www.politico.com/story/2016/12/foreign-travelers-social-\nmedia-232930>.\n\nRouast, Philipp V., Marc Adam, and Raymond Chiong. “Deep Learning for Human\nAffect Recognition: Insights and New Developments.” In _IEEE_ _Transactions on\nAffective Computing_ , 2019, 1. <https://doi.org/10.1109/TAFFC.2018.2890471>.\n\n“Royal Free–Google DeepMind Trial Failed to Comply with Data Protection Law.”\nInformation Commissioner’s Office, July 3, 2017. <https://ico.org.uk/about-\nthe-ico/news-and-events/news-and-blogs/2017/07/royal-free-google-deepmind-\ntrial-failed-to-comply-with-data-protection-law/>.\n\nRussell, Andrew. _Open Standards and the Digital Age: History, Ideology, and\nNetworks._ New York: Cambridge University Press, 2014.\n\nRussell, James A. “Is There Universal Recognition of Emotion from Facial\nExpression? A Review of the Cross-Cultural Studies.” _Psychological Bulletin_\n115, no. 1 (1994): 102–41. <https://doi.org/10.1037/0033-2909.115.1.102>.\n\nRussell, Stuart J., and Peter Norvig. _Artificial Intelligence: A Modern\nApproach._ 3rd ed. Upper Saddle River, N.J.: Pearson, 2010.\n\nSadowski, Jathan. “When Data Is Capital: Datafication, Accumulation, and\nExtraction.” _Big Data and Society_ 6, no. 1 (2019): 1–12.\n<https://doi.org/10.1177/2053951718820549>.\n\nSadowski, Jathan. “Potemkin AI.” _Real Life,_ August 6, 2018.\n\nSample, Ian. “What Is the Internet? 13 Key Questions Answered.” _Guardian_ ,\nOctober 22, 2018. <https://www.theguardian.com/technology/2018/oct/22/what-is-\nthe-internet-13-key-questions-answered>.\n\nSánchez-Monedero, Javier, and Lina Dencik. “The Datafication of the\nWorkplace.” Working paper, Data Justice Lab, Cardiff University, May 9, 2019.\n<https://datajusticeproject.net/wp-content/uploads/sites/30/2019/05/Report-\nThe-datafication-of-the-workplace.pdf>.\n\nSanville, Samantha. “Towards Humble Geographies.” _Area_ (2019): 1–9.\n<https://doi.org/10.1111/area.12664>.\n\nSatisky, Jake. “A Duke Study Recorded Thousands of Students’ Faces; Now\nThey’re Being Used All over the World.” _Chronicle_ , June 12, 2019.\n<https://www.dukechronicle.com/article/2019/06/duke-university-facial-\nrecognition-data-set-study-surveillance-video-students-china-uyghur>.\n\nScahill, Jeremy, and Glenn Greenwald. “The NSA’s Secret Role in the U.S.\nAssassination Program.” _The Intercept_ (blog), February 10, 2014.\n<https://theintercept.com/2014/02/10/the-nsas-secret-role/>.\n\nSchaake, Marietje. “What Principles Not to Disrupt: On AI and Regulation.”\n_Medium_ (blog), November 5, 2019. <https://medium.com/@marietje.schaake/what-\nprinciples-not-to-disrupt-on-ai-and-regulation-cabbd92fd30e>.\n\nSchaffer, Simon. “Babbage’s Calculating Engines and the Factory System.”\n_Réseaux: Communication—Technologie—Société_ 4, no. 2 (1996): 271–98.\n<https://doi.org/10.3406/reso.1996.3315>.\n\nScharmen, Fred. _Space Settlements._ New York: Columbia University Press,\n2019.\n\nScharre, Paul, et al. “Eric Schmidt Keynote Address at the Center for a New\nAmerican Security Artificial Intelligence and Global Security Summit.” Center\nfor a New American Security, November 13, 2017.\n<https://www.cnas.org/publications/transcript/eric-schmidt-keynote-address-at-\nthe-center-for-a-new-american-security-artificial-intelligence-and-global-\nsecurity-summit>.\n\nScheuerman, Morgan Klaus, et al. “How We’ve Taught Algorithms to See Identity:\nConstructing Race and Gender in Image Databases for Facial Analysis.”\n_Proceedings of the ACM on Human-Computer Interaction_ 4, issue CSCW1 (2020):\n1–35. <https://doi.org/10.1145/3392866>.\n\nScheyder, Ernest. “Tesla Expects Global Shortage of Electric Vehicle Battery\nMinerals.” _Reuters_ , May 2, 2019. <https://www.reuters.com/article/us-usa-\nlithium-electric-tesla-exclusive-idUSKCN1S81QS>.\n\nSchlanger, Zoë. “If Shipping Were a Country, It Would Be the Sixth-Biggest\nGreenhouse Gas Emitter.” _Quartz_ , April 17, 2018.\n<https://qz.com/1253874/if-shipping-were-a-country-it-would-the-worlds-sixth-\nbiggest-greenhouse-gas-emitter/>.\n\nSchmidt, Eric. “I Used to Run Google; Silicon Valley Could Lose to China.”\n_New York Times_ , February 27, 2020.\n<https://www.nytimes.com/2020/02/27/opinion/eric-schmidt-ai-china.html>.\n\nSchneier, Bruce. “Attacking Tor: How the NSA Targets Users’ Online Anonymity.”\n_Guardian_ , October 4, 2013.\n<https://www.theguardian.com/world/2013/oct/04/tor-attacks-nsa-users-online-\nanonymity>.\n\nSchwartz, Oscar. “Don’t Look Now: Why You Should Be Worried about Machines\nReading Your Emotions.” _Guardian_ , March 6, 2019.\n<https://www.theguardian.com/technology/2019/mar/06/facial-recognition-\nsoftware-emotional-science>.\n\nScott, James C. _Seeing Like a State: How Certain Schemes to Improve the Human\nCondition Have Failed._ New Haven: Yale University Press, 1998.\n\nSedgwick, Eve Kosofsky. _Touching Feeling: Affect, Pedagogy, Performativity._\nDurham, N.C.: Duke University Press, 2003.\n\nSedgwick, Eve Kosofsky, Adam Frank, and Irving E. Alexander, eds. _Shame and\nIts Sisters: A Silvan Tomkins Reader._ Durham, N.C.: Duke University Press,\n1995.\n\nSekula, Allan. “The Body and the Archive.” _October_ 39 (1986): 3–64.\n<https://doi.org/10.2307/778312>.\n\nSenechal, Thibaud, Daniel McDuff, and Rana el Kaliouby. “Facial Action Unit\nDetection Using Active Learning and an Efficient Non-Linear Kernel\nApproximation.” In _2015 IEEE International Conference on Computer Vision\nWorkshop (ICCVW)_ , 10–18. <https://doi.org/10.1109/ICCVW.2015.11>.\n\nSenior, Ana. “John Hancock Leaves Traditional Life Insurance Model Behind to\nIncentivize Longer, Healthier Lives.” Press release, John Hancock, September\n19, 2018.\n\nSeo, Sungyong, et al. “Partially Generative Neural Networks for Gang Crime\nClassification with Partial Information.” In _Proceedings of the 2018 AAAI/ACM\nConference on AI, Ethics, and Society_ , 257–263.\n<https://doi.org/10.1145/3278721.3278758>.\n\nShaer, Matthew. “The Asteroid Miner’s Guide to the Galaxy.” _Foreign Policy_\n(blog), April 28, 2016. <https://foreignpolicy.com/2016/04/28/the-asteroid-\nminers-guide-to-the-galaxy-space-race-mining-asteroids-planetary-research-\ndeep-space-industries/>.\n\nShane, Scott, and Daisuke Wakabayashi. “‘The Business of War’: Google\nEmployees Protest Work for the Pentagon.” _New York Times_ , April 4, 2018.\n<https://www.nytimes.com/2018/04/04/technology/google-letter-ceo-pentagon-\nproject.html>.\n\nShankleman, Jessica, et al. “We’re Going to Need More Lithium.” _Bloomberg_ ,\nSeptember 7, 2017. <https://www.bloomberg.com/graphics/2017-lithium-battery-\nfuture/>.\n\nSHARE Foundation. “Serbian Government Is Implementing Unlawful Video\nSurveillance with Face Recognition in Belgrade.” Policy brief, undated.\n<https://www.sharefoundation.info/wp-content/uploads/Serbia-Video-\nSurveillance-Policy-brief-final.pdf>.\n\nSiebers, Tobin. _Disability Theory._ Ann Arbor: University of Michigan Press,\n2008.\n\nSiegel, Erika H., et al. “Emotion Fingerprints or Emotion Populations? A Meta-\nAnalytic Investigation of Autonomic Features of Emotion Categories.”\n_Psychological Bulletin_ 144, no. 4 (2018): 343–93.\n<https://doi.org/10.1037/bul0000128>.\n\nSilberman, M. S., et al. “Responsible Research with Crowds: Pay Crowdworkers\nat Least Minimum Wage.” _Communications of the ACM_ 61, no. 3 (2018): 39–41.\n<https://doi.org/10.1145/3180492>.\n\nSilver, David, et al. “Mastering the Game of Go without Human Knowledge.”\n_Nature_ 550 (2017): 354–59. <https://doi.org/10.1038/nature24270>.\n\nSimmons, Brandon. “Rekor Software Adds License Plate Reader Technology to Home\nSurveillance, Causing Privacy Concerns.” _WKYC_ , January 31, 2020.\n<https://www.wkyc.com/article/tech/rekor-software-adds-license-plate-reader-\ntechnology-to-home-surveillance-causing-privacy-\nconcerns/95-7c9834d9-5d54-4081-b983-b2e6142a3213>.\n\nSimpson, Cam. “The Deadly Tin inside Your Smartphone.” _Bloomberg_ , August\n24, 2012. <https://www.bloomberg.com/news/articles/2012-08-23/the-deadly-tin-\ninside-your-smartphone>.\n\nSingh, Amarjot. _Eye in the Sky: Real-Time Drone Surveillance System (DSS)_\n_for Violent Individuals Identification._ Video, June 2, 2018.\n[https://www.youtube.com/watch?time_continue=1&v=zYypJPJipYc](https://www.youtube.com/watch?time_continue=1&v=zYypJPJipYc).\n\n“SKYNET: Courier Detection via Machine Learning.” _The Intercept_ (blog), May\n8, 2015. <https://theintercept.com/document/2015/05/08/skynet-courier/>.\n\nSloane, Garett. “Online Ads for High-Paying Jobs Are Targeting Men More Than\nWomen.” _AdWeek_ (blog), July 7, 2015.\n<https://www.adweek.com/digital/seemingly-sexist-ad-targeting-offers-more-men-\nwomen-high-paying-executive-jobs-165782/>.\n\nSmith, Adam. _An Inquiry into the Nature and Causes of the Wealth of Nations._\nChicago: University of Chicago Press, 1976.\n\nSmith, Brad. “Microsoft Will Be Carbon Negative by 2030.” _Official Microsoft\nBlog_ (blog), January 20, 2020.\n<https://blogs.microsoft.com/blog/2020/01/16/microsoft-will-be-carbon-\nnegative-by-2030/>.\n\n———. “Technology and the US Military.” _Microsoft on the Issues_ (blog),\nOctober 26, 2018. <https://blogs.microsoft.com/on-the-\nissues/2018/10/26/technology-and-the-us-military/>.\n\n“Snowden Archive: The SIDtoday Files.” _The Intercept_ (blog), May 29, 2019.\n<https://theintercept.com/snowden-sidtoday/>.\n\nSolon, Olivia. “Facial Recognition’s ‘Dirty Little Secret’: Millions of Online\nPhotos Scraped without Consent.” NBC News, March 12, 2019.\n<https://www.nbcnews.com/tech/internet/facial-recognition-s-dirty-little-\nsecret-millions-online-photos-scraped-n981921>.\n\nSouriau, Étienne. _The Different Modes of Existence._ Translated by Erik\nBeranek and Tim Howles. Minneapolis: University of Minnesota Press, 2015.\n\nSpangler, Todd. “Listen to the Big Ticket with Marc Malkin.” IHeartRadio, May\n3, 2019. <https://www.iheart.com/podcast/28955447/>.\n\nSpargo, John. _Syndicalism, Industrial Unionism, and Socialism_ [1913]. St.\nPetersburg, Fla.: Red and Black, 2009.\n\nSpecht, Joshua. _Red Meat Republic: A Hoof-to-Table History of How Beef\nChanged America._ Princeton, N.J.: Princeton University Press, 2019.\n\nStandage, Tom. _The Turk: The Life and Times of the Famous Eighteenth-Century\nChess-Playing Machine._ New York: Walker, 2002.\n\nStark, Luke. “Facial Recognition Is the Plutonium of AI.” _XRDS: Crossroads,\nThe ACM Magazine for Students_ 25, no. 3 (2019).\n<https://doi.org/10.1145/3313129>.\n\nStark, Luke, and Anna Lauren Hoffmann. “Data Is the New What? Popular\nMetaphors and Professional Ethics in Emerging Data Culture.” _Journal of\nCultural Analytics_ 1, no. 1 (2019). <https://doi.org/10.22148/16.036>.\n\nStarosielski, Nicole. _The Undersea Network._ Durham, N.C.: Duke University\nPress, 2015.\n\nSteadman, Philip. “Samuel Bentham’s Panopticon.” _Journal of Bentham Studies_\n2 (2012): 1–30. <https://doi.org/10.14324/111.2045-757X.044>.\n\nSteinberger, Michael. “Does Palantir See Too Much?” _New York Times Magazine,_\nOctober 21, 2020.\n<https://www.nytimes.com/interactive/2020/10/21/magazine/palantir-alex-\nkarp.html>.\n\nStewart, Ashley, and Nicholas Carlson. “The President of Microsoft Says It\nTook Its Bid for the $10 Billion JEDI Cloud Deal as an Opportunity to Improve\nIts Tech—and That’s Why It Beat Amazon.” _Business Insider_ , January 23,\n2020. <https://www.businessinsider.com/brad-smith-microsofts-jedi-win-over-\namazon-was-no-surprise-2020-1>.\n\nStewart, Russell. _Brainwash Dataset._ Stanford Digital Repository, 2015.\n<https://purl.stanford.edu/sx925dc9385>.\n\nStoller, Bill. “Why the Northern Virginia Data Center Market Is Bigger Than\nMost Realize.” Data Center Knowledge, February 14, 2019.\n<https://www.datacenterknowledge.com/amazon/why-northern-virginia-data-center-\nmarket-bigger-most-realize>.\n\nStrand, Ginger Gail. “Keyword: Evil.” _Harper’s Magazine_ , March 2008.\n<https://harpers.org/archive/2008/03/keyword/>.\n\n“A Strategy for Surveillance Powers.” _New York Times_ , February 23, 2012.\n<https://www.nytimes.com/interactive/2013/11/23/us/politics/23nsa-sigint-\nstrategy-document.html>.\n\n“Street Homelessness.” San Francisco Department of Homelessness and Supportive\nHousing. <http://hsh.sfgov.org/street-homelessness/>.\n\nStrubell, Emma, Ananya Ganesh, and Andrew McCallum. “Energy and Policy\nConsiderations for Deep Learning in NLP.” _ArXiv:1906.02243 [Cs]_ , June 5,\n2019. <http://arxiv.org/abs/1906.02243>.\n\nSuchman, Lucy. “Algorithmic Warfare and the Reinvention of Accuracy.”\n_Critical Studies on Security_ (2020): n. 18.\n<https://doi.org/10.1080/21624887.2020.1760587>.\n\nSullivan, Mark. “Fact: Apple Reveals It Has 900 Million iPhones in the Wild.”\n_Fast Company_ , January 29, 2019. <https://www.fastcompany.com/90298944/fact-\napple-reveals-it-has-900-million-iphones-in-the-wild>.\n\nSutton, Rich. “The Bitter Lesson.” March 13, 2019.\n<http://www.incompleteideas.net/IncIdeas/BitterLesson.html>.\n\nSwinhoe, Dan. “What Is Spear Phishing? Why Targeted Email Attacks Are So\nDifficult to Stop.” CSO Online, January 21, 2019.\n<https://www.csoonline.com/article/3334617/what-is-spear-phishing-why-\ntargeted-email-attacks-are-so-difficult-to-stop.html>.\n\nSzalai, Jennifer. “How the ‘Temp’ Economy Became the New Normal.” _New York\nTimes_ , August 22, 2018. <https://www.nytimes.com/2018/08/22/books/review-\ntemp-louis-hyman.html>.\n\nTani, Maxwell. “The Intercept Shuts Down Access to Snowden Trove.” _Daily_\n_Beast_ , March 14, 2019. <https://www.thedailybeast.com/the-intercept-shuts-\ndown-access-to-snowden-trove>.\n\nTaylor, Astra. “The Automation Charade.” _Logic Magazine_ , August 1, 2018.\n<https://logicmag.io/failure/the-automation-charade/>.\n\n———. _The People’s Platform: Taking Back Power and Culture in the Digital\nAge._ London: Picador, 2015.\n\nTaylor, Frederick Winslow. _The Principles of Scientific Management._ New\nYork: Harper and Brothers, 1911.\n\nTaylor, Jill Bolte. “The 2009 Time 100.” _Time_ , April 30, 2009.\n<http://content.time.com/time/specials/packages/article/0,28804,1894410_1893209_1893475,00.html>.\n\nTheobald, Ulrich. “Liji.” _Chinaknowledge.de_ , July 24, 2010.\n<http://www.chinaknowledge.de/Literature/Classics/liji.html>.\n\nThiel, Peter. “Good for Google, Bad for America.” _New York Times_ , August 1,\n2019. <https://www.nytimes.com/2019/08/01/opinion/peter-thiel-google.html>.\n\nThomas, David Hurst. _Skull Wars: Kennewick Man, Archaeology, and the Battle\nfor Native American Identity._ New York: Basic Books, 2002.\n\nThompson, Edward P. “Time, Work-Discipline, and Industrial Capitalism.” _Past\nand Present_ 38 (1967): 56–97.\n\nTishkoff, Sarah A., and Kenneth K. Kidd. “Implications of Biogeography of\nHuman Populations for ‘Race’ and Medicine.” _Nature Genetics_ 36, no. 11\n(2004): S21–S27. <https://doi.org/10.1038/ng1438>.\n\nTockar, Anthony. “Riding with the Stars: Passenger Privacy in the NYC Taxicab\nDataset.” September 15, 2014. <https://agkn.wordpress.com/2014/09/15/riding-\nwith-the-stars-passenger-privacy-in-the-nyc-taxicab-dataset/>.\n\nTomkins, Silvan S. _Affect Imagery Consciousness: The Complete Edition._ New\nYork: Springer, 2008.\n\nTomkins, Silvan S., and Robert McCarter. “What and Where Are the Primary\nAffects? Some Evidence for a Theory.” _Perceptual and Motor Skills_ 18, no. 1\n(1964): 119–58. <https://doi.org/10.2466/pms.1964.18.1.119>.\n\nToscano, Marion E., and Elizabeth Maynard. “Understanding the Link:\n‘Homosexuality,’ Gender Identity, and the _DSM._ ” _Journal of LGBT Issues in\nCounseling_ 8, no. 3 (2014): 248–63.\n<https://doi.org/10.1080/15538605.2014.897296>.\n\nTrainer, Ted. _Renewable Energy Cannot Sustain a Consumer Society._ Dordrecht,\nNetherlands: Springer, 2007.\n\n“Transforming Intel’s Supply Chain with Real-Time Analytics.” Intel, September\n2017. <https://www.intel.com/content/dam/www/public/us/en/documents/white-\npapers/transforming-supply-chain-with-real-time-analytics-whitepaper.pdf>.\n\nTronchin, Lamberto. “The ‘Phonurgia Nova’ of Athanasius Kircher: The\nMarvellous Sound World of 17th Century.” Conference paper, 155th Meeting\nAcoustical Society of America, January 2008.\n<https://doi.org/10.1121/1.2992053>.\n\nTsukayama, Hayley. “Facebook Turns to Artificial Intelligence to Fight Hate\nand Misinformation in Myanmar.” _Washington Post_ , August 15, 2018.\n<https://www.washingtonpost.com/technology/2018/08/16/facebook-turns-\nartificial-intelligence-fight-hate-misinformation-myanmar/>.\n\nTucker, Patrick. “Refugee or Terrorist? IBM Thinks Its Software Has the\nAnswer.” Defense One, January 27, 2016.\n<https://www.defenseone.com/technology/2016/01/refugee-or-terrorist-ibm-\nthinks-its-software-has-answer/125484/>.\n\nTully, John. “A Victorian Ecological Disaster: Imperialism, the Telegraph, and\nGutta-Percha.” _Journal of World History_ 20, no. 4 (2009): 559–79.\n<https://doi.org/10.1353/jwh.0.0088>.\n\nTuring, A. M. “Computing Machinery and Intelligence.” _Mind_ , October 1,\n1950, 433–60. <https://doi.org/10.1093/mind/LIX.236.433>.\n\nTurner, Graham. “Is Global Collapse Imminent? An Updated Comparison of The\nLimits to Growth with Historical Data.” Research Paper no. 4, Melbourne\nSustainable Society Institute, University of Melbourne, August 2014.\n\nTurner, H. W. “Contribution to the Geology of the Silver Peak Quadrangle,\nNevada.” _Bulletin of the Geological Society of America_ 20, no. 1 (1909):\n223–64.\n\nTuschling, Anna. “The Age of Affective Computing.” In _Timing of Affect:\nEpistemologies, Aesthetics, Politics_ , edited by Marie-Luise Angerer, Bernd\nBösel, and Michaela Ott, 179–90. Zurich: Diaphanes, 2014.\n\nTversky, Amos, and Daniel Kahneman. “Judgment under Uncertainty: Heuristics\nand Biases.” _Science_ 185 (1974): 1124–31.\n<https://doi.org/10.1126/science.185.4157.1124>.\n\nUllman, Ellen. _Life in Code: A Personal History of Technology._ New York:\nMCD, 2017.\n\nUnited Nations Conference on Trade and Development. _Review of Maritime\nTransport, 2017._ <https://unctad.org/en/PublicationsLibrary/rmt2017_en.pdf>.\n\nU.S. Commercial Space Launch Competitiveness Act. Pub. L. No. 114–90, 114th\nCong. (2015). <https://www.congress.gov/114/plaws/publ90/PLAW-114publ90.pdf>.\n\nU.S. Congress. Senate Select Committee on Intelligence Activities. _Covert\nAction in Chile, 1963–1973._ Staff Report, December 18, 1975.\n<https://www.archives.gov/files/declassification/iscap/pdf/2010-009-doc17.pdf>.\n\nU.S. Energy Information Administration, “What Is U.S. Electricity Generation\nby Energy Source?”\n[https://www.eia.gov/tools/faqs/faq.php?id=427&t=21](https://www.eia.gov/tools/faqs/faq.php?id=427&t=21).\n\n“Use of the ‘Not Releasable to Foreign Nationals’ (NOFORN) Caveat on\nDepartment of Defense (DoD) Information.” U.S. Department of Defense, May 17,\n2005. <https://fas.org/sgp/othergov/dod/noforn051705.pdf>.\n\n“UTKFace—Aicip.” <http://aicip.eecs.utk.edu/wiki/UTKFace>.\n\nVidal, John. “Health Risks of Shipping Pollution Have Been ‘Underestimated.’”\n_Guardian_ , April 9, 2009.\n<https://www.theguardian.com/environment/2009/apr/09/shipping-pollution>.\n\n“Vigilant Solutions.” NCPA. <http://www.ncpa.us/Vendors/Vigilant%20Solutions>.\n\nVincent, James. “AI ‘Emotion Recognition’ Can’t Be Trusted.’” _The Verge_ ,\nJuly 25, 2019. <https://www.theverge.com/2019/7/25/8929793/emotion-\nrecognition-analysis-ai-machine-learning-facial-expression-review>.\n\n———. “Drones Taught to Spot Violent Behavior in Crowds Using AI.” _The Verge_\n, June 6, 2018. <https://www.theverge.com/2018/6/6/17433482/ai-automated-\nsurveillance-drones-spot-violent-behavior-crowds>.\n\nVollmann, William T. “Invisible and Insidious.” _Harper’s Magazine_ , March\n2015. <https://harpers.org/archive/2015/03/invisible-and-insidious/>.\n\nvon Neumann, John. _The Computer and the Brain._ New Haven: Yale University,\n1958.\n\nWade, Lizzie. “Tesla’s Electric Cars Aren’t as Green as You Might Think.”\n_Wired_ , March 31, 2016. <https://www.wired.com/2016/03/teslas-electric-cars-\nmight-not-green-think/>.\n\nWajcman, Judy. “How Silicon Valley Sets Time.” _New Media and Society_ 21, no.\n6 (2019): 1272–89. <https://doi.org/10.1177/1461444818820073>.\n\n———. _Pressed for Time: The Acceleration of Life in Digital Capitalism._\nChicago: University of Chicago Press, 2015.\n\nWakabayashi, Daisuke. “Google’s Shadow Work Force: Temps Who Outnumber Full-\nTime Employees.” _New York Times_ , May 28, 2019.\n<https://www.nytimes.com/2019/05/28/technology/google-temp-workers.html>.\n\nWald, Ellen. “Tesla Is a Battery Business, Not a Car Business.” _Forbes_ ,\nApril 15, 2017. <https://www.forbes.com/sites/ellenrwald/2017/04/15/tesla-is-\na-battery-business-not-a-car-business/>.\n\nWaldman, Peter, Lizette Chapman, and Jordan Robertson. “Palantir Knows\nEverything about You.” _Bloomberg_ , April 19, 2018.\n<https://www.bloomberg.com/features/2018-palantir-peter-thiel/>.\n\nWang, Yilun, and Michal Kosinski. “Deep Neural Networks _Are_ More Accurate\nThan Humans at Detecting Sexual Orientation from Facial Images.” _Journal of\nPersonality and Social Psychology_ 114, no. 2 (2018): 246–57.\n<https://doi.org/10.1037/pspa0000098>.\n\n“The War against Immigrants: Trump’s Tech Tools Powered by Palantir.”\n_Mijente_ , August 2019. <https://mijente.net/wp-\ncontent/uploads/2019/08/Mijente-The-War-Against-Immigrants_-Trumps-Tech-Tools-\nPowered-by-Palantir_.pdf>.\n\nWard, Bob. _Dr. Space: The Life of Wernher von Braun._ Annapolis, Md.: Naval\nInstitute Press, 2009.\n\nWeigel, Moira. “Palantir goes to the Frankfurt School.” _boundary 2_ (blog),\nJuly 10, 2020. <https://www.boundary2.org/2020/07/moira-weigel-palantir-goes-\nto-the-frankfurt-school/>.\n\nWeinberger, Sharon. “Airport Security: Intent to Deceive?” _Nature_ 465\n(2010): 412–15. <https://doi.org/10.1038/465412a>.\n\nWeizenbaum, Joseph. _Computer Power and Human Reason: From Judgment to\nCalculation._ San Francisco: W. H. Freeman, 1976.\n\n———. “On the Impact of the Computer on Society: How Does One Insult a\nMachine?” _Science_ , n.s., 176 (1972): 609–14.\n\nWelch, Chris. “Elon Musk: First Humans Who Journey to Mars Must ‘Be Prepared\nto Die.’” _The Verge_ , September 27, 2016.\n<https://www.theverge.com/2016/9/27/13080836/elon-musk-spacex-mars-mission-\ndeath-risk>.\n\nWerrett, Simon. “Potemkin and the Panopticon: Samuel Bentham and the\nArchitecture of Absolutism in Eighteenth Century Russia.” _Journal of Bentham\nStudies_ 2 (1999). <https://doi.org/10.14324/111.2045-757X.010>.\n\nWest, Cornel. “A Genealogy of Modern Racism.” In _Race Critical Theories: Text\nand Context_ , edited by Philomena Essed and David Theo Goldberg, 90–112.\nMalden, Mass.: Blackwell, 2002.\n\nWest, Sarah Myers. “Redistribution and Rekognition: A Feminist Critique of\nFairness.” _Catalyst: Feminism, Theory, and Technoscience_ (forthcoming,\n2020).\n\nWest, Sarah Myers, Meredith Whittaker, and Kate Crawford. “Discriminating\nSystems: Gender, Race, and Power in AI.” AI Now Institute, April 2019.\n<https://ainowinstitute.org/discriminatingsystems.pdf>.\n\nWhittaker, Meredith, et al. _AI Now Report 2018._ AI Now Institute, December\n2018. <https://ainowinstitute.org/AI_Now_2018_Report.pdf>.\n\n———. “Disability, Bias, and AI.” AI Now Institute, November 2019.\n<https://ainowinstitute.org/disabilitybiasai-2019.pdf>.\n\n“Why Asteroids.” Planetary Resources. <https://www.planetaryresources.com/why-\nasteroids/>.\n\nWilson, Mark. “Amazon and Target Race to Revolutionize the Cardboard Shipping\nBox.” _Fast Company_ , May 6, 2019.\n<https://www.fastcompany.com/90342864/rethinking-the-cardboard-box-has-never-\nbeen-more-important-just-ask-amazon-and-target>.\n\nWilson, Megan R. “Top Lobbying Victories of 2015.” The Hill, December 16,\n2015. <https://thehill.com/business-a-lobbying/business-a-\nlobbying/263354-lobbying-victories-of-2015>.\n\nWinston, Ali, and Ingrid Burrington. “A Pioneer in Predictive Policing Is\nStarting a Troubling New Project.” _The Verge_ (blog), April 26, 2018.\n<https://www.theverge.com/2018/4/26/17285058/predictive-policing-predpol-\npentagon-ai-racial-bias>.\n\nWinner, Langdon. _The Whale and the Reactor: A Search for Limits in an Age of\nHigh Technology._ Chicago: University of Chicago Press, 2001.\n\nWood, Bryan. “What Is Happening with the Uighurs in China?” PBS NewsHour.\n<https://www.pbs.org/newshour/features/uighurs/>.\n\nWood III, Pat, William L. Massey, and Nora Mead Brownell. “FERC Order\nDirecting Release of Information.” Federal Energy Regulatory Commission, March\n21, 2003. <https://www.caiso.com/Documents/FERCOrderDirectingRelease-\nInformationinDocketNos_PA02–2-000_etal__Manipulation-\nElectricandGasPrices_.pdf>.\n\nWu, Xiaolin, and Xi Zhang. “Automated Inference on Criminality Using Face\nImages.” _arXiv:1611.04135v1 [cs.CV]_ , November 13, 2016.\n<https://arxiv.org/abs/1611.04135v1>.\n\nYahoo! “Datasets.”\n[https://webscope.sandbox.yahoo.com/catalog.php?datatype=i&did=67&guccounter=1](https://webscope.sandbox.yahoo.com/catalog.php?datatype=i&did=67&guccounter=1).\n\nYang, Kaiyu, et al. “Towards Fairer Datasets: Filtering and Balancing the\nDistribution of the People Subtree in the ImageNet Hierarchy.” In _FAT* ’20:\nProceedings of the 2020 Conference on Fairness, Accountability, and\nTransparency_ , 547–558. New York: ACM Press, 2020.\n<https://dl.acm.org/doi/proceedings/10.1145/3351095>.\n\n“YFCC100M Core Dataset.” Multimedia Commons Initiative, December 4, 2015.\n<https://multimediacommons.wordpress.com/yfcc100m-core-dataset/>.\n\nYuan, Li. “How Cheap Labor Drives China’s A.I. Ambitions.” _New York Times_ ,\nNovember 25, 2018. <https://www.nytimes.com/2018/11/25/business/china-\nartificial-intelligence-labeling.html>.\n\nZhang, Zhimeng, et al. “Multi-Target, Multi-Camera Tracking by Hierarchical\nClustering: Recent Progress on DukeMTMC Project.” _arXiv:1712.09531 [cs.CV]_ ,\nDecember 27, 2017. <https://arxiv.org/abs/1712.09531>.\n\nZuboff, Shoshana. _The Age of Surveillance Capitalism: The Fight for a Human\nFuture at the New Frontier of Power._ New York: PublicAffairs, 2019.\n\n———. “Big Other: Surveillance Capitalism and the Prospects of an Information\nCivilization.” _Journal of Information Technology_ 30, no. 1 (2015): 75–89.\n<https://doi.org/10.1057/jit.2015.5>.\n\n\n## Index\n\n_Figures and notes are indicated by f and n following the page number._\n\nAbraham, David, [36](ch01.html#page_36)\n\n_The Elements of Power_ , [248](notes.html#page_248)n31\n\nabstraction, [18](intro.html#page_18), [217](concl.html#page_217)–18,\n[225](concl.html#page_225)\n\nAdvanced Research Projects Agency (ARPA), [159](ch05.html#page_159),\n[167](ch05.html#page_167), [184](ch06.html#page_184)\n\nAffectiva, [154](ch05.html#page_154)–55, [169](ch05.html#page_169)\n\naffect recognition, [17](intro.html#page_17), [151](ch05.html#page_151)–79,\n[221](concl.html#page_221)\n\ncritiques of Ekman’s theories, [171](ch05.html#page_171)–75\n\ndatasets for, [168](ch05.html#page_168)–71\n\nemotional performance and, [168](ch05.html#page_168)–71,\n[172](ch05.html#page_172)–73\n\ngender bias in, [177](ch05.html#page_177)\n\nin hiring decisions, [154](ch05.html#page_154)\n\nmicroexpressions and, [154](ch05.html#page_154), [165](ch05.html#page_165)\n\nmotivation and, [157](ch05.html#page_157)\n\nfrom physiognomy to photography, [161](ch05.html#page_161)–67\n\npolitics of, [176](ch05.html#page_176)–79\n\nprofitability of, [153](ch05.html#page_153)–56\n\nracial bias in, [177](ch05.html#page_177)\n\nAgre, Phil, [207](ch06.html#page_207)–8\n\nAgricola, Georgius, [26](ch01.html#page_26)\n\nAI Now, [243](ack.html#page_243), [244](ack.html#page_244),\n[248](notes.html#page_248)n50\n\nAlbemarle Corporation, [29](ch01.html#page_29)\n\nAleksander, Igor, [166](ch05.html#page_166)\n\nAlexa, [42](ch01.html#page_42)–43, [58](ch02.html#page_58)\n\nalgorithmic assessment: as collaboration, [58](ch02.html#page_58)\n\ncrime classification, [116](ch03.html#page_116)\n\nlabor and, [56](ch02.html#page_56)\n\nlearners vs. classifiers, [96](ch03.html#page_96)–97\n\nalgorithmic scheduling, [75](ch02.html#page_75)–76\n\nAlgorithmic Warfare Cross-Functional Team (Project Maven),\n[189](ch06.html#page_189)–92\n\nAlibaba Group, [44](ch01.html#page_44), [187](ch06.html#page_187)\n\nAllende, Salvador, [160](ch05.html#page_160)\n\nAlphaGo Zero, [212](concl.html#page_212)–13, [212](concl.html#page_212) _f_ ,\n[215](concl.html#page_215)\n\nALPR (automatic license-plate recognition) cameras,\n[199](ch06.html#page_199)–201\n\nAmazon: affect recognition used by, [155](ch05.html#page_155),\n[176](ch05.html#page_176)–77\n\nAlexa, [42](ch01.html#page_42)–43, [58](ch02.html#page_58)\n\nbias in algorithms of, [129](ch04.html#page_129)–30\n\ncarbon footprint of, [41](ch01.html#page_41), [42](ch01.html#page_42)–43\n\nlabor extraction by, [53](ch02.html#page_53)–56, [82](ch02.html#page_82)–85,\n[219](concl.html#page_219)\n\nMechanical Turk, [64](ch02.html#page_64), [67](ch02.html#page_67)–68,\n[108](ch03.html#page_108), [143](ch04.html#page_143),\n[219](concl.html#page_219)\n\nRing doorbell cameras, [183](ch06.html#page_183), [201](ch06.html#page_201)–2\n\nstate power and, [187](ch06.html#page_187)\n\nsupply chain, [37](ch01.html#page_37)\n\nsurveillance by, [201](ch06.html#page_201)–2\n\nworker protests against, [84](ch02.html#page_84)–85\n\nAmazon Web Services, [230](code.html#page_230)\n\nAmerican Psychiatric Association, [138](ch04.html#page_138)\n\nAnanny, Mike, [12](intro.html#page_12)\n\nAndrejevic, Mark, [13](intro.html#page_13), [268](notes.html#page_268)n25\n\nAnduril Industries, [263](notes.html#page_263)n38\n\nApple: affect recognition used by, [154](ch05.html#page_154)\n\nbias in algorithms of, [128](ch04.html#page_128)\n\ncarbon footprint of, [42](ch01.html#page_42)–44\n\nsupply chain, [34](ch01.html#page_34), [37](ch01.html#page_37)\n\nArgentina, affect recognition studies in, [160](ch05.html#page_160)\n\nAristotle, [161](ch05.html#page_161)\n\nARPA (Advanced Research Projects Agency), [159](ch05.html#page_159),\n[167](ch05.html#page_167), [184](ch06.html#page_184)\n\nartificial intelligence (overview): atlas conception of,\n[9](intro.html#page_9)–13\n\ndata extraction and, [16](intro.html#page_16)\n\ndata use by, [16](intro.html#page_16)–17\n\ndefining, [7](intro.html#page_7)–9\n\nas extractive industry, [15](intro.html#page_15)\n\nhuman labor and, [15](intro.html#page_15)–16\n\nas knowledge refinement, [7](intro.html#page_7)\n\nlogistical layer of, [46](ch01.html#page_46)–48\n\nas megamachine, [48](ch01.html#page_48)–50\n\nmineral extraction and, [15](intro.html#page_15)\n\nmythologies of intelligence in, [5](intro.html#page_5)\n\nas power structure, [18](intro.html#page_18)\n\nstate power and, [17](intro.html#page_17)–18\n\ntopographies of computation, [14](intro.html#page_14)–18\n\nArtificial Intelligence Principles (Google), [191](ch06.html#page_191)\n\nAustralia, lithium mining in, [33](ch01.html#page_33)\n\nBabbage, Charles, [57](ch02.html#page_57), [69](ch02.html#page_69)–71,\n[251](notes.html#page_251)n36\n\nBabich, Babette, [221](concl.html#page_221)\n\nBahl, Lalit, [100](ch03.html#page_100), [101](ch03.html#page_101)\n\nBaidu, [149](ch04.html#page_149)\n\nBailey, F. G., [214](concl.html#page_214)\n\nBaotou, Mongolia, rare earth mineral extraction in, [36](ch01.html#page_36)–37\n\nBarrett, Lisa Feldman, [171](ch05.html#page_171)–72,\n[174](ch05.html#page_174), [175](ch05.html#page_175)\n\nBartlett, Marian, [167](ch05.html#page_167)\n\nbehaviorism, [156](ch05.html#page_156)\n\nBelkhir, Lotfi, [42](ch01.html#page_42)\n\nBell, Derrick, [227](concl.html#page_227)\n\nBenjamin, Ruha, [13](intro.html#page_13), [128](ch04.html#page_128),\n[227](concl.html#page_227)\n\nBentham, Jeremy, [61](ch02.html#page_61)\n\nBentham, Samuel, [57](ch02.html#page_57), [61](ch02.html#page_61)–62\n\nBertillon, Alphonse, [91](ch03.html#page_91)\n\nBezos, Jeff, [68](ch02.html#page_68), [229](code.html#page_229)–30,\n[231](code.html#page_231)–32, [235](code.html#page_235)–36\n\nbias: cognitive, [135](ch04.html#page_135)\n\nin data classification algorithms, [128](ch04.html#page_128)–31,\n[133](ch04.html#page_133)–36\n\ndefinitions of, [133](ch04.html#page_133)–36\n\ngender, [128](ch04.html#page_128), [130](ch04.html#page_130),\n[138](ch04.html#page_138), [222](concl.html#page_222)\n\nlimits of debiasing systems, [131](ch04.html#page_131)–33\n\nracial, [144](ch04.html#page_144)–47, [177](ch05.html#page_177),\n[197](ch06.html#page_197)–98, [222](concl.html#page_222)\n\nstatistical, [135](ch04.html#page_135)\n\nBiddle, Wayne, [268](notes.html#page_268)n27\n\nbiometrics, [91](ch03.html#page_91), [119](ch03.html#page_119)\n\nBirdwhistell, Ray, [161](ch05.html#page_161)\n\nBlair, Nevada, [50](ch01.html#page_50)–51, [50](ch01.html#page_50) _f_\n\nBledsoe, Woody, [11](intro.html#page_11)\n\nBlue Origin, [230](code.html#page_230), [231](code.html#page_231)–32,\n[237](code.html#page_237)\n\nBluffdale, Utah, water supply for NSA data center in,\n[44](ch01.html#page_44)–45\n\nBolivia, lithium mining in, [32](ch01.html#page_32)–33\n\nBorene, Andrew, [204](ch06.html#page_204)\n\nBorges, Jorge Luis, [137](ch04.html#page_137)\n\nBowker, Geoffrey, [12](intro.html#page_12), [127](ch04.html#page_127)–28,\n[139](ch04.html#page_139), [145](ch04.html#page_145),\n[148](ch04.html#page_148), [254](notes.html#page_254)n52\n\nBratton, Benjamin, [12](intro.html#page_12), [186](ch06.html#page_186),\n[208](ch06.html#page_208)\n\nBraverman, Harry, [72](ch02.html#page_72)\n\nBrayne, Sarah, [197](ch06.html#page_197)–98, [264](notes.html#page_264)n57\n\nBrazil, affect recognition studies in, [160](ch05.html#page_160)\n\nBrechin, Gray, [26](ch01.html#page_26), [27](ch01.html#page_27)\n\nBroussard, Meredith, [13](intro.html#page_13), [245](notes.html#page_245)n10,\n[265](notes.html#page_265)n6\n\nBrown, Harold, [188](ch06.html#page_188)\n\nBrown, Peter, [100](ch03.html#page_100)\n\nBrown Corpus, [98](ch03.html#page_98), [136](ch04.html#page_136)\n\nBrowne, Simone, [13](intro.html#page_13), [133](ch04.html#page_133),\n[144](ch04.html#page_144)\n\nBünting, Heinrich: _The Bünting Clover Leaf Map_ , [14](intro.html#page_14)\n_f_\n\nBuolamwini, Joy, [131](ch04.html#page_131)\n\nBurgess, Jean, [254](notes.html#page_254)n56\n\nBush, George W., [203](ch06.html#page_203)\n\nBush, Vannevar, [98](ch03.html#page_98)–99, [121](ch03.html#page_121)\n\nByteDance, [187](ch06.html#page_187)\n\nCalGang database, [116](ch03.html#page_116)\n\nCambridge Analytica, [182](ch06.html#page_182)\n\nCampolo, Alex, [213](concl.html#page_213), [258](notes.html#page_258)n1\n\nCarey, James, [80](ch02.html#page_80)\n\ncargo containers, [46](ch01.html#page_46)–47\n\nCarter, Ash, [187](ch06.html#page_187)–88\n\nCatherine the Great (Russia), [62](ch02.html#page_62)\n\nCentral Intelligence Agency (CIA): affect recognition and,\n[170](ch05.html#page_170)\n\nPalantir and, [194](ch06.html#page_194), [195](ch06.html#page_195)\n\nstate power and, [203](ch06.html#page_203), [222](concl.html#page_222)\n\ncerium, [33](ch01.html#page_33)\n\nCetina, Karin Knorr, [17](intro.html#page_17), [131](ch04.html#page_131)\n\nChan, Hau, [117](ch03.html#page_117)\n\nChernan (company), [37](ch01.html#page_37)\n\nChile, affect recognition studies in, [160](ch05.html#page_160)\n\nChina: AI development in, [186](ch06.html#page_186)–89\n\ndata surveillance and collection in, [185](ch06.html#page_185)\n\nrare earth mineral extraction by, [36](ch01.html#page_36)–37\n\nstate power and, [222](concl.html#page_222)–23\n\nChinese Society of Rare Earths, [36](ch01.html#page_36)\n\nChun, Wendy, [12](intro.html#page_12), [187](ch06.html#page_187)\n\nChurch, Frank, [160](ch05.html#page_160)\n\nCIA. _See_ Central Intelligence Agency\n\nCitizen (app), [201](ch06.html#page_201)\n\nClever Hans Effect, [1](intro.html#page_1)–4, [6](intro.html#page_6)\n\nClickworker, [64](ch02.html#page_64)\n\nclimate justice, [226](concl.html#page_226)–27. _See also_ environmental\nimpacts\n\ncloud computing, carbon footprint of, [41](ch01.html#page_41)–42\n\nClub of Rome, [267](notes.html#page_267)n17; _The Limits to Growth_ ,\n[232](code.html#page_232)\n\nCMA CGM Group, [46](ch01.html#page_46)–47\n\ncobalt, [33](ch01.html#page_33)\n\ncognitive bias, [135](ch04.html#page_135)\n\nCognitive Science Laboratory, [136](ch04.html#page_136)\n\nCohen, Julie, [13](intro.html#page_13)\n\nCohn, Jeffrey, [167](ch05.html#page_167)\n\nCohn-Kanade (CK) dataset, [167](ch05.html#page_167),\n[168](ch05.html#page_168), [169](ch05.html#page_169) _f_\n\ncolonialism, [38](ch01.html#page_38), [126](ch04.html#page_126),\n[233](code.html#page_233)\n\nCommercial Space Launch Competitiveness Act of 2015 (U.S.),\n[233](code.html#page_233)–34\n\nCOMPAS criminal risk assessment software, [128](ch04.html#page_128)\n\ncompute maximalism, [43](ch01.html#page_43)\n\nconflict minerals, [34](ch01.html#page_34)–35\n\nCongo, lithium mining in, [32](ch01.html#page_32)–33\n\nconsent for data extraction, [109](ch03.html#page_109)–11\n\nConsumer Technology Association, [31](ch01.html#page_31)\n\nCoole, Diana, [249](notes.html#page_249)n67\n\nCortana, [58](ch02.html#page_58)\n\nCostanza-Chock, Sasha, [13](intro.html#page_13)\n\nCouldry, Nick, [254](notes.html#page_254)n58\n\nCounterDrug Technology Development Program Office, [104](ch03.html#page_104)\n\ncraniometry, [125](ch04.html#page_125)–26\n\ncredit scoring, [205](ch06.html#page_205)–7\n\ncrime classification algorithms, [116](ch03.html#page_116),\n[146](ch04.html#page_146)\n\ncriminal justice. _See_ police\n\ncrowdsourcing, [64](ch02.html#page_64), [67](ch02.html#page_67)–68\n\nThe Dalles, Oregon, [215](concl.html#page_215)–16\n\nDARPA (Defense Advanced Research Projects Agency), [184](ch06.html#page_184)\n\nDarwin, Charles: _The Expression of the Emotions in Man and Animals_ ,\n[158](ch05.html#page_158), [162](ch05.html#page_162)\n\nDaston, Lorraine, [10](intro.html#page_10), [12](intro.html#page_12)\n\ndata architectures, [40](ch01.html#page_40)–41\n\ndata centers: carbon footprint of, [41](ch01.html#page_41)–42\n\nelectricity demands of, [43](ch01.html#page_43)–44\n\nwater supply requirements for, [44](ch01.html#page_44)–45\n\ndata classification, [16](intro.html#page_16)–17, [123](ch04.html#page_123)–49\n\nbias in, [128](ch04.html#page_128)–31, [133](ch04.html#page_133)–36,\n[144](ch04.html#page_144)–47\n\ngender bias in, [128](ch04.html#page_128), [130](ch04.html#page_130),\n[138](ch04.html#page_138), [144](ch04.html#page_144)–47\n\ngeneralization errors, [134](ch04.html#page_134)–35\n\nImageNet and, [136](ch04.html#page_136)–44, [149](ch04.html#page_149)\n\nlimits of debiasing systems, [131](ch04.html#page_131)–33\n\nlimits of measurement for, [147](ch04.html#page_147)–49\n\npower to define “person,” 139–44\n\nracial bias in, [144](ch04.html#page_144)–47\n\nsystems of circular logic and, [128](ch04.html#page_128)–31\n\ntraining sets for, [136](ch04.html#page_136)–39\n\nunderfitting vs. overfitting data, [134](ch04.html#page_134)\n\nvariance in, [134](ch04.html#page_134)\n\ndata colonialism, [254](notes.html#page_254)n58\n\ndata extraction, [16](intro.html#page_16), [89](ch03.html#page_89)–121,\n[220](concl.html#page_220)\n\nalgorithmic assessment and, [96](ch03.html#page_96)–97,\n[116](ch03.html#page_116)\n\nbiometrics, [91](ch03.html#page_91), [119](ch03.html#page_119)\n\nconsent for, [109](ch03.html#page_109)–11\n\ndata mining, [31](ch01.html#page_31), [113](ch03.html#page_113)\n\nfacial recognition algorithms and, [92](ch03.html#page_92),\n[104](ch03.html#page_104)–5\n\nhistory of demand for data, [98](ch03.html#page_98)–103\n\nfrom internet, [106](ch03.html#page_106)–7, [119](ch03.html#page_119)–21\n\nmug shot databases, [89](ch03.html#page_89)–92, [93](ch03.html#page_93)\n\nmyths and metaphors of data, [111](ch03.html#page_111)–15\n\nprivacy issues, [119](ch03.html#page_119)–21\n\nspeech recognition and, [100](ch03.html#page_100)–101\n\nstate power and, [207](ch06.html#page_207)–9\n\ntraining machines to see, [96](ch03.html#page_96)–98\n\nof voice data, [119](ch03.html#page_119)\n\nDaugherty, James Monroe, [236](code.html#page_236)\n\ndeductive inferences, [97](ch03.html#page_97)\n\nDeepMind, [43](ch01.html#page_43), [120](ch03.html#page_120),\n[212](concl.html#page_212)\n\nDefense Advanced Research Projects Agency (DARPA), [184](ch06.html#page_184)\n\nDefense Department: affect recognition and, [168](ch05.html#page_168),\n[174](ch05.html#page_174)–75\n\ndata extraction and, [104](ch03.html#page_104)\n\nPalantir and, [194](ch06.html#page_194), [195](ch06.html#page_195)\n\nProject Maven and, [189](ch06.html#page_189)–92\n\nstate power and, [188](ch06.html#page_188), [189](ch06.html#page_189)\n\nDell, [35](ch01.html#page_35)\n\ndemocratic participation, [223](concl.html#page_223)–24\n\nDemocratic Republic of Congo, mining industry in, [34](ch01.html#page_34)\n\nDesai, Bhairavi, [86](ch02.html#page_86)\n\n_Diagnostic and Statistical Manual_ (American Psychiatric Association),\n[138](ch04.html#page_138)\n\nDiamandis, Peter, [231](code.html#page_231)\n\nDick, Philip K., [267](notes.html#page_267)n20\n\nDidi-Huberman, Georges, [10](intro.html#page_10)\n\ndigital epidermalization, [133](ch04.html#page_133)\n\ndisability, [127](ch04.html#page_127), [146](ch04.html#page_146),\n[261](notes.html#page_261)n89\n\ndiscrimination. _See_ bias\n\nDiversity in Faces (DiF) dataset, [131](ch04.html#page_131)–33,\n[149](ch04.html#page_149)\n\ndivision of labor, [60](ch02.html#page_60), [69](ch02.html#page_69),\n[72](ch02.html#page_72), [73](ch02.html#page_73)–74\n\nDodd-Frank Act of 2010 (U.S.), [34](ch01.html#page_34)\n\nDomino’s Pizza, [76](ch02.html#page_76)–77\n\nDouglass, Frederick, [149](ch04.html#page_149)\n\nDreyfus, Hubert: _What Computers Can’t Do_ , [6](intro.html#page_6)\n\ndrone strikes, [203](ch06.html#page_203)–4\n\nDuchenne de Boulogne, Guillaume-Benjamin-Amand, [162](ch05.html#page_162)–65,\n[164](ch05.html#page_164) _f_\n\n_Mécanisme de la physionomie humaine ou Analyse électro-physiologique de\nl’expression des passions applicable à la pratique des arts plastiques_ ,\n[158](ch05.html#page_158), [162](ch05.html#page_162)\n\nDuke University, [109](ch03.html#page_109)–10\n\ndysprosium, [33](ch01.html#page_33), [36](ch01.html#page_36)\n\ndystopian vs. utopian perspectives, [214](concl.html#page_214)–15\n\nEco, Umberto, [148](ch04.html#page_148)\n\nEdwards, Paul: _The Closed World_ , [184](ch06.html#page_184)\n\nefficiency of labor, [53](ch02.html#page_53)–54, [56](ch02.html#page_56),\n[70](ch02.html#page_70)–71, [77](ch02.html#page_77),\n[213](concl.html#page_213)\n\nEisenhower, Dwight, [262](notes.html#page_262)n23\n\nEkman, Paul, [17](intro.html#page_17), [151](ch05.html#page_151)–53,\n[156](ch05.html#page_156)–61, [165](ch05.html#page_165)–67,\n[170](ch05.html#page_170)\n\nelectric vehicles, environmental impact of, [30](ch01.html#page_30)\n\nELIZA, [5](intro.html#page_5)\n\nElmeligi, Ahmed, [42](ch01.html#page_42)\n\nEmotient, [154](ch05.html#page_154), [167](ch05.html#page_167)\n\nenchanted determinism, [213](concl.html#page_213)–14\n\nEnron Corporation, [102](ch03.html#page_102)\n\nenvironmental impacts: cargo containers and, [46](ch01.html#page_46)–47\n\ndata centers and, [41](ch01.html#page_41)–45\n\nelectric vehicles and, [30](ch01.html#page_30)\n\ne-waste dumping grounds, [32](ch01.html#page_32)\n\ngold mining, [25](ch01.html#page_25)–27, [34](ch01.html#page_34)\n\nlatex extraction industry, [38](ch01.html#page_38)–39\n\nlithium mining, [24](ch01.html#page_24)–25, [28](ch01.html#page_28)–36,\n[51](ch01.html#page_51)\n\nrare earth minerals and, [33](ch01.html#page_33)\n\nsilver mining, [26](ch01.html#page_26)\n\nsubmarine cable construction and, [38](ch01.html#page_38)–39,\n[46](ch01.html#page_46)\n\nwater supply, [44](ch01.html#page_44)–45\n\nepistemic machinery, [17](intro.html#page_17), [131](ch04.html#page_131)\n\nEpstein, Jeffrey, [245](notes.html#page_245)n10\n\nerbium, [33](ch01.html#page_33)\n\nethical frameworks for AI, [223](concl.html#page_223)–25\n\nEubanks, Virginia, [13](intro.html#page_13); _Automating Inequality_ ,\n[206](ch06.html#page_206)\n\nEuclid (spatial analytics startup), [76](ch02.html#page_76)\n\neuropium, [33](ch01.html#page_33)\n\ne-waste dumping grounds, [32](ch01.html#page_32)\n\nexpert systems approach, [99](ch03.html#page_99)–100\n\nexploitation of labor, [32](ch01.html#page_32), [57](ch02.html#page_57)–58,\n[63](ch02.html#page_63), [66](ch02.html#page_66)–73, [82](ch02.html#page_82),\n[86](ch02.html#page_86), [219](concl.html#page_219)\n\nExtended Cohn-Kanade (CK+) Dataset, [169](ch05.html#page_169)\n\nextractive industries: AI as, [15](intro.html#page_15),\n[217](concl.html#page_217)\n\ndata collection as, [89](ch03.html#page_89)–121\n\ngold mining, [25](ch01.html#page_25)–27, [34](ch01.html#page_34)\n\nlabor extraction via AI, [63](ch02.html#page_63)–69\n\nlatex extraction industry, [38](ch01.html#page_38)–39\n\nlithium mining, [24](ch01.html#page_24)–25, [28](ch01.html#page_28)–36,\n[51](ch01.html#page_51)\n\nsilver mining, [26](ch01.html#page_26). _See also_ data extraction; mineral\nextraction\n\nFace API, [155](ch05.html#page_155)\n\nFacebook: bias in algorithms of, [128](ch04.html#page_128)\n\ndata classification algorithms, [149](ch04.html#page_149)\n\npersonal assistant “M,” 65\n\nstate power and, [187](ch06.html#page_187)\n\nFace Recognition Technology (FERET), [104](ch03.html#page_104),\n[105](ch03.html#page_105), [168](ch05.html#page_168)\n\nFacial Action Coding System (FACS), [165](ch05.html#page_165)–66,\n[166](ch05.html#page_166) _f_ , [167](ch05.html#page_167),\n[169](ch05.html#page_169), [170](ch05.html#page_170)\n\nFacial Action Scoring Technique (FAST), [165](ch05.html#page_165)\n\nfacial recognition algorithms, [92](ch03.html#page_92),\n[104](ch03.html#page_104)–5, [145](ch04.html#page_145)–46,\n[201](ch06.html#page_201). _See also_ affect recognition\n\nFacial Recognition Verification Testing, [93](ch03.html#page_93)\n\nFALCON program, [195](ch06.html#page_195)–96, [196](ch06.html#page_196) _f_\n\n“fauxtomation,” 66–67\n\nFederal Bureau of Investigation (FBI): affect recognition and,\n[170](ch05.html#page_170)\n\ndata extraction and, [90](ch03.html#page_90)–91, [119](ch03.html#page_119)\n\nfingerprint identification and, [252](notes.html#page_252)n3\n\nPalantir and, [194](ch06.html#page_194)\n\nFederal Energy Regulatory Commission, [102](ch03.html#page_102)\n\nFederici, Silvia, [58](ch02.html#page_58)\n\nfeedback loops, [198](ch06.html#page_198)\n\nFERET (Face Recognition Technology), [104](ch03.html#page_104),\n[105](ch03.html#page_105), [168](ch05.html#page_168)\n\nFerguson, Andrew, [196](ch06.html#page_196)–97\n\nFernández-Dols, José-Miguel, [171](ch05.html#page_171)\n\nFigure Eight, [64](ch02.html#page_64)\n\n_Financial Times:_ on affect recognition systems, [154](ch05.html#page_154)\n\non facial recognition datasets, [253](notes.html#page_253)n46\n\nfingerprint recognition, [91](ch03.html#page_91), [252](notes.html#page_252)n3\n\nFive Eyes network, [181](ch06.html#page_181), [261](notes.html#page_261)n2\n\nFlickr, [119](ch03.html#page_119), [131](ch04.html#page_131)–32\n\nFord, Henry, [57](ch02.html#page_57), [73](ch02.html#page_73),\n[76](ch02.html#page_76), [81](ch02.html#page_81)\n\nFore people of Okapa (Papua New Guinea), [151](ch05.html#page_151)–52,\n[161](ch05.html#page_161)\n\nFoucault, Michel, [251](notes.html#page_251)n66; _Discipline and Punish_ ,\n[61](ch02.html#page_61)\n\nFourcade, Marion, [112](ch03.html#page_112), [114](ch03.html#page_114),\n[205](ch06.html#page_205)\n\nFOXACID program, [183](ch06.html#page_183), [201](ch06.html#page_201),\n[262](notes.html#page_262)n5\n\nFoxconn, [37](ch01.html#page_37), [74](ch02.html#page_74),\n[219](concl.html#page_219)\n\nFranklin, Ursula, [10](intro.html#page_10), [20](intro.html#page_20)\n\nfraud detection, [104](ch03.html#page_104)\n\nFriesen, Wallace, [165](ch05.html#page_165)–66, [166](ch05.html#page_166) _f_\n\nFrost, Samantha, [249](notes.html#page_249)n67\n\ngadolinium, [33](ch01.html#page_33)\n\nGajdusek, D. Carleton, [151](ch05.html#page_151)–52\n\nGalison, Peter, [12](intro.html#page_12), [80](ch02.html#page_80)\n\nGall, Franz Joseph, [162](ch05.html#page_162)\n\nGalton, Francis, [40](ch01.html#page_40), [91](ch03.html#page_91)–92\n\ngang-related crime datasets, [116](ch03.html#page_116)–17\n\nGAO (Government Accountability Office), [170](ch05.html#page_170)\n\nGarwin, Dick, [101](ch03.html#page_101)\n\nGebru, Timnit, [131](ch04.html#page_131)\n\ngender: affect recognition and, [177](ch05.html#page_177)\n\nbias in data classification algorithms, [128](ch04.html#page_128),\n[130](ch04.html#page_130), [138](ch04.html#page_138),\n[222](concl.html#page_222)\n\ndata classification constructs for, [144](ch04.html#page_144)–47\n\nGendron, Maria, [171](ch05.html#page_171)\n\ngeneralization errors, [134](ch04.html#page_134)–35\n\nGeneral Motors, [77](ch02.html#page_77)\n\nGeorge, Rose, [47](ch01.html#page_47)\n\ngermanium, [33](ch01.html#page_33)\n\nGibson, Jennifer, [203](ch06.html#page_203)\n\nGillespie, Tarleton, [13](intro.html#page_13), [64](ch02.html#page_64)\n\nGitelman, Lisa, [113](ch03.html#page_113)\n\nGithub, [78](ch02.html#page_78)\n\nGoldman Sachs, [154](ch05.html#page_154)\n\ngold mining, [25](ch01.html#page_25)–27, [34](ch01.html#page_34)\n\nGoogle: Artificial Intelligence Principles, [191](ch06.html#page_191)\n\ncarbon footprint of, [43](ch01.html#page_43)–44\n\ndata centers, [215](concl.html#page_215)–16\n\ndata classification algorithms, [149](ch04.html#page_149)\n\nDeepMind, [43](ch01.html#page_43), [120](ch03.html#page_120),\n[212](concl.html#page_212)\n\nimage search, [119](ch03.html#page_119)\n\nProject Maven and, [190](ch06.html#page_190)–91\n\nreCAPTCHA, [68](ch02.html#page_68)–69\n\nSpanner, [78](ch02.html#page_78)–79\n\nGoogle Street View, [220](concl.html#page_220)\n\nGou, Terry, [74](ch02.html#page_74)\n\nGould, Stephen Jay: _The Mismeasure of Man_ , [125](ch04.html#page_125)–26\n\nGovernment Accountability Office (GAO), [170](ch05.html#page_170)\n\nGovernment Communication Headquarters (UK), [181](ch06.html#page_181),\n[185](ch06.html#page_185)\n\nGraeber, David: _The Utopia of Rules_ , [250](notes.html#page_250)n4\n\nGray, Mary, [63](ch02.html#page_63)\n\nGreene, Dan, [252](notes.html#page_252)n75\n\nGreenpeace, [44](ch01.html#page_44)\n\nGregg, Melissa, [58](ch02.html#page_58)\n\ngutta-percha, [38](ch01.html#page_38)–39\n\nHacking, Ian, [12](intro.html#page_12), [139](ch04.html#page_139),\n[147](ch04.html#page_147)\n\nHall, Stuart, [12](intro.html#page_12)\n\nHalliburton, [193](ch06.html#page_193)\n\nHans (horse), [1](intro.html#page_1)–4, [3](intro.html#page_3) _f_ ,\n[6](intro.html#page_6)\n\nHaraway, Donna, [145](ch04.html#page_145), [226](concl.html#page_226)\n\nHardt, Michael, [217](concl.html#page_217)\n\nHarvey, Adam, [110](ch03.html#page_110)\n\nHaskins, Caroline, [202](ch06.html#page_202)\n\nHassabis, Demis, [212](concl.html#page_212)–13\n\nHayden, Michael, [203](ch06.html#page_203)\n\nHealth and Human Services Department (U.S.), [195](ch06.html#page_195)\n\nHealy, Kieran, [112](ch03.html#page_112), [114](ch03.html#page_114),\n[205](ch06.html#page_205)\n\nHicks, Mar, [13](intro.html#page_13)\n\nHines, Alicia Headlam, [20](intro.html#page_20)\n\nHird, Myra, [36](ch01.html#page_36)\n\nHireVue, [154](ch05.html#page_154)\n\nHodal, Kate, [37](ch01.html#page_37)\n\nHoffman, Anna Lauren, [113](ch03.html#page_113), [117](ch03.html#page_117)\n\nholmium, [33](ch01.html#page_33)\n\nHomeland Security Department (U.S.), [195](ch06.html#page_195)\n\nHopper, Grace, [6](intro.html#page_6)\n\nHough, Lee, [159](ch05.html#page_159)–60\n\nHu, Tung-Hui, [13](intro.html#page_13), [187](ch06.html#page_187),\n[202](ch06.html#page_202); _A Prehistory of the Cloud_ ,\n[41](ch01.html#page_41)–42\n\nHuawei, [186](ch06.html#page_186), [187](ch06.html#page_187)\n\nHuet, Ellen, [65](ch02.html#page_65)\n\nHui, Yuk, [13](intro.html#page_13)\n\nHuman (company), [154](ch05.html#page_154)\n\nhuman labor. _See_ labor\n\nHwang, Tim, [254](notes.html#page_254)n56\n\nIBM: affect recognition used by, [155](ch05.html#page_155),\n[177](ch05.html#page_177)\n\nContinuous Speech Recognition, [100](ch03.html#page_100)–102\n\nDiversity in Faces dataset, [131](ch04.html#page_131)–33\n\nstate power targeting and, [204](ch06.html#page_204)–5\n\nImageNet: data classification scheme, [136](ch04.html#page_136)–39,\n[149](ch04.html#page_149)\n\ngoals of, [11](intro.html#page_11); “person” categories in,\n[139](ch04.html#page_139)–44\n\ntraining datasets for, [97](ch03.html#page_97)–98,\n[107](ch03.html#page_107)–9, [220](concl.html#page_220)\n\nImageNet Roulette, [257](notes.html#page_257)n52\n\nImmigration and Customs Enforcement (ICE), [194](ch06.html#page_194),\n[195](ch06.html#page_195), [200](ch06.html#page_200)\n\nIndonesia: lithium mining in, [33](ch01.html#page_33)\n\nmineral extraction in, [37](ch01.html#page_37)–38\n\ninductive inferences, [97](ch03.html#page_97), [134](ch04.html#page_134)\n\nIndustrial Revolution, [59](ch02.html#page_59)\n\nIn-Q-Tel (venture capital firm), [194](ch06.html#page_194)\n\ninstitutional review boards (IRBs), [115](ch03.html#page_115)–16\n\nIntel, [34](ch01.html#page_34), [154](ch05.html#page_154)\n\nIntelligence Advanced Research Projects Activity (IARPA),\n[92](ch03.html#page_92)\n\nIntelligence Community Comprehensive National Cybersecurity Initiative Data\nCenter (Utah), [44](ch01.html#page_44)–45\n\nInternational Labour Organization (ILO), [64](ch02.html#page_64)\n\nIrani, Lilly, [63](ch02.html#page_63)\n\nIsrael, data surveillance and collection in, [185](ch06.html#page_185)\n\nJapan, affect recognition studies in, [160](ch05.html#page_160)\n\nJelinek, Fred, [100](ch03.html#page_100)\n\nJoint Enterprise Defense Infrastructure (JEDI), [189](ch06.html#page_189),\n[191](ch06.html#page_191), [263](notes.html#page_263)n36\n\njustice: AI and, [19](intro.html#page_19)–20\n\nclimate justice, [226](concl.html#page_226)–27\n\nconnected movements for, [223](concl.html#page_223)–27\n\ndata classification and, [147](ch04.html#page_147)–48\n\ndata extraction and, [94](ch03.html#page_94), [117](ch03.html#page_117)\n\ngoal of, [50](ch01.html#page_50), [223](concl.html#page_223)–27\n\nKahneman, Daniel, [135](ch04.html#page_135)\n\nKanade, Takeo, [167](ch05.html#page_167)\n\nKappas, Arvid, [173](ch05.html#page_173)–74\n\nKarolinska Directed Emotional Faces database, [168](ch05.html#page_168)\n\nKarp, Alex, [193](ch06.html#page_193)\n\nKemeny, John, [78](ch02.html#page_78)\n\nKeyes, Os, [138](ch04.html#page_138)\n\nknowledge workers, [58](ch02.html#page_58)\n\nKroc, Ray, [75](ch02.html#page_75)\n\nKurdi, Alan, [204](ch06.html#page_204)\n\nKurtz, Thomas, [78](ch02.html#page_78)\n\nLabban, Mazen, [49](ch01.html#page_49)\n\nlabor, [15](intro.html#page_15)–16, [53](ch02.html#page_53)–87\n\nalgorithmic assessment and, [56](ch02.html#page_56)\n\nalgorithmic scheduling, [75](ch02.html#page_75)–76\n\ncrowdsourcing, [64](ch02.html#page_64), [67](ch02.html#page_67)–68\n\ndivision of,\n\nlabor (continued)60, [69](ch02.html#page_69), [72](ch02.html#page_72),\n[73](ch02.html#page_73)–74\n\nefficiency of, [53](ch02.html#page_53)–54, [56](ch02.html#page_56),\n[70](ch02.html#page_70)–71, [77](ch02.html#page_77),\n[213](concl.html#page_213)\n\nexploitation of, [32](ch01.html#page_32), [57](ch02.html#page_57)–58,\n[63](ch02.html#page_63), [66](ch02.html#page_66)–73, [82](ch02.html#page_82),\n[86](ch02.html#page_86), [219](concl.html#page_219)\n\nmeat-packing industry, [72](ch02.html#page_72)–75\n\nmechanization and, [59](ch02.html#page_59)–60\n\nPotemkin AI and Mechanical Turks, [63](ch02.html#page_63)–69\n\nprehistories of workplace AI, [57](ch02.html#page_57)–63\n\nsurveillance and, [56](ch02.html#page_56), [60](ch02.html#page_60)–61,\n[70](ch02.html#page_70), [75](ch02.html#page_75)–77\n\ntime coordination and, [56](ch02.html#page_56), [74](ch02.html#page_74)–82\n\nworkplace automation, [69](ch02.html#page_69)–71\n\nLakoff, George, [139](ch04.html#page_139)\n\nlanthanum, [33](ch01.html#page_33)\n\nLaPlace, Jules, [110](ch03.html#page_110)\n\nlatex extraction industry, [38](ch01.html#page_38)–39\n\nLavater, Johann Kaspar: _Essays on Physiognomy_ , [162](ch05.html#page_162)\n\nlaw enforcement. _See_ police\n\nLawrence Livermore National Laboratory, [24](ch01.html#page_24)\n\nLehrer, Tom, [117](ch03.html#page_117)\n\nLem, Stanislaw, [101](ch03.html#page_101)\n\nLevy, Karen, [254](notes.html#page_254)n56\n\nLeys, Ruth, [258](notes.html#page_258)n25; _The Ascent of Affect_ ,\n[172](ch05.html#page_172)\n\nLG, [37](ch01.html#page_37)\n\nLi, Fei-Fei, [11](intro.html#page_11), [107](ch03.html#page_107)–8,\n[190](ch06.html#page_190)\n\nLi, Xiaochang, [100](ch03.html#page_100)–101\n\nLicklider, J. C. R., [6](intro.html#page_6)\n\n_Lie to Me_ (television show), [170](ch05.html#page_170)\n\nLievrouw, Leah, [249](notes.html#page_249)n67\n\nLight, Jennifer, [99](ch03.html#page_99)\n\nlithium mining, [15](intro.html#page_15), [24](ch01.html#page_24)–25,\n[28](ch01.html#page_28)–36, [51](ch01.html#page_51)\n\nLockheed Martin, [23](ch01.html#page_23), [193](ch06.html#page_193)\n\nlogistical layer of AI, [46](ch01.html#page_46)–48\n\nLombroso, Cesare, [162](ch05.html#page_162)\n\nlooping effect, [139](ch04.html#page_139)\n\nLos Angeles Police Department, [116](ch03.html#page_116),\n[197](ch06.html#page_197)–98\n\nLuckey, Palmer, [263](notes.html#page_263)n38\n\nlutetium, [33](ch01.html#page_33)\n\nMacKenzie, Donald, [12](intro.html#page_12)\n\nMaersk, [46](ch01.html#page_46)–47\n\nMalaysia, latex extraction industry in, [38](ch01.html#page_38)–39\n\nManhattan Project, [48](ch01.html#page_48)\n\nManufacturing Automation Protocol (MAP), [77](ch02.html#page_77)\n\n_mappa mundi_ , [13](intro.html#page_13), [14](intro.html#page_14) _f_\n\nMarshall Space Flight Center, [235](code.html#page_235)\n\nMarx, Karl, [60](ch02.html#page_60), [74](ch02.html#page_74),\n[250](notes.html#page_250)n6\n\nmateriality, [18](intro.html#page_18), [217](concl.html#page_217),\n[249](notes.html#page_249)n67\n\nMattern, Shannon, [12](intro.html#page_12), [224](concl.html#page_224)\n\nMaughan, Tim, [36](ch01.html#page_36)\n\nMayhew, Clare, [76](ch02.html#page_76)\n\nMbembé, Achille, [12](intro.html#page_12), [185](ch06.html#page_185),\n[225](concl.html#page_225)–26, [227](concl.html#page_227)\n\nMcCarthy, John, [6](intro.html#page_6), [78](ch02.html#page_78)\n\nMcDonald’s, [74](ch02.html#page_74)–76\n\nMcLuhan, Marshall, [31](ch01.html#page_31)\n\nMcNamara, Robert, [262](notes.html#page_262)n23\n\nMead, Margaret, [171](ch05.html#page_171), [175](ch05.html#page_175)\n\nMeat Inspection Act of 1906 (U.S.), [72](ch02.html#page_72)\n\nmeat-packing industry, [72](ch02.html#page_72)–75\n\nMechanical Turk, [67](ch02.html#page_67)–68\n\nMedicare fraud detection, [195](ch06.html#page_195)\n\nMediterranean Shipping Company, [46](ch01.html#page_46)–47\n\nMejías, Ulises, [254](notes.html#page_254)n58\n\nMercer, Robert, [100](ch03.html#page_100), [101](ch03.html#page_101),\n[102](ch03.html#page_102), [112](ch03.html#page_112)\n\nmetadata, [185](ch06.html#page_185)\n\nMezzadra, Sandro, [247](notes.html#page_247)n21\n\nMichalski, Ryszard, [252](notes.html#page_252)n16\n\nMichie, Donald, [7](intro.html#page_7)\n\nMichigan Integrated Data Automated System (MiDAS), [206](ch06.html#page_206)\n\nmicroexpressions, [154](ch05.html#page_154), [165](ch05.html#page_165)\n\nmicrophysics of power, [251](notes.html#page_251)n66\n\nMicrosoft: affect recognition used by, [155](ch05.html#page_155)\n\ncarbon footprint of, [43](ch01.html#page_43)–44\n\ncarbon footprint of data centers, [41](ch01.html#page_41)\n\nfacial recognition and, [253](notes.html#page_253)n46\n\nMS-Celeb dataset, [110](ch03.html#page_110)\n\nProject Maven and, [191](ch06.html#page_191), [263](notes.html#page_263)n36\n\nMicroworkers, [64](ch02.html#page_64)\n\nmineral extraction: AI’s need for, [32](ch01.html#page_32)–36\n\nconflict linked to, [34](ch01.html#page_34)–35\n\nmyth of clean tech, [41](ch01.html#page_41)–46\n\nsupply chains, [34](ch01.html#page_34)–35\n\ntrue costs of, [26](ch01.html#page_26), [36](ch01.html#page_36)–41. _See also_\nextractive industries\n\nMinsky, Marvin, [5](intro.html#page_5), [6](intro.html#page_6),\n[245](notes.html#page_245)n10\n\nMirzoeff, Nicholas: _The Right to Look_ , [62](ch02.html#page_62)\n\nMIT: affect recognition research, [154](ch05.html#page_154),\n[169](ch05.html#page_169)\n\n“Management and the Computer of the Future” lecture series,\n[5](intro.html#page_5)–6\n\nMedia Lab, [169](ch05.html#page_169)\n\nMoffett Federal Airfield, [23](ch01.html#page_23)\n\nMongolia: lithium mining in, [33](ch01.html#page_33)\n\nrare earth mineral extraction in, [36](ch01.html#page_36)–37\n\nMorozov, Evgeny, [110](ch03.html#page_110)\n\nMorton, Samuel, [123](ch04.html#page_123)–26, [127](ch04.html#page_127)\n\nMS-Celeb dataset, [110](ch03.html#page_110), [220](concl.html#page_220)\n\nmug shot databases, [89](ch03.html#page_89)–92, [93](ch03.html#page_93)\n\nMultiple Encounter Dataset, [89](ch03.html#page_89)–92\n\nMumford, Lewis, [33](ch01.html#page_33), [48](ch01.html#page_48)\n\nMuse, Abdi, [84](ch02.html#page_84)–85\n\nMusk, Elon, [29](ch01.html#page_29), [231](code.html#page_231),\n[233](code.html#page_233)\n\nmyth of clean tech, [41](ch01.html#page_41)–46\n\nNational Health Service (UK), [120](ch03.html#page_120)\n\nNational Institute of Standards and Technology (NIST),\n[89](ch03.html#page_89)–92, [93](ch03.html#page_93),\n[220](concl.html#page_220), [252](notes.html#page_252)n3\n\nNational Science Foundation, [109](ch03.html#page_109)\n\nNational Security Agency (NSA): data center, [44](ch01.html#page_44)–45\n\ndrone strike targeting and, [203](ch06.html#page_203)\n\nPalantir and, [194](ch06.html#page_194)\n\nProject Maven and, [189](ch06.html#page_189)–90\n\nstate power and, [181](ch06.html#page_181)–83, [183](ch06.html#page_183) _f_ ,\n[185](ch06.html#page_185), [188](ch06.html#page_188),\n[209](ch06.html#page_209), [222](concl.html#page_222)\n\nsurveillance by, [194](ch06.html#page_194)–95\n\nnatural language processing (NLP) model, [42](ch01.html#page_42)–43\n\nnecropolitics, [185](ch06.html#page_185)\n\nNegri, Antonio, [217](concl.html#page_217)\n\nNeighbors (app), [201](ch06.html#page_201), [202](ch06.html#page_202)\n\nNeilson, Brett, [247](notes.html#page_247)n21\n\nNelson, Alondra, [12](intro.html#page_12), [20](intro.html#page_20)\n\nneodymium, [33](ch01.html#page_33)\n\nNetwork Time Protocol (NTP), [77](ch02.html#page_77)–78\n\nNeville-Neil, George V., [265](notes.html#page_265)n6\n\nNewell, Allen, [6](intro.html#page_6)\n\nNewton, Isaac, [79](ch02.html#page_79)\n\nNew York City Taxi and Limousine Commission, [110](ch03.html#page_110)–11\n\nNew York Taxi Workers Alliance, [86](ch02.html#page_86)\n\n_New York Times:_ on Clever Hans, [1](intro.html#page_1)\n\non Ekman, [170](ch05.html#page_170)\n\nNextdoor (app), [201](ch06.html#page_201)\n\nNext Generation Identification (NGI), [92](ch03.html#page_92)\n\nNietzsche, Friedrich, [222](concl.html#page_222)\n\nNilsson, Nils, [111](ch03.html#page_111)–12\n\nNIST. _See_ National Institute of Standards and Technology\n\nNLP (natural language processing) model, [42](ch01.html#page_42)–43\n\nNoble, Safiya Umoja, [13](intro.html#page_13), [128](ch04.html#page_128)\n\nNorvig, Peter, [7](intro.html#page_7), [11](intro.html#page_11)\n\nNSA. _See_ National Security Agency\n\nNTP (Network Time Protocol), [77](ch02.html#page_77)–78\n\nobserver-expectancy effect, [1](intro.html#page_1)–4\n\nobsolescence cycle, [31](ch01.html#page_31)\n\nOffice of Naval Research (U.S.), [136](ch04.html#page_136),\n[184](ch06.html#page_184)\n\nOmar, Ilhan Abdullahi, [176](ch05.html#page_176)–77\n\nO’Neill, Gerard K.: _The High Frontier: Human Colonies in Space_ ,\n[231](code.html#page_231)–32, [233](code.html#page_233)\n\nOpenAI, [43](ch01.html#page_43)\n\nOperation Desert Storm (1991), [262](notes.html#page_262)n23\n\nOuter Space Treaty (1967), [233](code.html#page_233)\n\nPage, Larry, [231](code.html#page_231)\n\nPaglen, Trevor, [141](ch04.html#page_141), [257](notes.html#page_257)n52\n\nPalantir, [193](ch06.html#page_193)–99, [196](ch06.html#page_196) _f_ ,\n[223](concl.html#page_223)\n\npanopticon, [61](ch02.html#page_61)\n\nPapua New Guinea, [151](ch05.html#page_151)–52, [161](ch05.html#page_161)\n\nParikka, Jussi: _A Geology of Media_ , [31](ch01.html#page_31)\n\nPearson, Karl, [40](ch01.html#page_40)\n\nPenn Treebank Project, [102](ch03.html#page_102)\n\nPhilips N.V., [34](ch01.html#page_34)–35\n\nphrenology, [123](ch04.html#page_123)–25\n\nphysiognomy, [161](ch05.html#page_161)–67, [172](ch05.html#page_172)\n\nPicard, Rosalind, [154](ch05.html#page_154)\n\nPittsburgh Silver Peak Gold Mining Company, [50](ch01.html#page_50)–51\n\nPlanetary Resources, [230](code.html#page_230)–31\n\nPoitras, Laura, [110](ch03.html#page_110), [181](ch06.html#page_181),\n[265](notes.html#page_265)n95\n\npolice: affect recognition systems and, [153](ch05.html#page_153),\n[170](ch05.html#page_170)\n\nfacial recognition systems and, [186](ch06.html#page_186)\n\ngang-related crime datasets, [116](ch03.html#page_116)–17\n\nmug shot databases, [89](ch03.html#page_89)–92, [93](ch03.html#page_93)\n\nsurveillance systems and, [197](ch06.html#page_197)–202\n\npolygenism, [124](ch04.html#page_124)–25\n\nPotemkin, Prince, [61](ch02.html#page_61)–62\n\nPouget, Émile, [81](ch02.html#page_81)\n\npower structures: AI and, [8](intro.html#page_8)–9, [18](intro.html#page_18)\n\nin atlas conception of AI, [10](intro.html#page_10)–11\n\nmicrophysics of power, [251](notes.html#page_251)n66. _See also_ state power\n\npraseodymium, [33](ch01.html#page_33)\n\nPrecision Time Protocol (PTP), [77](ch02.html#page_77)–78\n\nPrinceton University, [136](ch04.html#page_136)\n\nprivacy issues, [119](ch03.html#page_119)–21\n\nProject Maven, [189](ch06.html#page_189)–92, [263](notes.html#page_263)n38\n\npromethium, [33](ch01.html#page_33)\n\npsychoanalysis, [156](ch05.html#page_156)\n\nPTP (Precision Time Protocol), [77](ch02.html#page_77)–78\n\nPT Timah (company), [37](ch01.html#page_37)\n\nPuschmann, Cornelius, [254](notes.html#page_254)n56\n\nPython (programming language), [78](ch02.html#page_78)\n\nQuinlan, Michael, [76](ch02.html#page_76)\n\nrace: affect recognition and, [177](ch05.html#page_177)\n\nAI surveillance programs and, [197](ch06.html#page_197)–98\n\ndata classification algorithms for, [144](ch04.html#page_144)–47,\n[222](concl.html#page_222)\n\nracial justice, [227](concl.html#page_227)\n\nrailroads, time coordination by, [80](ch02.html#page_80)\n\nrare earth minerals, [33](ch01.html#page_33), [218](concl.html#page_218),\n[248](notes.html#page_248)n31\n\nreCAPTCHA, [68](ch02.html#page_68)–69\n\nrenewable energy, [42](ch01.html#page_42)\n\nRing doorbell cameras, [183](ch06.html#page_183), [201](ch06.html#page_201)–2\n\nRoberts, Sarah, [64](ch02.html#page_64)\n\nRockwood Holdings, Inc., [29](ch01.html#page_29)\n\nRussell, James, [171](ch05.html#page_171)\n\nRussell, Stuart, [7](intro.html#page_7), [11](intro.html#page_11)\n\nRussia, data surveillance and collection in, [185](ch06.html#page_185)\n\nRyge, Leif, [257](notes.html#page_257)n52\n\nSadowski, Jathan, [114](ch03.html#page_114)\n\nSalisbury, Lord, [80](ch02.html#page_80)\n\nsamarium, [33](ch01.html#page_33)\n\nSamsung, [37](ch01.html#page_37)\n\nSan Francisco: gold mining’s impact on, [25](ch01.html#page_25)–27\n\nhomelessness in, [28](ch01.html#page_28)\n\ntech industry in, [23](ch01.html#page_23), [27](ch01.html#page_27). _See also_\nSilicon Valley\n\nSaville, Samantha, [13](intro.html#page_13)\n\nScahill, Jeremy, [265](notes.html#page_265)n95\n\nscandium, [33](ch01.html#page_33)\n\nSchaake, Marietje, [223](concl.html#page_223)–24\n\nSchaffer, Simon, [70](ch02.html#page_70)–71\n\nScharmen, Fred, [232](code.html#page_232)\n\nSchmidt, Eric, [191](ch06.html#page_191), [231](code.html#page_231)\n\nSchultz, Jason, [199](ch06.html#page_199)\n\nScreening of Passengers by Observation Techniques (SPOT) program,\n[170](ch05.html#page_170)\n\nSecret Service (U.S.), [170](ch05.html#page_170)\n\nSedgwick, Eve Kosofsky, [258](notes.html#page_258)n25\n\nSejnowski, Terry, [167](ch05.html#page_167)\n\nSekula, Allan, [91](ch03.html#page_91)\n\nShenmao (company), [37](ch01.html#page_37)\n\nshipping industry, [46](ch01.html#page_46)–47\n\nSilicon Valley: geography of, [23](ch01.html#page_23)\n\nstate power and AI development in, [188](ch06.html#page_188)\n\ntime coordination and, [77](ch02.html#page_77). _See also_ _specific tech\ncompanies_\n\nsilver mining, [26](ch01.html#page_26)\n\nSilver Peak, Nevada, [24](ch01.html#page_24)–32, [51](ch01.html#page_51),\n[247](notes.html#page_247)n10\n\nSimon, Herbert, [6](intro.html#page_6)\n\nSinclair, Upton: _The Jungle_ , [72](ch02.html#page_72)\n\nSiri, [42](ch01.html#page_42)–43, [58](ch02.html#page_58)\n\nslavery, [62](ch02.html#page_62)\n\nSmith, Adam, [69](ch02.html#page_69)–70; _Wealth of Nations_ ,\n[59](ch02.html#page_59)\n\nSmith, Brad, [191](ch06.html#page_191), [263](notes.html#page_263)n36\n\nSnowden archive, [181](ch06.html#page_181)–82, [188](ch06.html#page_188),\n[203](ch06.html#page_203), [209](ch06.html#page_209),\n[265](notes.html#page_265)n95\n\nSnyder, Rick, [206](ch06.html#page_206)\n\nsocial credit scoring, [205](ch06.html#page_205)–7\n\nSony, [37](ch01.html#page_37)\n\nSorenson, E. Richard, [151](ch05.html#page_151)–52\n\nSouriau, Étienne, [20](intro.html#page_20)\n\nSouth Africa, racial classification schemes in, [145](ch04.html#page_145)\n\nspace colonization, [229](code.html#page_229)–34\n\nSpaceX, [231](code.html#page_231), [233](code.html#page_233)\n\nspeech recognition, [100](ch03.html#page_100)–101\n\nSPOT (Screening of Passengers by Observation Techniques) program,\n[170](ch05.html#page_170)\n\nSproull, Robert, [184](ch06.html#page_184)\n\nSpurzheim, Joseph Gaspar, [162](ch05.html#page_162)\n\nStanford University, [110](ch03.html#page_110)\n\nStar, Susan Leigh, [12](intro.html#page_12), [127](ch04.html#page_127)–28,\n[139](ch04.html#page_139), [145](ch04.html#page_145),\n[148](ch04.html#page_148)\n\nStark, Luke, [113](ch03.html#page_113), [146](ch04.html#page_146)\n\nStarosielski, Nicole, [39](ch01.html#page_39)\n\nstate power, [17](intro.html#page_17)–18, [181](ch06.html#page_181)–209,\n[222](concl.html#page_222)–23\n\nAI as weapon of war, [186](ch06.html#page_186)–89\n\ndata extraction and, [207](ch06.html#page_207)–9\n\noutsourced AI development and, [192](ch06.html#page_192)–203\n\nProject Maven, [189](ch06.html#page_189)–92\n\ntargeting and, [203](ch06.html#page_203)–7\n\nThird Offset strategy and, [186](ch06.html#page_186)–89\n\nstatistical bias, [135](ch04.html#page_135)\n\nStrubell, Emma, [42](ch01.html#page_42)\n\nsubmarine cable construction, [38](ch01.html#page_38)–39,\n[46](ch01.html#page_46)\n\nSuchman, Lucy, [12](intro.html#page_12), [192](ch06.html#page_192),\n[222](concl.html#page_222), [263](notes.html#page_263)n41\n\nSuri, Sid, [63](ch02.html#page_63)\n\nsurveillance: facial-recognition algorithms, [92](ch03.html#page_92)\n\nlabor and, [56](ch02.html#page_56), [60](ch02.html#page_60)–61,\n[70](ch02.html#page_70), [75](ch02.html#page_75)–77,\n[219](concl.html#page_219)\n\nstate power and, [196](ch06.html#page_196)–98\n\nSutton, Rich, [43](ch01.html#page_43)\n\nSyria: data surveillance and collection in, [185](ch06.html#page_185)\n\nrefugee crisis, [204](ch06.html#page_204)–5\n\ntantalum, [34](ch01.html#page_34)\n\nTaylor, Astra, [13](intro.html#page_13), [66](ch02.html#page_66),\n[71](ch02.html#page_71), [252](notes.html#page_252)n75\n\nTaylor, Frederick Winslow, [57](ch02.html#page_57), [73](ch02.html#page_73)–74\n\n_Principles of Scientific Management_ , [74](ch02.html#page_74)\n\nTCP/IP networks, [77](ch02.html#page_77)\n\ntelegraph, time coordination by, [80](ch02.html#page_80)–81\n\nTeller, Edward, [24](ch01.html#page_24)\n\nTencent, [187](ch06.html#page_187)\n\nTensorFlow AI infrastructure, [190](ch06.html#page_190)\n\nterbium, [33](ch01.html#page_33), [36](ch01.html#page_36)\n\nterrorism, [203](ch06.html#page_203)–5\n\nTesla: lithium consumption by, [29](ch01.html#page_29)–30\n\nsupply chain, [37](ch01.html#page_37)\n\nThiel, Peter, [193](ch06.html#page_193)\n\nThompson, E. P., [60](ch02.html#page_60)\n\nthulium, [33](ch01.html#page_33)\n\nTikTok, [149](ch04.html#page_149)\n\ntime coordination: labor and, [16](intro.html#page_16),\n[56](ch02.html#page_56), [74](ch02.html#page_74)–82\n\nneed for, [53](ch02.html#page_53)–56\n\nprivatization of, [77](ch02.html#page_77)–82\n\nrailroads and, [80](ch02.html#page_80)\n\ntelegraph and, [80](ch02.html#page_80)–81\n\ntin, [34](ch01.html#page_34), [37](ch01.html#page_37)–38\n\nTomkins, Silvan, [156](ch05.html#page_156)–59, [160](ch05.html#page_160),\n[173](ch05.html#page_173)\n\n_Affect Imagery Consciousness_ , [156](ch05.html#page_156)\n\ntransgender individuals, [138](ch04.html#page_138)\n\ntransparency, [12](intro.html#page_12)\n\nTransportation Security Administration (TSA), [170](ch05.html#page_170)\n\nTREASUREMAP program, [182](ch06.html#page_182)–83, [183](ch06.html#page_183)\n_f_ , [201](ch06.html#page_201)\n\nTrueTime, [79](ch02.html#page_79)\n\nTu, Thuy Linh, [20](intro.html#page_20)\n\nTully, John, [38](ch01.html#page_38)–39\n\ntungsten, [34](ch01.html#page_34)\n\nTuring, Alan, [5](intro.html#page_5)\n\nTurner, H. W., [247](notes.html#page_247)n10\n\nTversky, Amos, [135](ch04.html#page_135)\n\nUber, [86](ch02.html#page_86)\n\nUllman, Ellen, [7](intro.html#page_7)\n\nUnilever, [154](ch05.html#page_154)\n\nUniversity of California–San Diego, [154](ch05.html#page_154)\n\nUniversity of Colorado, [109](ch03.html#page_109)\n\nUniversity of Maryland, [177](ch05.html#page_177)\n\nUniversity of Pennsylvania, [102](ch03.html#page_102)\n\nUniversity of Tennessee, [144](ch04.html#page_144)\n\nUSA Freedom Act of 2015, [188](ch06.html#page_188)\n\nU.S. Army Research Laboratory, [104](ch03.html#page_104),\n[109](ch03.html#page_109)\n\nU.S. Geological Survey, [33](ch01.html#page_33)\n\nUTKFace dataset, [144](ch04.html#page_144)–45, [149](ch04.html#page_149)\n\nutopian vs. dystopian perspectives, [214](concl.html#page_214)–15\n\nvalue production chain, [70](ch02.html#page_70)–71\n\nvariance in data classification, [134](ch04.html#page_134)\n\nVigilant Solutions, [199](ch06.html#page_199)–201\n\nvoice data, collection of, [119](ch03.html#page_119)\n\nVon Braun, Wernher, [117](ch03.html#page_117), [230](code.html#page_230),\n[234](code.html#page_234)–35, [268](notes.html#page_268)n27\n\nVon Kempelen, Wolfgang, [67](ch02.html#page_67)\n\nVon Neumann, John, [5](intro.html#page_5)\n\nVon Osten, Wilhelm, [1](intro.html#page_1)–4, [3](intro.html#page_3) _f_\n\nWajcman, Judy, [77](ch02.html#page_77)\n\nWalker, Kent, [192](ch06.html#page_192)\n\nwater supply, [44](ch01.html#page_44)–45, [218](concl.html#page_218)\n\nWeigel, Moira, [193](ch06.html#page_193)\n\nWeizenbaum, Joseph, [5](intro.html#page_5), [118](ch03.html#page_118)\n\nWest, Cornel, [126](ch04.html#page_126)\n\nWestern Union, [80](ch02.html#page_80)\n\nWeWork, [76](ch02.html#page_76)\n\nWhite Sands Missile Range, [234](code.html#page_234)\n\nWiener, Norbert, [6](intro.html#page_6)\n\nWikipedia, [143](ch04.html#page_143)\n\nWinner, Langdon, [225](concl.html#page_225)\n\nWISARD (machine learning object-recognition system), [166](ch05.html#page_166)\n\nWordNet, [98](ch03.html#page_98), [136](ch04.html#page_136)–37,\n[143](ch04.html#page_143)\n\nWork, Robert, [188](ch06.html#page_188)\n\nWorld Shipping Council, [47](ch01.html#page_47)\n\nx.ai (digital personal assistant firm), [65](ch02.html#page_65)\n\nXRVision Sentinel AI, [177](ch05.html#page_177)\n\nYork, Jillian, [110](ch03.html#page_110)\n\nYouTube, [119](ch03.html#page_119)\n\nytterbium, [33](ch01.html#page_33)\n\nyttrium, [33](ch01.html#page_33)\n\nZuboff, Shoshana: _Surveillance Capitalism_ , [110](ch03.html#page_110)\n\n",
    "book_id": "the_atlas_of_ai",
    "book_title": "The Atlas of AI",
    "book_author": "Kate Crawford",
    "topic_id": "ai_theory",
    "topic_label": "theory",
    "chunk_index": 6
  },
  {
    "chunk_full": "![cover](assets/front_cover.png)\n\n\n# Software Above the Level of a Single Device\n\nThe Implications\n\nTim O’Reilly\n\n\n# Software Above the Level of a Single Device: The Implications\n\nby Tim  O’Reilly\n\nCopyright © 2015 O’Reilly Media. All rights reserved.\n\nPrinted in the United States of America.\n\nPublished by O’Reilly Media, Inc., 1005 Gravenstein Highway North, Sebastopol,\nCA 95472.\n\nO’Reilly books may be purchased for educational, business, or sales\npromotional use. Online editions are also available for most titles\n(_<http://safaribooksonline.com>_). For more information, contact our\ncorporate/institutional sales department: 800-998-9938 or\n_corporate@oreilly.com_.\n\n  * Adapted by: Troy Mott\n  * Editor: Brian Sawyer\n  * Production Editor: Melanie Yarbrough\n  * Interior Designer: David Futato\n  * Cover Designer: Karen Montgomery\n  * Illustrator: Rebecca Demarest\n\n  * February 2015: First Edition\n\n# Revision History for the First Edition\n\n  * 2015-01-26: First Release\n\nThe O’Reilly logo is a registered trademark of O’Reilly Media, Inc. _Software\nAbove the Level of a Single Device_ , the cover image, and related trade dress\nare trademarks of O’Reilly Media, Inc.\n\nWhile the publisher and the author have used good faith efforts to ensure that\nthe information and instructions contained in this work are accurate, the\npublisher and the author disclaim all responsibility for errors or omissions,\nincluding without limitation responsibility for damages resulting from the use\nof or reliance on this work. Use of the information and instructions contained\nin this work is at your own risk. If any code samples or other technology this\nwork contains or describes is subject to open source licenses or the\nintellectual property rights of others, it is your responsibility to ensure\nthat your use thereof complies with such licenses and/or rights.\n\n978-1-449-37451-8\n\n[LSI]\n\n\n  1. [1\\. Software Above the Level of a Single Device: The Implications](ch01.html#idp127120)\n     1. [Multiple Smart Things](ch01.html#idp140672)\n     2. [Importance of Human Input](ch01.html#idp153968)\n     3. [Implicit Versus Explicit Input](ch01.html#idp161344)\n     4. [Types of Sensors](ch01.html#idp161648)\n     5. [The System as a User Interface](ch01.html#idp172016)\n     6. [A Network of Devices](ch01.html#idp172272)\n     7. [The Robustness Principle](ch01.html#idp184368)\n     8. [Software Above the Level of a Single Device](ch01.html#idp185232)\n     9. [System of Interaction](ch01.html#idp180112)\n     10. [How the World “Should” Work](ch01.html#idp198720)\n     11. [Think About Things That Seem Hard](ch01.html#idp207344)\n  2. [O’Reilly Solid Conference](preface01.html#idp127584)\n\n\n# Chapter 1. Software Above the Level of a Single Device: The Implications\n\n_The following document is adapted from\nthe[keynote](https://www.youtube.com/watch?v=jvG6GbqxNfY) address by [Tim\nO’Reilly](http://www.oreilly.com/pub/au/27), [Software Above the Level of a\nSingle Device: The\nImplications](http://solidcon.com/solid2014/public/schedule/detail/34057),\ngiven at the Solid 2014 conference. Follow along to learn how you can best\ntake advantage of new technology known as the Internet of Things._\n\n##### Highlights\n\n  * Humans and machines work together in a complex pattern, where data is captured through human activity, stored in the cloud, pre-processed, and then used by a robot in the Internet of Things.\n  * If we really want it to be an Internet, as opposed to a set of Intranets, we have to think about interoperability.\n  * The smartphone that we carry in our pockets is filled with sensors, and it’s filled with capabilities, which is the key component to so many of these things.\n\nThere is some pretty amazing stuff we are seeing here at the Solid 2014\nConference. I want to give you a little bit of perspective, though.\n\nOne of my favorite quotes is this one from Edwin Schlossberg. If you’ve heard\nme talk, you’ve heard it before: “The skill of writing is to create a context\nin which other people can think.”\n\nAnd that means that the way we talk about things is a kind of map. And like\nany map, it can either take us to the right place, or it can lead us astray. I\nwant to talk a little bit about some of the words that we use in the current\ncontext and start thinking about what we might be missing.\n\nI did a word cloud of the Internet of Things article on Wikipedia and as you\ncan see,  _things_ is a really big word in the cloud, and not only that but\n_devices_ and _objects_ also appear a lot. And sure enough, there are some\npretty amazing smart things. That word, _smart_ , also shows up in the word\ncloud.\n\n![Alt Text](assets/wordcloud.png)\n\n# Multiple Smart Things\n\nOut front in the demo hall of the building, you probably saw the\n[Taktia](http://www.taktia.com) smart router. I’m a home craftsman, and I want\none of these. I’m not as good as I’d like to be, and this thing would make me\nbetter. It’s a human augmentation, and it is super awesome.\n\n![Alt Text](assets/taktia.png)\n\nThe [Onewheel](http://rideonewheel.com/): I tried it yesterday, not very well,\nbut it is also pretty awesome.\n\n![Alt Text](assets/onewheel.png)\n\nAnd of course the [Makani](http://www.google.com/makani/) airborne wind\nturbines use incredible smart control to generate power. This is an invention\noriginally made by my son-in-law, Saul Griffith, so I’m very proud of that.\n\n![Alt Text](assets/makani.png)\n\n# Importance of Human Input\n\nThere is something missing in this word cloud, because we shouldn’t just be\ntalking about smart things. _P_ _eople_ and _time_ are also concepts in there.\nBut they are way, way too small. I think one of the things I’d like to have as\nan outcome of this talk is that the people in this room go read that Wikipedia\nentry and make it better, because I really don’t think it actually captures so\nmany of the concepts that we need to be thinking about today.\n\nI want to talk a little bit about these aspects of people and time that are\ntoo small in that graph.\n\nWhen we think of the Internet of Smart Things, we tend to imagine that these\nthings—the Nest thermostat or the Google self-driving car—they’re sensor and\ndata driven, they are autonomous, not really needing human input, and they are\noperating in real time. That is really our first blush imagination.\n\nBut in fact, one of the super interesting things about the Google self-driving\ncar is its connection to the human-driven Google street view vehicle that did\nall of the initial mapping. What you see here is actually humans and machines\noperating together in a complex pattern in which data is captured through\nhuman activity, stored in the cloud, pre-processed, and then used by a robot.\n\nAnd I think that pattern is a really, really important one to pay attention to\nas you design applications. Think about how data, generated by humans, is\ncaptured over time and is stored and acted upon later by a device. It is not\nall real time.\n\n# Implicit Versus Explicit Input\n\nThe human input is critical, and it may also be implicit rather than explicit.\nSo, when you see the web page for the Nest learning thermostat, the whole\npitch is that it learns implicitly from you, but it also learns explicitly:\nyou set the temperature. (By the way, Nest, you really need to use energy-\nfriendly temperatures in your advertisements!)\n\n![Alt Text](assets/nest.png)\n\nThe Nest auto-detects when you’ve been away for awhile and turns off the heat\nor air conditioning. Okay, that’s sort of our image of the smart thing, but we\nhave to remember that we are still giving that input, it’s just through a\ndifferent kind of interface. We are so used to talking about how we give input\nthrough a keyboard. Then we are giving input through a mouse and now we are\ngiving it through a touch screen. And now, we say “Wow! When we don’t show up\nin the room, that is user input to this device.” It is still user input!\n\nThink of this as a user experience problem and not an autonomous device\nproblem. And of course you have multiple input modalities, since you have a\nNest app. And one of the things I’ve noticed is that once I set the schedule\nusing this app, the device doesn’t seem to learn any more. The interaction\nbetween the explicit, the implicit, and what modes of implicit really matter.\n\n# Types of Sensors\n\nThink a little bit more broadly about the kinds of sensors that you have. When\nsensors first appear, we don’t use them that often, and as a result they still\nseem magical. And then we take them for granted, until someone figures out a\nnew way to make them more powerful. Some of the most important sensors that we\nhave that are now entering this rediscovery phase are the camera and the\nmicrophone in our phones. Both [Siri](https://www.apple.com/ios/siri/) and\n[Google Now](https://www.google.com/landing/now/) are using the microphone as\nthe key to very, very powerful new interfaces, and ones that are going to get\nbetter very rapidly. They are going to be a big part of the user interface mix\nfor this Internet of Things.\n\nThe point is that sensors allow us to create new kinds of user interfaces. But\nyou still need to remember that it is a user interface.\n\n# The System as a User Interface\n\nI do have an example of a bad user interface. And that is this wonderful smart\nkey for my Tesla. And it does wonderful things. I walk up to my car and the\ncar automatically opens. I don’t have to stick it in a tiny slot or turn it,\nor any of those things we were so used to in the mechanical age.\n\n![Alt Text](assets/tesla_key.png)\n\nBut it’s got one really bad flaw, which is all over the forums, which is you\ncan’t hang it on a key ring. That’s actually a stupid device, because it\ndidn’t think about how I might want to use it. And any time I leave my car in\nvalet at the airport it comes back in a little plastic bag because they can’t\nhang it on the hook. Sometimes they lose it. The entire system in which we\noperate is the user interface.\n\nThere’s this great sticker that was given to me by Liam Maxwell, who is the\nCTO of the British government.\n\n![Alt Text](assets/sticker.png)\n\nThey’ve really focused hard on this idea: what is the user need? And, they\nmade that the first of their design principles, to start with needs.\n\nAs we design this new world we need to think about user needs first.\n\n# A Network of Devices\n\nComing back to this Internet of Things word cloud, I want to move on and pick\nout one of the big words that we use, and that is _Internet_. And the Internet\nreally matters. When you look at that smart device, it’s not a standalone\ndevice. Yes, it’s controlled by its smartphone, and yes, Nest now offers other\ndevices connected to the network with the thermostat as its hub, and they talk\nto each other. In fact, all of these things are connected to satellites and\ndata centers, and potentially to other similar devices or other smart devices.\n\n![Alt Text](assets/datacenter.png)\n\nAnd, by the way, even in that data center, you actually have smart networks of\nthings. The cooling is actually controlled semi-autonomously. So there is this\nbig network all over. There’s a network of data centers all over the world, so\nthe Internet is clearly very involved. But let’s remember the original ground\nrules of the Internet.\n\n# The Robustness Principle\n\nWe used to call the Internet “The Network of Networks,” because it was this\nmagical thing that connected all of these incompatible networks. And\ninteroperability was the focus. One of the things I worry about as we move\ninto this new world is that we may have forgotten that interoperability. We\nhave vendors who are trying to own it all, building systems that talk to their\ndevices, but not to everyone else’s. We have to think a lot about\ninteroperability.\n\nAnd we have to think about this wonderful principle that was put out by one of\nthe saints of the early Internet, Jon Postel. (I wonder sometimes what would\nhave happened if he hadn’t died too young of a heart attack.) He wrote in the\n[TCP RFC](https://www.ietf.org/rfc/rfc793.txt), “Be conservative in what you\nsend, be liberal in what you accept from others.” It’s become known as the\nRobustness Principle.\n\nThat is such an important principle, and I want those of you who are designing\ndevices or systems to think about interoperability and to remember if we\nreally want it to be an Internet, as opposed to a set of Intranets, you have\nto think about interoperability.\n\n# Software Above the Level of a Single Device\n\nThe next thing I want to cover briefly is the notion of software above the\nlevel of a single device. This is a phrase that I got from Dave Stutz, who\nwrote a [fabulous letter](http://www.synthesist.net/writing/onleavingms.html)\nwhen he left Microsoft back in 2003. It was his parting advice. And it ended\nwith the line, “Useful software written above the level of the single device\nwill command high margins for a long time to come.\"\n\nThis was very, very prescient, because a lot of focus was still on the PC and\neven on the network; it was very small thinking. And his notion of software\nabove the level of a single device stuck in my head. I’ve used it for years.\nIt was part of my core Web 2.0 principles. But I want to bring it out in the\nexample of the Uber app.\n\n# System of Interaction\n\nLet’s not get too taken up with new wearables. Uber is a smart things app. We\nforget that the phone is our most widely used smart thing. This thing that we\ncarry in our pockets is filled with sensors, and it’s filled with\ncapabilities.\n\n![Alt Text](assets/uber.png)\n\nBut what’s really interesting about Uber is, of course, that it doesn’t work\nin isolation. There’s an app for the passenger, but there’s also an app for\nthe driver. And those two things are coordinating in real time using a kind of\nInternet operating system. There are various types of functions for\ncommunication, and for GPS to locate everybody, and to track progress. There\nis a payment and a rating system. All of these things are part of a “system of\ninteraction.” That’s a wonderful phrase that was coined by someone at IBM.\n\nWhat I want you to think about here is that once you have new UI capabilities\nand new augmentations of humans via sensors, you can actually start to think\nabout things differently.\n\nThere’s a wonderful quote from [Aaron Levie](https://twitter.com/levie) at Box\nthat I saw on Twitter. He said, “Uber is the lesson in building for how the\nworld should work, instead of optimizing for how the world does work.” That’s\nour opportunity with this new technology that we’ve been given.\n\n# How the World “Should” Work\n\nWe can start thinking about how the world _should_ work, instead of optimizing\nhow the world does work. So, back to the [Makani wind\nturbine](http://www.google.com/makani/technology/) out there. The idea is\nthat, instead of having a turbine sitting on the ground like we’ve had since\nthe Dutch windmills, you could actually put one in the sky using this\nincredible control technology. Now, they are still working on it, but the\nnotion that this thing has to be able to fly autonomously for thousands of\nhours, and has to be able to take off and land by itself, and it’s generating\npower up high where there is always wind. That’s thinking about how the world\nshould work, rather than how the world does work.\n\nThis notion is critical and the sensors let us do new things. A company that\nwe are invested in at O’Reilly Alpha Tech Ventures is\n[Cover](http://www.paywithcover.com/).\n\n![Alt Text](assets/cover.png)\n\nBecause you are registered with the cloud, and because the application is able\nto tell where you are, you can walk into a restaurant and sit down. You are\nidentified, fed, and when you are done with the meal you walk out and your\ncredit card is charged. It’s kind of magic. Because of sensors, we can rethink\nthe way things work.\n\nAnother great example is [Makespace](https://www.makespace.com/): your closet\nin the cloud. This is an example of how sensors don’t have to be complicated.\nThese guys have realized that if you can actually take pictures of what you\nput in the boxes when you put stuff in storage, and you can identify what’s in\nthat box, you can put this stuff away in a warehouse where space is cheap.\nThey can then bring you just the box you want. You don’t have to go rooting\nthrough your storage closet, because you can effectively go into a robotic\nwarehouse. Once again, they are rethinking a familiar process, because we now\nhave new capabilities.\n\n![Alt Text](assets/makespace.png)\n\n# Think About Things That Seem Hard\n\nWhere I want to go with this is a final piece of advice, which is don’t just\ntry to re-create the experiences and the technologies that we have today. Try\nto think about new things, and in particular, think about things that seem\nhard—things that might have seemed impossible before you had these new\ncapabilities.\n\nOne of the things I’m most excited about in this technology revolution is how\nit is giving us amazing new capabilities to affect the physical world. And the\nphysical world, in the end, is where we all live, and where the biggest\nproblems that we face as a society are to be found.\n\nWe have to feed the world. We have to generate energy. We have to deal with\nclimate change. We have to deal with the problems of our society. And there\nare amazing new capabilities, and I want you to not just make cute, cool, and\namazing consumer devices. I want you to think about hard problems that you can\nsolve. Take this technology and make the world a better place.\n\nThank you.\n\n\n[![](assets/solid.png)](http://oreil.ly/1uG4nAr)\n\n",
    "book_id": "software_above_device",
    "book_title": "Software Above the Level of a Single Device",
    "book_author": "Tim O'Reilly",
    "topic_id": "ai_theory",
    "topic_label": "theory",
    "chunk_index": 7
  }
]