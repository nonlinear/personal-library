[
  {
    "chunk_full": "Vol.:(0123456789)\n1 3\nAI and Ethics (2021) 1:463â€“476 \nhttps://doi.org/10.1007/s43681-021-00054-3\nORIGINAL RESEARCH\nA framework toÂ contest andÂ justify algorithmic decisions\nClÃ©mentÂ Henin1,2â€Š Â Â· DanielÂ LeÂ MÃ©tayer1\nReceived: 22 January 2021 / Accepted: 7 April 2021 / Published online: 4 May 2021 \nÂ© The Author(s), under exclusive licence to Springer Nature Switzerland AG 2021\nAbstract\nIn this paper, we argue that the possibility of contesting the results of Algorithmic Decision Systems (ADS) is a key require-\nment for ADS used to make decisions with a high impact on individuals. We discuss the limitations of explanations and \nmotivate the need for better facilities to contest or justify the results of an ADS. While the goal of an explanation is to make \nit possible for a human being to understand, the goal of a justification is to convince that the decision is good or appropriate. \nTo claim that a result is good, it is necessary (1) to refer to an independent definition of what a good result is (the norm) and \n(2) to provide evidence that the norm applies to the case. Based on these definitions, we present a challenge and justification \nframework including three types of norms, a proof-of-concept implementation of this framework and its application to a \ncredit decision system.\nKeywordsâ€‚ ChallengeÂ Â· JustificationÂ Â· Machine learningÂ Â· Training datasetÂ Â· EvidenceÂ Â· Norm\n1â€‚ Introduction\nThe possibility of contesting the results of Algorithmic Deci-\nsion Systems (ADS) is a key requirement for ADS used to \nmake decisions with a high impact on individuals. This is the \ncase, for example, for decisions made by health profession-\nals, by judges or by bankers. This need is acknowledged, to \nsome extent, by the GDPR, which states that a person who is \nâ€œsubject to a decision based solely on automated processingâ€ \nhas â€œthe right to obtain human intervention on the part of the \ncontroller, to express his or her point of view and to contest \nthe decisionâ€ (Article 22(3)). However, there can be many \nbarriers preventing this right from being exercised, the first \none being the practical difficulty to understand the grounds \nfor a decision based on the results of an ADS. To ensure \nthat this right can be effective, we argue that contestability \nshould be supported by appropriate tools. As stated by Mul-\nligan etÂ al., â€œcontestability can support critical, generative, \nand responsible engagement between users and algorithms, \nusers and system designers, and ideally between users and \nthose subject to decisions (when they are not the users), as \nwell as the publicâ€ [29]. Indeed, providing ways to contest a \ndecision can be beneficial in many respects:\nâ€“\t It makes the ADS more effective because it enhances the \nability of the human decision maker to detect inappropri-\nate results (suggestions of wrong decisions).\nâ€“\t It makes the ADS more accountable because decisions \ncan be accompanied by justifications.\nâ€“\t It makes the ADS more acceptable from the ethical point \nof view because it preserves the autonomy of the human \ndecision maker. Indeed, even if the human decision \nmaker does not have any formal obligation to follow the \nsuggestion of the ADS, his/her autonomy is question-\nable if he/she does not have any possibility to contest it. \nBecause â€œcontestability fosters engagement rather than \npassivity, questioning rather than acquiescenceâ€ [29], it \nis a key condition to empower human decision makers.\nSome authors have already advocated contestability by \ndesign and analyzed the challenges to be addressed to imple-\nment it [2, 13]. Opacity is often put forward as a first obsta-\ncle to the contestation of ADS-based decisions. Indeed, it is \ndifficult to contest the results of a system when information \nabout its logic, operation or input data is too scarce or not \nintelligible. The possibility to produce intelligible explana-\ntions about the results or the overall logic of an ADS is \n *\t ClÃ©ment Henin \n\t\nclement.henin@inria.fr\n\t\nDaniel Le MÃ©tayer \n\t\ndaniel.le-metayer@inria.fr\n1\t\nUniv Lyon, Inria, INSA Lyon, CITI, Villeurbanne, France\n2\t\nÃ‰cole des Ponts ParisTech, Champsâ€‘surâ€‘Marne, France\n",
    "book_id": "a_framework_to_contest_and_justify_algorithmic_decisions",
    "book_title": "A framework to contest and justify algorithmic decisions",
    "book_author": "ClÃ©ment Henin ",
    "topic_id": "ai_policy",
    "topic_label": "policy",
    "chunk_index": 0
  },
  {
    "chunk_full": "464\n\t\nAI and Ethics (2021) 1:463â€“476\n1 3\ntherefore very useful to enhance contestability. However, as \nstated by Mulligan etÂ al., providing explanations is not suffi-\ncient: â€œregulatory approaches should seek to put profession-\nals and decision support systems in conversation, not posi-\ntion professionals as passive recipients of system wisdom \nwho must rely on out-of-system mechanisms to challenge \nthem. For these reasons, calls for explainability fall short \nand should be replaced by regulatory approaches that drive \ncontestable designâ€ [29]. In this paper, we take the same \nstance and argue that the answer to contestations should be \njustifications and, even though the two words are sometimes \nused interchangeably, there are essential differences between \nexplanations and justifications.\nThe word â€œjustificationâ€ itself is used with different \nmeanings in the AI literature. In this paper, we propose the \nfollowing distinctions, which are consistent with T. Millerâ€™s \ncharacterization of justifications [24]:\nâ€“\t The goal of an explanation is to make it possible for a \nhuman being (designer, user, affected person, etc.) to \nunderstand (a result or the whole system). In contrast, the \ngoal of a justification is to convince that the decision is \ngood. For example, an explanation for a bank loan appli-\ncation rejection could be that the number of outstanding \nloans is too high. This information helps to understand \nthe logic of the system through a salient feature used by \nthe ADS. The appropriateness of the decision is not ques-\ntioned. By contrast, a justification of the same decision \ncould be that applications with many outstanding loans \nhave a high probability to lead to credit defaults, which \nis a risk that the bank wants to reduce. Even if they often \nsupport each other, explanations and justifications have \ndifferent goals: a user can understand the logic leading to \na particular result without agreeing on the fact that this \nresult is good; vice versa, he/she may want to contest a \nresult (being convinced that it is bad) without knowing \nor understanding the logic behind the algorithm.\nâ€“\t Explanations are descriptive and intrinsic in the sense \nthat they only depend on the system itself. In contrast, \njustifications are normative and extrinsinc in the sense \nthat they depend on a reference (or a norm) according to \nwhich the validity of the results can be assessed. Indeed, \nto claim that a result is good, it is necessary (1) to refer \nto an independent definition of what a good result is (the \nnorm) and (2) to provide evidence that this norm applies \nto the case. In the above example, the norm is the objec-\ntive of the bank to reduce credit defaults.\nIt is important to stress that the notion of norm is central \nto the challenge-justification dialectic. In general, different \ntypes of norms can be applicable to an ADS. These norms \ncan have different sources of legitimacy (legal, ethical, \nsocial, economic, etc.) and can be expressed in different \nways (e.g. through law or jurisprudence for legal norms). \nWhen several norms apply, they may be in tension, or even \nin contradiction1. In some cases, it is possible to rely on \npriority rules to establish precedence of a norm over another \none (e.g., international law usually prevails over domestic \nlaw, constitution prevails over ordinary laws, which prevail \nover decrees, etc.); in other cases, such rules may not exist \nand the conflicts between them must be solved by a human \ndecision maker on a case by case basis.\nChallenges and justifications are dual notions: a challenge \ncan be seen as a statement that a decision is not good, sup-\nported by evidence, while a justification is a statement that \na decision is good, supported by evidence. In both cases, \nevidence refers to a given norm.\nAs this discussion shows, there can be many different \nways to challenge and to justify decisions. Our main contri-\nbutions in this paper are the following:\nâ€“\t A general definition of the notions of challenge, justifica-\ntion, norm, evidence, statement and argument.\nâ€“\t A generic framework based on the above definitions \nincluding a challenge and justification protocol. Three \ntypes of norms are considered in this paper; they are \ninspired by the three main moral theories (virtue ethics, \nconsequentialism and deontological ethics).\nâ€“\t A proof of concept (PoC) implementation of the frame-\nwork called Algocate and its application to a credit deci-\nsion system.\nSectionÂ 2 introduces the notions used in the paper illustrated \nwith a case study. SectionÂ 3 and SectionÂ 4 present respec-\ntively the framework and its application to the design of \nAlgocate, our challenge and justification system. SectionÂ 5 \nshows examples of our Algocate implementation applied to \na credit decision system. SectionÂ 6 is a discussion of related \nwork and SectionÂ 7 concludes with a discussion and sugges-\ntions for further research.\n2â€‚ \u0007Challenges andÂ justifications: informal \nintroduction\nTo illustrate our framework on a concrete example, we \nintroduce an hypothetical bank credit decision system. We \nassume that this ADS relies on black-box machine learning \ntechnology. Application files are composed of information \nabout the applicant (age, gender, marital status, revenue, \nlevel of education and outstanding loans) and information \n1â€‚ In the area of ethics, the existence of incompatible commitments is \nsometimes called â€œmoral overloadâ€ [14].\n",
    "book_id": "a_framework_to_contest_and_justify_algorithmic_decisions",
    "book_title": "A framework to contest and justify algorithmic decisions",
    "book_author": "ClÃ©ment Henin ",
    "topic_id": "ai_policy",
    "topic_label": "policy",
    "chunk_index": 1
  },
  {
    "chunk_full": "465\nAI and Ethics (2021) 1:463â€“476\t\n1 3\nabout the credit itself (amount, duration, interest rate and \ninsurance).\nWe assume that the norms shown in Fig.Â 1 apply to this \ncase study. These norms have different sources (fundamental \nrights, sectorial rules, business rules, etc.). The justification \nsystem provides ways to express these norms but discussions \nor debates about their legitimacy are beyond its scope. In \nfact, the justification system does not even assume that the \nlegitimacy of a norm is accepted by all parties. For example, \nin the case study considered here, the bank could refer to \ninternal business rules which are not known or accepted by \ncustomers. In such situations, if a party refers to a norm that \nis not accepted by the other party, the challenge-justification \nprotocol requires the intervention of the human decision \nmaker (called the â€œarbitratorâ€ hereinafter, even if it does \nnot have to be a arbitrator in the legal sense of the term). \nThe benefit of the protocol in such cases is that a party rely-\ning on a norm is compelled to elicit it and to submit it to the \napproval of the other party (or the arbitrator as a last resort).\nFigureÂ 2 presents an example of the interaction of a user \nwith a justification system in our framework. The statements \nof the user of the system, which are expressed in natural \nlanguage, are labelled with Ui and the system answers are \nlabelled with Ai:\nâ€“\t In U1, the user challenges the rejection of his/her \ncredit application and expresses the reasons why he/she \nbelieves that his/her application should be accepted. \nThe user is not required to produce any evidence for \nhis/her challenge because he/she may not be in a posi-\ntion to do so (typically he/she may not have access to \nthe relevant data).\nâ€“\t In A1, the justification system provides evidence sup-\nporting the userâ€™s challenge. In this case, the evidence \nis generated from the learning data set of the ADS.\nâ€“\t In A2, the justification system provides a justification \nfor the rejection decision and the evidence supporting \nit. The justification is a refinement of the challenge \nincluding an additional attribute, the number of out-\nstanding loans. The evidence is again generated from \nthe learning data set of the ADS. It shows that the num-\nber of outstanding loans of the applicant has generally \nbeen considered as a strong argument to reject similar \napplications in the past. In this case, the justification is \nconsidered stronger than the challenge because of the \nvery low ratio of accepted applications among refer-\nence decisions (5% of the decisions accepted, which is \n65% less than the average, when the difference for the \nchallenge in A1 is only 16%), while the number of 260 \ncases is reasonable. The justification system relies on \na strength relationship between arguments to decide if \na justification should prevail or not over a challenge. In \ngeneral, it may be the case that two arguments (justifi-\ncation and challenge) are not comparable. In any case, \nif two parties disagree, the final decision rests with the \narbitrator, the goal of the system being to provide the \nmost valuable information possible to help the latter in \nthis task.\nFig.â€¯1â€‚ â€‰Examples of norms\n1. Compliance with anti-discrimination regulation: gender, ethnicity and country\nof origin should not have any impact on credit decisions.\n2. Banking prudency rule: credit amounts above 20 000 euros must be insured.\n3. Bank Objective: the number of credit defaults must be minimized.\n4. Bank Policy: ADS decisions must be consistent with the decisions made in the\npast by human banking agents, subject to compliance with Bank Objective.\nFig.â€¯2â€‚ â€‰Example of interaction \nwith Algocateâ€Š\nU1 My revenue is greater than 50 000 euros and my credit amount is lower than\n5 000 euros, so my application should be accepted.\nA1 Among the decisions used as reference by the credit decision system, there were\n2 000 applications with revenue greater than 50 000 euros and credit amount\nlower than 5 000 euros and 86 % were accepted (overall average: 70 %).\nA2 However, among the decisions used as reference by the credit decision system,\nthere were 260 applications with revenue greater than 50 000 euros, credit\namount lower than 5 000 euros and 3 or more outstanding loans (as yours) and\n5 % were accepted (overall average: 70 %).\nU2 Credit application CA2 has been accepted. This application has the same\nattributes as mine, except for the gender. Therefore, my application should be\naccepted as well.\nA3 The initial decision would indeed breach anti-discrimination regulation. Your\napplication must therefore be accepted.\n",
    "book_id": "a_framework_to_contest_and_justify_algorithmic_decisions",
    "book_title": "A framework to contest and justify algorithmic decisions",
    "book_author": "ClÃ©ment Henin ",
    "topic_id": "ai_policy",
    "topic_label": "policy",
    "chunk_index": 2
  },
  {
    "chunk_full": "466\n\t\nAI and Ethics (2021) 1:463â€“476\n1 3\nâ€“\t The interaction could then proceed in different ways \ndepending on the fact that the user accepts or not the \nnorm used to support the decision in A2. If he/she does \nnot, then the arbitrator has to decide whether it is accept-\nable or not. Otherwise, the user can either accept the \ndecision or challenge it in a different way. This is the \noption followed in Fig.Â 2: U2 challenges the decision \nbased on anti-discrimination regulation relying on a simi-\nlar application that has been accepted.\nâ€“\t A3 is the answer produced by the justification system \nafter verification of the validity of the U2 challenge. This \nis the last step of the protocol which, in this case, does \nnot require the intervention of the arbitrator since the \nparties have agreed on a revision of the decision.\nThe interactions presented in Fig.Â 2 show that the frame-\nwork is useful both for users contesting the decisions of the \nADS (e.g. individuals affected by the decisions, regulators, \nor human decision makers who are not sure about the sug-\ngestion of the ADS) and for users who want to justify them \n(usually the operator of the ADS)2. The justification system \nis neutral, in the sense that it is designed to find the best \narguments (i.e. statements with evidence supporting them) \nfor both parties. This neutrality is of prime importance, \ngiven the usual imbalance of powers between individuals \nwho are affected by the decisions and the designers or opera-\ntors of the ADS.\n3â€‚ \u0007A framework toÂ challenge andÂ justify \ndecisions\nFigureÂ 2 illustrates only some of the interactions provided \nby Algocate. In this section, we define more precisely the \nnotions used in this paper and introduce our framework \nbefore presenting the Algocate system, which is a particu-\nlar implementation of this framework, in the next section. \nThe framework relies on three notions that were introduced \ninformally in the previous section: statements, norms and \nevidence. TableÂ 1 provides some examples for each of these \nnotions.\nâ€“\t A statement defines what a decision should be (e.g. \nâ€œacceptedâ€ or â€œrejectedâ€), according to a party, and the \nparticular aspects of the case (input data of the ADS, e.g. \napplication file) that make him/her believe so.\nâ€“\t A norm is a reference that can be used by a party to sup-\nport a statement3.\nâ€“\t Evidence is used to show that the norm applies to the \ncase.\nA triple <statement, norm, evidence> is called an argument, \nwhich can be either a justification (if the statement supports \nthe decision) or a challenge (if the statement contradicts the \ndecision). When conflicting arguments are issued by differ-\nent parties, it is useful to be able to compare them. To this \naim, we will introduce a strength relation on arguments.\nIn the following, we define successively statements \n(Sect.Â 3.1), norms (Sect.Â 3.2), and evidence (Sect.Â 3.3) before \nintroducing the strength relation (Sect.Â 3.4).\nTableâ€¯1â€‚ â€‰Examples of statements, norms and evidence\nSTATEMENT\nAbsolute\nâˆ€x âˆˆA, x[education] = PhD âˆ§x[outstanding-loans] â‰¤2\nAll cases having a PhD and less than two outstanding\n=â‡’f(x) = 1\nloans should be accepted\nRelative\nâˆ€x âˆˆA, (âˆƒxâ€² âˆˆA, f(xâ€²) = 1 âˆ§(xâ€²[revenue] â‰¤x[revenue]\nAll cases for which there exists an accepted case with\nâˆ§xâ€²[amount] â‰¥x[amount])) =â‡’f(x) = 1\nlower revenue and greater amount should be accepted\nNORM\nRule\n[Rule, âˆ€x âˆˆA, x[outstanding-loans] â‰¥4 =â‡’f(x) = 0]\nCredit with four or more outstanding loans should be rejected\n(absolute)\nRule\n[Rule, âˆ€x âˆˆA, (âˆƒxâ€² âˆˆA, f(xâ€²) = 1 âˆ§(xâ€²[revenue] = x[revenue]\nAll cases for which there exists an accepted case with the same\n(relative)\nâˆ§xâ€²[education] = x[education] âˆ§xâ€²[age] â‰¥x[age])) =â‡’f(x) = 1]\neducation level and revenue and higher age, should be accepted\nObjective\n[Obj, âˆ†O, nd]\nThe number of accepted non-default cases should be maximised\n(with x[nd] = 1 for non-default cases and x[nd] = 0 otherwise)\n(the number of defaults should be minimised)\nReference\n[Ref, âˆ†R, d]\nThe decisions of the ADS should reï¬‚ect the decisions\n(with x[d] the reference decision for a case x)\nmade in the past by the bank experts\nEVIDENCE\nObjective\n[âˆ†O, 1200, 0.06](i)\nThe non-default rate for the 1200 historical credits\nwhere âˆ†O is the historical database with the objective attribute x[o]\ncorresponding to the statement is 93 % (overall average: 87 %)\nReference\n[âˆ†R, 500, 0.28](ii)\nAmong the 500 past experts decisions corresponding\nwhere âˆ†R is the reference database with the reference attribute x[d]\nto the statement, 95 % were accepted (overall average: 67 %)\n(i) The value 0.06 is the result of the difference 0.93â€“0.87\n(ii) The value 0.28 is the result of the difference 0.95â€“0.67\n2â€‚ In Fig.Â 2, U1 and U2 are contestations while A2 is a justification of \nthe decision.\n3â€‚ As suggested in Sect.Â 2, Algocate is agnostic about the source and \nthe legitimacy of a norm.\n",
    "book_id": "a_framework_to_contest_and_justify_algorithmic_decisions",
    "book_title": "A framework to contest and justify algorithmic decisions",
    "book_author": "ClÃ©ment Henin ",
    "topic_id": "ai_policy",
    "topic_label": "policy",
    "chunk_index": 3
  },
  {
    "chunk_full": "467\nAI and Ethics (2021) 1:463â€“476\t\n1 3\nWe use the following mathematical notations in the \nsequel. Cases are characterized by tuples < a1, â€¦ , am > of \nm attributes. The set of possible values for attribute aj is \ndenoted í€í£ and í€= í€íŸÃ— â‹¯Ã— í€í¦ is the set of all possi-\nble cases. The decision function is called f âˆ¶í€â†’í”¹ with \ní”¹= {0, 1} (we assume that decisions are binary). We use \nthe notation x[j] to refer to the jth attribute of x âˆˆí€â€Š. There-\nfore, x[j] âˆˆí€í£â€Š. The notation is extended to sets S of cases \nwith S[j] = {x[j]|x âˆˆS}â€Š. For the sake of readability, we use \nconstant names such as â€œamountâ€ or â€œdurationâ€ rather than \nindexes in examples. For instance, if the third field of a case \nx represents the amount of a credit, we write x[amount] to \ndenote it (with amount = 3â€Š). The case under consideration \nis called xs âˆˆí€.\n3.1â€‚ \u0007Statements\nAs suggested in the previous section, statements involve two \npieces of information:\nâ€“\t The decision that should be made, according to the issuer \nof the statement (1 for â€œacceptedâ€ or 0 for â€œrejectedâ€) and\nâ€“\t The property of the case that argues in favour of this \ndecision, according to the issuer of the statement.\nIn mathematical terms, statements are, therefore, defined as \nfollows:\nwith í›¿âˆˆí”¹ the decision supported by the issuer and C(x) \nthe property supposed to justify this decision. A statement \nis relevant for a case xs only if C(xs) is true, which will be \nassumed thereafter. If í›¿â‰ f(xs)â€Š, the goal of the statement is \nto contest the decision. We call it a challenging statement. \nVice versa, if í›¿= f(xs)â€Š, the goal of the statement is to sup-\nport the decision. We call it a justifying statement.\nIn general, C(x) could involve comparisons of x with any \nnumber of other cases. For the sake of simplicity, we con-\nsider only two options here: conditions involving zero or one \nother case. The definitions can be easily generalized to any \nnumber of other cases4.\nThe first type of statements, called absolute statements, \ndoes not refer to any other case. Examples of absolute state-\nments appear in the first interaction step (U1) of Fig.Â 2 and in \nthe first line of TableÂ 1. The general form of condition C(x) \nfor absolute statements is the following:\n(1)\nâˆ€x âˆˆí€, C(x) âŸ¹f(x) = í›¿\n(2)\nC(x) = (x[i1]â™¢1v1) âˆ§â‹¯âˆ§(x[ik]â™¢kvk)\nwith ip âˆˆ{1, â€¦ , m}â€Š, â™¢p âˆˆ{=, â‰¤, <, â‰¥, >} for p in {1, â€¦ , k} \nand vp âˆˆí€í¢í©â€Š. The first line of TableÂ 1 provides an example \nof condition expressed in this syntax.\nIn contrast, relative statements involve a comparison with \nanother case xâ€²â€Š. The second user interaction (U2) of Fig.Â 2 \nand the second line of TableÂ 1 are examples of relative state-\nments. More formally, condition C(x) for relative statements \nhas the following form:\nwith for all p in {1, â€¦ , k}â€Š, ip âˆˆ{1, â€¦ , m}â€Š, \nâ™¢p âˆˆ{=, â‰¤, <, â‰ª, â‰¥, >, â‰«}â€Š. Operators â‰ª and â‰«â€Š, which \nstand for, respectively, â€œmuch less thanâ€ and â€œmuch greater \nthanâ€, are defined as follows: x â‰ªy â‡”x + k â‰¤y and \nx â‰«y â‡”x â‰¥y + k with k an application-dependent param-\neter. The statement corresponds to a situation in which \nanother case xâ€² is associated with a decision í›¿ and the rela-\ntion between the two cases would justify that the same deci-\nsion is made for x. The second line of TableÂ 1 provides an \nexample of condition expressed in this syntax.\n3.2â€‚ \u0007Norms\nAs discussed in the introduction, in order to challenge or to \njustify a decision it is necessary that a statement is backed by \nan applicable norm. We consider three types of norms here, \ncalled respectively rule-based, objective-based and refer-\nence-based norms, which correspond to three typical ways \nto support a challenge (or a justification). Interestingly, these \nthree modes also reflect the approaches followed by the three \nmain families of moral theories (respectively deontological \nethics, consequentialism and virtue ethics). Other types of \nnorms can be easily added to the framework, provided that \ntheir meaning is defined precisely, as done in this section, \nand their strength is characterized as done in Sect.Â 3.4.\nRule-based norms\nExamples of rule-based norms expressed in an informal \nway can be found in the first two lines of Fig.Â 1. As the name \nsuggests, a rule-based norm is defined by a fixed rule. For-\nmally speaking, these rules can be expressed as:\nwith Rule a flag defining the type of the norm and Def its \ncontent. Def is expressed in the same language as statements, \nthat is, definition (1) of Sect.Â 3.1 with C defined by Eq. (2) \nif the norm is expressed in terms of a single case (absolute \n(3)\nC(x) = âˆƒx\u001b âˆˆí€, f(x\u001b) = í›¿\nâˆ§\n(x\u001b[i1]â™¢1x[i1]) âˆ§â‹¯âˆ§(x\u001b[ik]â™¢kx[ik])\n[Rule, Def]\n4â€‚ We have not encounetred any practical situation in which such gen-\neralization would be useful, though.\n",
    "book_id": "a_framework_to_contest_and_justify_algorithmic_decisions",
    "book_title": "A framework to contest and justify algorithmic decisions",
    "book_author": "ClÃ©ment Henin ",
    "topic_id": "ai_policy",
    "topic_label": "policy",
    "chunk_index": 4
  },
  {
    "chunk_full": "468\n\t\nAI and Ethics (2021) 1:463â€“476\n1 3\nrule) or (3) if it involves a second case (relative rule). Lines \n3 and 4 of TableÂ 1 show respectively an example of absolute \nrule (business rule) and an example of relative rule.\nThis type of norm is common, inter alia, in law (regula-\ntions, directives, acts, contractual rules, etc.) and business \n(sectorial rules, corporate rules, procedural rules, etc.). In \nterms of moral theories, it is also the spirit of deontological \nethics5 which relies on moral obligations (such as â€œThou \nShalt Not Tell Liesâ€) that must be followed by any rational \nagent [33].\nObjective-based norms\nThe second way to define a norm consists of using meas-\nurable objectives that can be used to assess decisions. The \nthird line of Fig.Â 1 shows an informal example of objective-\nbased norm. In formal terms, an objective is expressed as an \nattribute that should be maximised6. In Fig.Â 1, this attribute \nis the number of non-default cases for Bank Objective.\nFormally speaking, objective-based norms can be \nexpressed as\nwhere Obj is a flag defining the type of the norm, í›¥O a data-\nbase containing the values í›¥O[ob] of the objective attribute \nob. Line 5 of TableÂ 1 shows an example of objective-based \nnorm.\nObjective-based norms are common in business and \norganizations in general. In terms of moral theories, they \ncan be related to consequentialism, which, as stated by Mark \nTimmons â€œexplains the deontic status of actions and other \nitems of moral evaluations entirely in terms of the values of \nthe consequences of actions and other items being morally \nevaluatedâ€. Even if we do not restrict the scope of norms \nto moral issues, as shown in the examples, objective-based \nnorms follow the same approach as consequentialism in the \nsense that, rather than relying on fixed rules (as rule-based \nnorms and deontological ethics), they focus on the assess-\nment of the impact of the decisions.\nReference-based norms\nThe third type of norm is illustrated by Bank Policy in \nFig.Â 1. This type of norm is applicable when reference data \ní›¥R about past decisions is available. The principle in this \ncase is that the decisions are justified if they are consist-\nent with past decisions. The implicit assumption is that the \nreference data is valid or legitimate, in the sense that it can \nbe used as a model for future decisions. This assumption \n[Obj, í›¥O, ob]\ncan actually be disputed when a decision is challenged, as \ndiscussed in Sect.Â 4.\nFormally speaking, reference-based norms can be \nexpressed as\nwhere Ref is a flag defining the type of the norm, í›¥R a data-\nbase containing the values í›¥R[d] of the decision attribute \nd. Line 6 of TableÂ 1 shows an example of reference-based \nnorm.\nThis type of norm is common in law (use of previous \ncases or jurisprudence). In terms of moral theories, they can \nbe related to virtue ethics, which is introduced as follows by \nMark Timmons: â€œWe often look to others as models for the \ntype of person we would like to be because we think they \npossess certain admirable character traits.â€ Indeed, we can \nsee the reference data as a model of â€œgood behaviourâ€ (here \na model for â€œgood decisionsâ€), the goal of the ADS being \nto mirror as closely as possible the behaviour of this model. \nWhen an ADS relies on supervised machine learning, the \nlearning data set can obviously be used as the reference data.\n3.3â€‚ \u0007Evidence\nIn an argument <statement, norm, evidence>, the evidence \ncomponent shows how the statement is supported by the \nnorm. Evidence can take different forms depending on the \ntype of the norm. Actually, the main difference is between:\nâ€“\t On the one hand, rule-based norms, which do not rely on \ndatabases: they either support or do not support a state-\nment (binary stance).\nâ€“\t On the other hand, objective-based norms and reference-\nbased norms, which rely on databases and can be sup-\nported by quantitative evidence. A statement can be more \nor less supported by such norms.\nThe case for rule-based norms is simple since these norms \nare expressed in the same language as statements. A \nstatement:\nis supported by a rule:\nif and only if:\nwhich can be derived using a simple inference system com-\nparing term by term the (x[ij]â™¢jvj) components of Cs and Cr.\nEvidence involving objective-based norms and refer-\nence-based norms relies on two numerical values called \n[Ref, í›¥R, d]\n(4)\nâˆ€x âˆˆí€, Cs(x) âŸ¹f(x) = í›¿s\n(5)\nâˆ€x âˆˆí€, Cr(x) âŸ¹f(x) = í›¿r\n(6)\ní›¿s = í›¿r âˆ§âˆ€x âˆˆí€, (Cs(x) âŸ¹Cr(x))\n5â€‚ The main representative of this school of thought is Kantâ€™s moral \ntheory.\n6â€‚ Of course, it is also possible to express minimisation by consider-\ning the opposite of the relevant attribute.\n",
    "book_id": "a_framework_to_contest_and_justify_algorithmic_decisions",
    "book_title": "A framework to contest and justify algorithmic decisions",
    "book_author": "ClÃ©ment Henin ",
    "topic_id": "ai_policy",
    "topic_label": "policy",
    "chunk_index": 5
  },
  {
    "chunk_full": "469\nAI and Ethics (2021) 1:463â€“476\t\n1 3\nrespectively the â€œcoverageâ€ and the â€œdeviationâ€. If the state-\nment is defined by:\nand í›¥ is the relevant dataset (either í›¥O or í›¥Râ€Š), we first \ndefine í›¥|Câ€Š, the subset of cases matching the condition of \nthe statement:\nThe coverage í›¾(í›¥, C) is defined as follows:\nwith card(S) the cardinal of a set S.\nFor objective feature ob, the deviation íœ‡(í›¥, C, í›¿) is \ndefined as follows:\nwith S the average of the values of the set S. The deviation \níœ‡ measures the difference of averages between the subset \ncharacterized by C and the whole population. The factor \n(2í›¿âˆ’1) is justified as follows. If the difference is positive \n(objective higher in í›¥|Câ€Š) then the evidence supports state-\nments such that í›¿= 1 and negates statements such that \ní›¿= 0â€Š. Vice versa, if the difference is negative (objective \nlower in í›¥|Câ€Š) then the evidence supports statements í›¿= 0 \nand negates statements with í›¿= 1â€Š. For the sake of read-\nability, it is preferable to show to the user both the subset \naverage í›¥|C[ob] and the population average í›¥[ob] as done in \nTableÂ 1. The deviation for reference-based norms is defined \nin the same way.\nThe full definition of evidence is the following:\nwith í›¥ the relevant dataset (â€Ší›¥O or í›¥Râ€Š). The last two lines of \nTableÂ 1 show examples of evidence for an objective-based \nnorm and a reference-based norm. As suggested above, \ndata-based evidence can be more or less supporting. For \ninstance, in the last line of TableÂ 1, the statement character-\nizes a subset of size 500 with an average number of accepted \napplications 28 % higher than the whole population (95 % \nâˆ’67% = 28%â€Š). In this case, the argument is strong because \nboth the coverage (500) and the deviation (0.28) are high, \nbut it is not the case for the penultimate line of TableÂ 1 in \nwhich the deviation is only 0.06. We discuss further the \nnotion of strength in the next section.\nWe consider only well-formed arguments here, that is, \narguments < S, N, E > such that E supports, to some extent, \nS. More precisely, if N is a rule-based norm, then argument \n< S, N, E > is well-formed only if Condition (6) is met. If N \nis a data-based norm, it is well-formed only if í›¾(í›¥, C) â‰ 0 \nand ğœ‡(ğ›¥, C, ğ›¿) > 0.\n(7)\nâˆ€x âˆˆí€, C(x) âŸ¹f(x) = í›¿\n(8)\ní›¥|C = {x âˆˆí›¥|C(x)}\n(9)\ní›¾(í›¥, C) = card(í›¥|C)\n(10)\níœ‡(í›¥, C, í›¿) = (2í›¿âˆ’1)(í›¥|C[ob] âˆ’í›¥[ob])\n(11)\n[í›¥, í›¾(í›¥, C), íœ‡(í›¥, C, í›¿)]\n3.4â€‚ \u0007Strength relation\nWhen two parties disagree about a decision and each party \nprovides an argument to support his/her position, it is impor-\ntant to be able to compare these arguments. To this aim, \nwe define a preorder relation7 â©¾a between arguments. This \nrelation (hereafter â€œstrength relationâ€) is defined in terms of \nthe components of the arguments:\nIn other terms, an argument A is stronger than an argument \nAâ€² in two cases:\nâ€“\t A relies on a stronger norm than Aâ€² or\nâ€“\t A and Aâ€² rely on the same norm but Aâ€™s evidence is \nstronger than Aâ€²â€Šâ€™s evidence.\nThis generic definition can be complemented, on a case by \ncase basis, depending on the context.8\nThe strength relation â©¾n between norms is essentially \ndomain-dependent and it has to be defined for each applica-\ntion. As an illustration, in the example of Fig.Â 1, the bank \nhas specified an explicit strength relation between two norms \n(Bank Objective â©¾n Bank Policy). In addition, the first norm, \nwhich is a fundamental right, is stronger than all the oth-\ners. It should be clear that the strength relation is partial: \nthere are situations in which two applicable norms are not \ncomparable. We come back to this issue in the next section.\nAs far as â©¾e is concerned, it is only relevant for data-based \nevidence, since rule-based evidence is not quantitative. A \nsimple way to define it for objective-based norms could be \nthe following:\nLarge coverages lead to stronger arguments because they \nimprove the statistical significance of the evidence. The \nstrength of the evidence also grows with íœ‡ because higher \nvalues of íœ‡ correspond to more supportive objective values \nor reference decisions. This definition of â©¾e is intuitive but \nit is conservative in the sense that it leads to a great number \nof incomparable pieces of evidence. A more sophisticated \ndefinition is proposed in the next section.\n(12)\n[S, N, E] â©¾a [S\u001d, N\u001d, E\u001d]\nâ‡”(N â©¾n N\u001d) âˆ¨((N = N\u001d) âˆ§(E â©¾e E\u001d))\n(13)\n[í›¥, í›¾, íœ‡] â©¾e [í›¥, í›¾\u001d, íœ‡\u001d] â‡”(í›¾â‰¥í›¾\u001d âˆ§íœ‡â‰¥íœ‡\u001d)\n7â€‚ Binary relation that is reflexive and transitive.\n8â€‚ See Sect.Â 4 for an example.\n",
    "book_id": "a_framework_to_contest_and_justify_algorithmic_decisions",
    "book_title": "A framework to contest and justify algorithmic decisions",
    "book_author": "ClÃ©ment Henin ",
    "topic_id": "ai_policy",
    "topic_label": "policy",
    "chunk_index": 6
  },
  {
    "chunk_full": "470\n\t\nAI and Ethics (2021) 1:463â€“476\n1 3\n4â€‚ \u0007Algocate: aÂ challenge andÂ justification \nsystem\nThe framework introduced in the previous section can be \ninstantiated in different ways to build a challenge and justi-\nfication system. In this section, we sketch the main choices \nmade in the design of Algocate, our proof-of-concept imple-\nmentation. We first describe in Sect.Â 4.1 the main steps of \nthe interaction protocol, which shows the functionalities \nprovided by the system. Then, we define more precisely the \nstrength relation used in Algocate in Sect.Â 4.2 and its use for \nthe generation of statements in Sect.Â 4.3 .\n4.1â€‚ \u0007The Algocate protocol\nAn Algocate session is always associated with a decision, \nwhich is called the initial decision in the sequel. The user \ncan be any stakeholder or party concerned by the ADS \n(designer, operator, human decision maker, person affected \nby the decision, auditor, etc.) and his/her motivation can be \nvaried, for example to find arguments to support the initial \ndecision, to contest it or to enhance his/her trust9. We also \nassume that, in case of disagreement, final decisions are \nalways taken by a human agent (called the arbitrator in this \npaper). Last but not least, we assume that a set of norms and \nevidence have been supplied to Algocate by the stakeholders \nand/or by independent third parties (e.g. regulation bodies). \nHowever, no assumption is made neither about the compre-\nhensiveness of this initial set of information nor about the \nfact that all users will necessarily accept these norms. As \nshown below, this set of information may evolve, in particu-\nlar to take into account the verdicts of the arbitrator. \n1.\t The interaction with Algocate starts with an initial state-\nment (called the user statement) by a user. If the user has \na specific norm in mind to support his/her statement, he/\nshe can also provide it, but this information is not man-\ndatory. In the following, we consider the most common \n(and most complex) situation in which the only input to \nthe system is a user statement.\n2.\t Algocate analyses the user statement and searches for \nappropriate norms and evidence to generate the strong-\nest well-formed argument(s) supporting this statement. \nSince the strength relation is not total, some arguments \nmay not be comparable. In such cases, Algocate returns \nseveral arguments. The strength relation used by Algo-\ncate is defined in Sect.Â 4.2.\n3.\t The next step for Algocate is to try to reinforce the user \nstatement to find stronger arguments. At this stage, \nAlgocate adopts a neutral position and considers both \narguments supporting the initial decision and argu-\nments challenging it. Algocate returns the strongest \nwell-formed arguments based on these new statements \n(called generated statements). The generation of state-\nments is described more precisely in Sect.Â 4.3.\nSeveral options are possible for the user at this stage. The \nmost interesting situation is the case of the user whose goal \nis to challenge the initial decision:\nâ€“\t If Algocate has generated strong evidence supporting the \nstatement of the user (in Step (2) or Step (3)), then he/\nshe can provide this evidence to the arbitrator to request \na revision of the initial decision.\nâ€“\t If Algocate has generated stronger evidence against the \nstatement of the user (in Step (3)), then the user can \neither be convinced that the initial decision is legitimate \nor not. In the first case, the protocol stops since the disa-\ngreement has been solved. In the second case, the first \noption for the user is to try to challenge the decision on \na different basis (with a new statement), trigerring a new \niteration of the protocol. The second option, if he/she \nbelieves that the argument generated by Algocate is not \nlegitimate, is to submit it to the arbitrator. This can be \nthe case, for instance, if the argument relies on a norm \nwhich is not accepted by the user. Typical examples \ncan be biased reference data or norms corresponding to \ncorporate rules that are not known by customers. If the \narbitrator confirms that the norm is not acceptable, the \nimpact of his/her verdict goes beyond the decision chal-\nlenged by the user: the operator of the ADS must modify \nthe system to correct the situation (and the set of norms \nused by Algocate should be updated accordingly).\n4.2â€‚ \u0007Strength relation\nThe strength relation used to compare arguments should \nmeet two criteria: it should reflect the intuition of the par-\nties (and the intuition of the arbitrator) and it should make \nit possible to compare all (or most) arguments. These two \ncriteria can be in tension, as shown in Sect.Â 3.4 which intro-\nduces a simple and intuitive relation that leaves many argu-\nments incomparable. To solve this tension, Algocate relies \non a score function t measuring the strength of data-based \nevidence such that:\nIntuitively, evidence is strong if the deviation is high, mean-\ning that condition C has a strong impact on the average. \n(14)\n[í›¥, í›¾, íœ‡] â©¾e [í›¥, í›¾\u001d, íœ‡\u001d] â‡”t(í›¥, í›¾, íœ‡) â‰¥t(í›¥, í›¾\u001d, íœ‡\u001d)\n(15)\nt(í›¥, í›¾, íœ‡) = íœ‡\nâˆš\ní›¾\n9â€‚ For the avoidance of doubt, Algocate users may challenge or justify \ndecisions of an ADS, not the outcomes of Algocate itself.\n",
    "book_id": "a_framework_to_contest_and_justify_algorithmic_decisions",
    "book_title": "A framework to contest and justify algorithmic decisions",
    "book_author": "ClÃ©ment Henin ",
    "topic_id": "ai_policy",
    "topic_label": "policy",
    "chunk_index": 7
  },
  {
    "chunk_full": "471\nAI and Ethics (2021) 1:463â€“476\t\n1 3\nHowever, if the size of í›¥|C is too small, the deviation could \nbe high only by chance. For instance, evidence involving a \nset of only two reference cases should be considered weaker \nthan evidence relying on a subset of one hundred reference \ncases (if their deviations are close). To take this factor into \naccount, it is a common practice to compare the average \nvalue of í›¥|C[o] with the expected average value of a random \ndrawing of the same size. From the law of large numbers, \nwe know that the expected standard deviation of this ran-\ndomly drawn subset is proportional to 1âˆ•âˆší›¾â€Š. Definition (15) \namounts to the well-known studentâ€™s t test10 and it can be \nconverted into a p value11. Even though Definition (15) is \nrather intuitive and has good statistical properties, it is pos-\nsible to opt for different versions of strengh in Algocate (e.g. \nrelying on Shannon entropy). We do not discuss them further \nin this paper for the sake of conciseness.\n4.3â€‚ \u0007Generation ofÂ statements\nFor a neutral challenge and justification system, it is not suf-\nficient to generate evidence to support the statement issued \nby the user. As shown in Fig.Â 2, the fact that this statement \nis supported by some evidence does not mean that a stronger \nargument cannot be found to support either the same posi-\ntion or the opposite.\nThe objective of Algocate statement generation proce-\ndure is illustrated by Fig.Â 3. The left side of the figure (a) \nshows the historical data set í›¥ with the initial decision in \nred and the statement of the user (stating that the decisions \nin the orange hatched area, which represents í›¥|Câ€Š, should \nbe 1). This statement is weakly supported by the dataset. \nThe middle part of figure (b) shows a stronger argument \nsupporting the same conclusion (with an additional condi-\ntion represented by the horizontal bar). However, the right \nside of figure (c) shows an even stronger argument against \nthe statement of the user (represented by the two vertical \nbars). This is indeed the strongest argument according to the \nstrength relation â‰¥e defined in the previous section, which is \nconsistent with the intuition since the size of the selected set \nis approximately similar but the ratio of negative decisions \nis much higher than the ratio of positive decisions in (b).\nMore precisely, the goal of the search procedure is to find \nconditions Câˆ— and conclusion í›¿ (0 or 1) such that: \n1.\t the generated statement includes the initial decision xsâ€Š: \nCâˆ—(xs) is true,\n2.\t the generated statement strengthens the initial statement: \nâˆ€x, Câˆ—(x) âŸ¹C(x),\n3.\t the evidence supporting this statement is maximal: \nt(í›¥, í›¾âˆ—, íœ‡âˆ—) is maximized.\nwith í›¾âˆ—= í›¾(í›¥, Câˆ—) and íœ‡âˆ—= íœ‡(í›¥, Câˆ—, í›¿).\nComing back to Fig.Â 3, we can see that all hatched areas \ninclude the initial decision (red point) and strengthen the \ninitial statement (the vertical line in (a) is reused in (b) and \n(c)). Also, we see intuitively that a strong statement should \ncover as many corroborating data points as possible (many \norange points in Fig.Â 3b and purple points in Fig.Â 3c).\nThe benefit of generating statements that strengthen \nthe argument of the user is to ensure that the answers of \nthe system take into account the concern of the user. This \ncustomization makes the interaction more constructive and \ninsightful. Of course, it is always possible, for any party, to \n(a)\n(b)\n(c)\nFig.â€¯3â€‚ â€‰Visual representation of the generation of absolute statements \nfor reference-based norms. Plots represent the reference (training) \ndata. The two classes are represented in orange and purple. (a) The \ninitial statement of the user (orange vertical line) states that all deci-\nsions in the orange hatched area should be 1. It is not strongly sup-\nported by the training data (the orange hatched area contains many \npurple points). (b) Optimal statement supporting the position of the \nuser : it states that all decisions in the orange hatched area should be \n1. Strong evidence supports this statement as the ratio orange / purple \nin the hatched area is high. (c) Optimal statement against the posi-\ntion of the user: it states that all decisions in the purple hatched area \nshould be 0. Strong evidence supports this statement as the ratio pur-\nple/orange is 1\n10â€‚ Up to a constant involving the standard deviation of í›¥[o].\n11â€‚ The p-value can be interpreted as the probability that the observed \ndeviation is coincidental\n",
    "book_id": "a_framework_to_contest_and_justify_algorithmic_decisions",
    "book_title": "A framework to contest and justify algorithmic decisions",
    "book_author": "ClÃ©ment Henin ",
    "topic_id": "ai_policy",
    "topic_label": "policy",
    "chunk_index": 8
  },
  {
    "chunk_full": "472\n\t\nAI and Ethics (2021) 1:463â€“476\n1 3\nstart a new interaction on a completely different basis (with \na different statement), as illustrated by U2 in Fig.Â 2. Further \nexamples of statement generation are presented in Sect.Â 5.\nTechnically speaking, the search procedure considers all \npossible norms in decreasing strength order. Each step takes \nas inputs (1) the initial decision xsâ€Š, (2) the user statement \nwhich contains two pieces of information (C and í›¿â€Š) and (3) \nthe database í›¥ corresponding to the norm. For absolute state-\nments, it outputs a set of triplets {(aâˆ—\ni , â™¢âˆ—\ni , vâˆ—\ni ), i = 1, ..., K}\n12. The output statement is obtained by concatenating these \ntriplets to the initial statement:\nWith this definition of Câˆ—â€Š, the coverage of the generated \nstatement is í›¾(í›¥, Câˆ—) (the number of points in the hatched \nareas of Fig.Â 3b, c). The goal of the search procedure is \nto find the set of triplets resulting in the greatest value of \nt(í›¥, í›¾, íœ‡)â€Š. The global search objective can be written:\nAs there is no analytical solution to this maximization prob-\nlem, to the best of our knowledge, it is implemented by an \nexhaustive search strategy. Furthermore, as the complexity \nof the problem is exponential in K, a greedy search algo-\nrithm is used to find an approximate solution in a reason-\nable time. At each step, all possible triplets (a\u001e\ni, â™¢\u001e\ni, v\u001e\ni) such \nthat xs[a\u001e\ni]â™¢\u001e\niv\u001e\ni are considered and the triplet leading to the \ngreatest value of t(í›¥, í›¾\u001e, íœ‡\u001e) is selected. The iteration stops \nwhen the p-value associated with the t test of the best triplet \nis above a given threshold.13\n(16)\nCâˆ—= C âˆ§aâˆ—\n1â™¢âˆ—\n1vâˆ—\n1 âˆ§â‹¯âˆ§aâˆ—\nKâ™¢âˆ—\nKvâˆ—\nK\n(17)\nmax\n(a\u001e\ni,â™¢\u001e\ni,v\u001e\ni),i=1...K t(í›¥, í›¾\u001e, íœ‡\u001e)\n5â€‚ \u0007Algocate atÂ work\nIn this section, we provide some examples of use of our \nproof-of-concept implementation of the Algocate system to \nillustrate the benefits of the framework and the feasibility \nof the approach.\nWe use as a case study a publicly available dataset: the \nGerman credit dataset14 called í›¥G in the sequel. It contains \ninformation about credit applications (credit amount, savings \nstatus, etc.) and the conclusions of the bank experts (low risk \nor high risk). TableÂ  2 shows the values of attributes used in \nthe examples presented in this section. We trained a random \nforest classifier on í›¥G to build an ADS predicting the conclu-\nsions of the bank experts.\nWe use three norms N1â€Š, N2 and N3 for this case study, with \nN1 â‰¥n N2 â‰¥n N3:\nâ€“\t N1 (rule-based norm):\n\t\nâ€ƒâˆ€x âˆˆA, x[savings_status] = â€œno savingsâ€ âŸ¹f(x) = 0\nâ€“\t N2 (reference-based norm): í›¥G should be used as a refer-\nence dataset with decision the reference attribute.\nâ€“\t N3 (objective-based norm): í›¥G should be used as a objec-\ntive dataset with the opposite of the duration attribute \nused as the objective. The strategy of the bank is to \nfavour credits with short durations to reduce the risk of \ndefaults and to foster short term profits.\nThe value of an objective attribute cannot be known at the \ntime of the decision as it should be a consequence of the \ndecision used to assess its impact. Therefore, for the sake of \nthis example, we assumed that the duration attribute was not \nknown at the time of the decision (although it is included in \nTableâ€¯2â€‚ â€‰Attribute values of \nthe cases used in the examples. \nThe checking status and \nsaving status attributes refer \nrespectively to the status of the \nchecking and saving accounts. \nThe install commitment \nattribute is a categorization \n(from 1 to 4) of the ratio of \nthe repayment instalments and \ndisposable income (higher \nvalues being associated with \nhigher ratios). The values of the \ndecision attribute are 0 (for high \nrisk) and 1 (for low risk)\nExample 1 (5.1)\ncase A\nExample 1 (5.1\n case Aâ€²\nExample 2 (5.2)\ncase A\nchecking_status\n<0\n<0\n0<=X<200\nresidence_since\n4\n1\n2\nnumber_dependent\n1\n2\n1\ncredit_amount\n2578\n7629\n2762\nage\n55\n46\n25\ninstall_commitment\n3\n4\n1\nsavings_status\n<100\n100<=X<500\nno savings\ndecision\n0\n1\n0\n14â€‚ https://â€‹archiâ€‹ve.â€‹ics.â€‹uci.â€‹edu/â€‹ml/â€‹datasâ€‹ets/â€‹statlâ€‹og+â€‹(german+â€‹credit+â€‹\ndata).\n12â€‚ Set of pairs {(aâˆ—\ni , â™¢âˆ—\ni ), i = 1, ..., K} for relative statements. We do \nnot discuss further relative statements here, as they are implemented \nin a very similar way.\n13â€‚ The current implementation of Algocate uses the value 0.2, but \nthis value is a parameter that can easily be ajusted.\n",
    "book_id": "a_framework_to_contest_and_justify_algorithmic_decisions",
    "book_title": "A framework to contest and justify algorithmic decisions",
    "book_author": "ClÃ©ment Henin ",
    "topic_id": "ai_policy",
    "topic_label": "policy",
    "chunk_index": 9
  },
  {
    "chunk_full": "473\nAI and Ethics (2021) 1:463â€“476\t\n1 3\nthe German credit data set) to use it as an objective attribute. \nIn this hypothetical credit decision system, the credit dura-\ntion is supposed to be flexible and the borrower can adapt \nthe payments to his/her reimbursement capacities.\nOperators â‰ª and â‰« (â€œmuch less thanâ€ and â€œmuch greater \nthanâ€, as introduced in Sect.Â 3.1) are defined with k equal to \ntwice the estimated standard deviation of the corresponding \nattribute. The interactions shown in Sects. 5.1 andÂ 5.2 were \ngenerated using our PoC implementation of Algocate. As in \nFigureÂ 2, the statements of the user, which are expressed in a \nrestricted natural language15, are labelled with Ui and Algo-\ncate answers are labelled with Ai. The computation time on \na laptop was of the order of few seconds for each example.\n5.1â€‚ \u0007Example 1\nThis first example illustrates the ability of Algocate to gen-\nerate â€œoptimisedâ€ statements. Here, the user is a customer \nwhose credit application file A has been rejected and who \nwants to challenge the decision. The user happens to know \nanother application Aâ€² that has been accepted and uses it in \nhis/her statement (which is therefore a relative statement). \nThe values of the features corresponding to user A and user \nAâ€² are presented in TableÂ 2. For the sake of this first example, \nwe consider only reference-based decisions. The interactions \nshown in Fig.Â 4 follow the three steps of the protocol pre-\nsented in Sect.Â 4.1: statement of the user (U1), argument \ngenerated by Algocate for this statement (A1) and a stronger \nargument (based on a new statement generated by Algocate) \nsupporting the same position. At the end of the interaction, \nthe customer is in a better position to challenge the decision \n(for example with the bank or in court).\n5.2â€‚ \u0007Example 2\nThe second example shows how Algocate can be used by \ndifferent parties to challenge or to support a given decision. \nHere, the first party is a customer whose application has \nbeen rejected and who disagrees with the decision and the \nsecond party is a banking agent who is asked to justify the \ndecision.\nThe customer issues a challenging statement and obtains \nan argument with strong objective-based evidence (Fig.Â 5). \nLet us assume that the customer decides to contest the deci-\nsion with this argument in front of the bank. To support \nthe decision of the system, the agent issues his/her own \nFig.â€¯4â€‚ â€‰Example of interaction \nwith Algocate for a rejected \ncredit application A wishing to \nchallenge the model relatively \nto another file Aâ€²â€Š. Evidence \nshown in A1 is weak (only 66% \nof accepted reference deci-\nsions). Evidence shown in A2 \nis stronger (88% of accepted \nreference decisions)\nU1 Aâ€² has been accepted. Aâ€² has a checking status value equal to A and a res-\nidence since value much lower than A. Therefore, A should be accepted as\nwell.\nA1 Among the decisions used as reference by the credit decision system, there\nwere 413 reference decisions with the same checking status value, and a much\ngreater residence since value than an accepted case and 66 % of them were\naccepted (population average 70 %).\nA2 Among the decisions used as reference by the credit decision system, there were\n157 reference decisions with the same checking status value, a much greater\nresidence since value, a much lower number dependent value and a greater age\nvalue than an accepted case and 88 % were accepted (population average 70\n%) (strongest generated argument).\nFig.â€¯5â€‚ â€‰Interaction initiated by \na customer challenging the \ninitial decision. In A1 and A2, \nevidence supports the user state-\nment as the final duration of the \ncredit is below the population \naverage, which corresponds to \nobjective N3\nU1 A credit amount value is lower than 4000 and age is greater than 25, so A\nshould be accepted.\nA1 Among the decisions used as reference by the credit decision system, there were\n602 decisions with credit amount value lower than 4000, and age value greater\nthan 25. The average credit duration among them is 17 months (population\naverage 20 months).\nA2 Among the decisions used as reference by the credit decision system, there were\n116 decisions with credit amount value lower than 2764, age value greater\nthan 25, and install commitment value lower than 2. The average duration\namong them is 12 months (population average 20 months) (strongest generated\nargument).\n15â€‚ Transcript of the abstract syntax used in Algocate.\n",
    "book_id": "a_framework_to_contest_and_justify_algorithmic_decisions",
    "book_title": "A framework to contest and justify algorithmic decisions",
    "book_author": "ClÃ©ment Henin ",
    "topic_id": "ai_policy",
    "topic_label": "policy",
    "chunk_index": 10
  },
  {
    "chunk_full": "474\n\t\nAI and Ethics (2021) 1:463â€“476\n1 3\nstatement (Fig.Â 6). Algocate finds rule-based evidence to \nsupport this statement. Assuming that this argument is \nstronger than the customerâ€™s argument generated in Fig.Â 5, \nthe customer can either accept the decision (for example, if \nhe/she realizes that N1 is imposed by law) or decide to chal-\nlenge it (for example, if N1 is an internal rule of the bank that \ndoes not reflect common practice). If the verdict of the arbi-\ntrator is in favour of the customer, the operator of the ADS \nshould modify in to take this decision into account and the \nnorm database of Algocate should be updated accordingly.\n6â€‚ \u0007Related work\nIn the field of explainable AI, the distinction between \nexplanations and justifications is sometimes blurred. In \nsome papers [20], justifications are seen as ways to make \nunderstandable the inner operations of a complex system, \nwhich would be called white-box explanations according to \nour definitions. Similarly, [8, 9] make a distinction between \nâ€œhow did the system arrive at the predictionâ€ and â€œwhy \nshould we believe the prediction is correctâ€ but they do not \nrefer to any external norm, and therefore provide explana-\ntions (based on narrative roles explaining the effect of the \nfeatures of the input data used by the system) rather than \njustifications in our sense. Another distinction is made by \nKlass and Finin [16] based on the intention, which should be \nto â€œproduce knowledge in the hearerâ€ for explanations and \nâ€œto affect the beliefs of the hearerâ€ for justifications. From a \ndifferent perspective, [10], introduces a classification of jus-\ntifications in machine learning related to the performances \n(accuracy) of the systems.\nA series of works [7, 24, 28] refer to justifications as ways \nof ensuring that a decision is good (in contrast to under-\nstanding a decision), which is in line with the approach fol-\nlowed in this paper and definitions of explanations and jus-\ntifications in philosophy [3]. Regarding the extrinsic nature \nof explanation, [31] distinguishes between justifications and \nexplanations based on the origin of the information they \nrefer to: explanations describe how the system works while \njustifications use domain knowledge to show that decisions \nare correct. The normative nature of justifications has also \nbeen pointed out in the field of intelligent systems [18]: â€œan \nintelligent system exhibits justified agency if it follows soci-\netyâ€™s norms and explains its activities in those termsâ€. In \n[19], the authors qualify explanations as â€œunjustifiedâ€ when \nthere are not supported by training data, which is related \nto our notion of justification with reference-based norms \n(Sect.Â 3.2). However, in this context justifiability applies to \nexplanations rather than to the decisions themselves.\nThe term â€œjustificationâ€ is sometimes used in the field \nof autonomous agents to refer to motivations to perform a \nspecific action. The main difference with our approach is \nthat norms are made available to autonomous agents to make \ntheir decisions. For example, the framework introduced in \n[21] relies on norms (called â€œprinciplesâ€) inferred from \nethical judgments using inductive logic programming. In a \nnutshell, a value-driven agent refers to an ethical preference \nordering of the actions before making any decision. Justifica-\ntions of the decisions can be built using assumption-based \nargumentation with deductive rules representing the accept-\nance of different ethical consequences. It is worth noting \nthat it is appropriate to present justifications as a type of \nexplanation in this context since norms are internalised. Our \napproach is complementary to this trend of work in the sense \nthat it is not limited to norms expressed in terms of rules and \nsituations in which all choices can be automated.\nFinally, closer to our approach, [22] criticizes â€œanorma-\ntiveâ€ explanations and calls for the explicit definition of \nalgorithm â€œgoalsâ€ that should be understandable. These \ngoals being defined, a decision is justified when the â€œevi-\ndenceâ€ that it meets the goals can be provided. However, \nunlike the framework proposed in this paper, goals must be \ndefined in advance and included in the design objectives of \nthe system. Also the possibility to contest a decision based \non these goals is not mentioned. In the same spirit, but at \nanother level, [32] uses a refinement structure to provide \njustifications about the decisions made during the design \nof the system.\nThe need to design systems that are contestable has been \npointed out by several authors recently. For instance [2] calls \nfor systems that are â€œcontestable by designâ€. In [30], the \nauthors show the importance of being able to challenge algo-\nrithmic decisions or recommendation systems in the field of \nmedicine.\nThe interest for more interactive machine learning sys-\ntems manifests in a variety of ways [1]. The need to conceive \nexplanations as an interactive process has been argued by \nseveral authors [25, 27]. The â€œhuman-in-the-loopâ€ approach \nleverages on human feedback during the training process to \nobtain more accurate classifiers [17]. A lot of work has also \nbeen done on argumentation and dialog games [4, 5, 23] \nbut the focus in these areas is generally the logical struc-\nture of the framework to express and to relate arguments \nor the protocol to exchange arguments. In contrast, we take \nFig.â€¯6â€‚ â€‰Interaction initiated by \nthe banking agent to justify the \ndecision\nU1 A savings status value is equal to â€œno savingsâ€, so A should be rejected.\nA1 This statement is supported by the rule-based norm N1.\n",
    "book_id": "a_framework_to_contest_and_justify_algorithmic_decisions",
    "book_title": "A framework to contest and justify algorithmic decisions",
    "book_author": "ClÃ©ment Henin ",
    "topic_id": "ai_policy",
    "topic_label": "policy",
    "chunk_index": 11
  },
  {
    "chunk_full": "475\nAI and Ethics (2021) 1:463â€“476\t\n1 3\nan empirical approach to assess challenges and justifica-\ntions and we consider a very basic protocol in this paper \n(Sect.Â 4.1).\nMore closely related to our work, [15] relies on â€œdebatesâ€ \nbetween two competing algorithms exchanging arguments \nand counterarguments to convince a human user that their \nclassification is correct. However, the goal of this work is \nto â€œalign an agentâ€™s actions with the values and preferences \nof humansâ€, which is seen as a â€œtraining-time problemâ€. \nOur objective in this paper was different but an interesting \navenue for further research could be the application of our \napproach to design or to improve an ADS.\nTo the best of our knowledge, none of these contributions \nhas led to the proposal of a framework or a tool to challenge \nand to justify decisions comparable to the work presented \nin this paper.\n7â€‚ \u0007Conclusion\nThe ultimate goal of the work presented in this paper is to \nimprove the integration of algorithmic decision tools in the \noverall decision making process and ensure that they can be \nused for the benefit of people relying on them or affected by \ntheir decisions. As stated in Sect.Â 2, Algocate is neutral, in \nthe sense that it is designed to find the best arguments for \nboth parties. Ideally, this type of system should be certified \nand made available to all parties. We also emphasize that \nthe framework presented here is not intended to replace a \nhuman decision maker but to put him/her in the best posi-\ntion to make a decision. It can also be useful to better justify \ndecisions and to reconcile the viewpoints of the parties.\nA system like Algocate is also a contribution to account-\nability, which has been pointed out as a key and complex \nissue for ADS [6, 11]. For example, [6] states that â€œanyone \ndeploying algorithmic models inevitably also engages a set \nof normative principles (at least implicitly)â€ and â€œaccount-\nability involves the provision of reasons, explanations and \njustifications and this ought to involve drawing out these \nimplicit epistemic and normative standardsâ€. As such, Algo-\ncate could complement proposals such as model cards [26] \n(which can be used to provide information about a machine-\nlearning model, such as its intended use, metrics to assess \nit potential impacts, training data, evaluation data, quantita-\ntive analyses, etc.) and datasheets for datasets [26] (which \ncan be used to describe the characteristics of a dataset) in \norder to provide an interactive and easily accessible form of \naccountability.\nWe believe that this kind of system is most welcome \ninÂ situations in which the stakes are high (e.g. in the health-\ncare or justice sectors) and the requirements of the ADS are \nnot entirely formalized or cannot be guaranteed by design. \nThis may be the case for various reasons, for example when \nrequirements involve ethical aspects, or legal aspects leaving \nroom for interpretation, or evolving constraints (e.g. pro-\ncedures or regulations), or technical aspects which are not \nprone to formal definitions (e.g. image or text analysis). As \nan illustration, the legal constraints applicable to a given \nsystem are rarely formalized before its development16. A \nsystem like Algocate makes it possible to provide, indepen-\ndently from the design of the ADS and incrementally, the \nmaterial (norms and evidence) useful to justify decisions. \nWe hold that this contribution is a first step to address the \ncall expressed in [12] â€œfor tangible action to move from \nhigh-level abstractions and conceptual arguments towards \napplying ethics in practice and creating accountability \nmechanisms.â€\nAs far as further work is concerned, Algocate should be \ntested through randomized user studies involving different \ntypes of users in order to prove its usability in real life. We \nplan to carry out these experiments in the near future in col-\nlaboration with partners in the public sector (tax authorities) \nand the private sector (insurance companies).\nAs stated in the introduction, our framework works in a \nblack-box mode, in the sense that no assumption is made \nabout the internals of the ADS. As a result, it may also be \nuseful to justify or contest decisions made by human beings \n(without the help of an ADS). Further work is needed to \nassess the relevance of Algocate in this context.\nAuthor contributionsâ€‚ The authors contributed to the manuscript \nequally.\nFundingâ€‚ The research was supported by Inria and Ã‰cole des Ponts \nParis- Tech.\nDeclarationsâ€‚\nConflict of interestâ€‚ The authors declare that they have no competing \ninterests.\nReferences\n\t 1.\t Abdul, A., Vermeulen, J., Wang, D., Lim, B.Y., Kankanhalli, M.: \nTrends and trajectories for explainable, accountable and intelligi-\nble systems: an HCI research agenda. In: Proceedings of the 2018 \nCHI Conference on Human Factors in Computing Systems - CHI \nâ€™18, p. 1â€“18. ACM Press (2018). https://â€‹doi.â€‹org/â€‹10.â€‹1145/â€‹31735â€‹\n74.â€‹31741â€‹56\n\t 2.\t Almada, M.: Human intervention in automated decision-making: \ntoward the construction of contestable systems. In: Proceedings of \nthe Seventeenth International Conference on Artificial Intelligence \nand Law - ICAIL â€™19, p. 2â€“11. ACM Press (2019). https://â€‹doi.â€‹org/â€‹\n10.â€‹1145/â€‹33226â€‹40.â€‹33266â€‹99\n16â€‚ In any case, regulations cannot be entirely formalized.\n",
    "book_id": "a_framework_to_contest_and_justify_algorithmic_decisions",
    "book_title": "A framework to contest and justify algorithmic decisions",
    "book_author": "ClÃ©ment Henin ",
    "topic_id": "ai_policy",
    "topic_label": "policy",
    "chunk_index": 12
  },
  {
    "chunk_full": "476\n\t\nAI and Ethics (2021) 1:463â€“476\n1 3\n\t 3.\t Alvarez, M.: Reasons for action: justification, motivation, expla-\nnation. In: Zalta, E.N. (ed.) The Stanford encyclopedia of phi-\nlosophy, winter, 2017th edn. Stanford University, Metaphysics \nResearch Lab (2017)\n\t 4.\t Atkinson, K., Baroni, P., Giacomin, M., Hunter, A., Prakken, H., \nReed, C., Simari, G., Thimm, M., Villata, S.: Towards artificial \nargumentation. AI Mag. 38(3), 25â€“36 (2017). https://â€‹doi.â€‹org/â€‹10.â€‹\n1609/â€‹aimag.â€‹v38i3.â€‹2704\n\t 5.\t Bex, F., Walton, D.: Combining explanation and argumentation \nin dialogue. Argum. Comput. 7(1), 55â€“68 (2011)\n\t 6.\t Binns, R.: Algorithmic accountability and public reason. \nPhilos. Technol. 31, 543â€“556 (2018). https://â€‹doi.â€‹org/â€‹10.â€‹1007/â€‹\ns13347-â€‹017-â€‹0263-5\n\t 7.\t Biran, O., Cotton, C.: Explanation and justification in machine \nlearning: a survey. In: IJCAI-17 workshop on explainable AI \n(XAI), vol.Â 8, pp. 8â€“13 (2017)\n\t 8.\t Biran, O., McKeown, K.: Justification narratives for individual \nclassifications. Proc. AutoML Works. ICML 2014, 1â€“7 (2014)\n\t 9.\t Biran, O., McKeown, K.R.: Human-centric justification of \nmachine learning predictions. In: IJCAI, p. 1461â€“1467 (2017)\n\t10.\t Corfield, D.: Varieties of justification in machine learning. \nMinds Mach. 20(2), 291â€“301 (2010). https://â€‹doi.â€‹org/â€‹10.â€‹1007/â€‹\ns11023-â€‹010-â€‹9191-1\n\t11.\t Diakopoulos, N.: Accountability in algorithmic decision making. \nCommun. ACM 59(2), 56â€“62 (2016). https://â€‹doi.â€‹org/â€‹10.â€‹1145/â€‹\n28441â€‹10\n\t12.\t Hickok, M.: Lessons learned from AI ethics principles for \nfuture actions. AI and Ethics. (2020)https://â€‹doi.â€‹org/â€‹10.â€‹1007/â€‹\ns43681-â€‹020-â€‹00008-1\n\t13.\t Hirsch, T., Merced, K., Narayanan, S., Imel, Z.E., Atkins, D.C.: \nDesigning contestability: Interaction design, machine learning, \nand mental health. In: Proceedings of the 2017 Conference on \nDesigning Interactive Systems, DIS â€™17, p. 95â€“99. Association \nfor Computing Machinery, New York, NY, USA (2017). https://â€‹\ndoi.â€‹org/â€‹10.â€‹1145/â€‹30646â€‹63.â€‹30647â€‹03\n\t14.\t van den hoven, J., Lokhorst, gj, Poel, I.: Engineering and the prob-\nlem of moral overload. Sci. Eng. Ethics 18, 143â€“55 (2011). https://â€‹\ndoi.â€‹org/â€‹10.â€‹1007/â€‹s11948-â€‹011-â€‹9277-z\n\t15.\t Irving, G., Christiano, P., Amodei, D.: AI safety via debate. arXiv:â€‹\n1805.â€‹00899 [cs, stat] (2018)\n\t16.\t Kass, R., Finin, T., etÂ al.: The need for user models in generating \nexpert system explanations. Int J Expert Syst 1(4), (1988)\n\t17.\t Kim, B.: Interactive and interpretable machine learning models \nfor human machine collaboration. Ph.D. thesis, Massachusetts \nInstitute of Technology (2015)\n\t18.\t Langley, P.: Explainable, normative, and justified agency. Proc. \nAAAI Conf. Artifi. Intell. 33, 9775â€“9779 (2019). https://â€‹doi.â€‹org/â€‹\n10.â€‹1609/â€‹aaai.â€‹v33i01.â€‹33019â€‹775\n\t19.\t Laugel, T., Lesot, M.J., Marsala, C., Renard, X., Detyniecki, M.: \nThe dangers of post-hoc interpretability: Unjustified counterfac-\ntual explanations. In: Proceedings of the Twenty-Eighth Interna-\ntional Joint Conference on Artificial Intelligence, p. 2801â€“2807. \nInternational Joint Conferences on Artificial Intelligence Organi-\nzation (2019). https://â€‹doi.â€‹org/â€‹10.â€‹24963/â€‹ijcai.â€‹2019/â€‹388\n\t20.\t Lei, T., Barzilay, R., Jaakkola, T.: Rationalizing neural predic-\ntions. In: Proceedings of the 2016 Conference on Empirical \nMethods in Natural Language Processing, p. 107â€“117. Asso-\nciation for Computational Linguistics (2016) https://â€‹doi.â€‹org/â€‹10.â€‹\n18653/â€‹v1/â€‹D16-â€‹1011\n\t21.\t Liao, B., Anderson, M., Anderson, S.L.: Representation, justifica-\ntion, and explanation in a value-driven agent: an argumentation-\nbased approach. AI and Ethics. (2020) https://â€‹doi.â€‹org/â€‹10.â€‹1007/â€‹\ns43681-â€‹020-â€‹00001-8\n\t22.\t Loi, M., Ferrario, A., ViganÃ², E.: Transparency as design pub-\nlicity: explaining and justifying inscrutable algorithms. SSRN \nElectron. J. (2019) https://â€‹doi.â€‹org/â€‹10.â€‹2139/â€‹ssrn.â€‹34040â€‹40\n\t23.\t Madumal, P., Miller, T., Sonenberg, L., Vetere, F.: A grounded \ninteraction protocol for explainable artificial intelligence. In: Pro-\nceedings of the 18th International Conference on Autonomous \nAgents and MultiAgent Systems, AAMAS â€™19, p. 1033â€“1041. \nInternational Foundation for Autonomous Agents and Multiagent \nSystems, Richland, SC (2019)\n\t24.\t Miller, T.: Explanation in artificial intelligence: insights from \nthe social sciences. Artif. Intell. 267, (2017). https://â€‹doi.â€‹org/â€‹10.â€‹\n1016/j.â€‹artint.â€‹2018.â€‹07.â€‹007\n\t25.\t Miller, T., Howe, P., Sonenberg, L.: Explainable AI: beware of \ninmates running the asylum. In: IJCAI-17 Workshop on Explain-\nable AI (XAI), vol.Â 36 (2017)\n\t26.\t Mitchell, M., Wu, S., Zaldivar, A., Barnes, P., Vasserman, L., \nHutchinson, B., Spitzer, E., Raji, I.D., Gebru, T.: Model cards for \nmodel reporting. In: Proceedings of the Conference on Fairness, \nAccountability, and Transparency - FAT* â€™19 (2019). https://â€‹doi.â€‹\norg/â€‹10.â€‹1145/â€‹32875â€‹60.â€‹32875â€‹96\n\t27.\t Mittelstadt, B., Russell, C., Wachter, S.: Explaining explanations \nin ai. In: Proceedings of the conference on fairness, accountability, \nand transparency, p. 279â€“288 (2019)\n\t28.\t Mueller, S.T., Hoffman, R.R., Clancey, W., Emrey, A., Klein, G.: \nExplanation in human-ai systems: a literature meta-review, synop-\nsis of key ideas and publications, and bibliography for explainable \nai. arXiv preprint arXiv:â€‹1902.â€‹01876 (2019)\n\t29.\t Mulligan, D.K., Kluttz, D., Kohli, N.: Shaping our tools: contest-\nability as a means to promote responsible algorithmic decision \nmaking in the professions. Available at SSRN 3311894 (2019). \nhttps://â€‹ssrn.â€‹com/â€‹abstrâ€‹act=â€‹33118â€‹94\n\t30.\t Ploug, T., Holm, S.: The four dimensions of contestable ai diag-\nnosticsâ€”a patient-centric approach to explainable ai. Artif. Intell. \nMed. 107, 101901 (2020). https://â€‹doi.â€‹org/â€‹10.â€‹1016/j.â€‹artmed.â€‹2020.â€‹\n101901\n\t31.\t Swartout, W.R.: Explaining and justifying expert consulting \nprograms. In: Computer-assisted medical decision making, pp. \n254â€“271. Springer (1985)\n\t32.\t SÃ¸rmo, F., Cassens, J., Aamodt, A.: Explanation in case-based \nreasoning-perspectives and goals. Artif. Intell. Rev. 24(2), 109â€“\n143 (2005). https://â€‹doi.â€‹org/â€‹10.â€‹1007/â€‹s10462-â€‹005-â€‹4607-7\n\t33.\t Timmons, M.: Moral theory. Rowman and Littlefield Publishers, \nLanham (2013)\nPublisherâ€™s Noteâ€‚ Springer Nature remains neutral with regard to \njurisdictional claims in published maps and institutional affiliations.\n",
    "book_id": "a_framework_to_contest_and_justify_algorithmic_decisions",
    "book_title": "A framework to contest and justify algorithmic decisions",
    "book_author": "ClÃ©ment Henin ",
    "topic_id": "ai_policy",
    "topic_label": "policy",
    "chunk_index": 13
  },
  {
    "chunk_full": " Electronic copy available at: https://ssrn.com/abstract=3015350 \nDRAFT \n \n1 \nARTIFICIAL INTELLIGENCE POLICY: A ROADMAP \n \nRyan Calo* \n \nThe year is 2017 and talk of artificial intelligence is everywhere. People marvel at \nthe capacity of machines to translate any language and master any game.1 Others \ncondemn the use of secret algorithms to sentence criminal defendants2 or recoil at \nthe prospect of machines gunning for blue, pink, and white-collar jobs.3 Some worry \naloud that artificial intelligence will be humankindâ€™s â€œfinal invention.â€4   \n \nThe attention we pay to AI today is hardly new: looking back twenty, forty, or even a \nhundred years, one encounters similar hopes and concerns around AI systems and \nthe robots they inhabit. Batya Friedman and Helen Nissenbaum wrote Bias in \nComputer Systems, a framework for evaluating and responding to machines that \ndiscriminate unfairly, in 1996.5 The New York Times headline â€œA Robot is After Your \nJobâ€ could as easily appear this September as September of 1980.6  \n \nThe field of artificial intelligence itself dates back at least to the nineteen fifties, \nwhen the term was coined by John McCarthy, Marvin Minsky, and others one \nsummer at Dartmouth College, and likely generations earlier in the work of Charles \n                                                        \n* Lane Powell and D. Wayne Gittinger Associate Professor, University of Washington School of Law. \nThe author would like to thank a variety of individuals within industry, government, and academia \nwho have shared their thoughts regarding the key issues in artificial intelligence policy (see \nappendix) and to thank Madeline Lamo for excellent research assistance. The author is co-director of \nan interdisciplinary lab that receives funding to study emerging technology, including from the \nMicrosoft Corporation, the William and Flora Hewlett Foundation, and the John D. and Catherine T. \nMacArthur Foundation. \n1 Cade Metz, In a Huge Breakthrough, Googleâ€™s AI Beats a Top Player at the Game of Go, Wired (Jan. \n27, 2016), https://www.wired.com/2016/01/in-a-huge-breakthrough-googles-ai-beats-a-top-\nplayer-at-the-game-of-go/.  \n2 Julia Angwin et al., Machine Bias, ProPublica (May 23, 2016), \nhttps://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing; Cathy \nOâ€™Neil, Weapons of Math Destruction: How Big Data Increases Inequality and Threatens Democracy \n(2016).  \n3 Martin Ford, Rise of the Robots: Technology and the Threat of a Jobless Future (2015).  \n4 James Barrat, Our Final Invention: Artificial Intelligence and the End of the Human Era (2013).  \n5 Batya Friedman and Helen Nissenbaum, Bias in Computer Systems, 14 ACM Transactions on \nInformation Systems 330-47 (Jul. 1996). \n6 Harley Shaiken, A Robot is After Your Job; New technology isnâ€™t a panacea, N.Y. Times (Sept. 3, \n1980), at A19. For an excellent timeline of coverage of robots displacing labor, see Louis Anslow, \nRobots have been about to take all the jobs for more than 200 years, Timeline (May 16, 2016), \nhttps://timeline.com/robots-have-been-about-to-take-all-the-jobs-for-more-than-200-years-\n5c9c08a2f41d.  \n",
    "book_id": "ssrn-id3015350",
    "book_title": "Microsoft Word - AI Policy Roadmap SSRN.docx",
    "book_author": "Unknown",
    "topic_id": "ai_policy",
    "topic_label": "policy",
    "chunk_index": 14
  },
  {
    "chunk_full": " Electronic copy available at: https://ssrn.com/abstract=3015350 \nDRAFT \n \n2 \nBabbage, Ada Lovelace, and Alan Turing.7 Nearly every technique we use todayâ€”\nincluding the biologically-inspired neural nets at the heart of the practical AI \nbreakthroughs currently making headlinesâ€”were developed decades ago by \nresearchers in the United States, Canada, and elsewhere.8  \n \nIf the terminology, constituent techniques, and hopes and fears around artificial \nintelligence are not new, what exactly is? At least two differences characterize the \npresent climate. First, as is widely remarked, a vast increase in computational power \nand access to training data has led to practical breakthroughs in machine learning, a \nsingularly important branch of AI.9 These breakthroughs underpin recent successes \nacross a variety of applied domains, from diagnosing precancerous moles to driving \na vehicle, and dramatize the potential of AI for both good and ill.  \n \nSecond, policymakers are finally paying close attention. In 1960, when John F. \nKennedy was elected, there were calls for him to hold a conference around robots \nand labor.10 He declined.11  Later there were calls to form a Federal Automation \nCommission.12 None was formed. I looked and found no hearings on artificial \nintelligence in the House or Senate until, within months of one another in 2016, the \nHouse Energy and Commerce held a hearing on Advanced Robotics (robots with AI) \nand the Senate Committee on Commerce, Science, and Transportation held the â€œfirst \never hearing focused solely on artificial intelligence.â€13 That same year, the Obama \nWhite House held four workshops on AI and published three official reports \ndetailing its findings.14 Formal policymaking around AI abroad is, if anything, more \nadvanced: the governments of Japan and the European Union have formed official \ncommissions around robots and AI in recent years.15  \n                                                        \n7 Peter Stone et al., Artificial Intelligence and Life in 2030, Stanford University (Sept. 2016), \nhttps://ai100.stanford.edu/sites/default/files/ai_100_report_0831fnl.pdf.  \n8 Id., at 50. \n9 Id., at 13. See also National Science and Technology Council, Exec. Office of the President, Preparing \nfor the Future of Artificial Intelligence (2016). \n10 Anslow, supra note 6.  \n11 He did, however, give a speech on the necessity of â€œeffective and vigorous government leadershipâ€ \nto help solve the â€œproblems of automation.â€ Papers of John F. Kennedy. Pre-Presidential Papers. \nPresidential Campaign Files, 1960. Speeches and the Press. Speeches, Statements, and Sections, 1958-\n1960. Labor: Meeting the problems of automation (Jun. 7, 1960).  \n12 Anslow, supra note 6.  \n13 U.S. Senate Committee on Commerce, Science, & Transportation, The Dawn of Artificial Intelligence \n(Nov. 30, 2016), http://www.commerce.senate.gov/public/index.cfm/hearings?ID=042DC718-9250-\n44C0-9BFE-E0371AFAEBAB; U.S. Congress Joint Economic Committee, The Transformative Impact of \nRobots and Automation (May 5, 2016), https://www.jec.senate.gov/public/index.cfm/hearings-\ncalendar?ID=BB1C3FD8-9FD1-46BA-917C-E5B3C585F1CC.  \n14 Supra, note 9, Preparing for the Future of Artificial Intelligence, at 12. \n15 Ilina Lietzen, Robots: Legal Affairs Committee calls for EU-wide rules, European Parliament News \n(Jan. 12, 2017), http://www.europarl.europa.eu/news/en/news-room/20170110IPR57613/robots-\n",
    "book_id": "ssrn-id3015350",
    "book_title": "Microsoft Word - AI Policy Roadmap SSRN.docx",
    "book_author": "Unknown",
    "topic_id": "ai_policy",
    "topic_label": "policy",
    "chunk_index": 15
  },
  {
    "chunk_full": "DRAFT \n \n3 \n \nThis essay, prepared in connection with the UC Davis Law Reviewâ€™s 50th \nAnniversary symposium Future-Proofing Law: from rDNA to Robots, is my attempt at \nintroducing the AI policy debate to recent audiences as well as offering a conceptual \norganization for existing participants. The essay is designed to help policymakers, \ninvestors, scholars, and students understand the contemporary policy environment \naround artificial intelligence and the key challenges it presents. These include: \n \nâ€¢ Justice and equity  \nâ€¢ Use of force \nâ€¢ Safety and certification \nâ€¢ Privacy (including data parity); and \nâ€¢ Taxation and displacement of labor \n \nIn addition to these topics, the essay will touch briefly on a selection of broader \nsystemic questions: \n \nâ€¢ Institutional configuration and expertise \nâ€¢ Investment and procurement \nâ€¢ Removing hurdles to accountability; and \nâ€¢ Correcting mental models of AI \n \nIn each instance, I endeavor to give sufficient detail to describe the challenge \nwithout prejudging the policy outcome. This essay is meant to be a roadmap, not the \nroad itself. Its primary goal is to point the new entrant toward a wider debate and \nequip her or him with the context for further exploration and research.  \n \nI am a law professor with no formal training in AI. For whatever reasons, I have had \na front row seat to many of the recent efforts to assess and channel the impact of AI \non society.16 I am familiar with the burgeoning literature and commentary on this \ntopic and have reached out to individuals in the field to get their sense of what is \nimportant. That said, I certainly would not suggest that the inventory of policy \nquestions I identify here is somehow a matter of consensus. I do not speak for the AI \npolicy community as a whole. Rather, the views that follow are idiosyncratic and \nreflect, in the end, one scholarâ€™s interpretation of a complex landscape.17  \n                                                                                                                                                                     \nlegal-affairs-committee-calls-for-eu-wide-rules;  Robotics Policy Office is to be Established in METI, \nMinistry of Economy, Trade and Industry (July 1, 2015), \nhttp://www.meti.go.jp/english/press/2015/0701_01.html.  \n16 For example, I hosted the first White House workshop on artificial intelligence policy, participated \nas an expert in the inaugural panel of the Stanford AI 100 study, organized AI workshops for the \nNational Science Foundation, the Department of Homeland Security, and the National Academy of \nSciences, advise AI Now and FAT*, and co-founded the We Robot conference.  \n17 Earlier AI pioneer Herbert Simon argues that it is the duty of people who study a new technology \nto offer their interpretations regarding its likely effects on society. Herbert Simon, The Shape of \nAutomation and the Management of Men, vi (1965). But: â€œSuch interpretations should be, of course, \n",
    "book_id": "ssrn-id3015350",
    "book_title": "Microsoft Word - AI Policy Roadmap SSRN.docx",
    "book_author": "Unknown",
    "topic_id": "ai_policy",
    "topic_label": "policy",
    "chunk_index": 16
  },
  {
    "chunk_full": "DRAFT \n \n4 \n \nThe remainder of the essay proceeds as follows. Part I offers a short background on \nartificial intelligence and defends the terminology of policy over comparable terms \nsuch as ethics and governance. Part II lays out what I consider to be the key policy \nconcerns of AI as of this writing. Part III addresses the oddly tenacious and \nprevalent fear that AI poses an existential threat to humanityâ€”a concern that, if \ntrue, would seem to dwarf all other policy concerns. A final section concludes.  \n \nThank you for reading this essay. Your thoughts are warmly welcome.  \n \nI. \nBACKGROUND \n \nWhat is AI?  \n \nThere is no straightforward, consensus definition of artificial intelligence. AI is best \nunderstood as a set of techniques aimed at approximating some aspect of human or \nanimal cognition using machines. Early theorists conceived of symbolic systemsâ€”\nthe organization of abstract symbols using logical rulesâ€”as the most fruitful path \ntoward computers that can â€œthink.â€18 But the approach of building a reasoning \nmachine upon which to scaffold all other cognitive tasks, as originally envisioned by \nTuring and others, did not deliver upon initial expectations. What seems possible in \ntheory has yet to yield many viable applications in practice.19 \n \nSome blame an over-commitment to symbolic systems relative to other available \ntechniques (e.g., reinforcement learning) for the dwindling of research funding in \nthe late 1980s known as the AI Winter.20 Regardless, as limitations to the capacity of \nâ€œgood old fashioned AIâ€ to deliver practical applications became apparent, \nresearchers pursued a variety of other approaches to approximating cognition \ngrounded in the analysis and manipulation of real world data.21 An important \nconsequence of the shift was that researchers began to try to solve specific \nproblems or master particular â€œdomains,â€ such as converting speech to text or \nplaying chess, instead of pursuing a holistic intelligence capable of performing every \ncognitive task within one system.22 \n                                                                                                                                                                     \nthe beginning and not the end of public discussion.â€ Id. I vehemently agree. For another \ninterpretation, focusing on careers in AI policy, see Miles Brundage, Guide to working in AI policy and \nstrategy, 80,000 Hours (2017), https://80000hours.org/articles/ai-policy-guide/.  \n18 Stone et al., supra note 7.  \n19 Id.  \n20 Id. See also supra, note 9, Preparing for the Future of Artificial Intelligence. \n21 Stone et al., supra note 7.  \n22 Originally the community drew a distinction between â€œweakâ€ or â€œnarrowâ€ AI, designed to solve a \nsingle problem like chess, and â€œstrongâ€ AI with human-like capabilities across the boards. Today the \nterm strong AI has given way to terms like artificial general intelligence (AGI), which refer to systems \nthat can accomplish tasks in more than one domain without necessarily mastering all cognitive tasks.   \n",
    "book_id": "ssrn-id3015350",
    "book_title": "Microsoft Word - AI Policy Roadmap SSRN.docx",
    "book_author": "Unknown",
    "topic_id": "ai_policy",
    "topic_label": "policy",
    "chunk_index": 17
  },
  {
    "chunk_full": "DRAFT \n \n5 \n \nAll manner of AI techniques see study and use today. Much of the contemporary \nexcitement around AI, however, flows from the enormous promise of a particular \nset of techniques known collectively as machine learning.23 Machine learning or ML \nrefers to the capacity of a system to improve its performance at a task over time.24 \nOften this task involves recognizing patterns in datasets, although ML outputs can \ninclude everything from translating languages and diagnosing precancerous moles \nto grasping objects or helping to drive a car. As I allude to just above, most every \ntechnique that underpins ML have been around for decades. The recent explosion of \nefficacy comes from a combination of much faster computers and much more data.  \n \nIn other words, AI is an umbrella term, comprised by many different techniques. \nTodayâ€™s cutting-edge practitioners tend to emphasize approaches such as deep \nlearning within ML that leverage many-layered structures to extract features from \nenormous data sets in service of practical tasks requiring pattern recognition, or use \nreinforcement learning or convex optimization to similar effect. As we will see, these \ngeneral features of contemporary AIâ€”the shift toward practical applications, for \nexample, and the reliance on dataâ€”also inform our policy questions.  \n \nWhere is AI developed and deployed? \n \nDevelopment of AI is most advanced within industry, academia, and the military.25 \nIndustry in particular is taking the lead on AI, with tech companies hiring away top \nscientists from universities and leveraging unparalleled access to enormous \ncomputational power and voluminous, timely data.26 This was not always the case: \nas with many technologies, AI had its origins in academic research catalyzed by \nconsiderable military funding.27 But industry has long held a significant role. The AI \nWinter gave way to the present AI Spring in part thanks to the continued efforts of \nresearchers at Xerox Park and Bell Labs. Even today, much of the AI research \noccurring at firms is happening in research departments structurally insulated, to \nsome degree, from the demands of the companyâ€™s bottom line. Still, it is worth \nnoting that as few as seven for profit institutionsâ€”Google, Facebook, IBM, Amazon, \n                                                        \n23 See supra, note 9, Preparing for the Future of Artificial Intelligence. \n24 Harry Surden, Machine Learning and the Law, 89 Washington Law Review 87, 89-90 (2014). \n25 There are other, private organizations and public labs with considerable acumen in artificial \nintelligence, including the Allen Institute for AI and the Stanford Research Institute (SRI). \n26 E.g., Jordan Pearson, Uberâ€™s AI Hub in Pittsburgh Gutted a University Labâ€”Now Itâ€™s in Toronto, \nVice Motherboard (May 9, 2017), https://motherboard.vice.com/en_us/article/3dxkej/ubers-ai-hub-\nin-pittsburgh-gutted-a-university-lab-now-its-in-toronto.  \n27 Joseph Weizenbaum, Computers, Power, and Human Reason: from Calculation to Judgment 271-72 \n(1976) (discussing funding sources for AI research).  \n",
    "book_id": "ssrn-id3015350",
    "book_title": "Microsoft Word - AI Policy Roadmap SSRN.docx",
    "book_author": "Unknown",
    "topic_id": "ai_policy",
    "topic_label": "policy",
    "chunk_index": 18
  },
  {
    "chunk_full": "DRAFT \n \n6 \nMicrosoft, Apple, and Baidu in Chinaâ€”hold AI capabilities that vastly outstrip all \nother institutions as of this writing.28 \n \nAI is deployed across a wide variety of devices and settings. How wide depends on \nwhom you ask. Some would characterize spam filters that leverage ML or simple \nchat bots on social mediaâ€”programmed to, for instance, reply to posts about \nclimate change by denying its basis in scienceâ€”as AI. Others would limit the term to \nhighly complex instantiations such as DARPAâ€™s Cognitive Assistant that Learns and \nOrganizes (CALO)29 or the guidance software of a fully driverless car. We might also \ndraw a distinction between disembodied AI, which acquires, processes, and outputs \ninformation as data, and robotics or other cyber-physical systems, which leverage AI \nto act physically upon the world. Indeed, there is reason to believe the law will treat \nthese two categories differently.30  \n \nRegardless, many of the devices and services we access todayâ€”from iPhone \nautocorrect to Google Imagesâ€”leverage trained pattern recognition systems or \ncomplex algorithms that a generous definition of AI might encompass. The \ndiscussion that follows does not assume a minimal threshold of AI complexity but \nfocuses instead on what is different about contemporary AI from previous or \nconstituent technologies such as computers and the Internet.   \n \nWhy AI â€œpolicyâ€?  \n \nThat artificial intelligence lacks a stable, consensus definition or instantiation \ncomplicates efforts to develop an appropriate policy infrastructure. We might \nquestion the very utility of the word â€œpolicyâ€ in describing societal efforts to channel \nAI in the public interest. There are other terms in circulation. A new initiative \nanchored by MITâ€™s Media Lab and Harvard Universityâ€™s Berkman Klein Center for \nInternet and Society, for instance, refers to itself as the â€œEthics and Governance of \nArtificial Intelligence Fund.â€ Perhaps these are better words. Or perhaps it makes no \ndifference, in the end, what labels we use as long as the task is to explore and \nchannel AIâ€™s social impacts and our work is nuanced and rigorous. \n \nI select the term policy deliberately for several reasons. First, I personally struggle \nwith the alternatives. I consider the study and practice of ethics to be of vital \nimportance, of course, and I would argue that AI presents unique and important \nethical questions. Several efforts are underway, within industry, academia, and \n                                                        \n28 Vinod Iyengar, Why AI consolidation will create the worst monopoly in U.S. history, TechCrunch \n(Aug. 24, 2016), https://techcrunch.com/2016/08/24/why-ai-consolidation-will-create-the-worst-\nmonopoly-in-us-history/. See also Quora, What Companies Are Winning the Race for Artificial \nIntelligence?, Forbes (Feb. 24, 2017), https://www.forbes.com/sites/quora/2017/02/24/what-\ncompanies-are-winning-the-race-for-artificial-intelligence/#2af852e6f5cd.  \n29 See http://www.ai.sri.com/project/CALO. No relation.  \n30 Ryan Calo, Robotics and the Lessons of Cyberlaw, 103 California Law Review 513, 532 (2015).  \n",
    "book_id": "ssrn-id3015350",
    "book_title": "Microsoft Word - AI Policy Roadmap SSRN.docx",
    "book_author": "Unknown",
    "topic_id": "ai_policy",
    "topic_label": "policy",
    "chunk_index": 19
  },
  {
    "chunk_full": "DRAFT \n \n7 \nother organizations, to sort out the ethics of AI.31 But these efforts likely cannot \nsubstitute for policymaking. Ethics as a construct is notoriously malleable and \ncontested: both Kant and Bentham get to say â€œshould.â€32 Policyâ€”in the sense of \nofficial policy, at leastâ€”has a degree of finality once promulgated.33 Moreover, even \nassuming moral consensus, ethics lacks a hard enforcement mechanism. A handful \nof companies dominate the emerging AI industry. They are going to prefer ethical \nstandards over binding rules for the obvious reason that no tangible penalties attach \nto changing or disregarding ethics should the necessity arise.  \n \nIndeed, the unfolding development of a professional ethics of AI, while at one level \nwelcome and even necessary, merits ongoing attention.34 History is replete with \nexamples of new industries forming ethical codes of conduct, only to have those \ncodes invalidated by the federal government (the Department of Justice or Federal \nTrade Commission) as a restraint on trade. The National Society of Professional \nEngineers alone has been the subject of litigation across several decades. In the \n1970s, the DOJ sued the NSPE for establishing a â€œcanon of ethicsâ€ that prohibited \ncertain bidding practice; in the 1990s, the FTC sued the NSPE for restricting \nadvertising practices.35 The ethical codes of structural engineers have also been the \nsubject of complaints, as have the codes of numerous other industries.36 Will AI \nengineers fare differently? This is not to say companies or groups should avoid \nethical principles, only that we should pay attention to their effects. \n \nThe term governance has its attractions. Like policy, governance is a flexible term \nthat can accommodate many modalities and structures. Perhaps too flexible: it is not \nentirely clearly what is being governed and by whom. Regardless, governance \ncarries its own intellectual baggageâ€”baggage that, like ethics, is complicated by \nindustryâ€™s dominance of AI development and application. Setting aside the specific \n                                                        \n31 E.g., IEEE, Ethically Aligned Design: A Vision for Prioritizing Human Wellbeing with Artificial \nIntelligence and Autonomous Systems (Dec. 13, 2016), \nhttp://standards.ieee.org/develop/indconn/ec/ead_v1.pdf. I participated in this effort as a member \nof the Law Committee. Id. at 125.  \n32 See JosÃ© de Sousa E Brito, Right, Duty, and Utility: from Bentham to Kant and from Mill to Aristotle, \nXVII/2 Revista Iberoamericana de Estudios Utilitaristas 91, 92 (2010). \n33 Law has, in H.L.A. Hartâ€™s terminology, a rule of recognition. H.L.A. Hart, The Concept of Law (1961).  \n34 Romain Dillet, Apple Joins Amazon, Facebook, Google, IBM and Microsoft in AI Initiative, \nTechCrunch (Jan. 27, 2017), https://techcrunch.com/2017/01/27/apple-joins-amazon-facebook-\ngoogle-ibm-and-microsoft-in-ai-initiative. My own interactions with the Partnership on AI, which has \na diverse board of industry and civil society, suggests that participants are genuinely interested in \nchanneling AI toward the social good.  \n35 See, e.g., Natâ€™l Socâ€™y of Profâ€™l Engâ€™rs v. United States, 435 U.S. 679 (1978); In the Matter of Natâ€™l Socâ€™y \nof Profâ€™l Engâ€™rs, 116 F.T.C. 787 (1993). \n36 Structural Engineers Association of Northern California, 112 F.T.C. 530 (1989) (invalidating code \nof ethics). See also, e.g., In the Matter of the American Medical Association, et al., 94 F.T.C. 701 (1979) \n(invalidating the ethical guidelines of doctors); In the Matter of Connecticut Chiropractic Assâ€™n, 114 \nF.T.C. 708 (1991) (invalidating the ethical code of chiropractors). \n",
    "book_id": "ssrn-id3015350",
    "book_title": "Microsoft Word - AI Policy Roadmap SSRN.docx",
    "book_author": "Unknown",
    "topic_id": "ai_policy",
    "topic_label": "policy",
    "chunk_index": 20
  },
  {
    "chunk_full": "DRAFT \n \n8 \nassociations with â€œcorporate governance,â€37 much contemporary governance \nliterature embeds the claim that authority will or should devolve to actors other \nthan the state.38 While it is true that invoking the term governance can help insulate \ntechnologies from overt government interferenceâ€”as in the case of Internet \ngovernance through non-governmental bodies such as ICANN and the IETF39â€”the \ngovernance model also resists official policy by tacitly devolving responsibility from \nto industry from the state.  \n \nMeanwhile, several aspects of policy recommend it. Policy admits of the possibility \nof new laws, but does not require them. It may not be wise or even feasible to pass \ngeneral laws about artificial intelligence at this early stage. Whereas it is very likely \nwise and timely to plan for AIâ€™s effects on society, including through the \ndevelopment of expertise, the investigation of AIâ€™s current and likely social impacts, \nand perhaps smaller changes to appropriate doctrines and laws in response to AIâ€™s \npositive and negative affordances. Industry may seek to influence public policy but \nit is not its role ultimately to set it. Policy conveys the necessity of exploration and \nplanning, the finality of law, and the primacy of public interest without definitely \nendorsing or rejecting regulatory intervention. For these reasons, I have consciously \nchosen it as my frame. \n \nII. \nKEY QUESTIONS FOR AI POLICY \n \nHaving discussed what I mean by artificial intelligence and policy, I will now turn to \nthe main goal of this essay: a roadmap to the various challenges that AI poses for \npolicymakers. I will start with discrete challenges, in the sense of specific domains \nwhere attention is warranted, and then discuss some general questions that tend to \ncut across domains. For the most part, I will avoid getting into detail about specific \nlaws or doctrines that require reexamination and instead emphasize questions of \noverall strategy and planning.  \n \nThe primary purpose of this Part is to give newer entrants to the AI policy worldâ€”\nwhether from government, industry, media, academia, or otherwiseâ€”a general \nsense of what kinds of questions are being asked by the community and why. A \nsecondary purpose is to help bring cohesion to this multifaceted and growing field. \nThe inventory hopes to provide a roadmap for individuals and institutions to the \n                                                        \n37 Brian Cheffins, The History of Corporate Governance, in Mike Wright et al., eds., Oxford Handbook \nof Corporate Governance (2013).  \n38 Wendy Brown, Undoing the Demos: Neoliberalismâ€™s Stealth Revolution, 122-23 (2015) (â€œall \nscholars and definitions converge on the idea that governanceâ€ involves â€œnetworked, integrated, \ncooperative, partnered disseminated, and at least partly self-organizedâ€ control); R.A.W. Rhodes, The \nNew Governance: Governing without Government, XLIV Political Studies 652-57 (1996). \n39 The United States government stood up both ICANN and IETF but today they run largely \ninterdependent of state control as non-profits.  \n",
    "book_id": "ssrn-id3015350",
    "book_title": "Microsoft Word - AI Policy Roadmap SSRN.docx",
    "book_author": "Unknown",
    "topic_id": "ai_policy",
    "topic_label": "policy",
    "chunk_index": 21
  },
  {
    "chunk_full": "DRAFT \n \n9 \nvarious policy questions that arguably require their attention. The essay tees up \nquestions; it does not purport to answer them. \n \nA limitation of virtually any taxonomic approach is the need to articulate criteria for \ninclusionâ€”why are some questions on this list and not others?40 Experts may vary \non the stops they would include in a roadmap of key policy issues and I welcome \ncritique. There are several places where I draw distinctions or parallels that are not \nrepresented elsewhere in the literature, with which others may disagree. Ultimately \nthis represents but one informed scholars take on a complex and dynamic area of \nstudy.  \n \nA. Justice and Equity \n \nPerhaps the most visible and developed area of AI policy to date involves the \ncapacity of algorithms or trained systems to reflect human values such as fairness, \naccountability, and transparency (FAT).41 This topic is the subject of considerable \nstudy, including an established but accelerating literature on technological due \nprocess and at least one annual conference on the design of FAT systems.42 The \ntopic is also potentially quite broad, encompassing both the prospect of bias in AI-\nenabled features or products as well as the use of AI in making material decisions \nregarding financial, health, and even liberty outcomes. In service of teasing out \nspecific policy issues, I separate â€œapplied inequalityâ€ from â€œconsequential decision-\nmakingâ€ while acknowledging the considerable overlap.   \n \nInequality in application \n \nBy inequality in application, I mean to refer to a particular set of problems involving \nthe design and deployment of AI that works well for everyone. The examples here \ninclude everything from a camera that wonâ€™t take an Asian American familyâ€™s \npicture because the software believes they are blinking,43 to an image recognition \n                                                        \n40 Cf. Ryan Calo, The Boundaries of Privacy Harm, 86 Indiana Law Journal 1132, 1139-42 (2011) \n(critiquing Daniel Soloveâ€™s taxonomy of privacy). If I have an articulable criterion for inclusion, it is \nsustained attention by academics and policymakers. Some version of the questions in this Part \nappear in the social scientific literature, in the White House reports on AI, in the Stanford AI 100 \nreport, in the latest U.S. Robotics Roadmap, in the Senate hearing on AI, in the research wish list of \nthe Partnership on AI, and in the various important public and private workshops such as AI Now, \nFAT/ML, and We Robot.  \n41 E.g., Kate Crawford, Meredith Whittaker, et al., The AI Now Report: The Social and Economic \nImplications of Artificial Intelligence Technologies in the Near Term (Sep. 2016), \nhttps://artificialintelligencenow.com/media/documents/AINowSummaryReport_3_RpmwKHu.pdf;P\nartnership on AI to benefit people and society, Thematic Pillars, \nhttps://www.partnershiponai.org/thematic-pillars/.  \n42 Fairness, Accountability, and Transparency in Machine Learning (FAT/ML), \nhttp://www.fatml.org/.  \n43 Adam Rose, Are Face-Detection Cameras Racist?, Time (Jan. 22, 2010), \nhttp://content.time.com/time/business/article/0,8599,1954643,00.html.  \n",
    "book_id": "ssrn-id3015350",
    "book_title": "Microsoft Word - AI Policy Roadmap SSRN.docx",
    "book_author": "Unknown",
    "topic_id": "ai_policy",
    "topic_label": "policy",
    "chunk_index": 22
  },
  {
    "chunk_full": "DRAFT \n \n10\nsystem that characterizes an African American couple as gorillas,44 to a translation \nengine that associates the role of doctor with being male and the role of nurse with \nbeing female.45 These scenarios can be policy relevant in their own right, as when \nAfrican Americans fail to see opportunities on Facebook due to the platformâ€™s (now \ndiscontinued) discriminatory affordances,46 or Asian Americans pay more for test \npreparation due to a price discriminatory algorithm.47 They can also hold \ndownstream policy ramifications, as when a person of Asian decent has trouble \ngetting a passport,48 or a young woman in Turkey is researching international \nopportunities in higher education and finds only references to nursing.49 \n \nThere are a variety of reasons why AI systems might not work well for minority \npopulations. For example, the designs may be using models trained on data where \nminorities are underrepresented and hence not well reflected. More white faces in \nthe training set of an image recognition AI means the system performs best for \nCaucasians. There are also systems that are selective applied to the vulnerable. \nThus, for example, police use â€œheat mapsâ€ that purport to predict areas of future \ncriminal activity to determine where to patrol but in fact lead to disproportionate \nharassment of African Americans.50 But police do not routinely turn such techniques \ninward to predict which officers are likely to engage in excessive force.51 Nor do \n                                                        \n44 Jessica Guynn, Google Photos labelled black people â€œGorillas,â€ USA Today (Jul. 1, 2015), \nhttps://www.usatoday.com/story/tech/2015/07/01/google-apologizes-after-photos-identify-black-\npeople-as-gorillas/29567465/.  \n45 Aylin Caliskan, Joanna J. Bryson, and Arvind Narayanan, Semantics derived automatically from \nlanguage corpora contain human-like biases, 356 Science 183-86 (Apr. 2017).  \n46 Julia Angwin and Terry Parris, Jr., Facebook Lets Advertisers Exclude Users by Race, ProPublica \n(Oct. 28, 2016), https://www.propublica.org/article/facebook-lets-advertisers-exclude-users-by-\nrace.  \n47 Julia Angwin, Surya Mattu, and Jeff Larson, The Tiger Mom Tax: Asians Are Nearly Twice as Likely \nto Get a Higher Price from Princeton Review, ProPublica (Sep. 1, 2015), \nhttps://www.propublica.org/article/asians-nearly-twice-as-likely-to-get-higher-price-from-\nprinceton-review.  \n48 Selina Chang, An algorithm rejected an Asian manâ€™s passport photo for having â€˜closed eyes,â€™ Quartz \n(Dec. 7, 2016)., https://qz.com/857122/an-algorithm-rejected-an-asian-mans-passport-photo-for-\nhaving-closed-eyes/.  \n49 Aylin et al., supra note 45. See also Adam Hadhazy, Biased bots: Artificial-intelligence systems echo \nhuman prejudices, Princeton University (Apr. 18, 2017), \nhttps://www.princeton.edu/news/2017/04/18/biased-bots-artificial-intelligence-systems-echo-\nhuman-prejudices (â€œTurkish uses a gender-neutral, third person pronoun, â€˜o.â€™ Plugged into the online \ntranslation service Google Translate, however, the Turkish sentences â€˜o bir doktorâ€™ and â€˜o bir \nhemÅŸireâ€™ are translated into English as â€˜he is a doctorâ€™ and â€˜she is a nurse.â€™â€). \n50 Jessica Saunders, Priscillia Hunt, and John S. Hollwood, Predictions Put Into Practice: A Quasi \nExperimental Evaluation of Chicagoâ€™s Predictive Policing Pilot, 12 Journal of Experimental \nCriminology, 347-71 (Sep. 2016).  \n51 Kate Crawford & Ryan Calo, There is a blind spot in AI research, 538 Nature 311-13 (Oct. 20, 2016), \nhttp://www.nature.com/news/there-is-a-blind-spot-in-ai-research-1.20805.  \n",
    "book_id": "ssrn-id3015350",
    "book_title": "Microsoft Word - AI Policy Roadmap SSRN.docx",
    "book_author": "Unknown",
    "topic_id": "ai_policy",
    "topic_label": "policy",
    "chunk_index": 23
  },
  {
    "chunk_full": "DRAFT \n \n11\ninvestment firms initiate transactions on the basis of machine learning that they \ncannot explain to wealthy, sophisticated investors.52  \n \nThe policy questions here are at least twofold. First, what constitutes best practice \nin minimizing discriminatory bias and by what mechanism (antidiscrimination laws, \nconsumer protection, industry standards) does society incentivize development and \nadoption of best practice?53 And second, how do we ensure that the risks and \nbenefits of artificial intelligence are evenly distributed across society? Each set of \nquestions is already occupying considerable resources and attention, including \nwithin the industries that build AI into their products, and yet few would dispute we \nhave a long way to go before resolving them.  \n \nConsequential decision-making \n \nClosely related, but distinct in my view, is the question of how to design systems \nthat aid in making consequential decisions about people. The question is distinct \nfrom unequal application in general in that consequential decision-making, \nespecially by government, often takes place against a backdrop of procedural or \nprocess guarantees.54 For example, in the United States, the Constitution guarantees \ndue process and equal protection by the government,55 and European Union citizens \nhave the right to request that consequential decisions by private firms involve a \nhuman (current) as well as a right of explanation for adverse decisions by a machine \n(pending).56 Despite these representations, participants in the criminal justice \nsystem are already using algorithms to determine whom to police, whom to parole, \nand how long a defendant should stay in prison.57  \n \nI see at least three distinct facets to a thorough exploration of the role of AI in \nconsequential decision-making. The first involves cataloguing the objectives and \n                                                        \n52 Id. See also Will Knight, The Financial World Wants to Open AIâ€™s Black Boxes, MIT Technology \nReview (Apr. 13, 2017), https://www.technologyreview.com/s/604122/the-financial-world-wants-\nto-open-ais-black-boxes/.  \n53 E.g., Solon Barocas and Andrew D. Selbst, Big Dataâ€™s Disparate Impact, 104 California Law Review \n671 (2016).  \n54 See generally Danielle Keats Citron, Technological Due Process, 85 Washington University Law \nReview 1249 (2008).  \n55 Kate Crawford and Jason Schultz, Big Data and Due Process: Toward a Framework to Redress \nPredictive Privacy Harms, 55 Boston College Law Review 93 (2014); Barocas and Selbst, supra note \n53.  \n56 Bryce Goodman and Seth Flaxman, European Union regulations on algorithmic decision-making \nand a â€˜right to explanation,â€™ Proceedings of Workshop on Human Interpretability in Machine \nLearning (WHI 2016), https://arxiv.org/abs/1606.08813.  \n57 Saunders et al., supra note 50 (discussing heat zones in predictive policing); Joseph Walker, State \nParole Boards Use Software to Decide Which Intimates to Release, Wall Street Journal (Oct. 11, \n2013); Angwin et al., supra note 2 (discussing the use of algorithmically-generated risk scores). \n",
    "book_id": "ssrn-id3015350",
    "book_title": "Microsoft Word - AI Policy Roadmap SSRN.docx",
    "book_author": "Unknown",
    "topic_id": "ai_policy",
    "topic_label": "policy",
    "chunk_index": 24
  },
  {
    "chunk_full": "DRAFT \n \n12\nvalues that procedure and process are trying to advance in a particular context. \nWithout a thorough understanding of what it is that laws, norms, and other \nsafeguards are trying to achieve, we cannot assess whether existing systems are \nadequate let alone design new systems that are.58 This task is further complicated \nby the tradeoffs and tensions inherent in such safeguards, as when the Federal Rules \nof Civil Procedure call simultaneously for a â€œjust, speedy, and inexpensiveâ€ \nproceeding59 or where the Sixth Amendment lays out labor-intensive conditions for \na fair criminal trial that also has to occur quickly.60  \n \nThe second facet involves determining which of these objectives and values can and \nshould be imported into the context of machines. Deep learning, as a technique, may \nbe effective in establishing correlation but unable to yield or articulate a causal \nmechanism.61 AI here can say what will happen but not why. If so, the outputs of \nmulti-layer neural nets may be inappropriate affiants for warrants, bad witnesses in \ncourt, or poor bases for judicial determinations of fact.62 Notions such as \nprosecutorial discretion, the rule of lenity,63 and executive pardon may not admit of \nmechanization at all. Certain decisions, such as the decision to take an individual off \nof life support, raise fundamental concerns over human dignity and thus perhaps \ncannot be made even by objectively well-designed machines.64 \n \nA third facet involves the design and vetting of consequential decision-making \nsystems in practice. There is widespread consensus that such systems should be \nfair, accountable, and transparent.65 But other valuesâ€”such as efficiencyâ€”are less \nwell developed. The overall efficiency of an AI-enabled justice system, as distinct \nfrom its fairness or accuracy in the individual case, constitutes an important \nomission. As the saying goes, â€œjustice delayed is justice deniedâ€: we should not aim \n                                                        \n58 Citron, supra note 54; Crawford and Schultz, supra note 55. Cf. Joshua Kroll et al., Accountable \nAlgorithms, 165 University of Pennsylvania Law Review 633 (2017).  \n59 Federal Rules of Civil Procedure, Title I, Rule 1: Scope and Purpose (2016). I owe this point to my \ncolleague Elizabeth Porter.  \n60 U.S. Constitution, Amendment VI. \n61 Jason Millar and Ian Kerr, Delegation, Relinquishment, and Responsibility: The Prospect of Expert \nRobots, in Ryan Calo, A. Michael Froomkin, and Ian Kerr, eds., Robot Law (2015).  \n62 Id.; Andrew Roth, Machine Testimony, 126 Yale Law Journal 1972 (2017); Michael L. Rich, Machine \nLearning, Automated Suspicion Algorithms, and the Fourth Amendment, 164 University of \nPennsylvania Law Review 871 (2016).  \n63 The rule of lenity requires courts to construe criminal statutes narrowly, even where legislative \nintent appeared to militate toward a broader reading. E.g., McBoyle v. United States, 283 U.S. 25 \n(1931) (declining to extend a stolen vehicle statute to stolen airplanes). For a discussion of the limits \nof translating laws into machine code, see Harry Surden, Technological Opacity, Predictability, and \nSelf-Driving Cars, 38 Cardozo Law Review 121, 162â€“63 (2016). \n64 James H. Moor, Are There Decisions Computers Should Never Make, 1 Nature and Systems, 217-29 \n(1979). This concerns is also reflected in Part II.D concerning the use of force.  \n65 See supra notes 41-42 and accompanying text.  \n",
    "book_id": "ssrn-id3015350",
    "book_title": "Microsoft Word - AI Policy Roadmap SSRN.docx",
    "book_author": "Unknown",
    "topic_id": "ai_policy",
    "topic_label": "policy",
    "chunk_index": 25
  },
  {
    "chunk_full": "DRAFT \n \n13\nas a society to hold a perfectly fair, accountable, and transparent process for only a \nhandful of people a year.  \n \nInterestingly, the value tensions inherent in processual guarantees seem to find \nanalogs, if imperfect ones, in the machine learning literature around performance \ntradeoffs.66 Several researchers have measured how making a system more \ntransparent or less biased can decrease its accuracy overall.67  More obviously than \nefficiency, accuracy is an important dimension of fairness: We would not think of \nrolling a die to determine sentence length as fair, even if it is transparent to \nparticipants and unbiased as to demographics.  The policy challenge involves how to \nmanage these tradeoffs, either by designing techno-social systems that somehow \nmaximize for all values, or by embracing a particular tradeoff in a way society is \nprepared to recognize as valid. The end game of designing systems that reflect \njustice and equity will involve very considerable, interdisciplinary efforts and is \nlikely to prove a defining policy issue of our time.  \n \nB. Use of Force \n \nA special case of AI-enabled decision-making involves the decision to use force. As \nallude to above, there are decisionsâ€”particularly involving the deliberate taking of \nlifeâ€”that policymakers may decide never to commit exclusively to machines. Such \nis the gist of many debates regarding the development and deployment of \nautonomous weapons.68 There is something of an international consensus around \nthe idea that people should never give up â€œmeaningful human controlâ€ over a kill \ndecision.69 But debate lingers as to the meaning and scope of meaningful human \ncontrol. Is monitoring enough? Target selection? And does the prescription extend \nto defensive systems as well, or only offensive tactics and weapons? None of these \nimportant questions appear settled.70 \n \nThere is also the question of who bears responsibility for the choices of machines. \nThere may be circumstances in which the automation of weapons is desirable or \n                                                        \n66 Jon Kleinberg, Sendhil Mullainathan, and Manish Raghavan, Inherent Trade-Offs in the Fair \nDetermination of Risk Scores, Proceedings of Innovations in Theoretical Computer Science (ITCS \n2017), https://arxiv.org/abs/1609.05807.  \n67 Id.  \n68 Note that force is deployed in more contexts than military conflict. We might also ask after the \npropriety of the domestic use of force by border patrols, police, or even private security guards. For a \ndiscussion, see Elizabeth Joh, Policing Robot Police, 64 UCLA Law Review Discourse 516 (2016).  \n69 Heather M. Roff and Richard Moyes, Meaningful Human Control, Artificial Intelligence and \nAutonomous Weapons,â€ Briefing paper prepared for the Informal Meeting of Experts on Lethal \nAutonomous Weapons Systems, UN Convention on Certain Conventional Weapons, April 2016. \n70 Rebecca Crootof, A Meaningful Floor for Meaningful Human Control, 20 Temple International and \nComparative Law Journal 53 (2016). \n",
    "book_id": "ssrn-id3015350",
    "book_title": "Microsoft Word - AI Policy Roadmap SSRN.docx",
    "book_author": "Unknown",
    "topic_id": "ai_policy",
    "topic_label": "policy",
    "chunk_index": 26
  },
  {
    "chunk_full": "DRAFT \n \n14\neven inevitable.71 It seems unlikely, for example, that the United States military \nwould permit its military rivals to have faster or more flexible response capabilities \nthan its own whatever their control mechanism.72 Regardless, establishing a \nconsensus around meaningful human control would not obviate all inquiry into \nresponsibility in the event of mistake or war crime. There are uses of AI that \npresuppose human decision but nevertheless implicate deep questions of policy and \nethicsâ€”as when the intelligence community leverages algorithms to select targets \nfor remotely operated drone strikes.73 And there are concerns that soldiers will be \nplaced into the loop for the sole purpose of absorbing liability for wrongdoing, as \nanthropologist Madeline Clare Elish argues.74 Thus, policymakers must work toward \na framework for responsibility around AI and force that is fair and satisfactory to all \nstakeholders.  \n \nC. Safety and Certification \n \nAs the preceding section demonstrates, AI systems do more than process \ninformation and assist officials to make decisions of consequence. Many systemsâ€”\nsuch as the software that controls an airplane on autopilot or a fully driverless carâ€”\nexert direct and physical control over objects in the human environment. Others \nprovide sensitive services that, when performed by people, require training and \ncertification. These applications raise additional questions concerning the standards \nto which AI systems are held and the procedures and techniques available to ensure \nthose standards are being met.75  \n \nSetting and validating safety thresholds \n \nRobots and other cyber-physical systems have to be safe. The question is how safe, \nand how do we know. In a wide variety of contexts, from commercial aviation to \n                                                        \n71 Kenneth Anderson and Matthew Waxman in particular have made important contributions to the \nrealpolitik of AI weapons. E.g., Kenneth Anderson and Matthew Waxman, Law and Ethics for \nAutonomous Weapon Systems: Why a Ban Wonâ€™t Work and How the Laws of War Can, Hoover \nInstitution at Stanford University (2013), \nhttp://media.hoover.org/sites/default/files/documents/Anderson-\nWaxman_LawAndEthics_r2_FINAL.pdf.  \n72 Id.  \n73 John Naughton, Death by drone strike, dished out by algorithm, The Guardian (Feb. 21, 2016) \n(â€œGeneral Michael Hayden, a former director of both the CIA and the NSA, said this: â€˜We kill people \nbased on metadata.â€™â€) \n74 Madeline Clare Elish, Moral Crumple Zones: Cautionary Tales in Human-Robot Interaction, \nProceedings of We Robot (2016). See also Madeline Clare Elish and Tim Hwang, When your self-\ndriving car crashes, you could still be the one who gets sued, Quartz (Jul. 25, 2015), \nhttps://qz.com/461905/when-your-self-driving-car-crashes-you-could-still-be-the-one-who-gets-\nsued/.  \n75 Stone et al., supra note 7; Henrik I. Christensen et al., A Roadmap for US Robotics: From Internet to \nRobotics (Nov. 7, 2016), http://jacobsschool.ucsd.edu/contextualrobotics/docs/rm3-final-rs.pdf.  \n",
    "book_id": "ssrn-id3015350",
    "book_title": "Microsoft Word - AI Policy Roadmap SSRN.docx",
    "book_author": "Unknown",
    "topic_id": "ai_policy",
    "topic_label": "policy",
    "chunk_index": 27
  },
  {
    "chunk_full": "DRAFT \n \n15\nfood safety, regulatory agencies set specific safety standards and lay out \nrequirements for how those standards must be met. Such requirements do not exist \nfor many robots.  \n \nMembers of Congress and others have argued that we should embrace, for instance, \ndriverless cars, on the basis that robots are safer drivers than humans.76 But â€œsafer \nthan humansâ€ seems like an inadequate standard by which to vet any given \nautonomous system. Must the system be safer than humans unaided or humans \nassisted by cutting-edge safety features? Must the system be safer than humans \noverall or across all driving conditions? And just how much safer must driverless \ncars be than people before we tolerate or incentivize them? These are ultimately \ndifficult questions not of technology but of policy.77 \n \nEven assuming policymakers set satisfactory safety thresholds for driverless cars, \ndrone delivery, and other instantiations of AI, we need to determine a proper and \nacceptable means of verifying that these standards are met. This process has an \ninstitutional or â€œwhoâ€ componentâ€”e.g., government testing, third-party \nindependent certification, and self-certification by industryâ€”as well as a technical \nor â€œhowâ€ componentâ€”e.g., unit testing, fault-injection, virtualization, supervision.78 \nLocal and international standards can be a starting point but considerable work has \nyet to be doneâ€”especially as new potential applications and settings arise. For \nexample, we might resolve safety thresholds for drone delivery or warehouse \nretrieval only to revisit the question anew for sidewalk delivery and fast food \npreparation.  \n \nThere are further complications still. Some systemsâ€”such as high speed trading \nalgorithms that can destabilize the stock market or cognitive radio systems that can \ninterfere with emergency communicationsâ€”might hold the potential, alone or in \ncombination, to cause serious indirect harm.79 Others may engage in harmful acts \nsuch as disinformation that simultaneously implicate free speech concerns.80 \nPolicymakers must determine what kinds of non-physical or indirect harms rise to \nthe level that regulatory standards are required. Courts have a role in setting safety \npolicy in the United States though the imposition of liability. It turns out that AIâ€”\n                                                        \n76 Self-Driving Vehicle Legislation, Energy and Commerce Committee, U.S. House of Representatives \n(Jun. 27, 2017), https://energycommerce.house.gov/hearings-and-votes/hearings/self-driving-\nvehicle-legislation.  \n77 Cf. Guido Calabresi, The Costs of Accidents: A Legal and Economic Analysis (1970).  \n78 Cf. Bryant Walker Smith, How Governments Can Promote Automated Driving, 47 New Mexico Law \nReview 99 (2017).  \n79 Ryan Calo, The Case for a Federal Robotics Commission, Brookings Institution (Sep. 15, 2014), \nhttps://www.brookings.edu/research/the-case-for-a-federal-robotics-commission/.  \n80 E.g., Bence Kollanyi, Philip N. Howard, and Samuel C. Woolley.  â€œBots and Automation over Twitter \nduring the Second U.S. Presidential Debate.â€ Data Memo 2016.2. Oxford, UK: Project on \nComputational Propaganda. www.politicalbots.org.   \n",
    "book_id": "ssrn-id3015350",
    "book_title": "Microsoft Word - AI Policy Roadmap SSRN.docx",
    "book_author": "Unknown",
    "topic_id": "ai_policy",
    "topic_label": "policy",
    "chunk_index": 28
  },
  {
    "chunk_full": "DRAFT \n \n16\nespecially AI that displays emergent propertiesâ€”may pose challenges for civil \nliability.81 This misalignment must be remedy by doctrine or else factored into any \nregulatory considerations of the safety of AI. And markets have a role as well, for \ninstance, through the availability and conditions of insurance.82 \n \nCertification \n \nA closely related policy question arises where AI performs a task that, when done by \na human, requires evidence of specialized skill or training.83 In some contexts, \nsociety has seemed comfortable thus far dispensing with the formal requirement of \ncertification when technology can be shown to be capable through supervised use. \nThis is true of the autopilot modes of airplanes, which donâ€™t have to attend flight \nschool, and now vehicles. But what of technology under development today, such as \nautonomous surgical robots, the very value of which turns on bringing skills into an \nenvironment where no one has them? And how do we think of systems that purport \nto dispense legal, health, or financial advice, which requires adherence to complex \nfiduciary and other duties pegged to human judgment? Surgeons and lawyers must \ncomplete medical or law school and pass boards or bars. This approach may or may \nnot serve an environment rich in AI, a dynamic that is already unfolding as the Food \nand Drug Administration works to approve downloadable mobile apps as medical \ndevices84 and other apps dispute parking tickets.85  \n \nCybersecurity \n \nFinally, there is a growing awareness that AI complicates an already intractable \ncybersecurity landscape.86 First, as discussed, AI is increasingly deployed to act \ndirectly and even physically on the world. When a malicious party gains access to a \ncyber-physical system, suddenly bones instead of bits are on the line. And second, \nML and other AI techniques have the potential to alter both the offensive and \ndefensive capabilities around cybersecurity, as dramatized by a recent competition \n                                                        \n81 Calo, supra note 30, at 538-45. \n82 For an overview, see Andrea Bertoloni et al., On Robots and Insurance, 8 International Journal of \nSocial Robotics 381-91 (2016).  \n83 See Christensen et al., supra note 75.  \n84 Megan Molteni, Wellness Apps Evade the FDA, Only to Land in Court, Wired (Apr. 3, 2017), \nhttps://www.wired.com/2017/04/wellness-apps-evade-fda-land-court/.  \n85 Arezou Rezvani, â€˜Robot Lawyerâ€™ Makes the Case Against Parking Tickets, NPR All Things \nConsidered (Jan. 16, 2017), http://www.npr.org/2017/01/16/510096767/robot-lawyer-makes-the-\ncase-against-parking-tickets.  \n86 Greg Allen and Taniel Chen, Artificial Intelligence and National Security, Harvard Kennedy School \nBelfer Center Study on behalf of U.S. Intelligence Advanced Research Projects Activity (IARPA) (July \n2017).  \n",
    "book_id": "ssrn-id3015350",
    "book_title": "Microsoft Word - AI Policy Roadmap SSRN.docx",
    "book_author": "Unknown",
    "topic_id": "ai_policy",
    "topic_label": "policy",
    "chunk_index": 29
  },
  {
    "chunk_full": "DRAFT \n \n17\nheld by DARPA where AI agents attacked and defended a network autonomously.87 \nAI itself creates a new attack surface in the sense that ML and other techniques can \nbe coopted purposefully to trick the systemâ€”an area known as adversarial machine \nlearning. New threat models, standards, and techniques must be developed to \naddress the new challenges of securing information and physical infrastructures.  \n \nD. Privacy \n \nOver the past decade, the discourse around privacy has shifted perceptibly.88 What \nstarted out as a conversation about individual control over personal information has \nevolved into a conversation around the power of information more generally, i.e., \nthe control institutions have over consumers and citizens by virtue of possessing so \nmuch information about them.89 The acceleration of artificial intelligence, which is \nintimately tied to the availability of data, will play a significant role in this evolving \nconversation in at least two ways: (1) the problem of pattern recognition and (2) the \nproblem of data parity. Note that unlike some of the policy questions discussed \nabove, which envision the consequential deployment of imperfect AI, the privacy \nquestions that follow assume AI that is performing its assigned tasks only too well.  \n \nThe problem of pattern recognition \n \nThe capacity of AI to recognize patterns people cannot themselves detect threatens \nto eviscerate the already unstable boundary between what is public and what is \nprivate. Artificial intelligence is increasingly able to derive the intimate from the \navailable. This means that freely shared information of seeming innocenceâ€”where \nyou ate lunch, for example, or what you bought at the grocery storeâ€”can lead to \ninsights of a deeply sensitive nature. With enough data about you and the \npopulation at large, firms, governments, and other institutions with access to AI will \none day make guesses about you that you cannot imagineâ€”what you like, whom \nyou love, what you have done.90 \n \nSeveral serious policy challenges follow. The first set of challenges involves the \nacceleration of an existing trend around information extraction. Consumers will \nhave next to no ability to appreciate the consequences of sharing information. This \n                                                        \n87 Cyber Grand Challenge, DefCon 24 (Aug. 4-7, 2016), https://www.defcon.org/html/defcon-24/dc-\n24-cgc.html; â€˜Mayemâ€™ Declared Preliminary Winner of Historic Cyber Grand Challenge, News and \nEvents, Defense Advanced Research Projects Agency (Sep. 4, 2016), https://www.darpa.mil/news-\nevents/2016-08-04.  \n88 The flagship privacy law workshopâ€”Privacy Law Scholars Conferenceâ€”recently celebrated its \ntenth anniversary, although of course privacy discourse goes back much further.  \n89 E.g., Neil M. Richards, The Dangers of Surveillance, 126 Harvard Law Review 1934 (2013).   \n90 Tal Zarsky has been a particularly close student of this phenomenon. E.g., Tal Zarsky, Transparent \nPredictions, 2013 University of Illinois Law Review 1503 (2013).  \n",
    "book_id": "ssrn-id3015350",
    "book_title": "Microsoft Word - AI Policy Roadmap SSRN.docx",
    "book_author": "Unknown",
    "topic_id": "ai_policy",
    "topic_label": "policy",
    "chunk_index": 30
  },
  {
    "chunk_full": "DRAFT \n \n18\nis a well-understood problem in privacy scholarship.91 The community has \naddressed these challenges to privacy management under several labels, from \ndatabases to big data. In that the entire purpose of AI is to spot patterns people \ncannot, however, the issue is rapidly coming to a head. Perhaps the mainstreaming \nof AI technology will increase the pressure on policymakers to step in and protect \nconsumers. Perhaps not. Researchers are, at any rate, already exploring various \nalternatives to the status quo: fighting fire with fire by putting AI in the hands of \nconsumers, for example, or abandoning notice and choice altogether in favor of \nrules and standards. Whatever path we take should bear in mind the many ways \npowerful firms can subvert and end run consumer interventions and the \nunlikelihood consumers will keep up in a technological arms race. \n \nConsumer privacy is under siege. Citizens, meanwhile, will have next to no ability to \nresist or reform surveillance.92 Two doctrines in particular interact poorly with the \nnew affordances of artificial intelligence, both related to the reasonable expectation \nof privacy standard embedded in American constitutional law. First, the \ninterpretation of the Fourth Amendment by the courts that citizens enjoy no \nreasonable expectation of privacy in public or from a public vantage does not seem \nlong for this world.93 If everyone in public can be identified through facial \nrecognition, and if the â€œpublicâ€ habits of individuals or groups permit AI to derive \nprivate facts, then citizens will have little choice but to convey information to a \ngovernment bent on public surveillance. Second, and related, the interpretation by \nthe courts that individuals have no reasonable expectation of privacy in (non-\ncontent) information they convey to a third party such as the telephone company \nwill continue to come under strain.94 \n \nHere is an area where grappling with legal doctrine seems inevitable. Courts are \npolicymakers of a kind and the judiciary is already responding to these new realities \nby requiring warrants or probable cause in contexts involving public movements or \nthird party information. For example, the Supreme Court required a warrant for \nofficers to affix a GPS to a defendantâ€™s vehicle for the purpose of continuous \nmonitoring, which five Justices worrying aloud over law enforcementâ€™s ability to \nderive intimate information from public travel over time.95 There is a case before \n                                                        \n91 Daniel J. Solove, Privacy Self-Management and the Consent Dilemma, 126 Harvard Law Review \n1880 (2013).  \n92 Ryan Calo, Can Americans Resist Surveillance?, 83 The University of Chicago Law Review 23 \n(2016).  \n93 Joel Reidenberg, Privacy in Public, 69 University of Miami Law Review 141 (2014).  \n94 Courts and statutes tend to recognize that the content of a message such as an email deserves \ngreater protection than the non-content that accompanies the message, e.g., where it is going, \nwhether it is encrypted, whether it contains attachments, and so on. Cf. Riley v. California, 573 U.S. __ \n(2014) (invalidating the warrantless search and seizure of a mobile phone incident to arrest).  \n95 United States v. Jones, 132 S. Ct. 945 (2012).  \n",
    "book_id": "ssrn-id3015350",
    "book_title": "Microsoft Word - AI Policy Roadmap SSRN.docx",
    "book_author": "Unknown",
    "topic_id": "ai_policy",
    "topic_label": "policy",
    "chunk_index": 31
  },
  {
    "chunk_full": "DRAFT \n \n19\nthe Court as of this writing concerning the ability of police to demand historic \nlocation data about citizens in the possession of their mobile phone provider.96  \n \nOn the other hand, in the dog-sniffing case Florida v. Jardines the Court also \nreaffirmed the principle that individuals have no reasonable expectation of privacy \nin contraband such as illegal drugs.97 Thus, in theory, even if the courts resolve to \nrecognize a reasonable expectation of privacy in public and in information convened \nto a third party, courts might still permit the government to leverage AI to search \nexclusively for illegal activity. Indeed, some argue that AI is not a search at all given \nthat no human need to access the data unless or until the AI identifies something \nunlawful. Even assuming away the likely false positives, a reasonable question for \nlaw and policy is whether we want to live in a society with perfect enforcement.98  \n \nThe second set of policy challenges involves not what information states and firms \ncollect but the way highly granular information gets deployed. Again, the privacy \nconversation has evolved to focus not on the capacity of the individual to protect her \ndata, but on the power over an individual or group that comes from knowing so \nmuch about them. For example, firms can manipulate other market participants \nthrough a fine-tuned understanding of the individual and collective cognitive \nlimitations of consumers.99 Bots can gain our confidences to extract personal \ninformation.100 Politicians and political operatives can micro-target messages, \nincluding misleading ones, in an effort to sway aggregate public attention.101 All of \nthese capacities are dramatically enhanced by the ability of AI to detect patterns in a \ncomplex world. Thus, a distinct area of study is the best law and policy \ninfrastructure for a world of such exquisite and hyper-targeted control.  \n \nThe data parity problem \n \nThe data-intensive nature of machine learning, the technique yielding the most \npowerful applications of AI at the moment, has ramifications that are distinct from \nthe pattern recognition problem. Simply put, the greater access to data a firm has, \nthe better positioned it is to solve difficult problems with ML. As Amanda \nLevendowski explores, ML practitioners have essentially three options in securing \n                                                        \n96 Carpenter v. United States of America, 819 F.3d 880 (6th Cir. 2016), cert. granted.  \n97 Florida v. Jardines, 569 U.S. ___ (2013).  \n98 Christina M. Mulligan, Perfect Enforcement of Law: When to Limit and When to Use Technology, 14 \nRichmond Journal of Law and Technology 13 (2008).  \n99 Ryan Calo, Digital Market Manipulation, 82 George Washington Law Review 995 (2014).  \n100 Ian Kerr, Bots, Babes, and the Californication of Commerce, 1 University of Ottawa Law and Tech \nJournal 285 (2004) (presciently describing the role of chat bots in online commerce).  \n101 Ira S. Rubenstein, Voter Privacy in the Age of Big Data, 2014 Wisconsin Law Review 861 (2014).  \n",
    "book_id": "ssrn-id3015350",
    "book_title": "Microsoft Word - AI Policy Roadmap SSRN.docx",
    "book_author": "Unknown",
    "topic_id": "ai_policy",
    "topic_label": "policy",
    "chunk_index": 32
  },
  {
    "chunk_full": "DRAFT \n \n20\nsufficient data.102 They can build the databases themselves, they can buy the data, or \nthey can use â€œlow frictionâ€ alternatives such as content in the public domain.103 The \nlast option carries perils for bias discussed above. The first two are avenues largely \navailable to big firms or institutions such as Facebook or the military.  \n \nThe reality that a handful of large entities (literally, fewer than a human has fingers) \npossess orders of magnitude more data than anyone else leads to a policy question \naround data parity. Smaller firms will have trouble entering and competing in the \nmarketplace. Industry research labs will come to far outstrip public labs or \nuniversities, to the extend they do not already. Accordingly, cutting-edge AI \npractitioners will face even greater incentives to enter the private sphere, and ML \napplications will bend systematically toward the goals of profit-driven companies \nand not society at large. Companies will possess not only more and better \ninformation but a monopoly on its serious analysis.  \n \nWhy do I label the question of asymmetric access to data a â€œprivacyâ€ question? I do \nso because privacy ultimately governs the set of responsible policy outcomes that \narise in response to the data parity problem. Firms will, and already do, invoke \nconsumer privacy as a rationale for not permitting access to their data. This is partly \nwhy the AI policy community must maintain a healthy dose of skepticism toward \nâ€œethical codes of conductâ€ developed by industry.104 Such codes are likely to contain \na principle of privacy that, unless carefully crafted, operates to help shield the \ncompany from an obligation to share training data with other stakeholders.  \n \nThere is also the related question of access to government data. Or, more precisely, \ncitizen data held by the government. Governments possess an immense amount of \ninformation; data that citizens are obligated to provide to the state forms the \nbackbone of the contemporary data broker industry.105 Firms big and small, as well \nas university and other researchers, may be able to access government data on \ncomparable terms. But there are policy challenges here as well. Governments can \nand should place limits and conditions around sharing data.106 In the United States \nat least, this means carefully crafting policies to avoid constitutional scrutiny as \ninfringements on speech. The government cannot pick and choose with impunity \nthe sorts of uses to which private actors place data released by the state.107 At the \n                                                        \n102 Amanda Levendowski, How Copyright Law Creates Biased Artificial Intelligence, Proceedings of \nWe Robot 2017, http://www.werobot2017.com/wp-content/uploads/2017/03/Levendowski-How-\nCopyright-Law-Creates-Biased-Artificial-Intelligence-Abstract-and-Introduction-1.pdf.  \n103 Id.  \n104 See supra, Part I.  \n105 E.g., Jan Whittington et al., Push, Pull, and Spill: A Transdisciplinary Case Study in Municipal Open \nGovernment, 30 Berkeley Technology Law Journal 1900, 1904 (2016). \n106 Cf. Julia Powles and Hal Hodson, Google DeepMind and healthcare in an age of algorithms, Health \nTechnology (Mar. 16, 2017), https://link.springer.com/article/10.1007%2Fs12553-017-0179-1.  \n107 Sorrell v. IMS Health Inc., 564 U.S. 552 (2011).  \n",
    "book_id": "ssrn-id3015350",
    "book_title": "Microsoft Word - AI Policy Roadmap SSRN.docx",
    "book_author": "Unknown",
    "topic_id": "ai_policy",
    "topic_label": "policy",
    "chunk_index": 33
  },
  {
    "chunk_full": "DRAFT \n \n21\nsame time, governments may be able to put sensible restrictions in place before \ncompelling citizens to release private data.  \n \nTo be clear: I do not think society should run roughshod over privacy in its pursuit \nof data parity. Indeed, I present this issue as a key policy challenge precisely because \nI believe we need mechanisms by which to achieve a greater measure of data parity \nwithout sacrificing personal or collective privacy. Some within academic and \nindustry are already working on methodsâ€”including differential privacy and \nfederated trainingâ€”that seek to minimize the privacy impact of granting broader \naccess to data-intensive systems.108 The hard policy question is how to incentivize \ntechnical, legal, social, and other interventions that safeguard privacy even as AI is \ndemocratized.  \n \nE. Taxation and Displacement of Labor \n \nA common concern, especially in public discourse, is that AI will displace jobs by \nmastering tasks currently performed by people.109 The classic example is the truck \ndriver: many have observed that self-driving vehicles could obviate, or at least \nradically transform, this very common role. Machines have been replacing people \nsince the Industrial Revolution (which was hard enough on society). The difference, \nmany suppose, is twofold: first, the process of automation will be much faster, and \nsecond, very few sectors will remain untouched by AIâ€™s contemporary and \nanticipated capabilities.110 This would widen the populations that could feel AIâ€™s \nimpact and limits the efficacy of unemployment benefits or retraining.  \n \nIn its exploration of AIâ€™s impact on America, the Obama White House specifically \ninquired into the impact of AI on the job force and issued a report recommending a \nthicker social safety net to manage the upcoming disruption.111 Some predict that \nnew jobs will arise even as old ones fall away, or that AI will often improve the day \nto day of workers by permitting them to focus on more reword tasks involving \njudgment and creativity with which AI struggles. Others explore the eventual need \nfor a universal basic income, presumably underwritten by gains in productivity for \n                                                        \n108 Differential Privacy, Cynthia Dwork, 33rd International Colloquium on Automata, Languages and \nProgramming, part II (ICALP 2006), https://www.microsoft.com/en-us/research/wp-\ncontent/uploads/2016/02/dwork.pdf.  \n109 Ford, supra note 3. \n110 Erik Brynjolfsson and Andrew McAfee, The Second Machine Age: Work, Progress, and Prosperity \nin a Time of Brilliant Technologies (2014).  \n111 White House, Artificial Intelligence, Automation, and the Economy (Dec. 2016), \nhttps://obamawhitehouse.archives.gov/sites/whitehouse.gov/files/documents/Artificial-\nIntelligence-Automation-Economy.PDF.  \n",
    "book_id": "ssrn-id3015350",
    "book_title": "Microsoft Word - AI Policy Roadmap SSRN.docx",
    "book_author": "Unknown",
    "topic_id": "ai_policy",
    "topic_label": "policy",
    "chunk_index": 34
  },
  {
    "chunk_full": "DRAFT \n \n22\nautomation, so that even those displaced entirely by AI have access to resources.112 \nStill others wisely call for more and better information specific to automation so as \nto be able better to predict and scope the effects of AI.113 \n \nIn addition to assessing impact and addressing displacement, policymakers will \nhave to think through the effects of AI on the public fisc. Taxation is a highly \ncomplex policy domain that touches upon virtual all aspects of society; AI is no \nexception. Robots do not pay taxes, as the IRS once remarked in letter.114 Bill Gates, \nJr. thinks they should.115 Others warn that a tax on automation amounts to a tax on \ninnovation and progress. Ultimately federal and state policymakers will have to \nfigure out how to keep the lights on in the absence of, for instance, the bulk of \ntodayâ€™s income taxes.  \n \nF. Cross-Cutting Questions (Selected) \n \nThe preceding list of questions is scarcely exhaustive as to consequences of artificial \nintelligence for law and policy. Notably missing is any systemic review of the ways \nAI challenges existing legal doctrines. For example, that AIâ€™s are capable of \ngenerating spontaneous speech or content raises doctrinal questions around the \nlimits of the First Amendment as well as the contours of intellectual property.116  \nBelow I separately discuss the prospect that AI will wake up and kill us, which, if \ntrue, would seem to render every other policy context moot. But the preceding \ninventory does cover most of the common big picture policy questions that tend to \ndominant serious discourse around artificial intelligence.  \n \nIn addition to these specific policy contexts such as privacy, labor, or the use of \nforce, recurrent issues arise that cut across domains. I have selected a few here that \ndeserve greater attention: determining the best institutional configuration for \ngoverning AI, investing collective resources in AI that benefit individuals and \nsociety, addressing hurdles to AI accountability, and addressing our tendency to \nanthropomorphize technologies such as AI. I will discuss each of these systemic \nquestions briefly in turn.  \n \n                                                        \n112 Queena Kim, As Our Jobs Are Automated, Some Say Weâ€™ll Need a Guaranteed Basic Income, NPR \nWeekend Edition (Sep. 24, 2016), http://www.npr.org/2016/09/24/495186758/as-our-jobs-are-\nautomated-some-say-well-need-a-guaranteed-basic-income.  \n113 I am thinking particularly of the ongoing work of Robert Seamus at NYU Stern.  \n114 25 Tax Notes 20 (1984) (â€œ[I]nanimate objects are not required to file income tax returns.â€). \n115 Kevin J. Delaney, The robot that takes your job should pay taxes, says Bill Gates, Quartz (Feb. 17, \n2017).  \n116 Ronald Collins and David Skover, Robotica: Speech Rights and Artificial Intelligence (forthcoming \n2018); James Grimmelman, Copyright for Literature Robots, 101 Iowa Law Review 657 (2016); \nAnnemarie Bridy, Coding Creativity: Copyright and the Artificially Intelligence Author, 2012 Stanford \nTechnology Law Review 5 (2012).  \n",
    "book_id": "ssrn-id3015350",
    "book_title": "Microsoft Word - AI Policy Roadmap SSRN.docx",
    "book_author": "Unknown",
    "topic_id": "ai_policy",
    "topic_label": "policy",
    "chunk_index": 35
  },
  {
    "chunk_full": "DRAFT \n \n23\nInstitutional Configuration and Expertise \n \nThe prospect that AI presents individual or systemic risk, while simultaneously \npromising enormous potential benefits to people and society if responsibly \ndeployed, presents policymakers with an acute challenge around the best \ninstitutional configuration for channeling AI. Today AI policy is done, if at all, by \npiecemeal; federal agencies, states, cities, and other government units tackle issues \nthat most relate to them in isolation. There are advantages to this approach similar \nto the advantages of experimentation inherent in federalismâ€”the approach is \nsensitive to differences across contexts and preserves room for experimentation. \nBut some see the piecemeal approach as problematic, calling, for instance, for a kind \nof FDA for algorithms to vet every system with a serious potential to cause harm.117  \n \nAI prefigures into a common, but I think misguided, observation about the \nrelationship between law and technology. The public sees law as too slow to catch \nup to technologic innovation. Sometimes it is true that particular laws or regulations \nbecome long outdated as technology moves beyond where it was when the law was \npassed. For example, the Electronic Communications Privacy Act, passed in 1986, \ninteracts poorly with a post Internet environment in part because of ECPAâ€™s \nassumptions about how electronic communications would work.118 But this is \nhardly inevitable, and often political. The Federal Trade Commission has continued \nin its mission of protecting markets and consumers unabated, in part because it \nenforces a standardâ€”that of unfair and deceptive practiceâ€”that is largely neutral \nas to technology.119 In other contexts, agencies have passed new rules or interpreted \nrules differently to address new techniques and practices.  \n \nThe better-grounded observation is that government lacks the requisite expertise to \nmanage society in such a deeply technically-mediated world.120 Government bodies \nare slow to hire up and face steep competition from industry. When the state does \nnot have its own experts, it must either rely on the self-interested word of private \nfirms or their proxies, or experience a paralysis of decision and action that ill-serves \ninnovation.121 Thus, one overarching policy challenge is how best to introduce \nexpertise about AI and robotics into all branches and levels of government so they \ncan make better decisions with greater confidence.  \n \nThe solution could involve new advisory bodies, such as an official Federal Advisory \nCommittee on Artificial intelligence with an existing department or a standalone \n                                                        \n117 Andrew Tutt, An FDA for Algorithms, 89 Administrative Law Review 83 (2017).  \n118 Orin Kerr, The Next Generation Communications Privacy Act, 162 University of Pennsylvania Law \nReview, 373, 375 (2014).  \n119 Woodrow Hartzog, Unfair and Deceptive Robots, 74 Maryland Law Review 785, 810-19 (2015). \n120 Calo, supra note 79.  \n121 Id. (giving examples).  \n",
    "book_id": "ssrn-id3015350",
    "book_title": "Microsoft Word - AI Policy Roadmap SSRN.docx",
    "book_author": "Unknown",
    "topic_id": "ai_policy",
    "topic_label": "policy",
    "chunk_index": 36
  },
  {
    "chunk_full": "DRAFT \n \n24\nFederal Robotics Commission.122 Or it could involve resuscitating the Office of \nTechnology Assessment, building out the Congressional Research Service, or \ngrowing the Office of Science and Technology Policy. Yet another approach involves \neach branch hiring its own technical staff at every level. The technical knowledge \nand affordances of the governmentâ€”from the ability to test claims in a laboratory to \na working understanding of AI in lawmakers and the judiciaryâ€”will ultimately \naffect the governmentâ€™s capacity to generate wise AI policy.   \n \nInvestment and Procurement \n \nThe government possesses a wide variety of means by which to channel AI in the \npublic good. As recognized by the Obama White House, which published a separate \non the topic, one way to shape AI is by investing in it.123 Investment opportunities \ninclude not only basic AI research, which advance the state of computer science and \nhelp ensure the United States remains globally competitive, but also support of \nsocial scientific research into AIâ€™s impacts on society. Policymakers can be strategic \nabout where funds are committed and emphasize, for example, projects with an \ninterdisciplinary research agenda and a vision for the public good. \n \nIn addition, and sometimes less well recognized, the government can influence \npolicy through what it decides to purchase.124 States are capable of exerting \nconsiderable market pressures. Thus, policymakers at all levels ought to be thinking \nabout the qualities and characteristics of the AI-enabled products government will \npurchase and the companies that create them. Policymakers can also use contract to \nhelp ensure best practice around privacy, security, and other values. This can in turn \nmove the entire market toward more responsible practice and benefit society \noverall.  \n \nRemoving Hurdles to Accountability  \n \nMany AI systems in use or development today are proprietary and owners of AI \nsystems have inadequate incentives to open them up to scrutiny. In many contexts, \noutside analysis is necessary for accountability. For example, in the context of \njustice and equity, defendants may seek to challenge adverse risk scores.125 Or, in \n                                                        \n122 Id. See also Tom Krazit, Washingtonâ€™s Sen. Cantwell prepping bill calling for AI committee, \nGeekWire (Jul. 10, 2017), https://www.geekwire.com/2017/washingtons-sen-cantwell-reportedly-\nprepping-bill-calling-ai-committee/.  \n123 White House, National Science and Technology Council, The National Artificial Intelligence \nResearch and Development Strategic Plan (Oct. 2016), \nhttps://obamawhitehouse.archives.gov/sites/default/files/whitehouse_files/microsites/ostp/NSTC\n/national_ai_rd_strategic_plan.pdf.  \n124 Whittington et al., supra note 105 (discussing procurement in connection with open municipal \ndata); Walker Smith, supra note 78 (discussing procurement in connection with driverless cars). \n125 Loomis v. State of Wisconsin, 2016 WI 68 (2016), cert. granted.  \n",
    "book_id": "ssrn-id3015350",
    "book_title": "Microsoft Word - AI Policy Roadmap SSRN.docx",
    "book_author": "Unknown",
    "topic_id": "ai_policy",
    "topic_label": "policy",
    "chunk_index": 37
  },
  {
    "chunk_full": "DRAFT \n \n25\nthe context of safety and certification, third-parties seek to verify claims of safety or \nto evidence a lack of compliance. Several reports, briefs, and research papers have \ncalled upon policymakers to remove actual or perceived barriers to accountability, \nincluding: (1) trade secret law,126 (2) the Computer Fraud and Abuse Act,127 and (3) \nthe anti-circumvention provision of the Digital Millennium Copyright Act.128 This \nhas led a number of experts to recommend the formal policy step of planning to \nremove such barriers in order to foster greater accountability for AI.  \n \nMental Models of AI \n \nThe next and final Part is devoted to a discussion of whether AI is likely to end \nhumanity, itself partly a reflection of the special set of fears that tend to accompany \nanthropomorphic technology such as AI. Policymakers arguably owe it to their \nconstituents to hold a clear and accurate mental model of AI themselves, and may \nhave a role in education citizens about the technology and its potential effects. Here \nthey face an uphill battle, at least in the United States, due to decades of books, films, \ntelevision shows, and even plays that depict AI as a threatening substitute for \npeople.129 That the task is difficult, however, does not discharge policymakers from \ntheir responsibilities.  \n \nAt a more granular level, the fact that instantiations of AI such as Alexa (Echo), Siri, \nand Cortana, not to mention countless chat bots on a variety of social media \nplatforms, take the form of socials agent presents special challenges for policy \ndriven by our hardwired responses to social technology as though it were human.130 \nThese include the potential to influence children and other vulnerable groups in \ncommercial settings and the prospect of disrupting civic or political discourse,131 or \nthe further diminution of possibilities for solitude through a constant sense of being \nin the presence of another.132 Others are concerned about the prospect of intimacy, \n                                                        \n126 Rebecca Wexler, When a Computer Program Keeps You in Jail, New York Times (Jun. 13, 2017), \nhttps://www.nytimes.com/2017/06/13/opinion/how-computers-are-harming-criminal-\njustice.html.  \n127 Crawford, Whittaker, et al., supra note 41; Stone et al., supra note 7. \n128 Crawford, Whittaker, et al., supra note 41; Stone et al., supra note 7. \n129 There are examples dating back to the origin of the word robot, Karel ÄŒapek, R.U.R. (1920), to the \nheyday of German silent film, Metropolis (1927), to contemporary American cinema, Ex Machina \n(2014).  But the robot-as-villain narrative is not ubiquitous. Adults in Japan, for instance, grew up \nreading Mighty Atom (aka Astro Boy) (1952-1968), a Manga or comic in which the robot is a hero. \n130 Kate Darling, â€˜Who's Johnny?â€™ Anthropomorphic Framing in Human-Robot Interaction, \nIntegration, and Policy, in Patrick Lin, Keith Abney, and Ryan Jenkins, eds., Robot Ethics 2.0 (2017). \n131 Supra notes 80, 100, and 119. \n132 Ryan Calo, People Can Be So Fake: A New Dimension to Privacy and Technology Scholarship, 114 \nPennsylvania State Law Review 809 (2009).  \n",
    "book_id": "ssrn-id3015350",
    "book_title": "Microsoft Word - AI Policy Roadmap SSRN.docx",
    "book_author": "Unknown",
    "topic_id": "ai_policy",
    "topic_label": "policy",
    "chunk_index": 38
  },
  {
    "chunk_full": "DRAFT \n \n26\nincluding sexual, between people and machines.133  Whatever the particulars, that \neven the simplest AI can trigger social and emotional responses in people requires \nmuch more study and thought. \n \nIII. \nON THE AI APOCALYPSE \n \nSome set of readers may feel I have left out a key question: does artificial \nintelligence present an existential threat to humanity? If so, perhaps all other \ndiscussions constitute the policy equivalent of rearranging deck chairs on the \nTitanic. Why fix the human world if AI is going to end it?  \n \nMy own view is that AI does not present an existential threat to humanity, at least \nnot in anything like the foreseeable future. Further, devoting disproportionate \nattention and resources to the AI apocalypse has the potential to distract \npolicymakers from addressing AIâ€™s more immediate harms and challenges and could \ndiscourage investment in research on AIâ€™s present social impacts.134 How much \nattention to pay to a remote but dire threat is itself a difficult question of policy. \n \nEntrepreneur Elon Musk, physicist Stephen Hawking, and other famous individuals \napparently believe AI represents civilizationâ€™s greatest threat to date.135 The most \ncommon citation for this proposition is the work of a British speculative \nphilosopher named Nick Bostrom. In Superintelligence, Bostrom purports to \ndemonstrate that we are on a path toward developing AI that is both enormously \nsuperior to human intelligence and presents a significant danger of turning on its \ncreators.136 Bostrom, it should be said, does not see a malignant superintelligence as \ninevitable. But he presents the danger as acute enough to merit serious \nconsideration.  \n \nA number of prominent voices in artificial intelligence have convincingly challenged \nSuperintelligenceâ€™s thesis along several lines.137 First, they argue that there is simply \nno path toward machine intelligence that rivals our own across all contexts or \n                                                        \n133 Sharkey, N., van Wynsberghe, A., Robbins, S., & Hancock, E. (2017). Our Sexual Future with Robots. \nThe Hague, Netherlands: Foundation for Responsible Robotics. \n134 Crawford and Calo, supra note 51. \n135 Sonali Kohli, Bill Gates joins Elon Musk and Stephen Hawking in saying artificial intelligence is \nscary, Quartz (Jan. 29, 2015), https://qz.com/335768/bill-gates-joins-elon-musk-and-stephen-\nhawking-in-saying-artificial-intelligence-is-scary/.  \n136 Nick Bostrom, Superintelligence: Paths, Dangers, Strategies (2014).  \n137 In other work, Bostrom argues that we are likely all living in a computer simulation created by our \ndistant descendants. Nick Bostrom, Are you living in a simulation? 53 Philosophic Quarterly 211 \n(2003). This prior claim raises an interesting paradox: if AI kills everyone in the future, then we \ncannot be living in a computer simulation created by our decedents. And if we are living in a \ncomputer simulation created by our decedents, then AI didnâ€™t kill everyone. I think it a fair deduction \nthat Professor Bostrom is wrong about something.  \n",
    "book_id": "ssrn-id3015350",
    "book_title": "Microsoft Word - AI Policy Roadmap SSRN.docx",
    "book_author": "Unknown",
    "topic_id": "ai_policy",
    "topic_label": "policy",
    "chunk_index": 39
  },
  {
    "chunk_full": "DRAFT \n \n27\ndomains. Yes, a machine specifically designed to do so can beat any human at chess. \nBut nothing in the current literature around ML, search, reinforcement learning, or \nany other aspect of AI points the way toward modeling even the intelligence of a \nlower mammal in full, let alone human intelligence.138 Some say this explains why \nclaims of a pending AI apocalypse come almost exclusively from the ranks of \nindividuals such as Musk, Hawking, and Bostrom who possess no formal training in \nthe field.139 Second, critics of the AI apocalypse argue that even if we were able \neventually to create a superintelligence, there is no reason to believe it would be \nbent on world domination, unless this were for some reason programmed into the \nsystem. As Yann LeCun, deep learning pioneer and head of AI at Facebook colorfully \nputs it, computers donâ€™t have testosterone.140  \n \nNote that the threat to humanity could come in several forms. The first is that AI \nwakes up and purposefully kills everyone out of animus or to make more room for \nitself. This is the stuff of Hollywood movies and books by Daniel Wilson and finds \nnext to no support in the computer science literature (which is why we call it \nscience fiction).141 The second is that AI accidently kills everyone in the blind pursuit \nof some arbitrary goalâ€”for example, an irresistibly powerful AI charged with \nmaking paperclips destroys the Earth in the process of mining for materials.142 \nFantasy is replete with examples of this scenario as well, from The Sorcererâ€™s \nApprentice in Disneyâ€™s Fantasia to the ill-fated King Midas who demands the wrong \nblessing.143 \n \nEven if you believe the mainstream AI community that we are hundreds of years \naway from understanding how to create machines capable of formulating an intent \nto harm, and would not do so anyway, you might be worried about the second \n                                                        \n138 Erik Sofge, Why Artificial Intelligence Will Not Obliterate Humanity, Popular Science (Mar. 19, \n2015), http://www.popsci.com/why-artificial-intelligence-will-not-obliterate-humanity. â€œWe have \nbeen doing artificial intelligence since that terms was coined in the 1950s,â€ Australian computer \nscientist Mary Anne Williams once remarked to me, â€œand today robots are about as smart as insects.â€ \n139 Connie Loizos, This famous roboticists doesnâ€™t think Elon Musk understands AI, TechCrunch (Jul. \n19, 2017) (quoting Rodney Brooks as noting that AI alarmists â€œshare a common thread, in that: they \ndonâ€™t work in AI themselvesâ€), https://techcrunch.com/2017/07/19/this-famous-roboticist-doesnt-\nthink-elon-musk-understands-ai/.  \n140 Dave Blanchard, Muskâ€™s Warning Sparks Call for Regulating Artificial Intelligence, NPR All Tech \nConsidered (Jul. 19, 2017) (citing an observation by Yan LeCunn that the desire to dominate is not \nnecessarily correlated with intelligence),   \nhttp://www.npr.org/sections/alltechconsidered/2017/07/19/537961841/musks-warning-sparks-\ncall-for-regulating-artificial-intelligence.  \n141 E.g., Daniel Wilson, Roboapocalyse: A Novel (2011). Wilsonâ€™s book is thrilling in part because \nWilson has training in robotics and selectively adds accurate details to lend verisimilitude. \n142 Bostrom, supra note 136, at 123.  \n143 Fantasia (1940); Aristotle, Politics, Book 1, 1257b. I owe the analogy to King Midas to Stuart \nRussell, a prominent computer scientist at UC Berkeley who is among the handful of AI experts to \njoin Musk and others in worrying aloud about AIâ€™s capacity to threaten humanity. \n",
    "book_id": "ssrn-id3015350",
    "book_title": "Microsoft Word - AI Policy Roadmap SSRN.docx",
    "book_author": "Unknown",
    "topic_id": "ai_policy",
    "topic_label": "policy",
    "chunk_index": 40
  },
  {
    "chunk_full": "DRAFT \n \n28\nscenario. The argument has its attractions: people can set goals for AI that lead to \nunintended consequences. Computers do what you tell them to do, as the saying \ngoes, not what you want them to do. But think for a moment about the \ncharacteristics of the system AI doomsayers envision. It is simultaneously so \nprimitive as to perceive a singular goal, such as making paperclips, arbitrarily \nassigned by a person, and yet so advanced as to be capable of outwitting and \noverpowering the sum total of humanity in pursuit of this goal. I find this \ncombination of qualities at least as unlikely as the malicious AI bent on world \ndomination. \n \nOnly time can tell us for certain who is wrong and who is right. Although it is not the \nmainstream view among AI researcher and practitioners, I have attended several \nevents where established computer scientists and other smart people reflected \nsome version of the doomsday scenario.144 If there is even a remote chance that AI \nwill wake up and kill us, i.e., if the AI apocalypse is a low probability, high loss \nproblem, then perhaps we should pay some attention to the issue. The question is: \nhow much?  \n \nThe strongest argument against focusing on Skynet or HAL in 2017 is the \nopportunity cost. AI presents numerous pressing challenges to individuals and \nsociety in the very short term. â€œThe problem is not that artificial intelligence will get \ntoo smart and take over the world,â€ computer scientist Pedro Domingos writes, â€œthe \nproblem is that itâ€™s too stupid and already has.â€145 At best, investment in the study of \nAIâ€™s existential threat diverts millions of dollars (and billions of neurons) away from \nresearch on serious questions such as how to protect societyâ€™s most vulnerable \nmembers. At worst, government, civil society, and the public at large become \ndisaffected by the lack of meaningful progress in AI policy that comes from trying to \ngain traction on a problem that does not exist and cease to invest in AI policy at all. \nBy focusing so much energy on a quixotic existential threat, we risk, in information \nscientist Solon Barocasâ€™ words, an AI Policy Winter. \n \nCONCLUSION \n \nThis essay had two goals. First, it sought to provide a brief primer on artificial \nintelligence by defining AI in relation to previous and constituent technologies and \nby noting the ways the contemporary conversation around AI may be unique. One of \nthe most obvious breaks with the past is the extent and sophistication of the policy \nresponse to AI in the United States and around the world. Thus the essay sought, \nsecond, to provide an inventory or roadmap of the serious policy questions that \nhave arisen to date. The purpose of this inventory is to inform AI policymaking, \nbroadly understood, by identifying the issues and developing the questions to the \n                                                        \n144 E.g., the Future of Life Institute, https://futureoflife.org/.  \n145 Pedro Domingos, The Master Algorithm: How the Quest for the Ultimate Learning Machine Will \nRemake Our World (2015).  \n",
    "book_id": "ssrn-id3015350",
    "book_title": "Microsoft Word - AI Policy Roadmap SSRN.docx",
    "book_author": "Unknown",
    "topic_id": "ai_policy",
    "topic_label": "policy",
    "chunk_index": 41
  },
  {
    "chunk_full": "DRAFT \n \n29\npoint that readers can initiate their own investigation. The roadmap is idiosyncratic \nto the author but informed by longstanding participation in AI policy. \n \nAI is remaking aspects of society today and likely to shepherd in much greater \nchanges in the coming years. As this essay emphasized, the process of societal \ntransformation carries with it many distinct and difficult questions of policy. \nOverall, however, there is reason for hope. We have certain advantages over our \npredecessors. The previous industrial revolutions had their lessons and we have \naccess today to many more policymaking bodies and tools. We have also made \ninterdisciplinary collaboration much more of a standard practice. But perhaps the \ngreatest advantage is timing: AI has managed to capture policymakersâ€™ imaginations \nearly enough in its life-cycle that there is hope we can yet channel it toward the \npublic interest. I hope this essay contributes in some small way to this process.  \n \n \n \n \n",
    "book_id": "ssrn-id3015350",
    "book_title": "Microsoft Word - AI Policy Roadmap SSRN.docx",
    "book_author": "Unknown",
    "topic_id": "ai_policy",
    "topic_label": "policy",
    "chunk_index": 42
  },
  {
    "chunk_full": "DRAFT \n \n30\nAPPENDIX \n",
    "book_id": "ssrn-id3015350",
    "book_title": "Microsoft Word - AI Policy Roadmap SSRN.docx",
    "book_author": "Unknown",
    "topic_id": "ai_policy",
    "topic_label": "policy",
    "chunk_index": 43
  },
  {
    "chunk_full": "15 February, 2018 | created using FiveFilters.org\nTechnology & Ethos \nVol. 2 Book of Life \nby Imamu Amiri Baraka (LeRoi Jones) \nfrom Raise Rage Rays Raze: Essays Since 1965 \nÂ© 1969, 1970, 1971 by LeRoi Jones\nISBN: 0âˆ’394âˆ’71706âˆ’6. This essay ï¬rst appeared in Amistad2, \npublished by Random House, Inc.\nMachines (as Norbert Weiner said) are an extension of their \ninventor- creators. That is not simple once you think. Machines, \nthe entire technology of the West, is just that, the technology of \nthe West.\nNothing has to look or function the way it does. The West manâ€™s \nfreedom, unscientiï¬cally got at the expense of the rest of the \nworldâ€™s people, has allowed him to xpand his mindâ€“spread his \nsensibility wherever it cdgo, & so shaped the world, & its \npowerful artifact- engines.\nPolitical power is also the power to createâ€“not only what you \nwillâ€“but to be freed to go where ever you can goâ€“(mentally \nphysically as well). Black creationâ€“creation powered by the \nBlack ethos brings very special results,\nThink of yourself, Black creator, freed of european restraint \nwhich ï¬rst means the restraint of self determined mind \ndevelopment. Think what would be the results of the unfettered \nblood inventor- creator with the resources of a nation behind \nhim. To imagineâ€“to thinkâ€“to constructâ€“to energize!!!\nHow do you communicate with the great masses of Black \npeople? How do you use the earth to feed masses of people? \nHow do you cure illness? How do you prevent illness? What are \nthe Black purposes of space travel?\nIt staggers the mind. To be free go let the mind do what it will \nas constructive progress force, availed of the total knowledge \nresource energy of a nation.\nThese white scientists on lifetime fellowships, or pondering \nproblems at Princetonâ€™s Institute For Advanced Study.\nSo that a telephone is one cultureâ€™s solution to the problem of \nsending words through space. It is political power that has \nallowed this technology to emerge, & seem the sole direction \nfor the result desired.\nA typewriter?â€“why shd it only make use of the tips of the ï¬ngers \nas contact points of ï¬‚owing multi directional creativity. If I \ninvented a word placing machine, an â€œexpression- scriber,â€ if \nyou will, then I would have a kind of instrument into which I \ncould step & sit or sprawl or hang & use not only my ï¬ngers to \nmake words express feelings but elbows, feet, head, behind, \nand all the sounds I wanted, screams, grunts, taps, itches, Iâ€™d \nhave magnetically recorded, at the same time, & translated into \nwordâ€“or perhaps even the ï¬nal xpressed thought/ feeling wd not \nbe merely word or sheet, but itself, the xpression, three \ndimensionalâ€“able to be touched, or tasted or felt, or entered, or \nheard or carried like a speaking singing constantly \ncommunicating charm. A typewriter is corny!!\nThe so called ï¬ne artist realizes, those of us who have freed \nourselves, that our creations need not emulate the white manâ€™s, \nbut it is time the engineers, architects, chemists, electronics \ncraftsmen, ie ï¬lm too, radio, sound, &c., that learning western \ntechnology must not be the end of our understanding of the \nparticular discipline weâ€™re involved in. Most of that west shaped \ninformation is like mud and sand when youâ€™re panning for gold!\nThe actual beginnings of our expression are post Western (just \nas they certainly are pre- western). It is only necessary that we \narm ourselves with complete self knowledge the whole \ntechnology (which is after all just expression of who ever) will \nchange to reï¬‚ect the essence of a freed people. Freed of an \noppressor, but also as TourÃ© has reminded we must be â€œfree \nfrom the oppressorâ€™s spirit,â€ as well. It is this spirit as emotional\nconstruct that can manifest as expression as art or technology \nor any form.\nBut what is our spirit, what will it project? What machines will \nit produce? What will they achieve? What will be their morality? \nCheck the diï¬€erent morality of the Chinese birthday celebration \nï¬recracker & the white boyâ€™s bomb. Machines have the morality\nof their inventors.\nSee everything fresh and â€œwithout formâ€â€“then make forms that \nwill express us truthfully and totally and by this certainly free \nus eventually.\nThe new technology must be spiritually oriented because it \nmust aspire to raise manâ€™s spirituality and expand manâ€™s \nconsciousness. It must begin by being â€œhumanisticâ€ though the \nwhite boy has yet to achieve this. Witness a technology that \nkills both plants & animals, poisons the air & degenerates or \nenslaves man.\nThe technology itself must represent human striving. It must \nrepresent at each point the temporary perfection of the \nevolutional man. And be obsolete only because nothing is ever \nperfect, the only constant is change.\nAmiri Baraka \nAmistad 2, 1970\nsoulsista: \nshine \nifa\n",
    "book_id": "technology__ethos",
    "book_title": "Technology & Ethos",
    "book_author": "Unknown",
    "topic_id": "ai_policy",
    "topic_label": "policy",
    "chunk_index": 44
  },
  {
    "chunk_full": "The Moral Character of Cryptographic Workâ‹†\nPhillip Rogaway\nDepartment of Computer Science\nUniversity of California, Davis, USA\nrogaway@cs.ucdavis.edu\nDecember 2015\n(minor revisions March 2016)\nAbstract. Cryptography rearranges power: it conï¬gures who can do\nwhat, from what. This makes cryptography an inherently political tool,\nand it confers on the ï¬eld an intrinsically moral dimension. The Snowden\nrevelations motivate a reassessment of the political and moral positioning\nof cryptography. They lead one to ask if our inability to eï¬€ectively\naddress mass surveillance constitutes a failure of our ï¬eld. I believe that\nit does. I call for a community-wide eï¬€ort to develop more eï¬€ective means\nto resist mass surveillance. I plead for a reinvention of our disciplinary\nculture to attend not only to puzzles and math, but, also, to the societal\nimplications of our work.\nKeywords:\ncryptography Â· ethics Â· mass surveillance Â· privacy Â·\nSnowden Â· social responsibility\nPreamble. Most academic cryptographers seem to think that our ï¬eld is a fun,\ndeep, and politically neutral gameâ€”a set of puzzles involving communicating\nparties and notional adversaries. This vision of who we are animates a ï¬eld\nwhose work is intellectually impressive and rapidly produced, but also quite\ninbred and divorced from real-world concerns. Is this what cryptography should\nbe like? Is it how we should expend the bulk of our intellectual capital?\nFor me, these questions came to a head with the Snowden disclosures of 2013.\nIf cryptographyâ€™s most basic aim is to enable secure communications, how could\nit not be a colossal failure of our ï¬eld when ordinary people lack even a modicum\nof communication privacy when interacting electronically? Yet I soon realized\nthat most cryptographers didnâ€™t see it this way. Most seemed to feel that the\ndisclosures didnâ€™t even implicate us cryptographers.\nI think that they do. So I want to talk about the moral obligations of cryp-\ntographers, and my community as a whole. This is not a topic cryptographers\nroutinely discuss. In this post-Snowden era, I think it needs to be.\nâ‹†This is an essay written to accompany an invited talk (the 2015 IACR Distinguished\nLecture) given at Asiacrypt 2015 on December 2, 2015, in Auckland, New Zealand.\nThe essay and talk are addressed to the cryptographic communityâ€”my communityâ€”\nand the words â€œweâ€ and â€œourâ€ should be so interpreted. I apologize in advance if I\noï¬€end anyone with any of my comments; nothing of the sort is my intent.\n",
    "book_id": "the_moral_character_of_cryptographic_work",
    "book_title": "The Moral Character of Cryptographic Work",
    "book_author": "Unknown",
    "topic_id": "ai_policy",
    "topic_label": "policy",
    "chunk_index": 45
  },
  {
    "chunk_full": "2\nP. Rogaway\nPart 1: Social responsibility of scientists and engineers\nA famous manifesto. Iâ€™d like to begin with a storyâ€”a true story.1 To set the\nstage, it is London, the summer of 1955. A roomful of reporters have assembled\nfor a press conference in Caxton Hall, a red brick building in Westminister. The\nmedia have been summoned in a plan hatched by Bertrand Russell, with some\nhelp from the editor of The Observer newspaper. The reporters donâ€™t know just\nwhy they are here, having only been told that a team of the worldâ€™s leading\nscientists were ready to release something of world-wide signiï¬cance. The press\nknows that Sir Bertrand Russell is involved. With Einsteinâ€™s recent death, Russell\nhas become the worldâ€™s most famous living intellectual.\nRussell has been in his home, hiding, all week. All day long the phone\nrings, the doorbell rings. Reporters are trying to ï¬nd out what is this big\nannouncement. Russellâ€™s wife and his housekeeper make excuses and shoo the\nreporters away.\nAs the press conference begins, the reporters learn from Russell and\naccompanying physicist Joseph Rotblat that they have not been assembled\nto hear of some new scientiï¬c discovery, but to receive a prepared, political\nstatement. Itâ€™s a fairly brief statement, but itâ€™s been signed by eleven2 of the\nworldâ€™s leading scientistsâ€”nine of them Nobel laureates. Albert Einstein is\namong the signatories, signing just days before he became ill and died.\nThe document would become known as the Russellâ€“Einstein manifesto.3 I\nhope that its contents are known to you. It speaks of the existential threat\nto mankind posed by nuclear weapons. Itâ€™s ï¬nal passage sounds desperately\nplaintive as Russell writes:\nWe appeal, as human beings, to human beings: Remember your humanity, and\nforget the rest. If you can do so, the way lies open to a new Paradise; if you\ncannot, there lies before you the risk of universal death.4\nThe reporters ask questions and soon warm to the manifestoâ€™s importance. The\nnext day, the manifesto is carried as front-page news of most the worldâ€™s major\nnewspapers. For the next several days, at least, it is the talk of the world.\nThe Russellâ€“Einstein manifesto galvanized the peace and disarmament\nmovements. It led to the Pugwash conferences, for which Joseph Rotblat and\nthe conferences-series itself would eventually win the Nobel Peace Prize (1995).\nRotblat credits the manifesto for helping to create the conditions that gave rise\nto the Nuclear Non-Proliferation Treaty (NPT, 1970).5 In his Nobel Peace Prize\nacceptance speech, Rotblat explains:\n1 This account is largely taken from Sandra Butcher: The origins of the Russellâ€“\nEinstein manifesto. Pugwash Conference on Science and World Aï¬€airs, May 2005.\n2 At the time of the press conference, Russell had heard from only eight.\n3 The name would seem to be an instance of the â€œMatthew eï¬€ect,â€ as signatories Max\nBorn, FrÂ´edÂ´eric Joliot-Curie, and Joseph Rotblat all played roles at least as large as\nEinsteinâ€™s.\n4 Quoted in Joseph Rotblat, ed., Proceedings of the First Pugwash Conference on\nScience and World Aï¬€airs, Pugwash Council, 1982.\n5 Butcher, op. cit., Foreword, p. 3.\n",
    "book_id": "the_moral_character_of_cryptographic_work",
    "book_title": "The Moral Character of Cryptographic Work",
    "book_author": "Unknown",
    "topic_id": "ai_policy",
    "topic_label": "policy",
    "chunk_index": 46
  },
  {
    "chunk_full": "The Moral Character of Cryptographic Work\n3\nFrom my earliest days I had a passion for science. But science, the exercise of\nthe supreme power of the human intellect, was always linked in my mind with\nbeneï¬t to people. I saw science as being in harmony with humanity. I did not\nimagine that the second half of my life would be spent on eï¬€orts to avert a\nmortal danger to humanity created by science.6\nTwo modes of behaving politically. I begin with the Russellâ€“Einstein\nmanifesto to remind you of two things: ï¬rst, that technical work itself can\nimplicate politics; and second, that some scientists, in response, do take on\novertly political roles. These two ways to behave politically are diï¬€erent (even\nif, to people like Rotblat, they go hand-in-hand). Letâ€™s look at each.\nImplicit politics. A scientist engages in what Iâ€™ll call implicit politics by\ninï¬‚uencing power relations as a byproduct of technical work. Politics is about\npowerâ€”who has how much of it, and what sort. The nuclear bomb is the ultimate\nexpression of coercive power; it is politics incarnate. Had Rotblat shunned every\nostensibly political role in his life, his lifeâ€™s work would still have been political.\nImmensely, if implicitly, so.\nBut we donâ€™t need the specter of mushroom clouds to be dealing with\npolitically relevant technology: scientiï¬c and technical work routinely implicates\npolitics. This is an overarching insight from decades of work at the crossroads of\nscience, technology, and society.7 Technological ideas and technological things\nare not politically neutral: routinely, they have strong, built-in tendencies.\nTechnological advances are usefully considered not only from the lens of how\nthey work, but also why they came to be as they did, whom they help, and whom\nthey harm. Emphasizing the breadth of manâ€™s agency and technological options,\nand borrowing a beautiful phrase of Borges, it has been said that innovation is\na garden of forking paths.8\nStill, cryptographic ideas can be quite mathematical; mightnâ€™t this make them\nrelatively apolitical? Absolutely not. That cryptographic work is deeply tied to\npolitics is a claim so obvious that only a cryptographer could fail to see it. Thus\nIâ€™ll devote considerable time to this claim. But let me ï¬rst speak of the second\nway for the scientist to behave politically.\nOvert politics. A scientist can engage in overt politics through the mechanisms\nof activism and participatory democracy. In writing the Russellâ€“Einstein\n6 Joseph Rotblat, â€œRemember Your Humanity.â€ Acceptance and Nobel lecture, 1995.\nText available at Nobelprize.org\n7 The literature in this direction is too vast to possibly survey. University programs in\nthis space often go by the acronym STS, for science and technology studies or science,\ntechnology, and society. The work of Langdon Winner is particularly concerned with\nthe relation between technological artifacts and their implicit political dimension.\n8 The quote is from: Robin Williams and David Edge: The social shaping of technology.\nResearch Policy (25), 865â€“899, Elsevier Science B.V., (1966). The implicit reference\nis to the eerie Borges short story â€œEl jardÂ´Ä±n de senderos que se bifurcanâ€ (The Garden\nof Forking Paths) (1941).\n",
    "book_id": "the_moral_character_of_cryptographic_work",
    "book_title": "The Moral Character of Cryptographic Work",
    "book_author": "Unknown",
    "topic_id": "ai_policy",
    "topic_label": "policy",
    "chunk_index": 47
  },
  {
    "chunk_full": "4\nP. Rogaway\nmanifesto and in rolling it out the way he did, Russell was working masterfully in\nthis domain. Russell was not only a mathematician: he had broad contributions\nacross philosophy, had won the Nobel prize in literature, and was a well-known\nsocial critic and anti-war activist.\nThe ethic of responsibility. Bertrand Russellâ€™s breadth was extraordinary.\nBut the mere existence of the politically engaged intellectual doesnâ€™t suggest that\nthis pairing is at all representative. To what extent are scientists and engineers\nsocially engaged? And to what extent do societal norms demand that they be?9\nNowadays, an ethic of responsibility is preached in university courses and\nadvocated by professional organizations. It is the doctrinal view. The putative\nnorm contends that scientists and engineers have an obligation to select work\nthat promotes the social good (a positive right), or, at the very least, to refrain\nfrom work that damages mankind or the environment (a negative right).10 The\nobligation stems from three basic truths: that the work of scientists and engineers\ntransforms society; that this transformation can be for the better or for the worse;\nand that what we do is arcane enough that we bring an essential perspective to\npublic discourse. The socially engaged scientist is expected to bring a normative\nvision for how work in his or her ï¬eld should impact society. He or she aims to\nsteer things in that direction.\nTo be sure, decision making under the ethic of responsibility is not easy. It\ncan be impossible to foresee if a line of work is going to be used for good or for ill.\nAdditionally, the for-good-or-for-ill dichotomy can be simplistic and subjective to\nthe point of meaninglessness. Still, despite such diï¬ƒculties, the socially engaged\nscientist is supposed to investigate, think, and decide what work he will or will\nnot do, and what organizations he will or will not work for. The judgment should\nbe made without over-valuing ones own self-interest.\nHistorical events shaping the ethic of responsibility. The ascendancy of\nthe ethic of responsibility was shaped by three historical events of World War 2\nand its aftermath.\n1. The ï¬rst, already touched on, was the experience of the atomic scientists.\nAfter the war, with science left in a position both revered and feared, prominent\nphysicists became public ï¬gures. Some became outspoken in their advocacy for\npeace, or their opposition to further weapons development. Recall the widespread\nconcerns from physicists to Reaganâ€™s Strategic Defense Initiative (SDI)11 or\nHans Betheâ€™s famous letter to Bill Clinton where he argued against another\n9 Some of my comments in the remainder of this section were informed by Matthew\nWisnioski: Engineers for Change: Competing Visions of Technology in 1960s\nAmerica. MIT Press, 2012.\n10 While these two possibilities are very diï¬€erent, distinguishing between them will not\nbe important for the discussion of this essay.\n11 The debate within the scientiï¬c community got signiï¬cant media attention following\nrelease of a book-length study from the American Physical Society (APS) Study\nGroup (N. Bloembergen, C. K. Patel, cochairmen), Report to The American Physical\nSociety of the Study Group on Science and Technology of Directed Energy Weapons,\n",
    "book_id": "the_moral_character_of_cryptographic_work",
    "book_title": "The Moral Character of Cryptographic Work",
    "book_author": "Unknown",
    "topic_id": "ai_policy",
    "topic_label": "policy",
    "chunk_index": 48
  },
  {
    "chunk_full": "The Moral Character of Cryptographic Work\n5\nround of U.S. nuclear-weapons development.12 A willingness to speak truth to\npower 13 became a tradition among physicistsâ€”one that, I think, continues to\nshape physicistsâ€™ identity.14\nAs an example, recall the pepper-spray incident of 2011 at my own campus,\nthe University of California, Davis.15 Carrying out the Chancellorâ€™s instructions\nto clear â€œOccupyâ€ protesters, police oï¬ƒcer John Pike pepper-sprayed students\nwho sat, arms linked, on the universityâ€™s central quad. Videos of the event went\nviral,16 while memes of Oï¬ƒcer Pike casually pepper-spraying anything became\na second Internet sensation. But the observation Iâ€™d like to make is that, in the\naftermath of the incident, the only UCD department outside the humanities to\ncondemn the Chancellor or call for her resignation was Physics.17 The Chancellor\nwas mystiï¬ed. She understood the strong reaction from our (underfunded and\npolitically liberal) English department, but she didnâ€™t anticipate complaints\nfrom a (well-funded and generally conservative) Physics department.18 What\nthe Chancellor might not have internalized is that physicists retain a post-war\nlegacy not only of snuggling up close to power, but also of nipping at its ankles.\n2. A second historical event that helped shape the post-war view of\nmoral responsibility was the Nuremberg trials (1945â€“1946). While the defense\nrepeatedly proï¬€ered that the accused were simply following orders, this view\nwas almost universally rejected: following orders did not eï¬€ace legal or moral\nculpability. The Nuremberg trials began with the Medical Case, the prosecution\nof 23 scientists, physicians, and other senior oï¬ƒcials for gruesome and routinely\nfatal medical experiments on prisoners.19\nAPS, New York (April 1987). The debate was not one-sided: many physicists\nsupported SDI, and many felt the APS study to be too negative or too political.\n12 See http://fas.org/bethecr.htm#letter for Betheâ€™s 1995 letter.\n13 The well-known expression comes from a pamphlet â€œSpeak truth to power: a Quaker\nsearch for an alternative to violence,â€ 1955. http://www.quaker.org/sttp.html\n14 Of course not every prominent physicist was oppositional. Edward Teller famously\nchampioned the development of fusion bombs and SDI. These positions were divisive,\nwith some of Tellerâ€™s peers seeing him as irresponsible, jingoistic, or insane.\n15 The deï¬nitive report on the incident is: Reynoso, C., Blando, P., Bush, T., Herbert,\nP., McKenna, W., Rauchway, E., Balcklock, P., Brownstein, A., Dooley, D., Kolesar,\nK., Penny, C., Sterline, R., Sakaki, J.: UC Davis Nov. 18, 2011 â€œPepper Spray\nIncidentâ€ Task Force Report: The Reynoso Task Force Report. (March 2012)\nAvailable at http://www.ucsf.edu/sites/default/ï¬les/legacy ï¬les/reynoso-report.pdf\n(visited 2015.08.07)\n16 There are 2 million views of https://www.youtube.com/watch?v=6AdDLhPwpp4\n17 Nathan Brown: Op-ed: Reviewing the case for Katehiâ€™s resignation. Davis Enterprise\nnewspaper (Dec. 18, 2011). UC Davis Physics Department, untitled press release\n(Nov. 22, 2011), available at http://tinyurl.com/ucd-physics-pepper-spray (last\nvisited 2015.08.07).\n18 The Chancellor expressed this sentiment in a meeting between her and interested\nfaculty of the College of Engineering (date unknown, probably early 2012).\n19 See, for example, Israel Gutman: Encyclopedia of the Holocaust, Macmillan Library\nReference USA, 1990, entries: â€œMedical Experimentsâ€ by Nava Cohen, pp. 957â€“966,\nand â€œPhysicians, Naziâ€ by Robert Jay Lifton and Amy Hackett, pp. 1127â€“1132.\n",
    "book_id": "the_moral_character_of_cryptographic_work",
    "book_title": "The Moral Character of Cryptographic Work",
    "book_author": "Unknown",
    "topic_id": "ai_policy",
    "topic_label": "policy",
    "chunk_index": 49
  },
  {
    "chunk_full": "6\nP. Rogaway\nYears later, as though in sequel, the world would watch in nervous fascination\nthe trial of Adolf Eichmann (1961). Hannah Arendtâ€™s controversial portrayal\nof Eichmann would come to be formative in shaping our understanding of\nwhat, ethically, had transpired during the Holocaust. She wrote of the utter\nordinariness of the man.20 Arendtâ€™s book on the trial, memorably subtitled The\nBanality of Evil, would be published the same year (1963) as Stanley Milgramâ€™s\nclassical experiments on obedience, where Milgram produced the stunning (and\nwidely repeated) ï¬nding that a large fraction of volunteers would follow a white-\ncoated scientistâ€™s gentle urging to deliver apparently life-threatening shocks to\nsomeone they thought was a fellow test subject.21\n3. Finally, I would mention the rise of the environmental movement as\ncontributing to the rise of an ethic of responsibility. While environmentalism\ndates to the mid-nineteenth century and before, as a signiï¬cant social movement,\nthe 1962 publication of Rachel Carsonâ€™s Silent Spring is a milestone. Her book\npainted a picture of the end of life not by the drama of nuclear warfare, but\nthe disappearance of songbirds, silenced by the routine if oversized activities of\nchemical manufacturing and non-speciï¬c pesticides.\nThe good scientist. The three experiences I have just described implied a\ndemocratization of responsibility. Scientists had to assume responsibility for what\nthey did, for technology would take us to a very dark place if they did not.\nStripped of ethical restraint, science would bring a world of nightmare bombs,\ngas chambers, and macabre human experiments. It would bring a dying, poisoned\nworld.\nAnd so, in the decades following the war, the ethic of responsibility becameâ€”\nat least rhetoricallyâ€”the doctrinal norm. Increasing numbers of scientists and\nengineers, as well as their professional organizations, began to engage on\nissues of social responsibility. The Pugwash Conferences began in 1955. The\nNational Society of Professional Engineers adopted a code of ethics in 1964\nthat gave primacy to social responsibility. As its ï¬rst imperative, the code\nsays that â€œEngineers, in the fulï¬llment of their professional duties, shall hold\nparamount the safety, health, and welfare of the public.â€ Similar language\nwould spread across other codes of ethics, including those of the ACM and\nIEEE.22 The Union of Concerned Scientists was formed at MIT in 1969â€”the\nsame year a work stoppage at MIT, coordinated with 30 other universities,\nenjoyed substantial student, faculty, and administrative support. It called for\na realignment of research directions away from military pursuits and towards\nhuman needs. Computer Professionals for Social Responsibility (CPSR) began\n20 Hannah Arendt: Eichmann in Jerusalem: A Report on the Banality of Evil. Viking\nAdult (1963)\n21 Stanley Milgram: Behavioral study of obedience. Journal of Abnormal and Social\nPsychology, 67(4), pp. 371â€“378 (1963).\n22 In the US, only mining engineers failed to adopt a code of ethics, according to\nWikipedia article â€œEngineering ethics.â€\n",
    "book_id": "the_moral_character_of_cryptographic_work",
    "book_title": "The Moral Character of Cryptographic Work",
    "book_author": "Unknown",
    "topic_id": "ai_policy",
    "topic_label": "policy",
    "chunk_index": 50
  },
  {
    "chunk_full": "The Moral Character of Cryptographic Work\n7\nits work opposing the SDI in 1983.23 That same year, the IACR was founded, its\nself-described mission not only to advance the theory and practice of cryptology\nbut also, lest we forget, to serve the public welfare.24 The Electronic Frontier\nFoundation (EFF) and Privacy International (PI) were both formed in 1990, and\nbecame eï¬€ective advocates in such matters as the defeat of the Clipper Chip.\nAll of this is but a sampling of the overt politics from scientists and engineers.\nAgainst this backdrop, the ï¬gure of the brilliant but humane scientist become\na cultural motif. Jonas Salk had wiped out polio. Einstein became a cultural icon,\none unfazed by the inconvenience of his death. The image of him sticking out his\ntongue may be the most widely recognizable photograph of any scientist, ever.\nRichard Feynman would be painted in equally colorful ways, the no-nonsense\ngenius pounding on bongo drums and shoving black rubbery stuï¬€into ice water.\nGene Roddenberryâ€™s Star Trek imagined a future that featured the scientistâ€“\nhumanistâ€“hero as one team, if not one individual. Carl Sagan, speaking gently\nto the camera in episodes of Cosmos (1980), seemed the real-life embodiment of\nthis aspirational package.\nThe ethic of responsibility in decline. And yet, for all I have said, the\nscientist or engineer seriously concerned about the social impact of his work\nis, I think, so rare as to be nearly a matter of myth. Never during the cold\nwar, nor in any of the subsequent US wars, did US companies have diï¬ƒculty\nrecruiting or retaining the hundreds of thousands of scientists and engineers\nengaged in building weapons systems.25 Universities like my own were happy to\nadd their support; the University of California would, for decades, run the USAâ€™s\nnuclear weapons design laboratories.26 In nearly 20 years advising students at my\nuniversity, I have observed that a wish for right livelihood27 almost never ï¬gures\ninto the employment decisions of undergraduate computer science students. And\nthis isnâ€™t unique to computer scientists: of the ï¬ve most highly ranked websites\nI found on a Google search of deciding among job oï¬€ers, not one suggests\nconsidering the institutional goals of the employer or the social worth of what\nthey do.28\n23 The organization dissolved in 2013.\n24 Article II of the Bylaws of the International Association for Cryptologic Research:\nâ€œThe purposes of the IACR are to advance the theory and practice of cryptology\nand related ï¬elds, and to promote the interests of its members with respect\nthereto, and to serve the public welfare.â€ Last revised Nov. 18, 2013. Available\nat www.iacr.org/docs/bylaws.pdf\n25 See, for example, Thomas P. Hughes: Rescuing Prometheus, 1998 (on the Atlas\nproject).\n26 Following\nsome\nsecurity\nbreaches,\nthey\nnow\ndo\nthis\nin\npartnership\nwith\nindustry,\nparticularly\nwith\nBechtel.\nSee\nhttp://www.bechtel.com/projects/us-\nnational-laboratories/\n27 The term is associated to Buddhism, right livelihood being one of the virtues of the\nNoble Eightfold Path.\n28 My top ï¬ve Google search results to â€œdeciding among job oï¬€ersâ€ (no quotes)\non Aug 26, 2015 were: (1) â€œHelp! How Do I Choose Between Two Job Of-\n",
    "book_id": "the_moral_character_of_cryptographic_work",
    "book_title": "The Moral Character of Cryptographic Work",
    "book_author": "Unknown",
    "topic_id": "ai_policy",
    "topic_label": "policy",
    "chunk_index": 51
  },
  {
    "chunk_full": "8\nP. Rogaway\nNowadays I ask computer-science faculty candidates to explain their view on\nthe ethical responsibilities of computer scientists. Some respond like a deer in\nheadlights, unsure what such a question could even mean. One recent faculty\ncandidate, a data-mining researcher whose work seemed a compendium of DoD-\nfunded projects for socially reprehensible aims, admitted that she felt no social\nresponsibility. â€œI am a body without a soul,â€ she earnestly explained. It was\nsincereâ€”and creepy.\nStanley Fish, a well-known literary theorist, professor, and dean, admonishes\nfaculty not to pursue research programs rooted in values. (His 2012 book is titled\nSave the World on Your Own Time.) Fish advises professors to\ndo your job; donâ€™t try to do someone elseâ€™s job . . .; and donâ€™t let anyone else\ndo your job. In other words, donâ€™t confuse your academic obligations with the\nobligation to save the world; thatâ€™s not your job as an academic . . .\nMarx famously said that our job is not to interpret the world, but to change\nit. In the academy, however, it is exactly the reverse: our job is not to change\nthe world, but to interpret it.29\nPerhaps such amorality, however revolting, is harmless in Fishâ€™s intellectual\nrealm: one doesnâ€™t particularly expect literary theory to change the world. But\nscientists and engineers do just that. A refusal to direct the change we do is\nboth morally bankrupt and ingracious. Our work as academics, we should never\nforget, is subsidized by society.30\nSo far I have not said why the post-war ethic-of-responsibility didnâ€™t catch on.\nI could give multiple answers, starting with the rise of radical individualism.31\nBut I prefer to focus on something else: extreme technological optimism.\nfers.â€ CareerCast website, www.careercast.com/career-news/help-how-do-i-choose-\nbetween-two-job-oï¬€ers. (2) â€œ4 questions to help you decide between job oï¬€ers.â€\nNatalie Wearstler, Sep. 16, 2013. theweek.com/articles/460019/4-questions-help-\ndecide-between-job-oï¬€ers. (3) â€œYou Got the Jobs! How to Decide Between Oï¬€ers.â€\nForbes, June 6, 2011. www.forbes.com/sites/prettyyoungprofessional/2011/06/06/\nyou-got-the-jobs-how-to-decide-between-oï¬€ers/. (4) â€œ6 Secrets to Choosing Between\nJob Oï¬€ers.â€ July 31, 2013. www.aegistech.com/how-to-choose-between-multiple-\njob-oï¬€ers/. (5) â€œHow to Choose Between Multiple Job Oï¬€ers.â€ Sam Tomarchio,\nAegistech. www.aegistech.com/how-to-choose-between-multiple-job-oï¬€ers/. The in-\ndicated websites do not contain the words ethics, moral, or social, nor any\ncomments concerning the ethical character or social value of ones work. The author\nacknowledges that Google search results are not reproducible.\n29 Stanley Fish: Why we built the ivory tower. NY Times, Opinion Section, May 21,\n2004.\n30 Elisabeth Pain: The social responsibility of scientists. Career Magazine of Science,\nFeb. 16, 2013. Report on the 2013 AAAS meeting and, in particular, the comments of\nMark Frankel. The speaker asks listeners, especially graduate students, to keep ï¬rmly\nin mind that scientiï¬c research is a social institution, and one that is subsidized by\nsociety.\n31 Radical individualism is the belief that ones personal interests are more important\nthan those of society. It is well captured by the â€œGreed is Goodâ€ speech from\nOliver Stoneâ€™s â€œWall Streetâ€ (1987), although what the individual cares about can\nbe something other than personal wealth.\n",
    "book_id": "the_moral_character_of_cryptographic_work",
    "book_title": "The Moral Character of Cryptographic Work",
    "book_author": "Unknown",
    "topic_id": "ai_policy",
    "topic_label": "policy",
    "chunk_index": 52
  },
  {
    "chunk_full": "The Moral Character of Cryptographic Work\n9\nTechnological optimism. Technological optimists believe that technology\nmakes life better. According to this view, we live longer, have more freedom,\nenjoy more leisure. Technology enriches us with artifacts, knowledge, and\npotential. Coupled with capitalism, technology has become this extraordinary\ntool for human development. At this point, it is central to mankindâ€™s mission.\nWhile technology does bring some unintended consequences, innovation itself\nwill see us through.\nTechnological optimism animates everyone from school children to Turing\nAward winners. Accepting his 2012 Turing Award, Silvio Micali, said that\nComputer science is marking an epical change in human history. We are\nconquering a new and vast scientiï¬c continent. . . . Virtually all areas of human\nactivity . . . [and] virtually all areas of human knowledge . . . are beneï¬ting from\nour conceptual and technical contributions. . . . Long live computer science!32\nIf youâ€™re a technological optimist, a rosy future ï¬‚ows from the wellspring\nof your work. This implies a limitation on ethical responsibility. The important\nthing is to do the work, and do it well. This even becomes a moral imperative,\nas the work itself is your social contribution.\nBut what if computer science is not beneï¬ting man? Technological pessimists\nlike Jacques Ellul, Herbert Marcuse, and Lewis Mumford certainly didnâ€™t think\nthat it was. They saw modern technology as an interlocking, out-of-control\nsystem that, instead of fulï¬lling human needs, engendered pointless wants and\ndeadlier weapons. Man is becoming little more than the sex organs of the machine\nworld.33\nTaking a less â€œextremeâ€ view,34 technological contextualists35 acknowledge\nthe concerns of the pessimists, but emphasize manâ€™s essential agency and the\n32 Silvio Micali, ACM Turing Award Acceptance speech. ACM awards ceremony, San\nFrancisco, June 15, 2013. https://www.youtube.com/watch?v=W0N4WnjGHwQ.\nEnding his speech with a cheer, there is, perhaps, an element of facetiousness in\nSilvioâ€™s unbridled optimism. Yet over-the-top optimism seems endemic to discourse\non computer science.\nThe awards ceremony took place a little over a week after the ï¬rst news story based\non a Snowden document (June 6, 2013). But the threat of computing technology was\nnot to be acknowledged, only its promise praised.\n33 Understanding Media, 1964: â€œMan becomes, as it were, the sex organs of the machine\nworld, as the bee of the plant world, enabling it to fecundate and evolve ever new\nforms.â€\n34 To be clear, I do not think thereâ€™s anything extreme in the views of prominent\ntechnological pessimists; if anything, prevailing optimistic views seems to me far less\nreasoned and more extreme.\n35 This is the phrase used by Ian Barbour: Ethics in an Age of Technology: The Giï¬€ord\nLectures, Volume Two), HarperCollins (1993), which contains a good description\nof the writings of the contextualists (which include the author himself). Langdon\nWinner speaks of an ideology of technological politics for roughly the same concept\nas contextualism; see Autonomous Technology: Technics-out-of-Control as a Theme\nin Political Thought. MIT Press, 1977.\n",
    "book_id": "the_moral_character_of_cryptographic_work",
    "book_title": "The Moral Character of Cryptographic Work",
    "book_author": "Unknown",
    "topic_id": "ai_policy",
    "topic_label": "policy",
    "chunk_index": 53
  },
  {
    "chunk_full": "10\nP. Rogaway\nmalleability of technology. Contextualism dominates the dialectic of technology\nstudies.\nThe ethic of responsibility is always paired with the contextualist view of\nsociotechnology. At some level, this must be so: a normative need vanishes if, in\nthe garden of forking paths, all paths lead to good (or, for that matter, to bad).\nBut it is technological optimism that most people buy into, especially scientists\nand engineers. And unbridled technological optimism undermines the basic need\nfor social responsibility.\nConclusion to part 1. Ultimately, I think the post-war turn towards social\nresponsibility in science and engineering was less a turn than a sideways glance.\nWhile the rhetoric of responsibility would provide cover from technologyâ€™s critics,\nfew scientists or engineers would ever come to internalize that their work\nembodied socially relevant values. If researchers like us were actually supposed\nto know or care about this stuï¬€in any operationally signiï¬cant way, well, I think\nwe didnâ€™t get the memo.\nSo let me retransmit it. It says that your moral duties extend beyond the\nimperative that you personally do no harm: you have to try to promote the social\ngood, too. Also, it says that your moral duties stem not just from your stature\nas a moral individual, but, also, from the professional communities to which you\nbelong: cryptographer, computer scientist, scientist, technologist.\nWith few exceptions, the atomic scientists who worked on disarmament\nwere not the same individuals as those who built the bomb. Their colleaguesâ€”\nfellow physicistsâ€”did that. Cryptographers didnâ€™t turn the Internet into an\ninstrument of total surveillance, but our colleaguesâ€”fellow computer scientists\nand engineersâ€”did that. And cryptographers have some capacity to help.\nBut you will only believe that claim if you recognize that cryptography can\ninï¬‚uence power relations. I suspect that many of you see no real connection\nbetween social, political, and ethical values and what you work on. You donâ€™t\nbuild bombs, experiment on people, or destroy the environment. You donâ€™t spy\non populations. You hack math and write papers. This doesnâ€™t sound ethically\nladen. I want to show you that it is.\nPart 2: The political character of cryptographic work\nScientist or spy? Thereâ€™s an irony in discussing the claim that cryptographic\nwork is political, and it is this: to someone unconnected to the ï¬eld, and also\nto the crypto-hobbyist, the claim may seem obviously true. But the young\nresearcher who spends his life writing papers in cryptography, the claim may\nseem just as obviously false. What gives?\nThe outsiderâ€™s view of cryptography might be based on cinematic portrayals.\nFilms like Sneakers (1992), Pi (1998), A Beautiful Mind (2001), Enigma (2001),\nTraveling Salesman (2012), Citizenfour (2014), and The Imitation Game (2014)\ndepict cryptography as a ï¬eld intertwined with politics. Cryptographers are the\nbrilliant and handsome mathematicians that power needs to have working on its\n",
    "book_id": "the_moral_character_of_cryptographic_work",
    "book_title": "The Moral Character of Cryptographic Work",
    "book_author": "Unknown",
    "topic_id": "ai_policy",
    "topic_label": "policy",
    "chunk_index": 54
  },
  {
    "chunk_full": "The Moral Character of Cryptographic Work\n11\nside. We are, I am happy to report, heroic geniuses. A little crazy, to be sure,\nbut that just adds to the luster.\nSimilarly, the crypto hobbyist may have read historical accounts dealing with\ncryptography, like the books of James Bamford or David Kahn.36 Such accounts\ndemonstrate that, historically, cryptography is about power. Itâ€™s a realm in\nwhich governments spend enormous sums of money,37 and maybe not unwisely:\nthe work shapes the outcome of wars, and undergirds diplomatic and economic\nmaneuvering.38\nYet no academic cryptographer would confuse historical or ï¬ctional accounts\nof cryptography with what we actually do. Our discipline investigates academic\nproblems that fall within our disciplinary boundaries. Pick up a Springer\nproceedings or browse ePrint papers and our ï¬eld looks utterly non-political.\nIf power is anywhere in the picture, it is in the abstract capacities of notional\nadversaries39 or, in a diï¬€erent branch of our ï¬eld, the power expenditure,\nmeasured in watts, for some hardware. We work on problems that strike us as\ninteresting or scientiï¬cally important. Weâ€™re not aiming to advance the interests\nof anything but science itself (or, perhaps, oneâ€™s own career).\nSo distinct claims about cryptographyâ€™s connectedness to power stem, at least\nin part, from radically diï¬€erent archetypes of what the cryptographer is: scientist\nor spy. The NSA/GCHQ employee who hacks Gemalto to implant malware and\nsteal SIM keys40 is every bit as deserving of being called a â€œcryptographerâ€ as\nthe MIT-trained theorist who devises a new approach for functional encryption.\nBoth are dealing in questions of privacy, communications, adversaries, and clever\ntechniques, and we would do well to emphasize these commonalities if we want\nto see our disciplinary universe in context or nudge it towards greater relevance.\nAcademic cryptography used to be more political. The ascendance of\na new cryptographer archetypeâ€”the academic cryptographerâ€”fails to really\n36 The series of books by James Bamford on the history of the NSA are The Puzzle\nPalace: A Report on Americaâ€™s Most Secret Agency (1982), Body of Secrets:\nAnatomy of the Ultra-Secret National Security Agency (2001), and The Shadow\nFactory: The NSA from 9/11 to the Eavesdropping on America (2008). The classic\nbook of David Kahn on the history of code breaking is The Codebreakers: The Story\nof Secret Writing (1967).\n37 The U.S. â€œConsolidated Cryptologic Programâ€ includes about 35,000 employees.\nNSA\nbudget\nfor\n2013\nwas\n$10.3\nbillion,\nwith\n$1\nbillion\nof\nthis\nmarked\nas â€œcryptanalysis and exploitation servicesâ€ and $429 million â€œresearch and\ntechnology.â€ intelligence operations between 2004â€“2013. See Barton Gellman and\nGreg Miller, â€˜Black budgetâ€™ summary details U.S. spy networkâ€™s successes, failures,\nand objectives; The Washington Post, April 29, 2013.\n38 For an account of the extreme utility of surveillance and cryptography in recent U.S.\nwars, see Shane Harris: @War: The Rise of the Military-Internet Complex. Eamon\nDolan/Houghton Miï¬„in Harcourt (2014).\n39 For example, the phrase â€œan all-powerful adversaryâ€ is usually meant as an agent\nhaving no computational restrictions, but which conforms to a speciï¬ed model.\n40 Jeremy Scahill and Josh Begley: The great SIM heist: how spies stole the keys to\nthe encryption castle. The Intercept, Feb. 19, 2015.\n",
    "book_id": "the_moral_character_of_cryptographic_work",
    "book_title": "The Moral Character of Cryptographic Work",
    "book_author": "Unknown",
    "topic_id": "ai_policy",
    "topic_label": "policy",
    "chunk_index": 55
  },
  {
    "chunk_full": "12\nP. Rogaway\nexplain our politically detached posture. For one thing, academic cryptographers\nwere once more concerned with our ï¬eldâ€™s sociopolitical dimensions. Some even\ncame to cryptography for such reasons. Consider, for example, this fragment of\nWhit Diï¬ƒeâ€™s testimony at the Newegg trial. Speaking of his wife, Diï¬ƒe says:\nI told her that we were headed into a world where people would have important,\nintimate, long-term relationships with people they had never met face to face.\nI was worried about privacy in that world, and thatâ€™s why I was working on\ncryptography.41\nDiï¬ƒe and his advisor, Martin Hellman, have long evinced a concern for\nsociopolitical problems touching technology. You see it in their criticism of DESâ€™s\nkey length,42 in Hellmanâ€™s activism on nuclear disarmament,43 in Diï¬ƒeâ€™s book\non the politics of wiretapping with Susan Landau,44 and in his co-invention of\nforward secrecy.45 You see it in the New Directions paper:46 when the authors\nboldly begin â€œWe stand today on the brink of a revolution in cryptography,â€ the\nanticipated revolution was not, at least primarily, the theory community bringing\nforth mind-altering notions of provable security, simulatability, or multiparty\ncomputation.47 The authors were interested in technological changes that were\ntranspiring, and concomitant social opportunities and needs.48\nStill more ostensibly political is David Chaumâ€™s body of scientiï¬c work,\nwhich thoroughly embeds concerns for democracy and individual autonomy.\n41 Joe Mulllin: Newegg trial: crypto legend takes the stand, goes for knockout patent\npunch. Ars Technica, Nov. 24, 2013.\n42 Whitï¬eld Diï¬ƒe and Martin Hellman: Exhaustive cryptanalysis of the NBS Data\nEncryption Standard. Computer 10(6), pp. 74â€“84 (1977).\n43 See, for example: Anatoly Gromyko and Martin Hellman: Breakthrough: Emerging\nNew Thinking: Soviet and Western Scholars Issue a Challenge to Build a World\nBeyond War, 1988.\n44 Whitï¬eld Diï¬ƒe and Susan Landau: Privacy on the Line: The Politics of Wiretapping\nand Encryption, MIT Press (2007).\n45 Whitï¬eld\nDiï¬ƒe,\nPaul\nvan\nOorschot,\nMichael\nWiener:\nAuthentication\nand\nauthenticated key exchanges. Designs, Codes and Cryptography, 2(2), pp. 107â€“125\n(1992).\n46 Whitï¬eld Diï¬ƒe and Martin Hellman: New directions in cryptography. IEEE\nTransactions on Information Theory, 22(6), pp. 644-654 (1976).\n47 Yet the authors did seem to anticipate such a possibility: â€œAt the same time,\ntheoretical developments in information theory and computer science show promise\nof providing provably secure cryptosystems, changing this ancient art into a science.â€\nIbid., p. 29.\n48 They write, for example, that â€œThe development of cheap digital hardware has freed\nit [cryptography] from the design limitations of mechanical computing and brought\nthe cost of high grade cryptographic devices down to where they can be used in such\ncommercial applications as remote cash dispensers and computer terminals. . . . The\ndevelopment of computer controlled communication networks promises eï¬€ortless and\ninexpensive contact between people or computers on opposite sides of the world . . ..â€\nIbid, p. 29.\n",
    "book_id": "the_moral_character_of_cryptographic_work",
    "book_title": "The Moral Character of Cryptographic Work",
    "book_author": "Unknown",
    "topic_id": "ai_policy",
    "topic_label": "policy",
    "chunk_index": 56
  },
  {
    "chunk_full": "The Moral Character of Cryptographic Work\n13\nChaumâ€™s 1981 paper49 Untraceable Electronic Mail, Return Addresses, and\nDigital Pseudonyms, [Chaum81], suggests that a crucial privacy goal when\nsending an email is to hide who is communicating with whom. The metadata, in\nmodern political parlance. The author oï¬€ered mix nets for a solution.50\nChaum would go on to provide the founding ideas for anonymous electronic\ncash and electronic voting. His papers would routinely draw on overtly political\nmotivations.51 In a recent conversation, Chaum expressed surprise at the extent\nto which academics gravitated to a ï¬eldâ€”cryptographyâ€”so connected to issues\nof power.52\nStripping out the politics. But as academics gravitated to cryptography, they\ntended to sanitize it, stripping it of ostensible connectedness to power. Applied\nand privacy-related work drifted outside of the ï¬eldâ€™s core venues, the IACR\nconferences. It is as though a chemical synthesis would take place, transforming\nthis powerful powder into harmless dust.\nConsider that there is now a conference named â€œReal World Cryptographyâ€\n(RWC).53 There is humorâ€”but maybe gallows humorâ€”that a ï¬eld with a genesis\nand capability as real-world as ours should ï¬nd reason to create a venue so\n49 The paper appears in Communications of the ACM (CACM), 24(2), pp. 84â€“88\n(1981).\n50 In a nutshell, a mix net works like this. Each userâ€™s communications will pass through\na series of routers. (For the approach to add value, these routers, or mixes, should\nhave diversity in administrative control, jurisdictional control, or the code they run.)\nPlaintexts will be multiply encrypted, using one key for each router in the series.\nEach router will peel oï¬€one layer of encryption. Before passing the result to the\nnext router, it will wait until it has some number of outgoing messages, and then\nreorder them. An adversary who observes network traï¬ƒcâ€”even one who controls\nsome subset of the routersâ€”should be unable to identify who has sent what to\nwhom.\n51 For example: â€œThe foundation is being laid for a dossier society, in which computers\ncould be used to infer individualsâ€™ life-styles, habits, whereabouts, and associations\nfrom data collected in ordinary consumer transactions. Uncertainty about whether\ndata will remain secure against abuse by those maintaining or tapping it can have\na â€˜chilling eï¬€ect,â€™ causing people to alter their observable activities.â€ David Chaum:\nSecurity without identiï¬cation: transaction systems to make big brother obsolete.\nCommunications of the ACM (CACM), 28(10), pp. 1030â€“1044, 1985.\n52 Speaking of cryptography, Chaum told me â€œIâ€™ve been aware from the beginning of the\nï¬eld, that it was a powerful ï¬eld, and would become more and more as information\ntechnology advanced. I was surprised that academics were as interested as they were\nin cryptography, because it was an area so connected to power.â€ Conversation with\nDavid Chaum, 17 Aug 2015. Santa Barbara, California.\n53 Starting in 2012, this annual event already attracts more attendees than Crypto.\nRWCâ€™s steering committee is Dan Boneh, Aggelos Kiayias, Brian LaMacchia,\nKenny Paterson, Tom Ristenpart, Tom Shrimpton, and Nigel Smart. Webpage\nhttp://www.realworldcrypto.com/ explains: â€œThis annual conference aims to bring\ntogether cryptography researchers with developers implementing cryptography in\nreal-world systems. The conference goal is to strengthen the dialog between these\n",
    "book_id": "the_moral_character_of_cryptographic_work",
    "book_title": "The Moral Character of Cryptographic Work",
    "book_author": "Unknown",
    "topic_id": "ai_policy",
    "topic_label": "policy",
    "chunk_index": 57
  },
  {
    "chunk_full": "14\nP. Rogaway\nnamed.54 Ask a colleague in Graphics or Cloud Computing how it would ï¬‚y in\ntheir community if someone started a conference called Real World Computer\nGraphics (RWCG 2015) or Real World Cloud Computing (RWCC 2016). They\nwill laugh.\nAn especially problematic excision of the political is the marginalization\nwithin the cryptographic community of the secure-messaging problem,55 an\ninstance of which was the problem addressed by [Chaum81]. Secure-messaging\nis the most fundamental privacy problem in cryptography: how can parties\ncommunicate in such a way that nobody knows who said what. More than a\ndecade after the problem was introduced, Rackoï¬€and Simon would comment on\nthe near-absence of attention being paid to the it.56 Another 20-plus years later,\nthe situation is this: there is now a mountain of work on secure-messaging, but\nitâ€™s unclear what most of it actually does. A recent systemization-of-knowledge\narticle57 paints a picture of a cryptographic task enjoying a ï¬‚ourishing of ad hoc\nsolutions, but little of it arising from the cryptographic community, as narrowly\nconstrued, or tied to much theory.58 While one could certainly claim that this is\ntrue for almost all practical security goals that employ cryptography, I think the\ncase is diï¬€erent for secure-messaging: here the work feels almost intentionally\npushed aside.\ntwo communities. Topics covered focus on uses of cryptography in real-world\nenvironments such as the Internet, the cloud, and embedded devices.â€\n54 Some theorists might take oï¬€ense by an implication that cryptographic theory is\nnot â€œreal world,â€ suggesting that those who do theory must be studying something\nunreal, or not of this world (the vapors of poltergeists?). The implicit dichotomies\nof cryptographic theory vs. cryptographic practice, real-world and its complement\n(whatever that might be), would not survive a critical assessment. Yet discourse on\nwhat communities do seems unavoidably full of capricious boundaries.\n55 Tellingly, the problem lacks even a widely recognized name. The version of the secure-\nmessaging problem Chaum was interested in is a high-latency, public-key version.\n56 Charles Rackoï¬€and Daniel Simon: Cryptographic defense against traï¬ƒc analysis.\nSTOC â€™93, pp. 672â€“681. The quote is from pp. 672â€“673.\n57 Nik Unger, Sergej Dechand, Joseph Bonneau, Sascha Fahl, Henning Perl, Ian\nGoldberg, Matthew Smith: SoK: Secure messaging. IEEE Symposium on Security\nand Privacy 2015, pp. 232â€“249.\n58 Little, but not none. See, for example: Nik Unger and Ian Goldberg: Deniable\nkey exchanges for secure messaging. ACM CCS 2015. N. Borisov, I. Goldberg,\nand E. Brewer: Oï¬€-the-Record communication, or, why not to use PGP. Privacy\nin the Electronic Society, pp. 77-84, 2004. Ron Berman, Amos Fiat, Marcin\nGomulkiewicz, Marek Klonowski, Miroslaw Kutylowski, Tomer Levinboim, Amnon\nTa-Shma: Provable unlinkability against traï¬ƒc analysis with low message overhead.\nJ. of Cryptology, 28(3), pp. 623â€“640, 2015. Jan Camenisch and Anna Lysyanskaya:\nA formal treatment of onion routing. CRYPTO 2005. Joan Feigenbaum, Aaron\nJohnson, and Paul F. Syverson: Probabilistic analysis of onion routing in a black-box\nmodel. ACM Transactions on Information and System Security (TISSEC), 15(3),\n2012.\n",
    "book_id": "the_moral_character_of_cryptographic_work",
    "book_title": "The Moral Character of Cryptographic Work",
    "book_author": "Unknown",
    "topic_id": "ai_policy",
    "topic_label": "policy",
    "chunk_index": 58
  },
  {
    "chunk_full": "The Moral Character of Cryptographic Work\n15\nChildren of [Chaum81] and [GM82]. Why would I make such a claim?\nAn illuminating case study is provided by comparing the venues of the most\ncited papers citing [Chaum81] and [GM82], Goldwasser and Micaliâ€™s Probabilistic\nEncryption.59 The two papers appeared around the same time and have\ncomparable citation counts.60\nThe [GM82] paper put forward the deï¬nitionally centered, reduction-based\napproach to dealing with cryptographic problems. It became a seminal work\nof the cryptographic community. The most cited papers that cite it appear in\nCrypto and Eurocrypt, FOCS, STOC, and ACM CCS.61 The [Chaum81] paper\nput forward the secure email problem and suggested a solution. This paper would\nbe just as seminalâ€”but in spawning work mostly outside the core cryptographic\ncommunity. The ten most cited papers that cite Chaumâ€™s paper appear in venues\nthat, mostly, I had never heard of. Venues not crypto-focused, like MobiSys and\nSIGOPS. In fact, the venues for the ten most cited papers citing [GM82] and the\nvenues for the ten most cited papers citing [Chaum81] have void intersection. I\nï¬nd this fairly remarkable. It reï¬‚ects a research community split into fragments\nthat include a GM-derived one and a Chaum-derived one, the second fragment\nnot really being a part of the cryptographic community at all.62\nWhy did this fragmentation occur? The most obvious explanation has\nto do with rigor: [GM82] oï¬€ered a mathematically precise approach to its\nsubject, while [Chaum81] did not. So a partitioning might seem to make sense:\ncryptographic work that can be mathematically formal goes to the right; ad hoc\nstuï¬€, over to the left.\nThe problem with this explanation is that itâ€™s wrong. The [Chaum81] paper\nsupports rigor just ï¬ne. Indeed provable security would eventually catch up to\nmix nets, although the ï¬rst deï¬nition would take more than 20 years to appear\n(2003), in a paper by Abe and Imai.63 That the [Chaum81] paper itself didnâ€™t\n59 Chaum, op. cit. Shaï¬Goldwasser and Silvio Micali: Probabilistic encryption and how\nto play mental poker keeping secret all partial information. STOC 1982, pp. 365â€“\n377. Journal version as: Probabilistic encryption, Journal of Computer and System\nScience (JCSS), 28(2), pp. 270â€“299, April 1984.\n60 According to Google scholar: 4481 citations to the Chaum paper, and 3818 citations\nfor the Goldwasser-Micali paper (both versions, combined). Data as of Oct 14, 2015.\n61 The one outlier is a paper by Perrig, Szewczyk, Tygar, Wen, and Culler in MobiCom.\n62 In greater detail, the ten most cited [GM82] (both versions combined) appeared at\nCCS, Crypto 3Ã—, Eurocrypt, FOCS (2Ã—) MobiCom, STOC (2Ã—). Only MobiCom\nis an â€œunexpectedâ€ venue. The ten most cited [Chaum81]-citing papers appeared at\nACM Comp. Surveys, ACM J. of Wireless Networks, ACM MobiSys, ACM Tran.\non Inf. Sys., ACM SIGOPS, IEEE SAC, Proc. of the IEEE USENIX Security\nSymposium, and workshops named IPTPS and Designing Privacy Enhancing\nTechnologies. None of these ten venues is crypto-focused.\n63 Masayuki Abe and Hideki Imai: Flaws in some robust optimistic mix-nets.\nInformation Security and Privacy (ACISP), pp. 39â€“50, 2003. Journal version:\nMasayuki Abe and Hideki Imai: Flaws in robust optimistic mix-nets and stronger\nsecurity notions. IEICE Trans. on Fundamentals of Electronics, Communications\nand Computer Science, vol. E89A, no. 1, pp. 99â€“105, Jan 2006.\n",
    "book_id": "the_moral_character_of_cryptographic_work",
    "book_title": "The Moral Character of Cryptographic Work",
    "book_author": "Unknown",
    "topic_id": "ai_policy",
    "topic_label": "policy",
    "chunk_index": 59
  },
  {
    "chunk_full": "16\nP. Rogaway\nprovide a formal treatment says nothing about the formalizability of the problem\nor what communities would later embrace it; after all, Diï¬ƒe and Hellmanâ€™s\npaper64 only informally described trapdoor permutations, public-key encryption,\nand digital signatures, but all would be absorbed into the cryptographic fold.\nNow one might well counter that the problem addressed by [Chaum81] is more\ndiï¬ƒcult to formalize than any of the examples just named. Thatâ€™s true. But\nitâ€™s simpler to formalize than MPC,65 say, which would quickly gain entrÂ´ee and\nstature in the cryptographic communityâ€”even without deï¬nitions or proofs. So,\nultimately, neither formalizability nor complexity goes far to explain why secure-\nmessaging has been marginalized.\nA better answer (but by no means the only answer) is made obvious by\ncomparing the introductions to the most-cited papers citing [GM82] and the\nmost-cited papers citing [Chaum81]. Papers citing [GM82] frame problems\nscientiï¬cally. Authors claim to solve important technical questions. The tone is\nassertive, with hints of technological optimism. In marked contrast, papers citing\n[Chaum81] frame problems socio-politically. Authors speak about some social\nproblem or need. The tone is reserved, and explicitly contextualist views are\nroutine. One observes the exact same distinction in tone and stated motivations\nwhen comparing survey articles.66\nIn 2015, I attended PETS (Privacy Enhancing Technologies Symposium) for\nthe ï¬rst time. Listening to people in this community interact is a bit like watching\nthe cryptographic community through a lens that magically inverts most things.\nThe PETS community attends closely to the values embedded in work. They\ncare about artifacts that support human values. They aim to serve the users\nof those artifacts. Theyâ€™re deeply concerned with the politics and technology of\nsurveillance. Where, after Chaum, did the moral soul of academic cryptography\ngo? Maybe it moved to PETS.\nThere is a lesson in all this. Some might think that a communityâ€™s focus is\nmostly determined by the technical character of the topic it aims to study. It is\nnot. It is extra-scientiï¬c considerations that shape what gets treated where.\n64 Diï¬ƒe and Hellman, op. cit.\n65 Multiparty computation. Andrew Chi-Chih Yao: Protocols for secure computations\n(Extended Abstract). FOCS 1982, pp. 160â€“164. Oded Goldreich, Silvio Micali,\nAvi Wigderson: How to play any mental game or a completeness theorem for\nprotocols with honest majority. STOC 1987, pp. 218â€“229. Michael Ben-Or, Shaï¬\nGoldwasser, Avi Wigderson: Completeness theorems for non-cryptographic fault-\ntolerant distributed computation (extended abstract). STOC 1988, pp. 1â€“10.\nDavid Chaum, Claude CrÂ´epeau, Ivan DamgËšard: Multiparty unconditionally Secure\nprotocols (extended abstract). STOC 1988, pp. 11â€“19.\n66 For example, compare the opening pages of: Oded Goldreich: Foundations of\ncryptographyâ€”a primer. Foundations and Trends in Theoretical Computer Science,\n1(1), pp. 1â€“116, 2004. And: George Danezis and Seda GÂ¨urses: A critical review\nof 10 years of privacy technology. Proceedings of Surveillance Cultures: A Global\nSurveillance Society?, 2010.\n",
    "book_id": "the_moral_character_of_cryptographic_work",
    "book_title": "The Moral Character of Cryptographic Work",
    "book_author": "Unknown",
    "topic_id": "ai_policy",
    "topic_label": "policy",
    "chunk_index": 60
  },
  {
    "chunk_full": "The Moral Character of Cryptographic Work\n17\nThe cypherpunks. Now there is a group that has long worked at the nexus\nof cryptography and politics: the cypherpunks.67 The cypherpunks emerged in\nthe late 1980â€™s, uniï¬ed by a mailing list and some overlapping values. The core\nbelief is that cryptography can be a key tool for protecting individual autonomy\nthreatened by power.68\nThe cypherpunks believed that a key question of our age was whether state\nand corporate interests would eviscerate liberty through electronic surveillance\nand its consequences, or if, instead, people would protect themselves through the\nartful use of cryptography. The cypherpunks did not seek a world of universal\nprivacy: many wanted privacy for the individual, and transparency for the\ngovernment and corporate elites. The cypherpunks envisioned that one could\nhack power relations by writing the right code. Cypherpunk-styled creationsâ€”\nthink of Bitcoin, PGP, Tor, and WikiLeaksâ€”were to be transformative because\nthey challenge authority and address basic freedoms: freedom of speech,\nmovement, and economic engagement.69\nExactly how such tools are to shape society is not always obvious. Consider\nWikiLeaks. The hope is not just that a better informed public will demand\naccountability and change. Rather, Assange sees governmental and corporate\nabuse as forms of conspiracy that could be throttled by the mere threat of leaks.\nConspiracies are like graphs, the conspirators nodes, the pairwise relations among\nthem, the edges. Instead of removing nodes or disrupting links, you can weaken\nany conspiracy by suï¬€using it in an ever-present threat of leaks. The more unjust\nthe conspiracy, the more likely leaks will occur, and the more damage they will\ndo. As elites become fearful to conspire, they do so reservedly. The conspiratorial\ncreatureâ€™s blood thickens and it dies.70 It is a fascinating vision.\nIt is cypherpunks, not cryptographers, who are normally the strongest\nadvocates for cryptography. Julian Assange writes:\n67 Andy Greenberg: This Machine Kills Secrets: How WikiLeakers, Cypherpunks,\nand Hacktivists Aim to Free the Worldâ€™s Information. Dutton, 2012. Steven Levy:\nCrypto: How the Code Rebels Beat the Government Saving Privacy in the Digital\nAge. Viking Adult, 2001. Julian Assange, Jacob Appelbaum, Andy MÂ¨uller-Maguhn,\nJÂ´erÂ´emie Zimmermann: Cypherpunks: Freedom and the Future of the Internet. OR\nBooks, 2012. Robert Manne: The cypherpunk revolutionary: on Julian Assange. The\nMonthly: Australian Politics, Society, & Culture. March 2011.\n68 It is the same belief embedded in the work already discussed by David Chaum.\n69 I have heard academic cryptographers saying that we deserve credit for these\nartifacts, for, regardless of who wrote the code, the ideas stem from cryptographic\nnotions that come from us. While there is some truth to this, the claim still feels\nungenuine. The work, in each case, arose from somewhere else, employed only old\nand basic tools, and was at most modestly embraced by our community.\n70 Julian\nAssange:\nConspiracy\nas\ngovernance.\nManuscript,\nDecember\n3,\n2006.\nhttp://cryptome.org/0002/ja-conspiracies.pdf. Finn Brunton: Keyspace: reï¬‚ections\non WikiLeaks and the Assange papers. Radical Philosophy 166, pp. 8â€“20, Mar/Apr\n2011.\n",
    "book_id": "the_moral_character_of_cryptographic_work",
    "book_title": "The Moral Character of Cryptographic Work",
    "book_author": "Unknown",
    "topic_id": "ai_policy",
    "topic_label": "policy",
    "chunk_index": 61
  },
  {
    "chunk_full": "18\nP. Rogaway\nBut we discovered something. Our one hope against total domination. A hope\nthat with courage, insight and solidarity we could use to resist. A strange\nproperty of the physical universe that we live in.\nThe universe believes in encryption.\nIt is easier to encrypt information than it is to decrypt it.\nWe saw we could use this strange property to create the laws of a new\nworld.71\nSimilarly, Edward Snowden writes:72\nIn words from history, let us speak no more of faith in man, but bind him\ndown from mischief by the chains of cryptography.73\nWhen I ï¬rst encountered such discourse, I smugly thought the authors were\nway over-promising: they needed to tone down this rhetoric to be accurate. I\nno longer think this way. More engaged in implementing systems than Iâ€™ll ever\nbe, top cypherpunks understand more than I about insecure operating systems,\nmalware, programming bugs, subversion, side channels, poor usability, small\nanonymity sets, and so on. Cypherpunks believe that despite such obstacles,\ncryptography can still be transformative.\nCryptography favors whom? Cypherpunk discourse seems sometimes to\nassume that cryptography will beneï¬t ordinary people. But one has to be careful\nhere. Cryptography can be developed in directions that tend to beneï¬t the weak\nor the powerful. It can also be pursued in ways likely to beneï¬t nobody but the\ncryptographer. Letâ€™s look at some examples.\nEncryption. One reason people might assume cryptography to beneï¬t the weak\nis that theyâ€™re thinking of cryptography as conventional encryption. Individuals\nwith minimal resources can encrypt plaintexts in a manner that even a state-level\nadversary, lacking the key, wonâ€™t be able to decrypt.\nBut does it necessarily come out that way? To work, cryptographic primitives\nmust be embedded into systems, and those systems can realize arrangements of\npower that donâ€™t trivially ï¬‚ow from the nature of the tool. In his typically pithy\nway, Schneier reminds people that â€œencryption is just a bunch of math, and math\nhas no agency.â€74 If a content-provider streams an encrypted ï¬lm to a customer\nwho holds the decryption key locked within a hardware or software boundary\nshe has no realistic ability to penetrate,75 weâ€™ve empowered content providers,\n71 Julian Assange: Cypherpunks, op. cit.\n72 I do not know if Snowden considers himself a cypherpunk, but the sentiment\nexpressed in this quote nicely reï¬‚ects cypherpunk discourse.\n73 Edward Snowden, quoted in Glenn Greenwald, No Place to Hide: Edward Snowden,\nthe NSA, and the U.S. Surveillance State, 2014. Snowden is reformulating a quote of\nThomas Jeï¬€erson: â€œIn questions of power then, let no more be heard of conï¬dence\nin man but bind him down from mischief by the chains of the Constitution.â€\n74 Bruce Schneier: Data and Goliath: The Hidden Battles to Collect Your Data and\nControl Your World. W.W. Norton & Company, 2015. The quote is from p. 131.\n75 This was the dream behind the the Trusted Platform Module (TPM), a coprocessor\nthat implements a standard set of cryptographic operations within a tamper-resistant\n",
    "book_id": "the_moral_character_of_cryptographic_work",
    "book_title": "The Moral Character of Cryptographic Work",
    "book_author": "Unknown",
    "topic_id": "ai_policy",
    "topic_label": "policy",
    "chunk_index": 62
  },
  {
    "chunk_full": "The Moral Character of Cryptographic Work\n19\nnot users. If we couple public-key cryptography with a key-escrow system that\nthe FBI and NSA can exploit, we empower governments, not people.76\nAll that said, I do believe it accurate to say that conventional encryption does\nembed a tendency to empower ordinary people. Encryption directly supports\nfreedom of speech. It doesnâ€™t require expensive or diï¬ƒcult-to-obtain resources.\nItâ€™s enabled by a thing thatâ€™s easily shared. An individual can refrain from\nusing backdoored systems.77 Even the customary language for talking about\nencryption suggests a worldview in which ordinary peopleâ€”the worldâ€™s Alices\nand Bobsâ€”are to be aï¬€orded the opportunity of private discourse. And coming\nat it from the other direction, one has to work to embed encryption within an\narchitecture that props up power, and one may encounter major obstacles to\nsuccess. The Clipper Chip completely failed. Trusted Computing mostly did.78\nIBE.79 What about identity-based encryption, IBE? The setting was proposed\nby Shamir, with Boneh and Franklin, years later, providing a satisfying, provably\nsecure realization.80 The aim is to allow a partyâ€™s email address, for example, to\nserve as his public key. So if Alice already knows the email address for Bob, she\nwonâ€™t need to obtain his public key to send him an encrypted message: she just\nencrypts under Bobâ€™s email address.\nBut this convenience is enabled by a radical change in the trust model: Bobâ€™s\nsecret key is no longer self-selected. It is issued by a trusted authority. That\nauthority knows everyoneâ€™s secret key in the system. IBE embeds key escrowâ€”\nindeed a form of key escrow where a single entity implicitly holds all secret\nkeysâ€”even ones that havenâ€™t yet been issued. And even if you do trust the key-\ngenerating authority, a state-level adversary now has an extremely attractive\nlocus to subpoena or subvert. In the end, from a personal-privacy point of view,\nIBE might seem like an enormous leap backwards.\nboundary. See Wikipedia entry: Trusted Platform Module, and Ross Andersonâ€™s\nFAQ: â€˜Trusted Computingâ€™ Frequently Asked Questions. http://www.cl.cam.ac.uk/\nâˆ¼rja14/tcpa-faq.html\n76 For a nice discussion of the Clipper Chip, see Michael Froomkin: The metaphor is the\nkey: cryptography, the clipper chip, and the constitution. University of Pennsylvania\nLaw Review, 143(3), pp. 709â€“897, 1995.\n77 This may be theoretically possible but practically diï¬ƒcult: it might be hard to\naccomplish for technically unsophisticated people. This is why it is said: When\ncrypto is outlawed only outlaws will have crypto (or privacy, or security) (original\nattribution unknown).\n78 On the other hand, some other forms of cryptographically enabled DRM (Digital\nRights Management), including the CSS (Content Scrambling System) used to make\nmore diï¬ƒcult the copying of DVDs, has been reasonably eï¬€ective (from the point of\nview of content owners).\n79 Ideas expressed on this topic are joint with Mihir Bellare.\n80 Adi Shamir: Identity-based cryptosystems and signature schemes. Crypto â€™84, pp. 47-\n53, 1984. Dan Boneh and Matthew Franklin: Identity-based encryption from the\nWeil pairing. SIAM J. of Computing, 32(3), pp. 586â€“615, 2003. See also Ryuichi\nSakai, Kiyoshi Ohgishi, Masao Kasahara: Cryptosystems based on pairing. The 2000\nSymposium on Cryptography and Information Security, 2000.\n",
    "book_id": "the_moral_character_of_cryptographic_work",
    "book_title": "The Moral Character of Cryptographic Work",
    "book_author": "Unknown",
    "topic_id": "ai_policy",
    "topic_label": "policy",
    "chunk_index": 63
  },
  {
    "chunk_full": "20\nP. Rogaway\nDescriptions of IBE donâ€™t usually emphasize the change in trust model.81\nAnd the key-issuing authority seems never to be named anything like that: itâ€™s\njust the PKG, for Private Key Generator. This sounds more innocuous than\nit is, and more like an algorithm than an entity. In papers, the PKG further\nrecedes from view because it is the tuple of algorithms, not the entities that one\nimagines to run them, that grounds formal deï¬nitions and proofs.\nTo be clear, I am not condemning IBE as some sort of fascist technology. That\nsounds silly. Nor am I suggesting that IBE canâ€™t be reï¬ned in ways to make\nthe trust model less authoritarian.82 Yet one can easily see the authoritarian\ntendency built into IBE. And technologies, while adaptable, are not inï¬nitely\nso. As they evolve, they tend to retain their implicit orientations.\nDiï¬€erential privacy. Letâ€™s consider diï¬€erential privacy.83 Dwork says that\nÏµ-diï¬€erential privacy â€œaddresses concerns that any participant might have about\nthe leakage of her personal information: even if the participant removed her\ndata from the data set, no outputs . . . would become signiï¬cantly more or less\nlikely.â€84\nAt some level, this sounds great: donâ€™t we want to protect individuals from\nprivacy-compromising disclosures from corporate or governmental datasets? But\na more critical and less institutionally friendly perspective makes this deï¬nitional\nline seem oï¬€.85 Most basically, the model implicitly paints the database owner\n(the curator) as the good guy, and the users querying it, the adversary. If power\nwould just agree to fudge with the answers in the right way, it would be ï¬ne for\nit to hold massive amounts of personal data about each of us. But the history\nof data-privacy breaches suggests that the principal threat to us is from the\ndatabase owner itself, and those that gain wholesale access to the data (for\nexample, by theft or secret government programs). Second, the harm diï¬€erential-\nprivacy seeks to avoid is conceived of in entirely individualistic terms. But privacy\nviolations harm entire communities. The individualistic focus presupposes a\nnarrow conception of privacyâ€™s value. Finally,86 diï¬€erential privacy implicitly\npresupposes that the data collection serves some public good. But, routinely,\nthis is a highly contestable claim. The alternative of less data collection, or no\n81 For example, the Wikipedia entry â€œID-based encryptionâ€ (Nov. 2015) has a lead\nsection that fails to even mention the presence of a key-issuing authority.\n82 For many IBE schemes it is easy to distribute the PKGâ€™s secret, the master key,\nacross a set of parties. One could further arrange for distributed generation of this\nkey, so no one party ever held it. Both of these points are made in the original Boneh\nand Franklin paper: op. cit., Section 6, Distributed PKG.\n83 See, for example: Cynthia Dwork: Diï¬€erential privacy: a survey of results. Theory\nand Applications of Models of Computation, pp. 1â€“19, Springer, 2008.\n84 Ibid, p. 2.\n85 For\na\nquite\ndiï¬€erent\ncritique\nof\ndiï¬€erential\nprivacy,\nsee\nJane\nBambauer,\nKrishnamurty Muralidhar, Rathindra Sarathy: Foolâ€™s gold: an illustrated critique\nof diï¬€erential privacy. Vanderbilt Journal of Entertainment and Technology Law,\n16(4), pp. 701â€“755, Summer 2014.\n86 Colleen Swanson, personal communications.\n",
    "book_id": "the_moral_character_of_cryptographic_work",
    "book_title": "The Moral Character of Cryptographic Work",
    "book_author": "Unknown",
    "topic_id": "ai_policy",
    "topic_label": "policy",
    "chunk_index": 64
  },
  {
    "chunk_full": "The Moral Character of Cryptographic Work\n21\ndata collection at all, is rarely even mentioned. In the end, one must compare\nthe reduction in harm actually aï¬€orded by using diï¬€erential privacy with the\nincrease in harm aï¬€orded by corporations having another means of whitewash,\nand policy-makers believing, quite wrongly, that there is some sort of crypto-\nmagic to protect people from data misuse.\nI recently asked an expert in diï¬€erential privacy, Ilya Mironov, for his reaction\nto my harsh critique. He explained that the abstract roles in diï¬€erential-privacy\nsettings need not correspond to business relationships in the obvious way. For\nexample, a privacy-conscious organization might choose to make its own analysts\naccess sensitive data through an API that provides some diï¬€erential-privacy\nguarantee, eï¬€ectively treating its own employees as â€œadversaries.â€ Mironov\nalso explained that there are variant notions of diï¬€erential privacy that do\nnot implicitly regard the database owner as good, and those querying it as\nbad. He described diï¬€erential privacy in the local model,87 where everyone\nkeeps his data to himself. They can distributively compute the responses to\nqueries. Fundamentally, Mironov explained, the deï¬nition of diï¬€erential privacy\nis agnostic to the data model.\nWhile everything explained makes good sense, I donâ€™t think it changes the\nlandscape. No actual mechanism can be agnostic to what data resides where.\nAnd at the point when a data-mining architecture and mechanism is laid\ndown, considerations of eï¬ƒciency, familiarity, and economicsâ€”not to mention\nauthoritiesâ€™ fundamental desire to have and to hold the dataâ€”make it easy to\npredict what will happen: almost always, a centralized design will emerge. To\nme, diï¬€erential privacy may be as authoritarian in its conceptual underpinnings\nas IBE.\nFHE and iO. Ever since Craig Gentryâ€™s groundbreaking work,88 fully homo-\nmorphic encryption (FHE) has been a target of enormous intellectual capital.\nIn brief, FHE allows you to outsource your data, encrypted under a public key\nof your choosing, to a service provider. Later, you can ask that party whatever\nyouâ€™d like about your plaintext. The service provider computes the encrypted\nanswer, not knowing what it means. This is returned to you for decryption.\nFrom a political perspective, FHE sounds empoweringâ€”even utopian. The\npowerful party, say a cloud service provider, is denied access to your data. You\nsidestep the Faustian bargain that routinely underlies cloud computing.89\n87 Cynthia Dwork, Krishnaram Kenthapadi, Frank McSherry, Ilya Mironov, and Moni\nNaor: Our data, ourselves: privacy via distributed noise generation. Eurocrypt 2006,\npp. 486â€“503. For an exploration of limitations of local model, also see: Shiva Prasad\nKasiviswanathan, Homin K. Lee, Kobbi Nissim, Sofya Raskhodnikova, and Adam\nSmith: What can we learn privately? SIAM Journal of Computing, 40(3), pp. 793â€“\n826, 2011. Amos Beimel, Kobbi Nissim, and Eran Omri: Distributed private data\nanalysis: on simultaneously solving how and what. CRYPTO 2008, pp. 451-468. Also\narXiv:1103.2626\n88 Craig Gentry: Fully homomorphic encryption using ideal lattices. STOC 2009.\n89 I refer, of course, to the exchange of personal information for a network service.\n",
    "book_id": "the_moral_character_of_cryptographic_work",
    "book_title": "The Moral Character of Cryptographic Work",
    "book_author": "Unknown",
    "topic_id": "ai_policy",
    "topic_label": "policy",
    "chunk_index": 65
  },
  {
    "chunk_full": "22\nP. Rogaway\nBut the analysis above is specious. It is quite speculative if FHE will ever\nevolve into something practically useful. If you want to assess the political\nleanings of something of speculative utility, you shouldnâ€™t just assume that it\nwill give rise to the touted applications, and then try to see who would win and\nwho would lose. Itâ€™s too conjectural. It is better to focus on how the pursuit\nchanges us in the here and now.\nAnd on that, I would say that FHE, along with iO,90 have engendered a new\nwave of exuberance. In grant proposals, media interviews, and talks, leading\ntheorists speak of FHE and iO as game-changing indications of where we have\ncome.91 Nobody seems to emphasize just how speculative it is that any of this\nwill ever have any impact on practice. Nor do people emphasize our vanishing\nprivacy, our lousy computer security, or how little modern cryptography has\nreally done to change this landscape. And this has consequences. (a) It misleads\nthe public about where we stand. (b) It shifts ï¬nancial resources away from areas\nmore likely to have social utility. (c) It encourages bright young researchers to\nwork on highly impractical directions. (d) And it provides useful cover to the\nstrongest opponents of privacy: defense and intelligence agencies.\nLet me expand on the last claim. Here is what DARPA Program Director\nDan Kaufman had to say about FHE in a 2014 interview:\nImagine a future that says: OK, I have to collect everything for big data to\nwork because if I knew what wasnâ€™t relevant it wouldnâ€™t be big data. But I\ndonâ€™t want the government to just willy-nilly look through my emails: that\nfeels creepy. . . .\nSo this guy, Craig Gentry, . . . showed that you could . . . take a piece of\ndata, encrypt it, send it down the wire, never decrypt it, [but] still perform\n[computation] . . . on it. It sounds crazy, except he showed you can do it . . ..\nYou could imagine the following: . . . [Organizations] collect . . . data but\nonly in . . . encrypted form . . .. Now letâ€™s say you believe there is a bad guy\nhiding somewhere in this encrypted data. So, I come up with a bunch of search\nterms . . .. I could then go to a court . . . [and they] could say â€œyeah, that looks\nreasonable.â€ I put the search into the engine but . . . all that comes out is a\nnumber: how many people meet that criteria . . . You go back to the FISA\ncourt, and say O.K. guys, we have 12. . . . I picture FISA putting in a key, and\n90 The acronym is for indistinguishability obfuscation. Sanjam Garg, Craig Gentry,\nShai\nHalevi,\nMariana\nRaykova,\nAmit\nSahai,\nand\nBrent\nWaters:\nCandidate\nindistinguishability obfuscation and functional encryption for all circuits. FOCS\n2013, pp. 40â€“49. Earlier work introducing iO: Boaz Barak, Oded Goldreich, Russell\nImpagliazzo, Steven Rudich, Amit Sahai, Salil Vadhan, and Ke Yang. On the\n(im)possibility of obfuscating programs. JACM, 59(2):6, 2012. Earlier version in\nCRYPTO 2001.\n91 See, for example: Erica Klarreich: Perfecting the art of sensible nonsense. Quanta\nMagazine, Jan. 30, 2014. â€œResearchers are hailing the new work as a watershed\nmoment for cryptography. . . . If the problem of obfuscation has been solved, what\nremains for cryptographers?â€ Kevin Hartnett: A New Design for Cryptographyâ€™s\nBlack Box. Quanta Magazine, Sep. 2, 2015. â€œNew advances show how near-perfect\ncomputer security might be surprisingly close at hand.â€\n",
    "book_id": "the_moral_character_of_cryptographic_work",
    "book_title": "The Moral Character of Cryptographic Work",
    "book_author": "Unknown",
    "topic_id": "ai_policy",
    "topic_label": "policy",
    "chunk_index": 66
  },
  {
    "chunk_full": "The Moral Character of Cryptographic Work\n23\nthen the Agency putting in a key, and they both turn it. And [at] that point,\nfor the ï¬rst time, . . . are those 12 now revealed.92\nOf course, itâ€™s utter nonsense. To begin with, thereâ€™s no way to make sense of who\nholds what key and what data for FHE to even apply. Weâ€™re also told: that we\nneed to collect everything because, if we didnâ€™t, we wouldnâ€™t have enough data\nto have lots of data; that the government will be careful, as it would be â€œcreepyâ€\nif they werenâ€™t; that theyâ€™ll get court ordersâ€”even, apparently, to discover the\nnumber of people in datasets who satisfy some speciï¬ed search criteria; and that\nto get personally identiï¬able information, theyâ€™ll need to have the cooperation\nof the NSA and the FISA court.\nKaufmanâ€™s inchoate interview is but a tiny patch of discourse from an ocean\nof misdirection on privacy. It doesnâ€™t impugn FHE, but it does suggest how power\naims to use such work: to let them mumble words that sound privacy-friendly.\nProviding strong funding for FHE and iO provides risk-free political cover. It\nsupports a storyline that cloud storage and computing is safe. It helps entrench\nfavored values within the cryptographic community: speculative, theory-centric\ndirections. And it helps keep harmless academics who could, if they got feisty,\nstart to innovate in more sensitive directions.\nCryptanalysis. Finally, let me brieï¬‚y mention cryptanalysis. One might\nmisinterpret the academic cryptanalytic undertaking as an attack on the privacy\nof legitimate usersâ€”an attack on the inoï¬€ensive Alice and Bobâ€”which would\nthus seem to favor power.93 But this is the opposite of the right view. The reason\nthat academic cryptographers do cryptanalysis is to better inform the designers\nand users of cryptosystems about what is and what is not safe to do. The activity\nis not done to surveil people, but to help ensure that people are not surveilledâ€”\nat least by cryptanalytic means. And the work routinely has exactly that eï¬€ect.\nThe history of WEP provides a nice example.94\nWhen the NSA or GCHQ engage in cryptanalysis, it is for a very diï¬€erent\npurpose, and it has a very diï¬€erent eï¬€ect. Does that mean that cryptanalysis\ndone by one group of people (spooks) will tend to favor authority, while\ncryptanalysis done by another group of people (academics) will tend in the exact\nopposite direction? It does. The speciï¬c work will be diï¬€erent; its dissemination\nwill be diï¬€erent; and its impact on human rights will be diï¬€erent.\nUnthreateningly engaged. Of course it hasnâ€™t escaped the notice of intelli-\ngence agencies that the majority of the academic cryptographic community is\n92 Evelyn M. Rusli: A Darpa [sic] director on fully homomorphic encryption (or one\nway the U.S. could collect data). Wall Street Journal blog: blogs.wsj.com. March 9,\n2014. Quote edited for punctuation.\n93 Adi Shamir, personal communications, December 2015.\n94 WEP (Wired Equivalent Privacy) was a badly ï¬‚awed cryptographic protocol\nunderlying early 802.11 (Wi-Fi) networks. A series of devastating attacks resulted in\nWEPâ€™s replacement by a better scheme. The ï¬rst of the attacks was: Scott Fluhrer,\nItski Mantin, and Adi Shamir: Weaknesses in the key scheduling algorithm of RC4.\nSelected Areas of Cryptography (SAC 2001), pp. 1â€“24, 2001.\n",
    "book_id": "the_moral_character_of_cryptographic_work",
    "book_title": "The Moral Character of Cryptographic Work",
    "book_author": "Unknown",
    "topic_id": "ai_policy",
    "topic_label": "policy",
    "chunk_index": 67
  },
  {
    "chunk_full": "24\nP. Rogaway\nunthreateningly engaged. In a declassiï¬ed trip-report about Eurocrypt 1992, the\nNSA author opines, for example:95\nThere were no proposals of cryptosystems, no novel cryptanalysis of old\ndesigns, even very little on hardware design. I really donâ€™t see how things\ncould have been better for our purposes.\nThe NSAâ€™s newsletter in which this report appears would never again mention\nthat academic cryptographic community.96 Nor did any released Snowden-\nderived document discuss anything of our community.97 Itâ€™s as though we pro-\ngressed from a band of philosophers98 worth a few pages of snarky commentary99\nto an assemblage too insigniï¬cant even for that.\nConclusion to part 2. A 2013 essay by Arvind Narayanan suggests a simple\ntaxonomy for cryptographic work:100 thereâ€™s crypto-for-security and crypto-for-\nprivacy. Crypto-for-security is crypto for commercial purposes. Itâ€™s the crypto in\nTLS, payment cards, and cell phones. Crypto-for-privacy has social or political\naims. Here the author distinguishes between pragmatic cryptoâ€”which is about\ntrying to use cryptography to retain our predigital privacyâ€”and cypherpunk\ncryptoâ€”the grander hope of using cryptography to precipitate sweeping social\nor political reforms. The author suggests that crypto-for-security has done well,\nbut crypto-for-privacy has fared badly.\nI think Narayananâ€™s division is illuminating, but he fails to mention that most\nacademic cryptography isnâ€™t really crypto-for-security or crypto-for-privacy: it\nis, one could say, crypto-for-cryptoâ€”meaning that it doesnâ€™t ostensibly beneï¬t\ncommerce or privacy, and itâ€™s quite speculative if it will ever evolve to do either.\nPerhaps every ï¬eld eventually becomes primarily self-referential. Maybe this is\neven necessary, to some extent. But for cryptography, much is lost when we\nbecome so inward-looking that almost nobody is working on problems we could\nhelp with that address some basic human need. Crypto-for-crypto starves crypto-\nfor-privacy, leaving a hole, both technical and ethical, in what we collectively do.\n95 Eurocrypt 1992 reviewed. Author name redacted. National Security Agency,\nCRYPTOLOG. First issue of 1994. Available at http://tinyurl.com/eurocrypt1992\n96 136 editions of the NSA newsletter, CRYPTOLOG, are available, in redacted form.\nThe period covered is 1974â€“1997. An archive can be found at https://www.nsa.gov/\npublic info/declass/cryptologs.shtml\n97 Of course Tor is extensively discussed in Snowden documentsâ€”but it would be wrong\nto view Tor as a contribution springing from the cryptographic community, nor as\nsomething much addressed within it.\n98 This is the term NSA author above prefers.\n99 The use of the word â€œsnarkyâ€ here is from Bruce Schneier: Snarky 1992 NSA report\non academic cryptography. Blog post, Nov. 18, 2014. https://www.schneier.com/\nblog/archives/2014/11/snarky 1992 nsa.html\n100 Arvind Narayanan: What happened to the crypto dream? Part 1 in IEEE Security\nand Privacy Magazine, 11(2), pp. 75â€“76, 2013. Part 2 in IEEE Security and Privacy\nMagazine, 11(3), pp. 68â€“71, 2013.\n",
    "book_id": "the_moral_character_of_cryptographic_work",
    "book_title": "The Moral Character of Cryptographic Work",
    "book_author": "Unknown",
    "topic_id": "ai_policy",
    "topic_label": "policy",
    "chunk_index": 68
  },
  {
    "chunk_full": "The Moral Character of Cryptographic Work\n25\nPart 3: The dystopian world of pervasive surveillance\nMass surveillance has motivated the contents of this essay, but is it so serious\na thing? Before the Snowden revelations,101 I myself didnâ€™t really think so.\nEnvironmental problems seemed more threatening to manâ€™s future, and my\ncountryâ€™s endless wars seemed more deserving of moral consternation. It wasnâ€™t\nuntil Snowden that I ï¬nally internalized that the surveillance issue was grave, was\nclosely tied to our values and our profession, and was being quite misleadingly\nframed.\nLaw-enforcement framing. The framing of mass surveillance determines what\none thinks it is about.102 And mass surveillance has been brilliantly framed by\nauthority so as to slant discourse in a particular and predictable direction. Let me\ndescribe what Iâ€™ll call the law-enforcement framing, as regularly communicated\nby (U.S.) FBI Director James Comey:103\n1. Privacy is personal good. Itâ€™s about your desire to control personal informa-\ntion about you.\n2. Security, on the other hand, is a collective good. Itâ€™s about living in a safe\nand secure world.\n3. Privacy and security are inherently in conï¬‚ict. As you strengthen one, you\nweaken the other. We need to ï¬nd the right balance.\n4. Modern communications technology has destroyed the former balance. Itâ€™s\nbeen a boon to privacy, and a blow to security. Encryption is especially\nthreatening. Our laws just havenâ€™t kept up.104\n5. Because of this, bad guys may win. The bad guys are terrorists, murderers,\nchild pornographers, drug traï¬ƒckers, and money launderers.105 The technol-\nogy that we good guys useâ€”the bad guys use it too, to escape detection.\n101 For a review of key information learned from the Snowden revelations, see the\nEFF webpages â€œNSA Spying on America,â€ https://www.eï¬€.org/nsa-spying, and\nthe ACLU webpage, â€œNSA Surveillance,â€ https://www.aclu.org/issues/national-\nsecurity/privacy-and-surveillance/nsa-surveillance\n102 Collin Bennett: The Privacy Advocates: Resisting the Spread of Surveillance. The\nMIT Press, 2008.\n103 James Comey: â€œGoing Dark: Are Technology, Privacy, and Public Safety on\na Collision Course?â€ Speech at the Brookings Institute, October 16, 2014.\nhttp://tinyurl.com/comey-going-dark\n104 Comey speaks particularly of CALEA, the Communications Assistance for Law\nEnforcement Act, the 1994 U.S. law mandating lawful intercept capabilities be\nbuilt into telecommunications equipmentâ€”but does not mandate such abilities for\nencrypted email, instant messaging, and the like.\n105 The traditional cast of evil-doers, the â€œfour horsemen of the info-pocalypse,â€ omits\nmurderers from this list of ï¬ve; see Assange et al., Cypherpunks, op. cit.. Note that\nin Comeyâ€™s Brookings Institute speech, the speaker adds murderers to this list, and\nsubtracts money launderers. The move is shrewed; money laundering is technocratic\nand legalistic crime compared to murder.\n",
    "book_id": "the_moral_character_of_cryptographic_work",
    "book_title": "The Moral Character of Cryptographic Work",
    "book_author": "Unknown",
    "topic_id": "ai_policy",
    "topic_label": "policy",
    "chunk_index": 69
  },
  {
    "chunk_full": "26\nP. Rogaway\n6. At this point, we run the risk of Going Dark.106 Warrants will be issued,\nbut, due to encryption, theyâ€™ll be meaningless. Weâ€™re becoming a country of\nunopenable closets. Default encryption may make a good marketing pitch,\nbut itâ€™s reckless design. It will lead us to a very dark place.\nThe narrative is inconsistent with the history of intelligence gathering, and with\nthe NSAâ€™s own mission statement.107 Yet the narrativeâ€™s uneasy coexistence\nwith reality hasnâ€™t mattered. It is, in fact, beautifully crafted to frame matters\nin a way guaranteed to lead discourse where authority wants it to go. It is a\nbrilliant discourse of fear: fear of crime; fear of losing our parentsâ€™ protection;\neven fear of the dark. The narrativeâ€™s well-honed deceptiveness is itself a form\nof tradecraft.108\nSurveillance-studies framing. Of course there are radically diï¬€erent ways\nto frame mass surveillance. Consider the following way to do so, which follows\noften-heard thoughts from cypherpunks and surveillance studies.109\n1. Surveillance is an instrument of power.110 It is part of an apparatus of\ncontrol. Power need not be in-your-face to be eï¬€ective: subtle, psychological,\nnearly invisible methods can actually be more eï¬€ective.\n2. While surveillance is nothing new, technological changes have given govern-\nments and corporations an unprecedented capacity to monitor everyoneâ€™s\ncommunication and movement. Surveilling everyone has became cheaper\nthan ï¬guring out whom to surveil, and the marginal cost is now tiny.111 The\nInternet, once seen by many as a tool for emancipation, is being transformed\ninto the most dangerous facilitator for totalitarianism ever seen.112\n3. Governmental surveillance is strongly linked to cyberwar. Security vulner-\nabilities that enable one enable the other. And, at least in the USA, the\n106 The phrase (including the capitalization) is from the Comey speech just cited.\n107 Church Committee Reports, 1975â€“1976. Available at http://www.aarclibrary.org/\npublib/church/reports/contents.htm. Frank Donner: The Age of Surveillance: The\nAims and Methods of Americaâ€™s Political Intelligence System. Vintage Press, 1981.\nClassiï¬ed NSA document: SIGINT Mission Strategic Plan FY2008â€“2013. Oct. 3,\n2007. http://tinyurl.com/sigint-plan\n108 â€œVerbal deception is itself an intelligence practice,â€ explains Donner, op. cit., p. xiv.\nSee too his Appendix I, pp. 464â€“466.\n109 Michel Foucault: Discipline and Punish: The Birth of the Prison. Alan Sheriden,\ntranslator. 1975/1977. Frank Donner, op. cit. Assange et al., op. cit. David Lyon:\nSurveillance Studies: An Overview. 2007. David Murakami Wood and Kirstie Ball:\nA Report on the Surveillance Society. Sep. 2006.\n110 Even the etymology of surveillance suggests this: from the French: sur, meaning\nover, plus veiller, meaning to watch. Thus: to watch from above, from a position\nof power. An interesting turn on this conception is the notion of sousveillance, as\ncoined by Steve Mann. See Wikipedia entry: Sousveillance.\n111 Adding one more phone number to a wiretap is almost free. Kevin S. Bankston and\nAshkan Soltani: Tiny constables and the cost of surveillance: making cents out of\nUnited States v. Jones. The Yale Law Journal, vol. 123, Jan. 2014.\n112 This sentence is a slightly modiï¬ed from Assange et al., Cypherpunks, op. cit..\n",
    "book_id": "the_moral_character_of_cryptographic_work",
    "book_title": "The Moral Character of Cryptographic Work",
    "book_author": "Unknown",
    "topic_id": "ai_policy",
    "topic_label": "policy",
    "chunk_index": 70
  },
  {
    "chunk_full": "The Moral Character of Cryptographic Work\n27\nsame individuals and agencies handle both jobs. Surveillance is also strongly\nlinked to conventional warfare. As Gen. Michael Hayden has explained, â€œwe\nkill people based on metadata.â€113 Surveillance and assassination by drones\nare one technological ecosystem.\n4. The law-enforcement narrative is wrong to position privacy as an individual\ngood when it is, just as much, a social good. It is equally wrong to regard\nprivacy and security as conï¬‚icting values, as privacy enhances security as\noften as it rubs against it.\n5. Mass surveillance will tend to produce uniform, compliant, and shallow\npeople.114 It will thwart or reverse social progress. In a world of ubiquitous\nmonitoring, there is no space for personal exploration, and no space to\nchallenge social norms, either. Living in fear, there is no genuine freedom.\n6. But creeping surveillance is hard to stop, because of interlocking corporate\nand governmental interests.115 Cryptography oï¬€ers at least some hope. With\nit, one might carve out a space free of powerâ€™s reach.\nHistory teaches that extensive governmental surveillance becomes political\nin character. As civil-rights attorney Frank Donner and the Church Commission\nreports thoroughly document, domestic surveillance under U.S. FBI director\nJ. Edgar Hoover served as a mechanism to protect the status quo and\nneutralize change movements.116 Very little of the FBIâ€™s surveillance-related\neï¬€orts were directed at law-enforcement: as the activities surveilled were rarely\nillegal, unwelcome behavior would result in sabotage, threats, blackmail, and\ninappropriate prosecutions, instead. For example, leveraging audio surveillance\ntapes, the FBIâ€™s attempted to get Dr. Martin Luther King, Jr., to kill himself.117\nU.S. universities were thoroughly inï¬ltrated with informants: selected students,\nfaculty, staï¬€, and administrators would report to an extensive network of\nFBI handlers on anything political going on on campus. The surveillance\nof dissent became an institutional pillar for maintaining political order. The\nU.S. COINTELPRO program would run for more than 15 years, permanently\nreshaping the U.S. political landscape.118\n113 Michael Hayden: The Johns Hopkins Foreign Aï¬€airs Symposium Presents: The\nPrice of Privacy: Re-Evaluating the NSA. May 9, 2014. Video, available at\nhttps://www.youtube.com/watch?v=kV2HDM86XgI\n114 For the last of these claims: â€œA life spent entirely in public, in the presence of others,\nbecomes, as we would say shallow.â€ Hannah Arendt, The Human Condition. Chicago\nUniversity Press, 1959.\n115 Of course corporations and governments also, on occasion, have competing interests,\nas when publicity on surveillance scares away customers. This is the force motivating\nthe principles enunciated in https://www.reformgovernmentsurveillance.com/\n116 Church, op. cit.; Donner, op. cit.; and Julian Sanchez: Wiretappingâ€™s true danger. Los\nAngeles Times, March 16, 2008. http://articles.latimes.com/2008/mar/16/opinion/\nop-sanchez16\n117 Nadia\nKayyali:\nFBIâ€™s\nâ€œSuicide\nLetterâ€\nto\nDr.\nMartin\nLuther\nKing,\nJr.,\nand\nthe\nDangers\nof\nUnchecked\nSurveillance.\nNov.\n12,\n2014.\nEFF\nwebsite,\nhttp://tinyurl.com/fbi-suicide-letter\n118 Church Committee Reports, op. cit., and Frank Donner, op. cit.\n",
    "book_id": "the_moral_character_of_cryptographic_work",
    "book_title": "The Moral Character of Cryptographic Work",
    "book_author": "Unknown",
    "topic_id": "ai_policy",
    "topic_label": "policy",
    "chunk_index": 71
  },
  {
    "chunk_full": "28\nP. Rogaway\nOur dystopian future. Where mass surveillance leads has been brilliantly\nexplored in ï¬ctional accounts, starting with Yevgeny Zamyatinâ€™s 1921 novel We\n(which inspired Orwellâ€™s 1984). Set in a future of total surveillance, the denizens\nof the â€œOne Stateâ€ have internalized lessons such as: â€œweâ€ is from God, and â€œIâ€\nis from the devil; that imagination is illness; and that the key to ridding man of\ncrime is ridding him of freedom.\nBut you donâ€™t have to reach to ï¬ctional or historical accounts to anticipate\nwhere we are headed. In a 2012 newsletter column, NSAâ€™s â€œSIGINT Philoso-\npher,â€ Jacob Weber, shares his own vision. After failing an NSA lie-detector\ntest, he says:\nI found myself wishing that my life would be constantly and completely\nmonitored. It might seem odd that a self-professed libertarian would wish\nan Orwellian dystopia on himself, but here was my rationale: If people knew a\nfew things about me, I might seem suspicious. But if people knew everything\nabout me, theyâ€™d see they had nothing to fear.\n. . . A target that119 has no ill will to the U.S., but which is being monitored,\nneeds better and more monitoring, not less. So if weâ€™re in for a penny, we need\nto be in for a pound.120\nShrouded in enormous secrecy and complexity, the basic contours of the\nsurveillance state are fundamentally unknowable. What is the individual to\ndo? With everyoneâ€™s communication machine monitored, he knows that heâ€™s a\nde facto target. Millions of observations are made of his life. He is analyzed\nby techniques he cannot remotely understand. He knows that todayâ€™s data,\nand yesterdayâ€™s, will be scrutinized by tomorrowâ€™s algorithms. These will\nemploy sophisticated natural-language processing, but probably wonâ€™t actually\nunderstand human discourse. With all this, the rational individual has no choice\nbut to watch what he says, and to try to act like everyone else.\nThe ï¬lm Citizenfour (2014) is at its best when it manages to sketch the shape\nof this emerging world. One reviewer writes of the ï¬lm\nevoking the modern state as an unseen, ubiquitous presence, an abstraction\nwith enormous coercive resources at its disposal. . . .\nIt is everywhere and nowhere, the leviathan whose belly is our native\natmosphere. Mr. Snowden, unplugging the telephone in his room, hiding under\n119 Observe the use of that and which, rather than who: the target is not a person but\na thing.\n120 Jacob Weber (deanonymized by The Intercept readers): The SIGINT Philosopher Is\nBack â€” with a New Face! SIDtoday. May 29, 2012. goo.gl/TyBzig\nPeter Maass writes of the SIGINT Philosopher in: The Philosopher of Surveillance,\nThe Intercept, Aug. 11, 2015. His article may be my favorite from the entire corpus\nof articles coming out of the Snowden revelations. It functions at multiple levels,\ngiving the reader the uncomfortable sense of doing to another (and even enjoying\nto do to another) precisely what he would not want done to himself. â€œModern life\nis such an unholy mix of voyeurism and exhibitionism,â€ says the character of Stella\nGibson (Gillian Anderson) in Allan Cubittâ€™s TV series The Fall (2014) (season 2,\nepisode 4).\n",
    "book_id": "the_moral_character_of_cryptographic_work",
    "book_title": "The Moral Character of Cryptographic Work",
    "book_author": "Unknown",
    "topic_id": "ai_policy",
    "topic_label": "policy",
    "chunk_index": 72
  },
  {
    "chunk_full": "The Moral Character of Cryptographic Work\n29\na blanket when typing on his laptop, looking mildly panicked when a ï¬re alarm\nis tested on his ï¬‚oor, can seem paranoid. He can also seem to be practicing a\nkind of avant-garde common sense. Itâ€™s hard to tell the diï¬€erence, and [this]\n. . . can induce a kind of epistemological vertigo. What do we know about what\nis known about us? Who knows it? Can we trust them?121\nTo be more prosaic: I pick up the phone and call my colleague, Mihir Bellare,\nor I tap out an email to him. How many copies of this communication will be\nstored, and by whom? What algorithms will analyze itâ€”now and in the future?\nWhat other data will it be combined with in an attempt to form a picture of me?\nWhat would trigger a human analyst to get involved? Might my call or email\ncontribute to a tax audit, a negative grant-funding decision, some Hoover-style\ndirty tricks, or even an assassination? There is not a single person who knows\nthe answer to these questions, and those who know most arenâ€™t about to tell.\nConclusion to part 3. Ultimately, Iâ€™m not much interested in individual\ngrievances over privacy; I am far more concerned with what surveillance does to\nsociety and human rights. Totalized surveillance vastly diminishes the possibility\nof eï¬€ective political dissent. And without dissent, social progress is unlikely.\nConsider an event like the 1971 burglary of the FBI branch oï¬ƒce in\nMedia, Pennsylvania.122 With the degree of surveillance we now live under,\nthe whistleblowersâ€”beginning with that feisty physics professor who led the\neï¬€ort123â€”would be promptly arrested, and even charged with espionage. They\nwould have spent years in prison, or even faced execution. Facing such outcomes\nand odds, the activists would not have attempted their daring burglary. In an\nessay that focuses on remedies for excessive surveillance, Richard Stallman asks\nWhere exactly is the maximum tolerable level of surveillance, beyond which\nit becomes oppressive? That happens when surveillance interferes with the\nfunctioning of democracy: when whistleblowers (such as Snowden) are likely\nto be caught.124\nOnline and telephone surveillance already results in the imprisonment of\npolitical dissidents around the world,125 and it undergirds my own countryâ€™s\ndrone-assassination program.126 In the U.S., Miami-model policing127 has made\n121 A. O. Scott: Intent on defying an all-seeing eye: â€˜Citizenfour,â€™ a documentary about\nEdward Snowden. New York Times movie review, Oct. 23, 2014.\n122 Betty Medsger: The Burglary: The Discovery of J. Edgar Hooverâ€™s Secret FBI. Knopf,\n2014.\n123 William Davidon, as revealed in Medsger, The Burglary, Ibid.\n124 Richard Stallman: How much surveillance can democracy withstand? Wired, Oct. 14,\n2013. See also http://www.gnu.org/philosophy/surveillance-vs-democracy.html\n125 Reporters without Borders: The Enemies of Internet: Special Edition: Surveillance.\n2013. http://surveillance.rsf.org/en/\n126 Jeremy Scahill and Glenn Greenwald: The NSAâ€™s secret role in the U.S. assassination\nprogram. The Intercept, Feb 9, 2014.\n127 Greg Elmer and Andy Opel: Preempting Dissent: The Politics of an Inevitable\nFuture. Arbeiter Ring Publishing, 2008. Also see the ï¬lm, Preempting Dissent (2014),\nby the same team. http://preemptingdissent.com/\n",
    "book_id": "the_moral_character_of_cryptographic_work",
    "book_title": "The Moral Character of Cryptographic Work",
    "book_author": "Unknown",
    "topic_id": "ai_policy",
    "topic_label": "policy",
    "chunk_index": 73
  },
  {
    "chunk_full": "30\nP. Rogaway\nattending political protests (or just being near one in your car, or with your\nphone) an intimidating proposition. With journalistsâ€™ communications routinely\nmonitored, investigative journalism is under attack.128 Is democracy or social\nprogress possible in such an environment?\nBut, despite all these arguments, I am skeptical about rationalist accounts\nof ethical aï¬€ronts, be it mass surveillance or anything else. If we behave morally,\nit is not because of rational analyses, but an instinctual preference for liberty,\nempathy, or companionship.129 As Schneier points out, animals donâ€™t like to be\nsurveilled because it makes them feel like prey, while it makes the surveillor feel\nlikeâ€”and act likeâ€”a predator.130 I think people know at an instinctual level\nthat a life in which our thoughts, discourse, and interactions are subjected to\nconstant algorithmic or human monitoring is no life at all. We are sprinting\ntowards a world that we know, even without rational thought, is not a place\nwhere man belongs.\nPart 4: Creating a more just and useful ï¬eld\nWhat can we cryptographers realistically do to collectively up our contribution\nto crypto-for-privacy? I claim no easy answers. I can oï¬€er only modest ideas.\nSecure messaging in the untrusted-server model. Problem selection is\nthe most obvious aspect in determining our communityâ€™s impact, and secure\nmessaging, in all its forms, remains the most outstanding problem in crypto-\nfor-privacy. While mix nets, onion routing, and DC nets have all proven to be\nhighly useful,131 it is not too late to be thinking on new architectures for secure\ncommunications.\nConsider the following problem, which is inspired by Pond and the PANDA\nprotocol that it can use.132 The aim is similar to Adam Langleyâ€™s Pond protocol:\nto create an alternative to email or instant messaging but where â€œbig brotherâ€\nis unable to ï¬gure out who is communicating with whom. Unlike Pond, I donâ€™t\nwant to rely on Tor, for we seek security in the face of a global, active adversary\n(as well as a clean, provable-security treatment). Tor can always be layered on\ntop, as a heuristic measure, to hide system participants.\n128 Human Rights Watch: With liberty to monitor all: how large-scale US surveillance\nis harming journalism, law, and American democracy. July 2014.\n129 Michael Gazzaniga: The Ethical Brain: The Science of Our Moral Dilemmas. Dana\nPress, 2005.\n130 Bruce Schneier, op. cit., paraphrasing a sentence of p. 127.\n131 Joan Feigenbaum and Bryan Ford: Seeking anonymity in an Internet Panopticon.\nCommunications of the ACM, 58(10), October 2015.\n132 Adam Langley: Pond. Webpages rooted at https://pond.imperialviolet.org/. For\nPANDA, see Jacob Appelbaum and â€œanother cypherpunkâ€: Going dark: phrase auto-\nmated nym discovery authentication: or, ï¬nding friends and lovers using the PANDA\nprotocol to re-nymber after everything is lost: or, discovering new Pond users easily.\nManuscript, Feb 21, 2014. https://github.com/agl/pond/tree/master/papers/panda\n",
    "book_id": "the_moral_character_of_cryptographic_work",
    "book_title": "The Moral Character of Cryptographic Work",
    "book_author": "Unknown",
    "topic_id": "ai_policy",
    "topic_label": "policy",
    "chunk_index": 74
  },
  {
    "chunk_full": "The Moral Character of Cryptographic Work\n31\nThe intent is this. Pairs of people who want to communicate are assumed\nto initially share a password. They wonâ€™t directly talk with one another; rather,\nall communications will go through an untrusted server. First, parties upgrade\ntheir shared password to a strong key with an anonymous rendezvous protocol.\nThereafter, the sender can deposit a (constant-length) encrypted message at the\nserver. When a party wants to retrieve his ith message, heâ€™ll interact with the\nsame server, which gives him a string computed from the database contents. The\nvalue permits the receiver to recover the intended messageâ€”or, alternatively, an\nindication that there is no such ith message for him. But, throughout, all the\nserver ever sees are parties depositing random-looking strings to the server, and\nparties collecting random-looking strings from the server, these computed by\napplying some non-secret function to the serverâ€™s non-secret database. Neither\nthe server nor an active, global adversary can ï¬gure out who has communicated\nwith whom, or even whether a communications has taken place. The goal is to\ndo all this as eï¬ƒciently as possibleâ€”in particular, much more eï¬ƒciently than\nthe server just handing each recipient its entire database of encrypted messages.\nIn ongoing work, colleagues and I are working out a provable-security\ntreatment for the approach above. It uses conventional, game-based deï¬nition,\nnot the fuzzy concepts or vocabulary from much of the anonymity literature.133\nWe hope that the anonymous messaging in this untrusted-server model will\neventually prove practical for the high-latency setting. We will see.\nBigkey cryptography Let me next describe some recent work by Mihir Bellare,\nDaniel Kane, and me that we call bigkey cryptography.134\nThe intent of bigkey cryptography is to allow cryptographic operations to\ndepend on enormous keysâ€”megabytes to terabytes long. We want our keys so\nlong that it becomes infeasible for an adversary to exï¬ltrate them. Yet using\nsuch a bigkey mustnâ€™t make things slow. This implies that, with each use, only\na small fraction of the bigkeyâ€™s bits will be inspected.\nThe basic idea is not new: the concept is usually referred to as security in the\nbounded-retrieval model.135 But our emphasis is new: practical and general tools,\n133 An ambitious but informal attempt to unify terminology and concepts in\nprivacy and anonymity is provided by Andreas Pï¬tzmann and Marit Hansen:\nAnonymity,\nunlinkability,\nundetectability,\nunobservability,\npseudonymity,\nand\nidentity management â€” a consolidated proposal for terminology. Version v0.34.\nAug. 10, 2010. Manuscript at http://dud.inf.tu-dresden.de/Anon Terminology.shtml\n134 Mihir Bellare, Daniel Kane, Phillip Rogaway: Bigkey cryptography: symmetric\nencryption with enormous keys and a general tool for achieving key-exï¬ltration\nresistance. Manuscript, 2015.\n135 Stefan Dziembowski: Intrusion-resilience via the bounded-storage model. TCC 2006.\nGiovanni Di Crescenzo, Richard J. Lipton, Shabsi Walï¬sh: Perfectly secure password\nprotocols in the bounded retrieval model. TCC 2006. David Cash, Yan Zong Ding,\nYevgeniy Dodis, Wenke Lee, Richard J. Lipton, Shabsi Walï¬sh: Intrusion-resilient\nkey exchange in the bounded retrieval model. TCC 2007. JoÂ¨el Alwen, Yevgeniy\nDodis, Daniel Wichs: Survey: leakage resilience and the bounded retrieval model.\nICITS 2009. Work in the bounded-retrieval model has earlier roots in Maurerâ€™s\n",
    "book_id": "the_moral_character_of_cryptographic_work",
    "book_title": "The Moral Character of Cryptographic Work",
    "book_author": "Unknown",
    "topic_id": "ai_policy",
    "topic_label": "policy",
    "chunk_index": 75
  },
  {
    "chunk_full": "32\nP. Rogaway\nwith sharp, concrete bounds. We have no objection to using the random-oracle\nmodel to achieve these ends.\nSuppose you have a bigkey K. You want to use it for some protocol P that\nhas been designed to use a conventional-length key K. So choose a random\nvalue R (maybe 256 bits) and hash it to get some number p of probes into the\nbigkey:\ni1 = H(R, 1)\ni2 = H(R, 2)\n. . .\nip = H(R, p) .\nEach probe ij points into K: itâ€™s a number between 1 and |K|. So you grab the p\nbits at those locations and hash them, along with R, to get a derived key K:\nK = Hâ€²(R, K[i1], . . . , K[ip]) = XKEY(K, R) .\nWhere you would otherwise have used the protocol P with a shared key K, you\nwill now use P with a shared bigkey K, a freshly chosen R, this determining the\nconventional key K = XKEY(K, R).\nWe show that derived-key K is indistinguishable from a uniformly random\nkey Kâ€² even if the adversary gets R and can learn lots of information about the\nbigkey K. The result is quantitative, measuring how good the derived key is as\na function of the length of the bigkey, the number of bits leaked from it, the\nnumber of probes p, the length of R, and the number of random-oracle calls.\nAt the heart of this result is an information-theoretic question we call the\nsubkey-prediction problem. Imagine a random key K that an adversary can\nexport â„“< |K| bits of information about. After that leakage, we select p random\nlocations into K, give those locations to the adversary, and ask the adversary\nto predict those p bits. How well can it do?\nIt turns out that the adversary can do better than just recording â„“bits of the\nkey K and hoping that lots of probes fall there. But it canâ€™t do much better. Had\nnothing been leaked to the adversary, â„“= 0, then each probe would contribute\nabout one bit of entropy to the random variable the adversary must guess. But if,\nsay, half the key is leaked, â„“â‰¤|K|/2, each probe will now contribute about 0.156\nbits of entropy.136 The adversaryâ€™s chance of winning the subkey-prediction game\nwill be bounded by something thatâ€™s around 2âˆ’0.156p. One needs about p = 820\nprobes for 128-bit security, or twice that for 256-bit security.\nI think that the subkey prediction problem, and the key-encapsulation\nalgorithm based on it, will give rise to nice means for exï¬ltration-resistant\nauthenticated-encryption and pseudorandom generators.137 In general, I see\nbounded-storage model: Ueli Maurer: Conditionally-perfect secrecy and a provably-\nsecure randomized cipher. Journal of Cryptology, 5(1), pp. 53â€“66, 1992.\n136 The peculiar-looking constant is the (approximate) value of âˆ’lg(1 âˆ’c) where c â‰ˆ\n0.1100 is the real number in [0, 1/2] satisfying H2(c) = 0.5 where H2 is the binary\nentropy function, H2(x) = âˆ’x lg x âˆ’(1 âˆ’x) lg(1 âˆ’x).\n137 The sort of PRG that takes input and maintains state, which will now include a\nbigkey. See: Boaz Barak, Shai Halevi: A model and architecture for pseudo-random\ngeneration with applications to /dev/random. ACM CCS 2005. Yevgeniy Dodis,\n",
    "book_id": "the_moral_character_of_cryptographic_work",
    "book_title": "The Moral Character of Cryptographic Work",
    "book_author": "Unknown",
    "topic_id": "ai_policy",
    "topic_label": "policy",
    "chunk_index": 76
  },
  {
    "chunk_full": "The Moral Character of Cryptographic Work\n33\nbigkey cryptography as one tool that cryptographers can contribute to make\nmass surveillance harder.\nMore examples. Here are a few more examples of crypto-for-privacy work.\nConsider the beautiful paper on Riposte, by Corrigan-Gibbs, Boneh, and\nMazi`eres.138 A user, speaking with others on the Internet, wants to broadcast a\nmessage, such as a leaked document, without revealing his identity. The network\nis subject to pervasive monitoring. The authors develop deï¬nitions, protocols,\nand proofs for the problem, attending closely to eï¬ƒciency.139 They implement\ntheir schemes. Combining all these elements is rareâ€”and very much needed.140\nOr consider the work of Colin Percival in which he introduced the hash\nfunction scrypt.141 Percival explained that, when applying an intentionally\nslow-to-compute hash function to a password and salt so as to up the cost of\ndictionary attacks,142 it is better if the hash function canâ€™t be sped up all that\nmuch with custom hardware. To achieve this aim, computing the hash function\nshouldnâ€™t just take lots of time, but lots of (sequentially accessed) memory. This\ninsightful idea comes from Abadi, Burrow, Manasse, and Wobber, who wanted\nto make sure that, for a variety of settings, computing an intentionally-slow\nhash function on a high-end system would take roughly as long as computing\nit on a low-end system.143 Quite recently, a Password Hashing Competition\nDavid Pointcheval, Sylvain Ruhault, Damien Vergnaud, Daniel Wichs: Security\nanalysis of pseudo-random number generators with input: /dev/random is not\nrobust. ACM CCS 2013.\n138 Henry Corrigan-Gibbs, Dan Boneh, David Mazi`eres: Riposte: An anonymous\nmessaging system handling millions of users. IEEE Symposium on Security and\nPrivacy, pp. 321â€“338, 2015.\n139 The techniques come mostly from PIRs, the body of work to make PIRs more\neï¬ƒcient, and the recent notion of a distributed point function, from Gilboa and Ishai.\nBenny Chor, Eyal Kushilevitz, Oded Goldreich, Madhu Sudan: Private information\nretrieval. JACM 45(6), pp. 965â€“981, 1998. Earlier version from FOCS 1995.\nNiv Gilboa, Yuval Ishai: Distributed point functions and their applications.\nEUROCRYPT 2014, pp. 640â€“658.\n140 Another recent paper that does a beautiful job at the nexus of systems, privacy,\nand cryptography is: Nikita Borisov, George Danezis, Ian Goldberg: DP5: A private\npresence service. Proceedings on Privacy Enhancing Technologies (PETS) vol. 2,\npp. 4â€“24, 2015. That paper again depends crucially on PIRsâ€”this time for creating\na service to tell youâ€”but not the service providerâ€”which of your â€œfriendsâ€ are\ncurrently online, using the same, Facebook-like service.\n141 Colin Percival: Stronger key derivation via sequential memory-hard functions.\nBSDCanâ€™09, May 2009. http://www.tarsnap.com/scrypt/scrypt.pdf\n142 The technique goes back to the UNIX crypt(3) functionality.\n143 MartÂ´Ä±n Abadi, Mike Burrows, Mark Manasse, and Ted Wobber: Moderately hard,\nmemory-bound functions. ACM Trans. on Internet Technology, 5(2), pp. 299â€“327,\nMay 2005. Earlier version in NDS 2003. This paper includes a concrete construction\nfor a memory-hard hash function. An earlier proposal for a hash function\nparameterized by both the time and space it should need is given in a proposal by\n",
    "book_id": "the_moral_character_of_cryptographic_work",
    "book_title": "The Moral Character of Cryptographic Work",
    "book_author": "Unknown",
    "topic_id": "ai_policy",
    "topic_label": "policy",
    "chunk_index": 77
  },
  {
    "chunk_full": "34\nP. Rogaway\n(PHC) concluded having chosen a scheme, Argon2,144 that follows this lead.\nMeanwhile, the theory for this sort of hash function has nicely progressed.145\nWhile we donâ€™t yet have good bounds on schemes like scrypt and Argon2, I\nthink weâ€™re getting there.146\nOr consider the paper on the susceptibility of symmetric encryption to mass\nsurveillance by colleagues and me.147 We discussed algorithm-substitution\nattacks, wherein big brother replaces a real symmetric encryption algorithm\nby a subverted one. Big brotherâ€™s aim is to surreptitiously decrypt all encrypted\ntraï¬ƒc. The idea goes back to Young and Yung;148 all we did was to rigorously\nexplore the idea in the context of symmetric encryption. Yet what we found\nwas disturbing: that almost all symmetric encryption schemes can be easily\nsubverted. Still, we showed that it is easy to make schemes where this isnâ€™t true.\nAnd then thereâ€™s the Logjam paper, showing, for the umpteenth time, that\nwe must watch out for the cryptanalytic value of precomputation.149 Attacks\nshould routinely be regarded as a two-step process: an expensive one that\ndepends on widely shared parameters, then a cheaper, individualized attack.150\nSuch thinking goes back to early time-memory tradeoï¬€s,151 and to many\ncryptographerâ€™s preference for nonuniform adversaries. It occurs in practical\nArnold Reinhold entitled HEKS: A Family of Key Stretching Algorithms, July 15,\n1999 (revised July 5, 2001), http://world.std.com/ reinhold/HEKSproposal.html\n144 Alex\nBiryukov,\nDaniel\nDinu,\nDmitry\nKhovratovich:\nArgon2.\nJuly\n8,\n2015.\nhttps://www.cryptolux.org/index.php/Argon2\n145 JoÂ¨el Alwen and Vladimir Serbinenko: High parallel complexity graphs and memory-\nhard functions. STOC 2015, pp. 595â€“603.\n146 Why is this topic crypto-for-privacy? First, itâ€™s about helping individuals avoid\ngetting their accounts compromised. Second, for electronic currency, it helps keep\nsmall-scale mining more cost-competitive with large-scale operations. That isnâ€™t at\nall true for bitcoin, where mining tends to be centralized and energy-intensive The\nGHash.IO mining pool boasts on its website a mining rate of 6.35 Ph/s (252.5 hashes\nper second), and the overall rate is about 465 Ph/s (258.7 hashes per second). Data\ntaken from https://ghash.io/ and https://blockchain.info/charts/ on Nov. 1, 2015.\n147 Mihir Bellare, Kenneth G. Paterson, and Phillip Rogaway: Security of symmetric\nencryption against mass surveillance. CRYPTO 2014.\n148 Adam Young, Moti Yung: The dark side of black-box cryptography, or: should we\ntrust capstone? CRYPTO 1996. Adam Young, Moti Yung: Kleptography: using\ncryptography against cryptography. EUROCRYPT 1997.\n149 David Adrian, Karthikeyan Bhargavan, Zakir Durumeric, Pierrick Gaudry, Matthew\nGreen, J. Alex Halderman, Nadia Heninger, Drew Springall, Emmanuel ThomÂ´e,\nLuke Valenta, Benjamin VanderSloot, Eric Wustrow, Santiago Zanella-BÂ´euelin,\nPaul Zimmermann: Imperfect forward secrecy: how Diï¬ƒe-Hellman fails in practice.\nComputer and Communications Security (CCS â€™15), 2015.\n150 Additionally, whenever possible, the active attack should be something that\ndetectableâ€”something that requires interaction.\n151 Martin\nHellman:\nA\ncryptanalytic\ntime-memory\ntrade-oï¬€.\nIEEE\nTrans.\non\nInformation Theory, 26(4), pp. 401â€“406, 1980.\n",
    "book_id": "the_moral_character_of_cryptographic_work",
    "book_title": "The Moral Character of Cryptographic Work",
    "book_author": "Unknown",
    "topic_id": "ai_policy",
    "topic_label": "policy",
    "chunk_index": 78
  },
  {
    "chunk_full": "The Moral Character of Cryptographic Work\n35\nwork, as in attacks on A5/1 in GSM phones.152 And it is also the model that\nintelligence agencies seem to gravitate to, as suggested by the NSAâ€™s attack on\nFPE scheme FF2 and the fact that they regarded this attack as serious.153\nChoose well. As I hope the examples I have given illustrate, there are important\ncrypto-for-privacy problems out there, and they are quite diverse. Choose your\nproblems well. Let values inform your choice. Many times I have spoken to people\nwho seem to have no real idea why they are studying what they are. The real\nanswer is often that they can do it, it gets published, and that people did this\nstuï¬€before. These are lousy reasons for doing something.\nIntrospection canâ€™t be rushed. In the rush to publish paper after paper, who\nhas the time? I think we should breathe, write fewer papers, and have them\nmatter more.\nâ–·Attend to problemsâ€™ social value. Do anti-surveillance research.\nâ–·Be introspective about why you are working on the problems you are.\nIn enumerating example directions for anti-surveillance research, I didnâ€™t\ninclude the kind of work, rather common in the PET (privacy-enhancing\ntechnology) literature, that assumes that there will be pervasive collection, and\nthen tries to do what one can to minimize misuse.154 Since the immorality occurs\nat the point of data collection, the aim here is to try to blunt the impact of the\nwrong already done. But it is hard to know how this plays out. I am concerned\nthat the work can play into the hands of those who seek technical support\nfor a position that says, in eï¬€ect, â€œthe collect-it-all approach is inevitable and\nonly temporarily problematic, for, once we ï¬gure this all out, privacy will be\nhandled downstream, when the data is used.â€ But pervasive collection itself\nchills free-speech and threatens liberal democracy, regardless of what one claims\nwill happen downstream.155\nPractice-oriented provable security. Itâ€™s not just the topics we work\non, but how we execute on them that shapes our ï¬eldâ€™s direction. For\nnearly 25 years Mihir Bellare and I have developed that we call practice-\noriented provable security. In a 2009 essay and talk,156 I discussed how various\ninessential choices engendered a theory of cryptography that was less useful than\nnecessary. Today, I might number among the important historical choices (1) a\npreference for asymptotic analyses and theorems, and the correspondingly coarse\n152 Karsten Nohl: Breaking GSM phone privacy. Black Hat USA 2010. Available on\nyoutube, URL https://www.youtube.com/watch?v=0hjn-BP8nro\n153 Morris Dworkin, Ray Perlner: Analysis of VAES3 (FF2). Cryptology ePrint Archive\nReport 2015/306. April 2, 2015. FPE stands for Format-Preserving Encryption.\n154 See, for example, Seny Kamara: Restructuring the NSA metadata program. Financial\nCryptography Workshops 2014, pp. 235â€“247, 2014.\n155 Reï¬‚ecting this, the U.S. Fourth Amendment speaks not only of particularized\nwarrants being required for search, but also for seizure.\n156 Phillip Rogaway: Practice-oriented provable security and the social construction of\ncryptography. Manuscript, May 2009, and invited talk at Eurocrypt 2009.\n",
    "book_id": "the_moral_character_of_cryptographic_work",
    "book_title": "The Moral Character of Cryptographic Work",
    "book_author": "Unknown",
    "topic_id": "ai_policy",
    "topic_label": "policy",
    "chunk_index": 79
  },
  {
    "chunk_full": "36\nP. Rogaway\nconceptualizations of security with which this is paired; (2) a preference towards\nminimalism, aesthetically construed, as a starting point for reductions; (3) the\ndismissal of symmetric primitives and ï¬nite functions as targets of rigorous\ninquiry; (4) a tradition of using nonconstructive language for stating results;\n(5) the marginalization of secure messaging; and (6) a condemnatory attitude\ntowards the random-oracle model, the random-permutation model, the ideal-\ncipher model, Dolev-Yao models,157 and any other model deemed non-standard.\nPractice-oriented provable security inverts such choices. It retains provable-\nsecurityâ€™s focus on deï¬nitions and proofs, but these are understood as tools that\nearn their value mostly by their utility to security or privacy. The approach\nis equally at home in those two realms, but it has been underused for privacy\nproblems like secure messaging. Better treating mix-nets and onion routing is\nan obvious place to start, which students and I are doing.\nâ–·Apply practice-oriented provable security to anti-surveillance problems.\nFunding.158 In the United States, it would seem that the majority of extramural\ncryptographic funding may now come from the military.159 From 2000 to 2010,\nfewer than 15% of the papers at CRYPTO that acknowledged U.S. extramural\nfunding acknowledged DoD funding.160 In 2011, this rose to 25%. From 2012 to\n2015, it rose to 65%.161 Nowadays, many cryptographers put together a large\npatchwork of grants, the largest of which are usually DoD. The following funding\nacknowledgment isnâ€™t so very atypical:\nThis work was supported by NSF, the DARPA PROCEED program, an\nAFOSR MURI award, a grant from ONR, an IARPA project provided via\nDoI/NBC, and by Samsung.162\n157 Danny Dolev, Andrew C. Yao: On the security of public key protocols. IEEE Trans.\non Information Theory, IT-29, pp. 198â€“208, 1983.\n158 This section deals exclusively with academic funding of cryptography in the U.S. I\nknow very little about cryptographic funding in other countries.\n159 I have been unable to locate statistics on this.\n160 DoD = Department of Defense. This includes organizations like AFOSR, IARPA,\nDARPA, and ONR.\n161 This data is based on an accounting I did myself, by hand, going through all these\nold proceedings.\n162 The acronyms are: AFOSR = Air Force Oï¬ƒce of Scientiï¬c Research; DARPA =\nDefense Advanced Research Projects Agency; DoI/NBC = Department of Interior\nNational Business Center; IARPA = Intelligence Advanced Research Projects\nActivity; MURI = Multidisciplinary University Research Initiative; NSF = National\nScience Foundation; ONR = Oï¬ƒce of Naval Research; and PROCEED =\nProgramming Computation on Encrypted Data. Following the statement came\nanother 35 words of legalistic language, including a statement that the paper had\nbeen cleared â€œApproved for Public Release.â€ http://eprint.iacr.org/2013/403.pdf\n",
    "book_id": "the_moral_character_of_cryptographic_work",
    "book_title": "The Moral Character of Cryptographic Work",
    "book_author": "Unknown",
    "topic_id": "ai_policy",
    "topic_label": "policy",
    "chunk_index": 80
  },
  {
    "chunk_full": "The Moral Character of Cryptographic Work\n37\nThe military funding of science invariably redirects it163 and creates moral\nhazards.164 Yet suggesting to someone that they might want to reconsider their\ntaking DoD funding may anger even a placid colleague, for it will be perceived\nas an assault both on ones character and his ability to succeed.\nNo matter what people say, our scientiï¬c work does change in response to\nsponsorâ€™s institutional aims. These aims may not be oneâ€™s own. For example,\nthe mission of DARPA is â€œto invest in the breakthrough technologies that can\ncreate the next generation of [U.S.] national security capabilities.â€ Having begun\nin the wake of Sputnik, the agency speaks of avoiding technological surpriseâ€”and\ncreating it for Americaâ€™s enemies.165 In the USA, the NSA advises other DoD\nagencies on crypto-related grants. At least sometimes, they advise the NSF. Back\nin 1996, the NSA tried to quash my own NSF CAREER award. I learned this\nfrom my former NSF program manager, Dana Latch, who not only refused the\nNSA request, but, annoyed by it, told me. An internal history of the NSA reports\non the mistake of theirs that allowed funding the grant leading to RSA.\nNSA had reviewed the Rivest [grant] application, but the wording was so\ngeneral that the Agency did not spot the threat and passed it back to NSF\nwithout comment. Since the technique had been jointly funded by NSF and the\nOï¬ƒce of Naval Research, NSAâ€™s new director, Admiral Bobby Inman, visited\nthe director of ONR to secure a commitment that ONR would get NSAâ€™s\ncoordination on all such future grant proposals.166\nPeople are often happy to get funding, regardless of its source. But I would\nsuggest that if a funding agency embraces values inconsistent with your own,\nthen maybe you shouldnâ€™t take their money. Institutions have values, no less\nthan men. Perhaps, in the modern era, they even have more.\nLarge organizations have multiple and sometimes conï¬‚icting aims. Military\norganizations with oï¬€ensive and defensive roles in cybersecurity have COIs built\ninto their design. Individuals are wrong to assume that their work is non-military\nwork errantly funded by the military.\nIn his farewell address of 1961, President Dwight D. Eisenhower introduced\nthe phrase, and concept, of the military-industrial complex. In an earlier version\nof that speech, Eisenhower tellingly called it the military-industrial-academic\ncomplex.167 If scientists wish to reverse our complicity in this convergence of\ninterests, maybe we need to step away from this trough.\nNone of this was clear to me when I ï¬rst joined the university. A few years\nago I joined in on a DoD grant proposal (fortunately, unfunded), which I would\n163 Daniel S. Greenberg: Science, Money, and Politics: Political Triumph and Ethical\nErosion. University of Chicago Press, 2003.\n164 A moral hazard is a situation in which one party gets the beneï¬ts and another takes\nthe risk. The term is common in economics.\n165 Darati Prabhakar: Understanding DARPAâ€™s Mission. http://tinyurl.com/darpa-\nmission YouTube version http://tinyurl.com/darpa-mission2\n166 Tom Johnson: Book III: Retrenchment and Reform, 1998. Formerly classiï¬ed book,\navailable, as a result of a FOIA request, at http://cryptome.org/0001/nsa-meyer.htm\n167 Henry A. Giroux: The University in Chains: Confronting the Military-Industrial-\nAcademic Complex, Routledge, 2007.\n",
    "book_id": "the_moral_character_of_cryptographic_work",
    "book_title": "The Moral Character of Cryptographic Work",
    "book_author": "Unknown",
    "topic_id": "ai_policy",
    "topic_label": "policy",
    "chunk_index": 81
  },
  {
    "chunk_full": "38\nP. Rogaway\nnot do today. It took me a long time to realize what eventually became obvious\nto me: that the funding we take both impacts our beliefs and reï¬‚ects on them.\nIn the end, a major reason that crypto-for-privacy has fared poorly may be\nthat funding agencies may not want to see progress in this direction,168 and most\ncompanies donâ€™t want progress here, either. Cryptographers have internalized\nthis. Mostly, weâ€™ve been in the business of helping business and government\nkeep things safe. Governments and companies have become our â€œcustomers,â€ not\nsome ragtag group of activists, journalists, or dissidents, and not some abstract\nnotion of the people. Crypto-for-privacy will fare better when cryptographers\nstop taking DoD funds and, more than that, start thinking of a very diï¬€erent\nconstituency for our output.\nâ–·Think twice, and then again, about accepting military funding.169\nâ–·Regard ordinary people as those whose needs you ultimately aim to satisfy.\nAcademic freedom. Those of us who are academics at universities enjoy a\ntradition of academic freedom. This refers to your rightâ€”and even obligationâ€”to\nthink about, speak about, and write about whatever you want that is connected\nto your work, even if it goes against the wishes of power: your university,\ncorporations, or the state. While academic freedom seems to be in decline,170 at\nleast for now, it recognizably persists.\nNormally, scientists and other academics donâ€™t actually need or use their\nacademic freedom: all they really need is funding and skill.171 But crypto-for-\nprivacy may be a rare topic where academic freedom is useful.172 I suggest that\npeople use this gift. Unexercised, academic freedom will wither and die.\nMany nonacademics also have something akin to academic freedom: suï¬ƒcient\nautonomy to work on what they think is important, without losing their jobs,\neven if itâ€™s not what their employer really wants or likes.\nâ–·Use the academic freedom that you have.\n168 People will of course point to Tor as a counterexample; it has received funding from\nDARPA, ONR, and the State Department. I donâ€™t think thereâ€™s much to explain.\nEvery large bureaucracy has within it competing and conï¬‚icting directions. Some\nsegments of the U.S. government can think Tor is great even when others would like\nto defund, dismantle, or subvert it.\n169 In the USA, this means AFOSR, ARO, DARPA, IARPA, MURI, NSA, ONR, and\nmore.\n170 Frank Donoghue: The Last Professors: The Corporate University and the Fate of the\nHumanities. Fordham University Press, 2008. Or see: The Center for Constitutional\nRights and Palestine Legal: The Palestine exception to free speech: a movement\nunder attack in the US. Sep. 30, 2015. https://ccrjustice.org/the-palestine-exception\n171 Lorren R. Graham: Money vs freedom: the Russian contradiction. Humanities, 20(5),\nSept/Oct 1999.\n172 For a description of an incident involving Matthew Green, see: Jeï¬€Larson and Justin\nElliott: Johns Hopkins and the Case of the Missing NSA Blog Post. ProPublica,\nSep. 9, 2013. For a description of an incident at Purdue involving Barton Gellman,\nsee his article: I showed leaked NSA slides at Purdue, so feds demanded the video\nbe destroyed. Ars Technica, Oct. 9, 2015. http://tinyurl.com/gellman-at-purdue\n",
    "book_id": "the_moral_character_of_cryptographic_work",
    "book_title": "The Moral Character of Cryptographic Work",
    "book_author": "Unknown",
    "topic_id": "ai_policy",
    "topic_label": "policy",
    "chunk_index": 82
  },
  {
    "chunk_full": "The Moral Character of Cryptographic Work\n39\nAgainst dogma. I think that many cryptographers would do well to foster a\nmore open-minded attitude to unfamiliar models, approaches, and goals. The dis-\nciplinary narrowing within cryptographyâ€™s tier-1 venues has been pronounced.173\nMany people seem to hold rather strident beliefs about what kinds of work are\ngood. Sometimes it borders on silliness, as when people refuse to use the word\nproof for proofs in the random-oracle model. (Obviously a proof in the random-\noracle model is no less a proof than a proof in any other model.)\nAs cryptographers, we must always be sensitive, and skeptical, about the\nrelationship between our models and actual privacy or security. This doesnâ€™t\nmean that we should not take models seriously. It means that should see them as\ntentative and dialectical. Thereâ€™s a lovely aphorism from statistician George Box,\nwho said that all models are wrong, but some are useful.174\nCryptography needs useful models. But the assessment of a modelâ€™s utility\nis itself problematic. We ask of deï¬nitions: How clean? How understandable?\nHow general? What aspects of the computing environment are covered? What\ndoes and doesnâ€™t it imply? The deï¬nitional enterprise sits at a juncture of math,\naesthetics, philosophy, technology, and culture. So situated, dogma is disease.\nIt has been claimed that the mission of theoretical cryptography is to deï¬ne\nand construct provably secure cryptographic protocols and schemes.175 But this\nis an activity of theoretical cryptography, not its mission. There are many other\nactivities. One might work on models and results that are completely rigorous but\nfall outside of the provable-security framework.176 Or one can take an important\nprotocol as ï¬xed and then analyze it, in whatever framework works best. The\naim for my own work has been to develop ideas that I hope will contribute\nto the construction of secure computing systems. In the symbology of Amit\nSahaiâ€™s lovely ï¬‚ower-garden,177 theory-minded cryptographers can be gardeners,\ngrowing seeds (hardness assumptions) into ï¬‚owers (cryptographic goals); but\n173 For the most part, hardware is gone, formalistic approaches to cryptography are gone\n(unless they claim to bridge to â€œrealâ€ crypto), cryptanalysis of real-world schemes\nis little to be seen, and so on.\n174 George E. P. Box: Robustness in the strategy of scientiï¬c model building. In: Launer,\nR. L.; Wilkinson, G. N., Robustness in Statistics, Academic Press, pp. 201â€“236, 1979.\nBox was not the ï¬rst to express this sentiment. For example, Georg Rasch explained,\nin 1960, that â€œWhen you construct a model you leave out all the details which you,\nwith the knowledge at your disposal, consider inessential. . . . Models should not be\ntrue, but it is important that they are applicable, and whether they are applicable\nfor any given purpose must of course be investigated. This also means that a model\nis never accepted ï¬nally, only on trial.â€ Georg Rasch: Probabilistic models for some\nintelligence and attainment tests. Copenhagen: Danmarks Paedogogiske Institut,\npp. 37â€“38, 1960. republished by University of Chicago Press, 1980.\n175 Shaï¬Goldwasser, Yael Tauman Kalai: Cryptographic assumptions: a position paper.\nCryptology ePrint Archive Report 2015/907, Sep. 16, 2015.\n176 The entire information-theoretic tradition of cryptography is in this vein.\n177 Amit Sahai: Obfuscation II. Talk at the Simons Institute. May 19, 2015. Available\nat https://simons.berkeley.edu/talks/amit-sahai-2015-05-19b\n",
    "book_id": "the_moral_character_of_cryptographic_work",
    "book_title": "The Moral Character of Cryptographic Work",
    "book_author": "Unknown",
    "topic_id": "ai_policy",
    "topic_label": "policy",
    "chunk_index": 83
  },
  {
    "chunk_full": "40\nP. Rogaway\nthey can do many other things as well. Which is fortunate, as cryptographic\npractice hasnâ€™t beneï¬ted all that much from our horticultural activities.\nâ–·Be open to diverse models. Regard all models as suspect and dialectical.\nA more expansive view. I would encourage cryptographersâ€”especially young\npeople in our ï¬eldâ€”to try to get a systems-level view of what is going on when\ncryptography is used. You need a way better view of things than a technophobe\nlike me will ever have.\nI remember reading that 2012 paper of Dan Boneh and his coauthors, The\nMost Dangerous Code in the World,178 and feeling humbled by the fact that\nthere was this entire universe of codeâ€”this middlewareâ€”that I didnâ€™t even know\nexisted, but that could, and routinely did, annul the cryptography that was\nthere. When the NSA revelations caused people to speculate as to how Internet\ncryptography was being defeated, it occurred to me that perhaps the NSA didnâ€™t\nneed any clever cryptanalysisâ€”what they needed, most of all, was to buy exploits\nand hire people with a systems-level view of the computing ecosystem.\nOne approach that might be useful for gaining a good vantage is to take an\nAPI-centric view of things.179 Not only are API misunderstandings a common\nsecurity problem, but gaps between cryptographic formalizations and APIs can\nproduce serious cryptographic problems.180 And in the constructive direction,\nthe notion of online-AE, for example,181 eï¬€ectively ï¬‚ows from taking an API-\ncentric view. APIs and â€œseriousâ€ cryptography need stronger bonds.\nResearch communities have a general tendency to become inward-looking. As\na community, we have fostered strong relationships to algorithms and complexity\ntheory, but have done less well attending to privacy research, programming\nlanguages, or the law. We will play a larger social role if we up our connections\nto neighbors.\nI recently saw a nice talk by Chris Soghoian in which he described his\nfrustration in trying to get media to report on, or anyone else to care about, the\nwell-known fact (that is actually not well known) that cell-phone conversations\nhave essentially no privacy.182 Cryptographers should be helping with such\n178 Martin Georgiev, Subodh Iyengar, Suman Jana, Rishita Anubhai, Dan Boneh, Vitaly\nShmatikov: The most dangerous code in the world: validating SSL certiï¬cates in non-\nbrowser software. ACM Conference on Computer and Communications Security,\npp. 38â€“49, 2012.\n179 API\nis\napplication\nprogramming\ninterface,\nthe\narchitected\ninterfaces\namong\ncomponent code.\n180 Serge Vaudenay: Security ï¬‚aws induced by CBC padding: applications to SSL,\nIPSEC, WTLS . . .. Eurocrypt 2002.\n181 Viet Tung Hoang, Reza Reyhanitabar, Phillip Rogaway, Damian VizÂ´ar: Online\nauthenticated-encryption and its nonce-reuse misuse-resistance. Crypto 2015, vol. 1,\npp. 493â€“517, 2015. Guido Bertoni, Joan Daemen, MichaÂ¨el Peeters, Gilles Van Assche:\nDuplexing the sponge: single-pass authenticated encryption and other applications.\nSelected Areas in Cryptography 2011, pp. 320â€“337, 2011.\n182 Chris Soghoian, Workshop on Surveillance and Technology (SAT 2015), Drexel\nUniversity, June 29, 2015. See also: Chris Soghoian: How to avoid surveillance. . .with\n",
    "book_id": "the_moral_character_of_cryptographic_work",
    "book_title": "The Moral Character of Cryptographic Work",
    "book_author": "Unknown",
    "topic_id": "ai_policy",
    "topic_label": "policy",
    "chunk_index": 84
  },
  {
    "chunk_full": "The Moral Character of Cryptographic Work\n41\ncommunications. But I wonder how much we have even paid attention. For most\nof us, if itâ€™s not what oneâ€™s working on, one doesnâ€™t really care. There isnâ€™t time.\nâ–·Get a systems-level view. Attend to that which surrounds our ï¬eld.\nLearn some privacy tools. I would like to gently suggest that we cryptogra-\nphers would do well to learn, and use, contemporary privacy tools. Very few of us\nuse tools like OTR, PGP, Signal, Tails, and Tor. Itâ€™s kind of an embarrassmentâ€”\nand I suspect our collective work suï¬€ers for it. Christopher Soghoian insightfully\nremarks: â€œItâ€™s as if the entire academic medical community smoked 20 cigarettes\na day, used intravenous drugs with shared needles, and had unprotected sex with\nrandom partners on a regular basis.â€183\nIâ€™m a bizarre person to advocate in this directionâ€”itâ€™s deï¬nitely a case of the\npot calling the kettle black. I am dispositionally uninterested in using technology,\nand am incompetent at doing so if I try. I donâ€™t even own a smartphone. Yet\nI suspect that there is nothing like experience to motivate cryptographers to\nidentify and solve the privacy problems that will help us to transform hard-to-\nuse tools for nerds into transparently embedded mechanisms for the masses. The\nï¬rst problem I suggested in Section 4 is something I thought of within days of\nstarting to use Pond.\nâ–·Learn some privacy tools. Use them. Improve them.\nNo cutesy adversaries. There is a long tradition of cutesiness in our ï¬eld.\nPeople spin fun and fanciful stories. Protocol participants are a caricatured Alice\nand Bob. Adversaries are little devils, complete with horns and a pitchfork. Some\ncrypto talks are so packed with clip-art you can hardly ï¬nd the content. I have\nnever liked this, but, after the Snowden revelations, it started to vex me like\nnever before.\nCryptography is serious, with ideas often hard to understand. When we try\nto explain them with cartoons and cute narratives, I donâ€™t think we make our\ncontributions easier to understand. What we actually do is add in a layer of\nobfuscation that must be peeled away to understand what has actually been\ndone. Worse, the cartoon-heavy cryptography can reshape our internal vision\nof our role. The adversary as a $53-billion-a-year military-industrial-surveillance\ncomplex and the adversary as a red-devil-with-horns induce entirely diï¬€erent\nthought processes. If we see adversaries in one of these ways, we will actually\nyour\nphone.\nTED\ntalk.\nhttps://www.youtube.com/watch?v=ni4FV5zL6lM.\nStephanie K. Pell and Christopher Soghoian: Your secret Stingrayâ€™s no secret\nanymore: the vanishing government monopoly over cell phone surveillance and its\nimpact on national security and consumer privacy. Harvard Journal of Law and\nTechnology, 28(1), Fall 2014.\n183 Christopher Soghoian, personal communications, Nov. 28, 2015.\n",
    "book_id": "the_moral_character_of_cryptographic_work",
    "book_title": "The Moral Character of Cryptographic Work",
    "book_author": "Unknown",
    "topic_id": "ai_policy",
    "topic_label": "policy",
    "chunk_index": 85
  },
  {
    "chunk_full": "42\nP. Rogaway\nsee at a diï¬€erent set of problems to work on than if we see things in the other.\nWhimsical adversaries engender a chimerical ï¬eld.184\nAs a graduate student, I wanted our ï¬eld to feel fantastical. I wanted a\ndiscipline full of space aliens and communicating millionaires. Not only was it\nfun, but it stroked my ego, eï¬€ectively embodying the sentiment: I am a scientist\ntoo smart to have to deal with small-minded concerns.\nAt this point, I think we would do well to put ourselves in the mindset of a\nreal adversary, not a notional one: the well-funded intelligence agency, the proï¬t-\nobsessed multinational, the drug cartel. You have an enormous budget. You\ncontrol lots of infrastructure. You have teams of attorneys more than willing to\ninterpret the law creatively. You have a huge portfolio of zero-days.185 You have a\nmountain of self-righteous conviction. Your aim is to Collect it All, Exploit it All,\nKnow it All.186 What would frustrate you? What problems do you not want a\nbunch of super-smart academics to solve?\nâ–·Stop with the cutesy pictures. Take adversaries seriously.\nA cryptographic commons. Many people see the Internet as some sort of\nmagniï¬cent commons. This is a fantasy. There are some successful commons\nwithin the Internet: Wikipedia, the free software movement, Creative Commons,\nOpenSSL, Tor, and more. But most people turn almost exclusively to services\nmediated by a handful of corporations that provide the electronic mail, instant\nmessaging, cloud storage, and cloud computing, for example, that people use.\nAnd they provide the hardware on which all this stuï¬€sits.\nWe need to erect a much expanded commons on the Internet. We need to\nrealize popular services in a secure, distributed, and decentralized way, powered\nby free software and free/open hardware. We need to build systems beyond the\nreach of super-sized companies and spy agencies. Such services must be based\non strong cryptography. Emphasizing that prerequisite, we need to expand our\ncryptographic commons.\nDreams for such a commons go back to the cypherpunks, who built remailers,\nfor example, as a communitarian service to enable secure communications. More\nrecently, Feigenbaum and Koenig articulate such a vision.187 After explaining\nthat centralized cloud services play a central role in enabling mass surveillance,\nthey call for a grass-roots eï¬€ort to develop new, global-scale cloud services based\non open-source, decentralized, conï¬guration-management tools.\n184 Despite all these comments, I think a graphic novel on cryptography could be great.\nSomething like the work of Keith Aoki, James Boyle: Bound By Law: Tales from the\nPublic Domain, 2006.\n185 Exploits that nobody else knows about.\n186 The phrases are from an NSA slide released by Snowden. Reprinted on p. 97 of Glenn\nGreenwald: No Place to Hide: Edward Snowden, the NSA, and the U.S. Surveillance\nState, 2014.\n187 Joan Feigenbaum, JÂ´erÂ´emie Koenig: On the feasibility of a technological response to\nthe surveillance morass. Security Protocols Workshop 2014, pp. 239â€“252, 2014.\n",
    "book_id": "the_moral_character_of_cryptographic_work",
    "book_title": "The Moral Character of Cryptographic Work",
    "book_author": "Unknown",
    "topic_id": "ai_policy",
    "topic_label": "policy",
    "chunk_index": 86
  },
  {
    "chunk_full": "The Moral Character of Cryptographic Work\n43\nWe might start small by doing our piece to improve the commons we do have:\nWikipedia. It could become a routine undertaking at IACR conferences and\nworkshops, or at Dagstuhl meeting, for folks to gather around for an afternoon\nor evening to write, revise, and verify selected Wikipedia pages dealing with\ncryptography. Itâ€™s the sort of eï¬€ort that will pay oï¬€in many unseen ways.\nâ–·Design and build a broadly useful cryptographic commons.\nCommunications. In advancing our ï¬eld, well-named notions have always been\nimportant. One has only to think back to zero-knowledge (and the competing\nterm minimal-disclosure) to recall how a beautiful phrase could help catapult\na beautiful idea into prominence. Similarly, the six-letter phrase 33 bits does\na remarkably good job of embodying an important concept without going\nanywhere near contested vocabulary.188 In both cryptography and privacy,\nlanguage is both formative and fraught.\nThe word privacy, its meaning abstract and debated, its connotations often\nnegative, is not a winning word. Privacy is for medical records, toileting, and\nsex â€” not for democracy or freedom. The word anonymity is even worse: modern\npolitical parlance has painted this as nearly a ï¬‚avor of terrorism. Security is\nmore winning a word and, in fact, I spoke of secure messaging instead of private\nmessaging or anonymous messaging because I think it better captures what I\nwant conveyed: that a communication whose endpoints are manifest is not at all\nsecure. A person needs to feel insecure if using such a channel.\nBut even the word security doesnâ€™t support a good framing of our problem:\nwe should try to speak of thwarting mass surveillance more than enhancing\nprivacy, anonymity, or security. As discussed before, we know instinctively\nthat ubiquitous surveillance is incompatible with freedom, democracy, and\nhuman rights.189 This makes surveillance a thing against which one can ï¬ght.\nThe surveillance camera and data center make visual our emerging dystopia,\nwhile privacy, anonymity, and security are so abstract as to nearly defy visual\nrepresentation.\nConcretely, research that aims to undermine objectionable surveillance\nmight be called anti-surveillance research.190 Tools for this end would be anti-\nsurveillance technologies.191 And choosing the problems one works on based on\nan ethical vision might be called conscience-based research.\nâ–·Choose language well. Communication is integral to having an impact.\n188 The value refers, of course, to the number of bit needed to identify a living human.\nSee Arvind Narayananâ€™s website 33bits.org\n189 For a beautiful exposition of this idea, see Eben Moglen: Privacy under attack: the\nNSA ï¬les revealed new threats to democracy. The Guardian, May 27, 2014. The\narticle is derived from the four-part lecture: Eben Moglen: Snowden and the future,\ndelivered Oct. 9, Oct. 30, Nov. 13, and Nov. 4, 2013, the Columbia Law School.\nhttp://snowdenandthefuture.info/\n190 This suggestion, as well as conscience-based research, are from Ron Rivest.\n191 While the term is already in use, the more customary term of art is privacy-enhancing\ntechnologies.\n",
    "book_id": "the_moral_character_of_cryptographic_work",
    "book_title": "The Moral Character of Cryptographic Work",
    "book_author": "Unknown",
    "topic_id": "ai_policy",
    "topic_label": "policy",
    "chunk_index": 87
  },
  {
    "chunk_full": "44\nP. Rogaway\nInstitutional values. This essay might seem to focus on the ethical weight of\neach scientistâ€™s personal, professional choices. But I am actually more concerned\nabout how we, as cryptographers and computer scientists, act in aggregate. Our\ncollective behavior embodies valuesâ€”and the institutions we create do, too.\nI do not intend to criticize any particular individual. People should and will\nwork on what they think to be most valuable. The problem occurs when our\ncommunity, as a whole, systematically devalues utility or social worth. Then we\nhave a collective failure. The failure falls on no one in particular, and yet it falls\non everyone.\nConclusion to it all. Many before me have discussed the importance of ethics,\ndisciplinary culture, and political context in shaping what we do. For example,\nNeal Koblitz asserts that the founding of the CRYPTO conference in 1981 was\nitself an act of deï¬ance. He warns of the corrupting role that funding can play.\nAnd he concludes his own essay with an assertion that drama and conï¬‚ict are\ninherent in cryptography, but that this also makes for some of the ï¬eldâ€™s fun.192\nSusan Landau reminds us that privacy reaches far beyond engineering, and into\nlaw, economics, and beyond. She reminds us that minimizing data collection is\npart of the ACM Code of Ethics and Professional Conduct.193\nAs computer scientists and cryptographers, we are twice culpable when it\ncomes to mass surveillance: computer science created the technologies that\nunderlie our communications infrastructure, and that are now turning it into\nan apparatus for surveillance and control; while cryptography contains within it\nthe underused potential to help redirect this tragic turn.194\nAuthors and ï¬lmmakers, futurists and scientists, have laid out many\ncompeting visions for manâ€™s demise. For example, Bill Joy worries about\nnanotechnology turning the biosphere into gray goo, or super-intelligent robots\ndeciding that man is a nuisance, or a pet.195 I donâ€™t lose sleep over such\npossibilities; I donâ€™t see them as our likely end. But a creeping surveillance that\ngrows organically in the public and private sectors, that becomes increasingly\ncomprehensive, entwined, and predictive, that becomes an instrument for\nassassination, political control, and the maintenance of powerâ€”well, this vision\ndoesnâ€™t merely seem possible, it seems to be happening before our eyes.\nI am not optimistic. The ï¬gure of the heroic cryptographer sweeping in to\nsave the world from totalitarian surveillance is ludicrous.196 And in a world where\n192 Neal Koblitz: The uneasy relationship between mathematics and cryptography.\nNotices of the AMS, 54(8), pp. 972â€“979, September 2007.\n193 Susan Landau: Privacy and security: a multidimensional problem. Communications\nof the ACM, 51(11), November 2008.\n194 Of course we cryptographers are not the only ones in the thick of this. People who\nwork on â€œdata scienceâ€ and â€œbig dataâ€ are especially involved.\n195 Bill Joy: Why the future doesnâ€™t need us. Wired, 8.04. April 2000.\n196 That said, there is considerable drama in the experiences of people like Julian\nAssange, William Binney, William Davidon, Tom Drake, Daniel Ellsberg, Mark\nKlein, Annie Machon, Chelsea Manning, Laura Poitras, Jesselyn Radack, Diane\nRoark, Aaron Swartz, Edward Snowden, and J. Kirk Wiebe.\n",
    "book_id": "the_moral_character_of_cryptographic_work",
    "book_title": "The Moral Character of Cryptographic Work",
    "book_author": "Unknown",
    "topic_id": "ai_policy",
    "topic_label": "policy",
    "chunk_index": 88
  },
  {
    "chunk_full": "The Moral Character of Cryptographic Work\n45\nintelligence agencies stockpile and exploit countless vulnerabilities, obtain CA\nsecret keys, subvert software-update mechanisms, inï¬ltrate private companies\nwith moles, redirect online discussions in favored directions, and exert enormous\ninï¬‚uence on standards bodies, cryptography alone will be an ineï¬€ectual response.\nAt best, cryptography might be a tool for creating possibilities within contours\ncircumscribed by other forces.\nStill, there are reasons to smile. A billion users are getting encrypted instant\nmessaging using WhatsApp and its embedded Axolotl protocol.197 Two million\nclients connect using Tor each day.198 Cryptography papers inspired by the\nSnowden revelations are starting to come out apace. More than 50 crypto\nand security researchers from the U.S.A. signed an open letter I co-organized\ndeploring society-wide surveillance.199 The 15-author Keys Under Doormats\nreport200 is an explicit attempt to have cryptographic expertise inform policy.\nAnd itâ€™s not as though crypto-for-privacy is something new or deprecated\nwithin our community. Cryptographers like Ross Anderson, Dan Bernstein, Matt\nBlaze, David Chaum, Joan Feigenbaum, Matt Green, Nadia Heninger, Tanja\nLange, Arjen Lenstra, Kenny Paterson, Ron Rivest, Adi Shamir, Nigel Smart,\nand Moti Yung, to name just a few, have been attending to practical privacy\nlong before it started to get trendy (if this is happening). The RWC (Real World\nCryptography) conference is creating a new and healthy mix of participants.\nTalks, workshops, and panel discussions on mass surveillance are helping\ncryptographers see that dealing with mass surveillance is a problem within our\ndiscipline. Bart Preneel and Adi Shamir have been going around giving talks\nentitled Post-Snowden Cryptography, and there were panel discussions with this\ntitle at Eurocrypt 2014 and RSA-CT 2015.\nArticles are emerging with titles like â€œCryptographers have an ethics\nproblem.â€201 When an attack on Tor by CMU researchers was allegedly used\n197 Moxie\nMarlinspike\nand\nTrevor\nPerrin:\nThe\nTextSecure\nRatchet\n(webpage).\nNov. 26, 2013. https://whispersystems.org/blog/advanced-ratcheting/. User num-\nbers\navailable\nat\nURL\nhttp://www.statista.com/statistics/260819/number-of-\nmonthly-active-whatsapp-users/\n198 Data from https://metrics.torproject.org/userstats-relay-country.html, 2014-11-01\nto 2015-11-29.\n199 An open letter from US researchers in cryptography and information security.\nJan. 24, 2014. http://masssurveillance.info/\n200 H. Abelson, R. Anderson, S. M. Bellovin, J. Benaloh, M. Blaze, W. Diï¬ƒe,\nJ. Gilmore, M. Green, S. Landau, P. G. Neumann, R. L. Rivest, J. I. Schiller,\nB. Schneier, M. Specter, D. J. Weitzner: Keys under doormats: mandating insecurity\nby requiring government access to all data and communications (2015). Available at\nhttp://www.crypto.com/papers/Keys Under Doormats FINAL.pdf. 2015. Earlier,\nrelated report: H. Abelson, R. Anderson, S. M. Bellovin, J. Benaloh, M. Blaze,\nW. Diï¬ƒe, J. Gilmore, P. G. Neumann, R. L. Rivest, J. I. Schiller, B. Schneier:\nThe risks of key recovery, key escrow, and trusted third-party encryption (1997).\nAvailable at http://academiccommons.columbia.edu/catalog/ac:127127\n201 Antonio Regalado: Cryptographers have an ethics problem. MIT Technology Review.\nSep. 13, 2013. John Bohannon: Breach of trust. Science Magazine, 347(6221), Jan. 30,\n2015.\n",
    "book_id": "the_moral_character_of_cryptographic_work",
    "book_title": "The Moral Character of Cryptographic Work",
    "book_author": "Unknown",
    "topic_id": "ai_policy",
    "topic_label": "policy",
    "chunk_index": 89
  },
  {
    "chunk_full": "46\nP. Rogaway\nto provide bulk anonymized data to the FBI, CMU and the researchers involved\nwere publicly shamed.202 The IACR itself has been getting more vocal, both\nwith the Copenhagen Resolution203 and the statement on Australiaâ€™s Defence\nTrade Controls Act.204\nWhile our community has embraced crypto-for-privacy less than I would like,\nthis has been a cultural issueâ€”and culture can change.\nI have heard it said that if you think cryptography is your solution, you donâ€™t\nunderstand your problem.205 If this quip is true, then our ï¬eld has gone seriously\nastray. But we can correct it. We need to make cryptography the solution to the\nproblem: â€œhow do you make surveillance more expensive?â€\nDan Bernstein speaks of interesting crypto and boring crypto. Interesting\ncrypto is crypto that supports plenty of academic papers. Boring crypto\nis â€œcrypto that simply works, solidly resists attacks, [and] never needs any\nupgrades.â€ Dan asks, in his typically ï¬‚ippant way,\nWhat will happen if the crypto users convince some crypto researchers to\nactually create boring crypto?\nNo more real-world attacks. No more emergency upgrades. Limited audience\nfor any minor attack improvements and for replacement crypto.\nThis is an existential threat against future crypto research.206\nIf this is boring crypto, we need to go do some.\nCypherpunk cryptography has been described as crypto with an attitude.207\nBut it is much more than that, for, more than anything else, what the\ncypherpunks wanted was crypto with values. And values, deeply felt and deeply\n202 Tor security advisory: â€œrelay earlyâ€ traï¬ƒc conï¬rmation attack. July 30, 2014.\nhttp://tinyurl.com/tor-attack1. Tor project blog: Did the FBI pay a university\nto attack Tor users? Nov. 11, 2015. http://tinyurl.com/tor-attack2. This article\nincluded: â€œWhatever academic security research should be in the 21st century, it\ncertainly does not include â€˜experimentsâ€™ for pay that indiscriminately endanger\nstrangers without their knowledge or consent.â€ Also see reaction like: Joe Papp: The\nattempt by CMU experts to unmask Tor project software was appalling. Pittsburgh\nPost-Gazette. Aug. 5, 2014. http://tinyurl.com/papp-letter\n203 The statement, adopted May 14, 2014 at the business meeting in Copenhagen,\nDenmark, says: The membership of the IACR repudiates mass surveillance and the\nundermining of cryptographic solutions and standards. Population-wide surveillance\nthreatens democracy and human dignity. We call for expediting research and\ndeployment of eï¬€ective techniques to protect personal privacy against governmental\nand corporate overreach. https://www.iacr.org/misc/statement-May2014.html\n204 See https://www.iacr.org/petitions/australia-dtca/\n205 According to Butler Lampson (personal communications, Dec. 2015), the quote\n(or one like it) is from Roger Needham. Yet Needham, apparently, used to\nattribute it to Lampson: Ross Anderson: Security Engineering: A Guide to Building\nDependable Distributed Systems, p. 367, 2008, http://www.cl.cam.ac.uk/âˆ¼rja14/\nPapers/SE-18.pdf\n206 Dan Bernstein: Boring crypto. Talk at SPACE 2015. Malaviya National Institute of\nTechnology, Jaipur. Slides and audio at http://cr.yp.to/talks.html\n207 Steven Levy: Crypto rebels. Wired, Feb. 01, 1993.\n",
    "book_id": "the_moral_character_of_cryptographic_work",
    "book_title": "The Moral Character of Cryptographic Work",
    "book_author": "Unknown",
    "topic_id": "ai_policy",
    "topic_label": "policy",
    "chunk_index": 90
  },
  {
    "chunk_full": "The Moral Character of Cryptographic Work\n47\nembedded into our work, is what the cryptographic community needs most. And\nperhaps a dose of that cypherpunk verve.208\nIt has been said that just because you donâ€™t take an interest in politics,\ndoesnâ€™t mean politics wonâ€™t take an interest in you.209 Since cryptography is a\ntool for shifting power, the people who know this subject well, like it or not,\ninherit some of that power. As a cryptographer, you can ignore this landscape of\npower, and all political and moral dimensions of our ï¬eld. But that wonâ€™t make\nthem go away. It will just tend to make your work less relevant or socially useful.\nMy hope for this essay is that you will internalize this fact and recognize it\nas the starting point for developing an ethically driven vision for what you want\nto accomplish with your scientiï¬c work.\nI began this essay speaking of the Russellâ€“Einstein manifesto, so let me end\nthere as well, with Joseph Rotblatâ€™s plea from his Nobel prize acceptance speech:\nAt a time when science plays such a powerful role in the life of society, when\nthe destiny of the whole of mankind may hinge on the results of scientiï¬c\nresearch, it is incumbent on all scientists to be fully conscious of that role, and\nconduct themselves accordingly. I appeal to my fellow scientists to remember\ntheir responsibility to humanity.210\nAcknowledgments\nMy thanks go ï¬rst to Mihir Bellare for countless discussions on the topic of this\nessay. For years, not only have we collaborated closely on technical matters, but\nwe have also much discussed the values and sensibilities implicitly embedded\nwithin cryptographic work. Without Mihir, not only would I have done far less\ntechnically, but I would also understand far less about who cryptographers are.\nRon Rivest not only provided useful comments, but has been much on my\nmind as I have agonized over this essay. Many other people have given me\nimportant suggestions and ideas. I would like to thank Jake Appelbaum, Ross\nAnderson, Tom Berson, Dan Boneh, David Chaum, Joan Feigenbaum, Pooya\nFarshim, Seda GÂ¨urses, Tanja Lange, Chip Martel, Chanathip Namprempre,\nIlya Mironov, Chris Patton, Charles Raab, Tom Ristenpart, Amit Sahai, Rylan\nSchaeï¬€er, Adi Shamir, Jessica Malekos Smith, Christopher Soghoian, Richard\nStallman, Colleen Swanson, BjÂ¨orn Tackmann, Helen Thom, Jesse Walker, Jacob\nWeber, and Yusi (James) Zhang for their comments, discussions, and corrections.\n208 John Perry Barlow: A declaration of the independence of cyberspace. Feb. 8,\n1996. Eric Hughes: A cypherpunkâ€™s manifesto. March 9, 1993. Timothy May: The\ncyphernomicon. Sept. 10, 1994. Aaron Swartz: Guerilla open access manifesto. July\n2008.\n209 Barry Popik indicates that the quote â€œhas been attributed to Greek leader Pericles\n(495â€“429 BC), but only since the late 1990s. The Greek source is never identiï¬ed in\nthe frequent citations. The quotation appears to be of modern origin.â€ Blog entry,\nJune 22, 2011. http://tinyurl.com/not-pericles\n210 Joseph Rotblat: Remember Your Humanity. Acceptance and Nobel lecture, 1995.\nText available at Nobelprize.org\n",
    "book_id": "the_moral_character_of_cryptographic_work",
    "book_title": "The Moral Character of Cryptographic Work",
    "book_author": "Unknown",
    "topic_id": "ai_policy",
    "topic_label": "policy",
    "chunk_index": 91
  },
  {
    "chunk_full": "48\nP. Rogaway\nMy view of what science is and what the scientist should be was strongly\nshaped by watching Jacob Bronowski when I was a child.211\nAll original technical work mentioned in this essay (e.g., what is described\nin the ï¬rst pages of Part 4) was supported by NSF Grant CNS 1228828. But I\nemphasize that all opinions, ï¬ndings, conclusions and recommendations in this\nessay (and this essay is mostly opinions and recommendations) reï¬‚ect the views\nof the author alone, not necessarily the views of the National Science Foundation.\nThanks to the Schloss Dagstuhl staï¬€and to the participants of workshop\n14401, Privacy and Security in an Age of Surveillance, where ideas related to\nthis essay were discussed.212\nSome of the work on this essay was done while I was a guest professor at\nENS, Paris, hosted by David Pointcheval.\nMy thanks to the IACR Board for the privilege of giving this yearâ€™s\nIACR Distinguished Lecture. It is an honor that happens at most once in a\ncryptographerâ€™s career, and I have tried my best to use this opportunity wisely.\nThis essay owes its existence to the courage of Edward Snowden.\n211 Jacob Bronowski: The Ascent of Man. TV series. BBC and Time-Life Films, 1973.\n212 Bart Preneel, Phillip Rogaway, Mark D. Ryan, and Peter Y. A. Ryan: Privacy and\nsecurity in an age of surveillance (Dagstuhl perspectives workshop 14401). Dagstuhl\nManifestos, 5(1), pp. 25-37, 2015.\n",
    "book_id": "the_moral_character_of_cryptographic_work",
    "book_title": "The Moral Character of Cryptographic Work",
    "book_author": "Unknown",
    "topic_id": "ai_policy",
    "topic_label": "policy",
    "chunk_index": 92
  }
]